2023-01-03 10:19:14,252 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-101914/2023.01.03-101914.log
2023-01-03 10:19:14,259 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-03 10:19:14,260 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-03 10:19:23,298 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-03 10:19:23,298 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-03 10:19:23,303 - 

2023-01-03 10:19:23,303 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:24,378 - Epoch: [0][   10/   37]    Overall Loss 1.096107    Objective Loss 1.096107                                        LR 0.000060    Time 0.107388    
2023-01-03 10:19:25,278 - Epoch: [0][   20/   37]    Overall Loss 1.092156    Objective Loss 1.092156                                        LR 0.000060    Time 0.098671    
2023-01-03 10:19:26,144 - Epoch: [0][   30/   37]    Overall Loss 1.084895    Objective Loss 1.084895                                        LR 0.000060    Time 0.094602    
2023-01-03 10:19:26,670 - Epoch: [0][   37/   37]    Overall Loss 1.075898    Objective Loss 1.075898    Top1 56.276151    LR 0.000060    Time 0.090909    
2023-01-03 10:19:26,702 - --- validate (epoch=0)-----------
2023-01-03 10:19:26,702 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:27,023 - Epoch: [0][    5/    5]    Loss 1.008039    Top1 59.064885    
2023-01-03 10:19:27,054 - ==> Top1: 59.065    Loss: 1.008

2023-01-03 10:19:27,054 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:27,057 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 0]
2023-01-03 10:19:27,057 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:27,067 - 

2023-01-03 10:19:27,067 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:28,203 - Epoch: [1][   10/   37]    Overall Loss 0.965123    Objective Loss 0.965123                                        LR 0.000060    Time 0.113414    
2023-01-03 10:19:29,123 - Epoch: [1][   20/   37]    Overall Loss 0.907193    Objective Loss 0.907193                                        LR 0.000060    Time 0.102669    
2023-01-03 10:19:30,015 - Epoch: [1][   30/   37]    Overall Loss 0.863777    Objective Loss 0.863777                                        LR 0.000060    Time 0.098163    
2023-01-03 10:19:30,543 - Epoch: [1][   37/   37]    Overall Loss 0.839654    Objective Loss 0.839654    Top1 56.903766    LR 0.000060    Time 0.093845    
2023-01-03 10:19:30,583 - --- validate (epoch=1)-----------
2023-01-03 10:19:30,583 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:30,874 - Epoch: [1][    5/    5]    Loss 0.717122    Top1 59.064885    
2023-01-03 10:19:30,905 - ==> Top1: 59.065    Loss: 0.717

2023-01-03 10:19:30,905 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:30,907 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 1]
2023-01-03 10:19:30,907 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:30,916 - 

2023-01-03 10:19:30,916 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:32,073 - Epoch: [2][   10/   37]    Overall Loss 0.726759    Objective Loss 0.726759                                        LR 0.000060    Time 0.115500    
2023-01-03 10:19:32,980 - Epoch: [2][   20/   37]    Overall Loss 0.723688    Objective Loss 0.723688                                        LR 0.000060    Time 0.103064    
2023-01-03 10:19:33,871 - Epoch: [2][   30/   37]    Overall Loss 0.720285    Objective Loss 0.720285                                        LR 0.000060    Time 0.098409    
2023-01-03 10:19:34,403 - Epoch: [2][   37/   37]    Overall Loss 0.718944    Objective Loss 0.718944    Top1 58.368201    LR 0.000060    Time 0.094153    
2023-01-03 10:19:34,439 - --- validate (epoch=2)-----------
2023-01-03 10:19:34,439 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:34,751 - Epoch: [2][    5/    5]    Loss 0.714863    Top1 59.064885    
2023-01-03 10:19:34,781 - ==> Top1: 59.065    Loss: 0.715

2023-01-03 10:19:34,782 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:34,784 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 2]
2023-01-03 10:19:34,784 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:34,794 - 

2023-01-03 10:19:34,795 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:35,914 - Epoch: [3][   10/   37]    Overall Loss 0.714974    Objective Loss 0.714974                                        LR 0.000060    Time 0.111762    
2023-01-03 10:19:36,812 - Epoch: [3][   20/   37]    Overall Loss 0.712541    Objective Loss 0.712541                                        LR 0.000060    Time 0.100717    
2023-01-03 10:19:37,704 - Epoch: [3][   30/   37]    Overall Loss 0.712779    Objective Loss 0.712779                                        LR 0.000060    Time 0.096868    
2023-01-03 10:19:38,248 - Epoch: [3][   37/   37]    Overall Loss 0.710620    Objective Loss 0.710620    Top1 58.786611    LR 0.000060    Time 0.093240    
2023-01-03 10:19:38,288 - --- validate (epoch=3)-----------
2023-01-03 10:19:38,288 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:38,591 - Epoch: [3][    5/    5]    Loss 0.697260    Top1 59.064885    
2023-01-03 10:19:38,626 - ==> Top1: 59.065    Loss: 0.697

2023-01-03 10:19:38,627 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:38,629 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 3]
2023-01-03 10:19:38,629 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:38,641 - 

2023-01-03 10:19:38,641 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:39,783 - Epoch: [4][   10/   37]    Overall Loss 0.709226    Objective Loss 0.709226                                        LR 0.000060    Time 0.114091    
2023-01-03 10:19:40,685 - Epoch: [4][   20/   37]    Overall Loss 0.707813    Objective Loss 0.707813                                        LR 0.000060    Time 0.102119    
2023-01-03 10:19:41,577 - Epoch: [4][   30/   37]    Overall Loss 0.706088    Objective Loss 0.706088                                        LR 0.000060    Time 0.097801    
2023-01-03 10:19:42,111 - Epoch: [4][   37/   37]    Overall Loss 0.705511    Objective Loss 0.705511    Top1 55.439331    LR 0.000060    Time 0.093708    
2023-01-03 10:19:42,151 - --- validate (epoch=4)-----------
2023-01-03 10:19:42,151 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:42,468 - Epoch: [4][    5/    5]    Loss 0.687930    Top1 59.064885    
2023-01-03 10:19:42,502 - ==> Top1: 59.065    Loss: 0.688

2023-01-03 10:19:42,502 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:42,504 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 4]
2023-01-03 10:19:42,504 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:42,517 - 

2023-01-03 10:19:42,517 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:43,690 - Epoch: [5][   10/   37]    Overall Loss 0.708656    Objective Loss 0.708656                                        LR 0.000060    Time 0.117186    
2023-01-03 10:19:44,592 - Epoch: [5][   20/   37]    Overall Loss 0.706150    Objective Loss 0.706150                                        LR 0.000060    Time 0.103621    
2023-01-03 10:19:45,467 - Epoch: [5][   30/   37]    Overall Loss 0.703676    Objective Loss 0.703676                                        LR 0.000060    Time 0.098257    
2023-01-03 10:19:46,006 - Epoch: [5][   37/   37]    Overall Loss 0.701979    Objective Loss 0.701979    Top1 58.158996    LR 0.000060    Time 0.094224    
2023-01-03 10:19:46,041 - --- validate (epoch=5)-----------
2023-01-03 10:19:46,041 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:46,339 - Epoch: [5][    5/    5]    Loss 0.684680    Top1 59.064885    
2023-01-03 10:19:46,375 - ==> Top1: 59.065    Loss: 0.685

2023-01-03 10:19:46,376 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:46,378 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 5]
2023-01-03 10:19:46,378 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:46,387 - 

2023-01-03 10:19:46,387 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:47,533 - Epoch: [6][   10/   37]    Overall Loss 0.700653    Objective Loss 0.700653                                        LR 0.000060    Time 0.114467    
2023-01-03 10:19:48,428 - Epoch: [6][   20/   37]    Overall Loss 0.697750    Objective Loss 0.697750                                        LR 0.000060    Time 0.101950    
2023-01-03 10:19:49,310 - Epoch: [6][   30/   37]    Overall Loss 0.698608    Objective Loss 0.698608                                        LR 0.000060    Time 0.097341    
2023-01-03 10:19:49,867 - Epoch: [6][   37/   37]    Overall Loss 0.699205    Objective Loss 0.699205    Top1 52.719665    LR 0.000060    Time 0.093964    
2023-01-03 10:19:49,902 - --- validate (epoch=6)-----------
2023-01-03 10:19:49,902 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:50,209 - Epoch: [6][    5/    5]    Loss 0.692602    Top1 59.064885    
2023-01-03 10:19:50,245 - ==> Top1: 59.065    Loss: 0.693

2023-01-03 10:19:50,245 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:50,247 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 6]
2023-01-03 10:19:50,247 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:50,258 - 

2023-01-03 10:19:50,259 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:51,380 - Epoch: [7][   10/   37]    Overall Loss 0.700538    Objective Loss 0.700538                                        LR 0.000060    Time 0.112002    
2023-01-03 10:19:52,268 - Epoch: [7][   20/   37]    Overall Loss 0.699031    Objective Loss 0.699031                                        LR 0.000060    Time 0.100372    
2023-01-03 10:19:53,164 - Epoch: [7][   30/   37]    Overall Loss 0.698143    Objective Loss 0.698143                                        LR 0.000060    Time 0.096774    
2023-01-03 10:19:53,710 - Epoch: [7][   37/   37]    Overall Loss 0.696983    Objective Loss 0.696983    Top1 59.623431    LR 0.000060    Time 0.093201    
2023-01-03 10:19:53,746 - --- validate (epoch=7)-----------
2023-01-03 10:19:53,746 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:54,058 - Epoch: [7][    5/    5]    Loss 0.687975    Top1 59.064885    
2023-01-03 10:19:54,088 - ==> Top1: 59.065    Loss: 0.688

2023-01-03 10:19:54,089 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:54,090 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 7]
2023-01-03 10:19:54,091 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:54,102 - 

2023-01-03 10:19:54,103 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:55,385 - Epoch: [8][   10/   37]    Overall Loss 0.697390    Objective Loss 0.697390                                        LR 0.000060    Time 0.128077    
2023-01-03 10:19:56,286 - Epoch: [8][   20/   37]    Overall Loss 0.695661    Objective Loss 0.695661                                        LR 0.000060    Time 0.109045    
2023-01-03 10:19:57,172 - Epoch: [8][   30/   37]    Overall Loss 0.695392    Objective Loss 0.695392                                        LR 0.000060    Time 0.102213    
2023-01-03 10:19:57,715 - Epoch: [8][   37/   37]    Overall Loss 0.695011    Objective Loss 0.695011    Top1 56.066946    LR 0.000060    Time 0.097538    
2023-01-03 10:19:57,759 - --- validate (epoch=8)-----------
2023-01-03 10:19:57,759 - 1048 samples (256 per mini-batch)
2023-01-03 10:19:58,072 - Epoch: [8][    5/    5]    Loss 0.699417    Top1 59.064885    
2023-01-03 10:19:58,106 - ==> Top1: 59.065    Loss: 0.699

2023-01-03 10:19:58,107 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:19:58,109 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 8]
2023-01-03 10:19:58,109 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:19:58,121 - 

2023-01-03 10:19:58,121 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:19:59,279 - Epoch: [9][   10/   37]    Overall Loss 0.692476    Objective Loss 0.692476                                        LR 0.000060    Time 0.115601    
2023-01-03 10:20:00,184 - Epoch: [9][   20/   37]    Overall Loss 0.694893    Objective Loss 0.694893                                        LR 0.000060    Time 0.103053    
2023-01-03 10:20:01,059 - Epoch: [9][   30/   37]    Overall Loss 0.694369    Objective Loss 0.694369                                        LR 0.000060    Time 0.097826    
2023-01-03 10:20:01,596 - Epoch: [9][   37/   37]    Overall Loss 0.693526    Objective Loss 0.693526    Top1 55.230126    LR 0.000060    Time 0.093822    
2023-01-03 10:20:01,634 - --- validate (epoch=9)-----------
2023-01-03 10:20:01,635 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:01,949 - Epoch: [9][    5/    5]    Loss 0.693642    Top1 59.064885    
2023-01-03 10:20:01,986 - ==> Top1: 59.065    Loss: 0.694

2023-01-03 10:20:01,987 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:01,989 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 9]
2023-01-03 10:20:01,989 - Saving checkpoint to: logs/2023.01.03-101914/checkpoint.pth.tar
2023-01-03 10:20:02,016 - 

2023-01-03 10:20:02,017 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:03,342 - Epoch: [10][   10/   37]    Overall Loss 0.859808    Objective Loss 0.859808                                        LR 0.000060    Time 0.132416    
2023-01-03 10:20:04,456 - Epoch: [10][   20/   37]    Overall Loss 0.785768    Objective Loss 0.785768                                        LR 0.000060    Time 0.121882    
2023-01-03 10:20:05,552 - Epoch: [10][   30/   37]    Overall Loss 0.753141    Objective Loss 0.753141                                        LR 0.000060    Time 0.117776    
2023-01-03 10:20:06,251 - Epoch: [10][   37/   37]    Overall Loss 0.739806    Objective Loss 0.739806    Top1 60.251046    LR 0.000060    Time 0.114358    
2023-01-03 10:20:06,291 - --- validate (epoch=10)-----------
2023-01-03 10:20:06,292 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:06,728 - Epoch: [10][    5/    5]    Loss 0.679844    Top1 59.064885    
2023-01-03 10:20:06,761 - ==> Top1: 59.065    Loss: 0.680

2023-01-03 10:20:06,762 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:06,763 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 10]
2023-01-03 10:20:06,764 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:06,775 - 

2023-01-03 10:20:06,775 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:08,158 - Epoch: [11][   10/   37]    Overall Loss 0.687054    Objective Loss 0.687054                                        LR 0.000060    Time 0.138156    
2023-01-03 10:20:09,261 - Epoch: [11][   20/   37]    Overall Loss 0.683349    Objective Loss 0.683349                                        LR 0.000060    Time 0.124159    
2023-01-03 10:20:10,378 - Epoch: [11][   30/   37]    Overall Loss 0.684267    Objective Loss 0.684267                                        LR 0.000060    Time 0.120001    
2023-01-03 10:20:11,085 - Epoch: [11][   37/   37]    Overall Loss 0.683845    Objective Loss 0.683845    Top1 58.158996    LR 0.000060    Time 0.116396    
2023-01-03 10:20:11,127 - --- validate (epoch=11)-----------
2023-01-03 10:20:11,127 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:11,540 - Epoch: [11][    5/    5]    Loss 0.677639    Top1 59.064885    
2023-01-03 10:20:11,574 - ==> Top1: 59.065    Loss: 0.678

2023-01-03 10:20:11,574 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:11,576 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 11]
2023-01-03 10:20:11,576 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:11,587 - 

2023-01-03 10:20:11,587 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:12,988 - Epoch: [12][   10/   37]    Overall Loss 0.684706    Objective Loss 0.684706                                        LR 0.000060    Time 0.139853    
2023-01-03 10:20:14,106 - Epoch: [12][   20/   37]    Overall Loss 0.683865    Objective Loss 0.683865                                        LR 0.000060    Time 0.125794    
2023-01-03 10:20:15,219 - Epoch: [12][   30/   37]    Overall Loss 0.684214    Objective Loss 0.684214                                        LR 0.000060    Time 0.120972    
2023-01-03 10:20:15,938 - Epoch: [12][   37/   37]    Overall Loss 0.683948    Objective Loss 0.683948    Top1 56.903766    LR 0.000060    Time 0.117483    
2023-01-03 10:20:15,976 - --- validate (epoch=12)-----------
2023-01-03 10:20:15,976 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:16,417 - Epoch: [12][    5/    5]    Loss 0.681951    Top1 59.064885    
2023-01-03 10:20:16,447 - ==> Top1: 59.065    Loss: 0.682

2023-01-03 10:20:16,447 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:16,450 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 12]
2023-01-03 10:20:16,450 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:16,462 - 

2023-01-03 10:20:16,462 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:17,818 - Epoch: [13][   10/   37]    Overall Loss 0.684284    Objective Loss 0.684284                                        LR 0.000060    Time 0.135394    
2023-01-03 10:20:18,921 - Epoch: [13][   20/   37]    Overall Loss 0.683115    Objective Loss 0.683115                                        LR 0.000060    Time 0.122803    
2023-01-03 10:20:20,043 - Epoch: [13][   30/   37]    Overall Loss 0.682709    Objective Loss 0.682709                                        LR 0.000060    Time 0.119273    
2023-01-03 10:20:20,748 - Epoch: [13][   37/   37]    Overall Loss 0.683533    Objective Loss 0.683533    Top1 57.531381    LR 0.000060    Time 0.115733    
2023-01-03 10:20:20,787 - --- validate (epoch=13)-----------
2023-01-03 10:20:20,787 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:21,243 - Epoch: [13][    5/    5]    Loss 0.675081    Top1 59.064885    
2023-01-03 10:20:21,279 - ==> Top1: 59.065    Loss: 0.675

2023-01-03 10:20:21,280 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:21,282 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 13]
2023-01-03 10:20:21,283 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:21,292 - 

2023-01-03 10:20:21,292 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:22,674 - Epoch: [14][   10/   37]    Overall Loss 0.683624    Objective Loss 0.683624                                        LR 0.000060    Time 0.138031    
2023-01-03 10:20:23,771 - Epoch: [14][   20/   37]    Overall Loss 0.685614    Objective Loss 0.685614                                        LR 0.000060    Time 0.123825    
2023-01-03 10:20:24,860 - Epoch: [14][   30/   37]    Overall Loss 0.684397    Objective Loss 0.684397                                        LR 0.000060    Time 0.118852    
2023-01-03 10:20:25,554 - Epoch: [14][   37/   37]    Overall Loss 0.683386    Objective Loss 0.683386    Top1 58.786611    LR 0.000060    Time 0.115092    
2023-01-03 10:20:25,594 - --- validate (epoch=14)-----------
2023-01-03 10:20:25,595 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:26,025 - Epoch: [14][    5/    5]    Loss 0.671600    Top1 59.064885    
2023-01-03 10:20:26,056 - ==> Top1: 59.065    Loss: 0.672

2023-01-03 10:20:26,057 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:26,059 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 14]
2023-01-03 10:20:26,059 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:26,070 - 

2023-01-03 10:20:26,070 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:27,452 - Epoch: [15][   10/   37]    Overall Loss 0.676653    Objective Loss 0.676653                                        LR 0.000060    Time 0.138013    
2023-01-03 10:20:28,556 - Epoch: [15][   20/   37]    Overall Loss 0.682849    Objective Loss 0.682849                                        LR 0.000060    Time 0.124151    
2023-01-03 10:20:29,647 - Epoch: [15][   30/   37]    Overall Loss 0.683815    Objective Loss 0.683815                                        LR 0.000060    Time 0.119122    
2023-01-03 10:20:30,346 - Epoch: [15][   37/   37]    Overall Loss 0.683037    Objective Loss 0.683037    Top1 56.485356    LR 0.000060    Time 0.115468    
2023-01-03 10:20:30,384 - --- validate (epoch=15)-----------
2023-01-03 10:20:30,385 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:30,817 - Epoch: [15][    5/    5]    Loss 0.678796    Top1 59.064885    
2023-01-03 10:20:30,857 - ==> Top1: 59.065    Loss: 0.679

2023-01-03 10:20:30,858 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:30,860 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 15]
2023-01-03 10:20:30,860 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:30,870 - 

2023-01-03 10:20:30,870 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:32,241 - Epoch: [16][   10/   37]    Overall Loss 0.682996    Objective Loss 0.682996                                        LR 0.000060    Time 0.137009    
2023-01-03 10:20:33,342 - Epoch: [16][   20/   37]    Overall Loss 0.681391    Objective Loss 0.681391                                        LR 0.000060    Time 0.123468    
2023-01-03 10:20:34,428 - Epoch: [16][   30/   37]    Overall Loss 0.681488    Objective Loss 0.681488                                        LR 0.000060    Time 0.118494    
2023-01-03 10:20:35,123 - Epoch: [16][   37/   37]    Overall Loss 0.682147    Objective Loss 0.682147    Top1 54.393305    LR 0.000060    Time 0.114856    
2023-01-03 10:20:35,160 - --- validate (epoch=16)-----------
2023-01-03 10:20:35,161 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:35,591 - Epoch: [16][    5/    5]    Loss 0.671281    Top1 59.064885    
2023-01-03 10:20:35,623 - ==> Top1: 59.065    Loss: 0.671

2023-01-03 10:20:35,623 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:35,625 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 16]
2023-01-03 10:20:35,625 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:35,637 - 

2023-01-03 10:20:35,638 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:37,029 - Epoch: [17][   10/   37]    Overall Loss 0.682796    Objective Loss 0.682796                                        LR 0.000060    Time 0.138959    
2023-01-03 10:20:38,141 - Epoch: [17][   20/   37]    Overall Loss 0.681313    Objective Loss 0.681313                                        LR 0.000060    Time 0.125052    
2023-01-03 10:20:39,242 - Epoch: [17][   30/   37]    Overall Loss 0.682065    Objective Loss 0.682065                                        LR 0.000060    Time 0.120067    
2023-01-03 10:20:39,942 - Epoch: [17][   37/   37]    Overall Loss 0.681479    Objective Loss 0.681479    Top1 57.740586    LR 0.000060    Time 0.116252    
2023-01-03 10:20:39,981 - --- validate (epoch=17)-----------
2023-01-03 10:20:39,981 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:40,417 - Epoch: [17][    5/    5]    Loss 0.672640    Top1 59.064885    
2023-01-03 10:20:40,450 - ==> Top1: 59.065    Loss: 0.673

2023-01-03 10:20:40,450 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:40,452 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 17]
2023-01-03 10:20:40,452 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:40,469 - 

2023-01-03 10:20:40,469 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:41,851 - Epoch: [18][   10/   37]    Overall Loss 0.677250    Objective Loss 0.677250                                        LR 0.000060    Time 0.138081    
2023-01-03 10:20:42,968 - Epoch: [18][   20/   37]    Overall Loss 0.680635    Objective Loss 0.680635                                        LR 0.000060    Time 0.124818    
2023-01-03 10:20:44,073 - Epoch: [18][   30/   37]    Overall Loss 0.680170    Objective Loss 0.680170                                        LR 0.000060    Time 0.120051    
2023-01-03 10:20:44,775 - Epoch: [18][   37/   37]    Overall Loss 0.680853    Objective Loss 0.680853    Top1 57.322176    LR 0.000060    Time 0.116287    
2023-01-03 10:20:44,813 - --- validate (epoch=18)-----------
2023-01-03 10:20:44,814 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:45,236 - Epoch: [18][    5/    5]    Loss 0.666596    Top1 59.064885    
2023-01-03 10:20:45,274 - ==> Top1: 59.065    Loss: 0.667

2023-01-03 10:20:45,274 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:45,276 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 18]
2023-01-03 10:20:45,276 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:45,293 - 

2023-01-03 10:20:45,293 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:46,693 - Epoch: [19][   10/   37]    Overall Loss 0.680767    Objective Loss 0.680767                                        LR 0.000060    Time 0.139823    
2023-01-03 10:20:47,807 - Epoch: [19][   20/   37]    Overall Loss 0.677836    Objective Loss 0.677836                                        LR 0.000060    Time 0.125577    
2023-01-03 10:20:48,907 - Epoch: [19][   30/   37]    Overall Loss 0.678364    Objective Loss 0.678364                                        LR 0.000060    Time 0.120376    
2023-01-03 10:20:49,623 - Epoch: [19][   37/   37]    Overall Loss 0.678508    Objective Loss 0.678508    Top1 53.765690    LR 0.000060    Time 0.116925    
2023-01-03 10:20:49,668 - --- validate (epoch=19)-----------
2023-01-03 10:20:49,668 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:50,110 - Epoch: [19][    5/    5]    Loss 0.677649    Top1 59.064885    
2023-01-03 10:20:50,148 - ==> Top1: 59.065    Loss: 0.678

2023-01-03 10:20:50,149 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:50,151 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 19]
2023-01-03 10:20:50,151 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:50,168 - 

2023-01-03 10:20:50,168 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:51,516 - Epoch: [20][   10/   37]    Overall Loss 0.677620    Objective Loss 0.677620                                        LR 0.000060    Time 0.134635    
2023-01-03 10:20:52,652 - Epoch: [20][   20/   37]    Overall Loss 0.676423    Objective Loss 0.676423                                        LR 0.000060    Time 0.124085    
2023-01-03 10:20:53,755 - Epoch: [20][   30/   37]    Overall Loss 0.675059    Objective Loss 0.675059                                        LR 0.000060    Time 0.119467    
2023-01-03 10:20:54,463 - Epoch: [20][   37/   37]    Overall Loss 0.674674    Objective Loss 0.674674    Top1 58.368201    LR 0.000060    Time 0.115980    
2023-01-03 10:20:54,501 - --- validate (epoch=20)-----------
2023-01-03 10:20:54,502 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:54,922 - Epoch: [20][    5/    5]    Loss 0.665873    Top1 59.064885    
2023-01-03 10:20:54,958 - ==> Top1: 59.065    Loss: 0.666

2023-01-03 10:20:54,958 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:20:54,960 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 155168 on epoch: 20]
2023-01-03 10:20:54,960 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:54,976 - 

2023-01-03 10:20:54,977 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:20:56,323 - Epoch: [21][   10/   37]    Overall Loss 0.668408    Objective Loss 0.668408                                        LR 0.000060    Time 0.134457    
2023-01-03 10:20:57,438 - Epoch: [21][   20/   37]    Overall Loss 0.670845    Objective Loss 0.670845                                        LR 0.000060    Time 0.122934    
2023-01-03 10:20:58,570 - Epoch: [21][   30/   37]    Overall Loss 0.666905    Objective Loss 0.666905                                        LR 0.000060    Time 0.119675    
2023-01-03 10:20:59,289 - Epoch: [21][   37/   37]    Overall Loss 0.664749    Objective Loss 0.664749    Top1 56.276151    LR 0.000060    Time 0.116454    
2023-01-03 10:20:59,331 - --- validate (epoch=21)-----------
2023-01-03 10:20:59,331 - 1048 samples (256 per mini-batch)
2023-01-03 10:20:59,768 - Epoch: [21][    5/    5]    Loss 0.647830    Top1 60.400763    
2023-01-03 10:20:59,804 - ==> Top1: 60.401    Loss: 0.648

2023-01-03 10:20:59,804 - ==> Confusion:
[[ 35 394   0]
 [ 21 598   0]
 [  0   0   0]]

2023-01-03 10:20:59,807 - ==> Best [Top1: 60.401   Sparsity:0.00   Params: 155168 on epoch: 21]
2023-01-03 10:20:59,807 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:20:59,823 - 

2023-01-03 10:20:59,823 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:01,210 - Epoch: [22][   10/   37]    Overall Loss 0.652277    Objective Loss 0.652277                                        LR 0.000060    Time 0.138540    
2023-01-03 10:21:02,352 - Epoch: [22][   20/   37]    Overall Loss 0.650276    Objective Loss 0.650276                                        LR 0.000060    Time 0.126341    
2023-01-03 10:21:03,495 - Epoch: [22][   30/   37]    Overall Loss 0.645124    Objective Loss 0.645124                                        LR 0.000060    Time 0.122289    
2023-01-03 10:21:04,214 - Epoch: [22][   37/   37]    Overall Loss 0.642317    Objective Loss 0.642317    Top1 62.970711    LR 0.000060    Time 0.118585    
2023-01-03 10:21:04,255 - --- validate (epoch=22)-----------
2023-01-03 10:21:04,255 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:04,682 - Epoch: [22][    5/    5]    Loss 0.628095    Top1 68.034351    
2023-01-03 10:21:04,721 - ==> Top1: 68.034    Loss: 0.628

2023-01-03 10:21:04,721 - ==> Confusion:
[[226 203   0]
 [132 487   0]
 [  0   0   0]]

2023-01-03 10:21:04,723 - ==> Best [Top1: 68.034   Sparsity:0.00   Params: 155168 on epoch: 22]
2023-01-03 10:21:04,723 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:04,733 - 

2023-01-03 10:21:04,733 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:06,237 - Epoch: [23][   10/   37]    Overall Loss 0.620056    Objective Loss 0.620056                                        LR 0.000060    Time 0.150245    
2023-01-03 10:21:07,375 - Epoch: [23][   20/   37]    Overall Loss 0.620007    Objective Loss 0.620007                                        LR 0.000060    Time 0.131999    
2023-01-03 10:21:08,513 - Epoch: [23][   30/   37]    Overall Loss 0.616275    Objective Loss 0.616275                                        LR 0.000060    Time 0.125917    
2023-01-03 10:21:09,219 - Epoch: [23][   37/   37]    Overall Loss 0.612385    Objective Loss 0.612385    Top1 67.573222    LR 0.000060    Time 0.121163    
2023-01-03 10:21:09,258 - --- validate (epoch=23)-----------
2023-01-03 10:21:09,259 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:09,695 - Epoch: [23][    5/    5]    Loss 0.589266    Top1 68.606870    
2023-01-03 10:21:09,732 - ==> Top1: 68.607    Loss: 0.589

2023-01-03 10:21:09,732 - ==> Confusion:
[[197 232   0]
 [ 97 522   0]
 [  0   0   0]]

2023-01-03 10:21:09,741 - ==> Best [Top1: 68.607   Sparsity:0.00   Params: 155168 on epoch: 23]
2023-01-03 10:21:09,742 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:09,751 - 

2023-01-03 10:21:09,751 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:11,134 - Epoch: [24][   10/   37]    Overall Loss 0.592745    Objective Loss 0.592745                                        LR 0.000060    Time 0.138185    
2023-01-03 10:21:12,273 - Epoch: [24][   20/   37]    Overall Loss 0.592572    Objective Loss 0.592572                                        LR 0.000060    Time 0.125996    
2023-01-03 10:21:13,416 - Epoch: [24][   30/   37]    Overall Loss 0.589892    Objective Loss 0.589892                                        LR 0.000060    Time 0.122096    
2023-01-03 10:21:14,128 - Epoch: [24][   37/   37]    Overall Loss 0.587666    Objective Loss 0.587666    Top1 69.246862    LR 0.000060    Time 0.118224    
2023-01-03 10:21:14,170 - --- validate (epoch=24)-----------
2023-01-03 10:21:14,171 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:14,616 - Epoch: [24][    5/    5]    Loss 0.553169    Top1 70.133588    
2023-01-03 10:21:14,650 - ==> Top1: 70.134    Loss: 0.553

2023-01-03 10:21:14,651 - ==> Confusion:
[[224 205   0]
 [108 511   0]
 [  0   0   0]]

2023-01-03 10:21:14,652 - ==> Best [Top1: 70.134   Sparsity:0.00   Params: 155168 on epoch: 24]
2023-01-03 10:21:14,653 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:14,669 - 

2023-01-03 10:21:14,669 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:16,080 - Epoch: [25][   10/   37]    Overall Loss 0.585207    Objective Loss 0.585207                                        LR 0.000060    Time 0.140970    
2023-01-03 10:21:17,198 - Epoch: [25][   20/   37]    Overall Loss 0.575151    Objective Loss 0.575151                                        LR 0.000060    Time 0.126323    
2023-01-03 10:21:18,307 - Epoch: [25][   30/   37]    Overall Loss 0.566753    Objective Loss 0.566753                                        LR 0.000060    Time 0.121189    
2023-01-03 10:21:19,024 - Epoch: [25][   37/   37]    Overall Loss 0.566057    Objective Loss 0.566057    Top1 72.384937    LR 0.000060    Time 0.117634    
2023-01-03 10:21:19,064 - --- validate (epoch=25)-----------
2023-01-03 10:21:19,064 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:19,486 - Epoch: [25][    5/    5]    Loss 0.544842    Top1 72.996183    
2023-01-03 10:21:19,524 - ==> Top1: 72.996    Loss: 0.545

2023-01-03 10:21:19,525 - ==> Confusion:
[[292 137   0]
 [146 473   0]
 [  0   0   0]]

2023-01-03 10:21:19,527 - ==> Best [Top1: 72.996   Sparsity:0.00   Params: 155168 on epoch: 25]
2023-01-03 10:21:19,527 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:19,543 - 

2023-01-03 10:21:19,544 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:20,950 - Epoch: [26][   10/   37]    Overall Loss 0.549028    Objective Loss 0.549028                                        LR 0.000060    Time 0.140479    
2023-01-03 10:21:22,089 - Epoch: [26][   20/   37]    Overall Loss 0.552828    Objective Loss 0.552828                                        LR 0.000060    Time 0.127146    
2023-01-03 10:21:23,205 - Epoch: [26][   30/   37]    Overall Loss 0.548715    Objective Loss 0.548715                                        LR 0.000060    Time 0.121925    
2023-01-03 10:21:23,927 - Epoch: [26][   37/   37]    Overall Loss 0.550843    Objective Loss 0.550843    Top1 71.966527    LR 0.000060    Time 0.118376    
2023-01-03 10:21:23,968 - --- validate (epoch=26)-----------
2023-01-03 10:21:23,968 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:24,404 - Epoch: [26][    5/    5]    Loss 0.532119    Top1 76.145038    
2023-01-03 10:21:24,436 - ==> Top1: 76.145    Loss: 0.532

2023-01-03 10:21:24,436 - ==> Confusion:
[[327 102   0]
 [148 471   0]
 [  0   0   0]]

2023-01-03 10:21:24,438 - ==> Best [Top1: 76.145   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-03 10:21:24,438 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:24,451 - 

2023-01-03 10:21:24,451 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:25,811 - Epoch: [27][   10/   37]    Overall Loss 0.529396    Objective Loss 0.529396                                        LR 0.000060    Time 0.135908    
2023-01-03 10:21:26,924 - Epoch: [27][   20/   37]    Overall Loss 0.535949    Objective Loss 0.535949                                        LR 0.000060    Time 0.123560    
2023-01-03 10:21:28,022 - Epoch: [27][   30/   37]    Overall Loss 0.537678    Objective Loss 0.537678                                        LR 0.000060    Time 0.118960    
2023-01-03 10:21:28,731 - Epoch: [27][   37/   37]    Overall Loss 0.534782    Objective Loss 0.534782    Top1 72.594142    LR 0.000060    Time 0.115587    
2023-01-03 10:21:28,767 - --- validate (epoch=27)-----------
2023-01-03 10:21:28,768 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:29,197 - Epoch: [27][    5/    5]    Loss 0.504545    Top1 73.187023    
2023-01-03 10:21:29,231 - ==> Top1: 73.187    Loss: 0.505

2023-01-03 10:21:29,232 - ==> Confusion:
[[222 207   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-03 10:21:29,235 - ==> Best [Top1: 76.145   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-03 10:21:29,235 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:29,243 - 

2023-01-03 10:21:29,243 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:30,619 - Epoch: [28][   10/   37]    Overall Loss 0.520188    Objective Loss 0.520188                                        LR 0.000060    Time 0.137493    
2023-01-03 10:21:31,735 - Epoch: [28][   20/   37]    Overall Loss 0.525026    Objective Loss 0.525026                                        LR 0.000060    Time 0.124500    
2023-01-03 10:21:32,862 - Epoch: [28][   30/   37]    Overall Loss 0.523483    Objective Loss 0.523483                                        LR 0.000060    Time 0.120542    
2023-01-03 10:21:33,560 - Epoch: [28][   37/   37]    Overall Loss 0.523349    Objective Loss 0.523349    Top1 74.686192    LR 0.000060    Time 0.116603    
2023-01-03 10:21:33,599 - --- validate (epoch=28)-----------
2023-01-03 10:21:33,599 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:34,036 - Epoch: [28][    5/    5]    Loss 0.479499    Top1 75.572519    
2023-01-03 10:21:34,068 - ==> Top1: 75.573    Loss: 0.479

2023-01-03 10:21:34,068 - ==> Confusion:
[[284 145   0]
 [111 508   0]
 [  0   0   0]]

2023-01-03 10:21:34,070 - ==> Best [Top1: 76.145   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-03 10:21:34,070 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:34,079 - 

2023-01-03 10:21:34,079 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:35,439 - Epoch: [29][   10/   37]    Overall Loss 0.518583    Objective Loss 0.518583                                        LR 0.000060    Time 0.135799    
2023-01-03 10:21:36,531 - Epoch: [29][   20/   37]    Overall Loss 0.516254    Objective Loss 0.516254                                        LR 0.000060    Time 0.122465    
2023-01-03 10:21:37,630 - Epoch: [29][   30/   37]    Overall Loss 0.516110    Objective Loss 0.516110                                        LR 0.000060    Time 0.118263    
2023-01-03 10:21:38,336 - Epoch: [29][   37/   37]    Overall Loss 0.513787    Objective Loss 0.513787    Top1 75.313808    LR 0.000060    Time 0.114949    
2023-01-03 10:21:38,375 - --- validate (epoch=29)-----------
2023-01-03 10:21:38,376 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:38,805 - Epoch: [29][    5/    5]    Loss 0.499519    Top1 77.862595    
2023-01-03 10:21:38,838 - ==> Top1: 77.863    Loss: 0.500

2023-01-03 10:21:38,839 - ==> Confusion:
[[341  88   0]
 [144 475   0]
 [  0   0   0]]

2023-01-03 10:21:38,840 - ==> Best [Top1: 77.863   Sparsity:0.00   Params: 155168 on epoch: 29]
2023-01-03 10:21:38,841 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:38,853 - 

2023-01-03 10:21:38,854 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:40,244 - Epoch: [30][   10/   37]    Overall Loss 0.512162    Objective Loss 0.512162                                        LR 0.000060    Time 0.138874    
2023-01-03 10:21:41,344 - Epoch: [30][   20/   37]    Overall Loss 0.505496    Objective Loss 0.505496                                        LR 0.000060    Time 0.124419    
2023-01-03 10:21:42,432 - Epoch: [30][   30/   37]    Overall Loss 0.504563    Objective Loss 0.504563                                        LR 0.000060    Time 0.119203    
2023-01-03 10:21:43,137 - Epoch: [30][   37/   37]    Overall Loss 0.508446    Objective Loss 0.508446    Top1 72.384937    LR 0.000060    Time 0.115697    
2023-01-03 10:21:43,174 - --- validate (epoch=30)-----------
2023-01-03 10:21:43,174 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:43,599 - Epoch: [30][    5/    5]    Loss 0.457450    Top1 76.145038    
2023-01-03 10:21:43,637 - ==> Top1: 76.145    Loss: 0.457

2023-01-03 10:21:43,637 - ==> Confusion:
[[247 182   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-03 10:21:43,640 - ==> Best [Top1: 77.863   Sparsity:0.00   Params: 155168 on epoch: 29]
2023-01-03 10:21:43,640 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:43,647 - 

2023-01-03 10:21:43,648 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:45,056 - Epoch: [31][   10/   37]    Overall Loss 0.506442    Objective Loss 0.506442                                        LR 0.000060    Time 0.140682    
2023-01-03 10:21:46,164 - Epoch: [31][   20/   37]    Overall Loss 0.502422    Objective Loss 0.502422                                        LR 0.000060    Time 0.125703    
2023-01-03 10:21:47,277 - Epoch: [31][   30/   37]    Overall Loss 0.505550    Objective Loss 0.505550                                        LR 0.000060    Time 0.120894    
2023-01-03 10:21:47,983 - Epoch: [31][   37/   37]    Overall Loss 0.500311    Objective Loss 0.500311    Top1 77.615063    LR 0.000060    Time 0.117090    
2023-01-03 10:21:48,025 - --- validate (epoch=31)-----------
2023-01-03 10:21:48,025 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:48,441 - Epoch: [31][    5/    5]    Loss 0.477735    Top1 76.526718    
2023-01-03 10:21:48,478 - ==> Top1: 76.527    Loss: 0.478

2023-01-03 10:21:48,478 - ==> Confusion:
[[351  78   0]
 [168 451   0]
 [  0   0   0]]

2023-01-03 10:21:48,480 - ==> Best [Top1: 77.863   Sparsity:0.00   Params: 155168 on epoch: 29]
2023-01-03 10:21:48,480 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:48,487 - 

2023-01-03 10:21:48,487 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:49,845 - Epoch: [32][   10/   37]    Overall Loss 0.490465    Objective Loss 0.490465                                        LR 0.000060    Time 0.135628    
2023-01-03 10:21:50,980 - Epoch: [32][   20/   37]    Overall Loss 0.498473    Objective Loss 0.498473                                        LR 0.000060    Time 0.124534    
2023-01-03 10:21:52,075 - Epoch: [32][   30/   37]    Overall Loss 0.495164    Objective Loss 0.495164                                        LR 0.000060    Time 0.119494    
2023-01-03 10:21:52,773 - Epoch: [32][   37/   37]    Overall Loss 0.496139    Objective Loss 0.496139    Top1 77.405858    LR 0.000060    Time 0.115755    
2023-01-03 10:21:52,811 - --- validate (epoch=32)-----------
2023-01-03 10:21:52,811 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:53,255 - Epoch: [32][    5/    5]    Loss 0.463405    Top1 76.622137    
2023-01-03 10:21:53,291 - ==> Top1: 76.622    Loss: 0.463

2023-01-03 10:21:53,292 - ==> Confusion:
[[270 159   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-03 10:21:53,295 - ==> Best [Top1: 77.863   Sparsity:0.00   Params: 155168 on epoch: 29]
2023-01-03 10:21:53,295 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:53,304 - 

2023-01-03 10:21:53,304 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:54,670 - Epoch: [33][   10/   37]    Overall Loss 0.490473    Objective Loss 0.490473                                        LR 0.000060    Time 0.136460    
2023-01-03 10:21:55,778 - Epoch: [33][   20/   37]    Overall Loss 0.489663    Objective Loss 0.489663                                        LR 0.000060    Time 0.123608    
2023-01-03 10:21:56,873 - Epoch: [33][   30/   37]    Overall Loss 0.486682    Objective Loss 0.486682                                        LR 0.000060    Time 0.118868    
2023-01-03 10:21:57,582 - Epoch: [33][   37/   37]    Overall Loss 0.486854    Objective Loss 0.486854    Top1 78.661088    LR 0.000060    Time 0.115540    
2023-01-03 10:21:57,620 - --- validate (epoch=33)-----------
2023-01-03 10:21:57,621 - 1048 samples (256 per mini-batch)
2023-01-03 10:21:58,050 - Epoch: [33][    5/    5]    Loss 0.472381    Top1 78.816794    
2023-01-03 10:21:58,086 - ==> Top1: 78.817    Loss: 0.472

2023-01-03 10:21:58,087 - ==> Confusion:
[[307 122   0]
 [100 519   0]
 [  0   0   0]]

2023-01-03 10:21:58,088 - ==> Best [Top1: 78.817   Sparsity:0.00   Params: 155168 on epoch: 33]
2023-01-03 10:21:58,088 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:21:58,100 - 

2023-01-03 10:21:58,100 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:21:59,476 - Epoch: [34][   10/   37]    Overall Loss 0.480488    Objective Loss 0.480488                                        LR 0.000060    Time 0.137393    
2023-01-03 10:22:00,593 - Epoch: [34][   20/   37]    Overall Loss 0.486870    Objective Loss 0.486870                                        LR 0.000060    Time 0.124506    
2023-01-03 10:22:01,729 - Epoch: [34][   30/   37]    Overall Loss 0.484023    Objective Loss 0.484023                                        LR 0.000060    Time 0.120851    
2023-01-03 10:22:02,426 - Epoch: [34][   37/   37]    Overall Loss 0.478635    Objective Loss 0.478635    Top1 78.870293    LR 0.000060    Time 0.116832    
2023-01-03 10:22:02,467 - --- validate (epoch=34)-----------
2023-01-03 10:22:02,468 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:02,894 - Epoch: [34][    5/    5]    Loss 0.451087    Top1 78.148855    
2023-01-03 10:22:02,931 - ==> Top1: 78.149    Loss: 0.451

2023-01-03 10:22:02,932 - ==> Confusion:
[[322 107   0]
 [122 497   0]
 [  0   0   0]]

2023-01-03 10:22:02,935 - ==> Best [Top1: 78.817   Sparsity:0.00   Params: 155168 on epoch: 33]
2023-01-03 10:22:02,935 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:02,943 - 

2023-01-03 10:22:02,943 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:04,301 - Epoch: [35][   10/   37]    Overall Loss 0.478178    Objective Loss 0.478178                                        LR 0.000060    Time 0.135704    
2023-01-03 10:22:05,403 - Epoch: [35][   20/   37]    Overall Loss 0.472027    Objective Loss 0.472027                                        LR 0.000060    Time 0.122906    
2023-01-03 10:22:06,514 - Epoch: [35][   30/   37]    Overall Loss 0.475689    Objective Loss 0.475689                                        LR 0.000060    Time 0.118966    
2023-01-03 10:22:07,230 - Epoch: [35][   37/   37]    Overall Loss 0.475242    Objective Loss 0.475242    Top1 71.757322    LR 0.000060    Time 0.115796    
2023-01-03 10:22:07,269 - --- validate (epoch=35)-----------
2023-01-03 10:22:07,270 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:07,705 - Epoch: [35][    5/    5]    Loss 0.441877    Top1 78.530534    
2023-01-03 10:22:07,744 - ==> Top1: 78.531    Loss: 0.442

2023-01-03 10:22:07,744 - ==> Confusion:
[[274 155   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-03 10:22:07,746 - ==> Best [Top1: 78.817   Sparsity:0.00   Params: 155168 on epoch: 33]
2023-01-03 10:22:07,747 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:07,753 - 

2023-01-03 10:22:07,754 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:09,091 - Epoch: [36][   10/   37]    Overall Loss 0.467851    Objective Loss 0.467851                                        LR 0.000060    Time 0.133616    
2023-01-03 10:22:10,207 - Epoch: [36][   20/   37]    Overall Loss 0.467782    Objective Loss 0.467782                                        LR 0.000060    Time 0.122570    
2023-01-03 10:22:11,338 - Epoch: [36][   30/   37]    Overall Loss 0.470244    Objective Loss 0.470244                                        LR 0.000060    Time 0.119401    
2023-01-03 10:22:12,044 - Epoch: [36][   37/   37]    Overall Loss 0.468361    Objective Loss 0.468361    Top1 78.242678    LR 0.000060    Time 0.115891    
2023-01-03 10:22:12,084 - --- validate (epoch=36)-----------
2023-01-03 10:22:12,084 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:12,517 - Epoch: [36][    5/    5]    Loss 0.464603    Top1 79.007634    
2023-01-03 10:22:12,554 - ==> Top1: 79.008    Loss: 0.465

2023-01-03 10:22:12,554 - ==> Confusion:
[[281 148   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-03 10:22:12,557 - ==> Best [Top1: 79.008   Sparsity:0.00   Params: 155168 on epoch: 36]
2023-01-03 10:22:12,557 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:12,567 - 

2023-01-03 10:22:12,568 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:13,946 - Epoch: [37][   10/   37]    Overall Loss 0.448215    Objective Loss 0.448215                                        LR 0.000060    Time 0.137760    
2023-01-03 10:22:15,075 - Epoch: [37][   20/   37]    Overall Loss 0.452587    Objective Loss 0.452587                                        LR 0.000060    Time 0.125249    
2023-01-03 10:22:16,211 - Epoch: [37][   30/   37]    Overall Loss 0.456545    Objective Loss 0.456545                                        LR 0.000060    Time 0.121358    
2023-01-03 10:22:16,975 - Epoch: [37][   37/   37]    Overall Loss 0.460422    Objective Loss 0.460422    Top1 78.242678    LR 0.000060    Time 0.119034    
2023-01-03 10:22:17,009 - --- validate (epoch=37)-----------
2023-01-03 10:22:17,009 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:17,434 - Epoch: [37][    5/    5]    Loss 0.423721    Top1 79.675573    
2023-01-03 10:22:17,471 - ==> Top1: 79.676    Loss: 0.424

2023-01-03 10:22:17,473 - ==> Confusion:
[[294 135   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-03 10:22:17,474 - ==> Best [Top1: 79.676   Sparsity:0.00   Params: 155168 on epoch: 37]
2023-01-03 10:22:17,475 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:17,490 - 

2023-01-03 10:22:17,490 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:18,834 - Epoch: [38][   10/   37]    Overall Loss 0.473502    Objective Loss 0.473502                                        LR 0.000060    Time 0.134245    
2023-01-03 10:22:19,955 - Epoch: [38][   20/   37]    Overall Loss 0.466242    Objective Loss 0.466242                                        LR 0.000060    Time 0.123167    
2023-01-03 10:22:21,070 - Epoch: [38][   30/   37]    Overall Loss 0.461507    Objective Loss 0.461507                                        LR 0.000060    Time 0.119258    
2023-01-03 10:22:21,794 - Epoch: [38][   37/   37]    Overall Loss 0.459977    Objective Loss 0.459977    Top1 78.242678    LR 0.000060    Time 0.116235    
2023-01-03 10:22:21,834 - --- validate (epoch=38)-----------
2023-01-03 10:22:21,835 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:22,256 - Epoch: [38][    5/    5]    Loss 0.452645    Top1 79.198473    
2023-01-03 10:22:22,291 - ==> Top1: 79.198    Loss: 0.453

2023-01-03 10:22:22,292 - ==> Confusion:
[[292 137   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-03 10:22:22,294 - ==> Best [Top1: 79.676   Sparsity:0.00   Params: 155168 on epoch: 37]
2023-01-03 10:22:22,295 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:22,302 - 

2023-01-03 10:22:22,302 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:23,790 - Epoch: [39][   10/   37]    Overall Loss 0.452955    Objective Loss 0.452955                                        LR 0.000060    Time 0.148710    
2023-01-03 10:22:24,937 - Epoch: [39][   20/   37]    Overall Loss 0.453047    Objective Loss 0.453047                                        LR 0.000060    Time 0.131638    
2023-01-03 10:22:26,063 - Epoch: [39][   30/   37]    Overall Loss 0.452616    Objective Loss 0.452616                                        LR 0.000060    Time 0.125287    
2023-01-03 10:22:26,762 - Epoch: [39][   37/   37]    Overall Loss 0.453517    Objective Loss 0.453517    Top1 75.732218    LR 0.000060    Time 0.120455    
2023-01-03 10:22:26,809 - --- validate (epoch=39)-----------
2023-01-03 10:22:26,810 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:27,261 - Epoch: [39][    5/    5]    Loss 0.467222    Top1 79.961832    
2023-01-03 10:22:27,293 - ==> Top1: 79.962    Loss: 0.467

2023-01-03 10:22:27,294 - ==> Confusion:
[[331  98   0]
 [112 507   0]
 [  0   0   0]]

2023-01-03 10:22:27,296 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-03 10:22:27,296 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:27,307 - 

2023-01-03 10:22:27,308 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:28,669 - Epoch: [40][   10/   37]    Overall Loss 0.451031    Objective Loss 0.451031                                        LR 0.000036    Time 0.135975    
2023-01-03 10:22:29,786 - Epoch: [40][   20/   37]    Overall Loss 0.454789    Objective Loss 0.454789                                        LR 0.000036    Time 0.123795    
2023-01-03 10:22:30,920 - Epoch: [40][   30/   37]    Overall Loss 0.455597    Objective Loss 0.455597                                        LR 0.000036    Time 0.120309    
2023-01-03 10:22:31,620 - Epoch: [40][   37/   37]    Overall Loss 0.449908    Objective Loss 0.449908    Top1 78.451883    LR 0.000036    Time 0.116472    
2023-01-03 10:22:31,664 - --- validate (epoch=40)-----------
2023-01-03 10:22:31,664 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:32,098 - Epoch: [40][    5/    5]    Loss 0.424209    Top1 79.580153    
2023-01-03 10:22:32,156 - ==> Top1: 79.580    Loss: 0.424

2023-01-03 10:22:32,156 - ==> Confusion:
[[276 153   0]
 [ 61 558   0]
 [  0   0   0]]

2023-01-03 10:22:32,159 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-03 10:22:32,159 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:32,167 - 

2023-01-03 10:22:32,167 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:33,528 - Epoch: [41][   10/   37]    Overall Loss 0.441894    Objective Loss 0.441894                                        LR 0.000036    Time 0.135863    
2023-01-03 10:22:34,644 - Epoch: [41][   20/   37]    Overall Loss 0.449722    Objective Loss 0.449722                                        LR 0.000036    Time 0.123736    
2023-01-03 10:22:35,742 - Epoch: [41][   30/   37]    Overall Loss 0.444740    Objective Loss 0.444740                                        LR 0.000036    Time 0.119059    
2023-01-03 10:22:36,449 - Epoch: [41][   37/   37]    Overall Loss 0.443760    Objective Loss 0.443760    Top1 78.451883    LR 0.000036    Time 0.115622    
2023-01-03 10:22:36,484 - --- validate (epoch=41)-----------
2023-01-03 10:22:36,484 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:36,933 - Epoch: [41][    5/    5]    Loss 0.411615    Top1 80.438931    
2023-01-03 10:22:36,975 - ==> Top1: 80.439    Loss: 0.412

2023-01-03 10:22:36,975 - ==> Confusion:
[[329 100   0]
 [105 514   0]
 [  0   0   0]]

2023-01-03 10:22:36,977 - ==> Best [Top1: 80.439   Sparsity:0.00   Params: 155168 on epoch: 41]
2023-01-03 10:22:36,978 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:36,987 - 

2023-01-03 10:22:36,988 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:38,360 - Epoch: [42][   10/   37]    Overall Loss 0.444402    Objective Loss 0.444402                                        LR 0.000036    Time 0.137120    
2023-01-03 10:22:39,456 - Epoch: [42][   20/   37]    Overall Loss 0.448926    Objective Loss 0.448926                                        LR 0.000036    Time 0.123310    
2023-01-03 10:22:40,560 - Epoch: [42][   30/   37]    Overall Loss 0.443250    Objective Loss 0.443250                                        LR 0.000036    Time 0.118969    
2023-01-03 10:22:41,262 - Epoch: [42][   37/   37]    Overall Loss 0.442524    Objective Loss 0.442524    Top1 83.054393    LR 0.000036    Time 0.115427    
2023-01-03 10:22:41,300 - --- validate (epoch=42)-----------
2023-01-03 10:22:41,300 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:41,733 - Epoch: [42][    5/    5]    Loss 0.432289    Top1 78.816794    
2023-01-03 10:22:41,769 - ==> Top1: 78.817    Loss: 0.432

2023-01-03 10:22:41,770 - ==> Confusion:
[[366  63   0]
 [159 460   0]
 [  0   0   0]]

2023-01-03 10:22:41,772 - ==> Best [Top1: 80.439   Sparsity:0.00   Params: 155168 on epoch: 41]
2023-01-03 10:22:41,772 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:41,780 - 

2023-01-03 10:22:41,780 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:43,163 - Epoch: [43][   10/   37]    Overall Loss 0.450309    Objective Loss 0.450309                                        LR 0.000036    Time 0.138139    
2023-01-03 10:22:44,265 - Epoch: [43][   20/   37]    Overall Loss 0.444878    Objective Loss 0.444878                                        LR 0.000036    Time 0.124155    
2023-01-03 10:22:45,368 - Epoch: [43][   30/   37]    Overall Loss 0.447071    Objective Loss 0.447071                                        LR 0.000036    Time 0.119502    
2023-01-03 10:22:46,077 - Epoch: [43][   37/   37]    Overall Loss 0.445425    Objective Loss 0.445425    Top1 78.870293    LR 0.000036    Time 0.116041    
2023-01-03 10:22:46,120 - --- validate (epoch=43)-----------
2023-01-03 10:22:46,120 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:46,544 - Epoch: [43][    5/    5]    Loss 0.421930    Top1 79.103053    
2023-01-03 10:22:46,581 - ==> Top1: 79.103    Loss: 0.422

2023-01-03 10:22:46,581 - ==> Confusion:
[[360  69   0]
 [150 469   0]
 [  0   0   0]]

2023-01-03 10:22:46,583 - ==> Best [Top1: 80.439   Sparsity:0.00   Params: 155168 on epoch: 41]
2023-01-03 10:22:46,583 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:46,592 - 

2023-01-03 10:22:46,592 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:47,970 - Epoch: [44][   10/   37]    Overall Loss 0.438195    Objective Loss 0.438195                                        LR 0.000036    Time 0.137642    
2023-01-03 10:22:49,078 - Epoch: [44][   20/   37]    Overall Loss 0.441412    Objective Loss 0.441412                                        LR 0.000036    Time 0.124157    
2023-01-03 10:22:50,172 - Epoch: [44][   30/   37]    Overall Loss 0.435067    Objective Loss 0.435067                                        LR 0.000036    Time 0.119242    
2023-01-03 10:22:50,878 - Epoch: [44][   37/   37]    Overall Loss 0.439543    Objective Loss 0.439543    Top1 77.405858    LR 0.000036    Time 0.115739    
2023-01-03 10:22:50,922 - --- validate (epoch=44)-----------
2023-01-03 10:22:50,923 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:51,357 - Epoch: [44][    5/    5]    Loss 0.411642    Top1 80.248092    
2023-01-03 10:22:51,396 - ==> Top1: 80.248    Loss: 0.412

2023-01-03 10:22:51,396 - ==> Confusion:
[[355  74   0]
 [133 486   0]
 [  0   0   0]]

2023-01-03 10:22:51,398 - ==> Best [Top1: 80.439   Sparsity:0.00   Params: 155168 on epoch: 41]
2023-01-03 10:22:51,399 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:51,407 - 

2023-01-03 10:22:51,407 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:52,776 - Epoch: [45][   10/   37]    Overall Loss 0.450776    Objective Loss 0.450776                                        LR 0.000036    Time 0.136754    
2023-01-03 10:22:53,906 - Epoch: [45][   20/   37]    Overall Loss 0.441798    Objective Loss 0.441798                                        LR 0.000036    Time 0.124825    
2023-01-03 10:22:55,030 - Epoch: [45][   30/   37]    Overall Loss 0.439212    Objective Loss 0.439212                                        LR 0.000036    Time 0.120659    
2023-01-03 10:22:55,743 - Epoch: [45][   37/   37]    Overall Loss 0.438434    Objective Loss 0.438434    Top1 77.824268    LR 0.000036    Time 0.117115    
2023-01-03 10:22:55,781 - --- validate (epoch=45)-----------
2023-01-03 10:22:55,781 - 1048 samples (256 per mini-batch)
2023-01-03 10:22:56,203 - Epoch: [45][    5/    5]    Loss 0.439117    Top1 79.961832    
2023-01-03 10:22:56,239 - ==> Top1: 79.962    Loss: 0.439

2023-01-03 10:22:56,240 - ==> Confusion:
[[276 153   0]
 [ 57 562   0]
 [  0   0   0]]

2023-01-03 10:22:56,242 - ==> Best [Top1: 80.439   Sparsity:0.00   Params: 155168 on epoch: 41]
2023-01-03 10:22:56,242 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:22:56,250 - 

2023-01-03 10:22:56,250 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:22:57,626 - Epoch: [46][   10/   37]    Overall Loss 0.429190    Objective Loss 0.429190                                        LR 0.000036    Time 0.137398    
2023-01-03 10:22:58,757 - Epoch: [46][   20/   37]    Overall Loss 0.432576    Objective Loss 0.432576                                        LR 0.000036    Time 0.125225    
2023-01-03 10:22:59,875 - Epoch: [46][   30/   37]    Overall Loss 0.434562    Objective Loss 0.434562                                        LR 0.000036    Time 0.120731    
2023-01-03 10:23:00,597 - Epoch: [46][   37/   37]    Overall Loss 0.432628    Objective Loss 0.432628    Top1 75.941423    LR 0.000036    Time 0.117404    
2023-01-03 10:23:00,640 - --- validate (epoch=46)-----------
2023-01-03 10:23:00,640 - 1048 samples (256 per mini-batch)
2023-01-03 10:23:01,088 - Epoch: [46][    5/    5]    Loss 0.431966    Top1 80.725191    
2023-01-03 10:23:01,120 - ==> Top1: 80.725    Loss: 0.432

2023-01-03 10:23:01,120 - ==> Confusion:
[[342  87   0]
 [115 504   0]
 [  0   0   0]]

2023-01-03 10:23:01,122 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 155168 on epoch: 46]
2023-01-03 10:23:01,122 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:23:01,137 - 

2023-01-03 10:23:01,137 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:23:02,515 - Epoch: [47][   10/   37]    Overall Loss 0.441947    Objective Loss 0.441947                                        LR 0.000036    Time 0.137718    
2023-01-03 10:23:03,632 - Epoch: [47][   20/   37]    Overall Loss 0.437286    Objective Loss 0.437286                                        LR 0.000036    Time 0.124654    
2023-01-03 10:23:04,742 - Epoch: [47][   30/   37]    Overall Loss 0.435893    Objective Loss 0.435893                                        LR 0.000036    Time 0.120077    
2023-01-03 10:23:05,440 - Epoch: [47][   37/   37]    Overall Loss 0.430921    Objective Loss 0.430921    Top1 80.543933    LR 0.000036    Time 0.116229    
2023-01-03 10:23:05,482 - --- validate (epoch=47)-----------
2023-01-03 10:23:05,482 - 1048 samples (256 per mini-batch)
2023-01-03 10:23:05,908 - Epoch: [47][    5/    5]    Loss 0.395562    Top1 79.961832    
2023-01-03 10:23:05,944 - ==> Top1: 79.962    Loss: 0.396

2023-01-03 10:23:05,944 - ==> Confusion:
[[346  83   0]
 [127 492   0]
 [  0   0   0]]

2023-01-03 10:23:05,948 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 155168 on epoch: 46]
2023-01-03 10:23:05,948 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:23:05,956 - 

2023-01-03 10:23:05,956 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:23:07,350 - Epoch: [48][   10/   37]    Overall Loss 0.424430    Objective Loss 0.424430                                        LR 0.000036    Time 0.139248    
2023-01-03 10:23:08,485 - Epoch: [48][   20/   37]    Overall Loss 0.426061    Objective Loss 0.426061                                        LR 0.000036    Time 0.126361    
2023-01-03 10:23:09,613 - Epoch: [48][   30/   37]    Overall Loss 0.429410    Objective Loss 0.429410                                        LR 0.000036    Time 0.121801    
2023-01-03 10:23:10,325 - Epoch: [48][   37/   37]    Overall Loss 0.429177    Objective Loss 0.429177    Top1 78.870293    LR 0.000036    Time 0.117998    
2023-01-03 10:23:10,364 - --- validate (epoch=48)-----------
2023-01-03 10:23:10,364 - 1048 samples (256 per mini-batch)
2023-01-03 10:23:10,795 - Epoch: [48][    5/    5]    Loss 0.390254    Top1 81.774809    
2023-01-03 10:23:10,833 - ==> Top1: 81.775    Loss: 0.390

2023-01-03 10:23:10,834 - ==> Confusion:
[[306 123   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-03 10:23:10,836 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 155168 on epoch: 48]
2023-01-03 10:23:10,837 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:23:10,852 - 

2023-01-03 10:23:10,852 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:23:12,235 - Epoch: [49][   10/   37]    Overall Loss 0.438844    Objective Loss 0.438844                                        LR 0.000036    Time 0.138126    
2023-01-03 10:23:13,401 - Epoch: [49][   20/   37]    Overall Loss 0.423350    Objective Loss 0.423350                                        LR 0.000036    Time 0.127351    
2023-01-03 10:23:14,538 - Epoch: [49][   30/   37]    Overall Loss 0.425271    Objective Loss 0.425271                                        LR 0.000036    Time 0.122762    
2023-01-03 10:23:15,242 - Epoch: [49][   37/   37]    Overall Loss 0.427258    Objective Loss 0.427258    Top1 82.426778    LR 0.000036    Time 0.118567    
2023-01-03 10:23:15,285 - --- validate (epoch=49)-----------
2023-01-03 10:23:15,286 - 1048 samples (256 per mini-batch)
2023-01-03 10:23:15,714 - Epoch: [49][    5/    5]    Loss 0.407408    Top1 81.202290    
2023-01-03 10:23:15,750 - ==> Top1: 81.202    Loss: 0.407

2023-01-03 10:23:15,750 - ==> Confusion:
[[347  82   0]
 [115 504   0]
 [  0   0   0]]

2023-01-03 10:23:15,753 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 155168 on epoch: 48]
2023-01-03 10:23:15,753 - Saving checkpoint to: logs/2023.01.03-101914/qat_checkpoint.pth.tar
2023-01-03 10:23:15,761 - --- test ---------------------
2023-01-03 10:23:15,761 - 1317 samples (256 per mini-batch)
2023-01-03 10:23:16,271 - Test: [    6/    6]    Loss 0.450957    Top1 79.195140    
2023-01-03 10:23:16,309 - ==> Top1: 79.195    Loss: 0.451

2023-01-03 10:23:16,310 - ==> Confusion:
[[457 104   0]
 [170 586   0]
 [  0   0   0]]

2023-01-03 10:23:16,331 - 
2023-01-03 10:23:16,331 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-101914/2023.01.03-101914.log
