2023-01-03 09:45:58,033 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-094558/2023.01.03-094558.log
2023-01-03 09:45:58,043 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-03 09:45:58,044 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-03 09:46:07,001 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-03 09:46:07,002 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-03 09:46:07,006 - 

2023-01-03 09:46:07,006 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:08,538 - Epoch: [0][   10/   37]    Overall Loss 1.098140    Objective Loss 1.098140                                        LR 0.000060    Time 0.153048    
2023-01-03 09:46:09,844 - Epoch: [0][   20/   37]    Overall Loss 1.096960    Objective Loss 1.096960                                        LR 0.000060    Time 0.141825    
2023-01-03 09:46:11,151 - Epoch: [0][   30/   37]    Overall Loss 1.093258    Objective Loss 1.093258                                        LR 0.000060    Time 0.138095    
2023-01-03 09:46:11,969 - Epoch: [0][   37/   37]    Overall Loss 1.086493    Objective Loss 1.086493    Top1 56.276151    LR 0.000060    Time 0.134067    
2023-01-03 09:46:12,000 - --- validate (epoch=0)-----------
2023-01-03 09:46:12,001 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:12,407 - Epoch: [0][    5/    5]    Loss 1.028270    Top1 59.064885    
2023-01-03 09:46:12,438 - ==> Top1: 59.065    Loss: 1.028

2023-01-03 09:46:12,438 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:12,449 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 0]
2023-01-03 09:46:12,449 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:12,465 - 

2023-01-03 09:46:12,465 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:14,058 - Epoch: [1][   10/   37]    Overall Loss 0.968588    Objective Loss 0.968588                                        LR 0.000060    Time 0.159080    
2023-01-03 09:46:15,386 - Epoch: [1][   20/   37]    Overall Loss 0.883137    Objective Loss 0.883137                                        LR 0.000060    Time 0.145909    
2023-01-03 09:46:16,697 - Epoch: [1][   30/   37]    Overall Loss 0.838541    Objective Loss 0.838541                                        LR 0.000060    Time 0.140953    
2023-01-03 09:46:17,587 - Epoch: [1][   37/   37]    Overall Loss 0.818628    Objective Loss 0.818628    Top1 56.903766    LR 0.000060    Time 0.138327    
2023-01-03 09:46:17,636 - --- validate (epoch=1)-----------
2023-01-03 09:46:17,636 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:18,177 - Epoch: [1][    5/    5]    Loss 0.719077    Top1 59.064885    
2023-01-03 09:46:18,221 - ==> Top1: 59.065    Loss: 0.719

2023-01-03 09:46:18,222 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:18,226 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 1]
2023-01-03 09:46:18,227 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:18,253 - 

2023-01-03 09:46:18,253 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:19,858 - Epoch: [2][   10/   37]    Overall Loss 0.727495    Objective Loss 0.727495                                        LR 0.000060    Time 0.160405    
2023-01-03 09:46:21,299 - Epoch: [2][   20/   37]    Overall Loss 0.724079    Objective Loss 0.724079                                        LR 0.000060    Time 0.152208    
2023-01-03 09:46:22,606 - Epoch: [2][   30/   37]    Overall Loss 0.720393    Objective Loss 0.720393                                        LR 0.000060    Time 0.145006    
2023-01-03 09:46:23,441 - Epoch: [2][   37/   37]    Overall Loss 0.718892    Objective Loss 0.718892    Top1 58.368201    LR 0.000060    Time 0.140136    
2023-01-03 09:46:23,476 - --- validate (epoch=2)-----------
2023-01-03 09:46:23,476 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:23,852 - Epoch: [2][    5/    5]    Loss 0.714124    Top1 59.064885    
2023-01-03 09:46:23,888 - ==> Top1: 59.065    Loss: 0.714

2023-01-03 09:46:23,888 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:23,891 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 2]
2023-01-03 09:46:23,891 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:23,920 - 

2023-01-03 09:46:23,921 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:25,583 - Epoch: [3][   10/   37]    Overall Loss 0.714149    Objective Loss 0.714149                                        LR 0.000060    Time 0.166059    
2023-01-03 09:46:26,946 - Epoch: [3][   20/   37]    Overall Loss 0.711609    Objective Loss 0.711609                                        LR 0.000060    Time 0.151172    
2023-01-03 09:46:28,307 - Epoch: [3][   30/   37]    Overall Loss 0.711795    Objective Loss 0.711795                                        LR 0.000060    Time 0.146107    
2023-01-03 09:46:29,132 - Epoch: [3][   37/   37]    Overall Loss 0.709611    Objective Loss 0.709611    Top1 58.786611    LR 0.000060    Time 0.140750    
2023-01-03 09:46:29,170 - --- validate (epoch=3)-----------
2023-01-03 09:46:29,170 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:29,581 - Epoch: [3][    5/    5]    Loss 0.695658    Top1 59.064885    
2023-01-03 09:46:29,612 - ==> Top1: 59.065    Loss: 0.696

2023-01-03 09:46:29,613 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:29,616 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 3]
2023-01-03 09:46:29,616 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:29,643 - 

2023-01-03 09:46:29,643 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:31,287 - Epoch: [4][   10/   37]    Overall Loss 0.708342    Objective Loss 0.708342                                        LR 0.000060    Time 0.164253    
2023-01-03 09:46:32,726 - Epoch: [4][   20/   37]    Overall Loss 0.706878    Objective Loss 0.706878                                        LR 0.000060    Time 0.154058    
2023-01-03 09:46:34,106 - Epoch: [4][   30/   37]    Overall Loss 0.705171    Objective Loss 0.705171                                        LR 0.000060    Time 0.148678    
2023-01-03 09:46:34,935 - Epoch: [4][   37/   37]    Overall Loss 0.704612    Objective Loss 0.704612    Top1 55.439331    LR 0.000060    Time 0.142947    
2023-01-03 09:46:34,975 - --- validate (epoch=4)-----------
2023-01-03 09:46:34,976 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:35,366 - Epoch: [4][    5/    5]    Loss 0.687107    Top1 59.064885    
2023-01-03 09:46:35,402 - ==> Top1: 59.065    Loss: 0.687

2023-01-03 09:46:35,403 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:35,405 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 4]
2023-01-03 09:46:35,405 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:35,435 - 

2023-01-03 09:46:35,435 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:37,028 - Epoch: [5][   10/   37]    Overall Loss 0.707790    Objective Loss 0.707790                                        LR 0.000060    Time 0.159148    
2023-01-03 09:46:38,355 - Epoch: [5][   20/   37]    Overall Loss 0.705373    Objective Loss 0.705373                                        LR 0.000060    Time 0.145894    
2023-01-03 09:46:39,664 - Epoch: [5][   30/   37]    Overall Loss 0.702936    Objective Loss 0.702936                                        LR 0.000060    Time 0.140880    
2023-01-03 09:46:40,487 - Epoch: [5][   37/   37]    Overall Loss 0.701234    Objective Loss 0.701234    Top1 58.158996    LR 0.000060    Time 0.136455    
2023-01-03 09:46:40,523 - --- validate (epoch=5)-----------
2023-01-03 09:46:40,524 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:40,915 - Epoch: [5][    5/    5]    Loss 0.683130    Top1 59.064885    
2023-01-03 09:46:40,952 - ==> Top1: 59.065    Loss: 0.683

2023-01-03 09:46:40,952 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:40,955 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 5]
2023-01-03 09:46:40,955 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:40,985 - 

2023-01-03 09:46:40,985 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:42,556 - Epoch: [6][   10/   37]    Overall Loss 0.700313    Objective Loss 0.700313                                        LR 0.000060    Time 0.156975    
2023-01-03 09:46:43,880 - Epoch: [6][   20/   37]    Overall Loss 0.697443    Objective Loss 0.697443                                        LR 0.000060    Time 0.144621    
2023-01-03 09:46:45,181 - Epoch: [6][   30/   37]    Overall Loss 0.698259    Objective Loss 0.698259                                        LR 0.000060    Time 0.139789    
2023-01-03 09:46:46,009 - Epoch: [6][   37/   37]    Overall Loss 0.698860    Objective Loss 0.698860    Top1 52.719665    LR 0.000060    Time 0.135704    
2023-01-03 09:46:46,053 - --- validate (epoch=6)-----------
2023-01-03 09:46:46,053 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:46,421 - Epoch: [6][    5/    5]    Loss 0.692095    Top1 59.064885    
2023-01-03 09:46:46,457 - ==> Top1: 59.065    Loss: 0.692

2023-01-03 09:46:46,457 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:46,460 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 6]
2023-01-03 09:46:46,460 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:46,478 - 

2023-01-03 09:46:46,479 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:48,045 - Epoch: [7][   10/   37]    Overall Loss 0.700078    Objective Loss 0.700078                                        LR 0.000060    Time 0.156420    
2023-01-03 09:46:49,372 - Epoch: [7][   20/   37]    Overall Loss 0.698644    Objective Loss 0.698644                                        LR 0.000060    Time 0.144522    
2023-01-03 09:46:50,676 - Epoch: [7][   30/   37]    Overall Loss 0.697837    Objective Loss 0.697837                                        LR 0.000060    Time 0.139815    
2023-01-03 09:46:51,508 - Epoch: [7][   37/   37]    Overall Loss 0.696697    Objective Loss 0.696697    Top1 59.623431    LR 0.000060    Time 0.135834    
2023-01-03 09:46:51,549 - --- validate (epoch=7)-----------
2023-01-03 09:46:51,549 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:51,929 - Epoch: [7][    5/    5]    Loss 0.687703    Top1 59.064885    
2023-01-03 09:46:51,960 - ==> Top1: 59.065    Loss: 0.688

2023-01-03 09:46:51,960 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:51,962 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 7]
2023-01-03 09:46:51,962 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:51,984 - 

2023-01-03 09:46:51,985 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:53,684 - Epoch: [8][   10/   37]    Overall Loss 0.697317    Objective Loss 0.697317                                        LR 0.000060    Time 0.169783    
2023-01-03 09:46:55,014 - Epoch: [8][   20/   37]    Overall Loss 0.695616    Objective Loss 0.695616                                        LR 0.000060    Time 0.151358    
2023-01-03 09:46:56,336 - Epoch: [8][   30/   37]    Overall Loss 0.695381    Objective Loss 0.695381                                        LR 0.000060    Time 0.144969    
2023-01-03 09:46:57,187 - Epoch: [8][   37/   37]    Overall Loss 0.695026    Objective Loss 0.695026    Top1 56.066946    LR 0.000060    Time 0.140530    
2023-01-03 09:46:57,227 - --- validate (epoch=8)-----------
2023-01-03 09:46:57,228 - 1048 samples (256 per mini-batch)
2023-01-03 09:46:57,600 - Epoch: [8][    5/    5]    Loss 0.699706    Top1 59.064885    
2023-01-03 09:46:57,636 - ==> Top1: 59.065    Loss: 0.700

2023-01-03 09:46:57,636 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:46:57,639 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 8]
2023-01-03 09:46:57,640 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:46:57,658 - 

2023-01-03 09:46:57,658 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:46:59,236 - Epoch: [9][   10/   37]    Overall Loss 0.692725    Objective Loss 0.692725                                        LR 0.000060    Time 0.157620    
2023-01-03 09:47:00,573 - Epoch: [9][   20/   37]    Overall Loss 0.695249    Objective Loss 0.695249                                        LR 0.000060    Time 0.145622    
2023-01-03 09:47:01,881 - Epoch: [9][   30/   37]    Overall Loss 0.694771    Objective Loss 0.694771                                        LR 0.000060    Time 0.140661    
2023-01-03 09:47:02,729 - Epoch: [9][   37/   37]    Overall Loss 0.693918    Objective Loss 0.693918    Top1 55.230126    LR 0.000060    Time 0.136938    
2023-01-03 09:47:02,763 - --- validate (epoch=9)-----------
2023-01-03 09:47:02,764 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:03,155 - Epoch: [9][    5/    5]    Loss 0.694419    Top1 59.064885    
2023-01-03 09:47:03,190 - ==> Top1: 59.065    Loss: 0.694

2023-01-03 09:47:03,190 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:03,192 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 9]
2023-01-03 09:47:03,193 - Saving checkpoint to: logs/2023.01.03-094558/checkpoint.pth.tar
2023-01-03 09:47:03,232 - 

2023-01-03 09:47:03,232 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:05,256 - Epoch: [10][   10/   37]    Overall Loss 0.896426    Objective Loss 0.896426                                        LR 0.000060    Time 0.202231    
2023-01-03 09:47:07,033 - Epoch: [10][   20/   37]    Overall Loss 0.806821    Objective Loss 0.806821                                        LR 0.000060    Time 0.189964    
2023-01-03 09:47:08,822 - Epoch: [10][   30/   37]    Overall Loss 0.766941    Objective Loss 0.766941                                        LR 0.000060    Time 0.186255    
2023-01-03 09:47:09,984 - Epoch: [10][   37/   37]    Overall Loss 0.751018    Objective Loss 0.751018    Top1 60.251046    LR 0.000060    Time 0.182397    
2023-01-03 09:47:10,027 - --- validate (epoch=10)-----------
2023-01-03 09:47:10,027 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:10,617 - Epoch: [10][    5/    5]    Loss 0.680484    Top1 59.064885    
2023-01-03 09:47:10,654 - ==> Top1: 59.065    Loss: 0.680

2023-01-03 09:47:10,654 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:10,656 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 10]
2023-01-03 09:47:10,656 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:10,674 - 

2023-01-03 09:47:10,674 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:12,701 - Epoch: [11][   10/   37]    Overall Loss 0.686957    Objective Loss 0.686957                                        LR 0.000060    Time 0.202443    
2023-01-03 09:47:14,475 - Epoch: [11][   20/   37]    Overall Loss 0.683408    Objective Loss 0.683408                                        LR 0.000060    Time 0.189881    
2023-01-03 09:47:16,235 - Epoch: [11][   30/   37]    Overall Loss 0.684296    Objective Loss 0.684296                                        LR 0.000060    Time 0.185245    
2023-01-03 09:47:17,391 - Epoch: [11][   37/   37]    Overall Loss 0.683645    Objective Loss 0.683645    Top1 58.158996    LR 0.000060    Time 0.181427    
2023-01-03 09:47:17,429 - --- validate (epoch=11)-----------
2023-01-03 09:47:17,429 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:18,034 - Epoch: [11][    5/    5]    Loss 0.677848    Top1 59.064885    
2023-01-03 09:47:18,067 - ==> Top1: 59.065    Loss: 0.678

2023-01-03 09:47:18,067 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:18,069 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 11]
2023-01-03 09:47:18,069 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:18,091 - 

2023-01-03 09:47:18,092 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:20,166 - Epoch: [12][   10/   37]    Overall Loss 0.684729    Objective Loss 0.684729                                        LR 0.000060    Time 0.207229    
2023-01-03 09:47:21,931 - Epoch: [12][   20/   37]    Overall Loss 0.683602    Objective Loss 0.683602                                        LR 0.000060    Time 0.191857    
2023-01-03 09:47:23,700 - Epoch: [12][   30/   37]    Overall Loss 0.684197    Objective Loss 0.684197                                        LR 0.000060    Time 0.186861    
2023-01-03 09:47:24,853 - Epoch: [12][   37/   37]    Overall Loss 0.683887    Objective Loss 0.683887    Top1 56.903766    LR 0.000060    Time 0.182639    
2023-01-03 09:47:24,894 - --- validate (epoch=12)-----------
2023-01-03 09:47:24,894 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:25,483 - Epoch: [12][    5/    5]    Loss 0.682150    Top1 59.064885    
2023-01-03 09:47:25,515 - ==> Top1: 59.065    Loss: 0.682

2023-01-03 09:47:25,515 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:25,518 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 12]
2023-01-03 09:47:25,518 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:25,548 - 

2023-01-03 09:47:25,548 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:27,592 - Epoch: [13][   10/   37]    Overall Loss 0.684142    Objective Loss 0.684142                                        LR 0.000060    Time 0.204208    
2023-01-03 09:47:29,374 - Epoch: [13][   20/   37]    Overall Loss 0.682723    Objective Loss 0.682723                                        LR 0.000060    Time 0.191141    
2023-01-03 09:47:31,129 - Epoch: [13][   30/   37]    Overall Loss 0.682309    Objective Loss 0.682309                                        LR 0.000060    Time 0.185912    
2023-01-03 09:47:32,279 - Epoch: [13][   37/   37]    Overall Loss 0.683293    Objective Loss 0.683293    Top1 57.531381    LR 0.000060    Time 0.181816    
2023-01-03 09:47:32,320 - --- validate (epoch=13)-----------
2023-01-03 09:47:32,321 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:32,912 - Epoch: [13][    5/    5]    Loss 0.672113    Top1 59.064885    
2023-01-03 09:47:32,945 - ==> Top1: 59.065    Loss: 0.672

2023-01-03 09:47:32,946 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:32,948 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 13]
2023-01-03 09:47:32,949 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:32,968 - 

2023-01-03 09:47:32,968 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:35,013 - Epoch: [14][   10/   37]    Overall Loss 0.683168    Objective Loss 0.683168                                        LR 0.000060    Time 0.204369    
2023-01-03 09:47:36,793 - Epoch: [14][   20/   37]    Overall Loss 0.685416    Objective Loss 0.685416                                        LR 0.000060    Time 0.191140    
2023-01-03 09:47:38,570 - Epoch: [14][   30/   37]    Overall Loss 0.684323    Objective Loss 0.684323                                        LR 0.000060    Time 0.186637    
2023-01-03 09:47:39,746 - Epoch: [14][   37/   37]    Overall Loss 0.683392    Objective Loss 0.683392    Top1 58.786611    LR 0.000060    Time 0.183102    
2023-01-03 09:47:39,788 - --- validate (epoch=14)-----------
2023-01-03 09:47:39,789 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:40,384 - Epoch: [14][    5/    5]    Loss 0.671730    Top1 59.064885    
2023-01-03 09:47:40,424 - ==> Top1: 59.065    Loss: 0.672

2023-01-03 09:47:40,424 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:40,427 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 14]
2023-01-03 09:47:40,427 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:40,447 - 

2023-01-03 09:47:40,448 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:42,504 - Epoch: [15][   10/   37]    Overall Loss 0.677530    Objective Loss 0.677530                                        LR 0.000060    Time 0.205346    
2023-01-03 09:47:44,297 - Epoch: [15][   20/   37]    Overall Loss 0.683647    Objective Loss 0.683647                                        LR 0.000060    Time 0.192309    
2023-01-03 09:47:46,078 - Epoch: [15][   30/   37]    Overall Loss 0.684632    Objective Loss 0.684632                                        LR 0.000060    Time 0.187560    
2023-01-03 09:47:47,245 - Epoch: [15][   37/   37]    Overall Loss 0.683985    Objective Loss 0.683985    Top1 56.485356    LR 0.000060    Time 0.183595    
2023-01-03 09:47:47,287 - --- validate (epoch=15)-----------
2023-01-03 09:47:47,288 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:47,890 - Epoch: [15][    5/    5]    Loss 0.680120    Top1 59.064885    
2023-01-03 09:47:47,923 - ==> Top1: 59.065    Loss: 0.680

2023-01-03 09:47:47,924 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:47,926 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 15]
2023-01-03 09:47:47,926 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:47,954 - 

2023-01-03 09:47:47,954 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:50,018 - Epoch: [16][   10/   37]    Overall Loss 0.684386    Objective Loss 0.684386                                        LR 0.000060    Time 0.206266    
2023-01-03 09:47:51,808 - Epoch: [16][   20/   37]    Overall Loss 0.682680    Objective Loss 0.682680                                        LR 0.000060    Time 0.192555    
2023-01-03 09:47:53,593 - Epoch: [16][   30/   37]    Overall Loss 0.682634    Objective Loss 0.682634                                        LR 0.000060    Time 0.187876    
2023-01-03 09:47:54,778 - Epoch: [16][   37/   37]    Overall Loss 0.683341    Objective Loss 0.683341    Top1 54.393305    LR 0.000060    Time 0.184328    
2023-01-03 09:47:54,820 - --- validate (epoch=16)-----------
2023-01-03 09:47:54,820 - 1048 samples (256 per mini-batch)
2023-01-03 09:47:55,432 - Epoch: [16][    5/    5]    Loss 0.673762    Top1 59.064885    
2023-01-03 09:47:55,467 - ==> Top1: 59.065    Loss: 0.674

2023-01-03 09:47:55,467 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:47:55,470 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-03 09:47:55,470 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:47:55,489 - 

2023-01-03 09:47:55,489 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:47:57,546 - Epoch: [17][   10/   37]    Overall Loss 0.684339    Objective Loss 0.684339                                        LR 0.000060    Time 0.205484    
2023-01-03 09:47:59,340 - Epoch: [17][   20/   37]    Overall Loss 0.682805    Objective Loss 0.682805                                        LR 0.000060    Time 0.192369    
2023-01-03 09:48:01,131 - Epoch: [17][   30/   37]    Overall Loss 0.683931    Objective Loss 0.683931                                        LR 0.000060    Time 0.187944    
2023-01-03 09:48:02,307 - Epoch: [17][   37/   37]    Overall Loss 0.683395    Objective Loss 0.683395    Top1 57.740586    LR 0.000060    Time 0.184144    
2023-01-03 09:48:02,348 - --- validate (epoch=17)-----------
2023-01-03 09:48:02,349 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:02,954 - Epoch: [17][    5/    5]    Loss 0.675833    Top1 59.064885    
2023-01-03 09:48:02,992 - ==> Top1: 59.065    Loss: 0.676

2023-01-03 09:48:02,993 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:02,995 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 17]
2023-01-03 09:48:02,996 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:03,015 - 

2023-01-03 09:48:03,015 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:05,150 - Epoch: [18][   10/   37]    Overall Loss 0.680175    Objective Loss 0.680175                                        LR 0.000060    Time 0.213347    
2023-01-03 09:48:06,991 - Epoch: [18][   20/   37]    Overall Loss 0.683646    Objective Loss 0.683646                                        LR 0.000060    Time 0.198698    
2023-01-03 09:48:08,822 - Epoch: [18][   30/   37]    Overall Loss 0.683355    Objective Loss 0.683355                                        LR 0.000060    Time 0.193473    
2023-01-03 09:48:09,998 - Epoch: [18][   37/   37]    Overall Loss 0.683924    Objective Loss 0.683924    Top1 57.322176    LR 0.000060    Time 0.188644    
2023-01-03 09:48:10,045 - --- validate (epoch=18)-----------
2023-01-03 09:48:10,045 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:10,678 - Epoch: [18][    5/    5]    Loss 0.666368    Top1 59.064885    
2023-01-03 09:48:10,714 - ==> Top1: 59.065    Loss: 0.666

2023-01-03 09:48:10,715 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:10,718 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 18]
2023-01-03 09:48:10,718 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:10,741 - 

2023-01-03 09:48:10,741 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:12,865 - Epoch: [19][   10/   37]    Overall Loss 0.684292    Objective Loss 0.684292                                        LR 0.000060    Time 0.212193    
2023-01-03 09:48:14,650 - Epoch: [19][   20/   37]    Overall Loss 0.682113    Objective Loss 0.682113                                        LR 0.000060    Time 0.195326    
2023-01-03 09:48:16,458 - Epoch: [19][   30/   37]    Overall Loss 0.683219    Objective Loss 0.683219                                        LR 0.000060    Time 0.190468    
2023-01-03 09:48:17,636 - Epoch: [19][   37/   37]    Overall Loss 0.683409    Objective Loss 0.683409    Top1 53.765690    LR 0.000060    Time 0.186264    
2023-01-03 09:48:17,680 - --- validate (epoch=19)-----------
2023-01-03 09:48:17,680 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:18,296 - Epoch: [19][    5/    5]    Loss 0.684637    Top1 59.064885    
2023-01-03 09:48:18,333 - ==> Top1: 59.065    Loss: 0.685

2023-01-03 09:48:18,333 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:18,336 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 19]
2023-01-03 09:48:18,336 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:18,366 - 

2023-01-03 09:48:18,367 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:20,433 - Epoch: [20][   10/   37]    Overall Loss 0.684800    Objective Loss 0.684800                                        LR 0.000060    Time 0.206516    
2023-01-03 09:48:22,255 - Epoch: [20][   20/   37]    Overall Loss 0.683262    Objective Loss 0.683262                                        LR 0.000060    Time 0.194346    
2023-01-03 09:48:24,040 - Epoch: [20][   30/   37]    Overall Loss 0.683087    Objective Loss 0.683087                                        LR 0.000060    Time 0.189048    
2023-01-03 09:48:25,216 - Epoch: [20][   37/   37]    Overall Loss 0.683138    Objective Loss 0.683138    Top1 58.368201    LR 0.000060    Time 0.185044    
2023-01-03 09:48:25,255 - --- validate (epoch=20)-----------
2023-01-03 09:48:25,256 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:25,856 - Epoch: [20][    5/    5]    Loss 0.676252    Top1 59.064885    
2023-01-03 09:48:25,891 - ==> Top1: 59.065    Loss: 0.676

2023-01-03 09:48:25,891 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:25,893 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 20]
2023-01-03 09:48:25,894 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:25,917 - 

2023-01-03 09:48:25,917 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:27,977 - Epoch: [21][   10/   37]    Overall Loss 0.681096    Objective Loss 0.681096                                        LR 0.000060    Time 0.205830    
2023-01-03 09:48:29,782 - Epoch: [21][   20/   37]    Overall Loss 0.684499    Objective Loss 0.684499                                        LR 0.000060    Time 0.193140    
2023-01-03 09:48:31,597 - Epoch: [21][   30/   37]    Overall Loss 0.682661    Objective Loss 0.682661                                        LR 0.000060    Time 0.189248    
2023-01-03 09:48:32,771 - Epoch: [21][   37/   37]    Overall Loss 0.682949    Objective Loss 0.682949    Top1 54.602510    LR 0.000060    Time 0.185154    
2023-01-03 09:48:32,813 - --- validate (epoch=21)-----------
2023-01-03 09:48:32,813 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:33,443 - Epoch: [21][    5/    5]    Loss 0.673501    Top1 59.064885    
2023-01-03 09:48:33,481 - ==> Top1: 59.065    Loss: 0.674

2023-01-03 09:48:33,481 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:33,483 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 21]
2023-01-03 09:48:33,483 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:33,506 - 

2023-01-03 09:48:33,506 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:35,537 - Epoch: [22][   10/   37]    Overall Loss 0.683740    Objective Loss 0.683740                                        LR 0.000060    Time 0.202937    
2023-01-03 09:48:37,328 - Epoch: [22][   20/   37]    Overall Loss 0.682511    Objective Loss 0.682511                                        LR 0.000060    Time 0.190994    
2023-01-03 09:48:39,105 - Epoch: [22][   30/   37]    Overall Loss 0.681775    Objective Loss 0.681775                                        LR 0.000060    Time 0.186539    
2023-01-03 09:48:40,272 - Epoch: [22][   37/   37]    Overall Loss 0.681465    Objective Loss 0.681465    Top1 54.602510    LR 0.000060    Time 0.182795    
2023-01-03 09:48:40,315 - --- validate (epoch=22)-----------
2023-01-03 09:48:40,315 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:40,905 - Epoch: [22][    5/    5]    Loss 0.677049    Top1 59.064885    
2023-01-03 09:48:40,939 - ==> Top1: 59.065    Loss: 0.677

2023-01-03 09:48:40,940 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 09:48:40,942 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 22]
2023-01-03 09:48:40,942 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:40,970 - 

2023-01-03 09:48:40,971 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:43,141 - Epoch: [23][   10/   37]    Overall Loss 0.676880    Objective Loss 0.676880                                        LR 0.000060    Time 0.216931    
2023-01-03 09:48:44,916 - Epoch: [23][   20/   37]    Overall Loss 0.674435    Objective Loss 0.674435                                        LR 0.000060    Time 0.197160    
2023-01-03 09:48:46,686 - Epoch: [23][   30/   37]    Overall Loss 0.672172    Objective Loss 0.672172                                        LR 0.000060    Time 0.190416    
2023-01-03 09:48:47,859 - Epoch: [23][   37/   37]    Overall Loss 0.668215    Objective Loss 0.668215    Top1 61.506276    LR 0.000060    Time 0.186090    
2023-01-03 09:48:47,906 - --- validate (epoch=23)-----------
2023-01-03 09:48:47,907 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:48,508 - Epoch: [23][    5/    5]    Loss 0.640316    Top1 60.496183    
2023-01-03 09:48:48,543 - ==> Top1: 60.496    Loss: 0.640

2023-01-03 09:48:48,543 - ==> Confusion:
[[ 25 404   0]
 [ 10 609   0]
 [  0   0   0]]

2023-01-03 09:48:48,546 - ==> Best [Top1: 60.496   Sparsity:0.00   Params: 360896 on epoch: 23]
2023-01-03 09:48:48,546 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:48,574 - 

2023-01-03 09:48:48,574 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:50,625 - Epoch: [24][   10/   37]    Overall Loss 0.639892    Objective Loss 0.639892                                        LR 0.000060    Time 0.204911    
2023-01-03 09:48:52,426 - Epoch: [24][   20/   37]    Overall Loss 0.636577    Objective Loss 0.636577                                        LR 0.000060    Time 0.192467    
2023-01-03 09:48:54,190 - Epoch: [24][   30/   37]    Overall Loss 0.630564    Objective Loss 0.630564                                        LR 0.000060    Time 0.187094    
2023-01-03 09:48:55,350 - Epoch: [24][   37/   37]    Overall Loss 0.626131    Objective Loss 0.626131    Top1 65.690377    LR 0.000060    Time 0.183050    
2023-01-03 09:48:55,392 - --- validate (epoch=24)-----------
2023-01-03 09:48:55,392 - 1048 samples (256 per mini-batch)
2023-01-03 09:48:55,989 - Epoch: [24][    5/    5]    Loss 0.582986    Top1 67.843511    
2023-01-03 09:48:56,028 - ==> Top1: 67.844    Loss: 0.583

2023-01-03 09:48:56,028 - ==> Confusion:
[[213 216   0]
 [121 498   0]
 [  0   0   0]]

2023-01-03 09:48:56,031 - ==> Best [Top1: 67.844   Sparsity:0.00   Params: 360896 on epoch: 24]
2023-01-03 09:48:56,031 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:48:56,060 - 

2023-01-03 09:48:56,061 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:48:58,111 - Epoch: [25][   10/   37]    Overall Loss 0.608486    Objective Loss 0.608486                                        LR 0.000060    Time 0.204957    
2023-01-03 09:48:59,872 - Epoch: [25][   20/   37]    Overall Loss 0.599400    Objective Loss 0.599400                                        LR 0.000060    Time 0.190486    
2023-01-03 09:49:01,638 - Epoch: [25][   30/   37]    Overall Loss 0.590619    Objective Loss 0.590619                                        LR 0.000060    Time 0.185840    
2023-01-03 09:49:02,801 - Epoch: [25][   37/   37]    Overall Loss 0.590330    Objective Loss 0.590330    Top1 67.991632    LR 0.000060    Time 0.182085    
2023-01-03 09:49:02,848 - --- validate (epoch=25)-----------
2023-01-03 09:49:02,848 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:03,476 - Epoch: [25][    5/    5]    Loss 0.570840    Top1 70.610687    
2023-01-03 09:49:03,513 - ==> Top1: 70.611    Loss: 0.571

2023-01-03 09:49:03,513 - ==> Confusion:
[[287 142   0]
 [166 453   0]
 [  0   0   0]]

2023-01-03 09:49:03,516 - ==> Best [Top1: 70.611   Sparsity:0.00   Params: 360896 on epoch: 25]
2023-01-03 09:49:03,516 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:03,533 - 

2023-01-03 09:49:03,533 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:05,582 - Epoch: [26][   10/   37]    Overall Loss 0.582032    Objective Loss 0.582032                                        LR 0.000060    Time 0.204723    
2023-01-03 09:49:07,368 - Epoch: [26][   20/   37]    Overall Loss 0.577423    Objective Loss 0.577423                                        LR 0.000060    Time 0.191638    
2023-01-03 09:49:09,139 - Epoch: [26][   30/   37]    Overall Loss 0.575456    Objective Loss 0.575456                                        LR 0.000060    Time 0.186770    
2023-01-03 09:49:10,299 - Epoch: [26][   37/   37]    Overall Loss 0.578123    Objective Loss 0.578123    Top1 69.665272    LR 0.000060    Time 0.182761    
2023-01-03 09:49:10,338 - --- validate (epoch=26)-----------
2023-01-03 09:49:10,338 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:10,929 - Epoch: [26][    5/    5]    Loss 0.579856    Top1 69.847328    
2023-01-03 09:49:10,963 - ==> Top1: 69.847    Loss: 0.580

2023-01-03 09:49:10,963 - ==> Confusion:
[[215 214   0]
 [102 517   0]
 [  0   0   0]]

2023-01-03 09:49:10,967 - ==> Best [Top1: 70.611   Sparsity:0.00   Params: 360896 on epoch: 25]
2023-01-03 09:49:10,967 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:10,987 - 

2023-01-03 09:49:10,988 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:13,036 - Epoch: [27][   10/   37]    Overall Loss 0.560400    Objective Loss 0.560400                                        LR 0.000060    Time 0.204677    
2023-01-03 09:49:14,818 - Epoch: [27][   20/   37]    Overall Loss 0.565002    Objective Loss 0.565002                                        LR 0.000060    Time 0.191385    
2023-01-03 09:49:16,601 - Epoch: [27][   30/   37]    Overall Loss 0.566269    Objective Loss 0.566269                                        LR 0.000060    Time 0.186741    
2023-01-03 09:49:17,774 - Epoch: [27][   37/   37]    Overall Loss 0.564529    Objective Loss 0.564529    Top1 70.711297    LR 0.000060    Time 0.183112    
2023-01-03 09:49:17,814 - --- validate (epoch=27)-----------
2023-01-03 09:49:17,815 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:18,414 - Epoch: [27][    5/    5]    Loss 0.547214    Top1 72.328244    
2023-01-03 09:49:18,460 - ==> Top1: 72.328    Loss: 0.547

2023-01-03 09:49:18,461 - ==> Confusion:
[[296 133   0]
 [157 462   0]
 [  0   0   0]]

2023-01-03 09:49:18,463 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-03 09:49:18,463 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:18,485 - 

2023-01-03 09:49:18,485 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:20,552 - Epoch: [28][   10/   37]    Overall Loss 0.552173    Objective Loss 0.552173                                        LR 0.000060    Time 0.206549    
2023-01-03 09:49:22,321 - Epoch: [28][   20/   37]    Overall Loss 0.557438    Objective Loss 0.557438                                        LR 0.000060    Time 0.191677    
2023-01-03 09:49:24,101 - Epoch: [28][   30/   37]    Overall Loss 0.554597    Objective Loss 0.554597                                        LR 0.000060    Time 0.187117    
2023-01-03 09:49:25,262 - Epoch: [28][   37/   37]    Overall Loss 0.556183    Objective Loss 0.556183    Top1 67.991632    LR 0.000060    Time 0.183061    
2023-01-03 09:49:25,304 - --- validate (epoch=28)-----------
2023-01-03 09:49:25,305 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:25,895 - Epoch: [28][    5/    5]    Loss 0.533008    Top1 70.038168    
2023-01-03 09:49:25,932 - ==> Top1: 70.038    Loss: 0.533

2023-01-03 09:49:25,932 - ==> Confusion:
[[197 232   0]
 [ 82 537   0]
 [  0   0   0]]

2023-01-03 09:49:25,934 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-03 09:49:25,934 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:25,948 - 

2023-01-03 09:49:25,948 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:27,964 - Epoch: [29][   10/   37]    Overall Loss 0.555803    Objective Loss 0.555803                                        LR 0.000060    Time 0.201480    
2023-01-03 09:49:29,755 - Epoch: [29][   20/   37]    Overall Loss 0.555176    Objective Loss 0.555176                                        LR 0.000060    Time 0.190244    
2023-01-03 09:49:31,519 - Epoch: [29][   30/   37]    Overall Loss 0.554577    Objective Loss 0.554577                                        LR 0.000060    Time 0.185594    
2023-01-03 09:49:32,675 - Epoch: [29][   37/   37]    Overall Loss 0.551949    Objective Loss 0.551949    Top1 72.803347    LR 0.000060    Time 0.181724    
2023-01-03 09:49:32,716 - --- validate (epoch=29)-----------
2023-01-03 09:49:32,716 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:33,325 - Epoch: [29][    5/    5]    Loss 0.533952    Top1 71.087786    
2023-01-03 09:49:33,356 - ==> Top1: 71.088    Loss: 0.534

2023-01-03 09:49:33,357 - ==> Confusion:
[[350  79   0]
 [224 395   0]
 [  0   0   0]]

2023-01-03 09:49:33,360 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-03 09:49:33,360 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:33,380 - 

2023-01-03 09:49:33,381 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:35,453 - Epoch: [30][   10/   37]    Overall Loss 0.545362    Objective Loss 0.545362                                        LR 0.000060    Time 0.207091    
2023-01-03 09:49:37,217 - Epoch: [30][   20/   37]    Overall Loss 0.546090    Objective Loss 0.546090                                        LR 0.000060    Time 0.191684    
2023-01-03 09:49:38,966 - Epoch: [30][   30/   37]    Overall Loss 0.544420    Objective Loss 0.544420                                        LR 0.000060    Time 0.186084    
2023-01-03 09:49:40,147 - Epoch: [30][   37/   37]    Overall Loss 0.548753    Objective Loss 0.548753    Top1 68.828452    LR 0.000060    Time 0.182774    
2023-01-03 09:49:40,186 - --- validate (epoch=30)-----------
2023-01-03 09:49:40,187 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:40,777 - Epoch: [30][    5/    5]    Loss 0.521214    Top1 73.377863    
2023-01-03 09:49:40,813 - ==> Top1: 73.378    Loss: 0.521

2023-01-03 09:49:40,813 - ==> Confusion:
[[314 115   0]
 [164 455   0]
 [  0   0   0]]

2023-01-03 09:49:40,815 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 30]
2023-01-03 09:49:40,815 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:40,844 - 

2023-01-03 09:49:40,844 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:42,873 - Epoch: [31][   10/   37]    Overall Loss 0.541181    Objective Loss 0.541181                                        LR 0.000060    Time 0.202774    
2023-01-03 09:49:44,645 - Epoch: [31][   20/   37]    Overall Loss 0.539717    Objective Loss 0.539717                                        LR 0.000060    Time 0.189908    
2023-01-03 09:49:46,417 - Epoch: [31][   30/   37]    Overall Loss 0.548967    Objective Loss 0.548967                                        LR 0.000060    Time 0.185672    
2023-01-03 09:49:47,577 - Epoch: [31][   37/   37]    Overall Loss 0.545155    Objective Loss 0.545155    Top1 73.012552    LR 0.000060    Time 0.181866    
2023-01-03 09:49:47,618 - --- validate (epoch=31)-----------
2023-01-03 09:49:47,618 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:48,214 - Epoch: [31][    5/    5]    Loss 0.517242    Top1 72.614504    
2023-01-03 09:49:48,246 - ==> Top1: 72.615    Loss: 0.517

2023-01-03 09:49:48,247 - ==> Confusion:
[[234 195   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-03 09:49:48,249 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 30]
2023-01-03 09:49:48,250 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:48,273 - 

2023-01-03 09:49:48,273 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:50,310 - Epoch: [32][   10/   37]    Overall Loss 0.534603    Objective Loss 0.534603                                        LR 0.000060    Time 0.203506    
2023-01-03 09:49:52,085 - Epoch: [32][   20/   37]    Overall Loss 0.532923    Objective Loss 0.532923                                        LR 0.000060    Time 0.190474    
2023-01-03 09:49:53,848 - Epoch: [32][   30/   37]    Overall Loss 0.531661    Objective Loss 0.531661                                        LR 0.000060    Time 0.185739    
2023-01-03 09:49:55,009 - Epoch: [32][   37/   37]    Overall Loss 0.534479    Objective Loss 0.534479    Top1 72.594142    LR 0.000060    Time 0.181970    
2023-01-03 09:49:55,055 - --- validate (epoch=32)-----------
2023-01-03 09:49:55,056 - 1048 samples (256 per mini-batch)
2023-01-03 09:49:55,674 - Epoch: [32][    5/    5]    Loss 0.509270    Top1 73.377863    
2023-01-03 09:49:55,707 - ==> Top1: 73.378    Loss: 0.509

2023-01-03 09:49:55,707 - ==> Confusion:
[[243 186   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-03 09:49:55,710 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 32]
2023-01-03 09:49:55,711 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:49:55,732 - 

2023-01-03 09:49:55,732 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:49:57,775 - Epoch: [33][   10/   37]    Overall Loss 0.528447    Objective Loss 0.528447                                        LR 0.000060    Time 0.204134    
2023-01-03 09:49:59,535 - Epoch: [33][   20/   37]    Overall Loss 0.527353    Objective Loss 0.527353                                        LR 0.000060    Time 0.190068    
2023-01-03 09:50:01,318 - Epoch: [33][   30/   37]    Overall Loss 0.525250    Objective Loss 0.525250                                        LR 0.000060    Time 0.186120    
2023-01-03 09:50:02,486 - Epoch: [33][   37/   37]    Overall Loss 0.526336    Objective Loss 0.526336    Top1 71.757322    LR 0.000060    Time 0.182441    
2023-01-03 09:50:02,529 - --- validate (epoch=33)-----------
2023-01-03 09:50:02,529 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:03,139 - Epoch: [33][    5/    5]    Loss 0.491994    Top1 73.473282    
2023-01-03 09:50:03,177 - ==> Top1: 73.473    Loss: 0.492

2023-01-03 09:50:03,178 - ==> Confusion:
[[242 187   0]
 [ 91 528   0]
 [  0   0   0]]

2023-01-03 09:50:03,181 - ==> Best [Top1: 73.473   Sparsity:0.00   Params: 360896 on epoch: 33]
2023-01-03 09:50:03,181 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:03,204 - 

2023-01-03 09:50:03,204 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:05,234 - Epoch: [34][   10/   37]    Overall Loss 0.521053    Objective Loss 0.521053                                        LR 0.000060    Time 0.202810    
2023-01-03 09:50:07,018 - Epoch: [34][   20/   37]    Overall Loss 0.530690    Objective Loss 0.530690                                        LR 0.000060    Time 0.190589    
2023-01-03 09:50:08,775 - Epoch: [34][   30/   37]    Overall Loss 0.528179    Objective Loss 0.528179                                        LR 0.000060    Time 0.185612    
2023-01-03 09:50:09,937 - Epoch: [34][   37/   37]    Overall Loss 0.521929    Objective Loss 0.521929    Top1 77.824268    LR 0.000060    Time 0.181885    
2023-01-03 09:50:09,973 - --- validate (epoch=34)-----------
2023-01-03 09:50:09,973 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:10,584 - Epoch: [34][    5/    5]    Loss 0.490624    Top1 74.236641    
2023-01-03 09:50:10,628 - ==> Top1: 74.237    Loss: 0.491

2023-01-03 09:50:10,629 - ==> Confusion:
[[274 155   0]
 [115 504   0]
 [  0   0   0]]

2023-01-03 09:50:10,631 - ==> Best [Top1: 74.237   Sparsity:0.00   Params: 360896 on epoch: 34]
2023-01-03 09:50:10,631 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:10,652 - 

2023-01-03 09:50:10,652 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:12,702 - Epoch: [35][   10/   37]    Overall Loss 0.507534    Objective Loss 0.507534                                        LR 0.000060    Time 0.204879    
2023-01-03 09:50:14,487 - Epoch: [35][   20/   37]    Overall Loss 0.504536    Objective Loss 0.504536                                        LR 0.000060    Time 0.191639    
2023-01-03 09:50:16,259 - Epoch: [35][   30/   37]    Overall Loss 0.508094    Objective Loss 0.508094                                        LR 0.000060    Time 0.186815    
2023-01-03 09:50:17,437 - Epoch: [35][   37/   37]    Overall Loss 0.510527    Objective Loss 0.510527    Top1 70.920502    LR 0.000060    Time 0.183303    
2023-01-03 09:50:17,475 - --- validate (epoch=35)-----------
2023-01-03 09:50:17,476 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:18,070 - Epoch: [35][    5/    5]    Loss 0.494550    Top1 75.381679    
2023-01-03 09:50:18,106 - ==> Top1: 75.382    Loss: 0.495

2023-01-03 09:50:18,106 - ==> Confusion:
[[266 163   0]
 [ 95 524   0]
 [  0   0   0]]

2023-01-03 09:50:18,108 - ==> Best [Top1: 75.382   Sparsity:0.00   Params: 360896 on epoch: 35]
2023-01-03 09:50:18,108 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:18,138 - 

2023-01-03 09:50:18,138 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:20,156 - Epoch: [36][   10/   37]    Overall Loss 0.504608    Objective Loss 0.504608                                        LR 0.000060    Time 0.201625    
2023-01-03 09:50:21,947 - Epoch: [36][   20/   37]    Overall Loss 0.499761    Objective Loss 0.499761                                        LR 0.000060    Time 0.190297    
2023-01-03 09:50:23,721 - Epoch: [36][   30/   37]    Overall Loss 0.502986    Objective Loss 0.502986                                        LR 0.000060    Time 0.186003    
2023-01-03 09:50:24,876 - Epoch: [36][   37/   37]    Overall Loss 0.502245    Objective Loss 0.502245    Top1 74.895397    LR 0.000060    Time 0.182010    
2023-01-03 09:50:24,919 - --- validate (epoch=36)-----------
2023-01-03 09:50:24,920 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:25,525 - Epoch: [36][    5/    5]    Loss 0.477329    Top1 75.763359    
2023-01-03 09:50:25,558 - ==> Top1: 75.763    Loss: 0.477

2023-01-03 09:50:25,559 - ==> Confusion:
[[249 180   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-03 09:50:25,561 - ==> Best [Top1: 75.763   Sparsity:0.00   Params: 360896 on epoch: 36]
2023-01-03 09:50:25,562 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:25,589 - 

2023-01-03 09:50:25,589 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:27,643 - Epoch: [37][   10/   37]    Overall Loss 0.483305    Objective Loss 0.483305                                        LR 0.000060    Time 0.205281    
2023-01-03 09:50:29,426 - Epoch: [37][   20/   37]    Overall Loss 0.484079    Objective Loss 0.484079                                        LR 0.000060    Time 0.191732    
2023-01-03 09:50:31,182 - Epoch: [37][   30/   37]    Overall Loss 0.493611    Objective Loss 0.493611                                        LR 0.000060    Time 0.186351    
2023-01-03 09:50:32,371 - Epoch: [37][   37/   37]    Overall Loss 0.494442    Objective Loss 0.494442    Top1 72.594142    LR 0.000060    Time 0.183218    
2023-01-03 09:50:32,406 - --- validate (epoch=37)-----------
2023-01-03 09:50:32,406 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:33,009 - Epoch: [37][    5/    5]    Loss 0.470207    Top1 77.003817    
2023-01-03 09:50:33,045 - ==> Top1: 77.004    Loss: 0.470

2023-01-03 09:50:33,046 - ==> Confusion:
[[354  75   0]
 [166 453   0]
 [  0   0   0]]

2023-01-03 09:50:33,048 - ==> Best [Top1: 77.004   Sparsity:0.00   Params: 360896 on epoch: 37]
2023-01-03 09:50:33,049 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:33,072 - 

2023-01-03 09:50:33,072 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:35,116 - Epoch: [38][   10/   37]    Overall Loss 0.492337    Objective Loss 0.492337                                        LR 0.000060    Time 0.204296    
2023-01-03 09:50:36,883 - Epoch: [38][   20/   37]    Overall Loss 0.490552    Objective Loss 0.490552                                        LR 0.000060    Time 0.190450    
2023-01-03 09:50:38,682 - Epoch: [38][   30/   37]    Overall Loss 0.486630    Objective Loss 0.486630                                        LR 0.000060    Time 0.186896    
2023-01-03 09:50:39,840 - Epoch: [38][   37/   37]    Overall Loss 0.487051    Objective Loss 0.487051    Top1 74.476987    LR 0.000060    Time 0.182834    
2023-01-03 09:50:39,879 - --- validate (epoch=38)-----------
2023-01-03 09:50:39,880 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:40,486 - Epoch: [38][    5/    5]    Loss 0.505163    Top1 77.767176    
2023-01-03 09:50:40,523 - ==> Top1: 77.767    Loss: 0.505

2023-01-03 09:50:40,524 - ==> Confusion:
[[342  87   0]
 [146 473   0]
 [  0   0   0]]

2023-01-03 09:50:40,529 - ==> Best [Top1: 77.767   Sparsity:0.00   Params: 360896 on epoch: 38]
2023-01-03 09:50:40,530 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:40,554 - 

2023-01-03 09:50:40,555 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:42,901 - Epoch: [39][   10/   37]    Overall Loss 0.475242    Objective Loss 0.475242                                        LR 0.000060    Time 0.234472    
2023-01-03 09:50:44,663 - Epoch: [39][   20/   37]    Overall Loss 0.486757    Objective Loss 0.486757                                        LR 0.000060    Time 0.205311    
2023-01-03 09:50:46,437 - Epoch: [39][   30/   37]    Overall Loss 0.486222    Objective Loss 0.486222                                        LR 0.000060    Time 0.195989    
2023-01-03 09:50:47,581 - Epoch: [39][   37/   37]    Overall Loss 0.478628    Objective Loss 0.478628    Top1 81.171548    LR 0.000060    Time 0.189832    
2023-01-03 09:50:47,638 - --- validate (epoch=39)-----------
2023-01-03 09:50:47,638 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:48,264 - Epoch: [39][    5/    5]    Loss 0.485111    Top1 77.576336    
2023-01-03 09:50:48,304 - ==> Top1: 77.576    Loss: 0.485

2023-01-03 09:50:48,305 - ==> Confusion:
[[350  79   0]
 [156 463   0]
 [  0   0   0]]

2023-01-03 09:50:48,307 - ==> Best [Top1: 77.767   Sparsity:0.00   Params: 360896 on epoch: 38]
2023-01-03 09:50:48,307 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:48,318 - 

2023-01-03 09:50:48,319 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:50,353 - Epoch: [40][   10/   37]    Overall Loss 0.458324    Objective Loss 0.458324                                        LR 0.000036    Time 0.203298    
2023-01-03 09:50:52,132 - Epoch: [40][   20/   37]    Overall Loss 0.470816    Objective Loss 0.470816                                        LR 0.000036    Time 0.190562    
2023-01-03 09:50:53,924 - Epoch: [40][   30/   37]    Overall Loss 0.468741    Objective Loss 0.468741                                        LR 0.000036    Time 0.186763    
2023-01-03 09:50:55,081 - Epoch: [40][   37/   37]    Overall Loss 0.464795    Objective Loss 0.464795    Top1 78.870293    LR 0.000036    Time 0.182675    
2023-01-03 09:50:55,123 - --- validate (epoch=40)-----------
2023-01-03 09:50:55,124 - 1048 samples (256 per mini-batch)
2023-01-03 09:50:55,714 - Epoch: [40][    5/    5]    Loss 0.465672    Top1 78.053435    
2023-01-03 09:50:55,750 - ==> Top1: 78.053    Loss: 0.466

2023-01-03 09:50:55,750 - ==> Confusion:
[[319 110   0]
 [120 499   0]
 [  0   0   0]]

2023-01-03 09:50:55,752 - ==> Best [Top1: 78.053   Sparsity:0.00   Params: 360896 on epoch: 40]
2023-01-03 09:50:55,752 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:50:55,777 - 

2023-01-03 09:50:55,777 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:50:57,798 - Epoch: [41][   10/   37]    Overall Loss 0.458183    Objective Loss 0.458183                                        LR 0.000036    Time 0.201900    
2023-01-03 09:50:59,588 - Epoch: [41][   20/   37]    Overall Loss 0.463736    Objective Loss 0.463736                                        LR 0.000036    Time 0.190428    
2023-01-03 09:51:01,364 - Epoch: [41][   30/   37]    Overall Loss 0.455568    Objective Loss 0.455568                                        LR 0.000036    Time 0.186140    
2023-01-03 09:51:02,533 - Epoch: [41][   37/   37]    Overall Loss 0.458671    Objective Loss 0.458671    Top1 76.987448    LR 0.000036    Time 0.182512    
2023-01-03 09:51:02,568 - --- validate (epoch=41)-----------
2023-01-03 09:51:02,568 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:03,164 - Epoch: [41][    5/    5]    Loss 0.446147    Top1 77.576336    
2023-01-03 09:51:03,203 - ==> Top1: 77.576    Loss: 0.446

2023-01-03 09:51:03,204 - ==> Confusion:
[[359  70   0]
 [165 454   0]
 [  0   0   0]]

2023-01-03 09:51:03,208 - ==> Best [Top1: 78.053   Sparsity:0.00   Params: 360896 on epoch: 40]
2023-01-03 09:51:03,208 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:03,221 - 

2023-01-03 09:51:03,221 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:05,263 - Epoch: [42][   10/   37]    Overall Loss 0.462631    Objective Loss 0.462631                                        LR 0.000036    Time 0.204057    
2023-01-03 09:51:07,052 - Epoch: [42][   20/   37]    Overall Loss 0.465576    Objective Loss 0.465576                                        LR 0.000036    Time 0.191415    
2023-01-03 09:51:08,818 - Epoch: [42][   30/   37]    Overall Loss 0.456454    Objective Loss 0.456454                                        LR 0.000036    Time 0.186475    
2023-01-03 09:51:09,993 - Epoch: [42][   37/   37]    Overall Loss 0.455820    Objective Loss 0.455820    Top1 79.707113    LR 0.000036    Time 0.182933    
2023-01-03 09:51:10,032 - --- validate (epoch=42)-----------
2023-01-03 09:51:10,033 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:10,647 - Epoch: [42][    5/    5]    Loss 0.451607    Top1 78.244275    
2023-01-03 09:51:10,681 - ==> Top1: 78.244    Loss: 0.452

2023-01-03 09:51:10,682 - ==> Confusion:
[[358  71   0]
 [157 462   0]
 [  0   0   0]]

2023-01-03 09:51:10,684 - ==> Best [Top1: 78.244   Sparsity:0.00   Params: 360896 on epoch: 42]
2023-01-03 09:51:10,684 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:10,703 - 

2023-01-03 09:51:10,703 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:12,712 - Epoch: [43][   10/   37]    Overall Loss 0.450843    Objective Loss 0.450843                                        LR 0.000036    Time 0.200714    
2023-01-03 09:51:14,487 - Epoch: [43][   20/   37]    Overall Loss 0.448284    Objective Loss 0.448284                                        LR 0.000036    Time 0.189078    
2023-01-03 09:51:16,242 - Epoch: [43][   30/   37]    Overall Loss 0.453720    Objective Loss 0.453720                                        LR 0.000036    Time 0.184537    
2023-01-03 09:51:17,393 - Epoch: [43][   37/   37]    Overall Loss 0.453131    Objective Loss 0.453131    Top1 79.916318    LR 0.000036    Time 0.180713    
2023-01-03 09:51:17,433 - --- validate (epoch=43)-----------
2023-01-03 09:51:17,433 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:18,043 - Epoch: [43][    5/    5]    Loss 0.451307    Top1 78.435115    
2023-01-03 09:51:18,075 - ==> Top1: 78.435    Loss: 0.451

2023-01-03 09:51:18,075 - ==> Confusion:
[[293 136   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-03 09:51:18,078 - ==> Best [Top1: 78.435   Sparsity:0.00   Params: 360896 on epoch: 43]
2023-01-03 09:51:18,078 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:18,101 - 

2023-01-03 09:51:18,102 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:20,190 - Epoch: [44][   10/   37]    Overall Loss 0.452857    Objective Loss 0.452857                                        LR 0.000036    Time 0.208656    
2023-01-03 09:51:21,969 - Epoch: [44][   20/   37]    Overall Loss 0.455990    Objective Loss 0.455990                                        LR 0.000036    Time 0.193274    
2023-01-03 09:51:23,737 - Epoch: [44][   30/   37]    Overall Loss 0.446562    Objective Loss 0.446562                                        LR 0.000036    Time 0.187736    
2023-01-03 09:51:24,907 - Epoch: [44][   37/   37]    Overall Loss 0.447849    Objective Loss 0.447849    Top1 78.661088    LR 0.000036    Time 0.183840    
2023-01-03 09:51:24,950 - --- validate (epoch=44)-----------
2023-01-03 09:51:24,950 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:25,561 - Epoch: [44][    5/    5]    Loss 0.411968    Top1 80.629771    
2023-01-03 09:51:25,595 - ==> Top1: 80.630    Loss: 0.412

2023-01-03 09:51:25,596 - ==> Confusion:
[[311 118   0]
 [ 85 534   0]
 [  0   0   0]]

2023-01-03 09:51:25,599 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-03 09:51:25,600 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:25,618 - 

2023-01-03 09:51:25,618 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:27,650 - Epoch: [45][   10/   37]    Overall Loss 0.443146    Objective Loss 0.443146                                        LR 0.000036    Time 0.202985    
2023-01-03 09:51:29,407 - Epoch: [45][   20/   37]    Overall Loss 0.439572    Objective Loss 0.439572                                        LR 0.000036    Time 0.189302    
2023-01-03 09:51:31,174 - Epoch: [45][   30/   37]    Overall Loss 0.437534    Objective Loss 0.437534                                        LR 0.000036    Time 0.185079    
2023-01-03 09:51:32,333 - Epoch: [45][   37/   37]    Overall Loss 0.436786    Objective Loss 0.436786    Top1 78.870293    LR 0.000036    Time 0.181381    
2023-01-03 09:51:32,375 - --- validate (epoch=45)-----------
2023-01-03 09:51:32,376 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:32,971 - Epoch: [45][    5/    5]    Loss 0.433202    Top1 78.339695    
2023-01-03 09:51:33,007 - ==> Top1: 78.340    Loss: 0.433

2023-01-03 09:51:33,008 - ==> Confusion:
[[256 173   0]
 [ 54 565   0]
 [  0   0   0]]

2023-01-03 09:51:33,012 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-03 09:51:33,012 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:33,025 - 

2023-01-03 09:51:33,025 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:35,046 - Epoch: [46][   10/   37]    Overall Loss 0.424307    Objective Loss 0.424307                                        LR 0.000036    Time 0.201900    
2023-01-03 09:51:36,821 - Epoch: [46][   20/   37]    Overall Loss 0.433686    Objective Loss 0.433686                                        LR 0.000036    Time 0.189672    
2023-01-03 09:51:38,603 - Epoch: [46][   30/   37]    Overall Loss 0.432379    Objective Loss 0.432379                                        LR 0.000036    Time 0.185828    
2023-01-03 09:51:39,783 - Epoch: [46][   37/   37]    Overall Loss 0.433963    Objective Loss 0.433963    Top1 79.288703    LR 0.000036    Time 0.182543    
2023-01-03 09:51:39,824 - --- validate (epoch=46)-----------
2023-01-03 09:51:39,825 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:40,425 - Epoch: [46][    5/    5]    Loss 0.441081    Top1 79.198473    
2023-01-03 09:51:40,460 - ==> Top1: 79.198    Loss: 0.441

2023-01-03 09:51:40,460 - ==> Confusion:
[[359  70   0]
 [148 471   0]
 [  0   0   0]]

2023-01-03 09:51:40,462 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-03 09:51:40,462 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:40,476 - 

2023-01-03 09:51:40,476 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:42,498 - Epoch: [47][   10/   37]    Overall Loss 0.445148    Objective Loss 0.445148                                        LR 0.000036    Time 0.202094    
2023-01-03 09:51:44,274 - Epoch: [47][   20/   37]    Overall Loss 0.442492    Objective Loss 0.442492                                        LR 0.000036    Time 0.189783    
2023-01-03 09:51:46,052 - Epoch: [47][   30/   37]    Overall Loss 0.434832    Objective Loss 0.434832                                        LR 0.000036    Time 0.185755    
2023-01-03 09:51:47,206 - Epoch: [47][   37/   37]    Overall Loss 0.432296    Objective Loss 0.432296    Top1 81.380753    LR 0.000036    Time 0.181807    
2023-01-03 09:51:47,241 - --- validate (epoch=47)-----------
2023-01-03 09:51:47,242 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:47,846 - Epoch: [47][    5/    5]    Loss 0.407892    Top1 79.675573    
2023-01-03 09:51:47,878 - ==> Top1: 79.676    Loss: 0.408

2023-01-03 09:51:47,879 - ==> Confusion:
[[351  78   0]
 [135 484   0]
 [  0   0   0]]

2023-01-03 09:51:47,881 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-03 09:51:47,881 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:47,896 - 

2023-01-03 09:51:47,896 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:49,936 - Epoch: [48][   10/   37]    Overall Loss 0.421775    Objective Loss 0.421775                                        LR 0.000036    Time 0.203845    
2023-01-03 09:51:51,710 - Epoch: [48][   20/   37]    Overall Loss 0.418497    Objective Loss 0.418497                                        LR 0.000036    Time 0.190573    
2023-01-03 09:51:53,471 - Epoch: [48][   30/   37]    Overall Loss 0.428619    Objective Loss 0.428619                                        LR 0.000036    Time 0.185713    
2023-01-03 09:51:54,638 - Epoch: [48][   37/   37]    Overall Loss 0.428286    Objective Loss 0.428286    Top1 78.033473    LR 0.000036    Time 0.182120    
2023-01-03 09:51:54,679 - --- validate (epoch=48)-----------
2023-01-03 09:51:54,679 - 1048 samples (256 per mini-batch)
2023-01-03 09:51:55,286 - Epoch: [48][    5/    5]    Loss 0.399850    Top1 80.725191    
2023-01-03 09:51:55,321 - ==> Top1: 80.725    Loss: 0.400

2023-01-03 09:51:55,322 - ==> Confusion:
[[305 124   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-03 09:51:55,325 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 360896 on epoch: 48]
2023-01-03 09:51:55,325 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:51:55,354 - 

2023-01-03 09:51:55,355 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 09:51:57,401 - Epoch: [49][   10/   37]    Overall Loss 0.445050    Objective Loss 0.445050                                        LR 0.000036    Time 0.204499    
2023-01-03 09:51:59,173 - Epoch: [49][   20/   37]    Overall Loss 0.433374    Objective Loss 0.433374                                        LR 0.000036    Time 0.190807    
2023-01-03 09:52:00,947 - Epoch: [49][   30/   37]    Overall Loss 0.424039    Objective Loss 0.424039                                        LR 0.000036    Time 0.186325    
2023-01-03 09:52:02,119 - Epoch: [49][   37/   37]    Overall Loss 0.424788    Objective Loss 0.424788    Top1 80.962343    LR 0.000036    Time 0.182746    
2023-01-03 09:52:02,160 - --- validate (epoch=49)-----------
2023-01-03 09:52:02,160 - 1048 samples (256 per mini-batch)
2023-01-03 09:52:02,760 - Epoch: [49][    5/    5]    Loss 0.430492    Top1 80.629771    
2023-01-03 09:52:02,792 - ==> Top1: 80.630    Loss: 0.430

2023-01-03 09:52:02,792 - ==> Confusion:
[[298 131   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-03 09:52:02,795 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 360896 on epoch: 48]
2023-01-03 09:52:02,795 - Saving checkpoint to: logs/2023.01.03-094558/qat_checkpoint.pth.tar
2023-01-03 09:52:02,817 - --- test ---------------------
2023-01-03 09:52:02,817 - 1317 samples (256 per mini-batch)
2023-01-03 09:52:03,513 - Test: [    6/    6]    Loss 0.464377    Top1 77.524677    
2023-01-03 09:52:03,549 - ==> Top1: 77.525    Loss: 0.464

2023-01-03 09:52:03,550 - ==> Confusion:
[[385 176   0]
 [120 636   0]
 [  0   0   0]]

2023-01-03 09:52:03,570 - 
2023-01-03 09:52:03,570 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-094558/2023.01.03-094558.log
