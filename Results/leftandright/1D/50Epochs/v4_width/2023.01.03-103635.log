2023-01-03 10:36:35,540 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-103635/2023.01.03-103635.log
2023-01-03 10:36:35,549 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-03 10:36:35,549 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-03 10:36:47,734 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-03 10:36:47,734 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-03 10:36:47,739 - 

2023-01-03 10:36:47,739 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:36:48,746 - Epoch: [0][   10/   37]    Overall Loss 1.098202    Objective Loss 1.098202                                        LR 0.000060    Time 0.100576    
2023-01-03 10:36:49,525 - Epoch: [0][   20/   37]    Overall Loss 1.096737    Objective Loss 1.096737                                        LR 0.000060    Time 0.089202    
2023-01-03 10:36:50,287 - Epoch: [0][   30/   37]    Overall Loss 1.094647    Objective Loss 1.094647                                        LR 0.000060    Time 0.084845    
2023-01-03 10:36:50,757 - Epoch: [0][   37/   37]    Overall Loss 1.092354    Objective Loss 1.092354    Top1 43.723849    LR 0.000060    Time 0.081496    
2023-01-03 10:36:50,791 - --- validate (epoch=0)-----------
2023-01-03 10:36:50,791 - 1048 samples (256 per mini-batch)
2023-01-03 10:36:51,096 - Epoch: [0][    5/    5]    Loss 1.077171    Top1 40.935115    
2023-01-03 10:36:51,130 - ==> Top1: 40.935    Loss: 1.077

2023-01-03 10:36:51,131 - ==> Confusion:
[[429   0   0]
 [619   0   0]
 [  0   0   0]]

2023-01-03 10:36:51,132 - ==> Best [Top1: 40.935   Sparsity:0.00   Params: 104480 on epoch: 0]
2023-01-03 10:36:51,133 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:36:51,142 - 

2023-01-03 10:36:51,142 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:36:52,172 - Epoch: [1][   10/   37]    Overall Loss 1.067470    Objective Loss 1.067470                                        LR 0.000060    Time 0.102843    
2023-01-03 10:36:52,966 - Epoch: [1][   20/   37]    Overall Loss 1.050247    Objective Loss 1.050247                                        LR 0.000060    Time 0.091100    
2023-01-03 10:36:53,753 - Epoch: [1][   30/   37]    Overall Loss 1.023231    Objective Loss 1.023231                                        LR 0.000060    Time 0.086951    
2023-01-03 10:36:54,229 - Epoch: [1][   37/   37]    Overall Loss 1.000896    Objective Loss 1.000896    Top1 43.096234    LR 0.000060    Time 0.083348    
2023-01-03 10:36:54,269 - --- validate (epoch=1)-----------
2023-01-03 10:36:54,269 - 1048 samples (256 per mini-batch)
2023-01-03 10:36:54,570 - Epoch: [1][    5/    5]    Loss 0.891450    Top1 40.935115    
2023-01-03 10:36:54,600 - ==> Top1: 40.935    Loss: 0.891

2023-01-03 10:36:54,601 - ==> Confusion:
[[429   0   0]
 [619   0   0]
 [  0   0   0]]

2023-01-03 10:36:54,604 - ==> Best [Top1: 40.935   Sparsity:0.00   Params: 104480 on epoch: 1]
2023-01-03 10:36:54,604 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:36:54,612 - 

2023-01-03 10:36:54,613 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:36:55,667 - Epoch: [2][   10/   37]    Overall Loss 0.872462    Objective Loss 0.872462                                        LR 0.000060    Time 0.105343    
2023-01-03 10:36:56,469 - Epoch: [2][   20/   37]    Overall Loss 0.863840    Objective Loss 0.863840                                        LR 0.000060    Time 0.092702    
2023-01-03 10:36:57,250 - Epoch: [2][   30/   37]    Overall Loss 0.855574    Objective Loss 0.855574                                        LR 0.000060    Time 0.087815    
2023-01-03 10:36:57,702 - Epoch: [2][   37/   37]    Overall Loss 0.850154    Objective Loss 0.850154    Top1 58.368201    LR 0.000060    Time 0.083418    
2023-01-03 10:36:57,741 - --- validate (epoch=2)-----------
2023-01-03 10:36:57,742 - 1048 samples (256 per mini-batch)
2023-01-03 10:36:58,050 - Epoch: [2][    5/    5]    Loss 0.822788    Top1 59.064885    
2023-01-03 10:36:58,082 - ==> Top1: 59.065    Loss: 0.823

2023-01-03 10:36:58,083 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:36:58,084 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 2]
2023-01-03 10:36:58,084 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:36:58,092 - 

2023-01-03 10:36:58,093 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:36:59,161 - Epoch: [3][   10/   37]    Overall Loss 0.818493    Objective Loss 0.818493                                        LR 0.000060    Time 0.106700    
2023-01-03 10:36:59,952 - Epoch: [3][   20/   37]    Overall Loss 0.813666    Objective Loss 0.813666                                        LR 0.000060    Time 0.092845    
2023-01-03 10:37:00,716 - Epoch: [3][   30/   37]    Overall Loss 0.810172    Objective Loss 0.810172                                        LR 0.000060    Time 0.087338    
2023-01-03 10:37:01,170 - Epoch: [3][   37/   37]    Overall Loss 0.806900    Objective Loss 0.806900    Top1 58.786611    LR 0.000060    Time 0.083092    
2023-01-03 10:37:01,205 - --- validate (epoch=3)-----------
2023-01-03 10:37:01,205 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:01,491 - Epoch: [3][    5/    5]    Loss 0.787943    Top1 59.064885    
2023-01-03 10:37:01,523 - ==> Top1: 59.065    Loss: 0.788

2023-01-03 10:37:01,524 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:01,526 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 3]
2023-01-03 10:37:01,526 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:01,535 - 

2023-01-03 10:37:01,535 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:02,577 - Epoch: [4][   10/   37]    Overall Loss 0.790902    Objective Loss 0.790902                                        LR 0.000060    Time 0.104005    
2023-01-03 10:37:03,371 - Epoch: [4][   20/   37]    Overall Loss 0.787060    Objective Loss 0.787060                                        LR 0.000060    Time 0.091644    
2023-01-03 10:37:04,163 - Epoch: [4][   30/   37]    Overall Loss 0.782966    Objective Loss 0.782966                                        LR 0.000060    Time 0.087483    
2023-01-03 10:37:04,623 - Epoch: [4][   37/   37]    Overall Loss 0.780352    Objective Loss 0.780352    Top1 55.439331    LR 0.000060    Time 0.083367    
2023-01-03 10:37:04,660 - --- validate (epoch=4)-----------
2023-01-03 10:37:04,661 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:04,947 - Epoch: [4][    5/    5]    Loss 0.757232    Top1 59.064885    
2023-01-03 10:37:04,983 - ==> Top1: 59.065    Loss: 0.757

2023-01-03 10:37:04,983 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:04,987 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 4]
2023-01-03 10:37:04,987 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:04,996 - 

2023-01-03 10:37:04,996 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:06,041 - Epoch: [5][   10/   37]    Overall Loss 0.768711    Objective Loss 0.768711                                        LR 0.000060    Time 0.104295    
2023-01-03 10:37:06,860 - Epoch: [5][   20/   37]    Overall Loss 0.764981    Objective Loss 0.764981                                        LR 0.000060    Time 0.093072    
2023-01-03 10:37:07,654 - Epoch: [5][   30/   37]    Overall Loss 0.761268    Objective Loss 0.761268                                        LR 0.000060    Time 0.088493    
2023-01-03 10:37:08,126 - Epoch: [5][   37/   37]    Overall Loss 0.758652    Objective Loss 0.758652    Top1 58.158996    LR 0.000060    Time 0.084514    
2023-01-03 10:37:08,165 - --- validate (epoch=5)-----------
2023-01-03 10:37:08,166 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:08,471 - Epoch: [5][    5/    5]    Loss 0.736712    Top1 59.064885    
2023-01-03 10:37:08,505 - ==> Top1: 59.065    Loss: 0.737

2023-01-03 10:37:08,506 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:08,508 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 5]
2023-01-03 10:37:08,508 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:08,517 - 

2023-01-03 10:37:08,517 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:09,569 - Epoch: [6][   10/   37]    Overall Loss 0.750198    Objective Loss 0.750198                                        LR 0.000060    Time 0.105089    
2023-01-03 10:37:10,378 - Epoch: [6][   20/   37]    Overall Loss 0.746162    Objective Loss 0.746162                                        LR 0.000060    Time 0.092950    
2023-01-03 10:37:11,182 - Epoch: [6][   30/   37]    Overall Loss 0.745692    Objective Loss 0.745692                                        LR 0.000060    Time 0.088739    
2023-01-03 10:37:11,656 - Epoch: [6][   37/   37]    Overall Loss 0.745414    Objective Loss 0.745414    Top1 52.719665    LR 0.000060    Time 0.084766    
2023-01-03 10:37:11,693 - --- validate (epoch=6)-----------
2023-01-03 10:37:11,694 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:12,008 - Epoch: [6][    5/    5]    Loss 0.733855    Top1 59.064885    
2023-01-03 10:37:12,045 - ==> Top1: 59.065    Loss: 0.734

2023-01-03 10:37:12,045 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:12,046 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 6]
2023-01-03 10:37:12,047 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:12,056 - 

2023-01-03 10:37:12,056 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:13,081 - Epoch: [7][   10/   37]    Overall Loss 0.740752    Objective Loss 0.740752                                        LR 0.000060    Time 0.102322    
2023-01-03 10:37:13,897 - Epoch: [7][   20/   37]    Overall Loss 0.738219    Objective Loss 0.738219                                        LR 0.000060    Time 0.091912    
2023-01-03 10:37:14,683 - Epoch: [7][   30/   37]    Overall Loss 0.736342    Objective Loss 0.736342                                        LR 0.000060    Time 0.087446    
2023-01-03 10:37:15,162 - Epoch: [7][   37/   37]    Overall Loss 0.734447    Objective Loss 0.734447    Top1 59.623431    LR 0.000060    Time 0.083837    
2023-01-03 10:37:15,195 - --- validate (epoch=7)-----------
2023-01-03 10:37:15,195 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:15,504 - Epoch: [7][    5/    5]    Loss 0.721305    Top1 59.064885    
2023-01-03 10:37:15,538 - ==> Top1: 59.065    Loss: 0.721

2023-01-03 10:37:15,538 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:15,540 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 7]
2023-01-03 10:37:15,540 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:15,549 - 

2023-01-03 10:37:15,550 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:16,736 - Epoch: [8][   10/   37]    Overall Loss 0.730250    Objective Loss 0.730250                                        LR 0.000060    Time 0.118526    
2023-01-03 10:37:17,526 - Epoch: [8][   20/   37]    Overall Loss 0.727449    Objective Loss 0.727449                                        LR 0.000060    Time 0.098750    
2023-01-03 10:37:18,299 - Epoch: [8][   30/   37]    Overall Loss 0.726170    Objective Loss 0.726170                                        LR 0.000060    Time 0.091548    
2023-01-03 10:37:18,787 - Epoch: [8][   37/   37]    Overall Loss 0.725142    Objective Loss 0.725142    Top1 56.066946    LR 0.000060    Time 0.087416    
2023-01-03 10:37:18,826 - --- validate (epoch=8)-----------
2023-01-03 10:37:18,826 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:19,134 - Epoch: [8][    5/    5]    Loss 0.726399    Top1 59.064885    
2023-01-03 10:37:19,179 - ==> Top1: 59.065    Loss: 0.726

2023-01-03 10:37:19,179 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:19,182 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 8]
2023-01-03 10:37:19,182 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:19,191 - 

2023-01-03 10:37:19,191 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:20,266 - Epoch: [9][   10/   37]    Overall Loss 0.718636    Objective Loss 0.718636                                        LR 0.000060    Time 0.107317    
2023-01-03 10:37:21,076 - Epoch: [9][   20/   37]    Overall Loss 0.720578    Objective Loss 0.720578                                        LR 0.000060    Time 0.094105    
2023-01-03 10:37:21,856 - Epoch: [9][   30/   37]    Overall Loss 0.719491    Objective Loss 0.719491                                        LR 0.000060    Time 0.088729    
2023-01-03 10:37:22,330 - Epoch: [9][   37/   37]    Overall Loss 0.718164    Objective Loss 0.718164    Top1 55.230126    LR 0.000060    Time 0.084732    
2023-01-03 10:37:22,365 - --- validate (epoch=9)-----------
2023-01-03 10:37:22,365 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:22,663 - Epoch: [9][    5/    5]    Loss 0.716319    Top1 59.064885    
2023-01-03 10:37:22,694 - ==> Top1: 59.065    Loss: 0.716

2023-01-03 10:37:22,694 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:22,696 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 9]
2023-01-03 10:37:22,696 - Saving checkpoint to: logs/2023.01.03-103635/checkpoint.pth.tar
2023-01-03 10:37:22,724 - 

2023-01-03 10:37:22,725 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:23,937 - Epoch: [10][   10/   37]    Overall Loss 0.892896    Objective Loss 0.892896                                        LR 0.000060    Time 0.121162    
2023-01-03 10:37:24,911 - Epoch: [10][   20/   37]    Overall Loss 0.806832    Objective Loss 0.806832                                        LR 0.000060    Time 0.109247    
2023-01-03 10:37:25,948 - Epoch: [10][   30/   37]    Overall Loss 0.768327    Objective Loss 0.768327                                        LR 0.000060    Time 0.107364    
2023-01-03 10:37:26,630 - Epoch: [10][   37/   37]    Overall Loss 0.752445    Objective Loss 0.752445    Top1 60.251046    LR 0.000060    Time 0.105469    
2023-01-03 10:37:26,670 - --- validate (epoch=10)-----------
2023-01-03 10:37:26,670 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:27,122 - Epoch: [10][    5/    5]    Loss 0.681515    Top1 59.064885    
2023-01-03 10:37:27,158 - ==> Top1: 59.065    Loss: 0.682

2023-01-03 10:37:27,159 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:27,161 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 10]
2023-01-03 10:37:27,161 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:27,170 - 

2023-01-03 10:37:27,170 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:28,369 - Epoch: [11][   10/   37]    Overall Loss 0.687360    Objective Loss 0.687360                                        LR 0.000060    Time 0.119679    
2023-01-03 10:37:29,323 - Epoch: [11][   20/   37]    Overall Loss 0.683587    Objective Loss 0.683587                                        LR 0.000060    Time 0.107479    
2023-01-03 10:37:30,274 - Epoch: [11][   30/   37]    Overall Loss 0.684198    Objective Loss 0.684198                                        LR 0.000060    Time 0.103349    
2023-01-03 10:37:30,868 - Epoch: [11][   37/   37]    Overall Loss 0.683577    Objective Loss 0.683577    Top1 58.158996    LR 0.000060    Time 0.099835    
2023-01-03 10:37:30,910 - --- validate (epoch=11)-----------
2023-01-03 10:37:30,911 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:31,315 - Epoch: [11][    5/    5]    Loss 0.677537    Top1 59.064885    
2023-01-03 10:37:31,350 - ==> Top1: 59.065    Loss: 0.678

2023-01-03 10:37:31,350 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:31,352 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 11]
2023-01-03 10:37:31,352 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:31,361 - 

2023-01-03 10:37:31,361 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:32,558 - Epoch: [12][   10/   37]    Overall Loss 0.684872    Objective Loss 0.684872                                        LR 0.000060    Time 0.119522    
2023-01-03 10:37:33,523 - Epoch: [12][   20/   37]    Overall Loss 0.684010    Objective Loss 0.684010                                        LR 0.000060    Time 0.107978    
2023-01-03 10:37:34,501 - Epoch: [12][   30/   37]    Overall Loss 0.684448    Objective Loss 0.684448                                        LR 0.000060    Time 0.104541    
2023-01-03 10:37:35,087 - Epoch: [12][   37/   37]    Overall Loss 0.684223    Objective Loss 0.684223    Top1 56.903766    LR 0.000060    Time 0.100602    
2023-01-03 10:37:35,127 - --- validate (epoch=12)-----------
2023-01-03 10:37:35,127 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:35,558 - Epoch: [12][    5/    5]    Loss 0.682062    Top1 59.064885    
2023-01-03 10:37:35,592 - ==> Top1: 59.065    Loss: 0.682

2023-01-03 10:37:35,592 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:35,594 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 12]
2023-01-03 10:37:35,594 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:35,604 - 

2023-01-03 10:37:35,604 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:36,838 - Epoch: [13][   10/   37]    Overall Loss 0.684567    Objective Loss 0.684567                                        LR 0.000060    Time 0.123202    
2023-01-03 10:37:37,797 - Epoch: [13][   20/   37]    Overall Loss 0.683319    Objective Loss 0.683319                                        LR 0.000060    Time 0.109486    
2023-01-03 10:37:38,758 - Epoch: [13][   30/   37]    Overall Loss 0.682864    Objective Loss 0.682864                                        LR 0.000060    Time 0.105026    
2023-01-03 10:37:39,347 - Epoch: [13][   37/   37]    Overall Loss 0.683804    Objective Loss 0.683804    Top1 57.531381    LR 0.000060    Time 0.101052    
2023-01-03 10:37:39,388 - --- validate (epoch=13)-----------
2023-01-03 10:37:39,388 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:39,768 - Epoch: [13][    5/    5]    Loss 0.675974    Top1 59.064885    
2023-01-03 10:37:39,802 - ==> Top1: 59.065    Loss: 0.676

2023-01-03 10:37:39,803 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:39,804 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 13]
2023-01-03 10:37:39,805 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:39,814 - 

2023-01-03 10:37:39,814 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:41,048 - Epoch: [14][   10/   37]    Overall Loss 0.683726    Objective Loss 0.683726                                        LR 0.000060    Time 0.123180    
2023-01-03 10:37:42,017 - Epoch: [14][   20/   37]    Overall Loss 0.685980    Objective Loss 0.685980                                        LR 0.000060    Time 0.110008    
2023-01-03 10:37:42,977 - Epoch: [14][   30/   37]    Overall Loss 0.684684    Objective Loss 0.684684                                        LR 0.000060    Time 0.105340    
2023-01-03 10:37:43,588 - Epoch: [14][   37/   37]    Overall Loss 0.683645    Objective Loss 0.683645    Top1 58.786611    LR 0.000060    Time 0.101900    
2023-01-03 10:37:43,635 - --- validate (epoch=14)-----------
2023-01-03 10:37:43,635 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:44,035 - Epoch: [14][    5/    5]    Loss 0.671718    Top1 59.064885    
2023-01-03 10:37:44,071 - ==> Top1: 59.065    Loss: 0.672

2023-01-03 10:37:44,072 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:44,073 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 14]
2023-01-03 10:37:44,074 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:44,082 - 

2023-01-03 10:37:44,082 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:45,355 - Epoch: [15][   10/   37]    Overall Loss 0.677011    Objective Loss 0.677011                                        LR 0.000060    Time 0.127173    
2023-01-03 10:37:46,337 - Epoch: [15][   20/   37]    Overall Loss 0.683244    Objective Loss 0.683244                                        LR 0.000060    Time 0.112661    
2023-01-03 10:37:47,301 - Epoch: [15][   30/   37]    Overall Loss 0.684156    Objective Loss 0.684156                                        LR 0.000060    Time 0.107221    
2023-01-03 10:37:47,893 - Epoch: [15][   37/   37]    Overall Loss 0.683426    Objective Loss 0.683426    Top1 56.485356    LR 0.000060    Time 0.102935    
2023-01-03 10:37:47,937 - --- validate (epoch=15)-----------
2023-01-03 10:37:47,938 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:48,315 - Epoch: [15][    5/    5]    Loss 0.679102    Top1 59.064885    
2023-01-03 10:37:48,347 - ==> Top1: 59.065    Loss: 0.679

2023-01-03 10:37:48,348 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:48,350 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 15]
2023-01-03 10:37:48,350 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:48,359 - 

2023-01-03 10:37:48,359 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:49,608 - Epoch: [16][   10/   37]    Overall Loss 0.683545    Objective Loss 0.683545                                        LR 0.000060    Time 0.124678    
2023-01-03 10:37:50,580 - Epoch: [16][   20/   37]    Overall Loss 0.682070    Objective Loss 0.682070                                        LR 0.000060    Time 0.110942    
2023-01-03 10:37:51,538 - Epoch: [16][   30/   37]    Overall Loss 0.682225    Objective Loss 0.682225                                        LR 0.000060    Time 0.105860    
2023-01-03 10:37:52,133 - Epoch: [16][   37/   37]    Overall Loss 0.683028    Objective Loss 0.683028    Top1 54.393305    LR 0.000060    Time 0.101903    
2023-01-03 10:37:52,177 - --- validate (epoch=16)-----------
2023-01-03 10:37:52,177 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:52,573 - Epoch: [16][    5/    5]    Loss 0.674449    Top1 59.064885    
2023-01-03 10:37:52,609 - ==> Top1: 59.065    Loss: 0.674

2023-01-03 10:37:52,610 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:52,612 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 16]
2023-01-03 10:37:52,612 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:52,621 - 

2023-01-03 10:37:52,622 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:53,860 - Epoch: [17][   10/   37]    Overall Loss 0.683802    Objective Loss 0.683802                                        LR 0.000060    Time 0.123751    
2023-01-03 10:37:54,828 - Epoch: [17][   20/   37]    Overall Loss 0.682319    Objective Loss 0.682319                                        LR 0.000060    Time 0.110210    
2023-01-03 10:37:55,788 - Epoch: [17][   30/   37]    Overall Loss 0.683248    Objective Loss 0.683248                                        LR 0.000060    Time 0.105460    
2023-01-03 10:37:56,394 - Epoch: [17][   37/   37]    Overall Loss 0.682730    Objective Loss 0.682730    Top1 57.740586    LR 0.000060    Time 0.101879    
2023-01-03 10:37:56,441 - --- validate (epoch=17)-----------
2023-01-03 10:37:56,442 - 1048 samples (256 per mini-batch)
2023-01-03 10:37:56,840 - Epoch: [17][    5/    5]    Loss 0.674867    Top1 59.064885    
2023-01-03 10:37:56,877 - ==> Top1: 59.065    Loss: 0.675

2023-01-03 10:37:56,877 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:37:56,879 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 17]
2023-01-03 10:37:56,879 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:37:56,888 - 

2023-01-03 10:37:56,888 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:37:58,112 - Epoch: [18][   10/   37]    Overall Loss 0.679074    Objective Loss 0.679074                                        LR 0.000060    Time 0.122200    
2023-01-03 10:37:59,072 - Epoch: [18][   20/   37]    Overall Loss 0.682853    Objective Loss 0.682853                                        LR 0.000060    Time 0.109087    
2023-01-03 10:38:00,051 - Epoch: [18][   30/   37]    Overall Loss 0.682552    Objective Loss 0.682552                                        LR 0.000060    Time 0.105344    
2023-01-03 10:38:00,646 - Epoch: [18][   37/   37]    Overall Loss 0.683550    Objective Loss 0.683550    Top1 57.322176    LR 0.000060    Time 0.101469    
2023-01-03 10:38:00,687 - --- validate (epoch=18)-----------
2023-01-03 10:38:00,688 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:01,095 - Epoch: [18][    5/    5]    Loss 0.668951    Top1 59.064885    
2023-01-03 10:38:01,131 - ==> Top1: 59.065    Loss: 0.669

2023-01-03 10:38:01,132 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:01,133 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 18]
2023-01-03 10:38:01,133 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:01,142 - 

2023-01-03 10:38:01,142 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:02,398 - Epoch: [19][   10/   37]    Overall Loss 0.684181    Objective Loss 0.684181                                        LR 0.000060    Time 0.125447    
2023-01-03 10:38:03,385 - Epoch: [19][   20/   37]    Overall Loss 0.681355    Objective Loss 0.681355                                        LR 0.000060    Time 0.112070    
2023-01-03 10:38:04,337 - Epoch: [19][   30/   37]    Overall Loss 0.682191    Objective Loss 0.682191                                        LR 0.000060    Time 0.106421    
2023-01-03 10:38:04,927 - Epoch: [19][   37/   37]    Overall Loss 0.682539    Objective Loss 0.682539    Top1 53.765690    LR 0.000060    Time 0.102204    
2023-01-03 10:38:04,969 - --- validate (epoch=19)-----------
2023-01-03 10:38:04,969 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:05,358 - Epoch: [19][    5/    5]    Loss 0.683367    Top1 59.064885    
2023-01-03 10:38:05,398 - ==> Top1: 59.065    Loss: 0.683

2023-01-03 10:38:05,399 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:05,400 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 19]
2023-01-03 10:38:05,400 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:05,409 - 

2023-01-03 10:38:05,409 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:06,637 - Epoch: [20][   10/   37]    Overall Loss 0.683401    Objective Loss 0.683401                                        LR 0.000060    Time 0.122584    
2023-01-03 10:38:07,608 - Epoch: [20][   20/   37]    Overall Loss 0.682762    Objective Loss 0.682762                                        LR 0.000060    Time 0.109829    
2023-01-03 10:38:08,554 - Epoch: [20][   30/   37]    Overall Loss 0.682242    Objective Loss 0.682242                                        LR 0.000060    Time 0.104714    
2023-01-03 10:38:09,150 - Epoch: [20][   37/   37]    Overall Loss 0.682339    Objective Loss 0.682339    Top1 58.368201    LR 0.000060    Time 0.101013    
2023-01-03 10:38:09,191 - --- validate (epoch=20)-----------
2023-01-03 10:38:09,192 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:09,599 - Epoch: [20][    5/    5]    Loss 0.677195    Top1 59.064885    
2023-01-03 10:38:09,630 - ==> Top1: 59.065    Loss: 0.677

2023-01-03 10:38:09,630 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:09,632 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 20]
2023-01-03 10:38:09,632 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:09,640 - 

2023-01-03 10:38:09,640 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:10,887 - Epoch: [21][   10/   37]    Overall Loss 0.680503    Objective Loss 0.680503                                        LR 0.000060    Time 0.124500    
2023-01-03 10:38:11,873 - Epoch: [21][   20/   37]    Overall Loss 0.684137    Objective Loss 0.684137                                        LR 0.000060    Time 0.111505    
2023-01-03 10:38:12,818 - Epoch: [21][   30/   37]    Overall Loss 0.682050    Objective Loss 0.682050                                        LR 0.000060    Time 0.105823    
2023-01-03 10:38:13,468 - Epoch: [21][   37/   37]    Overall Loss 0.681841    Objective Loss 0.681841    Top1 54.602510    LR 0.000060    Time 0.103349    
2023-01-03 10:38:13,512 - --- validate (epoch=21)-----------
2023-01-03 10:38:13,513 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:13,892 - Epoch: [21][    5/    5]    Loss 0.675529    Top1 59.064885    
2023-01-03 10:38:13,924 - ==> Top1: 59.065    Loss: 0.676

2023-01-03 10:38:13,924 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:13,926 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 21]
2023-01-03 10:38:13,926 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:13,935 - 

2023-01-03 10:38:13,935 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:15,182 - Epoch: [22][   10/   37]    Overall Loss 0.683379    Objective Loss 0.683379                                        LR 0.000060    Time 0.124633    
2023-01-03 10:38:16,170 - Epoch: [22][   20/   37]    Overall Loss 0.682513    Objective Loss 0.682513                                        LR 0.000060    Time 0.111682    
2023-01-03 10:38:17,199 - Epoch: [22][   30/   37]    Overall Loss 0.681609    Objective Loss 0.681609                                        LR 0.000060    Time 0.108744    
2023-01-03 10:38:17,839 - Epoch: [22][   37/   37]    Overall Loss 0.681663    Objective Loss 0.681663    Top1 54.602510    LR 0.000060    Time 0.105455    
2023-01-03 10:38:17,880 - --- validate (epoch=22)-----------
2023-01-03 10:38:17,880 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:18,275 - Epoch: [22][    5/    5]    Loss 0.680237    Top1 59.064885    
2023-01-03 10:38:18,308 - ==> Top1: 59.065    Loss: 0.680

2023-01-03 10:38:18,309 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:18,311 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 22]
2023-01-03 10:38:18,311 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:18,320 - 

2023-01-03 10:38:18,321 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:19,679 - Epoch: [23][   10/   37]    Overall Loss 0.680025    Objective Loss 0.680025                                        LR 0.000060    Time 0.135728    
2023-01-03 10:38:20,651 - Epoch: [23][   20/   37]    Overall Loss 0.679757    Objective Loss 0.679757                                        LR 0.000060    Time 0.116446    
2023-01-03 10:38:21,601 - Epoch: [23][   30/   37]    Overall Loss 0.680512    Objective Loss 0.680512                                        LR 0.000060    Time 0.109266    
2023-01-03 10:38:22,204 - Epoch: [23][   37/   37]    Overall Loss 0.679810    Objective Loss 0.679810    Top1 59.205021    LR 0.000060    Time 0.104872    
2023-01-03 10:38:22,249 - --- validate (epoch=23)-----------
2023-01-03 10:38:22,249 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:22,694 - Epoch: [23][    5/    5]    Loss 0.669406    Top1 59.064885    
2023-01-03 10:38:22,729 - ==> Top1: 59.065    Loss: 0.669

2023-01-03 10:38:22,730 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:22,732 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 23]
2023-01-03 10:38:22,732 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:22,743 - 

2023-01-03 10:38:22,744 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:24,005 - Epoch: [24][   10/   37]    Overall Loss 0.674037    Objective Loss 0.674037                                        LR 0.000060    Time 0.126003    
2023-01-03 10:38:24,981 - Epoch: [24][   20/   37]    Overall Loss 0.677191    Objective Loss 0.677191                                        LR 0.000060    Time 0.111752    
2023-01-03 10:38:25,950 - Epoch: [24][   30/   37]    Overall Loss 0.678870    Objective Loss 0.678870                                        LR 0.000060    Time 0.106759    
2023-01-03 10:38:26,546 - Epoch: [24][   37/   37]    Overall Loss 0.678560    Objective Loss 0.678560    Top1 57.322176    LR 0.000060    Time 0.102668    
2023-01-03 10:38:26,588 - --- validate (epoch=24)-----------
2023-01-03 10:38:26,588 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:26,987 - Epoch: [24][    5/    5]    Loss 0.670843    Top1 59.064885    
2023-01-03 10:38:27,024 - ==> Top1: 59.065    Loss: 0.671

2023-01-03 10:38:27,024 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:27,026 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 24]
2023-01-03 10:38:27,026 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:27,035 - 

2023-01-03 10:38:27,035 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:28,297 - Epoch: [25][   10/   37]    Overall Loss 0.677433    Objective Loss 0.677433                                        LR 0.000060    Time 0.126026    
2023-01-03 10:38:29,268 - Epoch: [25][   20/   37]    Overall Loss 0.676160    Objective Loss 0.676160                                        LR 0.000060    Time 0.111532    
2023-01-03 10:38:30,236 - Epoch: [25][   30/   37]    Overall Loss 0.674401    Objective Loss 0.674401                                        LR 0.000060    Time 0.106597    
2023-01-03 10:38:30,822 - Epoch: [25][   37/   37]    Overall Loss 0.674879    Objective Loss 0.674879    Top1 54.393305    LR 0.000060    Time 0.102252    
2023-01-03 10:38:30,867 - --- validate (epoch=25)-----------
2023-01-03 10:38:30,867 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:31,261 - Epoch: [25][    5/    5]    Loss 0.672767    Top1 59.064885    
2023-01-03 10:38:31,299 - ==> Top1: 59.065    Loss: 0.673

2023-01-03 10:38:31,300 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:31,302 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 104480 on epoch: 25]
2023-01-03 10:38:31,303 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:31,311 - 

2023-01-03 10:38:31,311 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:32,556 - Epoch: [26][   10/   37]    Overall Loss 0.670614    Objective Loss 0.670614                                        LR 0.000060    Time 0.124312    
2023-01-03 10:38:33,526 - Epoch: [26][   20/   37]    Overall Loss 0.672669    Objective Loss 0.672669                                        LR 0.000060    Time 0.110654    
2023-01-03 10:38:34,478 - Epoch: [26][   30/   37]    Overall Loss 0.671606    Objective Loss 0.671606                                        LR 0.000060    Time 0.105471    
2023-01-03 10:38:35,081 - Epoch: [26][   37/   37]    Overall Loss 0.671576    Objective Loss 0.671576    Top1 56.276151    LR 0.000060    Time 0.101815    
2023-01-03 10:38:35,119 - --- validate (epoch=26)-----------
2023-01-03 10:38:35,120 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:35,549 - Epoch: [26][    5/    5]    Loss 0.667633    Top1 59.351145    
2023-01-03 10:38:35,588 - ==> Top1: 59.351    Loss: 0.668

2023-01-03 10:38:35,589 - ==> Confusion:
[[  5 424   0]
 [  2 617   0]
 [  0   0   0]]

2023-01-03 10:38:35,591 - ==> Best [Top1: 59.351   Sparsity:0.00   Params: 104480 on epoch: 26]
2023-01-03 10:38:35,591 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:35,600 - 

2023-01-03 10:38:35,601 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:36,831 - Epoch: [27][   10/   37]    Overall Loss 0.665721    Objective Loss 0.665721                                        LR 0.000060    Time 0.122944    
2023-01-03 10:38:37,785 - Epoch: [27][   20/   37]    Overall Loss 0.665708    Objective Loss 0.665708                                        LR 0.000060    Time 0.109127    
2023-01-03 10:38:38,754 - Epoch: [27][   30/   37]    Overall Loss 0.663742    Objective Loss 0.663742                                        LR 0.000060    Time 0.105029    
2023-01-03 10:38:39,358 - Epoch: [27][   37/   37]    Overall Loss 0.663517    Objective Loss 0.663517    Top1 59.623431    LR 0.000060    Time 0.101476    
2023-01-03 10:38:39,397 - --- validate (epoch=27)-----------
2023-01-03 10:38:39,397 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:39,799 - Epoch: [27][    5/    5]    Loss 0.669735    Top1 59.064885    
2023-01-03 10:38:39,836 - ==> Top1: 59.065    Loss: 0.670

2023-01-03 10:38:39,836 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-03 10:38:39,838 - ==> Best [Top1: 59.351   Sparsity:0.00   Params: 104480 on epoch: 26]
2023-01-03 10:38:39,838 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:39,845 - 

2023-01-03 10:38:39,846 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:41,100 - Epoch: [28][   10/   37]    Overall Loss 0.658854    Objective Loss 0.658854                                        LR 0.000060    Time 0.125310    
2023-01-03 10:38:42,082 - Epoch: [28][   20/   37]    Overall Loss 0.658571    Objective Loss 0.658571                                        LR 0.000060    Time 0.111737    
2023-01-03 10:38:43,047 - Epoch: [28][   30/   37]    Overall Loss 0.656131    Objective Loss 0.656131                                        LR 0.000060    Time 0.106642    
2023-01-03 10:38:43,657 - Epoch: [28][   37/   37]    Overall Loss 0.655342    Objective Loss 0.655342    Top1 65.690377    LR 0.000060    Time 0.102934    
2023-01-03 10:38:43,698 - --- validate (epoch=28)-----------
2023-01-03 10:38:43,699 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:44,089 - Epoch: [28][    5/    5]    Loss 0.648941    Top1 62.500000    
2023-01-03 10:38:44,126 - ==> Top1: 62.500    Loss: 0.649

2023-01-03 10:38:44,126 - ==> Confusion:
[[ 77 352   0]
 [ 41 578   0]
 [  0   0   0]]

2023-01-03 10:38:44,129 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 104480 on epoch: 28]
2023-01-03 10:38:44,130 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:44,140 - 

2023-01-03 10:38:44,140 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:45,373 - Epoch: [29][   10/   37]    Overall Loss 0.643944    Objective Loss 0.643944                                        LR 0.000060    Time 0.123144    
2023-01-03 10:38:46,358 - Epoch: [29][   20/   37]    Overall Loss 0.644743    Objective Loss 0.644743                                        LR 0.000060    Time 0.110787    
2023-01-03 10:38:47,311 - Epoch: [29][   30/   37]    Overall Loss 0.644875    Objective Loss 0.644875                                        LR 0.000060    Time 0.105604    
2023-01-03 10:38:47,896 - Epoch: [29][   37/   37]    Overall Loss 0.642541    Objective Loss 0.642541    Top1 66.317992    LR 0.000060    Time 0.101435    
2023-01-03 10:38:47,937 - --- validate (epoch=29)-----------
2023-01-03 10:38:47,937 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:48,338 - Epoch: [29][    5/    5]    Loss 0.641372    Top1 62.213740    
2023-01-03 10:38:48,383 - ==> Top1: 62.214    Loss: 0.641

2023-01-03 10:38:48,384 - ==> Confusion:
[[310 119   0]
 [277 342   0]
 [  0   0   0]]

2023-01-03 10:38:48,386 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 104480 on epoch: 28]
2023-01-03 10:38:48,386 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:48,392 - 

2023-01-03 10:38:48,392 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:49,642 - Epoch: [30][   10/   37]    Overall Loss 0.634233    Objective Loss 0.634233                                        LR 0.000060    Time 0.124800    
2023-01-03 10:38:50,633 - Epoch: [30][   20/   37]    Overall Loss 0.634706    Objective Loss 0.634706                                        LR 0.000060    Time 0.111926    
2023-01-03 10:38:51,619 - Epoch: [30][   30/   37]    Overall Loss 0.633143    Objective Loss 0.633143                                        LR 0.000060    Time 0.107460    
2023-01-03 10:38:52,213 - Epoch: [30][   37/   37]    Overall Loss 0.633049    Objective Loss 0.633049    Top1 56.903766    LR 0.000060    Time 0.103181    
2023-01-03 10:38:52,251 - --- validate (epoch=30)-----------
2023-01-03 10:38:52,252 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:52,658 - Epoch: [30][    5/    5]    Loss 0.611310    Top1 63.263359    
2023-01-03 10:38:52,691 - ==> Top1: 63.263    Loss: 0.611

2023-01-03 10:38:52,692 - ==> Confusion:
[[ 66 363   0]
 [ 22 597   0]
 [  0   0   0]]

2023-01-03 10:38:52,694 - ==> Best [Top1: 63.263   Sparsity:0.00   Params: 104480 on epoch: 30]
2023-01-03 10:38:52,694 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:52,704 - 

2023-01-03 10:38:52,704 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:53,922 - Epoch: [31][   10/   37]    Overall Loss 0.626215    Objective Loss 0.626215                                        LR 0.000060    Time 0.121607    
2023-01-03 10:38:54,880 - Epoch: [31][   20/   37]    Overall Loss 0.625285    Objective Loss 0.625285                                        LR 0.000060    Time 0.108658    
2023-01-03 10:38:55,812 - Epoch: [31][   30/   37]    Overall Loss 0.621088    Objective Loss 0.621088                                        LR 0.000060    Time 0.103507    
2023-01-03 10:38:56,418 - Epoch: [31][   37/   37]    Overall Loss 0.618322    Objective Loss 0.618322    Top1 71.757322    LR 0.000060    Time 0.100286    
2023-01-03 10:38:56,454 - --- validate (epoch=31)-----------
2023-01-03 10:38:56,455 - 1048 samples (256 per mini-batch)
2023-01-03 10:38:56,861 - Epoch: [31][    5/    5]    Loss 0.599694    Top1 66.603053    
2023-01-03 10:38:56,898 - ==> Top1: 66.603    Loss: 0.600

2023-01-03 10:38:56,898 - ==> Confusion:
[[125 304   0]
 [ 46 573   0]
 [  0   0   0]]

2023-01-03 10:38:56,900 - ==> Best [Top1: 66.603   Sparsity:0.00   Params: 104480 on epoch: 31]
2023-01-03 10:38:56,900 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:38:56,908 - 

2023-01-03 10:38:56,908 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:38:58,158 - Epoch: [32][   10/   37]    Overall Loss 0.609226    Objective Loss 0.609226                                        LR 0.000060    Time 0.124866    
2023-01-03 10:38:59,117 - Epoch: [32][   20/   37]    Overall Loss 0.605138    Objective Loss 0.605138                                        LR 0.000060    Time 0.110331    
2023-01-03 10:39:00,077 - Epoch: [32][   30/   37]    Overall Loss 0.602836    Objective Loss 0.602836                                        LR 0.000060    Time 0.105533    
2023-01-03 10:39:00,672 - Epoch: [32][   37/   37]    Overall Loss 0.602177    Objective Loss 0.602177    Top1 70.292887    LR 0.000060    Time 0.101628    
2023-01-03 10:39:00,714 - --- validate (epoch=32)-----------
2023-01-03 10:39:00,714 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:01,110 - Epoch: [32][    5/    5]    Loss 0.576536    Top1 67.366412    
2023-01-03 10:39:01,145 - ==> Top1: 67.366    Loss: 0.577

2023-01-03 10:39:01,146 - ==> Confusion:
[[134 295   0]
 [ 47 572   0]
 [  0   0   0]]

2023-01-03 10:39:01,148 - ==> Best [Top1: 67.366   Sparsity:0.00   Params: 104480 on epoch: 32]
2023-01-03 10:39:01,148 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:01,157 - 

2023-01-03 10:39:01,157 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:02,401 - Epoch: [33][   10/   37]    Overall Loss 0.590032    Objective Loss 0.590032                                        LR 0.000060    Time 0.124264    
2023-01-03 10:39:03,381 - Epoch: [33][   20/   37]    Overall Loss 0.586311    Objective Loss 0.586311                                        LR 0.000060    Time 0.111115    
2023-01-03 10:39:04,341 - Epoch: [33][   30/   37]    Overall Loss 0.582824    Objective Loss 0.582824                                        LR 0.000060    Time 0.106064    
2023-01-03 10:39:04,957 - Epoch: [33][   37/   37]    Overall Loss 0.582903    Objective Loss 0.582903    Top1 65.690377    LR 0.000060    Time 0.102630    
2023-01-03 10:39:05,000 - --- validate (epoch=33)-----------
2023-01-03 10:39:05,001 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:05,386 - Epoch: [33][    5/    5]    Loss 0.561040    Top1 69.179389    
2023-01-03 10:39:05,417 - ==> Top1: 69.179    Loss: 0.561

2023-01-03 10:39:05,418 - ==> Confusion:
[[151 278   0]
 [ 45 574   0]
 [  0   0   0]]

2023-01-03 10:39:05,420 - ==> Best [Top1: 69.179   Sparsity:0.00   Params: 104480 on epoch: 33]
2023-01-03 10:39:05,420 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:05,427 - 

2023-01-03 10:39:05,428 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:06,647 - Epoch: [34][   10/   37]    Overall Loss 0.573918    Objective Loss 0.573918                                        LR 0.000060    Time 0.121791    
2023-01-03 10:39:07,624 - Epoch: [34][   20/   37]    Overall Loss 0.577975    Objective Loss 0.577975                                        LR 0.000060    Time 0.109716    
2023-01-03 10:39:08,600 - Epoch: [34][   30/   37]    Overall Loss 0.572843    Objective Loss 0.572843                                        LR 0.000060    Time 0.105652    
2023-01-03 10:39:09,208 - Epoch: [34][   37/   37]    Overall Loss 0.568685    Objective Loss 0.568685    Top1 72.803347    LR 0.000060    Time 0.102091    
2023-01-03 10:39:09,249 - --- validate (epoch=34)-----------
2023-01-03 10:39:09,249 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:09,690 - Epoch: [34][    5/    5]    Loss 0.545924    Top1 73.377863    
2023-01-03 10:39:09,720 - ==> Top1: 73.378    Loss: 0.546

2023-01-03 10:39:09,721 - ==> Confusion:
[[246 183   0]
 [ 96 523   0]
 [  0   0   0]]

2023-01-03 10:39:09,723 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 104480 on epoch: 34]
2023-01-03 10:39:09,723 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:09,731 - 

2023-01-03 10:39:09,732 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:10,958 - Epoch: [35][   10/   37]    Overall Loss 0.559754    Objective Loss 0.559754                                        LR 0.000060    Time 0.122571    
2023-01-03 10:39:11,940 - Epoch: [35][   20/   37]    Overall Loss 0.554545    Objective Loss 0.554545                                        LR 0.000060    Time 0.110332    
2023-01-03 10:39:12,893 - Epoch: [35][   30/   37]    Overall Loss 0.556157    Objective Loss 0.556157                                        LR 0.000060    Time 0.105296    
2023-01-03 10:39:13,483 - Epoch: [35][   37/   37]    Overall Loss 0.557033    Objective Loss 0.557033    Top1 67.573222    LR 0.000060    Time 0.101311    
2023-01-03 10:39:13,519 - --- validate (epoch=35)-----------
2023-01-03 10:39:13,519 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:13,920 - Epoch: [35][    5/    5]    Loss 0.539268    Top1 70.992366    
2023-01-03 10:39:13,955 - ==> Top1: 70.992    Loss: 0.539

2023-01-03 10:39:13,955 - ==> Confusion:
[[171 258   0]
 [ 46 573   0]
 [  0   0   0]]

2023-01-03 10:39:13,957 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 104480 on epoch: 34]
2023-01-03 10:39:13,957 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:13,965 - 

2023-01-03 10:39:13,965 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:15,204 - Epoch: [36][   10/   37]    Overall Loss 0.545180    Objective Loss 0.545180                                        LR 0.000060    Time 0.123688    
2023-01-03 10:39:16,146 - Epoch: [36][   20/   37]    Overall Loss 0.543215    Objective Loss 0.543215                                        LR 0.000060    Time 0.108945    
2023-01-03 10:39:17,104 - Epoch: [36][   30/   37]    Overall Loss 0.544697    Objective Loss 0.544697                                        LR 0.000060    Time 0.104519    
2023-01-03 10:39:17,694 - Epoch: [36][   37/   37]    Overall Loss 0.544201    Objective Loss 0.544201    Top1 72.175732    LR 0.000060    Time 0.100686    
2023-01-03 10:39:17,731 - --- validate (epoch=36)-----------
2023-01-03 10:39:17,732 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:18,126 - Epoch: [36][    5/    5]    Loss 0.536085    Top1 74.141221    
2023-01-03 10:39:18,161 - ==> Top1: 74.141    Loss: 0.536

2023-01-03 10:39:18,162 - ==> Confusion:
[[237 192   0]
 [ 79 540   0]
 [  0   0   0]]

2023-01-03 10:39:18,164 - ==> Best [Top1: 74.141   Sparsity:0.00   Params: 104480 on epoch: 36]
2023-01-03 10:39:18,164 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:18,172 - 

2023-01-03 10:39:18,173 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:19,422 - Epoch: [37][   10/   37]    Overall Loss 0.532386    Objective Loss 0.532386                                        LR 0.000060    Time 0.124775    
2023-01-03 10:39:20,397 - Epoch: [37][   20/   37]    Overall Loss 0.532511    Objective Loss 0.532511                                        LR 0.000060    Time 0.111094    
2023-01-03 10:39:21,338 - Epoch: [37][   30/   37]    Overall Loss 0.532361    Objective Loss 0.532361                                        LR 0.000060    Time 0.105421    
2023-01-03 10:39:21,941 - Epoch: [37][   37/   37]    Overall Loss 0.533195    Objective Loss 0.533195    Top1 72.594142    LR 0.000060    Time 0.101762    
2023-01-03 10:39:21,983 - --- validate (epoch=37)-----------
2023-01-03 10:39:21,984 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:22,367 - Epoch: [37][    5/    5]    Loss 0.495665    Top1 75.000000    
2023-01-03 10:39:22,405 - ==> Top1: 75.000    Loss: 0.496

2023-01-03 10:39:22,405 - ==> Confusion:
[[248 181   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-03 10:39:22,407 - ==> Best [Top1: 75.000   Sparsity:0.00   Params: 104480 on epoch: 37]
2023-01-03 10:39:22,408 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:22,415 - 

2023-01-03 10:39:22,416 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:23,599 - Epoch: [38][   10/   37]    Overall Loss 0.534469    Objective Loss 0.534469                                        LR 0.000060    Time 0.118190    
2023-01-03 10:39:24,550 - Epoch: [38][   20/   37]    Overall Loss 0.530503    Objective Loss 0.530503                                        LR 0.000060    Time 0.106611    
2023-01-03 10:39:25,521 - Epoch: [38][   30/   37]    Overall Loss 0.525882    Objective Loss 0.525882                                        LR 0.000060    Time 0.103395    
2023-01-03 10:39:26,107 - Epoch: [38][   37/   37]    Overall Loss 0.523681    Objective Loss 0.523681    Top1 73.849372    LR 0.000060    Time 0.099675    
2023-01-03 10:39:26,146 - --- validate (epoch=38)-----------
2023-01-03 10:39:26,146 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:26,539 - Epoch: [38][    5/    5]    Loss 0.524724    Top1 75.000000    
2023-01-03 10:39:26,575 - ==> Top1: 75.000    Loss: 0.525

2023-01-03 10:39:26,576 - ==> Confusion:
[[241 188   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-03 10:39:26,578 - ==> Best [Top1: 75.000   Sparsity:0.00   Params: 104480 on epoch: 38]
2023-01-03 10:39:26,579 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:26,587 - 

2023-01-03 10:39:26,588 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:27,930 - Epoch: [39][   10/   37]    Overall Loss 0.515623    Objective Loss 0.515623                                        LR 0.000060    Time 0.134073    
2023-01-03 10:39:28,909 - Epoch: [39][   20/   37]    Overall Loss 0.523588    Objective Loss 0.523588                                        LR 0.000060    Time 0.115920    
2023-01-03 10:39:29,879 - Epoch: [39][   30/   37]    Overall Loss 0.523116    Objective Loss 0.523116                                        LR 0.000060    Time 0.109609    
2023-01-03 10:39:30,487 - Epoch: [39][   37/   37]    Overall Loss 0.517683    Objective Loss 0.517683    Top1 77.824268    LR 0.000060    Time 0.105301    
2023-01-03 10:39:30,531 - --- validate (epoch=39)-----------
2023-01-03 10:39:30,532 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:30,939 - Epoch: [39][    5/    5]    Loss 0.505665    Top1 76.240458    
2023-01-03 10:39:30,975 - ==> Top1: 76.240    Loss: 0.506

2023-01-03 10:39:30,976 - ==> Confusion:
[[266 163   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-03 10:39:30,978 - ==> Best [Top1: 76.240   Sparsity:0.00   Params: 104480 on epoch: 39]
2023-01-03 10:39:30,978 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:30,987 - 

2023-01-03 10:39:30,987 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:32,230 - Epoch: [40][   10/   37]    Overall Loss 0.508676    Objective Loss 0.508676                                        LR 0.000036    Time 0.124193    
2023-01-03 10:39:33,206 - Epoch: [40][   20/   37]    Overall Loss 0.515653    Objective Loss 0.515653                                        LR 0.000036    Time 0.110859    
2023-01-03 10:39:34,177 - Epoch: [40][   30/   37]    Overall Loss 0.514905    Objective Loss 0.514905                                        LR 0.000036    Time 0.106252    
2023-01-03 10:39:34,774 - Epoch: [40][   37/   37]    Overall Loss 0.509421    Objective Loss 0.509421    Top1 74.476987    LR 0.000036    Time 0.102278    
2023-01-03 10:39:34,814 - --- validate (epoch=40)-----------
2023-01-03 10:39:34,814 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:35,192 - Epoch: [40][    5/    5]    Loss 0.484793    Top1 74.809160    
2023-01-03 10:39:35,233 - ==> Top1: 74.809    Loss: 0.485

2023-01-03 10:39:35,234 - ==> Confusion:
[[230 199   0]
 [ 65 554   0]
 [  0   0   0]]

2023-01-03 10:39:35,236 - ==> Best [Top1: 76.240   Sparsity:0.00   Params: 104480 on epoch: 39]
2023-01-03 10:39:35,236 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:35,242 - 

2023-01-03 10:39:35,243 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:36,509 - Epoch: [41][   10/   37]    Overall Loss 0.499804    Objective Loss 0.499804                                        LR 0.000036    Time 0.126513    
2023-01-03 10:39:37,471 - Epoch: [41][   20/   37]    Overall Loss 0.504718    Objective Loss 0.504718                                        LR 0.000036    Time 0.111296    
2023-01-03 10:39:38,419 - Epoch: [41][   30/   37]    Overall Loss 0.502162    Objective Loss 0.502162                                        LR 0.000036    Time 0.105779    
2023-01-03 10:39:38,998 - Epoch: [41][   37/   37]    Overall Loss 0.503406    Objective Loss 0.503406    Top1 74.058577    LR 0.000036    Time 0.101419    
2023-01-03 10:39:39,041 - --- validate (epoch=41)-----------
2023-01-03 10:39:39,041 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:39,457 - Epoch: [41][    5/    5]    Loss 0.476602    Top1 77.480916    
2023-01-03 10:39:39,499 - ==> Top1: 77.481    Loss: 0.477

2023-01-03 10:39:39,500 - ==> Confusion:
[[312 117   0]
 [119 500   0]
 [  0   0   0]]

2023-01-03 10:39:39,502 - ==> Best [Top1: 77.481   Sparsity:0.00   Params: 104480 on epoch: 41]
2023-01-03 10:39:39,502 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:39,511 - 

2023-01-03 10:39:39,511 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:40,741 - Epoch: [42][   10/   37]    Overall Loss 0.504912    Objective Loss 0.504912                                        LR 0.000036    Time 0.122790    
2023-01-03 10:39:41,698 - Epoch: [42][   20/   37]    Overall Loss 0.506265    Objective Loss 0.506265                                        LR 0.000036    Time 0.109215    
2023-01-03 10:39:42,627 - Epoch: [42][   30/   37]    Overall Loss 0.500412    Objective Loss 0.500412                                        LR 0.000036    Time 0.103766    
2023-01-03 10:39:43,217 - Epoch: [42][   37/   37]    Overall Loss 0.499156    Objective Loss 0.499156    Top1 75.523013    LR 0.000036    Time 0.100066    
2023-01-03 10:39:43,263 - --- validate (epoch=42)-----------
2023-01-03 10:39:43,263 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:43,668 - Epoch: [42][    5/    5]    Loss 0.479596    Top1 76.526718    
2023-01-03 10:39:43,700 - ==> Top1: 76.527    Loss: 0.480

2023-01-03 10:39:43,701 - ==> Confusion:
[[349  80   0]
 [166 453   0]
 [  0   0   0]]

2023-01-03 10:39:43,703 - ==> Best [Top1: 77.481   Sparsity:0.00   Params: 104480 on epoch: 41]
2023-01-03 10:39:43,703 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:43,710 - 

2023-01-03 10:39:43,710 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:44,898 - Epoch: [43][   10/   37]    Overall Loss 0.495477    Objective Loss 0.495477                                        LR 0.000036    Time 0.118622    
2023-01-03 10:39:45,860 - Epoch: [43][   20/   37]    Overall Loss 0.495958    Objective Loss 0.495958                                        LR 0.000036    Time 0.107396    
2023-01-03 10:39:46,830 - Epoch: [43][   30/   37]    Overall Loss 0.497238    Objective Loss 0.497238                                        LR 0.000036    Time 0.103886    
2023-01-03 10:39:47,425 - Epoch: [43][   37/   37]    Overall Loss 0.496224    Objective Loss 0.496224    Top1 77.196653    LR 0.000036    Time 0.100312    
2023-01-03 10:39:47,469 - --- validate (epoch=43)-----------
2023-01-03 10:39:47,469 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:47,875 - Epoch: [43][    5/    5]    Loss 0.466248    Top1 78.244275    
2023-01-03 10:39:47,912 - ==> Top1: 78.244    Loss: 0.466

2023-01-03 10:39:47,913 - ==> Confusion:
[[335  94   0]
 [134 485   0]
 [  0   0   0]]

2023-01-03 10:39:47,914 - ==> Best [Top1: 78.244   Sparsity:0.00   Params: 104480 on epoch: 43]
2023-01-03 10:39:47,914 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:47,923 - 

2023-01-03 10:39:47,923 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:49,163 - Epoch: [44][   10/   37]    Overall Loss 0.487476    Objective Loss 0.487476                                        LR 0.000036    Time 0.123799    
2023-01-03 10:39:50,113 - Epoch: [44][   20/   37]    Overall Loss 0.494470    Objective Loss 0.494470                                        LR 0.000036    Time 0.109383    
2023-01-03 10:39:51,078 - Epoch: [44][   30/   37]    Overall Loss 0.487387    Objective Loss 0.487387                                        LR 0.000036    Time 0.105062    
2023-01-03 10:39:51,693 - Epoch: [44][   37/   37]    Overall Loss 0.490121    Objective Loss 0.490121    Top1 74.686192    LR 0.000036    Time 0.101793    
2023-01-03 10:39:51,736 - --- validate (epoch=44)-----------
2023-01-03 10:39:51,736 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:52,124 - Epoch: [44][    5/    5]    Loss 0.454390    Top1 78.244275    
2023-01-03 10:39:52,159 - ==> Top1: 78.244    Loss: 0.454

2023-01-03 10:39:52,159 - ==> Confusion:
[[339  90   0]
 [138 481   0]
 [  0   0   0]]

2023-01-03 10:39:52,161 - ==> Best [Top1: 78.244   Sparsity:0.00   Params: 104480 on epoch: 44]
2023-01-03 10:39:52,161 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:52,171 - 

2023-01-03 10:39:52,171 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:53,412 - Epoch: [45][   10/   37]    Overall Loss 0.498464    Objective Loss 0.498464                                        LR 0.000036    Time 0.123912    
2023-01-03 10:39:54,416 - Epoch: [45][   20/   37]    Overall Loss 0.490081    Objective Loss 0.490081                                        LR 0.000036    Time 0.112109    
2023-01-03 10:39:55,369 - Epoch: [45][   30/   37]    Overall Loss 0.486533    Objective Loss 0.486533                                        LR 0.000036    Time 0.106507    
2023-01-03 10:39:55,956 - Epoch: [45][   37/   37]    Overall Loss 0.484870    Objective Loss 0.484870    Top1 76.778243    LR 0.000036    Time 0.102194    
2023-01-03 10:39:55,996 - --- validate (epoch=45)-----------
2023-01-03 10:39:55,996 - 1048 samples (256 per mini-batch)
2023-01-03 10:39:56,400 - Epoch: [45][    5/    5]    Loss 0.490189    Top1 75.000000    
2023-01-03 10:39:56,439 - ==> Top1: 75.000    Loss: 0.490

2023-01-03 10:39:56,439 - ==> Confusion:
[[213 216   0]
 [ 46 573   0]
 [  0   0   0]]

2023-01-03 10:39:56,442 - ==> Best [Top1: 78.244   Sparsity:0.00   Params: 104480 on epoch: 44]
2023-01-03 10:39:56,442 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:39:56,449 - 

2023-01-03 10:39:56,449 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:39:57,692 - Epoch: [46][   10/   37]    Overall Loss 0.488725    Objective Loss 0.488725                                        LR 0.000036    Time 0.124052    
2023-01-03 10:39:58,650 - Epoch: [46][   20/   37]    Overall Loss 0.488877    Objective Loss 0.488877                                        LR 0.000036    Time 0.109911    
2023-01-03 10:39:59,602 - Epoch: [46][   30/   37]    Overall Loss 0.487563    Objective Loss 0.487563                                        LR 0.000036    Time 0.104994    
2023-01-03 10:40:00,196 - Epoch: [46][   37/   37]    Overall Loss 0.487032    Objective Loss 0.487032    Top1 73.849372    LR 0.000036    Time 0.101156    
2023-01-03 10:40:00,237 - --- validate (epoch=46)-----------
2023-01-03 10:40:00,237 - 1048 samples (256 per mini-batch)
2023-01-03 10:40:00,637 - Epoch: [46][    5/    5]    Loss 0.484019    Top1 78.339695    
2023-01-03 10:40:00,672 - ==> Top1: 78.340    Loss: 0.484

2023-01-03 10:40:00,673 - ==> Confusion:
[[338  91   0]
 [136 483   0]
 [  0   0   0]]

2023-01-03 10:40:00,675 - ==> Best [Top1: 78.340   Sparsity:0.00   Params: 104480 on epoch: 46]
2023-01-03 10:40:00,675 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:40:00,684 - 

2023-01-03 10:40:00,684 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:40:01,910 - Epoch: [47][   10/   37]    Overall Loss 0.485533    Objective Loss 0.485533                                        LR 0.000036    Time 0.122413    
2023-01-03 10:40:02,875 - Epoch: [47][   20/   37]    Overall Loss 0.483344    Objective Loss 0.483344                                        LR 0.000036    Time 0.109450    
2023-01-03 10:40:03,834 - Epoch: [47][   30/   37]    Overall Loss 0.481689    Objective Loss 0.481689                                        LR 0.000036    Time 0.104927    
2023-01-03 10:40:04,439 - Epoch: [47][   37/   37]    Overall Loss 0.478659    Objective Loss 0.478659    Top1 75.941423    LR 0.000036    Time 0.101415    
2023-01-03 10:40:04,481 - --- validate (epoch=47)-----------
2023-01-03 10:40:04,481 - 1048 samples (256 per mini-batch)
2023-01-03 10:40:04,870 - Epoch: [47][    5/    5]    Loss 0.438126    Top1 78.435115    
2023-01-03 10:40:04,904 - ==> Top1: 78.435    Loss: 0.438

2023-01-03 10:40:04,905 - ==> Confusion:
[[348  81   0]
 [145 474   0]
 [  0   0   0]]

2023-01-03 10:40:04,907 - ==> Best [Top1: 78.435   Sparsity:0.00   Params: 104480 on epoch: 47]
2023-01-03 10:40:04,907 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:40:04,915 - 

2023-01-03 10:40:04,915 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:40:06,146 - Epoch: [48][   10/   37]    Overall Loss 0.475670    Objective Loss 0.475670                                        LR 0.000036    Time 0.122987    
2023-01-03 10:40:07,116 - Epoch: [48][   20/   37]    Overall Loss 0.475228    Objective Loss 0.475228                                        LR 0.000036    Time 0.109956    
2023-01-03 10:40:08,094 - Epoch: [48][   30/   37]    Overall Loss 0.476625    Objective Loss 0.476625                                        LR 0.000036    Time 0.105889    
2023-01-03 10:40:08,706 - Epoch: [48][   37/   37]    Overall Loss 0.475807    Objective Loss 0.475807    Top1 76.569038    LR 0.000036    Time 0.102391    
2023-01-03 10:40:08,750 - --- validate (epoch=48)-----------
2023-01-03 10:40:08,750 - 1048 samples (256 per mini-batch)
2023-01-03 10:40:09,136 - Epoch: [48][    5/    5]    Loss 0.459216    Top1 77.958015    
2023-01-03 10:40:09,172 - ==> Top1: 77.958    Loss: 0.459

2023-01-03 10:40:09,172 - ==> Confusion:
[[299 130   0]
 [101 518   0]
 [  0   0   0]]

2023-01-03 10:40:09,175 - ==> Best [Top1: 78.435   Sparsity:0.00   Params: 104480 on epoch: 47]
2023-01-03 10:40:09,175 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:40:09,181 - 

2023-01-03 10:40:09,181 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-03 10:40:10,403 - Epoch: [49][   10/   37]    Overall Loss 0.488885    Objective Loss 0.488885                                        LR 0.000036    Time 0.122095    
2023-01-03 10:40:11,382 - Epoch: [49][   20/   37]    Overall Loss 0.473726    Objective Loss 0.473726                                        LR 0.000036    Time 0.109960    
2023-01-03 10:40:12,327 - Epoch: [49][   30/   37]    Overall Loss 0.469450    Objective Loss 0.469450                                        LR 0.000036    Time 0.104773    
2023-01-03 10:40:12,947 - Epoch: [49][   37/   37]    Overall Loss 0.472532    Objective Loss 0.472532    Top1 78.242678    LR 0.000036    Time 0.101687    
2023-01-03 10:40:12,984 - --- validate (epoch=49)-----------
2023-01-03 10:40:12,985 - 1048 samples (256 per mini-batch)
2023-01-03 10:40:13,372 - Epoch: [49][    5/    5]    Loss 0.455571    Top1 79.389313    
2023-01-03 10:40:13,409 - ==> Top1: 79.389    Loss: 0.456

2023-01-03 10:40:13,409 - ==> Confusion:
[[335  94   0]
 [122 497   0]
 [  0   0   0]]

2023-01-03 10:40:13,411 - ==> Best [Top1: 79.389   Sparsity:0.00   Params: 104480 on epoch: 49]
2023-01-03 10:40:13,412 - Saving checkpoint to: logs/2023.01.03-103635/qat_checkpoint.pth.tar
2023-01-03 10:40:13,421 - --- test ---------------------
2023-01-03 10:40:13,421 - 1317 samples (256 per mini-batch)
2023-01-03 10:40:13,867 - Test: [    6/    6]    Loss 0.483033    Top1 75.322703    
2023-01-03 10:40:13,900 - ==> Top1: 75.323    Loss: 0.483

2023-01-03 10:40:13,900 - ==> Confusion:
[[431 130   0]
 [195 561   0]
 [  0   0   0]]

2023-01-03 10:40:13,921 - 
2023-01-03 10:40:13,921 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.03-103635/2023.01.03-103635.log
