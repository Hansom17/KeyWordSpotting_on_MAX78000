2023-01-06 14:26:39,752 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-142639/2023.01.06-142639.log
2023-01-06 14:26:41,819 - => loading checkpoint v2mfccpretrained.pth.tar
2023-01-06 14:26:41,824 - => Checkpoint contents:
+----------------------+-------------+----------------+
| Key                  | Type        | Value          |
|----------------------+-------------+----------------|
| arch                 | str         | ai85kws20netv2 |
| compression_sched    | dict        |                |
| epoch                | int         | 161            |
| extras               | dict        |                |
| optimizer_state_dict | dict        |                |
| optimizer_type       | type        | Adam           |
| state_dict           | OrderedDict |                |
+----------------------+-------------+----------------+

2023-01-06 14:26:41,824 - => Checkpoint['extras'] contents:
+--------------+--------+----------+
| Key          | Type   |    Value |
|--------------+--------+----------|
| best_epoch   | int    | 161      |
| best_mAP     | int    |   0      |
| best_top1    | float  |  85.2099 |
| current_mAP  | int    |   0      |
| current_top1 | float  |  85.2099 |
+--------------+--------+----------+

2023-01-06 14:26:41,825 - Loaded compression schedule from checkpoint (epoch 161)
2023-01-06 14:26:41,828 - => loaded 'state_dict' from checkpoint 'v2mfccpretrained.pth.tar'
2023-01-06 14:26:41,837 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 14:26:41,837 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 14:27:33,109 - Dataset sizes:
	training=62882
	validation=6986
	test=13117
2023-01-06 14:27:33,109 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 14:27:33,112 - 

2023-01-06 14:27:33,113 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:27:34,095 - Epoch: [0][   10/  246]    Overall Loss 8.557665    Objective Loss 8.557665                                        LR 0.000060    Time 0.098208    
2023-01-06 14:27:34,343 - Epoch: [0][   20/  246]    Overall Loss 5.575372    Objective Loss 5.575372                                        LR 0.000060    Time 0.061471    
2023-01-06 14:27:34,595 - Epoch: [0][   30/  246]    Overall Loss 4.258317    Objective Loss 4.258317                                        LR 0.000060    Time 0.049370    
2023-01-06 14:27:34,844 - Epoch: [0][   40/  246]    Overall Loss 3.492027    Objective Loss 3.492027                                        LR 0.000060    Time 0.043240    
2023-01-06 14:27:35,091 - Epoch: [0][   50/  246]    Overall Loss 2.980502    Objective Loss 2.980502                                        LR 0.000060    Time 0.039503    
2023-01-06 14:27:35,334 - Epoch: [0][   60/  246]    Overall Loss 2.610481    Objective Loss 2.610481                                        LR 0.000060    Time 0.036953    
2023-01-06 14:27:35,587 - Epoch: [0][   70/  246]    Overall Loss 2.331205    Objective Loss 2.331205                                        LR 0.000060    Time 0.035279    
2023-01-06 14:27:35,839 - Epoch: [0][   80/  246]    Overall Loss 2.111423    Objective Loss 2.111423                                        LR 0.000060    Time 0.034010    
2023-01-06 14:27:36,080 - Epoch: [0][   90/  246]    Overall Loss 1.935652    Objective Loss 1.935652                                        LR 0.000060    Time 0.032914    
2023-01-06 14:27:36,328 - Epoch: [0][  100/  246]    Overall Loss 1.795918    Objective Loss 1.795918                                        LR 0.000060    Time 0.032089    
2023-01-06 14:27:36,581 - Epoch: [0][  110/  246]    Overall Loss 1.680269    Objective Loss 1.680269                                        LR 0.000060    Time 0.031475    
2023-01-06 14:27:36,828 - Epoch: [0][  120/  246]    Overall Loss 1.583521    Objective Loss 1.583521                                        LR 0.000060    Time 0.030907    
2023-01-06 14:27:37,081 - Epoch: [0][  130/  246]    Overall Loss 1.504394    Objective Loss 1.504394                                        LR 0.000060    Time 0.030474    
2023-01-06 14:27:37,327 - Epoch: [0][  140/  246]    Overall Loss 1.433416    Objective Loss 1.433416                                        LR 0.000060    Time 0.030052    
2023-01-06 14:27:37,577 - Epoch: [0][  150/  246]    Overall Loss 1.373614    Objective Loss 1.373614                                        LR 0.000060    Time 0.029707    
2023-01-06 14:27:37,824 - Epoch: [0][  160/  246]    Overall Loss 1.320884    Objective Loss 1.320884                                        LR 0.000060    Time 0.029391    
2023-01-06 14:27:38,064 - Epoch: [0][  170/  246]    Overall Loss 1.275398    Objective Loss 1.275398                                        LR 0.000060    Time 0.029077    
2023-01-06 14:27:38,314 - Epoch: [0][  180/  246]    Overall Loss 1.235352    Objective Loss 1.235352                                        LR 0.000060    Time 0.028845    
2023-01-06 14:27:38,561 - Epoch: [0][  190/  246]    Overall Loss 1.197953    Objective Loss 1.197953                                        LR 0.000060    Time 0.028623    
2023-01-06 14:27:38,809 - Epoch: [0][  200/  246]    Overall Loss 1.163204    Objective Loss 1.163204                                        LR 0.000060    Time 0.028434    
2023-01-06 14:27:39,056 - Epoch: [0][  210/  246]    Overall Loss 1.133957    Objective Loss 1.133957                                        LR 0.000060    Time 0.028252    
2023-01-06 14:27:39,304 - Epoch: [0][  220/  246]    Overall Loss 1.106211    Objective Loss 1.106211                                        LR 0.000060    Time 0.028097    
2023-01-06 14:27:39,551 - Epoch: [0][  230/  246]    Overall Loss 1.080058    Objective Loss 1.080058                                        LR 0.000060    Time 0.027944    
2023-01-06 14:27:39,800 - Epoch: [0][  240/  246]    Overall Loss 1.056347    Objective Loss 1.056347                                        LR 0.000060    Time 0.027818    
2023-01-06 14:27:39,929 - Epoch: [0][  246/  246]    Overall Loss 1.042663    Objective Loss 1.042663    Top1 84.449761    LR 0.000060    Time 0.027663    
2023-01-06 14:27:40,054 - --- validate (epoch=0)-----------
2023-01-06 14:27:40,055 - 6986 samples (256 per mini-batch)
2023-01-06 14:27:40,489 - Epoch: [0][   10/   28]    Loss 0.490462    Top1 86.367188    
2023-01-06 14:27:40,636 - Epoch: [0][   20/   28]    Loss 0.519174    Top1 85.097656    
2023-01-06 14:27:40,729 - Epoch: [0][   28/   28]    Loss 0.514054    Top1 85.098769    
2023-01-06 14:27:40,862 - ==> Top1: 85.099    Loss: 0.514

2023-01-06 14:27:40,862 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 14:27:40,864 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 360896 on epoch: 0]
2023-01-06 14:27:40,864 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:27:40,875 - 

2023-01-06 14:27:40,875 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:27:41,454 - Epoch: [1][   10/  246]    Overall Loss 0.521518    Objective Loss 0.521518                                        LR 0.000060    Time 0.057748    
2023-01-06 14:27:41,709 - Epoch: [1][   20/  246]    Overall Loss 0.513539    Objective Loss 0.513539                                        LR 0.000060    Time 0.041607    
2023-01-06 14:27:41,966 - Epoch: [1][   30/  246]    Overall Loss 0.514442    Objective Loss 0.514442                                        LR 0.000060    Time 0.036285    
2023-01-06 14:27:42,208 - Epoch: [1][   40/  246]    Overall Loss 0.516860    Objective Loss 0.516860                                        LR 0.000060    Time 0.033276    
2023-01-06 14:27:42,454 - Epoch: [1][   50/  246]    Overall Loss 0.519202    Objective Loss 0.519202                                        LR 0.000060    Time 0.031526    
2023-01-06 14:27:42,698 - Epoch: [1][   60/  246]    Overall Loss 0.522047    Objective Loss 0.522047                                        LR 0.000060    Time 0.030334    
2023-01-06 14:27:42,942 - Epoch: [1][   70/  246]    Overall Loss 0.520429    Objective Loss 0.520429                                        LR 0.000060    Time 0.029469    
2023-01-06 14:27:43,185 - Epoch: [1][   80/  246]    Overall Loss 0.517962    Objective Loss 0.517962                                        LR 0.000060    Time 0.028816    
2023-01-06 14:27:43,428 - Epoch: [1][   90/  246]    Overall Loss 0.515265    Objective Loss 0.515265                                        LR 0.000060    Time 0.028318    
2023-01-06 14:27:43,668 - Epoch: [1][  100/  246]    Overall Loss 0.515500    Objective Loss 0.515500                                        LR 0.000060    Time 0.027876    
2023-01-06 14:27:43,911 - Epoch: [1][  110/  246]    Overall Loss 0.516637    Objective Loss 0.516637                                        LR 0.000060    Time 0.027552    
2023-01-06 14:27:44,153 - Epoch: [1][  120/  246]    Overall Loss 0.516640    Objective Loss 0.516640                                        LR 0.000060    Time 0.027269    
2023-01-06 14:27:44,397 - Epoch: [1][  130/  246]    Overall Loss 0.518177    Objective Loss 0.518177                                        LR 0.000060    Time 0.027042    
2023-01-06 14:27:44,636 - Epoch: [1][  140/  246]    Overall Loss 0.517634    Objective Loss 0.517634                                        LR 0.000060    Time 0.026814    
2023-01-06 14:27:44,880 - Epoch: [1][  150/  246]    Overall Loss 0.516069    Objective Loss 0.516069                                        LR 0.000060    Time 0.026651    
2023-01-06 14:27:45,111 - Epoch: [1][  160/  246]    Overall Loss 0.516289    Objective Loss 0.516289                                        LR 0.000060    Time 0.026425    
2023-01-06 14:27:45,347 - Epoch: [1][  170/  246]    Overall Loss 0.515716    Objective Loss 0.515716                                        LR 0.000060    Time 0.026257    
2023-01-06 14:27:45,584 - Epoch: [1][  180/  246]    Overall Loss 0.516301    Objective Loss 0.516301                                        LR 0.000060    Time 0.026114    
2023-01-06 14:27:45,819 - Epoch: [1][  190/  246]    Overall Loss 0.514293    Objective Loss 0.514293                                        LR 0.000060    Time 0.025975    
2023-01-06 14:27:46,055 - Epoch: [1][  200/  246]    Overall Loss 0.512137    Objective Loss 0.512137                                        LR 0.000060    Time 0.025853    
2023-01-06 14:27:46,292 - Epoch: [1][  210/  246]    Overall Loss 0.512709    Objective Loss 0.512709                                        LR 0.000060    Time 0.025747    
2023-01-06 14:27:46,532 - Epoch: [1][  220/  246]    Overall Loss 0.512298    Objective Loss 0.512298                                        LR 0.000060    Time 0.025667    
2023-01-06 14:27:46,781 - Epoch: [1][  230/  246]    Overall Loss 0.511242    Objective Loss 0.511242                                        LR 0.000060    Time 0.025631    
2023-01-06 14:27:47,033 - Epoch: [1][  240/  246]    Overall Loss 0.510456    Objective Loss 0.510456                                        LR 0.000060    Time 0.025613    
2023-01-06 14:27:47,163 - Epoch: [1][  246/  246]    Overall Loss 0.509653    Objective Loss 0.509653    Top1 85.645933    LR 0.000060    Time 0.025514    
2023-01-06 14:27:47,271 - --- validate (epoch=1)-----------
2023-01-06 14:27:47,271 - 6986 samples (256 per mini-batch)
2023-01-06 14:27:47,698 - Epoch: [1][   10/   28]    Loss 0.479598    Top1 85.468750    
2023-01-06 14:27:47,833 - Epoch: [1][   20/   28]    Loss 0.489860    Top1 85.019531    
2023-01-06 14:27:47,922 - Epoch: [1][   28/   28]    Loss 0.486655    Top1 85.098769    
2023-01-06 14:27:48,060 - ==> Top1: 85.099    Loss: 0.487

2023-01-06 14:27:48,061 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 14:27:48,062 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 360896 on epoch: 1]
2023-01-06 14:27:48,062 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:27:48,084 - 

2023-01-06 14:27:48,084 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:27:48,796 - Epoch: [2][   10/  246]    Overall Loss 0.468990    Objective Loss 0.468990                                        LR 0.000060    Time 0.071033    
2023-01-06 14:27:49,042 - Epoch: [2][   20/  246]    Overall Loss 0.474880    Objective Loss 0.474880                                        LR 0.000060    Time 0.047795    
2023-01-06 14:27:49,269 - Epoch: [2][   30/  246]    Overall Loss 0.480275    Objective Loss 0.480275                                        LR 0.000060    Time 0.039420    
2023-01-06 14:27:49,512 - Epoch: [2][   40/  246]    Overall Loss 0.485495    Objective Loss 0.485495                                        LR 0.000060    Time 0.035639    
2023-01-06 14:27:49,755 - Epoch: [2][   50/  246]    Overall Loss 0.486087    Objective Loss 0.486087                                        LR 0.000060    Time 0.033362    
2023-01-06 14:27:50,002 - Epoch: [2][   60/  246]    Overall Loss 0.481607    Objective Loss 0.481607                                        LR 0.000060    Time 0.031915    
2023-01-06 14:27:50,242 - Epoch: [2][   70/  246]    Overall Loss 0.480335    Objective Loss 0.480335                                        LR 0.000060    Time 0.030780    
2023-01-06 14:27:50,487 - Epoch: [2][   80/  246]    Overall Loss 0.477577    Objective Loss 0.477577                                        LR 0.000060    Time 0.029988    
2023-01-06 14:27:50,724 - Epoch: [2][   90/  246]    Overall Loss 0.477652    Objective Loss 0.477652                                        LR 0.000060    Time 0.029285    
2023-01-06 14:27:50,962 - Epoch: [2][  100/  246]    Overall Loss 0.474504    Objective Loss 0.474504                                        LR 0.000060    Time 0.028740    
2023-01-06 14:27:51,199 - Epoch: [2][  110/  246]    Overall Loss 0.475162    Objective Loss 0.475162                                        LR 0.000060    Time 0.028273    
2023-01-06 14:27:51,441 - Epoch: [2][  120/  246]    Overall Loss 0.474511    Objective Loss 0.474511                                        LR 0.000060    Time 0.027936    
2023-01-06 14:27:51,681 - Epoch: [2][  130/  246]    Overall Loss 0.472269    Objective Loss 0.472269                                        LR 0.000060    Time 0.027633    
2023-01-06 14:27:51,919 - Epoch: [2][  140/  246]    Overall Loss 0.470544    Objective Loss 0.470544                                        LR 0.000060    Time 0.027355    
2023-01-06 14:27:52,159 - Epoch: [2][  150/  246]    Overall Loss 0.468379    Objective Loss 0.468379                                        LR 0.000060    Time 0.027131    
2023-01-06 14:27:52,401 - Epoch: [2][  160/  246]    Overall Loss 0.467148    Objective Loss 0.467148                                        LR 0.000060    Time 0.026948    
2023-01-06 14:27:52,642 - Epoch: [2][  170/  246]    Overall Loss 0.465360    Objective Loss 0.465360                                        LR 0.000060    Time 0.026779    
2023-01-06 14:27:52,884 - Epoch: [2][  180/  246]    Overall Loss 0.464279    Objective Loss 0.464279                                        LR 0.000060    Time 0.026634    
2023-01-06 14:27:53,125 - Epoch: [2][  190/  246]    Overall Loss 0.463842    Objective Loss 0.463842                                        LR 0.000060    Time 0.026498    
2023-01-06 14:27:53,364 - Epoch: [2][  200/  246]    Overall Loss 0.463131    Objective Loss 0.463131                                        LR 0.000060    Time 0.026367    
2023-01-06 14:27:53,609 - Epoch: [2][  210/  246]    Overall Loss 0.464153    Objective Loss 0.464153                                        LR 0.000060    Time 0.026274    
2023-01-06 14:27:53,851 - Epoch: [2][  220/  246]    Overall Loss 0.462428    Objective Loss 0.462428                                        LR 0.000060    Time 0.026180    
2023-01-06 14:27:54,095 - Epoch: [2][  230/  246]    Overall Loss 0.461465    Objective Loss 0.461465                                        LR 0.000060    Time 0.026102    
2023-01-06 14:27:54,348 - Epoch: [2][  240/  246]    Overall Loss 0.461016    Objective Loss 0.461016                                        LR 0.000060    Time 0.026066    
2023-01-06 14:27:54,477 - Epoch: [2][  246/  246]    Overall Loss 0.460013    Objective Loss 0.460013    Top1 86.124402    LR 0.000060    Time 0.025956    
2023-01-06 14:27:54,600 - --- validate (epoch=2)-----------
2023-01-06 14:27:54,600 - 6986 samples (256 per mini-batch)
2023-01-06 14:27:55,071 - Epoch: [2][   10/   28]    Loss 0.448275    Top1 84.648438    
2023-01-06 14:27:55,225 - Epoch: [2][   20/   28]    Loss 0.438107    Top1 85.000000    
2023-01-06 14:27:55,315 - Epoch: [2][   28/   28]    Loss 0.438942    Top1 85.156026    
2023-01-06 14:27:55,427 - ==> Top1: 85.156    Loss: 0.439

2023-01-06 14:27:55,428 - ==> Confusion:
[[  15    1  423]
 [   1    3  598]
 [  12    2 5931]]

2023-01-06 14:27:55,429 - ==> Best [Top1: 85.156   Sparsity:0.00   Params: 360896 on epoch: 2]
2023-01-06 14:27:55,429 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:27:55,452 - 

2023-01-06 14:27:55,452 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:27:56,029 - Epoch: [3][   10/  246]    Overall Loss 0.433665    Objective Loss 0.433665                                        LR 0.000060    Time 0.057596    
2023-01-06 14:27:56,279 - Epoch: [3][   20/  246]    Overall Loss 0.442024    Objective Loss 0.442024                                        LR 0.000060    Time 0.041215    
2023-01-06 14:27:56,529 - Epoch: [3][   30/  246]    Overall Loss 0.440672    Objective Loss 0.440672                                        LR 0.000060    Time 0.035801    
2023-01-06 14:27:56,787 - Epoch: [3][   40/  246]    Overall Loss 0.437862    Objective Loss 0.437862                                        LR 0.000060    Time 0.033290    
2023-01-06 14:27:57,047 - Epoch: [3][   50/  246]    Overall Loss 0.437786    Objective Loss 0.437786                                        LR 0.000060    Time 0.031826    
2023-01-06 14:27:57,309 - Epoch: [3][   60/  246]    Overall Loss 0.437738    Objective Loss 0.437738                                        LR 0.000060    Time 0.030886    
2023-01-06 14:27:57,565 - Epoch: [3][   70/  246]    Overall Loss 0.436091    Objective Loss 0.436091                                        LR 0.000060    Time 0.030125    
2023-01-06 14:27:57,824 - Epoch: [3][   80/  246]    Overall Loss 0.433063    Objective Loss 0.433063                                        LR 0.000060    Time 0.029583    
2023-01-06 14:27:58,088 - Epoch: [3][   90/  246]    Overall Loss 0.433103    Objective Loss 0.433103                                        LR 0.000060    Time 0.029232    
2023-01-06 14:27:58,352 - Epoch: [3][  100/  246]    Overall Loss 0.432491    Objective Loss 0.432491                                        LR 0.000060    Time 0.028941    
2023-01-06 14:27:58,614 - Epoch: [3][  110/  246]    Overall Loss 0.433916    Objective Loss 0.433916                                        LR 0.000060    Time 0.028685    
2023-01-06 14:27:58,862 - Epoch: [3][  120/  246]    Overall Loss 0.434386    Objective Loss 0.434386                                        LR 0.000060    Time 0.028359    
2023-01-06 14:27:59,101 - Epoch: [3][  130/  246]    Overall Loss 0.433039    Objective Loss 0.433039                                        LR 0.000060    Time 0.028009    
2023-01-06 14:27:59,362 - Epoch: [3][  140/  246]    Overall Loss 0.433222    Objective Loss 0.433222                                        LR 0.000060    Time 0.027866    
2023-01-06 14:27:59,624 - Epoch: [3][  150/  246]    Overall Loss 0.430630    Objective Loss 0.430630                                        LR 0.000060    Time 0.027752    
2023-01-06 14:27:59,887 - Epoch: [3][  160/  246]    Overall Loss 0.430462    Objective Loss 0.430462                                        LR 0.000060    Time 0.027660    
2023-01-06 14:28:00,147 - Epoch: [3][  170/  246]    Overall Loss 0.429593    Objective Loss 0.429593                                        LR 0.000060    Time 0.027561    
2023-01-06 14:28:00,407 - Epoch: [3][  180/  246]    Overall Loss 0.428642    Objective Loss 0.428642                                        LR 0.000060    Time 0.027472    
2023-01-06 14:28:00,660 - Epoch: [3][  190/  246]    Overall Loss 0.430407    Objective Loss 0.430407                                        LR 0.000060    Time 0.027351    
2023-01-06 14:28:00,915 - Epoch: [3][  200/  246]    Overall Loss 0.430061    Objective Loss 0.430061                                        LR 0.000060    Time 0.027256    
2023-01-06 14:28:01,163 - Epoch: [3][  210/  246]    Overall Loss 0.429796    Objective Loss 0.429796                                        LR 0.000060    Time 0.027136    
2023-01-06 14:28:01,415 - Epoch: [3][  220/  246]    Overall Loss 0.429547    Objective Loss 0.429547                                        LR 0.000060    Time 0.027044    
2023-01-06 14:28:01,663 - Epoch: [3][  230/  246]    Overall Loss 0.428399    Objective Loss 0.428399                                        LR 0.000060    Time 0.026945    
2023-01-06 14:28:01,925 - Epoch: [3][  240/  246]    Overall Loss 0.427906    Objective Loss 0.427906                                        LR 0.000060    Time 0.026909    
2023-01-06 14:28:02,054 - Epoch: [3][  246/  246]    Overall Loss 0.427809    Objective Loss 0.427809    Top1 84.210526    LR 0.000060    Time 0.026776    
2023-01-06 14:28:02,172 - --- validate (epoch=3)-----------
2023-01-06 14:28:02,173 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:02,935 - Epoch: [3][   10/   28]    Loss 0.399811    Top1 86.367188    
2023-01-06 14:28:03,077 - Epoch: [3][   20/   28]    Loss 0.404667    Top1 86.035156    
2023-01-06 14:28:03,169 - Epoch: [3][   28/   28]    Loss 0.409396    Top1 85.757229    
2023-01-06 14:28:03,309 - ==> Top1: 85.757    Loss: 0.409

2023-01-06 14:28:03,309 - ==> Confusion:
[[  86    2  351]
 [   5   19  578]
 [  54    5 5886]]

2023-01-06 14:28:03,310 - ==> Best [Top1: 85.757   Sparsity:0.00   Params: 360896 on epoch: 3]
2023-01-06 14:28:03,310 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:03,332 - 

2023-01-06 14:28:03,332 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:03,940 - Epoch: [4][   10/  246]    Overall Loss 0.415469    Objective Loss 0.415469                                        LR 0.000060    Time 0.060641    
2023-01-06 14:28:04,188 - Epoch: [4][   20/  246]    Overall Loss 0.409948    Objective Loss 0.409948                                        LR 0.000060    Time 0.042697    
2023-01-06 14:28:04,430 - Epoch: [4][   30/  246]    Overall Loss 0.411368    Objective Loss 0.411368                                        LR 0.000060    Time 0.036545    
2023-01-06 14:28:04,674 - Epoch: [4][   40/  246]    Overall Loss 0.411523    Objective Loss 0.411523                                        LR 0.000060    Time 0.033458    
2023-01-06 14:28:04,908 - Epoch: [4][   50/  246]    Overall Loss 0.411830    Objective Loss 0.411830                                        LR 0.000060    Time 0.031450    
2023-01-06 14:28:05,147 - Epoch: [4][   60/  246]    Overall Loss 0.410975    Objective Loss 0.410975                                        LR 0.000060    Time 0.030159    
2023-01-06 14:28:05,385 - Epoch: [4][   70/  246]    Overall Loss 0.410988    Objective Loss 0.410988                                        LR 0.000060    Time 0.029249    
2023-01-06 14:28:05,622 - Epoch: [4][   80/  246]    Overall Loss 0.413098    Objective Loss 0.413098                                        LR 0.000060    Time 0.028538    
2023-01-06 14:28:05,865 - Epoch: [4][   90/  246]    Overall Loss 0.412084    Objective Loss 0.412084                                        LR 0.000060    Time 0.028064    
2023-01-06 14:28:06,108 - Epoch: [4][  100/  246]    Overall Loss 0.411205    Objective Loss 0.411205                                        LR 0.000060    Time 0.027675    
2023-01-06 14:28:06,347 - Epoch: [4][  110/  246]    Overall Loss 0.411476    Objective Loss 0.411476                                        LR 0.000060    Time 0.027321    
2023-01-06 14:28:06,591 - Epoch: [4][  120/  246]    Overall Loss 0.408432    Objective Loss 0.408432                                        LR 0.000060    Time 0.027075    
2023-01-06 14:28:06,832 - Epoch: [4][  130/  246]    Overall Loss 0.409983    Objective Loss 0.409983                                        LR 0.000060    Time 0.026844    
2023-01-06 14:28:07,086 - Epoch: [4][  140/  246]    Overall Loss 0.410089    Objective Loss 0.410089                                        LR 0.000060    Time 0.026727    
2023-01-06 14:28:07,338 - Epoch: [4][  150/  246]    Overall Loss 0.410615    Objective Loss 0.410615                                        LR 0.000060    Time 0.026627    
2023-01-06 14:28:07,598 - Epoch: [4][  160/  246]    Overall Loss 0.411862    Objective Loss 0.411862                                        LR 0.000060    Time 0.026580    
2023-01-06 14:28:07,851 - Epoch: [4][  170/  246]    Overall Loss 0.410132    Objective Loss 0.410132                                        LR 0.000060    Time 0.026507    
2023-01-06 14:28:08,105 - Epoch: [4][  180/  246]    Overall Loss 0.410400    Objective Loss 0.410400                                        LR 0.000060    Time 0.026438    
2023-01-06 14:28:08,351 - Epoch: [4][  190/  246]    Overall Loss 0.409814    Objective Loss 0.409814                                        LR 0.000060    Time 0.026339    
2023-01-06 14:28:08,596 - Epoch: [4][  200/  246]    Overall Loss 0.408616    Objective Loss 0.408616                                        LR 0.000060    Time 0.026245    
2023-01-06 14:28:08,842 - Epoch: [4][  210/  246]    Overall Loss 0.408552    Objective Loss 0.408552                                        LR 0.000060    Time 0.026162    
2023-01-06 14:28:09,089 - Epoch: [4][  220/  246]    Overall Loss 0.408836    Objective Loss 0.408836                                        LR 0.000060    Time 0.026095    
2023-01-06 14:28:09,343 - Epoch: [4][  230/  246]    Overall Loss 0.409306    Objective Loss 0.409306                                        LR 0.000060    Time 0.026061    
2023-01-06 14:28:09,605 - Epoch: [4][  240/  246]    Overall Loss 0.409764    Objective Loss 0.409764                                        LR 0.000060    Time 0.026065    
2023-01-06 14:28:09,732 - Epoch: [4][  246/  246]    Overall Loss 0.409715    Objective Loss 0.409715    Top1 85.645933    LR 0.000060    Time 0.025947    
2023-01-06 14:28:09,839 - --- validate (epoch=4)-----------
2023-01-06 14:28:09,840 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:10,276 - Epoch: [4][   10/   28]    Loss 0.417883    Top1 85.312500    
2023-01-06 14:28:10,413 - Epoch: [4][   20/   28]    Loss 0.412655    Top1 85.410156    
2023-01-06 14:28:10,503 - Epoch: [4][   28/   28]    Loss 0.399911    Top1 85.857429    
2023-01-06 14:28:10,644 - ==> Top1: 85.857    Loss: 0.400

2023-01-06 14:28:10,645 - ==> Confusion:
[[ 144    2  293]
 [  12   16  574]
 [ 101    6 5838]]

2023-01-06 14:28:10,646 - ==> Best [Top1: 85.857   Sparsity:0.00   Params: 360896 on epoch: 4]
2023-01-06 14:28:10,646 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:10,669 - 

2023-01-06 14:28:10,669 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:11,386 - Epoch: [5][   10/  246]    Overall Loss 0.412440    Objective Loss 0.412440                                        LR 0.000060    Time 0.071665    
2023-01-06 14:28:11,625 - Epoch: [5][   20/  246]    Overall Loss 0.397598    Objective Loss 0.397598                                        LR 0.000060    Time 0.047753    
2023-01-06 14:28:11,877 - Epoch: [5][   30/  246]    Overall Loss 0.394549    Objective Loss 0.394549                                        LR 0.000060    Time 0.040211    
2023-01-06 14:28:12,130 - Epoch: [5][   40/  246]    Overall Loss 0.397291    Objective Loss 0.397291                                        LR 0.000060    Time 0.036479    
2023-01-06 14:28:12,376 - Epoch: [5][   50/  246]    Overall Loss 0.396450    Objective Loss 0.396450                                        LR 0.000060    Time 0.034095    
2023-01-06 14:28:12,620 - Epoch: [5][   60/  246]    Overall Loss 0.397633    Objective Loss 0.397633                                        LR 0.000060    Time 0.032470    
2023-01-06 14:28:12,868 - Epoch: [5][   70/  246]    Overall Loss 0.401799    Objective Loss 0.401799                                        LR 0.000060    Time 0.031360    
2023-01-06 14:28:13,113 - Epoch: [5][   80/  246]    Overall Loss 0.403912    Objective Loss 0.403912                                        LR 0.000060    Time 0.030504    
2023-01-06 14:28:13,360 - Epoch: [5][   90/  246]    Overall Loss 0.403207    Objective Loss 0.403207                                        LR 0.000060    Time 0.029853    
2023-01-06 14:28:13,607 - Epoch: [5][  100/  246]    Overall Loss 0.401251    Objective Loss 0.401251                                        LR 0.000060    Time 0.029330    
2023-01-06 14:28:13,860 - Epoch: [5][  110/  246]    Overall Loss 0.400807    Objective Loss 0.400807                                        LR 0.000060    Time 0.028956    
2023-01-06 14:28:14,114 - Epoch: [5][  120/  246]    Overall Loss 0.399873    Objective Loss 0.399873                                        LR 0.000060    Time 0.028662    
2023-01-06 14:28:14,368 - Epoch: [5][  130/  246]    Overall Loss 0.399499    Objective Loss 0.399499                                        LR 0.000060    Time 0.028404    
2023-01-06 14:28:14,622 - Epoch: [5][  140/  246]    Overall Loss 0.399098    Objective Loss 0.399098                                        LR 0.000060    Time 0.028189    
2023-01-06 14:28:14,878 - Epoch: [5][  150/  246]    Overall Loss 0.398145    Objective Loss 0.398145                                        LR 0.000060    Time 0.028009    
2023-01-06 14:28:15,130 - Epoch: [5][  160/  246]    Overall Loss 0.399572    Objective Loss 0.399572                                        LR 0.000060    Time 0.027832    
2023-01-06 14:28:15,377 - Epoch: [5][  170/  246]    Overall Loss 0.400931    Objective Loss 0.400931                                        LR 0.000060    Time 0.027646    
2023-01-06 14:28:15,623 - Epoch: [5][  180/  246]    Overall Loss 0.400106    Objective Loss 0.400106                                        LR 0.000060    Time 0.027473    
2023-01-06 14:28:15,870 - Epoch: [5][  190/  246]    Overall Loss 0.399323    Objective Loss 0.399323                                        LR 0.000060    Time 0.027326    
2023-01-06 14:28:16,115 - Epoch: [5][  200/  246]    Overall Loss 0.399084    Objective Loss 0.399084                                        LR 0.000060    Time 0.027184    
2023-01-06 14:28:16,362 - Epoch: [5][  210/  246]    Overall Loss 0.399518    Objective Loss 0.399518                                        LR 0.000060    Time 0.027063    
2023-01-06 14:28:16,606 - Epoch: [5][  220/  246]    Overall Loss 0.399317    Objective Loss 0.399317                                        LR 0.000060    Time 0.026939    
2023-01-06 14:28:16,853 - Epoch: [5][  230/  246]    Overall Loss 0.399605    Objective Loss 0.399605                                        LR 0.000060    Time 0.026838    
2023-01-06 14:28:17,109 - Epoch: [5][  240/  246]    Overall Loss 0.399181    Objective Loss 0.399181                                        LR 0.000060    Time 0.026783    
2023-01-06 14:28:17,238 - Epoch: [5][  246/  246]    Overall Loss 0.398937    Objective Loss 0.398937    Top1 84.210526    LR 0.000060    Time 0.026654    
2023-01-06 14:28:17,367 - --- validate (epoch=5)-----------
2023-01-06 14:28:17,367 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:17,802 - Epoch: [5][   10/   28]    Loss 0.406243    Top1 85.625000    
2023-01-06 14:28:17,943 - Epoch: [5][   20/   28]    Loss 0.393301    Top1 86.093750    
2023-01-06 14:28:18,033 - Epoch: [5][   28/   28]    Loss 0.387267    Top1 86.258231    
2023-01-06 14:28:18,167 - ==> Top1: 86.258    Loss: 0.387

2023-01-06 14:28:18,168 - ==> Confusion:
[[ 120    3  316]
 [   5   52  545]
 [  66   25 5854]]

2023-01-06 14:28:18,169 - ==> Best [Top1: 86.258   Sparsity:0.00   Params: 360896 on epoch: 5]
2023-01-06 14:28:18,169 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:18,191 - 

2023-01-06 14:28:18,191 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:18,784 - Epoch: [6][   10/  246]    Overall Loss 0.396486    Objective Loss 0.396486                                        LR 0.000060    Time 0.059190    
2023-01-06 14:28:19,028 - Epoch: [6][   20/  246]    Overall Loss 0.408083    Objective Loss 0.408083                                        LR 0.000060    Time 0.041795    
2023-01-06 14:28:19,279 - Epoch: [6][   30/  246]    Overall Loss 0.400545    Objective Loss 0.400545                                        LR 0.000060    Time 0.036190    
2023-01-06 14:28:19,528 - Epoch: [6][   40/  246]    Overall Loss 0.394129    Objective Loss 0.394129                                        LR 0.000060    Time 0.033362    
2023-01-06 14:28:19,787 - Epoch: [6][   50/  246]    Overall Loss 0.390010    Objective Loss 0.390010                                        LR 0.000060    Time 0.031866    
2023-01-06 14:28:20,034 - Epoch: [6][   60/  246]    Overall Loss 0.386695    Objective Loss 0.386695                                        LR 0.000060    Time 0.030667    
2023-01-06 14:28:20,281 - Epoch: [6][   70/  246]    Overall Loss 0.387457    Objective Loss 0.387457                                        LR 0.000060    Time 0.029792    
2023-01-06 14:28:20,534 - Epoch: [6][   80/  246]    Overall Loss 0.389122    Objective Loss 0.389122                                        LR 0.000060    Time 0.029223    
2023-01-06 14:28:20,785 - Epoch: [6][   90/  246]    Overall Loss 0.387068    Objective Loss 0.387068                                        LR 0.000060    Time 0.028748    
2023-01-06 14:28:21,035 - Epoch: [6][  100/  246]    Overall Loss 0.387464    Objective Loss 0.387464                                        LR 0.000060    Time 0.028366    
2023-01-06 14:28:21,285 - Epoch: [6][  110/  246]    Overall Loss 0.386957    Objective Loss 0.386957                                        LR 0.000060    Time 0.028057    
2023-01-06 14:28:21,536 - Epoch: [6][  120/  246]    Overall Loss 0.387548    Objective Loss 0.387548                                        LR 0.000060    Time 0.027802    
2023-01-06 14:28:21,787 - Epoch: [6][  130/  246]    Overall Loss 0.386522    Objective Loss 0.386522                                        LR 0.000060    Time 0.027579    
2023-01-06 14:28:22,042 - Epoch: [6][  140/  246]    Overall Loss 0.388600    Objective Loss 0.388600                                        LR 0.000060    Time 0.027433    
2023-01-06 14:28:22,303 - Epoch: [6][  150/  246]    Overall Loss 0.389506    Objective Loss 0.389506                                        LR 0.000060    Time 0.027328    
2023-01-06 14:28:22,556 - Epoch: [6][  160/  246]    Overall Loss 0.390393    Objective Loss 0.390393                                        LR 0.000060    Time 0.027202    
2023-01-06 14:28:22,817 - Epoch: [6][  170/  246]    Overall Loss 0.390236    Objective Loss 0.390236                                        LR 0.000060    Time 0.027132    
2023-01-06 14:28:23,071 - Epoch: [6][  180/  246]    Overall Loss 0.390336    Objective Loss 0.390336                                        LR 0.000060    Time 0.027033    
2023-01-06 14:28:23,322 - Epoch: [6][  190/  246]    Overall Loss 0.391322    Objective Loss 0.391322                                        LR 0.000060    Time 0.026931    
2023-01-06 14:28:23,572 - Epoch: [6][  200/  246]    Overall Loss 0.391375    Objective Loss 0.391375                                        LR 0.000060    Time 0.026833    
2023-01-06 14:28:23,821 - Epoch: [6][  210/  246]    Overall Loss 0.391792    Objective Loss 0.391792                                        LR 0.000060    Time 0.026738    
2023-01-06 14:28:24,072 - Epoch: [6][  220/  246]    Overall Loss 0.390814    Objective Loss 0.390814                                        LR 0.000060    Time 0.026652    
2023-01-06 14:28:24,317 - Epoch: [6][  230/  246]    Overall Loss 0.391244    Objective Loss 0.391244                                        LR 0.000060    Time 0.026559    
2023-01-06 14:28:24,581 - Epoch: [6][  240/  246]    Overall Loss 0.390723    Objective Loss 0.390723                                        LR 0.000060    Time 0.026541    
2023-01-06 14:28:24,710 - Epoch: [6][  246/  246]    Overall Loss 0.390145    Objective Loss 0.390145    Top1 88.038278    LR 0.000060    Time 0.026419    
2023-01-06 14:28:24,846 - --- validate (epoch=6)-----------
2023-01-06 14:28:24,846 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:25,288 - Epoch: [6][   10/   28]    Loss 0.396603    Top1 85.468750    
2023-01-06 14:28:25,430 - Epoch: [6][   20/   28]    Loss 0.379570    Top1 86.132812    
2023-01-06 14:28:25,520 - Epoch: [6][   28/   28]    Loss 0.387091    Top1 85.871743    
2023-01-06 14:28:25,652 - ==> Top1: 85.872    Loss: 0.387

2023-01-06 14:28:25,653 - ==> Confusion:
[[  70    0  369]
 [   1   23  578]
 [  33    6 5906]]

2023-01-06 14:28:25,654 - ==> Best [Top1: 86.258   Sparsity:0.00   Params: 360896 on epoch: 5]
2023-01-06 14:28:25,654 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:25,664 - 

2023-01-06 14:28:25,664 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:26,395 - Epoch: [7][   10/  246]    Overall Loss 0.415827    Objective Loss 0.415827                                        LR 0.000060    Time 0.073062    
2023-01-06 14:28:26,649 - Epoch: [7][   20/  246]    Overall Loss 0.386354    Objective Loss 0.386354                                        LR 0.000060    Time 0.049203    
2023-01-06 14:28:26,899 - Epoch: [7][   30/  246]    Overall Loss 0.386155    Objective Loss 0.386155                                        LR 0.000060    Time 0.041141    
2023-01-06 14:28:27,152 - Epoch: [7][   40/  246]    Overall Loss 0.389929    Objective Loss 0.389929                                        LR 0.000060    Time 0.037161    
2023-01-06 14:28:27,403 - Epoch: [7][   50/  246]    Overall Loss 0.388165    Objective Loss 0.388165                                        LR 0.000060    Time 0.034750    
2023-01-06 14:28:27,652 - Epoch: [7][   60/  246]    Overall Loss 0.390027    Objective Loss 0.390027                                        LR 0.000060    Time 0.033093    
2023-01-06 14:28:27,896 - Epoch: [7][   70/  246]    Overall Loss 0.387854    Objective Loss 0.387854                                        LR 0.000060    Time 0.031851    
2023-01-06 14:28:28,141 - Epoch: [7][   80/  246]    Overall Loss 0.386174    Objective Loss 0.386174                                        LR 0.000060    Time 0.030928    
2023-01-06 14:28:28,387 - Epoch: [7][   90/  246]    Overall Loss 0.385982    Objective Loss 0.385982                                        LR 0.000060    Time 0.030217    
2023-01-06 14:28:28,631 - Epoch: [7][  100/  246]    Overall Loss 0.385078    Objective Loss 0.385078                                        LR 0.000060    Time 0.029636    
2023-01-06 14:28:28,876 - Epoch: [7][  110/  246]    Overall Loss 0.384121    Objective Loss 0.384121                                        LR 0.000060    Time 0.029166    
2023-01-06 14:28:29,122 - Epoch: [7][  120/  246]    Overall Loss 0.383393    Objective Loss 0.383393                                        LR 0.000060    Time 0.028783    
2023-01-06 14:28:29,367 - Epoch: [7][  130/  246]    Overall Loss 0.385062    Objective Loss 0.385062                                        LR 0.000060    Time 0.028455    
2023-01-06 14:28:29,617 - Epoch: [7][  140/  246]    Overall Loss 0.384938    Objective Loss 0.384938                                        LR 0.000060    Time 0.028200    
2023-01-06 14:28:29,863 - Epoch: [7][  150/  246]    Overall Loss 0.385756    Objective Loss 0.385756                                        LR 0.000060    Time 0.027959    
2023-01-06 14:28:30,106 - Epoch: [7][  160/  246]    Overall Loss 0.387106    Objective Loss 0.387106                                        LR 0.000060    Time 0.027733    
2023-01-06 14:28:30,350 - Epoch: [7][  170/  246]    Overall Loss 0.387755    Objective Loss 0.387755                                        LR 0.000060    Time 0.027531    
2023-01-06 14:28:30,591 - Epoch: [7][  180/  246]    Overall Loss 0.388037    Objective Loss 0.388037                                        LR 0.000060    Time 0.027341    
2023-01-06 14:28:30,834 - Epoch: [7][  190/  246]    Overall Loss 0.387800    Objective Loss 0.387800                                        LR 0.000060    Time 0.027180    
2023-01-06 14:28:31,075 - Epoch: [7][  200/  246]    Overall Loss 0.386684    Objective Loss 0.386684                                        LR 0.000060    Time 0.027026    
2023-01-06 14:28:31,317 - Epoch: [7][  210/  246]    Overall Loss 0.386485    Objective Loss 0.386485                                        LR 0.000060    Time 0.026888    
2023-01-06 14:28:31,560 - Epoch: [7][  220/  246]    Overall Loss 0.386118    Objective Loss 0.386118                                        LR 0.000060    Time 0.026761    
2023-01-06 14:28:31,801 - Epoch: [7][  230/  246]    Overall Loss 0.387019    Objective Loss 0.387019                                        LR 0.000060    Time 0.026644    
2023-01-06 14:28:32,056 - Epoch: [7][  240/  246]    Overall Loss 0.386553    Objective Loss 0.386553                                        LR 0.000060    Time 0.026594    
2023-01-06 14:28:32,184 - Epoch: [7][  246/  246]    Overall Loss 0.386552    Objective Loss 0.386552    Top1 85.645933    LR 0.000060    Time 0.026465    
2023-01-06 14:28:32,319 - --- validate (epoch=7)-----------
2023-01-06 14:28:32,320 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:32,785 - Epoch: [7][   10/   28]    Loss 0.379709    Top1 85.976562    
2023-01-06 14:28:32,939 - Epoch: [7][   20/   28]    Loss 0.368729    Top1 86.816406    
2023-01-06 14:28:33,032 - Epoch: [7][   28/   28]    Loss 0.378303    Top1 86.401374    
2023-01-06 14:28:33,174 - ==> Top1: 86.401    Loss: 0.378

2023-01-06 14:28:33,174 - ==> Confusion:
[[ 144    1  294]
 [   9   77  516]
 [  85   45 5815]]

2023-01-06 14:28:33,176 - ==> Best [Top1: 86.401   Sparsity:0.00   Params: 360896 on epoch: 7]
2023-01-06 14:28:33,176 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:33,198 - 

2023-01-06 14:28:33,198 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:33,935 - Epoch: [8][   10/  246]    Overall Loss 0.369359    Objective Loss 0.369359                                        LR 0.000060    Time 0.073605    
2023-01-06 14:28:34,176 - Epoch: [8][   20/  246]    Overall Loss 0.386636    Objective Loss 0.386636                                        LR 0.000060    Time 0.048870    
2023-01-06 14:28:34,417 - Epoch: [8][   30/  246]    Overall Loss 0.383217    Objective Loss 0.383217                                        LR 0.000060    Time 0.040592    
2023-01-06 14:28:34,657 - Epoch: [8][   40/  246]    Overall Loss 0.383489    Objective Loss 0.383489                                        LR 0.000060    Time 0.036417    
2023-01-06 14:28:34,898 - Epoch: [8][   50/  246]    Overall Loss 0.383490    Objective Loss 0.383490                                        LR 0.000060    Time 0.033941    
2023-01-06 14:28:35,141 - Epoch: [8][   60/  246]    Overall Loss 0.382209    Objective Loss 0.382209                                        LR 0.000060    Time 0.032337    
2023-01-06 14:28:35,380 - Epoch: [8][   70/  246]    Overall Loss 0.383519    Objective Loss 0.383519                                        LR 0.000060    Time 0.031120    
2023-01-06 14:28:35,619 - Epoch: [8][   80/  246]    Overall Loss 0.380770    Objective Loss 0.380770                                        LR 0.000060    Time 0.030206    
2023-01-06 14:28:35,856 - Epoch: [8][   90/  246]    Overall Loss 0.381185    Objective Loss 0.381185                                        LR 0.000060    Time 0.029480    
2023-01-06 14:28:36,093 - Epoch: [8][  100/  246]    Overall Loss 0.382358    Objective Loss 0.382358                                        LR 0.000060    Time 0.028901    
2023-01-06 14:28:36,333 - Epoch: [8][  110/  246]    Overall Loss 0.382289    Objective Loss 0.382289                                        LR 0.000060    Time 0.028455    
2023-01-06 14:28:36,578 - Epoch: [8][  120/  246]    Overall Loss 0.383009    Objective Loss 0.383009                                        LR 0.000060    Time 0.028118    
2023-01-06 14:28:36,822 - Epoch: [8][  130/  246]    Overall Loss 0.383271    Objective Loss 0.383271                                        LR 0.000060    Time 0.027832    
2023-01-06 14:28:37,066 - Epoch: [8][  140/  246]    Overall Loss 0.383730    Objective Loss 0.383730                                        LR 0.000060    Time 0.027586    
2023-01-06 14:28:37,310 - Epoch: [8][  150/  246]    Overall Loss 0.382518    Objective Loss 0.382518                                        LR 0.000060    Time 0.027369    
2023-01-06 14:28:37,553 - Epoch: [8][  160/  246]    Overall Loss 0.382881    Objective Loss 0.382881                                        LR 0.000060    Time 0.027177    
2023-01-06 14:28:37,794 - Epoch: [8][  170/  246]    Overall Loss 0.381829    Objective Loss 0.381829                                        LR 0.000060    Time 0.026997    
2023-01-06 14:28:38,036 - Epoch: [8][  180/  246]    Overall Loss 0.381026    Objective Loss 0.381026                                        LR 0.000060    Time 0.026835    
2023-01-06 14:28:38,277 - Epoch: [8][  190/  246]    Overall Loss 0.381820    Objective Loss 0.381820                                        LR 0.000060    Time 0.026691    
2023-01-06 14:28:38,518 - Epoch: [8][  200/  246]    Overall Loss 0.381365    Objective Loss 0.381365                                        LR 0.000060    Time 0.026563    
2023-01-06 14:28:38,760 - Epoch: [8][  210/  246]    Overall Loss 0.382294    Objective Loss 0.382294                                        LR 0.000060    Time 0.026449    
2023-01-06 14:28:39,002 - Epoch: [8][  220/  246]    Overall Loss 0.381418    Objective Loss 0.381418                                        LR 0.000060    Time 0.026342    
2023-01-06 14:28:39,243 - Epoch: [8][  230/  246]    Overall Loss 0.381998    Objective Loss 0.381998                                        LR 0.000060    Time 0.026245    
2023-01-06 14:28:39,497 - Epoch: [8][  240/  246]    Overall Loss 0.382151    Objective Loss 0.382151                                        LR 0.000060    Time 0.026210    
2023-01-06 14:28:39,625 - Epoch: [8][  246/  246]    Overall Loss 0.382420    Objective Loss 0.382420    Top1 85.885167    LR 0.000060    Time 0.026091    
2023-01-06 14:28:39,764 - --- validate (epoch=8)-----------
2023-01-06 14:28:39,764 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:40,220 - Epoch: [8][   10/   28]    Loss 0.384245    Top1 85.195312    
2023-01-06 14:28:40,367 - Epoch: [8][   20/   28]    Loss 0.375604    Top1 86.152344    
2023-01-06 14:28:40,457 - Epoch: [8][   28/   28]    Loss 0.373446    Top1 86.186659    
2023-01-06 14:28:40,603 - ==> Top1: 86.187    Loss: 0.373

2023-01-06 14:28:40,603 - ==> Confusion:
[[ 124    1  314]
 [   6   31  565]
 [  67   12 5866]]

2023-01-06 14:28:40,605 - ==> Best [Top1: 86.401   Sparsity:0.00   Params: 360896 on epoch: 7]
2023-01-06 14:28:40,605 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:40,615 - 

2023-01-06 14:28:40,615 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:41,226 - Epoch: [9][   10/  246]    Overall Loss 0.378063    Objective Loss 0.378063                                        LR 0.000060    Time 0.061077    
2023-01-06 14:28:41,473 - Epoch: [9][   20/  246]    Overall Loss 0.372518    Objective Loss 0.372518                                        LR 0.000060    Time 0.042830    
2023-01-06 14:28:41,721 - Epoch: [9][   30/  246]    Overall Loss 0.370888    Objective Loss 0.370888                                        LR 0.000060    Time 0.036800    
2023-01-06 14:28:41,970 - Epoch: [9][   40/  246]    Overall Loss 0.373095    Objective Loss 0.373095                                        LR 0.000060    Time 0.033827    
2023-01-06 14:28:42,219 - Epoch: [9][   50/  246]    Overall Loss 0.372338    Objective Loss 0.372338                                        LR 0.000060    Time 0.032032    
2023-01-06 14:28:42,464 - Epoch: [9][   60/  246]    Overall Loss 0.376263    Objective Loss 0.376263                                        LR 0.000060    Time 0.030769    
2023-01-06 14:28:42,711 - Epoch: [9][   70/  246]    Overall Loss 0.379293    Objective Loss 0.379293                                        LR 0.000060    Time 0.029892    
2023-01-06 14:28:42,960 - Epoch: [9][   80/  246]    Overall Loss 0.376821    Objective Loss 0.376821                                        LR 0.000060    Time 0.029256    
2023-01-06 14:28:43,209 - Epoch: [9][   90/  246]    Overall Loss 0.377761    Objective Loss 0.377761                                        LR 0.000060    Time 0.028772    
2023-01-06 14:28:43,460 - Epoch: [9][  100/  246]    Overall Loss 0.377247    Objective Loss 0.377247                                        LR 0.000060    Time 0.028402    
2023-01-06 14:28:43,709 - Epoch: [9][  110/  246]    Overall Loss 0.376639    Objective Loss 0.376639                                        LR 0.000060    Time 0.028081    
2023-01-06 14:28:43,962 - Epoch: [9][  120/  246]    Overall Loss 0.374918    Objective Loss 0.374918                                        LR 0.000060    Time 0.027838    
2023-01-06 14:28:44,215 - Epoch: [9][  130/  246]    Overall Loss 0.373944    Objective Loss 0.373944                                        LR 0.000060    Time 0.027645    
2023-01-06 14:28:44,469 - Epoch: [9][  140/  246]    Overall Loss 0.375487    Objective Loss 0.375487                                        LR 0.000060    Time 0.027478    
2023-01-06 14:28:44,716 - Epoch: [9][  150/  246]    Overall Loss 0.376748    Objective Loss 0.376748                                        LR 0.000060    Time 0.027293    
2023-01-06 14:28:44,966 - Epoch: [9][  160/  246]    Overall Loss 0.376325    Objective Loss 0.376325                                        LR 0.000060    Time 0.027150    
2023-01-06 14:28:45,212 - Epoch: [9][  170/  246]    Overall Loss 0.377002    Objective Loss 0.377002                                        LR 0.000060    Time 0.026996    
2023-01-06 14:28:45,467 - Epoch: [9][  180/  246]    Overall Loss 0.376616    Objective Loss 0.376616                                        LR 0.000060    Time 0.026904    
2023-01-06 14:28:45,720 - Epoch: [9][  190/  246]    Overall Loss 0.377173    Objective Loss 0.377173                                        LR 0.000060    Time 0.026815    
2023-01-06 14:28:45,988 - Epoch: [9][  200/  246]    Overall Loss 0.377095    Objective Loss 0.377095                                        LR 0.000060    Time 0.026803    
2023-01-06 14:28:46,242 - Epoch: [9][  210/  246]    Overall Loss 0.376801    Objective Loss 0.376801                                        LR 0.000060    Time 0.026737    
2023-01-06 14:28:46,503 - Epoch: [9][  220/  246]    Overall Loss 0.376779    Objective Loss 0.376779                                        LR 0.000060    Time 0.026708    
2023-01-06 14:28:46,755 - Epoch: [9][  230/  246]    Overall Loss 0.377048    Objective Loss 0.377048                                        LR 0.000060    Time 0.026641    
2023-01-06 14:28:47,023 - Epoch: [9][  240/  246]    Overall Loss 0.377935    Objective Loss 0.377935                                        LR 0.000060    Time 0.026638    
2023-01-06 14:28:47,150 - Epoch: [9][  246/  246]    Overall Loss 0.378124    Objective Loss 0.378124    Top1 85.645933    LR 0.000060    Time 0.026505    
2023-01-06 14:28:47,286 - --- validate (epoch=9)-----------
2023-01-06 14:28:47,286 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:47,737 - Epoch: [9][   10/   28]    Loss 0.370654    Top1 86.406250    
2023-01-06 14:28:47,885 - Epoch: [9][   20/   28]    Loss 0.365397    Top1 86.875000    
2023-01-06 14:28:47,975 - Epoch: [9][   28/   28]    Loss 0.368784    Top1 86.644718    
2023-01-06 14:28:48,120 - ==> Top1: 86.645    Loss: 0.369

2023-01-06 14:28:48,121 - ==> Confusion:
[[ 135    2  302]
 [   8   78  516]
 [  73   32 5840]]

2023-01-06 14:28:48,122 - ==> Best [Top1: 86.645   Sparsity:0.00   Params: 360896 on epoch: 9]
2023-01-06 14:28:48,122 - Saving checkpoint to: logs/2023.01.06-142639/checkpoint.pth.tar
2023-01-06 14:28:48,163 - 

2023-01-06 14:28:48,163 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:48,896 - Epoch: [10][   10/  246]    Overall Loss 0.397225    Objective Loss 0.397225                                        LR 0.000060    Time 0.073274    
2023-01-06 14:28:49,139 - Epoch: [10][   20/  246]    Overall Loss 0.390860    Objective Loss 0.390860                                        LR 0.000060    Time 0.048750    
2023-01-06 14:28:49,393 - Epoch: [10][   30/  246]    Overall Loss 0.386330    Objective Loss 0.386330                                        LR 0.000060    Time 0.040938    
2023-01-06 14:28:49,642 - Epoch: [10][   40/  246]    Overall Loss 0.386798    Objective Loss 0.386798                                        LR 0.000060    Time 0.036933    
2023-01-06 14:28:49,896 - Epoch: [10][   50/  246]    Overall Loss 0.382004    Objective Loss 0.382004                                        LR 0.000060    Time 0.034618    
2023-01-06 14:28:50,150 - Epoch: [10][   60/  246]    Overall Loss 0.376511    Objective Loss 0.376511                                        LR 0.000060    Time 0.033069    
2023-01-06 14:28:50,406 - Epoch: [10][   70/  246]    Overall Loss 0.378539    Objective Loss 0.378539                                        LR 0.000060    Time 0.031987    
2023-01-06 14:28:50,658 - Epoch: [10][   80/  246]    Overall Loss 0.381264    Objective Loss 0.381264                                        LR 0.000060    Time 0.031125    
2023-01-06 14:28:50,908 - Epoch: [10][   90/  246]    Overall Loss 0.380882    Objective Loss 0.380882                                        LR 0.000060    Time 0.030440    
2023-01-06 14:28:51,156 - Epoch: [10][  100/  246]    Overall Loss 0.380498    Objective Loss 0.380498                                        LR 0.000060    Time 0.029877    
2023-01-06 14:28:51,406 - Epoch: [10][  110/  246]    Overall Loss 0.377743    Objective Loss 0.377743                                        LR 0.000060    Time 0.029425    
2023-01-06 14:28:51,654 - Epoch: [10][  120/  246]    Overall Loss 0.376106    Objective Loss 0.376106                                        LR 0.000060    Time 0.029040    
2023-01-06 14:28:51,905 - Epoch: [10][  130/  246]    Overall Loss 0.375488    Objective Loss 0.375488                                        LR 0.000060    Time 0.028734    
2023-01-06 14:28:52,163 - Epoch: [10][  140/  246]    Overall Loss 0.374732    Objective Loss 0.374732                                        LR 0.000060    Time 0.028521    
2023-01-06 14:28:52,421 - Epoch: [10][  150/  246]    Overall Loss 0.374602    Objective Loss 0.374602                                        LR 0.000060    Time 0.028338    
2023-01-06 14:28:52,661 - Epoch: [10][  160/  246]    Overall Loss 0.373611    Objective Loss 0.373611                                        LR 0.000060    Time 0.028063    
2023-01-06 14:28:52,901 - Epoch: [10][  170/  246]    Overall Loss 0.373683    Objective Loss 0.373683                                        LR 0.000060    Time 0.027821    
2023-01-06 14:28:53,147 - Epoch: [10][  180/  246]    Overall Loss 0.372859    Objective Loss 0.372859                                        LR 0.000060    Time 0.027641    
2023-01-06 14:28:53,400 - Epoch: [10][  190/  246]    Overall Loss 0.374192    Objective Loss 0.374192                                        LR 0.000060    Time 0.027513    
2023-01-06 14:28:53,662 - Epoch: [10][  200/  246]    Overall Loss 0.375216    Objective Loss 0.375216                                        LR 0.000060    Time 0.027446    
2023-01-06 14:28:53,924 - Epoch: [10][  210/  246]    Overall Loss 0.373671    Objective Loss 0.373671                                        LR 0.000060    Time 0.027382    
2023-01-06 14:28:54,192 - Epoch: [10][  220/  246]    Overall Loss 0.373536    Objective Loss 0.373536                                        LR 0.000060    Time 0.027356    
2023-01-06 14:28:54,462 - Epoch: [10][  230/  246]    Overall Loss 0.373970    Objective Loss 0.373970                                        LR 0.000060    Time 0.027339    
2023-01-06 14:28:54,739 - Epoch: [10][  240/  246]    Overall Loss 0.374360    Objective Loss 0.374360                                        LR 0.000060    Time 0.027351    
2023-01-06 14:28:54,871 - Epoch: [10][  246/  246]    Overall Loss 0.373879    Objective Loss 0.373879    Top1 87.320574    LR 0.000060    Time 0.027222    
2023-01-06 14:28:55,011 - --- validate (epoch=10)-----------
2023-01-06 14:28:55,011 - 6986 samples (256 per mini-batch)
2023-01-06 14:28:55,448 - Epoch: [10][   10/   28]    Loss 0.364547    Top1 86.523438    
2023-01-06 14:28:55,591 - Epoch: [10][   20/   28]    Loss 0.367721    Top1 86.484375    
2023-01-06 14:28:55,683 - Epoch: [10][   28/   28]    Loss 0.362204    Top1 86.573146    
2023-01-06 14:28:55,816 - ==> Top1: 86.573    Loss: 0.362

2023-01-06 14:28:55,816 - ==> Confusion:
[[ 142    1  296]
 [   8   58  536]
 [  78   19 5848]]

2023-01-06 14:28:55,818 - ==> Best [Top1: 86.573   Sparsity:0.00   Params: 360896 on epoch: 10]
2023-01-06 14:28:55,818 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:28:55,829 - 

2023-01-06 14:28:55,829 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:28:56,594 - Epoch: [11][   10/  246]    Overall Loss 0.375824    Objective Loss 0.375824                                        LR 0.000060    Time 0.076347    
2023-01-06 14:28:56,852 - Epoch: [11][   20/  246]    Overall Loss 0.377314    Objective Loss 0.377314                                        LR 0.000060    Time 0.051068    
2023-01-06 14:28:57,106 - Epoch: [11][   30/  246]    Overall Loss 0.371881    Objective Loss 0.371881                                        LR 0.000060    Time 0.042493    
2023-01-06 14:28:57,361 - Epoch: [11][   40/  246]    Overall Loss 0.376101    Objective Loss 0.376101                                        LR 0.000060    Time 0.038252    
2023-01-06 14:28:57,612 - Epoch: [11][   50/  246]    Overall Loss 0.372931    Objective Loss 0.372931                                        LR 0.000060    Time 0.035603    
2023-01-06 14:28:57,859 - Epoch: [11][   60/  246]    Overall Loss 0.372105    Objective Loss 0.372105                                        LR 0.000060    Time 0.033787    
2023-01-06 14:28:58,125 - Epoch: [11][   70/  246]    Overall Loss 0.372606    Objective Loss 0.372606                                        LR 0.000060    Time 0.032751    
2023-01-06 14:28:58,389 - Epoch: [11][   80/  246]    Overall Loss 0.373178    Objective Loss 0.373178                                        LR 0.000060    Time 0.031947    
2023-01-06 14:28:58,652 - Epoch: [11][   90/  246]    Overall Loss 0.372499    Objective Loss 0.372499                                        LR 0.000060    Time 0.031322    
2023-01-06 14:28:58,905 - Epoch: [11][  100/  246]    Overall Loss 0.373641    Objective Loss 0.373641                                        LR 0.000060    Time 0.030714    
2023-01-06 14:28:59,140 - Epoch: [11][  110/  246]    Overall Loss 0.372572    Objective Loss 0.372572                                        LR 0.000060    Time 0.030052    
2023-01-06 14:28:59,357 - Epoch: [11][  120/  246]    Overall Loss 0.371736    Objective Loss 0.371736                                        LR 0.000060    Time 0.029352    
2023-01-06 14:28:59,571 - Epoch: [11][  130/  246]    Overall Loss 0.371958    Objective Loss 0.371958                                        LR 0.000060    Time 0.028739    
2023-01-06 14:28:59,785 - Epoch: [11][  140/  246]    Overall Loss 0.371829    Objective Loss 0.371829                                        LR 0.000060    Time 0.028216    
2023-01-06 14:29:00,002 - Epoch: [11][  150/  246]    Overall Loss 0.371136    Objective Loss 0.371136                                        LR 0.000060    Time 0.027777    
2023-01-06 14:29:00,217 - Epoch: [11][  160/  246]    Overall Loss 0.372548    Objective Loss 0.372548                                        LR 0.000060    Time 0.027379    
2023-01-06 14:29:00,431 - Epoch: [11][  170/  246]    Overall Loss 0.373900    Objective Loss 0.373900                                        LR 0.000060    Time 0.027028    
2023-01-06 14:29:00,646 - Epoch: [11][  180/  246]    Overall Loss 0.375240    Objective Loss 0.375240                                        LR 0.000060    Time 0.026717    
2023-01-06 14:29:00,860 - Epoch: [11][  190/  246]    Overall Loss 0.373167    Objective Loss 0.373167                                        LR 0.000060    Time 0.026437    
2023-01-06 14:29:01,076 - Epoch: [11][  200/  246]    Overall Loss 0.373945    Objective Loss 0.373945                                        LR 0.000060    Time 0.026189    
2023-01-06 14:29:01,292 - Epoch: [11][  210/  246]    Overall Loss 0.373644    Objective Loss 0.373644                                        LR 0.000060    Time 0.025970    
2023-01-06 14:29:01,507 - Epoch: [11][  220/  246]    Overall Loss 0.373009    Objective Loss 0.373009                                        LR 0.000060    Time 0.025765    
2023-01-06 14:29:01,729 - Epoch: [11][  230/  246]    Overall Loss 0.372944    Objective Loss 0.372944                                        LR 0.000060    Time 0.025609    
2023-01-06 14:29:01,961 - Epoch: [11][  240/  246]    Overall Loss 0.371965    Objective Loss 0.371965                                        LR 0.000060    Time 0.025509    
2023-01-06 14:29:02,087 - Epoch: [11][  246/  246]    Overall Loss 0.370985    Objective Loss 0.370985    Top1 86.602871    LR 0.000060    Time 0.025398    
2023-01-06 14:29:02,262 - --- validate (epoch=11)-----------
2023-01-06 14:29:02,263 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:02,705 - Epoch: [11][   10/   28]    Loss 0.362447    Top1 86.718750    
2023-01-06 14:29:02,844 - Epoch: [11][   20/   28]    Loss 0.364889    Top1 86.875000    
2023-01-06 14:29:02,936 - Epoch: [11][   28/   28]    Loss 0.367796    Top1 86.830804    
2023-01-06 14:29:03,109 - ==> Top1: 86.831    Loss: 0.368

2023-01-06 14:29:03,110 - ==> Confusion:
[[ 128    2  309]
 [   8   83  511]
 [  58   32 5855]]

2023-01-06 14:29:03,111 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 360896 on epoch: 11]
2023-01-06 14:29:03,111 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:03,132 - 

2023-01-06 14:29:03,132 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:03,716 - Epoch: [12][   10/  246]    Overall Loss 0.366396    Objective Loss 0.366396                                        LR 0.000060    Time 0.058294    
2023-01-06 14:29:03,953 - Epoch: [12][   20/  246]    Overall Loss 0.364556    Objective Loss 0.364556                                        LR 0.000060    Time 0.040995    
2023-01-06 14:29:04,192 - Epoch: [12][   30/  246]    Overall Loss 0.366938    Objective Loss 0.366938                                        LR 0.000060    Time 0.035266    
2023-01-06 14:29:04,435 - Epoch: [12][   40/  246]    Overall Loss 0.362904    Objective Loss 0.362904                                        LR 0.000060    Time 0.032523    
2023-01-06 14:29:04,667 - Epoch: [12][   50/  246]    Overall Loss 0.363631    Objective Loss 0.363631                                        LR 0.000060    Time 0.030647    
2023-01-06 14:29:04,909 - Epoch: [12][   60/  246]    Overall Loss 0.363475    Objective Loss 0.363475                                        LR 0.000060    Time 0.029558    
2023-01-06 14:29:05,142 - Epoch: [12][   70/  246]    Overall Loss 0.364483    Objective Loss 0.364483                                        LR 0.000060    Time 0.028655    
2023-01-06 14:29:05,383 - Epoch: [12][   80/  246]    Overall Loss 0.368383    Objective Loss 0.368383                                        LR 0.000060    Time 0.028086    
2023-01-06 14:29:05,616 - Epoch: [12][   90/  246]    Overall Loss 0.368949    Objective Loss 0.368949                                        LR 0.000060    Time 0.027547    
2023-01-06 14:29:05,847 - Epoch: [12][  100/  246]    Overall Loss 0.367179    Objective Loss 0.367179                                        LR 0.000060    Time 0.027092    
2023-01-06 14:29:06,078 - Epoch: [12][  110/  246]    Overall Loss 0.368768    Objective Loss 0.368768                                        LR 0.000060    Time 0.026721    
2023-01-06 14:29:06,313 - Epoch: [12][  120/  246]    Overall Loss 0.368771    Objective Loss 0.368771                                        LR 0.000060    Time 0.026448    
2023-01-06 14:29:06,542 - Epoch: [12][  130/  246]    Overall Loss 0.368486    Objective Loss 0.368486                                        LR 0.000060    Time 0.026175    
2023-01-06 14:29:06,779 - Epoch: [12][  140/  246]    Overall Loss 0.368302    Objective Loss 0.368302                                        LR 0.000060    Time 0.025993    
2023-01-06 14:29:07,010 - Epoch: [12][  150/  246]    Overall Loss 0.367800    Objective Loss 0.367800                                        LR 0.000060    Time 0.025788    
2023-01-06 14:29:07,234 - Epoch: [12][  160/  246]    Overall Loss 0.366835    Objective Loss 0.366835                                        LR 0.000060    Time 0.025572    
2023-01-06 14:29:07,460 - Epoch: [12][  170/  246]    Overall Loss 0.366652    Objective Loss 0.366652                                        LR 0.000060    Time 0.025396    
2023-01-06 14:29:07,667 - Epoch: [12][  180/  246]    Overall Loss 0.365908    Objective Loss 0.365908                                        LR 0.000060    Time 0.025134    
2023-01-06 14:29:07,877 - Epoch: [12][  190/  246]    Overall Loss 0.366337    Objective Loss 0.366337                                        LR 0.000060    Time 0.024915    
2023-01-06 14:29:08,084 - Epoch: [12][  200/  246]    Overall Loss 0.367214    Objective Loss 0.367214                                        LR 0.000060    Time 0.024704    
2023-01-06 14:29:08,321 - Epoch: [12][  210/  246]    Overall Loss 0.367151    Objective Loss 0.367151                                        LR 0.000060    Time 0.024654    
2023-01-06 14:29:08,562 - Epoch: [12][  220/  246]    Overall Loss 0.367693    Objective Loss 0.367693                                        LR 0.000060    Time 0.024624    
2023-01-06 14:29:08,789 - Epoch: [12][  230/  246]    Overall Loss 0.367325    Objective Loss 0.367325                                        LR 0.000060    Time 0.024540    
2023-01-06 14:29:09,030 - Epoch: [12][  240/  246]    Overall Loss 0.367002    Objective Loss 0.367002                                        LR 0.000060    Time 0.024521    
2023-01-06 14:29:09,156 - Epoch: [12][  246/  246]    Overall Loss 0.367124    Objective Loss 0.367124    Top1 85.645933    LR 0.000060    Time 0.024435    
2023-01-06 14:29:09,293 - --- validate (epoch=12)-----------
2023-01-06 14:29:09,294 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:09,730 - Epoch: [12][   10/   28]    Loss 0.353793    Top1 86.718750    
2023-01-06 14:29:09,868 - Epoch: [12][   20/   28]    Loss 0.354079    Top1 87.070312    
2023-01-06 14:29:09,960 - Epoch: [12][   28/   28]    Loss 0.353697    Top1 87.202977    
2023-01-06 14:29:10,074 - ==> Top1: 87.203    Loss: 0.354

2023-01-06 14:29:10,074 - ==> Confusion:
[[ 153    1  285]
 [   7   92  503]
 [  65   33 5847]]

2023-01-06 14:29:10,076 - ==> Best [Top1: 87.203   Sparsity:0.00   Params: 360896 on epoch: 12]
2023-01-06 14:29:10,076 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:10,097 - 

2023-01-06 14:29:10,097 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:10,820 - Epoch: [13][   10/  246]    Overall Loss 0.372143    Objective Loss 0.372143                                        LR 0.000060    Time 0.072289    
2023-01-06 14:29:11,048 - Epoch: [13][   20/  246]    Overall Loss 0.369647    Objective Loss 0.369647                                        LR 0.000060    Time 0.047511    
2023-01-06 14:29:11,285 - Epoch: [13][   30/  246]    Overall Loss 0.372845    Objective Loss 0.372845                                        LR 0.000060    Time 0.039545    
2023-01-06 14:29:11,512 - Epoch: [13][   40/  246]    Overall Loss 0.371676    Objective Loss 0.371676                                        LR 0.000060    Time 0.035332    
2023-01-06 14:29:11,749 - Epoch: [13][   50/  246]    Overall Loss 0.369342    Objective Loss 0.369342                                        LR 0.000060    Time 0.033000    
2023-01-06 14:29:11,984 - Epoch: [13][   60/  246]    Overall Loss 0.366799    Objective Loss 0.366799                                        LR 0.000060    Time 0.031402    
2023-01-06 14:29:12,209 - Epoch: [13][   70/  246]    Overall Loss 0.362707    Objective Loss 0.362707                                        LR 0.000060    Time 0.030129    
2023-01-06 14:29:12,456 - Epoch: [13][   80/  246]    Overall Loss 0.365008    Objective Loss 0.365008                                        LR 0.000060    Time 0.029453    
2023-01-06 14:29:12,716 - Epoch: [13][   90/  246]    Overall Loss 0.363333    Objective Loss 0.363333                                        LR 0.000060    Time 0.029061    
2023-01-06 14:29:12,985 - Epoch: [13][  100/  246]    Overall Loss 0.362484    Objective Loss 0.362484                                        LR 0.000060    Time 0.028836    
2023-01-06 14:29:13,265 - Epoch: [13][  110/  246]    Overall Loss 0.359546    Objective Loss 0.359546                                        LR 0.000060    Time 0.028754    
2023-01-06 14:29:13,548 - Epoch: [13][  120/  246]    Overall Loss 0.359470    Objective Loss 0.359470                                        LR 0.000060    Time 0.028717    
2023-01-06 14:29:13,819 - Epoch: [13][  130/  246]    Overall Loss 0.359449    Objective Loss 0.359449                                        LR 0.000060    Time 0.028591    
2023-01-06 14:29:14,075 - Epoch: [13][  140/  246]    Overall Loss 0.360429    Objective Loss 0.360429                                        LR 0.000060    Time 0.028368    
2023-01-06 14:29:14,336 - Epoch: [13][  150/  246]    Overall Loss 0.360293    Objective Loss 0.360293                                        LR 0.000060    Time 0.028218    
2023-01-06 14:29:14,606 - Epoch: [13][  160/  246]    Overall Loss 0.359577    Objective Loss 0.359577                                        LR 0.000060    Time 0.028140    
2023-01-06 14:29:14,870 - Epoch: [13][  170/  246]    Overall Loss 0.361105    Objective Loss 0.361105                                        LR 0.000060    Time 0.028030    
2023-01-06 14:29:15,138 - Epoch: [13][  180/  246]    Overall Loss 0.362299    Objective Loss 0.362299                                        LR 0.000060    Time 0.027959    
2023-01-06 14:29:15,399 - Epoch: [13][  190/  246]    Overall Loss 0.362075    Objective Loss 0.362075                                        LR 0.000060    Time 0.027861    
2023-01-06 14:29:15,662 - Epoch: [13][  200/  246]    Overall Loss 0.363512    Objective Loss 0.363512                                        LR 0.000060    Time 0.027780    
2023-01-06 14:29:15,917 - Epoch: [13][  210/  246]    Overall Loss 0.363673    Objective Loss 0.363673                                        LR 0.000060    Time 0.027670    
2023-01-06 14:29:16,170 - Epoch: [13][  220/  246]    Overall Loss 0.363250    Objective Loss 0.363250                                        LR 0.000060    Time 0.027558    
2023-01-06 14:29:16,415 - Epoch: [13][  230/  246]    Overall Loss 0.362733    Objective Loss 0.362733                                        LR 0.000060    Time 0.027423    
2023-01-06 14:29:16,669 - Epoch: [13][  240/  246]    Overall Loss 0.362058    Objective Loss 0.362058                                        LR 0.000060    Time 0.027339    
2023-01-06 14:29:16,798 - Epoch: [13][  246/  246]    Overall Loss 0.362257    Objective Loss 0.362257    Top1 88.516746    LR 0.000060    Time 0.027197    
2023-01-06 14:29:16,918 - --- validate (epoch=13)-----------
2023-01-06 14:29:16,918 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:17,362 - Epoch: [13][   10/   28]    Loss 0.374115    Top1 86.796875    
2023-01-06 14:29:17,503 - Epoch: [13][   20/   28]    Loss 0.361542    Top1 87.382812    
2023-01-06 14:29:17,595 - Epoch: [13][   28/   28]    Loss 0.367090    Top1 87.074148    
2023-01-06 14:29:17,712 - ==> Top1: 87.074    Loss: 0.367

2023-01-06 14:29:17,712 - ==> Confusion:
[[ 153    6  280]
 [  10  148  444]
 [  70   93 5782]]

2023-01-06 14:29:17,714 - ==> Best [Top1: 87.203   Sparsity:0.00   Params: 360896 on epoch: 12]
2023-01-06 14:29:17,714 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:17,723 - 

2023-01-06 14:29:17,724 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:18,338 - Epoch: [14][   10/  246]    Overall Loss 0.347525    Objective Loss 0.347525                                        LR 0.000060    Time 0.061346    
2023-01-06 14:29:18,585 - Epoch: [14][   20/  246]    Overall Loss 0.366427    Objective Loss 0.366427                                        LR 0.000060    Time 0.043022    
2023-01-06 14:29:18,834 - Epoch: [14][   30/  246]    Overall Loss 0.369790    Objective Loss 0.369790                                        LR 0.000060    Time 0.036954    
2023-01-06 14:29:19,077 - Epoch: [14][   40/  246]    Overall Loss 0.369573    Objective Loss 0.369573                                        LR 0.000060    Time 0.033796    
2023-01-06 14:29:19,326 - Epoch: [14][   50/  246]    Overall Loss 0.368461    Objective Loss 0.368461                                        LR 0.000060    Time 0.031971    
2023-01-06 14:29:19,571 - Epoch: [14][   60/  246]    Overall Loss 0.365814    Objective Loss 0.365814                                        LR 0.000060    Time 0.030718    
2023-01-06 14:29:19,823 - Epoch: [14][   70/  246]    Overall Loss 0.365186    Objective Loss 0.365186                                        LR 0.000060    Time 0.029909    
2023-01-06 14:29:20,081 - Epoch: [14][   80/  246]    Overall Loss 0.365798    Objective Loss 0.365798                                        LR 0.000060    Time 0.029380    
2023-01-06 14:29:20,336 - Epoch: [14][   90/  246]    Overall Loss 0.360276    Objective Loss 0.360276                                        LR 0.000060    Time 0.028949    
2023-01-06 14:29:20,593 - Epoch: [14][  100/  246]    Overall Loss 0.361802    Objective Loss 0.361802                                        LR 0.000060    Time 0.028612    
2023-01-06 14:29:20,844 - Epoch: [14][  110/  246]    Overall Loss 0.360580    Objective Loss 0.360580                                        LR 0.000060    Time 0.028297    
2023-01-06 14:29:21,094 - Epoch: [14][  120/  246]    Overall Loss 0.359372    Objective Loss 0.359372                                        LR 0.000060    Time 0.028013    
2023-01-06 14:29:21,344 - Epoch: [14][  130/  246]    Overall Loss 0.357458    Objective Loss 0.357458                                        LR 0.000060    Time 0.027776    
2023-01-06 14:29:21,596 - Epoch: [14][  140/  246]    Overall Loss 0.357000    Objective Loss 0.357000                                        LR 0.000060    Time 0.027588    
2023-01-06 14:29:21,847 - Epoch: [14][  150/  246]    Overall Loss 0.357951    Objective Loss 0.357951                                        LR 0.000060    Time 0.027423    
2023-01-06 14:29:22,100 - Epoch: [14][  160/  246]    Overall Loss 0.355934    Objective Loss 0.355934                                        LR 0.000060    Time 0.027283    
2023-01-06 14:29:22,351 - Epoch: [14][  170/  246]    Overall Loss 0.355925    Objective Loss 0.355925                                        LR 0.000060    Time 0.027154    
2023-01-06 14:29:22,602 - Epoch: [14][  180/  246]    Overall Loss 0.356842    Objective Loss 0.356842                                        LR 0.000060    Time 0.027039    
2023-01-06 14:29:22,854 - Epoch: [14][  190/  246]    Overall Loss 0.358115    Objective Loss 0.358115                                        LR 0.000060    Time 0.026940    
2023-01-06 14:29:23,097 - Epoch: [14][  200/  246]    Overall Loss 0.358082    Objective Loss 0.358082                                        LR 0.000060    Time 0.026803    
2023-01-06 14:29:23,344 - Epoch: [14][  210/  246]    Overall Loss 0.357831    Objective Loss 0.357831                                        LR 0.000060    Time 0.026700    
2023-01-06 14:29:23,587 - Epoch: [14][  220/  246]    Overall Loss 0.357866    Objective Loss 0.357866                                        LR 0.000060    Time 0.026588    
2023-01-06 14:29:23,814 - Epoch: [14][  230/  246]    Overall Loss 0.357931    Objective Loss 0.357931                                        LR 0.000060    Time 0.026419    
2023-01-06 14:29:24,056 - Epoch: [14][  240/  246]    Overall Loss 0.358554    Objective Loss 0.358554                                        LR 0.000060    Time 0.026323    
2023-01-06 14:29:24,187 - Epoch: [14][  246/  246]    Overall Loss 0.358166    Objective Loss 0.358166    Top1 86.363636    LR 0.000060    Time 0.026215    
2023-01-06 14:29:24,325 - --- validate (epoch=14)-----------
2023-01-06 14:29:24,325 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:24,787 - Epoch: [14][   10/   28]    Loss 0.345464    Top1 87.187500    
2023-01-06 14:29:24,929 - Epoch: [14][   20/   28]    Loss 0.364252    Top1 86.835938    
2023-01-06 14:29:25,021 - Epoch: [14][   28/   28]    Loss 0.362185    Top1 86.945319    
2023-01-06 14:29:25,143 - ==> Top1: 86.945    Loss: 0.362

2023-01-06 14:29:25,144 - ==> Confusion:
[[  96    6  337]
 [   6   88  508]
 [  33   22 5890]]

2023-01-06 14:29:25,145 - ==> Best [Top1: 87.203   Sparsity:0.00   Params: 360896 on epoch: 12]
2023-01-06 14:29:25,145 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:25,162 - 

2023-01-06 14:29:25,163 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:25,896 - Epoch: [15][   10/  246]    Overall Loss 0.350847    Objective Loss 0.350847                                        LR 0.000060    Time 0.073232    
2023-01-06 14:29:26,125 - Epoch: [15][   20/  246]    Overall Loss 0.359229    Objective Loss 0.359229                                        LR 0.000060    Time 0.048078    
2023-01-06 14:29:26,371 - Epoch: [15][   30/  246]    Overall Loss 0.361466    Objective Loss 0.361466                                        LR 0.000060    Time 0.040243    
2023-01-06 14:29:26,619 - Epoch: [15][   40/  246]    Overall Loss 0.362748    Objective Loss 0.362748                                        LR 0.000060    Time 0.036366    
2023-01-06 14:29:26,880 - Epoch: [15][   50/  246]    Overall Loss 0.364649    Objective Loss 0.364649                                        LR 0.000060    Time 0.034295    
2023-01-06 14:29:27,107 - Epoch: [15][   60/  246]    Overall Loss 0.365192    Objective Loss 0.365192                                        LR 0.000060    Time 0.032359    
2023-01-06 14:29:27,328 - Epoch: [15][   70/  246]    Overall Loss 0.362990    Objective Loss 0.362990                                        LR 0.000060    Time 0.030881    
2023-01-06 14:29:27,571 - Epoch: [15][   80/  246]    Overall Loss 0.359655    Objective Loss 0.359655                                        LR 0.000060    Time 0.030057    
2023-01-06 14:29:27,812 - Epoch: [15][   90/  246]    Overall Loss 0.360233    Objective Loss 0.360233                                        LR 0.000060    Time 0.029395    
2023-01-06 14:29:28,057 - Epoch: [15][  100/  246]    Overall Loss 0.360150    Objective Loss 0.360150                                        LR 0.000060    Time 0.028899    
2023-01-06 14:29:28,319 - Epoch: [15][  110/  246]    Overall Loss 0.360308    Objective Loss 0.360308                                        LR 0.000060    Time 0.028651    
2023-01-06 14:29:28,580 - Epoch: [15][  120/  246]    Overall Loss 0.360387    Objective Loss 0.360387                                        LR 0.000060    Time 0.028438    
2023-01-06 14:29:28,843 - Epoch: [15][  130/  246]    Overall Loss 0.358936    Objective Loss 0.358936                                        LR 0.000060    Time 0.028263    
2023-01-06 14:29:29,104 - Epoch: [15][  140/  246]    Overall Loss 0.359132    Objective Loss 0.359132                                        LR 0.000060    Time 0.028108    
2023-01-06 14:29:29,365 - Epoch: [15][  150/  246]    Overall Loss 0.359220    Objective Loss 0.359220                                        LR 0.000060    Time 0.027974    
2023-01-06 14:29:29,628 - Epoch: [15][  160/  246]    Overall Loss 0.357274    Objective Loss 0.357274                                        LR 0.000060    Time 0.027861    
2023-01-06 14:29:29,892 - Epoch: [15][  170/  246]    Overall Loss 0.358171    Objective Loss 0.358171                                        LR 0.000060    Time 0.027773    
2023-01-06 14:29:30,110 - Epoch: [15][  180/  246]    Overall Loss 0.357534    Objective Loss 0.357534                                        LR 0.000060    Time 0.027438    
2023-01-06 14:29:30,322 - Epoch: [15][  190/  246]    Overall Loss 0.357940    Objective Loss 0.357940                                        LR 0.000060    Time 0.027108    
2023-01-06 14:29:30,542 - Epoch: [15][  200/  246]    Overall Loss 0.358493    Objective Loss 0.358493                                        LR 0.000060    Time 0.026853    
2023-01-06 14:29:30,775 - Epoch: [15][  210/  246]    Overall Loss 0.357634    Objective Loss 0.357634                                        LR 0.000060    Time 0.026680    
2023-01-06 14:29:30,997 - Epoch: [15][  220/  246]    Overall Loss 0.358438    Objective Loss 0.358438                                        LR 0.000060    Time 0.026476    
2023-01-06 14:29:31,225 - Epoch: [15][  230/  246]    Overall Loss 0.358493    Objective Loss 0.358493                                        LR 0.000060    Time 0.026313    
2023-01-06 14:29:31,455 - Epoch: [15][  240/  246]    Overall Loss 0.357776    Objective Loss 0.357776                                        LR 0.000060    Time 0.026177    
2023-01-06 14:29:31,571 - Epoch: [15][  246/  246]    Overall Loss 0.357828    Objective Loss 0.357828    Top1 89.234450    LR 0.000060    Time 0.026007    
2023-01-06 14:29:31,717 - --- validate (epoch=15)-----------
2023-01-06 14:29:31,717 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:32,166 - Epoch: [15][   10/   28]    Loss 0.357303    Top1 87.382812    
2023-01-06 14:29:32,309 - Epoch: [15][   20/   28]    Loss 0.355908    Top1 86.953125    
2023-01-06 14:29:32,400 - Epoch: [15][   28/   28]    Loss 0.351082    Top1 87.474950    
2023-01-06 14:29:32,541 - ==> Top1: 87.475    Loss: 0.351

2023-01-06 14:29:32,541 - ==> Confusion:
[[ 193    2  244]
 [  14  100  488]
 [  96   31 5818]]

2023-01-06 14:29:32,542 - ==> Best [Top1: 87.475   Sparsity:0.00   Params: 360896 on epoch: 15]
2023-01-06 14:29:32,542 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:32,567 - 

2023-01-06 14:29:32,568 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:33,290 - Epoch: [16][   10/  246]    Overall Loss 0.381019    Objective Loss 0.381019                                        LR 0.000060    Time 0.072141    
2023-01-06 14:29:33,519 - Epoch: [16][   20/  246]    Overall Loss 0.364552    Objective Loss 0.364552                                        LR 0.000060    Time 0.047436    
2023-01-06 14:29:33,744 - Epoch: [16][   30/  246]    Overall Loss 0.358163    Objective Loss 0.358163                                        LR 0.000060    Time 0.039102    
2023-01-06 14:29:33,978 - Epoch: [16][   40/  246]    Overall Loss 0.351872    Objective Loss 0.351872                                        LR 0.000060    Time 0.035179    
2023-01-06 14:29:34,204 - Epoch: [16][   50/  246]    Overall Loss 0.354087    Objective Loss 0.354087                                        LR 0.000060    Time 0.032638    
2023-01-06 14:29:34,423 - Epoch: [16][   60/  246]    Overall Loss 0.351124    Objective Loss 0.351124                                        LR 0.000060    Time 0.030846    
2023-01-06 14:29:34,637 - Epoch: [16][   70/  246]    Overall Loss 0.347034    Objective Loss 0.347034                                        LR 0.000060    Time 0.029499    
2023-01-06 14:29:34,855 - Epoch: [16][   80/  246]    Overall Loss 0.345116    Objective Loss 0.345116                                        LR 0.000060    Time 0.028529    
2023-01-06 14:29:35,079 - Epoch: [16][   90/  246]    Overall Loss 0.347183    Objective Loss 0.347183                                        LR 0.000060    Time 0.027838    
2023-01-06 14:29:35,298 - Epoch: [16][  100/  246]    Overall Loss 0.347448    Objective Loss 0.347448                                        LR 0.000060    Time 0.027242    
2023-01-06 14:29:35,515 - Epoch: [16][  110/  246]    Overall Loss 0.347673    Objective Loss 0.347673                                        LR 0.000060    Time 0.026736    
2023-01-06 14:29:35,734 - Epoch: [16][  120/  246]    Overall Loss 0.348421    Objective Loss 0.348421                                        LR 0.000060    Time 0.026328    
2023-01-06 14:29:35,967 - Epoch: [16][  130/  246]    Overall Loss 0.348518    Objective Loss 0.348518                                        LR 0.000060    Time 0.026093    
2023-01-06 14:29:36,201 - Epoch: [16][  140/  246]    Overall Loss 0.349220    Objective Loss 0.349220                                        LR 0.000060    Time 0.025898    
2023-01-06 14:29:36,445 - Epoch: [16][  150/  246]    Overall Loss 0.349225    Objective Loss 0.349225                                        LR 0.000060    Time 0.025791    
2023-01-06 14:29:36,689 - Epoch: [16][  160/  246]    Overall Loss 0.349930    Objective Loss 0.349930                                        LR 0.000060    Time 0.025695    
2023-01-06 14:29:36,932 - Epoch: [16][  170/  246]    Overall Loss 0.350515    Objective Loss 0.350515                                        LR 0.000060    Time 0.025613    
2023-01-06 14:29:37,176 - Epoch: [16][  180/  246]    Overall Loss 0.351135    Objective Loss 0.351135                                        LR 0.000060    Time 0.025533    
2023-01-06 14:29:37,420 - Epoch: [16][  190/  246]    Overall Loss 0.349881    Objective Loss 0.349881                                        LR 0.000060    Time 0.025461    
2023-01-06 14:29:37,663 - Epoch: [16][  200/  246]    Overall Loss 0.349752    Objective Loss 0.349752                                        LR 0.000060    Time 0.025394    
2023-01-06 14:29:37,906 - Epoch: [16][  210/  246]    Overall Loss 0.349185    Objective Loss 0.349185                                        LR 0.000060    Time 0.025335    
2023-01-06 14:29:38,149 - Epoch: [16][  220/  246]    Overall Loss 0.348853    Objective Loss 0.348853                                        LR 0.000060    Time 0.025279    
2023-01-06 14:29:38,393 - Epoch: [16][  230/  246]    Overall Loss 0.348939    Objective Loss 0.348939                                        LR 0.000060    Time 0.025230    
2023-01-06 14:29:38,650 - Epoch: [16][  240/  246]    Overall Loss 0.350822    Objective Loss 0.350822                                        LR 0.000060    Time 0.025240    
2023-01-06 14:29:38,780 - Epoch: [16][  246/  246]    Overall Loss 0.350790    Objective Loss 0.350790    Top1 84.928230    LR 0.000060    Time 0.025152    
2023-01-06 14:29:38,925 - --- validate (epoch=16)-----------
2023-01-06 14:29:38,925 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:39,367 - Epoch: [16][   10/   28]    Loss 0.352633    Top1 87.968750    
2023-01-06 14:29:39,518 - Epoch: [16][   20/   28]    Loss 0.347782    Top1 88.125000    
2023-01-06 14:29:39,610 - Epoch: [16][   28/   28]    Loss 0.348441    Top1 87.875752    
2023-01-06 14:29:39,736 - ==> Top1: 87.876    Loss: 0.348

2023-01-06 14:29:39,737 - ==> Confusion:
[[ 185    3  251]
 [  11  133  458]
 [  86   38 5821]]

2023-01-06 14:29:39,738 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-06 14:29:39,738 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:39,760 - 

2023-01-06 14:29:39,761 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:40,362 - Epoch: [17][   10/  246]    Overall Loss 0.365778    Objective Loss 0.365778                                        LR 0.000060    Time 0.060048    
2023-01-06 14:29:40,610 - Epoch: [17][   20/  246]    Overall Loss 0.352805    Objective Loss 0.352805                                        LR 0.000060    Time 0.042409    
2023-01-06 14:29:40,865 - Epoch: [17][   30/  246]    Overall Loss 0.352126    Objective Loss 0.352126                                        LR 0.000060    Time 0.036766    
2023-01-06 14:29:41,115 - Epoch: [17][   40/  246]    Overall Loss 0.349226    Objective Loss 0.349226                                        LR 0.000060    Time 0.033821    
2023-01-06 14:29:41,369 - Epoch: [17][   50/  246]    Overall Loss 0.348527    Objective Loss 0.348527                                        LR 0.000060    Time 0.032113    
2023-01-06 14:29:41,618 - Epoch: [17][   60/  246]    Overall Loss 0.348083    Objective Loss 0.348083                                        LR 0.000060    Time 0.030909    
2023-01-06 14:29:41,871 - Epoch: [17][   70/  246]    Overall Loss 0.346792    Objective Loss 0.346792                                        LR 0.000060    Time 0.030111    
2023-01-06 14:29:42,125 - Epoch: [17][   80/  246]    Overall Loss 0.348879    Objective Loss 0.348879                                        LR 0.000060    Time 0.029508    
2023-01-06 14:29:42,377 - Epoch: [17][   90/  246]    Overall Loss 0.347157    Objective Loss 0.347157                                        LR 0.000060    Time 0.029025    
2023-01-06 14:29:42,627 - Epoch: [17][  100/  246]    Overall Loss 0.348013    Objective Loss 0.348013                                        LR 0.000060    Time 0.028620    
2023-01-06 14:29:42,875 - Epoch: [17][  110/  246]    Overall Loss 0.346871    Objective Loss 0.346871                                        LR 0.000060    Time 0.028268    
2023-01-06 14:29:43,120 - Epoch: [17][  120/  246]    Overall Loss 0.346933    Objective Loss 0.346933                                        LR 0.000060    Time 0.027954    
2023-01-06 14:29:43,367 - Epoch: [17][  130/  246]    Overall Loss 0.348742    Objective Loss 0.348742                                        LR 0.000060    Time 0.027698    
2023-01-06 14:29:43,614 - Epoch: [17][  140/  246]    Overall Loss 0.348648    Objective Loss 0.348648                                        LR 0.000060    Time 0.027488    
2023-01-06 14:29:43,862 - Epoch: [17][  150/  246]    Overall Loss 0.348874    Objective Loss 0.348874                                        LR 0.000060    Time 0.027306    
2023-01-06 14:29:44,109 - Epoch: [17][  160/  246]    Overall Loss 0.348634    Objective Loss 0.348634                                        LR 0.000060    Time 0.027139    
2023-01-06 14:29:44,365 - Epoch: [17][  170/  246]    Overall Loss 0.348405    Objective Loss 0.348405                                        LR 0.000060    Time 0.027045    
2023-01-06 14:29:44,620 - Epoch: [17][  180/  246]    Overall Loss 0.347614    Objective Loss 0.347614                                        LR 0.000060    Time 0.026961    
2023-01-06 14:29:44,870 - Epoch: [17][  190/  246]    Overall Loss 0.346226    Objective Loss 0.346226                                        LR 0.000060    Time 0.026855    
2023-01-06 14:29:45,119 - Epoch: [17][  200/  246]    Overall Loss 0.346253    Objective Loss 0.346253                                        LR 0.000060    Time 0.026756    
2023-01-06 14:29:45,368 - Epoch: [17][  210/  246]    Overall Loss 0.345893    Objective Loss 0.345893                                        LR 0.000060    Time 0.026663    
2023-01-06 14:29:45,619 - Epoch: [17][  220/  246]    Overall Loss 0.346221    Objective Loss 0.346221                                        LR 0.000060    Time 0.026588    
2023-01-06 14:29:45,868 - Epoch: [17][  230/  246]    Overall Loss 0.346566    Objective Loss 0.346566                                        LR 0.000060    Time 0.026517    
2023-01-06 14:29:46,127 - Epoch: [17][  240/  246]    Overall Loss 0.346101    Objective Loss 0.346101                                        LR 0.000060    Time 0.026490    
2023-01-06 14:29:46,259 - Epoch: [17][  246/  246]    Overall Loss 0.345774    Objective Loss 0.345774    Top1 88.038278    LR 0.000060    Time 0.026377    
2023-01-06 14:29:46,372 - --- validate (epoch=17)-----------
2023-01-06 14:29:46,372 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:46,814 - Epoch: [17][   10/   28]    Loss 0.333857    Top1 88.046875    
2023-01-06 14:29:46,955 - Epoch: [17][   20/   28]    Loss 0.349072    Top1 87.324219    
2023-01-06 14:29:47,047 - Epoch: [17][   28/   28]    Loss 0.345515    Top1 87.517893    
2023-01-06 14:29:47,192 - ==> Top1: 87.518    Loss: 0.346

2023-01-06 14:29:47,193 - ==> Confusion:
[[ 196    1  242]
 [  14   92  496]
 [  94   25 5826]]

2023-01-06 14:29:47,194 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-06 14:29:47,194 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:47,204 - 

2023-01-06 14:29:47,204 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:47,907 - Epoch: [18][   10/  246]    Overall Loss 0.334538    Objective Loss 0.334538                                        LR 0.000060    Time 0.070286    
2023-01-06 14:29:48,116 - Epoch: [18][   20/  246]    Overall Loss 0.331674    Objective Loss 0.331674                                        LR 0.000060    Time 0.045533    
2023-01-06 14:29:48,335 - Epoch: [18][   30/  246]    Overall Loss 0.332625    Objective Loss 0.332625                                        LR 0.000060    Time 0.037626    
2023-01-06 14:29:48,553 - Epoch: [18][   40/  246]    Overall Loss 0.334643    Objective Loss 0.334643                                        LR 0.000060    Time 0.033660    
2023-01-06 14:29:48,771 - Epoch: [18][   50/  246]    Overall Loss 0.336791    Objective Loss 0.336791                                        LR 0.000060    Time 0.031278    
2023-01-06 14:29:48,988 - Epoch: [18][   60/  246]    Overall Loss 0.338487    Objective Loss 0.338487                                        LR 0.000060    Time 0.029655    
2023-01-06 14:29:49,205 - Epoch: [18][   70/  246]    Overall Loss 0.339758    Objective Loss 0.339758                                        LR 0.000060    Time 0.028512    
2023-01-06 14:29:49,425 - Epoch: [18][   80/  246]    Overall Loss 0.341987    Objective Loss 0.341987                                        LR 0.000060    Time 0.027696    
2023-01-06 14:29:49,641 - Epoch: [18][   90/  246]    Overall Loss 0.342957    Objective Loss 0.342957                                        LR 0.000060    Time 0.027016    
2023-01-06 14:29:49,859 - Epoch: [18][  100/  246]    Overall Loss 0.343741    Objective Loss 0.343741                                        LR 0.000060    Time 0.026482    
2023-01-06 14:29:50,075 - Epoch: [18][  110/  246]    Overall Loss 0.346652    Objective Loss 0.346652                                        LR 0.000060    Time 0.026036    
2023-01-06 14:29:50,290 - Epoch: [18][  120/  246]    Overall Loss 0.346615    Objective Loss 0.346615                                        LR 0.000060    Time 0.025659    
2023-01-06 14:29:50,505 - Epoch: [18][  130/  246]    Overall Loss 0.347080    Objective Loss 0.347080                                        LR 0.000060    Time 0.025335    
2023-01-06 14:29:50,721 - Epoch: [18][  140/  246]    Overall Loss 0.345526    Objective Loss 0.345526                                        LR 0.000060    Time 0.025067    
2023-01-06 14:29:50,940 - Epoch: [18][  150/  246]    Overall Loss 0.345437    Objective Loss 0.345437                                        LR 0.000060    Time 0.024849    
2023-01-06 14:29:51,173 - Epoch: [18][  160/  246]    Overall Loss 0.346167    Objective Loss 0.346167                                        LR 0.000060    Time 0.024752    
2023-01-06 14:29:51,401 - Epoch: [18][  170/  246]    Overall Loss 0.345837    Objective Loss 0.345837                                        LR 0.000060    Time 0.024633    
2023-01-06 14:29:51,629 - Epoch: [18][  180/  246]    Overall Loss 0.344791    Objective Loss 0.344791                                        LR 0.000060    Time 0.024528    
2023-01-06 14:29:51,858 - Epoch: [18][  190/  246]    Overall Loss 0.344676    Objective Loss 0.344676                                        LR 0.000060    Time 0.024439    
2023-01-06 14:29:52,089 - Epoch: [18][  200/  246]    Overall Loss 0.345214    Objective Loss 0.345214                                        LR 0.000060    Time 0.024371    
2023-01-06 14:29:52,323 - Epoch: [18][  210/  246]    Overall Loss 0.346082    Objective Loss 0.346082                                        LR 0.000060    Time 0.024322    
2023-01-06 14:29:52,557 - Epoch: [18][  220/  246]    Overall Loss 0.346901    Objective Loss 0.346901                                        LR 0.000060    Time 0.024279    
2023-01-06 14:29:52,788 - Epoch: [18][  230/  246]    Overall Loss 0.347321    Objective Loss 0.347321                                        LR 0.000060    Time 0.024219    
2023-01-06 14:29:53,028 - Epoch: [18][  240/  246]    Overall Loss 0.346605    Objective Loss 0.346605                                        LR 0.000060    Time 0.024207    
2023-01-06 14:29:53,150 - Epoch: [18][  246/  246]    Overall Loss 0.346403    Objective Loss 0.346403    Top1 90.669856    LR 0.000060    Time 0.024114    
2023-01-06 14:29:53,268 - --- validate (epoch=18)-----------
2023-01-06 14:29:53,268 - 6986 samples (256 per mini-batch)
2023-01-06 14:29:53,724 - Epoch: [18][   10/   28]    Loss 0.335604    Top1 88.007812    
2023-01-06 14:29:53,866 - Epoch: [18][   20/   28]    Loss 0.339292    Top1 87.871094    
2023-01-06 14:29:53,958 - Epoch: [18][   28/   28]    Loss 0.340659    Top1 87.789865    
2023-01-06 14:29:54,126 - ==> Top1: 87.790    Loss: 0.341

2023-01-06 14:29:54,126 - ==> Confusion:
[[ 168    8  263]
 [  10  128  464]
 [  69   39 5837]]

2023-01-06 14:29:54,128 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-06 14:29:54,128 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:29:54,149 - 

2023-01-06 14:29:54,149 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:29:54,746 - Epoch: [19][   10/  246]    Overall Loss 0.334823    Objective Loss 0.334823                                        LR 0.000060    Time 0.059589    
2023-01-06 14:29:54,984 - Epoch: [19][   20/  246]    Overall Loss 0.345094    Objective Loss 0.345094                                        LR 0.000060    Time 0.041640    
2023-01-06 14:29:55,231 - Epoch: [19][   30/  246]    Overall Loss 0.336678    Objective Loss 0.336678                                        LR 0.000060    Time 0.035988    
2023-01-06 14:29:55,465 - Epoch: [19][   40/  246]    Overall Loss 0.338783    Objective Loss 0.338783                                        LR 0.000060    Time 0.032834    
2023-01-06 14:29:55,689 - Epoch: [19][   50/  246]    Overall Loss 0.337750    Objective Loss 0.337750                                        LR 0.000060    Time 0.030734    
2023-01-06 14:29:55,920 - Epoch: [19][   60/  246]    Overall Loss 0.340902    Objective Loss 0.340902                                        LR 0.000060    Time 0.029461    
2023-01-06 14:29:56,162 - Epoch: [19][   70/  246]    Overall Loss 0.338499    Objective Loss 0.338499                                        LR 0.000060    Time 0.028704    
2023-01-06 14:29:56,406 - Epoch: [19][   80/  246]    Overall Loss 0.341571    Objective Loss 0.341571                                        LR 0.000060    Time 0.028159    
2023-01-06 14:29:56,634 - Epoch: [19][   90/  246]    Overall Loss 0.341042    Objective Loss 0.341042                                        LR 0.000060    Time 0.027556    
2023-01-06 14:29:56,874 - Epoch: [19][  100/  246]    Overall Loss 0.342871    Objective Loss 0.342871                                        LR 0.000060    Time 0.027195    
2023-01-06 14:29:57,113 - Epoch: [19][  110/  246]    Overall Loss 0.345101    Objective Loss 0.345101                                        LR 0.000060    Time 0.026893    
2023-01-06 14:29:57,346 - Epoch: [19][  120/  246]    Overall Loss 0.344686    Objective Loss 0.344686                                        LR 0.000060    Time 0.026588    
2023-01-06 14:29:57,583 - Epoch: [19][  130/  246]    Overall Loss 0.344926    Objective Loss 0.344926                                        LR 0.000060    Time 0.026360    
2023-01-06 14:29:57,815 - Epoch: [19][  140/  246]    Overall Loss 0.344917    Objective Loss 0.344917                                        LR 0.000060    Time 0.026134    
2023-01-06 14:29:58,048 - Epoch: [19][  150/  246]    Overall Loss 0.345775    Objective Loss 0.345775                                        LR 0.000060    Time 0.025944    
2023-01-06 14:29:58,285 - Epoch: [19][  160/  246]    Overall Loss 0.345464    Objective Loss 0.345464                                        LR 0.000060    Time 0.025797    
2023-01-06 14:29:58,528 - Epoch: [19][  170/  246]    Overall Loss 0.344440    Objective Loss 0.344440                                        LR 0.000060    Time 0.025707    
2023-01-06 14:29:58,750 - Epoch: [19][  180/  246]    Overall Loss 0.342522    Objective Loss 0.342522                                        LR 0.000060    Time 0.025514    
2023-01-06 14:29:58,981 - Epoch: [19][  190/  246]    Overall Loss 0.343028    Objective Loss 0.343028                                        LR 0.000060    Time 0.025383    
2023-01-06 14:29:59,212 - Epoch: [19][  200/  246]    Overall Loss 0.342177    Objective Loss 0.342177                                        LR 0.000060    Time 0.025265    
2023-01-06 14:29:59,445 - Epoch: [19][  210/  246]    Overall Loss 0.342948    Objective Loss 0.342948                                        LR 0.000060    Time 0.025173    
2023-01-06 14:29:59,683 - Epoch: [19][  220/  246]    Overall Loss 0.341805    Objective Loss 0.341805                                        LR 0.000060    Time 0.025102    
2023-01-06 14:29:59,918 - Epoch: [19][  230/  246]    Overall Loss 0.342092    Objective Loss 0.342092                                        LR 0.000060    Time 0.025029    
2023-01-06 14:30:00,149 - Epoch: [19][  240/  246]    Overall Loss 0.342867    Objective Loss 0.342867                                        LR 0.000060    Time 0.024947    
2023-01-06 14:30:00,271 - Epoch: [19][  246/  246]    Overall Loss 0.343053    Objective Loss 0.343053    Top1 89.234450    LR 0.000060    Time 0.024837    
2023-01-06 14:30:00,396 - --- validate (epoch=19)-----------
2023-01-06 14:30:00,397 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:00,843 - Epoch: [19][   10/   28]    Loss 0.400123    Top1 87.421875    
2023-01-06 14:30:00,985 - Epoch: [19][   20/   28]    Loss 0.405489    Top1 87.343750    
2023-01-06 14:30:01,075 - Epoch: [19][   28/   28]    Loss 0.403648    Top1 87.317492    
2023-01-06 14:30:01,220 - ==> Top1: 87.317    Loss: 0.404

2023-01-06 14:30:01,220 - ==> Confusion:
[[ 120    1  318]
 [   7   91  504]
 [  33   23 5889]]

2023-01-06 14:30:01,221 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-06 14:30:01,222 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:01,231 - 

2023-01-06 14:30:01,231 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:01,963 - Epoch: [20][   10/  246]    Overall Loss 0.394291    Objective Loss 0.394291                                        LR 0.000060    Time 0.073085    
2023-01-06 14:30:02,195 - Epoch: [20][   20/  246]    Overall Loss 0.375344    Objective Loss 0.375344                                        LR 0.000060    Time 0.048121    
2023-01-06 14:30:02,419 - Epoch: [20][   30/  246]    Overall Loss 0.378965    Objective Loss 0.378965                                        LR 0.000060    Time 0.039536    
2023-01-06 14:30:02,672 - Epoch: [20][   40/  246]    Overall Loss 0.374855    Objective Loss 0.374855                                        LR 0.000060    Time 0.035971    
2023-01-06 14:30:02,928 - Epoch: [20][   50/  246]    Overall Loss 0.373734    Objective Loss 0.373734                                        LR 0.000060    Time 0.033879    
2023-01-06 14:30:03,185 - Epoch: [20][   60/  246]    Overall Loss 0.370363    Objective Loss 0.370363                                        LR 0.000060    Time 0.032519    
2023-01-06 14:30:03,446 - Epoch: [20][   70/  246]    Overall Loss 0.369421    Objective Loss 0.369421                                        LR 0.000060    Time 0.031594    
2023-01-06 14:30:03,703 - Epoch: [20][   80/  246]    Overall Loss 0.368059    Objective Loss 0.368059                                        LR 0.000060    Time 0.030851    
2023-01-06 14:30:03,960 - Epoch: [20][   90/  246]    Overall Loss 0.365341    Objective Loss 0.365341                                        LR 0.000060    Time 0.030257    
2023-01-06 14:30:04,216 - Epoch: [20][  100/  246]    Overall Loss 0.364633    Objective Loss 0.364633                                        LR 0.000060    Time 0.029792    
2023-01-06 14:30:04,475 - Epoch: [20][  110/  246]    Overall Loss 0.363586    Objective Loss 0.363586                                        LR 0.000060    Time 0.029425    
2023-01-06 14:30:04,731 - Epoch: [20][  120/  246]    Overall Loss 0.362087    Objective Loss 0.362087                                        LR 0.000060    Time 0.029100    
2023-01-06 14:30:04,986 - Epoch: [20][  130/  246]    Overall Loss 0.360896    Objective Loss 0.360896                                        LR 0.000060    Time 0.028815    
2023-01-06 14:30:05,243 - Epoch: [20][  140/  246]    Overall Loss 0.360457    Objective Loss 0.360457                                        LR 0.000060    Time 0.028587    
2023-01-06 14:30:05,501 - Epoch: [20][  150/  246]    Overall Loss 0.358562    Objective Loss 0.358562                                        LR 0.000060    Time 0.028393    
2023-01-06 14:30:05,759 - Epoch: [20][  160/  246]    Overall Loss 0.358805    Objective Loss 0.358805                                        LR 0.000060    Time 0.028226    
2023-01-06 14:30:06,018 - Epoch: [20][  170/  246]    Overall Loss 0.357779    Objective Loss 0.357779                                        LR 0.000060    Time 0.028081    
2023-01-06 14:30:06,274 - Epoch: [20][  180/  246]    Overall Loss 0.357202    Objective Loss 0.357202                                        LR 0.000060    Time 0.027937    
2023-01-06 14:30:06,531 - Epoch: [20][  190/  246]    Overall Loss 0.356804    Objective Loss 0.356804                                        LR 0.000060    Time 0.027814    
2023-01-06 14:30:06,787 - Epoch: [20][  200/  246]    Overall Loss 0.356600    Objective Loss 0.356600                                        LR 0.000060    Time 0.027695    
2023-01-06 14:30:07,052 - Epoch: [20][  210/  246]    Overall Loss 0.356327    Objective Loss 0.356327                                        LR 0.000060    Time 0.027637    
2023-01-06 14:30:07,320 - Epoch: [20][  220/  246]    Overall Loss 0.356049    Objective Loss 0.356049                                        LR 0.000060    Time 0.027596    
2023-01-06 14:30:07,589 - Epoch: [20][  230/  246]    Overall Loss 0.355759    Objective Loss 0.355759                                        LR 0.000060    Time 0.027557    
2023-01-06 14:30:07,859 - Epoch: [20][  240/  246]    Overall Loss 0.354913    Objective Loss 0.354913                                        LR 0.000060    Time 0.027533    
2023-01-06 14:30:07,987 - Epoch: [20][  246/  246]    Overall Loss 0.354571    Objective Loss 0.354571    Top1 88.038278    LR 0.000060    Time 0.027382    
2023-01-06 14:30:08,125 - --- validate (epoch=20)-----------
2023-01-06 14:30:08,125 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:08,583 - Epoch: [20][   10/   28]    Loss 0.345254    Top1 87.851562    
2023-01-06 14:30:08,724 - Epoch: [20][   20/   28]    Loss 0.341348    Top1 87.734375    
2023-01-06 14:30:08,816 - Epoch: [20][   28/   28]    Loss 0.342625    Top1 87.775551    
2023-01-06 14:30:08,954 - ==> Top1: 87.776    Loss: 0.343

2023-01-06 14:30:08,954 - ==> Confusion:
[[ 135    9  295]
 [   7  120  475]
 [  36   32 5877]]

2023-01-06 14:30:08,955 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-06 14:30:08,956 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:08,972 - 

2023-01-06 14:30:08,972 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:09,706 - Epoch: [21][   10/  246]    Overall Loss 0.361664    Objective Loss 0.361664                                        LR 0.000060    Time 0.073247    
2023-01-06 14:30:09,948 - Epoch: [21][   20/  246]    Overall Loss 0.352001    Objective Loss 0.352001                                        LR 0.000060    Time 0.048710    
2023-01-06 14:30:10,197 - Epoch: [21][   30/  246]    Overall Loss 0.352203    Objective Loss 0.352203                                        LR 0.000060    Time 0.040751    
2023-01-06 14:30:10,438 - Epoch: [21][   40/  246]    Overall Loss 0.348415    Objective Loss 0.348415                                        LR 0.000060    Time 0.036588    
2023-01-06 14:30:10,677 - Epoch: [21][   50/  246]    Overall Loss 0.347020    Objective Loss 0.347020                                        LR 0.000060    Time 0.034043    
2023-01-06 14:30:10,915 - Epoch: [21][   60/  246]    Overall Loss 0.346753    Objective Loss 0.346753                                        LR 0.000060    Time 0.032333    
2023-01-06 14:30:11,151 - Epoch: [21][   70/  246]    Overall Loss 0.347324    Objective Loss 0.347324                                        LR 0.000060    Time 0.031078    
2023-01-06 14:30:11,389 - Epoch: [21][   80/  246]    Overall Loss 0.345436    Objective Loss 0.345436                                        LR 0.000060    Time 0.030154    
2023-01-06 14:30:11,636 - Epoch: [21][   90/  246]    Overall Loss 0.345277    Objective Loss 0.345277                                        LR 0.000060    Time 0.029543    
2023-01-06 14:30:11,905 - Epoch: [21][  100/  246]    Overall Loss 0.344529    Objective Loss 0.344529                                        LR 0.000060    Time 0.029282    
2023-01-06 14:30:12,171 - Epoch: [21][  110/  246]    Overall Loss 0.342894    Objective Loss 0.342894                                        LR 0.000060    Time 0.029034    
2023-01-06 14:30:12,439 - Epoch: [21][  120/  246]    Overall Loss 0.343021    Objective Loss 0.343021                                        LR 0.000060    Time 0.028844    
2023-01-06 14:30:12,702 - Epoch: [21][  130/  246]    Overall Loss 0.344160    Objective Loss 0.344160                                        LR 0.000060    Time 0.028644    
2023-01-06 14:30:12,952 - Epoch: [21][  140/  246]    Overall Loss 0.342689    Objective Loss 0.342689                                        LR 0.000060    Time 0.028380    
2023-01-06 14:30:13,198 - Epoch: [21][  150/  246]    Overall Loss 0.343796    Objective Loss 0.343796                                        LR 0.000060    Time 0.028121    
2023-01-06 14:30:13,447 - Epoch: [21][  160/  246]    Overall Loss 0.344487    Objective Loss 0.344487                                        LR 0.000060    Time 0.027919    
2023-01-06 14:30:13,694 - Epoch: [21][  170/  246]    Overall Loss 0.345259    Objective Loss 0.345259                                        LR 0.000060    Time 0.027724    
2023-01-06 14:30:13,945 - Epoch: [21][  180/  246]    Overall Loss 0.345203    Objective Loss 0.345203                                        LR 0.000060    Time 0.027577    
2023-01-06 14:30:14,192 - Epoch: [21][  190/  246]    Overall Loss 0.345342    Objective Loss 0.345342                                        LR 0.000060    Time 0.027423    
2023-01-06 14:30:14,445 - Epoch: [21][  200/  246]    Overall Loss 0.345962    Objective Loss 0.345962                                        LR 0.000060    Time 0.027314    
2023-01-06 14:30:14,694 - Epoch: [21][  210/  246]    Overall Loss 0.345815    Objective Loss 0.345815                                        LR 0.000060    Time 0.027199    
2023-01-06 14:30:14,940 - Epoch: [21][  220/  246]    Overall Loss 0.345348    Objective Loss 0.345348                                        LR 0.000060    Time 0.027078    
2023-01-06 14:30:15,181 - Epoch: [21][  230/  246]    Overall Loss 0.344435    Objective Loss 0.344435                                        LR 0.000060    Time 0.026944    
2023-01-06 14:30:15,434 - Epoch: [21][  240/  246]    Overall Loss 0.343748    Objective Loss 0.343748                                        LR 0.000060    Time 0.026874    
2023-01-06 14:30:15,564 - Epoch: [21][  246/  246]    Overall Loss 0.343761    Objective Loss 0.343761    Top1 86.124402    LR 0.000060    Time 0.026746    
2023-01-06 14:30:15,701 - --- validate (epoch=21)-----------
2023-01-06 14:30:15,702 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:16,153 - Epoch: [21][   10/   28]    Loss 0.346127    Top1 87.382812    
2023-01-06 14:30:16,300 - Epoch: [21][   20/   28]    Loss 0.336703    Top1 87.890625    
2023-01-06 14:30:16,390 - Epoch: [21][   28/   28]    Loss 0.337268    Top1 87.961638    
2023-01-06 14:30:16,517 - ==> Top1: 87.962    Loss: 0.337

2023-01-06 14:30:16,517 - ==> Confusion:
[[ 190    5  244]
 [  15  129  458]
 [  86   33 5826]]

2023-01-06 14:30:16,519 - ==> Best [Top1: 87.962   Sparsity:0.00   Params: 360896 on epoch: 21]
2023-01-06 14:30:16,519 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:16,543 - 

2023-01-06 14:30:16,543 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:17,144 - Epoch: [22][   10/  246]    Overall Loss 0.347894    Objective Loss 0.347894                                        LR 0.000060    Time 0.059951    
2023-01-06 14:30:17,379 - Epoch: [22][   20/  246]    Overall Loss 0.337566    Objective Loss 0.337566                                        LR 0.000060    Time 0.041709    
2023-01-06 14:30:17,620 - Epoch: [22][   30/  246]    Overall Loss 0.343511    Objective Loss 0.343511                                        LR 0.000060    Time 0.035819    
2023-01-06 14:30:17,858 - Epoch: [22][   40/  246]    Overall Loss 0.341764    Objective Loss 0.341764                                        LR 0.000060    Time 0.032817    
2023-01-06 14:30:18,101 - Epoch: [22][   50/  246]    Overall Loss 0.341778    Objective Loss 0.341778                                        LR 0.000060    Time 0.031100    
2023-01-06 14:30:18,342 - Epoch: [22][   60/  246]    Overall Loss 0.341296    Objective Loss 0.341296                                        LR 0.000060    Time 0.029924    
2023-01-06 14:30:18,584 - Epoch: [22][   70/  246]    Overall Loss 0.343289    Objective Loss 0.343289                                        LR 0.000060    Time 0.029080    
2023-01-06 14:30:18,824 - Epoch: [22][   80/  246]    Overall Loss 0.340192    Objective Loss 0.340192                                        LR 0.000060    Time 0.028446    
2023-01-06 14:30:19,066 - Epoch: [22][   90/  246]    Overall Loss 0.341293    Objective Loss 0.341293                                        LR 0.000060    Time 0.027950    
2023-01-06 14:30:19,307 - Epoch: [22][  100/  246]    Overall Loss 0.342361    Objective Loss 0.342361                                        LR 0.000060    Time 0.027561    
2023-01-06 14:30:19,548 - Epoch: [22][  110/  246]    Overall Loss 0.342471    Objective Loss 0.342471                                        LR 0.000060    Time 0.027235    
2023-01-06 14:30:19,789 - Epoch: [22][  120/  246]    Overall Loss 0.339831    Objective Loss 0.339831                                        LR 0.000060    Time 0.026970    
2023-01-06 14:30:20,029 - Epoch: [22][  130/  246]    Overall Loss 0.338889    Objective Loss 0.338889                                        LR 0.000060    Time 0.026730    
2023-01-06 14:30:20,271 - Epoch: [22][  140/  246]    Overall Loss 0.339955    Objective Loss 0.339955                                        LR 0.000060    Time 0.026539    
2023-01-06 14:30:20,511 - Epoch: [22][  150/  246]    Overall Loss 0.340464    Objective Loss 0.340464                                        LR 0.000060    Time 0.026358    
2023-01-06 14:30:20,751 - Epoch: [22][  160/  246]    Overall Loss 0.339349    Objective Loss 0.339349                                        LR 0.000060    Time 0.026202    
2023-01-06 14:30:20,992 - Epoch: [22][  170/  246]    Overall Loss 0.339356    Objective Loss 0.339356                                        LR 0.000060    Time 0.026068    
2023-01-06 14:30:21,233 - Epoch: [22][  180/  246]    Overall Loss 0.339007    Objective Loss 0.339007                                        LR 0.000060    Time 0.025953    
2023-01-06 14:30:21,476 - Epoch: [22][  190/  246]    Overall Loss 0.338413    Objective Loss 0.338413                                        LR 0.000060    Time 0.025867    
2023-01-06 14:30:21,716 - Epoch: [22][  200/  246]    Overall Loss 0.338566    Objective Loss 0.338566                                        LR 0.000060    Time 0.025772    
2023-01-06 14:30:21,959 - Epoch: [22][  210/  246]    Overall Loss 0.339952    Objective Loss 0.339952                                        LR 0.000060    Time 0.025692    
2023-01-06 14:30:22,198 - Epoch: [22][  220/  246]    Overall Loss 0.339620    Objective Loss 0.339620                                        LR 0.000060    Time 0.025608    
2023-01-06 14:30:22,437 - Epoch: [22][  230/  246]    Overall Loss 0.338257    Objective Loss 0.338257                                        LR 0.000060    Time 0.025528    
2023-01-06 14:30:22,691 - Epoch: [22][  240/  246]    Overall Loss 0.338433    Objective Loss 0.338433                                        LR 0.000060    Time 0.025516    
2023-01-06 14:30:22,810 - Epoch: [22][  246/  246]    Overall Loss 0.337868    Objective Loss 0.337868    Top1 87.320574    LR 0.000060    Time 0.025377    
2023-01-06 14:30:22,955 - --- validate (epoch=22)-----------
2023-01-06 14:30:22,955 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:23,393 - Epoch: [22][   10/   28]    Loss 0.329240    Top1 88.007812    
2023-01-06 14:30:23,538 - Epoch: [22][   20/   28]    Loss 0.329498    Top1 88.339844    
2023-01-06 14:30:23,630 - Epoch: [22][   28/   28]    Loss 0.330252    Top1 88.262239    
2023-01-06 14:30:23,765 - ==> Top1: 88.262    Loss: 0.330

2023-01-06 14:30:23,765 - ==> Confusion:
[[ 176    6  257]
 [  12  171  419]
 [  51   75 5819]]

2023-01-06 14:30:23,766 - ==> Best [Top1: 88.262   Sparsity:0.00   Params: 360896 on epoch: 22]
2023-01-06 14:30:23,766 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:23,791 - 

2023-01-06 14:30:23,791 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:24,542 - Epoch: [23][   10/  246]    Overall Loss 0.320195    Objective Loss 0.320195                                        LR 0.000060    Time 0.075038    
2023-01-06 14:30:24,789 - Epoch: [23][   20/  246]    Overall Loss 0.336097    Objective Loss 0.336097                                        LR 0.000060    Time 0.049818    
2023-01-06 14:30:25,037 - Epoch: [23][   30/  246]    Overall Loss 0.339398    Objective Loss 0.339398                                        LR 0.000060    Time 0.041469    
2023-01-06 14:30:25,297 - Epoch: [23][   40/  246]    Overall Loss 0.337504    Objective Loss 0.337504                                        LR 0.000060    Time 0.037589    
2023-01-06 14:30:25,550 - Epoch: [23][   50/  246]    Overall Loss 0.334569    Objective Loss 0.334569                                        LR 0.000060    Time 0.035125    
2023-01-06 14:30:25,790 - Epoch: [23][   60/  246]    Overall Loss 0.335816    Objective Loss 0.335816                                        LR 0.000060    Time 0.033266    
2023-01-06 14:30:26,021 - Epoch: [23][   70/  246]    Overall Loss 0.336530    Objective Loss 0.336530                                        LR 0.000060    Time 0.031806    
2023-01-06 14:30:26,277 - Epoch: [23][   80/  246]    Overall Loss 0.338739    Objective Loss 0.338739                                        LR 0.000060    Time 0.031033    
2023-01-06 14:30:26,527 - Epoch: [23][   90/  246]    Overall Loss 0.337033    Objective Loss 0.337033                                        LR 0.000060    Time 0.030351    
2023-01-06 14:30:26,764 - Epoch: [23][  100/  246]    Overall Loss 0.339385    Objective Loss 0.339385                                        LR 0.000060    Time 0.029689    
2023-01-06 14:30:27,007 - Epoch: [23][  110/  246]    Overall Loss 0.337222    Objective Loss 0.337222                                        LR 0.000060    Time 0.029197    
2023-01-06 14:30:27,264 - Epoch: [23][  120/  246]    Overall Loss 0.337018    Objective Loss 0.337018                                        LR 0.000060    Time 0.028900    
2023-01-06 14:30:27,515 - Epoch: [23][  130/  246]    Overall Loss 0.339278    Objective Loss 0.339278                                        LR 0.000060    Time 0.028606    
2023-01-06 14:30:27,768 - Epoch: [23][  140/  246]    Overall Loss 0.337509    Objective Loss 0.337509                                        LR 0.000060    Time 0.028362    
2023-01-06 14:30:28,013 - Epoch: [23][  150/  246]    Overall Loss 0.336664    Objective Loss 0.336664                                        LR 0.000060    Time 0.028099    
2023-01-06 14:30:28,266 - Epoch: [23][  160/  246]    Overall Loss 0.336475    Objective Loss 0.336475                                        LR 0.000060    Time 0.027918    
2023-01-06 14:30:28,520 - Epoch: [23][  170/  246]    Overall Loss 0.336822    Objective Loss 0.336822                                        LR 0.000060    Time 0.027770    
2023-01-06 14:30:28,774 - Epoch: [23][  180/  246]    Overall Loss 0.336867    Objective Loss 0.336867                                        LR 0.000060    Time 0.027633    
2023-01-06 14:30:29,023 - Epoch: [23][  190/  246]    Overall Loss 0.337589    Objective Loss 0.337589                                        LR 0.000060    Time 0.027482    
2023-01-06 14:30:29,293 - Epoch: [23][  200/  246]    Overall Loss 0.337465    Objective Loss 0.337465                                        LR 0.000060    Time 0.027458    
2023-01-06 14:30:29,564 - Epoch: [23][  210/  246]    Overall Loss 0.337718    Objective Loss 0.337718                                        LR 0.000060    Time 0.027440    
2023-01-06 14:30:29,832 - Epoch: [23][  220/  246]    Overall Loss 0.336574    Objective Loss 0.336574                                        LR 0.000060    Time 0.027408    
2023-01-06 14:30:30,092 - Epoch: [23][  230/  246]    Overall Loss 0.336389    Objective Loss 0.336389                                        LR 0.000060    Time 0.027347    
2023-01-06 14:30:30,358 - Epoch: [23][  240/  246]    Overall Loss 0.336022    Objective Loss 0.336022                                        LR 0.000060    Time 0.027312    
2023-01-06 14:30:30,485 - Epoch: [23][  246/  246]    Overall Loss 0.335915    Objective Loss 0.335915    Top1 88.038278    LR 0.000060    Time 0.027164    
2023-01-06 14:30:30,613 - --- validate (epoch=23)-----------
2023-01-06 14:30:30,613 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:31,068 - Epoch: [23][   10/   28]    Loss 0.335766    Top1 88.476562    
2023-01-06 14:30:31,221 - Epoch: [23][   20/   28]    Loss 0.332280    Top1 88.535156    
2023-01-06 14:30:31,312 - Epoch: [23][   28/   28]    Loss 0.337048    Top1 88.262239    
2023-01-06 14:30:31,444 - ==> Top1: 88.262    Loss: 0.337

2023-01-06 14:30:31,445 - ==> Confusion:
[[ 179    2  258]
 [  15  153  434]
 [  60   51 5834]]

2023-01-06 14:30:31,446 - ==> Best [Top1: 88.262   Sparsity:0.00   Params: 360896 on epoch: 23]
2023-01-06 14:30:31,446 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:31,471 - 

2023-01-06 14:30:31,471 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:32,072 - Epoch: [24][   10/  246]    Overall Loss 0.330706    Objective Loss 0.330706                                        LR 0.000060    Time 0.059973    
2023-01-06 14:30:32,319 - Epoch: [24][   20/  246]    Overall Loss 0.326259    Objective Loss 0.326259                                        LR 0.000060    Time 0.042327    
2023-01-06 14:30:32,567 - Epoch: [24][   30/  246]    Overall Loss 0.327415    Objective Loss 0.327415                                        LR 0.000060    Time 0.036469    
2023-01-06 14:30:32,814 - Epoch: [24][   40/  246]    Overall Loss 0.330778    Objective Loss 0.330778                                        LR 0.000060    Time 0.033505    
2023-01-06 14:30:33,060 - Epoch: [24][   50/  246]    Overall Loss 0.331561    Objective Loss 0.331561                                        LR 0.000060    Time 0.031720    
2023-01-06 14:30:33,309 - Epoch: [24][   60/  246]    Overall Loss 0.330547    Objective Loss 0.330547                                        LR 0.000060    Time 0.030576    
2023-01-06 14:30:33,559 - Epoch: [24][   70/  246]    Overall Loss 0.331067    Objective Loss 0.331067                                        LR 0.000060    Time 0.029776    
2023-01-06 14:30:33,809 - Epoch: [24][   80/  246]    Overall Loss 0.332552    Objective Loss 0.332552                                        LR 0.000060    Time 0.029165    
2023-01-06 14:30:34,053 - Epoch: [24][   90/  246]    Overall Loss 0.336537    Objective Loss 0.336537                                        LR 0.000060    Time 0.028627    
2023-01-06 14:30:34,301 - Epoch: [24][  100/  246]    Overall Loss 0.335708    Objective Loss 0.335708                                        LR 0.000060    Time 0.028246    
2023-01-06 14:30:34,551 - Epoch: [24][  110/  246]    Overall Loss 0.332949    Objective Loss 0.332949                                        LR 0.000060    Time 0.027940    
2023-01-06 14:30:34,799 - Epoch: [24][  120/  246]    Overall Loss 0.334472    Objective Loss 0.334472                                        LR 0.000060    Time 0.027676    
2023-01-06 14:30:35,040 - Epoch: [24][  130/  246]    Overall Loss 0.335139    Objective Loss 0.335139                                        LR 0.000060    Time 0.027400    
2023-01-06 14:30:35,281 - Epoch: [24][  140/  246]    Overall Loss 0.334628    Objective Loss 0.334628                                        LR 0.000060    Time 0.027156    
2023-01-06 14:30:35,521 - Epoch: [24][  150/  246]    Overall Loss 0.334048    Objective Loss 0.334048                                        LR 0.000060    Time 0.026941    
2023-01-06 14:30:35,760 - Epoch: [24][  160/  246]    Overall Loss 0.333795    Objective Loss 0.333795                                        LR 0.000060    Time 0.026749    
2023-01-06 14:30:36,000 - Epoch: [24][  170/  246]    Overall Loss 0.332805    Objective Loss 0.332805                                        LR 0.000060    Time 0.026586    
2023-01-06 14:30:36,265 - Epoch: [24][  180/  246]    Overall Loss 0.334537    Objective Loss 0.334537                                        LR 0.000060    Time 0.026581    
2023-01-06 14:30:36,524 - Epoch: [24][  190/  246]    Overall Loss 0.334975    Objective Loss 0.334975                                        LR 0.000060    Time 0.026539    
2023-01-06 14:30:36,775 - Epoch: [24][  200/  246]    Overall Loss 0.334018    Objective Loss 0.334018                                        LR 0.000060    Time 0.026464    
2023-01-06 14:30:37,021 - Epoch: [24][  210/  246]    Overall Loss 0.333643    Objective Loss 0.333643                                        LR 0.000060    Time 0.026375    
2023-01-06 14:30:37,273 - Epoch: [24][  220/  246]    Overall Loss 0.334137    Objective Loss 0.334137                                        LR 0.000060    Time 0.026319    
2023-01-06 14:30:37,530 - Epoch: [24][  230/  246]    Overall Loss 0.334530    Objective Loss 0.334530                                        LR 0.000060    Time 0.026293    
2023-01-06 14:30:37,797 - Epoch: [24][  240/  246]    Overall Loss 0.334178    Objective Loss 0.334178                                        LR 0.000060    Time 0.026306    
2023-01-06 14:30:37,926 - Epoch: [24][  246/  246]    Overall Loss 0.333578    Objective Loss 0.333578    Top1 90.191388    LR 0.000060    Time 0.026190    
2023-01-06 14:30:38,051 - --- validate (epoch=24)-----------
2023-01-06 14:30:38,052 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:38,491 - Epoch: [24][   10/   28]    Loss 0.344502    Top1 87.304688    
2023-01-06 14:30:38,639 - Epoch: [24][   20/   28]    Loss 0.325553    Top1 88.203125    
2023-01-06 14:30:38,731 - Epoch: [24][   28/   28]    Loss 0.321579    Top1 88.319496    
2023-01-06 14:30:38,879 - ==> Top1: 88.319    Loss: 0.322

2023-01-06 14:30:38,879 - ==> Confusion:
[[ 147   10  282]
 [   7  159  436]
 [  39   42 5864]]

2023-01-06 14:30:38,881 - ==> Best [Top1: 88.319   Sparsity:0.00   Params: 360896 on epoch: 24]
2023-01-06 14:30:38,881 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:38,906 - 

2023-01-06 14:30:38,906 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:39,625 - Epoch: [25][   10/  246]    Overall Loss 0.328895    Objective Loss 0.328895                                        LR 0.000060    Time 0.071822    
2023-01-06 14:30:39,871 - Epoch: [25][   20/  246]    Overall Loss 0.341707    Objective Loss 0.341707                                        LR 0.000060    Time 0.048227    
2023-01-06 14:30:40,122 - Epoch: [25][   30/  246]    Overall Loss 0.340458    Objective Loss 0.340458                                        LR 0.000060    Time 0.040502    
2023-01-06 14:30:40,372 - Epoch: [25][   40/  246]    Overall Loss 0.335514    Objective Loss 0.335514                                        LR 0.000060    Time 0.036599    
2023-01-06 14:30:40,618 - Epoch: [25][   50/  246]    Overall Loss 0.334284    Objective Loss 0.334284                                        LR 0.000060    Time 0.034204    
2023-01-06 14:30:40,866 - Epoch: [25][   60/  246]    Overall Loss 0.334277    Objective Loss 0.334277                                        LR 0.000060    Time 0.032622    
2023-01-06 14:30:41,112 - Epoch: [25][   70/  246]    Overall Loss 0.333653    Objective Loss 0.333653                                        LR 0.000060    Time 0.031479    
2023-01-06 14:30:41,358 - Epoch: [25][   80/  246]    Overall Loss 0.335863    Objective Loss 0.335863                                        LR 0.000060    Time 0.030606    
2023-01-06 14:30:41,603 - Epoch: [25][   90/  246]    Overall Loss 0.337078    Objective Loss 0.337078                                        LR 0.000060    Time 0.029920    
2023-01-06 14:30:41,850 - Epoch: [25][  100/  246]    Overall Loss 0.336572    Objective Loss 0.336572                                        LR 0.000060    Time 0.029393    
2023-01-06 14:30:42,104 - Epoch: [25][  110/  246]    Overall Loss 0.335379    Objective Loss 0.335379                                        LR 0.000060    Time 0.029024    
2023-01-06 14:30:42,358 - Epoch: [25][  120/  246]    Overall Loss 0.334839    Objective Loss 0.334839                                        LR 0.000060    Time 0.028719    
2023-01-06 14:30:42,610 - Epoch: [25][  130/  246]    Overall Loss 0.334074    Objective Loss 0.334074                                        LR 0.000060    Time 0.028450    
2023-01-06 14:30:42,865 - Epoch: [25][  140/  246]    Overall Loss 0.332998    Objective Loss 0.332998                                        LR 0.000060    Time 0.028236    
2023-01-06 14:30:43,123 - Epoch: [25][  150/  246]    Overall Loss 0.333840    Objective Loss 0.333840                                        LR 0.000060    Time 0.028068    
2023-01-06 14:30:43,376 - Epoch: [25][  160/  246]    Overall Loss 0.334363    Objective Loss 0.334363                                        LR 0.000060    Time 0.027891    
2023-01-06 14:30:43,634 - Epoch: [25][  170/  246]    Overall Loss 0.336028    Objective Loss 0.336028                                        LR 0.000060    Time 0.027768    
2023-01-06 14:30:43,889 - Epoch: [25][  180/  246]    Overall Loss 0.336954    Objective Loss 0.336954                                        LR 0.000060    Time 0.027637    
2023-01-06 14:30:44,146 - Epoch: [25][  190/  246]    Overall Loss 0.336732    Objective Loss 0.336732                                        LR 0.000060    Time 0.027532    
2023-01-06 14:30:44,399 - Epoch: [25][  200/  246]    Overall Loss 0.334953    Objective Loss 0.334953                                        LR 0.000060    Time 0.027418    
2023-01-06 14:30:44,655 - Epoch: [25][  210/  246]    Overall Loss 0.335458    Objective Loss 0.335458                                        LR 0.000060    Time 0.027331    
2023-01-06 14:30:44,912 - Epoch: [25][  220/  246]    Overall Loss 0.335145    Objective Loss 0.335145                                        LR 0.000060    Time 0.027253    
2023-01-06 14:30:45,177 - Epoch: [25][  230/  246]    Overall Loss 0.333096    Objective Loss 0.333096                                        LR 0.000060    Time 0.027221    
2023-01-06 14:30:45,443 - Epoch: [25][  240/  246]    Overall Loss 0.333198    Objective Loss 0.333198                                        LR 0.000060    Time 0.027192    
2023-01-06 14:30:45,573 - Epoch: [25][  246/  246]    Overall Loss 0.332828    Objective Loss 0.332828    Top1 88.277512    LR 0.000060    Time 0.027058    
2023-01-06 14:30:45,718 - --- validate (epoch=25)-----------
2023-01-06 14:30:45,719 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:46,159 - Epoch: [25][   10/   28]    Loss 0.337768    Top1 87.890625    
2023-01-06 14:30:46,299 - Epoch: [25][   20/   28]    Loss 0.334380    Top1 88.066406    
2023-01-06 14:30:46,393 - Epoch: [25][   28/   28]    Loss 0.324135    Top1 88.290867    
2023-01-06 14:30:46,533 - ==> Top1: 88.291    Loss: 0.324

2023-01-06 14:30:46,533 - ==> Confusion:
[[ 201    5  233]
 [  15  129  458]
 [  81   26 5838]]

2023-01-06 14:30:46,534 - ==> Best [Top1: 88.319   Sparsity:0.00   Params: 360896 on epoch: 24]
2023-01-06 14:30:46,534 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:46,544 - 

2023-01-06 14:30:46,544 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:47,300 - Epoch: [26][   10/  246]    Overall Loss 0.331894    Objective Loss 0.331894                                        LR 0.000060    Time 0.075545    
2023-01-06 14:30:47,544 - Epoch: [26][   20/  246]    Overall Loss 0.323380    Objective Loss 0.323380                                        LR 0.000060    Time 0.049913    
2023-01-06 14:30:47,789 - Epoch: [26][   30/  246]    Overall Loss 0.324592    Objective Loss 0.324592                                        LR 0.000060    Time 0.041442    
2023-01-06 14:30:48,036 - Epoch: [26][   40/  246]    Overall Loss 0.323621    Objective Loss 0.323621                                        LR 0.000060    Time 0.037246    
2023-01-06 14:30:48,282 - Epoch: [26][   50/  246]    Overall Loss 0.322632    Objective Loss 0.322632                                        LR 0.000060    Time 0.034701    
2023-01-06 14:30:48,532 - Epoch: [26][   60/  246]    Overall Loss 0.319062    Objective Loss 0.319062                                        LR 0.000060    Time 0.033082    
2023-01-06 14:30:48,777 - Epoch: [26][   70/  246]    Overall Loss 0.320751    Objective Loss 0.320751                                        LR 0.000060    Time 0.031845    
2023-01-06 14:30:49,021 - Epoch: [26][   80/  246]    Overall Loss 0.322020    Objective Loss 0.322020                                        LR 0.000060    Time 0.030908    
2023-01-06 14:30:49,262 - Epoch: [26][   90/  246]    Overall Loss 0.324373    Objective Loss 0.324373                                        LR 0.000060    Time 0.030154    
2023-01-06 14:30:49,504 - Epoch: [26][  100/  246]    Overall Loss 0.324801    Objective Loss 0.324801                                        LR 0.000060    Time 0.029556    
2023-01-06 14:30:49,746 - Epoch: [26][  110/  246]    Overall Loss 0.323892    Objective Loss 0.323892                                        LR 0.000060    Time 0.029067    
2023-01-06 14:30:49,987 - Epoch: [26][  120/  246]    Overall Loss 0.324800    Objective Loss 0.324800                                        LR 0.000060    Time 0.028651    
2023-01-06 14:30:50,229 - Epoch: [26][  130/  246]    Overall Loss 0.322921    Objective Loss 0.322921                                        LR 0.000060    Time 0.028306    
2023-01-06 14:30:50,469 - Epoch: [26][  140/  246]    Overall Loss 0.323393    Objective Loss 0.323393                                        LR 0.000060    Time 0.027993    
2023-01-06 14:30:50,709 - Epoch: [26][  150/  246]    Overall Loss 0.323795    Objective Loss 0.323795                                        LR 0.000060    Time 0.027725    
2023-01-06 14:30:50,950 - Epoch: [26][  160/  246]    Overall Loss 0.323954    Objective Loss 0.323954                                        LR 0.000060    Time 0.027495    
2023-01-06 14:30:51,193 - Epoch: [26][  170/  246]    Overall Loss 0.322770    Objective Loss 0.322770                                        LR 0.000060    Time 0.027307    
2023-01-06 14:30:51,434 - Epoch: [26][  180/  246]    Overall Loss 0.323223    Objective Loss 0.323223                                        LR 0.000060    Time 0.027129    
2023-01-06 14:30:51,674 - Epoch: [26][  190/  246]    Overall Loss 0.322640    Objective Loss 0.322640                                        LR 0.000060    Time 0.026964    
2023-01-06 14:30:51,916 - Epoch: [26][  200/  246]    Overall Loss 0.324351    Objective Loss 0.324351                                        LR 0.000060    Time 0.026823    
2023-01-06 14:30:52,164 - Epoch: [26][  210/  246]    Overall Loss 0.324807    Objective Loss 0.324807                                        LR 0.000060    Time 0.026726    
2023-01-06 14:30:52,421 - Epoch: [26][  220/  246]    Overall Loss 0.325443    Objective Loss 0.325443                                        LR 0.000060    Time 0.026676    
2023-01-06 14:30:52,687 - Epoch: [26][  230/  246]    Overall Loss 0.325738    Objective Loss 0.325738                                        LR 0.000060    Time 0.026670    
2023-01-06 14:30:52,954 - Epoch: [26][  240/  246]    Overall Loss 0.325857    Objective Loss 0.325857                                        LR 0.000060    Time 0.026672    
2023-01-06 14:30:53,084 - Epoch: [26][  246/  246]    Overall Loss 0.325669    Objective Loss 0.325669    Top1 87.799043    LR 0.000060    Time 0.026549    
2023-01-06 14:30:53,217 - --- validate (epoch=26)-----------
2023-01-06 14:30:53,218 - 6986 samples (256 per mini-batch)
2023-01-06 14:30:53,670 - Epoch: [26][   10/   28]    Loss 0.318016    Top1 88.125000    
2023-01-06 14:30:53,811 - Epoch: [26][   20/   28]    Loss 0.326834    Top1 88.066406    
2023-01-06 14:30:53,903 - Epoch: [26][   28/   28]    Loss 0.330745    Top1 88.176353    
2023-01-06 14:30:54,046 - ==> Top1: 88.176    Loss: 0.331

2023-01-06 14:30:54,047 - ==> Confusion:
[[ 145   10  284]
 [   9  163  430]
 [  42   51 5852]]

2023-01-06 14:30:54,048 - ==> Best [Top1: 88.319   Sparsity:0.00   Params: 360896 on epoch: 24]
2023-01-06 14:30:54,048 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:30:54,065 - 

2023-01-06 14:30:54,065 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:30:54,671 - Epoch: [27][   10/  246]    Overall Loss 0.332072    Objective Loss 0.332072                                        LR 0.000060    Time 0.060466    
2023-01-06 14:30:54,917 - Epoch: [27][   20/  246]    Overall Loss 0.318829    Objective Loss 0.318829                                        LR 0.000060    Time 0.042524    
2023-01-06 14:30:55,159 - Epoch: [27][   30/  246]    Overall Loss 0.324299    Objective Loss 0.324299                                        LR 0.000060    Time 0.036396    
2023-01-06 14:30:55,401 - Epoch: [27][   40/  246]    Overall Loss 0.321493    Objective Loss 0.321493                                        LR 0.000060    Time 0.033351    
2023-01-06 14:30:55,644 - Epoch: [27][   50/  246]    Overall Loss 0.322186    Objective Loss 0.322186                                        LR 0.000060    Time 0.031525    
2023-01-06 14:30:55,886 - Epoch: [27][   60/  246]    Overall Loss 0.320242    Objective Loss 0.320242                                        LR 0.000060    Time 0.030295    
2023-01-06 14:30:56,129 - Epoch: [27][   70/  246]    Overall Loss 0.321493    Objective Loss 0.321493                                        LR 0.000060    Time 0.029438    
2023-01-06 14:30:56,373 - Epoch: [27][   80/  246]    Overall Loss 0.321062    Objective Loss 0.321062                                        LR 0.000060    Time 0.028792    
2023-01-06 14:30:56,632 - Epoch: [27][   90/  246]    Overall Loss 0.319693    Objective Loss 0.319693                                        LR 0.000060    Time 0.028474    
2023-01-06 14:30:56,880 - Epoch: [27][  100/  246]    Overall Loss 0.318629    Objective Loss 0.318629                                        LR 0.000060    Time 0.028099    
2023-01-06 14:30:57,123 - Epoch: [27][  110/  246]    Overall Loss 0.321167    Objective Loss 0.321167                                        LR 0.000060    Time 0.027746    
2023-01-06 14:30:57,362 - Epoch: [27][  120/  246]    Overall Loss 0.323165    Objective Loss 0.323165                                        LR 0.000060    Time 0.027422    
2023-01-06 14:30:57,601 - Epoch: [27][  130/  246]    Overall Loss 0.324059    Objective Loss 0.324059                                        LR 0.000060    Time 0.027151    
2023-01-06 14:30:57,828 - Epoch: [27][  140/  246]    Overall Loss 0.323881    Objective Loss 0.323881                                        LR 0.000060    Time 0.026833    
2023-01-06 14:30:58,048 - Epoch: [27][  150/  246]    Overall Loss 0.323430    Objective Loss 0.323430                                        LR 0.000060    Time 0.026504    
2023-01-06 14:30:58,275 - Epoch: [27][  160/  246]    Overall Loss 0.322758    Objective Loss 0.322758                                        LR 0.000060    Time 0.026267    
2023-01-06 14:30:58,494 - Epoch: [27][  170/  246]    Overall Loss 0.323139    Objective Loss 0.323139                                        LR 0.000060    Time 0.026008    
2023-01-06 14:30:58,721 - Epoch: [27][  180/  246]    Overall Loss 0.322453    Objective Loss 0.322453                                        LR 0.000060    Time 0.025818    
2023-01-06 14:30:58,941 - Epoch: [27][  190/  246]    Overall Loss 0.323159    Objective Loss 0.323159                                        LR 0.000060    Time 0.025619    
2023-01-06 14:30:59,171 - Epoch: [27][  200/  246]    Overall Loss 0.322408    Objective Loss 0.322408                                        LR 0.000060    Time 0.025483    
2023-01-06 14:30:59,410 - Epoch: [27][  210/  246]    Overall Loss 0.323695    Objective Loss 0.323695                                        LR 0.000060    Time 0.025408    
2023-01-06 14:30:59,643 - Epoch: [27][  220/  246]    Overall Loss 0.323227    Objective Loss 0.323227                                        LR 0.000060    Time 0.025306    
2023-01-06 14:30:59,892 - Epoch: [27][  230/  246]    Overall Loss 0.322865    Objective Loss 0.322865                                        LR 0.000060    Time 0.025283    
2023-01-06 14:31:00,127 - Epoch: [27][  240/  246]    Overall Loss 0.323372    Objective Loss 0.323372                                        LR 0.000060    Time 0.025205    
2023-01-06 14:31:00,256 - Epoch: [27][  246/  246]    Overall Loss 0.323488    Objective Loss 0.323488    Top1 86.602871    LR 0.000060    Time 0.025112    
2023-01-06 14:31:00,414 - --- validate (epoch=27)-----------
2023-01-06 14:31:00,415 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:00,860 - Epoch: [27][   10/   28]    Loss 0.309262    Top1 89.218750    
2023-01-06 14:31:01,011 - Epoch: [27][   20/   28]    Loss 0.319329    Top1 88.710938    
2023-01-06 14:31:01,102 - Epoch: [27][   28/   28]    Loss 0.321151    Top1 88.591469    
2023-01-06 14:31:01,236 - ==> Top1: 88.591    Loss: 0.321

2023-01-06 14:31:01,236 - ==> Confusion:
[[ 216    7  216]
 [  19  209  374]
 [ 100   81 5764]]

2023-01-06 14:31:01,238 - ==> Best [Top1: 88.591   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-06 14:31:01,238 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:01,262 - 

2023-01-06 14:31:01,263 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:01,972 - Epoch: [28][   10/  246]    Overall Loss 0.317492    Objective Loss 0.317492                                        LR 0.000060    Time 0.070861    
2023-01-06 14:31:02,197 - Epoch: [28][   20/  246]    Overall Loss 0.307436    Objective Loss 0.307436                                        LR 0.000060    Time 0.046665    
2023-01-06 14:31:02,427 - Epoch: [28][   30/  246]    Overall Loss 0.313211    Objective Loss 0.313211                                        LR 0.000060    Time 0.038769    
2023-01-06 14:31:02,665 - Epoch: [28][   40/  246]    Overall Loss 0.312060    Objective Loss 0.312060                                        LR 0.000060    Time 0.035019    
2023-01-06 14:31:02,900 - Epoch: [28][   50/  246]    Overall Loss 0.311428    Objective Loss 0.311428                                        LR 0.000060    Time 0.032711    
2023-01-06 14:31:03,134 - Epoch: [28][   60/  246]    Overall Loss 0.314383    Objective Loss 0.314383                                        LR 0.000060    Time 0.031147    
2023-01-06 14:31:03,372 - Epoch: [28][   70/  246]    Overall Loss 0.316664    Objective Loss 0.316664                                        LR 0.000060    Time 0.030097    
2023-01-06 14:31:03,603 - Epoch: [28][   80/  246]    Overall Loss 0.316544    Objective Loss 0.316544                                        LR 0.000060    Time 0.029215    
2023-01-06 14:31:03,833 - Epoch: [28][   90/  246]    Overall Loss 0.318939    Objective Loss 0.318939                                        LR 0.000060    Time 0.028515    
2023-01-06 14:31:04,074 - Epoch: [28][  100/  246]    Overall Loss 0.320317    Objective Loss 0.320317                                        LR 0.000060    Time 0.028073    
2023-01-06 14:31:04,321 - Epoch: [28][  110/  246]    Overall Loss 0.320396    Objective Loss 0.320396                                        LR 0.000060    Time 0.027763    
2023-01-06 14:31:04,563 - Epoch: [28][  120/  246]    Overall Loss 0.319265    Objective Loss 0.319265                                        LR 0.000060    Time 0.027465    
2023-01-06 14:31:04,809 - Epoch: [28][  130/  246]    Overall Loss 0.317606    Objective Loss 0.317606                                        LR 0.000060    Time 0.027238    
2023-01-06 14:31:05,052 - Epoch: [28][  140/  246]    Overall Loss 0.318982    Objective Loss 0.318982                                        LR 0.000060    Time 0.027030    
2023-01-06 14:31:05,296 - Epoch: [28][  150/  246]    Overall Loss 0.319117    Objective Loss 0.319117                                        LR 0.000060    Time 0.026852    
2023-01-06 14:31:05,539 - Epoch: [28][  160/  246]    Overall Loss 0.318505    Objective Loss 0.318505                                        LR 0.000060    Time 0.026688    
2023-01-06 14:31:05,785 - Epoch: [28][  170/  246]    Overall Loss 0.319408    Objective Loss 0.319408                                        LR 0.000060    Time 0.026565    
2023-01-06 14:31:06,034 - Epoch: [28][  180/  246]    Overall Loss 0.319137    Objective Loss 0.319137                                        LR 0.000060    Time 0.026470    
2023-01-06 14:31:06,286 - Epoch: [28][  190/  246]    Overall Loss 0.319634    Objective Loss 0.319634                                        LR 0.000060    Time 0.026397    
2023-01-06 14:31:06,535 - Epoch: [28][  200/  246]    Overall Loss 0.320232    Objective Loss 0.320232                                        LR 0.000060    Time 0.026325    
2023-01-06 14:31:06,787 - Epoch: [28][  210/  246]    Overall Loss 0.321480    Objective Loss 0.321480                                        LR 0.000060    Time 0.026269    
2023-01-06 14:31:07,032 - Epoch: [28][  220/  246]    Overall Loss 0.321179    Objective Loss 0.321179                                        LR 0.000060    Time 0.026183    
2023-01-06 14:31:07,283 - Epoch: [28][  230/  246]    Overall Loss 0.321378    Objective Loss 0.321378                                        LR 0.000060    Time 0.026135    
2023-01-06 14:31:07,545 - Epoch: [28][  240/  246]    Overall Loss 0.321487    Objective Loss 0.321487                                        LR 0.000060    Time 0.026139    
2023-01-06 14:31:07,673 - Epoch: [28][  246/  246]    Overall Loss 0.321379    Objective Loss 0.321379    Top1 88.995215    LR 0.000060    Time 0.026021    
2023-01-06 14:31:07,820 - --- validate (epoch=28)-----------
2023-01-06 14:31:07,820 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:08,273 - Epoch: [28][   10/   28]    Loss 0.356794    Top1 87.343750    
2023-01-06 14:31:08,421 - Epoch: [28][   20/   28]    Loss 0.336602    Top1 87.929688    
2023-01-06 14:31:08,511 - Epoch: [28][   28/   28]    Loss 0.333926    Top1 87.975952    
2023-01-06 14:31:08,661 - ==> Top1: 87.976    Loss: 0.334

2023-01-06 14:31:08,662 - ==> Confusion:
[[ 195    3  241]
 [  18  115  469]
 [  90   19 5836]]

2023-01-06 14:31:08,663 - ==> Best [Top1: 88.591   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-06 14:31:08,663 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:08,680 - 

2023-01-06 14:31:08,680 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:09,259 - Epoch: [29][   10/  246]    Overall Loss 0.324626    Objective Loss 0.324626                                        LR 0.000060    Time 0.057854    
2023-01-06 14:31:09,508 - Epoch: [29][   20/  246]    Overall Loss 0.317346    Objective Loss 0.317346                                        LR 0.000060    Time 0.041349    
2023-01-06 14:31:09,754 - Epoch: [29][   30/  246]    Overall Loss 0.319085    Objective Loss 0.319085                                        LR 0.000060    Time 0.035754    
2023-01-06 14:31:10,002 - Epoch: [29][   40/  246]    Overall Loss 0.317970    Objective Loss 0.317970                                        LR 0.000060    Time 0.033003    
2023-01-06 14:31:10,250 - Epoch: [29][   50/  246]    Overall Loss 0.321308    Objective Loss 0.321308                                        LR 0.000060    Time 0.031357    
2023-01-06 14:31:10,498 - Epoch: [29][   60/  246]    Overall Loss 0.318963    Objective Loss 0.318963                                        LR 0.000060    Time 0.030257    
2023-01-06 14:31:10,744 - Epoch: [29][   70/  246]    Overall Loss 0.320950    Objective Loss 0.320950                                        LR 0.000060    Time 0.029442    
2023-01-06 14:31:10,988 - Epoch: [29][   80/  246]    Overall Loss 0.320018    Objective Loss 0.320018                                        LR 0.000060    Time 0.028807    
2023-01-06 14:31:11,231 - Epoch: [29][   90/  246]    Overall Loss 0.318092    Objective Loss 0.318092                                        LR 0.000060    Time 0.028303    
2023-01-06 14:31:11,476 - Epoch: [29][  100/  246]    Overall Loss 0.317509    Objective Loss 0.317509                                        LR 0.000060    Time 0.027911    
2023-01-06 14:31:11,723 - Epoch: [29][  110/  246]    Overall Loss 0.318582    Objective Loss 0.318582                                        LR 0.000060    Time 0.027622    
2023-01-06 14:31:11,979 - Epoch: [29][  120/  246]    Overall Loss 0.316795    Objective Loss 0.316795                                        LR 0.000060    Time 0.027447    
2023-01-06 14:31:12,231 - Epoch: [29][  130/  246]    Overall Loss 0.315916    Objective Loss 0.315916                                        LR 0.000060    Time 0.027274    
2023-01-06 14:31:12,486 - Epoch: [29][  140/  246]    Overall Loss 0.315408    Objective Loss 0.315408                                        LR 0.000060    Time 0.027144    
2023-01-06 14:31:12,739 - Epoch: [29][  150/  246]    Overall Loss 0.315632    Objective Loss 0.315632                                        LR 0.000060    Time 0.027012    
2023-01-06 14:31:12,995 - Epoch: [29][  160/  246]    Overall Loss 0.316383    Objective Loss 0.316383                                        LR 0.000060    Time 0.026923    
2023-01-06 14:31:13,247 - Epoch: [29][  170/  246]    Overall Loss 0.316636    Objective Loss 0.316636                                        LR 0.000060    Time 0.026821    
2023-01-06 14:31:13,503 - Epoch: [29][  180/  246]    Overall Loss 0.317526    Objective Loss 0.317526                                        LR 0.000060    Time 0.026752    
2023-01-06 14:31:13,756 - Epoch: [29][  190/  246]    Overall Loss 0.318246    Objective Loss 0.318246                                        LR 0.000060    Time 0.026669    
2023-01-06 14:31:14,012 - Epoch: [29][  200/  246]    Overall Loss 0.318493    Objective Loss 0.318493                                        LR 0.000060    Time 0.026613    
2023-01-06 14:31:14,263 - Epoch: [29][  210/  246]    Overall Loss 0.319285    Objective Loss 0.319285                                        LR 0.000060    Time 0.026540    
2023-01-06 14:31:14,510 - Epoch: [29][  220/  246]    Overall Loss 0.319533    Objective Loss 0.319533                                        LR 0.000060    Time 0.026456    
2023-01-06 14:31:14,760 - Epoch: [29][  230/  246]    Overall Loss 0.320330    Objective Loss 0.320330                                        LR 0.000060    Time 0.026391    
2023-01-06 14:31:15,020 - Epoch: [29][  240/  246]    Overall Loss 0.320601    Objective Loss 0.320601                                        LR 0.000060    Time 0.026374    
2023-01-06 14:31:15,151 - Epoch: [29][  246/  246]    Overall Loss 0.320079    Objective Loss 0.320079    Top1 88.995215    LR 0.000060    Time 0.026260    
2023-01-06 14:31:15,272 - --- validate (epoch=29)-----------
2023-01-06 14:31:15,272 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:15,722 - Epoch: [29][   10/   28]    Loss 0.310665    Top1 88.398438    
2023-01-06 14:31:15,866 - Epoch: [29][   20/   28]    Loss 0.313844    Top1 88.554688    
2023-01-06 14:31:15,958 - Epoch: [29][   28/   28]    Loss 0.317994    Top1 88.505583    
2023-01-06 14:31:16,097 - ==> Top1: 88.506    Loss: 0.318

2023-01-06 14:31:16,098 - ==> Confusion:
[[ 207    3  229]
 [  21  152  429]
 [  84   37 5824]]

2023-01-06 14:31:16,100 - ==> Best [Top1: 88.591   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-06 14:31:16,100 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:16,114 - 

2023-01-06 14:31:16,114 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:16,854 - Epoch: [30][   10/  246]    Overall Loss 0.314461    Objective Loss 0.314461                                        LR 0.000060    Time 0.073891    
2023-01-06 14:31:17,103 - Epoch: [30][   20/  246]    Overall Loss 0.321051    Objective Loss 0.321051                                        LR 0.000060    Time 0.049414    
2023-01-06 14:31:17,367 - Epoch: [30][   30/  246]    Overall Loss 0.311193    Objective Loss 0.311193                                        LR 0.000060    Time 0.041712    
2023-01-06 14:31:17,631 - Epoch: [30][   40/  246]    Overall Loss 0.317129    Objective Loss 0.317129                                        LR 0.000060    Time 0.037877    
2023-01-06 14:31:17,894 - Epoch: [30][   50/  246]    Overall Loss 0.321762    Objective Loss 0.321762                                        LR 0.000060    Time 0.035515    
2023-01-06 14:31:18,135 - Epoch: [30][   60/  246]    Overall Loss 0.322828    Objective Loss 0.322828                                        LR 0.000060    Time 0.033609    
2023-01-06 14:31:18,361 - Epoch: [30][   70/  246]    Overall Loss 0.321671    Objective Loss 0.321671                                        LR 0.000060    Time 0.032021    
2023-01-06 14:31:18,597 - Epoch: [30][   80/  246]    Overall Loss 0.323161    Objective Loss 0.323161                                        LR 0.000060    Time 0.030967    
2023-01-06 14:31:18,819 - Epoch: [30][   90/  246]    Overall Loss 0.322971    Objective Loss 0.322971                                        LR 0.000060    Time 0.029987    
2023-01-06 14:31:19,049 - Epoch: [30][  100/  246]    Overall Loss 0.320676    Objective Loss 0.320676                                        LR 0.000060    Time 0.029285    
2023-01-06 14:31:19,283 - Epoch: [30][  110/  246]    Overall Loss 0.322676    Objective Loss 0.322676                                        LR 0.000060    Time 0.028744    
2023-01-06 14:31:19,510 - Epoch: [30][  120/  246]    Overall Loss 0.320288    Objective Loss 0.320288                                        LR 0.000060    Time 0.028232    
2023-01-06 14:31:19,747 - Epoch: [30][  130/  246]    Overall Loss 0.318492    Objective Loss 0.318492                                        LR 0.000060    Time 0.027866    
2023-01-06 14:31:20,004 - Epoch: [30][  140/  246]    Overall Loss 0.317496    Objective Loss 0.317496                                        LR 0.000060    Time 0.027706    
2023-01-06 14:31:20,258 - Epoch: [30][  150/  246]    Overall Loss 0.317533    Objective Loss 0.317533                                        LR 0.000060    Time 0.027550    
2023-01-06 14:31:20,505 - Epoch: [30][  160/  246]    Overall Loss 0.317532    Objective Loss 0.317532                                        LR 0.000060    Time 0.027366    
2023-01-06 14:31:20,755 - Epoch: [30][  170/  246]    Overall Loss 0.317431    Objective Loss 0.317431                                        LR 0.000060    Time 0.027228    
2023-01-06 14:31:21,004 - Epoch: [30][  180/  246]    Overall Loss 0.316843    Objective Loss 0.316843                                        LR 0.000060    Time 0.027089    
2023-01-06 14:31:21,237 - Epoch: [30][  190/  246]    Overall Loss 0.316934    Objective Loss 0.316934                                        LR 0.000060    Time 0.026881    
2023-01-06 14:31:21,453 - Epoch: [30][  200/  246]    Overall Loss 0.318028    Objective Loss 0.318028                                        LR 0.000060    Time 0.026616    
2023-01-06 14:31:21,671 - Epoch: [30][  210/  246]    Overall Loss 0.317993    Objective Loss 0.317993                                        LR 0.000060    Time 0.026383    
2023-01-06 14:31:21,885 - Epoch: [30][  220/  246]    Overall Loss 0.316841    Objective Loss 0.316841                                        LR 0.000060    Time 0.026156    
2023-01-06 14:31:22,105 - Epoch: [30][  230/  246]    Overall Loss 0.316514    Objective Loss 0.316514                                        LR 0.000060    Time 0.025972    
2023-01-06 14:31:22,330 - Epoch: [30][  240/  246]    Overall Loss 0.315424    Objective Loss 0.315424                                        LR 0.000060    Time 0.025829    
2023-01-06 14:31:22,451 - Epoch: [30][  246/  246]    Overall Loss 0.315852    Objective Loss 0.315852    Top1 87.081340    LR 0.000060    Time 0.025688    
2023-01-06 14:31:22,613 - --- validate (epoch=30)-----------
2023-01-06 14:31:22,613 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:23,056 - Epoch: [30][   10/   28]    Loss 0.324434    Top1 88.320312    
2023-01-06 14:31:23,202 - Epoch: [30][   20/   28]    Loss 0.313773    Top1 88.652344    
2023-01-06 14:31:23,292 - Epoch: [30][   28/   28]    Loss 0.311775    Top1 88.691669    
2023-01-06 14:31:23,426 - ==> Top1: 88.692    Loss: 0.312

2023-01-06 14:31:23,426 - ==> Confusion:
[[ 227    5  207]
 [  19  175  408]
 [  97   54 5794]]

2023-01-06 14:31:23,427 - ==> Best [Top1: 88.692   Sparsity:0.00   Params: 360896 on epoch: 30]
2023-01-06 14:31:23,427 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:23,452 - 

2023-01-06 14:31:23,452 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:24,202 - Epoch: [31][   10/  246]    Overall Loss 0.323736    Objective Loss 0.323736                                        LR 0.000060    Time 0.074927    
2023-01-06 14:31:24,448 - Epoch: [31][   20/  246]    Overall Loss 0.328408    Objective Loss 0.328408                                        LR 0.000060    Time 0.049720    
2023-01-06 14:31:24,703 - Epoch: [31][   30/  246]    Overall Loss 0.322186    Objective Loss 0.322186                                        LR 0.000060    Time 0.041637    
2023-01-06 14:31:24,954 - Epoch: [31][   40/  246]    Overall Loss 0.318053    Objective Loss 0.318053                                        LR 0.000060    Time 0.037503    
2023-01-06 14:31:25,209 - Epoch: [31][   50/  246]    Overall Loss 0.320091    Objective Loss 0.320091                                        LR 0.000060    Time 0.035080    
2023-01-06 14:31:25,459 - Epoch: [31][   60/  246]    Overall Loss 0.319754    Objective Loss 0.319754                                        LR 0.000060    Time 0.033406    
2023-01-06 14:31:25,714 - Epoch: [31][   70/  246]    Overall Loss 0.315792    Objective Loss 0.315792                                        LR 0.000060    Time 0.032264    
2023-01-06 14:31:25,964 - Epoch: [31][   80/  246]    Overall Loss 0.314808    Objective Loss 0.314808                                        LR 0.000060    Time 0.031352    
2023-01-06 14:31:26,219 - Epoch: [31][   90/  246]    Overall Loss 0.314176    Objective Loss 0.314176                                        LR 0.000060    Time 0.030702    
2023-01-06 14:31:26,469 - Epoch: [31][  100/  246]    Overall Loss 0.314029    Objective Loss 0.314029                                        LR 0.000060    Time 0.030126    
2023-01-06 14:31:26,720 - Epoch: [31][  110/  246]    Overall Loss 0.313374    Objective Loss 0.313374                                        LR 0.000060    Time 0.029671    
2023-01-06 14:31:26,969 - Epoch: [31][  120/  246]    Overall Loss 0.312984    Objective Loss 0.312984                                        LR 0.000060    Time 0.029268    
2023-01-06 14:31:27,221 - Epoch: [31][  130/  246]    Overall Loss 0.312142    Objective Loss 0.312142                                        LR 0.000060    Time 0.028950    
2023-01-06 14:31:27,469 - Epoch: [31][  140/  246]    Overall Loss 0.312733    Objective Loss 0.312733                                        LR 0.000060    Time 0.028655    
2023-01-06 14:31:27,720 - Epoch: [31][  150/  246]    Overall Loss 0.312635    Objective Loss 0.312635                                        LR 0.000060    Time 0.028419    
2023-01-06 14:31:27,972 - Epoch: [31][  160/  246]    Overall Loss 0.312853    Objective Loss 0.312853                                        LR 0.000060    Time 0.028210    
2023-01-06 14:31:28,205 - Epoch: [31][  170/  246]    Overall Loss 0.313572    Objective Loss 0.313572                                        LR 0.000060    Time 0.027921    
2023-01-06 14:31:28,435 - Epoch: [31][  180/  246]    Overall Loss 0.314220    Objective Loss 0.314220                                        LR 0.000060    Time 0.027648    
2023-01-06 14:31:28,662 - Epoch: [31][  190/  246]    Overall Loss 0.313642    Objective Loss 0.313642                                        LR 0.000060    Time 0.027384    
2023-01-06 14:31:28,892 - Epoch: [31][  200/  246]    Overall Loss 0.312514    Objective Loss 0.312514                                        LR 0.000060    Time 0.027160    
2023-01-06 14:31:29,118 - Epoch: [31][  210/  246]    Overall Loss 0.312654    Objective Loss 0.312654                                        LR 0.000060    Time 0.026942    
2023-01-06 14:31:29,349 - Epoch: [31][  220/  246]    Overall Loss 0.312425    Objective Loss 0.312425                                        LR 0.000060    Time 0.026768    
2023-01-06 14:31:29,587 - Epoch: [31][  230/  246]    Overall Loss 0.312642    Objective Loss 0.312642                                        LR 0.000060    Time 0.026635    
2023-01-06 14:31:29,833 - Epoch: [31][  240/  246]    Overall Loss 0.313337    Objective Loss 0.313337                                        LR 0.000060    Time 0.026550    
2023-01-06 14:31:29,960 - Epoch: [31][  246/  246]    Overall Loss 0.312778    Objective Loss 0.312778    Top1 87.081340    LR 0.000060    Time 0.026417    
2023-01-06 14:31:30,080 - --- validate (epoch=31)-----------
2023-01-06 14:31:30,080 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:30,544 - Epoch: [31][   10/   28]    Loss 0.320819    Top1 88.828125    
2023-01-06 14:31:30,693 - Epoch: [31][   20/   28]    Loss 0.321733    Top1 88.671875    
2023-01-06 14:31:30,783 - Epoch: [31][   28/   28]    Loss 0.313458    Top1 88.920699    
2023-01-06 14:31:30,906 - ==> Top1: 88.921    Loss: 0.313

2023-01-06 14:31:30,907 - ==> Confusion:
[[ 207    4  228]
 [  13  194  395]
 [  73   61 5811]]

2023-01-06 14:31:30,908 - ==> Best [Top1: 88.921   Sparsity:0.00   Params: 360896 on epoch: 31]
2023-01-06 14:31:30,908 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:30,933 - 

2023-01-06 14:31:30,933 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:31,511 - Epoch: [32][   10/  246]    Overall Loss 0.335126    Objective Loss 0.335126                                        LR 0.000060    Time 0.057684    
2023-01-06 14:31:31,762 - Epoch: [32][   20/  246]    Overall Loss 0.335957    Objective Loss 0.335957                                        LR 0.000060    Time 0.041366    
2023-01-06 14:31:32,017 - Epoch: [32][   30/  246]    Overall Loss 0.320607    Objective Loss 0.320607                                        LR 0.000060    Time 0.036075    
2023-01-06 14:31:32,277 - Epoch: [32][   40/  246]    Overall Loss 0.315594    Objective Loss 0.315594                                        LR 0.000060    Time 0.033537    
2023-01-06 14:31:32,536 - Epoch: [32][   50/  246]    Overall Loss 0.312884    Objective Loss 0.312884                                        LR 0.000060    Time 0.032011    
2023-01-06 14:31:32,808 - Epoch: [32][   60/  246]    Overall Loss 0.311247    Objective Loss 0.311247                                        LR 0.000060    Time 0.031186    
2023-01-06 14:31:33,075 - Epoch: [32][   70/  246]    Overall Loss 0.310685    Objective Loss 0.310685                                        LR 0.000060    Time 0.030543    
2023-01-06 14:31:33,333 - Epoch: [32][   80/  246]    Overall Loss 0.311581    Objective Loss 0.311581                                        LR 0.000060    Time 0.029946    
2023-01-06 14:31:33,581 - Epoch: [32][   90/  246]    Overall Loss 0.311567    Objective Loss 0.311567                                        LR 0.000060    Time 0.029374    
2023-01-06 14:31:33,826 - Epoch: [32][  100/  246]    Overall Loss 0.311461    Objective Loss 0.311461                                        LR 0.000060    Time 0.028874    
2023-01-06 14:31:34,067 - Epoch: [32][  110/  246]    Overall Loss 0.313765    Objective Loss 0.313765                                        LR 0.000060    Time 0.028437    
2023-01-06 14:31:34,312 - Epoch: [32][  120/  246]    Overall Loss 0.313790    Objective Loss 0.313790                                        LR 0.000060    Time 0.028111    
2023-01-06 14:31:34,555 - Epoch: [32][  130/  246]    Overall Loss 0.313210    Objective Loss 0.313210                                        LR 0.000060    Time 0.027816    
2023-01-06 14:31:34,799 - Epoch: [32][  140/  246]    Overall Loss 0.313869    Objective Loss 0.313869                                        LR 0.000060    Time 0.027569    
2023-01-06 14:31:35,044 - Epoch: [32][  150/  246]    Overall Loss 0.315086    Objective Loss 0.315086                                        LR 0.000060    Time 0.027359    
2023-01-06 14:31:35,290 - Epoch: [32][  160/  246]    Overall Loss 0.314507    Objective Loss 0.314507                                        LR 0.000060    Time 0.027182    
2023-01-06 14:31:35,531 - Epoch: [32][  170/  246]    Overall Loss 0.312712    Objective Loss 0.312712                                        LR 0.000060    Time 0.026998    
2023-01-06 14:31:35,776 - Epoch: [32][  180/  246]    Overall Loss 0.312782    Objective Loss 0.312782                                        LR 0.000060    Time 0.026858    
2023-01-06 14:31:36,018 - Epoch: [32][  190/  246]    Overall Loss 0.311475    Objective Loss 0.311475                                        LR 0.000060    Time 0.026718    
2023-01-06 14:31:36,259 - Epoch: [32][  200/  246]    Overall Loss 0.311342    Objective Loss 0.311342                                        LR 0.000060    Time 0.026587    
2023-01-06 14:31:36,503 - Epoch: [32][  210/  246]    Overall Loss 0.311121    Objective Loss 0.311121                                        LR 0.000060    Time 0.026480    
2023-01-06 14:31:36,714 - Epoch: [32][  220/  246]    Overall Loss 0.310902    Objective Loss 0.310902                                        LR 0.000060    Time 0.026233    
2023-01-06 14:31:36,929 - Epoch: [32][  230/  246]    Overall Loss 0.310704    Objective Loss 0.310704                                        LR 0.000060    Time 0.026026    
2023-01-06 14:31:37,182 - Epoch: [32][  240/  246]    Overall Loss 0.311063    Objective Loss 0.311063                                        LR 0.000060    Time 0.025996    
2023-01-06 14:31:37,316 - Epoch: [32][  246/  246]    Overall Loss 0.310178    Objective Loss 0.310178    Top1 89.473684    LR 0.000060    Time 0.025905    
2023-01-06 14:31:37,465 - --- validate (epoch=32)-----------
2023-01-06 14:31:37,465 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:37,903 - Epoch: [32][   10/   28]    Loss 0.300778    Top1 89.218750    
2023-01-06 14:31:38,042 - Epoch: [32][   20/   28]    Loss 0.300713    Top1 89.453125    
2023-01-06 14:31:38,134 - Epoch: [32][   28/   28]    Loss 0.306918    Top1 89.106785    
2023-01-06 14:31:38,262 - ==> Top1: 89.107    Loss: 0.307

2023-01-06 14:31:38,262 - ==> Confusion:
[[ 179    7  253]
 [   9  178  415]
 [  35   42 5868]]

2023-01-06 14:31:38,263 - ==> Best [Top1: 89.107   Sparsity:0.00   Params: 360896 on epoch: 32]
2023-01-06 14:31:38,264 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:38,290 - 

2023-01-06 14:31:38,290 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:39,039 - Epoch: [33][   10/  246]    Overall Loss 0.316982    Objective Loss 0.316982                                        LR 0.000060    Time 0.074885    
2023-01-06 14:31:39,275 - Epoch: [33][   20/  246]    Overall Loss 0.320771    Objective Loss 0.320771                                        LR 0.000060    Time 0.049200    
2023-01-06 14:31:39,529 - Epoch: [33][   30/  246]    Overall Loss 0.313493    Objective Loss 0.313493                                        LR 0.000060    Time 0.041247    
2023-01-06 14:31:39,779 - Epoch: [33][   40/  246]    Overall Loss 0.309327    Objective Loss 0.309327                                        LR 0.000060    Time 0.037165    
2023-01-06 14:31:40,029 - Epoch: [33][   50/  246]    Overall Loss 0.310960    Objective Loss 0.310960                                        LR 0.000060    Time 0.034740    
2023-01-06 14:31:40,288 - Epoch: [33][   60/  246]    Overall Loss 0.310032    Objective Loss 0.310032                                        LR 0.000060    Time 0.033254    
2023-01-06 14:31:40,552 - Epoch: [33][   70/  246]    Overall Loss 0.313851    Objective Loss 0.313851                                        LR 0.000060    Time 0.032270    
2023-01-06 14:31:40,821 - Epoch: [33][   80/  246]    Overall Loss 0.312870    Objective Loss 0.312870                                        LR 0.000060    Time 0.031594    
2023-01-06 14:31:41,080 - Epoch: [33][   90/  246]    Overall Loss 0.311180    Objective Loss 0.311180                                        LR 0.000060    Time 0.030952    
2023-01-06 14:31:41,330 - Epoch: [33][  100/  246]    Overall Loss 0.310515    Objective Loss 0.310515                                        LR 0.000060    Time 0.030356    
2023-01-06 14:31:41,578 - Epoch: [33][  110/  246]    Overall Loss 0.310706    Objective Loss 0.310706                                        LR 0.000060    Time 0.029846    
2023-01-06 14:31:41,829 - Epoch: [33][  120/  246]    Overall Loss 0.311439    Objective Loss 0.311439                                        LR 0.000060    Time 0.029435    
2023-01-06 14:31:42,078 - Epoch: [33][  130/  246]    Overall Loss 0.312346    Objective Loss 0.312346                                        LR 0.000060    Time 0.029085    
2023-01-06 14:31:42,328 - Epoch: [33][  140/  246]    Overall Loss 0.313049    Objective Loss 0.313049                                        LR 0.000060    Time 0.028781    
2023-01-06 14:31:42,575 - Epoch: [33][  150/  246]    Overall Loss 0.312702    Objective Loss 0.312702                                        LR 0.000060    Time 0.028507    
2023-01-06 14:31:42,828 - Epoch: [33][  160/  246]    Overall Loss 0.313194    Objective Loss 0.313194                                        LR 0.000060    Time 0.028303    
2023-01-06 14:31:43,077 - Epoch: [33][  170/  246]    Overall Loss 0.312239    Objective Loss 0.312239                                        LR 0.000060    Time 0.028102    
2023-01-06 14:31:43,328 - Epoch: [33][  180/  246]    Overall Loss 0.312476    Objective Loss 0.312476                                        LR 0.000060    Time 0.027929    
2023-01-06 14:31:43,575 - Epoch: [33][  190/  246]    Overall Loss 0.311703    Objective Loss 0.311703                                        LR 0.000060    Time 0.027756    
2023-01-06 14:31:43,826 - Epoch: [33][  200/  246]    Overall Loss 0.310441    Objective Loss 0.310441                                        LR 0.000060    Time 0.027620    
2023-01-06 14:31:44,074 - Epoch: [33][  210/  246]    Overall Loss 0.309449    Objective Loss 0.309449                                        LR 0.000060    Time 0.027484    
2023-01-06 14:31:44,328 - Epoch: [33][  220/  246]    Overall Loss 0.309087    Objective Loss 0.309087                                        LR 0.000060    Time 0.027390    
2023-01-06 14:31:44,578 - Epoch: [33][  230/  246]    Overall Loss 0.309882    Objective Loss 0.309882                                        LR 0.000060    Time 0.027282    
2023-01-06 14:31:44,838 - Epoch: [33][  240/  246]    Overall Loss 0.307959    Objective Loss 0.307959                                        LR 0.000060    Time 0.027228    
2023-01-06 14:31:44,966 - Epoch: [33][  246/  246]    Overall Loss 0.307816    Objective Loss 0.307816    Top1 89.234450    LR 0.000060    Time 0.027084    
2023-01-06 14:31:45,079 - --- validate (epoch=33)-----------
2023-01-06 14:31:45,079 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:45,527 - Epoch: [33][   10/   28]    Loss 0.333695    Top1 87.890625    
2023-01-06 14:31:45,669 - Epoch: [33][   20/   28]    Loss 0.324450    Top1 88.144531    
2023-01-06 14:31:45,759 - Epoch: [33][   28/   28]    Loss 0.316803    Top1 88.233610    
2023-01-06 14:31:45,904 - ==> Top1: 88.234    Loss: 0.317

2023-01-06 14:31:45,904 - ==> Confusion:
[[ 239    7  193]
 [  21  222  359]
 [ 147   95 5703]]

2023-01-06 14:31:45,905 - ==> Best [Top1: 89.107   Sparsity:0.00   Params: 360896 on epoch: 32]
2023-01-06 14:31:45,906 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:45,916 - 

2023-01-06 14:31:45,916 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:46,642 - Epoch: [34][   10/  246]    Overall Loss 0.314997    Objective Loss 0.314997                                        LR 0.000060    Time 0.072542    
2023-01-06 14:31:46,888 - Epoch: [34][   20/  246]    Overall Loss 0.309385    Objective Loss 0.309385                                        LR 0.000060    Time 0.048568    
2023-01-06 14:31:47,141 - Epoch: [34][   30/  246]    Overall Loss 0.311032    Objective Loss 0.311032                                        LR 0.000060    Time 0.040771    
2023-01-06 14:31:47,392 - Epoch: [34][   40/  246]    Overall Loss 0.307594    Objective Loss 0.307594                                        LR 0.000060    Time 0.036864    
2023-01-06 14:31:47,640 - Epoch: [34][   50/  246]    Overall Loss 0.308929    Objective Loss 0.308929                                        LR 0.000060    Time 0.034441    
2023-01-06 14:31:47,889 - Epoch: [34][   60/  246]    Overall Loss 0.309740    Objective Loss 0.309740                                        LR 0.000060    Time 0.032839    
2023-01-06 14:31:48,135 - Epoch: [34][   70/  246]    Overall Loss 0.308825    Objective Loss 0.308825                                        LR 0.000060    Time 0.031652    
2023-01-06 14:31:48,379 - Epoch: [34][   80/  246]    Overall Loss 0.307187    Objective Loss 0.307187                                        LR 0.000060    Time 0.030743    
2023-01-06 14:31:48,624 - Epoch: [34][   90/  246]    Overall Loss 0.307262    Objective Loss 0.307262                                        LR 0.000060    Time 0.030043    
2023-01-06 14:31:48,868 - Epoch: [34][  100/  246]    Overall Loss 0.305529    Objective Loss 0.305529                                        LR 0.000060    Time 0.029476    
2023-01-06 14:31:49,124 - Epoch: [34][  110/  246]    Overall Loss 0.307991    Objective Loss 0.307991                                        LR 0.000060    Time 0.029120    
2023-01-06 14:31:49,373 - Epoch: [34][  120/  246]    Overall Loss 0.305558    Objective Loss 0.305558                                        LR 0.000060    Time 0.028757    
2023-01-06 14:31:49,625 - Epoch: [34][  130/  246]    Overall Loss 0.304014    Objective Loss 0.304014                                        LR 0.000060    Time 0.028485    
2023-01-06 14:31:49,872 - Epoch: [34][  140/  246]    Overall Loss 0.305862    Objective Loss 0.305862                                        LR 0.000060    Time 0.028213    
2023-01-06 14:31:50,124 - Epoch: [34][  150/  246]    Overall Loss 0.307033    Objective Loss 0.307033                                        LR 0.000060    Time 0.028008    
2023-01-06 14:31:50,372 - Epoch: [34][  160/  246]    Overall Loss 0.306652    Objective Loss 0.306652                                        LR 0.000060    Time 0.027801    
2023-01-06 14:31:50,624 - Epoch: [34][  170/  246]    Overall Loss 0.307599    Objective Loss 0.307599                                        LR 0.000060    Time 0.027645    
2023-01-06 14:31:50,870 - Epoch: [34][  180/  246]    Overall Loss 0.306378    Objective Loss 0.306378                                        LR 0.000060    Time 0.027475    
2023-01-06 14:31:51,121 - Epoch: [34][  190/  246]    Overall Loss 0.305604    Objective Loss 0.305604                                        LR 0.000060    Time 0.027348    
2023-01-06 14:31:51,367 - Epoch: [34][  200/  246]    Overall Loss 0.304771    Objective Loss 0.304771                                        LR 0.000060    Time 0.027210    
2023-01-06 14:31:51,619 - Epoch: [34][  210/  246]    Overall Loss 0.305492    Objective Loss 0.305492                                        LR 0.000060    Time 0.027110    
2023-01-06 14:31:51,865 - Epoch: [34][  220/  246]    Overall Loss 0.305914    Objective Loss 0.305914                                        LR 0.000060    Time 0.026994    
2023-01-06 14:31:52,111 - Epoch: [34][  230/  246]    Overall Loss 0.305118    Objective Loss 0.305118                                        LR 0.000060    Time 0.026886    
2023-01-06 14:31:52,365 - Epoch: [34][  240/  246]    Overall Loss 0.304595    Objective Loss 0.304595                                        LR 0.000060    Time 0.026824    
2023-01-06 14:31:52,495 - Epoch: [34][  246/  246]    Overall Loss 0.304957    Objective Loss 0.304957    Top1 87.559809    LR 0.000060    Time 0.026697    
2023-01-06 14:31:52,671 - --- validate (epoch=34)-----------
2023-01-06 14:31:52,671 - 6986 samples (256 per mini-batch)
2023-01-06 14:31:53,106 - Epoch: [34][   10/   28]    Loss 0.312181    Top1 88.671875    
2023-01-06 14:31:53,249 - Epoch: [34][   20/   28]    Loss 0.318212    Top1 88.515625    
2023-01-06 14:31:53,340 - Epoch: [34][   28/   28]    Loss 0.311558    Top1 88.677355    
2023-01-06 14:31:53,464 - ==> Top1: 88.677    Loss: 0.312

2023-01-06 14:31:53,464 - ==> Confusion:
[[ 205    9  225]
 [  21  173  408]
 [  83   45 5817]]

2023-01-06 14:31:53,465 - ==> Best [Top1: 89.107   Sparsity:0.00   Params: 360896 on epoch: 32]
2023-01-06 14:31:53,465 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:31:53,482 - 

2023-01-06 14:31:53,482 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:31:54,101 - Epoch: [35][   10/  246]    Overall Loss 0.289372    Objective Loss 0.289372                                        LR 0.000060    Time 0.061827    
2023-01-06 14:31:54,357 - Epoch: [35][   20/  246]    Overall Loss 0.294591    Objective Loss 0.294591                                        LR 0.000060    Time 0.043660    
2023-01-06 14:31:54,616 - Epoch: [35][   30/  246]    Overall Loss 0.293578    Objective Loss 0.293578                                        LR 0.000060    Time 0.037740    
2023-01-06 14:31:54,868 - Epoch: [35][   40/  246]    Overall Loss 0.296281    Objective Loss 0.296281                                        LR 0.000060    Time 0.034597    
2023-01-06 14:31:55,126 - Epoch: [35][   50/  246]    Overall Loss 0.303271    Objective Loss 0.303271                                        LR 0.000060    Time 0.032830    
2023-01-06 14:31:55,374 - Epoch: [35][   60/  246]    Overall Loss 0.307805    Objective Loss 0.307805                                        LR 0.000060    Time 0.031484    
2023-01-06 14:31:55,629 - Epoch: [35][   70/  246]    Overall Loss 0.308185    Objective Loss 0.308185                                        LR 0.000060    Time 0.030616    
2023-01-06 14:31:55,882 - Epoch: [35][   80/  246]    Overall Loss 0.305797    Objective Loss 0.305797                                        LR 0.000060    Time 0.029953    
2023-01-06 14:31:56,124 - Epoch: [35][   90/  246]    Overall Loss 0.304852    Objective Loss 0.304852                                        LR 0.000060    Time 0.029306    
2023-01-06 14:31:56,369 - Epoch: [35][  100/  246]    Overall Loss 0.302544    Objective Loss 0.302544                                        LR 0.000060    Time 0.028822    
2023-01-06 14:31:56,614 - Epoch: [35][  110/  246]    Overall Loss 0.302703    Objective Loss 0.302703                                        LR 0.000060    Time 0.028426    
2023-01-06 14:31:56,860 - Epoch: [35][  120/  246]    Overall Loss 0.305980    Objective Loss 0.305980                                        LR 0.000060    Time 0.028100    
2023-01-06 14:31:57,104 - Epoch: [35][  130/  246]    Overall Loss 0.306152    Objective Loss 0.306152                                        LR 0.000060    Time 0.027811    
2023-01-06 14:31:57,349 - Epoch: [35][  140/  246]    Overall Loss 0.305222    Objective Loss 0.305222                                        LR 0.000060    Time 0.027575    
2023-01-06 14:31:57,595 - Epoch: [35][  150/  246]    Overall Loss 0.304328    Objective Loss 0.304328                                        LR 0.000060    Time 0.027371    
2023-01-06 14:31:57,840 - Epoch: [35][  160/  246]    Overall Loss 0.304004    Objective Loss 0.304004                                        LR 0.000060    Time 0.027188    
2023-01-06 14:31:58,084 - Epoch: [35][  170/  246]    Overall Loss 0.302978    Objective Loss 0.302978                                        LR 0.000060    Time 0.027025    
2023-01-06 14:31:58,332 - Epoch: [35][  180/  246]    Overall Loss 0.301933    Objective Loss 0.301933                                        LR 0.000060    Time 0.026886    
2023-01-06 14:31:58,579 - Epoch: [35][  190/  246]    Overall Loss 0.300969    Objective Loss 0.300969                                        LR 0.000060    Time 0.026769    
2023-01-06 14:31:58,825 - Epoch: [35][  200/  246]    Overall Loss 0.300764    Objective Loss 0.300764                                        LR 0.000060    Time 0.026658    
2023-01-06 14:31:59,069 - Epoch: [35][  210/  246]    Overall Loss 0.301922    Objective Loss 0.301922                                        LR 0.000060    Time 0.026548    
2023-01-06 14:31:59,314 - Epoch: [35][  220/  246]    Overall Loss 0.301635    Objective Loss 0.301635                                        LR 0.000060    Time 0.026452    
2023-01-06 14:31:59,555 - Epoch: [35][  230/  246]    Overall Loss 0.301026    Objective Loss 0.301026                                        LR 0.000060    Time 0.026352    
2023-01-06 14:31:59,807 - Epoch: [35][  240/  246]    Overall Loss 0.302035    Objective Loss 0.302035                                        LR 0.000060    Time 0.026301    
2023-01-06 14:31:59,937 - Epoch: [35][  246/  246]    Overall Loss 0.301791    Objective Loss 0.301791    Top1 86.842105    LR 0.000060    Time 0.026186    
2023-01-06 14:32:00,073 - --- validate (epoch=35)-----------
2023-01-06 14:32:00,073 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:00,537 - Epoch: [35][   10/   28]    Loss 0.290995    Top1 89.765625    
2023-01-06 14:32:00,688 - Epoch: [35][   20/   28]    Loss 0.297458    Top1 89.355469    
2023-01-06 14:32:00,778 - Epoch: [35][   28/   28]    Loss 0.302685    Top1 89.221300    
2023-01-06 14:32:00,914 - ==> Top1: 89.221    Loss: 0.303

2023-01-06 14:32:00,915 - ==> Confusion:
[[ 189    4  246]
 [  11  209  382]
 [  43   67 5835]]

2023-01-06 14:32:00,916 - ==> Best [Top1: 89.221   Sparsity:0.00   Params: 360896 on epoch: 35]
2023-01-06 14:32:00,916 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:00,937 - 

2023-01-06 14:32:00,937 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:01,678 - Epoch: [36][   10/  246]    Overall Loss 0.330795    Objective Loss 0.330795                                        LR 0.000060    Time 0.074004    
2023-01-06 14:32:01,928 - Epoch: [36][   20/  246]    Overall Loss 0.311568    Objective Loss 0.311568                                        LR 0.000060    Time 0.049473    
2023-01-06 14:32:02,190 - Epoch: [36][   30/  246]    Overall Loss 0.307440    Objective Loss 0.307440                                        LR 0.000060    Time 0.041681    
2023-01-06 14:32:02,444 - Epoch: [36][   40/  246]    Overall Loss 0.309524    Objective Loss 0.309524                                        LR 0.000060    Time 0.037620    
2023-01-06 14:32:02,706 - Epoch: [36][   50/  246]    Overall Loss 0.306688    Objective Loss 0.306688                                        LR 0.000060    Time 0.035295    
2023-01-06 14:32:02,960 - Epoch: [36][   60/  246]    Overall Loss 0.304648    Objective Loss 0.304648                                        LR 0.000060    Time 0.033653    
2023-01-06 14:32:03,188 - Epoch: [36][   70/  246]    Overall Loss 0.303828    Objective Loss 0.303828                                        LR 0.000060    Time 0.032088    
2023-01-06 14:32:03,398 - Epoch: [36][   80/  246]    Overall Loss 0.301565    Objective Loss 0.301565                                        LR 0.000060    Time 0.030705    
2023-01-06 14:32:03,645 - Epoch: [36][   90/  246]    Overall Loss 0.303623    Objective Loss 0.303623                                        LR 0.000060    Time 0.030025    
2023-01-06 14:32:03,895 - Epoch: [36][  100/  246]    Overall Loss 0.302504    Objective Loss 0.302504                                        LR 0.000060    Time 0.029524    
2023-01-06 14:32:04,144 - Epoch: [36][  110/  246]    Overall Loss 0.301696    Objective Loss 0.301696                                        LR 0.000060    Time 0.029092    
2023-01-06 14:32:04,395 - Epoch: [36][  120/  246]    Overall Loss 0.300546    Objective Loss 0.300546                                        LR 0.000060    Time 0.028759    
2023-01-06 14:32:04,643 - Epoch: [36][  130/  246]    Overall Loss 0.300037    Objective Loss 0.300037                                        LR 0.000060    Time 0.028452    
2023-01-06 14:32:04,892 - Epoch: [36][  140/  246]    Overall Loss 0.299107    Objective Loss 0.299107                                        LR 0.000060    Time 0.028194    
2023-01-06 14:32:05,145 - Epoch: [36][  150/  246]    Overall Loss 0.299504    Objective Loss 0.299504                                        LR 0.000060    Time 0.027996    
2023-01-06 14:32:05,394 - Epoch: [36][  160/  246]    Overall Loss 0.298737    Objective Loss 0.298737                                        LR 0.000060    Time 0.027802    
2023-01-06 14:32:05,647 - Epoch: [36][  170/  246]    Overall Loss 0.299131    Objective Loss 0.299131                                        LR 0.000060    Time 0.027650    
2023-01-06 14:32:05,895 - Epoch: [36][  180/  246]    Overall Loss 0.300224    Objective Loss 0.300224                                        LR 0.000060    Time 0.027492    
2023-01-06 14:32:06,144 - Epoch: [36][  190/  246]    Overall Loss 0.300096    Objective Loss 0.300096                                        LR 0.000060    Time 0.027354    
2023-01-06 14:32:06,394 - Epoch: [36][  200/  246]    Overall Loss 0.300698    Objective Loss 0.300698                                        LR 0.000060    Time 0.027234    
2023-01-06 14:32:06,644 - Epoch: [36][  210/  246]    Overall Loss 0.301012    Objective Loss 0.301012                                        LR 0.000060    Time 0.027122    
2023-01-06 14:32:06,892 - Epoch: [36][  220/  246]    Overall Loss 0.301509    Objective Loss 0.301509                                        LR 0.000060    Time 0.027018    
2023-01-06 14:32:07,145 - Epoch: [36][  230/  246]    Overall Loss 0.301235    Objective Loss 0.301235                                        LR 0.000060    Time 0.026941    
2023-01-06 14:32:07,405 - Epoch: [36][  240/  246]    Overall Loss 0.301518    Objective Loss 0.301518                                        LR 0.000060    Time 0.026897    
2023-01-06 14:32:07,535 - Epoch: [36][  246/  246]    Overall Loss 0.300881    Objective Loss 0.300881    Top1 88.516746    LR 0.000060    Time 0.026770    
2023-01-06 14:32:07,656 - --- validate (epoch=36)-----------
2023-01-06 14:32:07,656 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:08,113 - Epoch: [36][   10/   28]    Loss 0.319062    Top1 89.101562    
2023-01-06 14:32:08,254 - Epoch: [36][   20/   28]    Loss 0.299137    Top1 89.472656    
2023-01-06 14:32:08,344 - Epoch: [36][   28/   28]    Loss 0.305431    Top1 89.378758    
2023-01-06 14:32:08,509 - ==> Top1: 89.379    Loss: 0.305

2023-01-06 14:32:08,509 - ==> Confusion:
[[ 191    8  240]
 [  12  224  366]
 [  49   67 5829]]

2023-01-06 14:32:08,510 - ==> Best [Top1: 89.379   Sparsity:0.00   Params: 360896 on epoch: 36]
2023-01-06 14:32:08,510 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:08,535 - 

2023-01-06 14:32:08,535 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:09,148 - Epoch: [37][   10/  246]    Overall Loss 0.313789    Objective Loss 0.313789                                        LR 0.000060    Time 0.061203    
2023-01-06 14:32:09,391 - Epoch: [37][   20/  246]    Overall Loss 0.304824    Objective Loss 0.304824                                        LR 0.000060    Time 0.042702    
2023-01-06 14:32:09,639 - Epoch: [37][   30/  246]    Overall Loss 0.303957    Objective Loss 0.303957                                        LR 0.000060    Time 0.036688    
2023-01-06 14:32:09,889 - Epoch: [37][   40/  246]    Overall Loss 0.303623    Objective Loss 0.303623                                        LR 0.000060    Time 0.033761    
2023-01-06 14:32:10,139 - Epoch: [37][   50/  246]    Overall Loss 0.307610    Objective Loss 0.307610                                        LR 0.000060    Time 0.031996    
2023-01-06 14:32:10,394 - Epoch: [37][   60/  246]    Overall Loss 0.305012    Objective Loss 0.305012                                        LR 0.000060    Time 0.030894    
2023-01-06 14:32:10,644 - Epoch: [37][   70/  246]    Overall Loss 0.304262    Objective Loss 0.304262                                        LR 0.000060    Time 0.030058    
2023-01-06 14:32:10,898 - Epoch: [37][   80/  246]    Overall Loss 0.300394    Objective Loss 0.300394                                        LR 0.000060    Time 0.029464    
2023-01-06 14:32:11,145 - Epoch: [37][   90/  246]    Overall Loss 0.300959    Objective Loss 0.300959                                        LR 0.000060    Time 0.028928    
2023-01-06 14:32:11,399 - Epoch: [37][  100/  246]    Overall Loss 0.300534    Objective Loss 0.300534                                        LR 0.000060    Time 0.028568    
2023-01-06 14:32:11,652 - Epoch: [37][  110/  246]    Overall Loss 0.299885    Objective Loss 0.299885                                        LR 0.000060    Time 0.028267    
2023-01-06 14:32:11,910 - Epoch: [37][  120/  246]    Overall Loss 0.298570    Objective Loss 0.298570                                        LR 0.000060    Time 0.028061    
2023-01-06 14:32:12,168 - Epoch: [37][  130/  246]    Overall Loss 0.297686    Objective Loss 0.297686                                        LR 0.000060    Time 0.027871    
2023-01-06 14:32:12,427 - Epoch: [37][  140/  246]    Overall Loss 0.297328    Objective Loss 0.297328                                        LR 0.000060    Time 0.027731    
2023-01-06 14:32:12,679 - Epoch: [37][  150/  246]    Overall Loss 0.300353    Objective Loss 0.300353                                        LR 0.000060    Time 0.027558    
2023-01-06 14:32:12,933 - Epoch: [37][  160/  246]    Overall Loss 0.301582    Objective Loss 0.301582                                        LR 0.000060    Time 0.027407    
2023-01-06 14:32:13,186 - Epoch: [37][  170/  246]    Overall Loss 0.300749    Objective Loss 0.300749                                        LR 0.000060    Time 0.027274    
2023-01-06 14:32:13,438 - Epoch: [37][  180/  246]    Overall Loss 0.300989    Objective Loss 0.300989                                        LR 0.000060    Time 0.027142    
2023-01-06 14:32:13,694 - Epoch: [37][  190/  246]    Overall Loss 0.302315    Objective Loss 0.302315                                        LR 0.000060    Time 0.027048    
2023-01-06 14:32:13,948 - Epoch: [37][  200/  246]    Overall Loss 0.301345    Objective Loss 0.301345                                        LR 0.000060    Time 0.026966    
2023-01-06 14:32:14,201 - Epoch: [37][  210/  246]    Overall Loss 0.300793    Objective Loss 0.300793                                        LR 0.000060    Time 0.026884    
2023-01-06 14:32:14,455 - Epoch: [37][  220/  246]    Overall Loss 0.300967    Objective Loss 0.300967                                        LR 0.000060    Time 0.026813    
2023-01-06 14:32:14,703 - Epoch: [37][  230/  246]    Overall Loss 0.300994    Objective Loss 0.300994                                        LR 0.000060    Time 0.026725    
2023-01-06 14:32:14,963 - Epoch: [37][  240/  246]    Overall Loss 0.301304    Objective Loss 0.301304                                        LR 0.000060    Time 0.026691    
2023-01-06 14:32:15,093 - Epoch: [37][  246/  246]    Overall Loss 0.301170    Objective Loss 0.301170    Top1 88.755981    LR 0.000060    Time 0.026568    
2023-01-06 14:32:15,230 - --- validate (epoch=37)-----------
2023-01-06 14:32:15,230 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:15,683 - Epoch: [37][   10/   28]    Loss 0.313841    Top1 89.570312    
2023-01-06 14:32:15,831 - Epoch: [37][   20/   28]    Loss 0.302181    Top1 89.433594    
2023-01-06 14:32:15,923 - Epoch: [37][   28/   28]    Loss 0.303077    Top1 89.350129    
2023-01-06 14:32:16,063 - ==> Top1: 89.350    Loss: 0.303

2023-01-06 14:32:16,063 - ==> Confusion:
[[ 208    9  222]
 [  16  223  363]
 [  67   67 5811]]

2023-01-06 14:32:16,064 - ==> Best [Top1: 89.379   Sparsity:0.00   Params: 360896 on epoch: 36]
2023-01-06 14:32:16,064 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:16,074 - 

2023-01-06 14:32:16,074 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:16,788 - Epoch: [38][   10/  246]    Overall Loss 0.287869    Objective Loss 0.287869                                        LR 0.000060    Time 0.071308    
2023-01-06 14:32:17,046 - Epoch: [38][   20/  246]    Overall Loss 0.297336    Objective Loss 0.297336                                        LR 0.000060    Time 0.048551    
2023-01-06 14:32:17,303 - Epoch: [38][   30/  246]    Overall Loss 0.296950    Objective Loss 0.296950                                        LR 0.000060    Time 0.040911    
2023-01-06 14:32:17,560 - Epoch: [38][   40/  246]    Overall Loss 0.294675    Objective Loss 0.294675                                        LR 0.000060    Time 0.037094    
2023-01-06 14:32:17,814 - Epoch: [38][   50/  246]    Overall Loss 0.296708    Objective Loss 0.296708                                        LR 0.000060    Time 0.034743    
2023-01-06 14:32:18,064 - Epoch: [38][   60/  246]    Overall Loss 0.298006    Objective Loss 0.298006                                        LR 0.000060    Time 0.033120    
2023-01-06 14:32:18,308 - Epoch: [38][   70/  246]    Overall Loss 0.297528    Objective Loss 0.297528                                        LR 0.000060    Time 0.031867    
2023-01-06 14:32:18,550 - Epoch: [38][   80/  246]    Overall Loss 0.296356    Objective Loss 0.296356                                        LR 0.000060    Time 0.030900    
2023-01-06 14:32:18,788 - Epoch: [38][   90/  246]    Overall Loss 0.297163    Objective Loss 0.297163                                        LR 0.000060    Time 0.030110    
2023-01-06 14:32:19,030 - Epoch: [38][  100/  246]    Overall Loss 0.297965    Objective Loss 0.297965                                        LR 0.000060    Time 0.029515    
2023-01-06 14:32:19,272 - Epoch: [38][  110/  246]    Overall Loss 0.297946    Objective Loss 0.297946                                        LR 0.000060    Time 0.029033    
2023-01-06 14:32:19,514 - Epoch: [38][  120/  246]    Overall Loss 0.297219    Objective Loss 0.297219                                        LR 0.000060    Time 0.028623    
2023-01-06 14:32:19,755 - Epoch: [38][  130/  246]    Overall Loss 0.296680    Objective Loss 0.296680                                        LR 0.000060    Time 0.028278    
2023-01-06 14:32:19,996 - Epoch: [38][  140/  246]    Overall Loss 0.297966    Objective Loss 0.297966                                        LR 0.000060    Time 0.027978    
2023-01-06 14:32:20,238 - Epoch: [38][  150/  246]    Overall Loss 0.297403    Objective Loss 0.297403                                        LR 0.000060    Time 0.027724    
2023-01-06 14:32:20,480 - Epoch: [38][  160/  246]    Overall Loss 0.296254    Objective Loss 0.296254                                        LR 0.000060    Time 0.027501    
2023-01-06 14:32:20,722 - Epoch: [38][  170/  246]    Overall Loss 0.295332    Objective Loss 0.295332                                        LR 0.000060    Time 0.027306    
2023-01-06 14:32:20,961 - Epoch: [38][  180/  246]    Overall Loss 0.295763    Objective Loss 0.295763                                        LR 0.000060    Time 0.027112    
2023-01-06 14:32:21,202 - Epoch: [38][  190/  246]    Overall Loss 0.294586    Objective Loss 0.294586                                        LR 0.000060    Time 0.026954    
2023-01-06 14:32:21,443 - Epoch: [38][  200/  246]    Overall Loss 0.294632    Objective Loss 0.294632                                        LR 0.000060    Time 0.026812    
2023-01-06 14:32:21,686 - Epoch: [38][  210/  246]    Overall Loss 0.294201    Objective Loss 0.294201                                        LR 0.000060    Time 0.026688    
2023-01-06 14:32:21,927 - Epoch: [38][  220/  246]    Overall Loss 0.295493    Objective Loss 0.295493                                        LR 0.000060    Time 0.026569    
2023-01-06 14:32:22,168 - Epoch: [38][  230/  246]    Overall Loss 0.297510    Objective Loss 0.297510                                        LR 0.000060    Time 0.026460    
2023-01-06 14:32:22,422 - Epoch: [38][  240/  246]    Overall Loss 0.298719    Objective Loss 0.298719                                        LR 0.000060    Time 0.026415    
2023-01-06 14:32:22,546 - Epoch: [38][  246/  246]    Overall Loss 0.298896    Objective Loss 0.298896    Top1 89.712919    LR 0.000060    Time 0.026275    
2023-01-06 14:32:22,670 - --- validate (epoch=38)-----------
2023-01-06 14:32:22,670 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:23,117 - Epoch: [38][   10/   28]    Loss 0.300150    Top1 89.375000    
2023-01-06 14:32:23,254 - Epoch: [38][   20/   28]    Loss 0.309713    Top1 89.101562    
2023-01-06 14:32:23,344 - Epoch: [38][   28/   28]    Loss 0.307318    Top1 89.149728    
2023-01-06 14:32:23,460 - ==> Top1: 89.150    Loss: 0.307

2023-01-06 14:32:23,460 - ==> Confusion:
[[ 205    7  227]
 [  12  176  414]
 [  66   32 5847]]

2023-01-06 14:32:23,462 - ==> Best [Top1: 89.379   Sparsity:0.00   Params: 360896 on epoch: 36]
2023-01-06 14:32:23,462 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:23,472 - 

2023-01-06 14:32:23,472 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:24,066 - Epoch: [39][   10/  246]    Overall Loss 0.307331    Objective Loss 0.307331                                        LR 0.000060    Time 0.059325    
2023-01-06 14:32:24,313 - Epoch: [39][   20/  246]    Overall Loss 0.300567    Objective Loss 0.300567                                        LR 0.000060    Time 0.042008    
2023-01-06 14:32:24,572 - Epoch: [39][   30/  246]    Overall Loss 0.304897    Objective Loss 0.304897                                        LR 0.000060    Time 0.036607    
2023-01-06 14:32:24,836 - Epoch: [39][   40/  246]    Overall Loss 0.303773    Objective Loss 0.303773                                        LR 0.000060    Time 0.033997    
2023-01-06 14:32:25,094 - Epoch: [39][   50/  246]    Overall Loss 0.303893    Objective Loss 0.303893                                        LR 0.000060    Time 0.032367    
2023-01-06 14:32:25,357 - Epoch: [39][   60/  246]    Overall Loss 0.305839    Objective Loss 0.305839                                        LR 0.000060    Time 0.031334    
2023-01-06 14:32:25,617 - Epoch: [39][   70/  246]    Overall Loss 0.304499    Objective Loss 0.304499                                        LR 0.000060    Time 0.030567    
2023-01-06 14:32:25,881 - Epoch: [39][   80/  246]    Overall Loss 0.306442    Objective Loss 0.306442                                        LR 0.000060    Time 0.030055    
2023-01-06 14:32:26,137 - Epoch: [39][   90/  246]    Overall Loss 0.304864    Objective Loss 0.304864                                        LR 0.000060    Time 0.029547    
2023-01-06 14:32:26,392 - Epoch: [39][  100/  246]    Overall Loss 0.301185    Objective Loss 0.301185                                        LR 0.000060    Time 0.029122    
2023-01-06 14:32:26,647 - Epoch: [39][  110/  246]    Overall Loss 0.300766    Objective Loss 0.300766                                        LR 0.000060    Time 0.028794    
2023-01-06 14:32:26,901 - Epoch: [39][  120/  246]    Overall Loss 0.300110    Objective Loss 0.300110                                        LR 0.000060    Time 0.028493    
2023-01-06 14:32:27,153 - Epoch: [39][  130/  246]    Overall Loss 0.298688    Objective Loss 0.298688                                        LR 0.000060    Time 0.028236    
2023-01-06 14:32:27,406 - Epoch: [39][  140/  246]    Overall Loss 0.298390    Objective Loss 0.298390                                        LR 0.000060    Time 0.028019    
2023-01-06 14:32:27,668 - Epoch: [39][  150/  246]    Overall Loss 0.298081    Objective Loss 0.298081                                        LR 0.000060    Time 0.027896    
2023-01-06 14:32:27,919 - Epoch: [39][  160/  246]    Overall Loss 0.296656    Objective Loss 0.296656                                        LR 0.000060    Time 0.027717    
2023-01-06 14:32:28,170 - Epoch: [39][  170/  246]    Overall Loss 0.296482    Objective Loss 0.296482                                        LR 0.000060    Time 0.027564    
2023-01-06 14:32:28,409 - Epoch: [39][  180/  246]    Overall Loss 0.297154    Objective Loss 0.297154                                        LR 0.000060    Time 0.027358    
2023-01-06 14:32:28,641 - Epoch: [39][  190/  246]    Overall Loss 0.295412    Objective Loss 0.295412                                        LR 0.000060    Time 0.027133    
2023-01-06 14:32:28,871 - Epoch: [39][  200/  246]    Overall Loss 0.294788    Objective Loss 0.294788                                        LR 0.000060    Time 0.026927    
2023-01-06 14:32:29,106 - Epoch: [39][  210/  246]    Overall Loss 0.295429    Objective Loss 0.295429                                        LR 0.000060    Time 0.026762    
2023-01-06 14:32:29,351 - Epoch: [39][  220/  246]    Overall Loss 0.295108    Objective Loss 0.295108                                        LR 0.000060    Time 0.026658    
2023-01-06 14:32:29,594 - Epoch: [39][  230/  246]    Overall Loss 0.294709    Objective Loss 0.294709                                        LR 0.000060    Time 0.026553    
2023-01-06 14:32:29,846 - Epoch: [39][  240/  246]    Overall Loss 0.295026    Objective Loss 0.295026                                        LR 0.000060    Time 0.026495    
2023-01-06 14:32:29,972 - Epoch: [39][  246/  246]    Overall Loss 0.294684    Objective Loss 0.294684    Top1 89.712919    LR 0.000060    Time 0.026362    
2023-01-06 14:32:30,086 - --- validate (epoch=39)-----------
2023-01-06 14:32:30,086 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:30,521 - Epoch: [39][   10/   28]    Loss 0.299781    Top1 89.453125    
2023-01-06 14:32:30,663 - Epoch: [39][   20/   28]    Loss 0.300895    Top1 89.472656    
2023-01-06 14:32:30,754 - Epoch: [39][   28/   28]    Loss 0.295884    Top1 89.464644    
2023-01-06 14:32:30,899 - ==> Top1: 89.465    Loss: 0.296

2023-01-06 14:32:30,899 - ==> Confusion:
[[ 186   16  237]
 [   5  242  355]
 [  45   78 5822]]

2023-01-06 14:32:30,901 - ==> Best [Top1: 89.465   Sparsity:0.00   Params: 360896 on epoch: 39]
2023-01-06 14:32:30,901 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:30,922 - 

2023-01-06 14:32:30,922 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:31,627 - Epoch: [40][   10/  246]    Overall Loss 0.291548    Objective Loss 0.291548                                        LR 0.000036    Time 0.070378    
2023-01-06 14:32:31,866 - Epoch: [40][   20/  246]    Overall Loss 0.286377    Objective Loss 0.286377                                        LR 0.000036    Time 0.047147    
2023-01-06 14:32:32,112 - Epoch: [40][   30/  246]    Overall Loss 0.277436    Objective Loss 0.277436                                        LR 0.000036    Time 0.039610    
2023-01-06 14:32:32,339 - Epoch: [40][   40/  246]    Overall Loss 0.278366    Objective Loss 0.278366                                        LR 0.000036    Time 0.035369    
2023-01-06 14:32:32,569 - Epoch: [40][   50/  246]    Overall Loss 0.279499    Objective Loss 0.279499                                        LR 0.000036    Time 0.032892    
2023-01-06 14:32:32,800 - Epoch: [40][   60/  246]    Overall Loss 0.278281    Objective Loss 0.278281                                        LR 0.000036    Time 0.031251    
2023-01-06 14:32:33,024 - Epoch: [40][   70/  246]    Overall Loss 0.285400    Objective Loss 0.285400                                        LR 0.000036    Time 0.029969    
2023-01-06 14:32:33,245 - Epoch: [40][   80/  246]    Overall Loss 0.285613    Objective Loss 0.285613                                        LR 0.000036    Time 0.028988    
2023-01-06 14:32:33,472 - Epoch: [40][   90/  246]    Overall Loss 0.286979    Objective Loss 0.286979                                        LR 0.000036    Time 0.028279    
2023-01-06 14:32:33,701 - Epoch: [40][  100/  246]    Overall Loss 0.287388    Objective Loss 0.287388                                        LR 0.000036    Time 0.027740    
2023-01-06 14:32:33,936 - Epoch: [40][  110/  246]    Overall Loss 0.286868    Objective Loss 0.286868                                        LR 0.000036    Time 0.027340    
2023-01-06 14:32:34,176 - Epoch: [40][  120/  246]    Overall Loss 0.287826    Objective Loss 0.287826                                        LR 0.000036    Time 0.027058    
2023-01-06 14:32:34,432 - Epoch: [40][  130/  246]    Overall Loss 0.287513    Objective Loss 0.287513                                        LR 0.000036    Time 0.026942    
2023-01-06 14:32:34,684 - Epoch: [40][  140/  246]    Overall Loss 0.287120    Objective Loss 0.287120                                        LR 0.000036    Time 0.026811    
2023-01-06 14:32:34,938 - Epoch: [40][  150/  246]    Overall Loss 0.286857    Objective Loss 0.286857                                        LR 0.000036    Time 0.026717    
2023-01-06 14:32:35,193 - Epoch: [40][  160/  246]    Overall Loss 0.287084    Objective Loss 0.287084                                        LR 0.000036    Time 0.026639    
2023-01-06 14:32:35,443 - Epoch: [40][  170/  246]    Overall Loss 0.286814    Objective Loss 0.286814                                        LR 0.000036    Time 0.026535    
2023-01-06 14:32:35,702 - Epoch: [40][  180/  246]    Overall Loss 0.287659    Objective Loss 0.287659                                        LR 0.000036    Time 0.026498    
2023-01-06 14:32:35,968 - Epoch: [40][  190/  246]    Overall Loss 0.287905    Objective Loss 0.287905                                        LR 0.000036    Time 0.026504    
2023-01-06 14:32:36,236 - Epoch: [40][  200/  246]    Overall Loss 0.288652    Objective Loss 0.288652                                        LR 0.000036    Time 0.026509    
2023-01-06 14:32:36,497 - Epoch: [40][  210/  246]    Overall Loss 0.287451    Objective Loss 0.287451                                        LR 0.000036    Time 0.026486    
2023-01-06 14:32:36,762 - Epoch: [40][  220/  246]    Overall Loss 0.287746    Objective Loss 0.287746                                        LR 0.000036    Time 0.026482    
2023-01-06 14:32:37,026 - Epoch: [40][  230/  246]    Overall Loss 0.288268    Objective Loss 0.288268                                        LR 0.000036    Time 0.026480    
2023-01-06 14:32:37,299 - Epoch: [40][  240/  246]    Overall Loss 0.288905    Objective Loss 0.288905                                        LR 0.000036    Time 0.026513    
2023-01-06 14:32:37,429 - Epoch: [40][  246/  246]    Overall Loss 0.288810    Objective Loss 0.288810    Top1 88.277512    LR 0.000036    Time 0.026392    
2023-01-06 14:32:37,555 - --- validate (epoch=40)-----------
2023-01-06 14:32:37,556 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:38,010 - Epoch: [40][   10/   28]    Loss 0.289553    Top1 89.648438    
2023-01-06 14:32:38,174 - Epoch: [40][   20/   28]    Loss 0.295520    Top1 89.335938    
2023-01-06 14:32:38,264 - Epoch: [40][   28/   28]    Loss 0.295102    Top1 89.421701    
2023-01-06 14:32:38,399 - ==> Top1: 89.422    Loss: 0.295

2023-01-06 14:32:38,400 - ==> Confusion:
[[ 180   11  248]
 [   8  209  385]
 [  42   45 5858]]

2023-01-06 14:32:38,401 - ==> Best [Top1: 89.465   Sparsity:0.00   Params: 360896 on epoch: 39]
2023-01-06 14:32:38,401 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:38,411 - 

2023-01-06 14:32:38,411 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:39,143 - Epoch: [41][   10/  246]    Overall Loss 0.272077    Objective Loss 0.272077                                        LR 0.000036    Time 0.073130    
2023-01-06 14:32:39,404 - Epoch: [41][   20/  246]    Overall Loss 0.286868    Objective Loss 0.286868                                        LR 0.000036    Time 0.049577    
2023-01-06 14:32:39,661 - Epoch: [41][   30/  246]    Overall Loss 0.281959    Objective Loss 0.281959                                        LR 0.000036    Time 0.041599    
2023-01-06 14:32:39,919 - Epoch: [41][   40/  246]    Overall Loss 0.282800    Objective Loss 0.282800                                        LR 0.000036    Time 0.037641    
2023-01-06 14:32:40,169 - Epoch: [41][   50/  246]    Overall Loss 0.283948    Objective Loss 0.283948                                        LR 0.000036    Time 0.035105    
2023-01-06 14:32:40,417 - Epoch: [41][   60/  246]    Overall Loss 0.285644    Objective Loss 0.285644                                        LR 0.000036    Time 0.033381    
2023-01-06 14:32:40,675 - Epoch: [41][   70/  246]    Overall Loss 0.287169    Objective Loss 0.287169                                        LR 0.000036    Time 0.032301    
2023-01-06 14:32:40,928 - Epoch: [41][   80/  246]    Overall Loss 0.287969    Objective Loss 0.287969                                        LR 0.000036    Time 0.031399    
2023-01-06 14:32:41,182 - Epoch: [41][   90/  246]    Overall Loss 0.289822    Objective Loss 0.289822                                        LR 0.000036    Time 0.030727    
2023-01-06 14:32:41,428 - Epoch: [41][  100/  246]    Overall Loss 0.289129    Objective Loss 0.289129                                        LR 0.000036    Time 0.030106    
2023-01-06 14:32:41,675 - Epoch: [41][  110/  246]    Overall Loss 0.286244    Objective Loss 0.286244                                        LR 0.000036    Time 0.029607    
2023-01-06 14:32:41,922 - Epoch: [41][  120/  246]    Overall Loss 0.285304    Objective Loss 0.285304                                        LR 0.000036    Time 0.029197    
2023-01-06 14:32:42,164 - Epoch: [41][  130/  246]    Overall Loss 0.284861    Objective Loss 0.284861                                        LR 0.000036    Time 0.028812    
2023-01-06 14:32:42,409 - Epoch: [41][  140/  246]    Overall Loss 0.283785    Objective Loss 0.283785                                        LR 0.000036    Time 0.028500    
2023-01-06 14:32:42,652 - Epoch: [41][  150/  246]    Overall Loss 0.284186    Objective Loss 0.284186                                        LR 0.000036    Time 0.028216    
2023-01-06 14:32:42,896 - Epoch: [41][  160/  246]    Overall Loss 0.285395    Objective Loss 0.285395                                        LR 0.000036    Time 0.027979    
2023-01-06 14:32:43,138 - Epoch: [41][  170/  246]    Overall Loss 0.284413    Objective Loss 0.284413                                        LR 0.000036    Time 0.027755    
2023-01-06 14:32:43,381 - Epoch: [41][  180/  246]    Overall Loss 0.285986    Objective Loss 0.285986                                        LR 0.000036    Time 0.027562    
2023-01-06 14:32:43,625 - Epoch: [41][  190/  246]    Overall Loss 0.286035    Objective Loss 0.286035                                        LR 0.000036    Time 0.027393    
2023-01-06 14:32:43,869 - Epoch: [41][  200/  246]    Overall Loss 0.286534    Objective Loss 0.286534                                        LR 0.000036    Time 0.027241    
2023-01-06 14:32:44,112 - Epoch: [41][  210/  246]    Overall Loss 0.286581    Objective Loss 0.286581                                        LR 0.000036    Time 0.027098    
2023-01-06 14:32:44,356 - Epoch: [41][  220/  246]    Overall Loss 0.286738    Objective Loss 0.286738                                        LR 0.000036    Time 0.026967    
2023-01-06 14:32:44,598 - Epoch: [41][  230/  246]    Overall Loss 0.286224    Objective Loss 0.286224                                        LR 0.000036    Time 0.026844    
2023-01-06 14:32:44,855 - Epoch: [41][  240/  246]    Overall Loss 0.286898    Objective Loss 0.286898                                        LR 0.000036    Time 0.026794    
2023-01-06 14:32:44,983 - Epoch: [41][  246/  246]    Overall Loss 0.286260    Objective Loss 0.286260    Top1 91.387560    LR 0.000036    Time 0.026661    
2023-01-06 14:32:45,120 - --- validate (epoch=41)-----------
2023-01-06 14:32:45,121 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:45,556 - Epoch: [41][   10/   28]    Loss 0.284471    Top1 89.765625    
2023-01-06 14:32:45,696 - Epoch: [41][   20/   28]    Loss 0.291487    Top1 89.257812    
2023-01-06 14:32:45,788 - Epoch: [41][   28/   28]    Loss 0.296590    Top1 89.192671    
2023-01-06 14:32:45,905 - ==> Top1: 89.193    Loss: 0.297

2023-01-06 14:32:45,905 - ==> Confusion:
[[ 165    7  267]
 [   5  208  389]
 [  36   51 5858]]

2023-01-06 14:32:45,907 - ==> Best [Top1: 89.465   Sparsity:0.00   Params: 360896 on epoch: 39]
2023-01-06 14:32:45,907 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:45,917 - 

2023-01-06 14:32:45,917 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:46,523 - Epoch: [42][   10/  246]    Overall Loss 0.282669    Objective Loss 0.282669                                        LR 0.000036    Time 0.060529    
2023-01-06 14:32:46,767 - Epoch: [42][   20/  246]    Overall Loss 0.276300    Objective Loss 0.276300                                        LR 0.000036    Time 0.042442    
2023-01-06 14:32:47,013 - Epoch: [42][   30/  246]    Overall Loss 0.276274    Objective Loss 0.276274                                        LR 0.000036    Time 0.036475    
2023-01-06 14:32:47,258 - Epoch: [42][   40/  246]    Overall Loss 0.279483    Objective Loss 0.279483                                        LR 0.000036    Time 0.033479    
2023-01-06 14:32:47,503 - Epoch: [42][   50/  246]    Overall Loss 0.283176    Objective Loss 0.283176                                        LR 0.000036    Time 0.031670    
2023-01-06 14:32:47,742 - Epoch: [42][   60/  246]    Overall Loss 0.287772    Objective Loss 0.287772                                        LR 0.000036    Time 0.030363    
2023-01-06 14:32:47,968 - Epoch: [42][   70/  246]    Overall Loss 0.286424    Objective Loss 0.286424                                        LR 0.000036    Time 0.029258    
2023-01-06 14:32:48,213 - Epoch: [42][   80/  246]    Overall Loss 0.284908    Objective Loss 0.284908                                        LR 0.000036    Time 0.028657    
2023-01-06 14:32:48,469 - Epoch: [42][   90/  246]    Overall Loss 0.288814    Objective Loss 0.288814                                        LR 0.000036    Time 0.028296    
2023-01-06 14:32:48,718 - Epoch: [42][  100/  246]    Overall Loss 0.289513    Objective Loss 0.289513                                        LR 0.000036    Time 0.027952    
2023-01-06 14:32:48,973 - Epoch: [42][  110/  246]    Overall Loss 0.287913    Objective Loss 0.287913                                        LR 0.000036    Time 0.027708    
2023-01-06 14:32:49,220 - Epoch: [42][  120/  246]    Overall Loss 0.286606    Objective Loss 0.286606                                        LR 0.000036    Time 0.027452    
2023-01-06 14:32:49,468 - Epoch: [42][  130/  246]    Overall Loss 0.287405    Objective Loss 0.287405                                        LR 0.000036    Time 0.027248    
2023-01-06 14:32:49,716 - Epoch: [42][  140/  246]    Overall Loss 0.287530    Objective Loss 0.287530                                        LR 0.000036    Time 0.027067    
2023-01-06 14:32:49,964 - Epoch: [42][  150/  246]    Overall Loss 0.287697    Objective Loss 0.287697                                        LR 0.000036    Time 0.026913    
2023-01-06 14:32:50,209 - Epoch: [42][  160/  246]    Overall Loss 0.287194    Objective Loss 0.287194                                        LR 0.000036    Time 0.026755    
2023-01-06 14:32:50,455 - Epoch: [42][  170/  246]    Overall Loss 0.285817    Objective Loss 0.285817                                        LR 0.000036    Time 0.026629    
2023-01-06 14:32:50,703 - Epoch: [42][  180/  246]    Overall Loss 0.285175    Objective Loss 0.285175                                        LR 0.000036    Time 0.026524    
2023-01-06 14:32:50,952 - Epoch: [42][  190/  246]    Overall Loss 0.284905    Objective Loss 0.284905                                        LR 0.000036    Time 0.026434    
2023-01-06 14:32:51,200 - Epoch: [42][  200/  246]    Overall Loss 0.285075    Objective Loss 0.285075                                        LR 0.000036    Time 0.026350    
2023-01-06 14:32:51,446 - Epoch: [42][  210/  246]    Overall Loss 0.285776    Objective Loss 0.285776                                        LR 0.000036    Time 0.026264    
2023-01-06 14:32:51,693 - Epoch: [42][  220/  246]    Overall Loss 0.285936    Objective Loss 0.285936                                        LR 0.000036    Time 0.026191    
2023-01-06 14:32:51,941 - Epoch: [42][  230/  246]    Overall Loss 0.285351    Objective Loss 0.285351                                        LR 0.000036    Time 0.026127    
2023-01-06 14:32:52,200 - Epoch: [42][  240/  246]    Overall Loss 0.284493    Objective Loss 0.284493                                        LR 0.000036    Time 0.026115    
2023-01-06 14:32:52,330 - Epoch: [42][  246/  246]    Overall Loss 0.284808    Objective Loss 0.284808    Top1 88.038278    LR 0.000036    Time 0.026005    
2023-01-06 14:32:52,504 - --- validate (epoch=42)-----------
2023-01-06 14:32:52,505 - 6986 samples (256 per mini-batch)
2023-01-06 14:32:52,937 - Epoch: [42][   10/   28]    Loss 0.278238    Top1 89.648438    
2023-01-06 14:32:53,077 - Epoch: [42][   20/   28]    Loss 0.279205    Top1 89.921875    
2023-01-06 14:32:53,169 - Epoch: [42][   28/   28]    Loss 0.292226    Top1 89.636416    
2023-01-06 14:32:53,302 - ==> Top1: 89.636    Loss: 0.292

2023-01-06 14:32:53,303 - ==> Confusion:
[[ 244   19  176]
 [  11  246  345]
 [  96   77 5772]]

2023-01-06 14:32:53,304 - ==> Best [Top1: 89.636   Sparsity:0.00   Params: 360896 on epoch: 42]
2023-01-06 14:32:53,304 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:32:53,325 - 

2023-01-06 14:32:53,325 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:32:54,038 - Epoch: [43][   10/  246]    Overall Loss 0.280770    Objective Loss 0.280770                                        LR 0.000036    Time 0.071230    
2023-01-06 14:32:54,277 - Epoch: [43][   20/  246]    Overall Loss 0.264622    Objective Loss 0.264622                                        LR 0.000036    Time 0.047535    
2023-01-06 14:32:54,521 - Epoch: [43][   30/  246]    Overall Loss 0.272810    Objective Loss 0.272810                                        LR 0.000036    Time 0.039819    
2023-01-06 14:32:54,769 - Epoch: [43][   40/  246]    Overall Loss 0.276131    Objective Loss 0.276131                                        LR 0.000036    Time 0.035995    
2023-01-06 14:32:55,013 - Epoch: [43][   50/  246]    Overall Loss 0.279216    Objective Loss 0.279216                                        LR 0.000036    Time 0.033667    
2023-01-06 14:32:55,237 - Epoch: [43][   60/  246]    Overall Loss 0.277455    Objective Loss 0.277455                                        LR 0.000036    Time 0.031797    
2023-01-06 14:32:55,482 - Epoch: [43][   70/  246]    Overall Loss 0.276707    Objective Loss 0.276707                                        LR 0.000036    Time 0.030750    
2023-01-06 14:32:55,727 - Epoch: [43][   80/  246]    Overall Loss 0.277325    Objective Loss 0.277325                                        LR 0.000036    Time 0.029957    
2023-01-06 14:32:55,970 - Epoch: [43][   90/  246]    Overall Loss 0.279727    Objective Loss 0.279727                                        LR 0.000036    Time 0.029326    
2023-01-06 14:32:56,212 - Epoch: [43][  100/  246]    Overall Loss 0.281494    Objective Loss 0.281494                                        LR 0.000036    Time 0.028811    
2023-01-06 14:32:56,456 - Epoch: [43][  110/  246]    Overall Loss 0.281343    Objective Loss 0.281343                                        LR 0.000036    Time 0.028413    
2023-01-06 14:32:56,709 - Epoch: [43][  120/  246]    Overall Loss 0.281377    Objective Loss 0.281377                                        LR 0.000036    Time 0.028147    
2023-01-06 14:32:56,958 - Epoch: [43][  130/  246]    Overall Loss 0.280989    Objective Loss 0.280989                                        LR 0.000036    Time 0.027895    
2023-01-06 14:32:57,211 - Epoch: [43][  140/  246]    Overall Loss 0.281463    Objective Loss 0.281463                                        LR 0.000036    Time 0.027708    
2023-01-06 14:32:57,459 - Epoch: [43][  150/  246]    Overall Loss 0.280742    Objective Loss 0.280742                                        LR 0.000036    Time 0.027510    
2023-01-06 14:32:57,711 - Epoch: [43][  160/  246]    Overall Loss 0.280366    Objective Loss 0.280366                                        LR 0.000036    Time 0.027363    
2023-01-06 14:32:57,958 - Epoch: [43][  170/  246]    Overall Loss 0.281804    Objective Loss 0.281804                                        LR 0.000036    Time 0.027208    
2023-01-06 14:32:58,202 - Epoch: [43][  180/  246]    Overall Loss 0.282855    Objective Loss 0.282855                                        LR 0.000036    Time 0.027046    
2023-01-06 14:32:58,449 - Epoch: [43][  190/  246]    Overall Loss 0.283284    Objective Loss 0.283284                                        LR 0.000036    Time 0.026923    
2023-01-06 14:32:58,699 - Epoch: [43][  200/  246]    Overall Loss 0.283029    Objective Loss 0.283029                                        LR 0.000036    Time 0.026828    
2023-01-06 14:32:58,951 - Epoch: [43][  210/  246]    Overall Loss 0.284059    Objective Loss 0.284059                                        LR 0.000036    Time 0.026745    
2023-01-06 14:32:59,193 - Epoch: [43][  220/  246]    Overall Loss 0.284197    Objective Loss 0.284197                                        LR 0.000036    Time 0.026631    
2023-01-06 14:32:59,435 - Epoch: [43][  230/  246]    Overall Loss 0.285073    Objective Loss 0.285073                                        LR 0.000036    Time 0.026524    
2023-01-06 14:32:59,690 - Epoch: [43][  240/  246]    Overall Loss 0.283575    Objective Loss 0.283575                                        LR 0.000036    Time 0.026478    
2023-01-06 14:32:59,820 - Epoch: [43][  246/  246]    Overall Loss 0.283429    Objective Loss 0.283429    Top1 89.473684    LR 0.000036    Time 0.026359    
2023-01-06 14:32:59,949 - --- validate (epoch=43)-----------
2023-01-06 14:32:59,949 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:00,400 - Epoch: [43][   10/   28]    Loss 0.300379    Top1 88.828125    
2023-01-06 14:33:00,543 - Epoch: [43][   20/   28]    Loss 0.288652    Top1 89.863281    
2023-01-06 14:33:00,635 - Epoch: [43][   28/   28]    Loss 0.294835    Top1 89.736616    
2023-01-06 14:33:00,775 - ==> Top1: 89.737    Loss: 0.295

2023-01-06 14:33:00,776 - ==> Confusion:
[[ 247    7  185]
 [  19  250  333]
 [  94   79 5772]]

2023-01-06 14:33:00,777 - ==> Best [Top1: 89.737   Sparsity:0.00   Params: 360896 on epoch: 43]
2023-01-06 14:33:00,777 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:00,798 - 

2023-01-06 14:33:00,798 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:01,571 - Epoch: [44][   10/  246]    Overall Loss 0.288585    Objective Loss 0.288585                                        LR 0.000036    Time 0.077126    
2023-01-06 14:33:01,833 - Epoch: [44][   20/  246]    Overall Loss 0.288776    Objective Loss 0.288776                                        LR 0.000036    Time 0.051659    
2023-01-06 14:33:02,101 - Epoch: [44][   30/  246]    Overall Loss 0.287986    Objective Loss 0.287986                                        LR 0.000036    Time 0.043376    
2023-01-06 14:33:02,365 - Epoch: [44][   40/  246]    Overall Loss 0.288512    Objective Loss 0.288512                                        LR 0.000036    Time 0.039126    
2023-01-06 14:33:02,621 - Epoch: [44][   50/  246]    Overall Loss 0.287613    Objective Loss 0.287613                                        LR 0.000036    Time 0.036408    
2023-01-06 14:33:02,875 - Epoch: [44][   60/  246]    Overall Loss 0.285469    Objective Loss 0.285469                                        LR 0.000036    Time 0.034560    
2023-01-06 14:33:03,128 - Epoch: [44][   70/  246]    Overall Loss 0.283904    Objective Loss 0.283904                                        LR 0.000036    Time 0.033225    
2023-01-06 14:33:03,383 - Epoch: [44][   80/  246]    Overall Loss 0.284614    Objective Loss 0.284614                                        LR 0.000036    Time 0.032263    
2023-01-06 14:33:03,637 - Epoch: [44][   90/  246]    Overall Loss 0.283293    Objective Loss 0.283293                                        LR 0.000036    Time 0.031493    
2023-01-06 14:33:03,894 - Epoch: [44][  100/  246]    Overall Loss 0.283155    Objective Loss 0.283155                                        LR 0.000036    Time 0.030911    
2023-01-06 14:33:04,148 - Epoch: [44][  110/  246]    Overall Loss 0.282725    Objective Loss 0.282725                                        LR 0.000036    Time 0.030398    
2023-01-06 14:33:04,402 - Epoch: [44][  120/  246]    Overall Loss 0.284546    Objective Loss 0.284546                                        LR 0.000036    Time 0.029985    
2023-01-06 14:33:04,655 - Epoch: [44][  130/  246]    Overall Loss 0.286924    Objective Loss 0.286924                                        LR 0.000036    Time 0.029618    
2023-01-06 14:33:04,909 - Epoch: [44][  140/  246]    Overall Loss 0.286713    Objective Loss 0.286713                                        LR 0.000036    Time 0.029313    
2023-01-06 14:33:05,158 - Epoch: [44][  150/  246]    Overall Loss 0.285719    Objective Loss 0.285719                                        LR 0.000036    Time 0.029017    
2023-01-06 14:33:05,406 - Epoch: [44][  160/  246]    Overall Loss 0.286966    Objective Loss 0.286966                                        LR 0.000036    Time 0.028750    
2023-01-06 14:33:05,656 - Epoch: [44][  170/  246]    Overall Loss 0.286137    Objective Loss 0.286137                                        LR 0.000036    Time 0.028528    
2023-01-06 14:33:05,867 - Epoch: [44][  180/  246]    Overall Loss 0.284742    Objective Loss 0.284742                                        LR 0.000036    Time 0.028112    
2023-01-06 14:33:06,080 - Epoch: [44][  190/  246]    Overall Loss 0.284515    Objective Loss 0.284515                                        LR 0.000036    Time 0.027749    
2023-01-06 14:33:06,328 - Epoch: [44][  200/  246]    Overall Loss 0.283490    Objective Loss 0.283490                                        LR 0.000036    Time 0.027599    
2023-01-06 14:33:06,573 - Epoch: [44][  210/  246]    Overall Loss 0.282578    Objective Loss 0.282578                                        LR 0.000036    Time 0.027450    
2023-01-06 14:33:06,819 - Epoch: [44][  220/  246]    Overall Loss 0.282642    Objective Loss 0.282642                                        LR 0.000036    Time 0.027318    
2023-01-06 14:33:07,066 - Epoch: [44][  230/  246]    Overall Loss 0.282080    Objective Loss 0.282080                                        LR 0.000036    Time 0.027204    
2023-01-06 14:33:07,324 - Epoch: [44][  240/  246]    Overall Loss 0.281629    Objective Loss 0.281629                                        LR 0.000036    Time 0.027145    
2023-01-06 14:33:07,454 - Epoch: [44][  246/  246]    Overall Loss 0.281424    Objective Loss 0.281424    Top1 90.191388    LR 0.000036    Time 0.027006    
2023-01-06 14:33:07,596 - --- validate (epoch=44)-----------
2023-01-06 14:33:07,597 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:08,039 - Epoch: [44][   10/   28]    Loss 0.294661    Top1 89.570312    
2023-01-06 14:33:08,187 - Epoch: [44][   20/   28]    Loss 0.290179    Top1 89.628906    
2023-01-06 14:33:08,278 - Epoch: [44][   28/   28]    Loss 0.284720    Top1 89.851131    
2023-01-06 14:33:08,427 - ==> Top1: 89.851    Loss: 0.285

2023-01-06 14:33:08,427 - ==> Confusion:
[[ 208    9  222]
 [  10  232  360]
 [  52   56 5837]]

2023-01-06 14:33:08,429 - ==> Best [Top1: 89.851   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-06 14:33:08,429 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:08,450 - 

2023-01-06 14:33:08,450 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:09,067 - Epoch: [45][   10/  246]    Overall Loss 0.278310    Objective Loss 0.278310                                        LR 0.000036    Time 0.061566    
2023-01-06 14:33:09,329 - Epoch: [45][   20/  246]    Overall Loss 0.276367    Objective Loss 0.276367                                        LR 0.000036    Time 0.043903    
2023-01-06 14:33:09,575 - Epoch: [45][   30/  246]    Overall Loss 0.277415    Objective Loss 0.277415                                        LR 0.000036    Time 0.037430    
2023-01-06 14:33:09,819 - Epoch: [45][   40/  246]    Overall Loss 0.275357    Objective Loss 0.275357                                        LR 0.000036    Time 0.034177    
2023-01-06 14:33:10,061 - Epoch: [45][   50/  246]    Overall Loss 0.274681    Objective Loss 0.274681                                        LR 0.000036    Time 0.032164    
2023-01-06 14:33:10,305 - Epoch: [45][   60/  246]    Overall Loss 0.274233    Objective Loss 0.274233                                        LR 0.000036    Time 0.030862    
2023-01-06 14:33:10,548 - Epoch: [45][   70/  246]    Overall Loss 0.276265    Objective Loss 0.276265                                        LR 0.000036    Time 0.029922    
2023-01-06 14:33:10,791 - Epoch: [45][   80/  246]    Overall Loss 0.280641    Objective Loss 0.280641                                        LR 0.000036    Time 0.029218    
2023-01-06 14:33:11,035 - Epoch: [45][   90/  246]    Overall Loss 0.279408    Objective Loss 0.279408                                        LR 0.000036    Time 0.028683    
2023-01-06 14:33:11,280 - Epoch: [45][  100/  246]    Overall Loss 0.278957    Objective Loss 0.278957                                        LR 0.000036    Time 0.028259    
2023-01-06 14:33:11,525 - Epoch: [45][  110/  246]    Overall Loss 0.279449    Objective Loss 0.279449                                        LR 0.000036    Time 0.027912    
2023-01-06 14:33:11,770 - Epoch: [45][  120/  246]    Overall Loss 0.278827    Objective Loss 0.278827                                        LR 0.000036    Time 0.027625    
2023-01-06 14:33:12,015 - Epoch: [45][  130/  246]    Overall Loss 0.279363    Objective Loss 0.279363                                        LR 0.000036    Time 0.027375    
2023-01-06 14:33:12,257 - Epoch: [45][  140/  246]    Overall Loss 0.280046    Objective Loss 0.280046                                        LR 0.000036    Time 0.027146    
2023-01-06 14:33:12,502 - Epoch: [45][  150/  246]    Overall Loss 0.281055    Objective Loss 0.281055                                        LR 0.000036    Time 0.026968    
2023-01-06 14:33:12,747 - Epoch: [45][  160/  246]    Overall Loss 0.280625    Objective Loss 0.280625                                        LR 0.000036    Time 0.026809    
2023-01-06 14:33:12,989 - Epoch: [45][  170/  246]    Overall Loss 0.279995    Objective Loss 0.279995                                        LR 0.000036    Time 0.026654    
2023-01-06 14:33:13,228 - Epoch: [45][  180/  246]    Overall Loss 0.280029    Objective Loss 0.280029                                        LR 0.000036    Time 0.026502    
2023-01-06 14:33:13,467 - Epoch: [45][  190/  246]    Overall Loss 0.280163    Objective Loss 0.280163                                        LR 0.000036    Time 0.026361    
2023-01-06 14:33:13,721 - Epoch: [45][  200/  246]    Overall Loss 0.281536    Objective Loss 0.281536                                        LR 0.000036    Time 0.026309    
2023-01-06 14:33:13,971 - Epoch: [45][  210/  246]    Overall Loss 0.282160    Objective Loss 0.282160                                        LR 0.000036    Time 0.026243    
2023-01-06 14:33:14,234 - Epoch: [45][  220/  246]    Overall Loss 0.281990    Objective Loss 0.281990                                        LR 0.000036    Time 0.026239    
2023-01-06 14:33:14,486 - Epoch: [45][  230/  246]    Overall Loss 0.282033    Objective Loss 0.282033                                        LR 0.000036    Time 0.026194    
2023-01-06 14:33:14,741 - Epoch: [45][  240/  246]    Overall Loss 0.281631    Objective Loss 0.281631                                        LR 0.000036    Time 0.026165    
2023-01-06 14:33:14,870 - Epoch: [45][  246/  246]    Overall Loss 0.281877    Objective Loss 0.281877    Top1 88.038278    LR 0.000036    Time 0.026047    
2023-01-06 14:33:15,003 - --- validate (epoch=45)-----------
2023-01-06 14:33:15,004 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:15,474 - Epoch: [45][   10/   28]    Loss 0.286881    Top1 89.570312    
2023-01-06 14:33:15,627 - Epoch: [45][   20/   28]    Loss 0.294160    Top1 89.394531    
2023-01-06 14:33:15,717 - Epoch: [45][   28/   28]    Loss 0.286263    Top1 89.665044    
2023-01-06 14:33:15,860 - ==> Top1: 89.665    Loss: 0.286

2023-01-06 14:33:15,861 - ==> Confusion:
[[ 235    8  196]
 [  15  209  378]
 [  73   52 5820]]

2023-01-06 14:33:15,862 - ==> Best [Top1: 89.851   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-06 14:33:15,862 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:15,872 - 

2023-01-06 14:33:15,872 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:16,621 - Epoch: [46][   10/  246]    Overall Loss 0.289065    Objective Loss 0.289065                                        LR 0.000036    Time 0.074847    
2023-01-06 14:33:16,871 - Epoch: [46][   20/  246]    Overall Loss 0.294832    Objective Loss 0.294832                                        LR 0.000036    Time 0.049873    
2023-01-06 14:33:17,131 - Epoch: [46][   30/  246]    Overall Loss 0.285333    Objective Loss 0.285333                                        LR 0.000036    Time 0.041912    
2023-01-06 14:33:17,381 - Epoch: [46][   40/  246]    Overall Loss 0.283126    Objective Loss 0.283126                                        LR 0.000036    Time 0.037656    
2023-01-06 14:33:17,632 - Epoch: [46][   50/  246]    Overall Loss 0.286999    Objective Loss 0.286999                                        LR 0.000036    Time 0.035140    
2023-01-06 14:33:17,888 - Epoch: [46][   60/  246]    Overall Loss 0.285420    Objective Loss 0.285420                                        LR 0.000036    Time 0.033545    
2023-01-06 14:33:18,151 - Epoch: [46][   70/  246]    Overall Loss 0.286125    Objective Loss 0.286125                                        LR 0.000036    Time 0.032471    
2023-01-06 14:33:18,406 - Epoch: [46][   80/  246]    Overall Loss 0.284047    Objective Loss 0.284047                                        LR 0.000036    Time 0.031600    
2023-01-06 14:33:18,667 - Epoch: [46][   90/  246]    Overall Loss 0.284687    Objective Loss 0.284687                                        LR 0.000036    Time 0.030967    
2023-01-06 14:33:18,923 - Epoch: [46][  100/  246]    Overall Loss 0.284469    Objective Loss 0.284469                                        LR 0.000036    Time 0.030419    
2023-01-06 14:33:19,183 - Epoch: [46][  110/  246]    Overall Loss 0.282525    Objective Loss 0.282525                                        LR 0.000036    Time 0.030002    
2023-01-06 14:33:19,438 - Epoch: [46][  120/  246]    Overall Loss 0.284131    Objective Loss 0.284131                                        LR 0.000036    Time 0.029623    
2023-01-06 14:33:19,698 - Epoch: [46][  130/  246]    Overall Loss 0.282637    Objective Loss 0.282637                                        LR 0.000036    Time 0.029328    
2023-01-06 14:33:19,952 - Epoch: [46][  140/  246]    Overall Loss 0.281941    Objective Loss 0.281941                                        LR 0.000036    Time 0.029047    
2023-01-06 14:33:20,209 - Epoch: [46][  150/  246]    Overall Loss 0.283245    Objective Loss 0.283245                                        LR 0.000036    Time 0.028809    
2023-01-06 14:33:20,464 - Epoch: [46][  160/  246]    Overall Loss 0.283842    Objective Loss 0.283842                                        LR 0.000036    Time 0.028595    
2023-01-06 14:33:20,724 - Epoch: [46][  170/  246]    Overall Loss 0.282715    Objective Loss 0.282715                                        LR 0.000036    Time 0.028431    
2023-01-06 14:33:20,984 - Epoch: [46][  180/  246]    Overall Loss 0.282653    Objective Loss 0.282653                                        LR 0.000036    Time 0.028294    
2023-01-06 14:33:21,249 - Epoch: [46][  190/  246]    Overall Loss 0.282290    Objective Loss 0.282290                                        LR 0.000036    Time 0.028198    
2023-01-06 14:33:21,508 - Epoch: [46][  200/  246]    Overall Loss 0.281010    Objective Loss 0.281010                                        LR 0.000036    Time 0.028082    
2023-01-06 14:33:21,768 - Epoch: [46][  210/  246]    Overall Loss 0.279458    Objective Loss 0.279458                                        LR 0.000036    Time 0.027982    
2023-01-06 14:33:22,021 - Epoch: [46][  220/  246]    Overall Loss 0.280065    Objective Loss 0.280065                                        LR 0.000036    Time 0.027857    
2023-01-06 14:33:22,277 - Epoch: [46][  230/  246]    Overall Loss 0.279510    Objective Loss 0.279510                                        LR 0.000036    Time 0.027756    
2023-01-06 14:33:22,539 - Epoch: [46][  240/  246]    Overall Loss 0.278966    Objective Loss 0.278966                                        LR 0.000036    Time 0.027690    
2023-01-06 14:33:22,669 - Epoch: [46][  246/  246]    Overall Loss 0.278512    Objective Loss 0.278512    Top1 89.234450    LR 0.000036    Time 0.027541    
2023-01-06 14:33:22,800 - --- validate (epoch=46)-----------
2023-01-06 14:33:22,800 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:23,242 - Epoch: [46][   10/   28]    Loss 0.287973    Top1 89.726562    
2023-01-06 14:33:23,385 - Epoch: [46][   20/   28]    Loss 0.297875    Top1 89.316406    
2023-01-06 14:33:23,477 - Epoch: [46][   28/   28]    Loss 0.288159    Top1 89.822502    
2023-01-06 14:33:23,601 - ==> Top1: 89.823    Loss: 0.288

2023-01-06 14:33:23,602 - ==> Confusion:
[[ 216    7  216]
 [  15  243  344]
 [  58   71 5816]]

2023-01-06 14:33:23,603 - ==> Best [Top1: 89.851   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-06 14:33:23,603 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:23,623 - 

2023-01-06 14:33:23,623 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:24,231 - Epoch: [47][   10/  246]    Overall Loss 0.277142    Objective Loss 0.277142                                        LR 0.000036    Time 0.060710    
2023-01-06 14:33:24,491 - Epoch: [47][   20/  246]    Overall Loss 0.274861    Objective Loss 0.274861                                        LR 0.000036    Time 0.043307    
2023-01-06 14:33:24,756 - Epoch: [47][   30/  246]    Overall Loss 0.273791    Objective Loss 0.273791                                        LR 0.000036    Time 0.037695    
2023-01-06 14:33:25,018 - Epoch: [47][   40/  246]    Overall Loss 0.275800    Objective Loss 0.275800                                        LR 0.000036    Time 0.034803    
2023-01-06 14:33:25,279 - Epoch: [47][   50/  246]    Overall Loss 0.278631    Objective Loss 0.278631                                        LR 0.000036    Time 0.033067    
2023-01-06 14:33:25,539 - Epoch: [47][   60/  246]    Overall Loss 0.277047    Objective Loss 0.277047                                        LR 0.000036    Time 0.031883    
2023-01-06 14:33:25,795 - Epoch: [47][   70/  246]    Overall Loss 0.275338    Objective Loss 0.275338                                        LR 0.000036    Time 0.030983    
2023-01-06 14:33:26,049 - Epoch: [47][   80/  246]    Overall Loss 0.277845    Objective Loss 0.277845                                        LR 0.000036    Time 0.030279    
2023-01-06 14:33:26,301 - Epoch: [47][   90/  246]    Overall Loss 0.278176    Objective Loss 0.278176                                        LR 0.000036    Time 0.029712    
2023-01-06 14:33:26,559 - Epoch: [47][  100/  246]    Overall Loss 0.277904    Objective Loss 0.277904                                        LR 0.000036    Time 0.029313    
2023-01-06 14:33:26,813 - Epoch: [47][  110/  246]    Overall Loss 0.276869    Objective Loss 0.276869                                        LR 0.000036    Time 0.028955    
2023-01-06 14:33:27,064 - Epoch: [47][  120/  246]    Overall Loss 0.277500    Objective Loss 0.277500                                        LR 0.000036    Time 0.028627    
2023-01-06 14:33:27,313 - Epoch: [47][  130/  246]    Overall Loss 0.277226    Objective Loss 0.277226                                        LR 0.000036    Time 0.028338    
2023-01-06 14:33:27,560 - Epoch: [47][  140/  246]    Overall Loss 0.277014    Objective Loss 0.277014                                        LR 0.000036    Time 0.028075    
2023-01-06 14:33:27,808 - Epoch: [47][  150/  246]    Overall Loss 0.276167    Objective Loss 0.276167                                        LR 0.000036    Time 0.027852    
2023-01-06 14:33:28,055 - Epoch: [47][  160/  246]    Overall Loss 0.276358    Objective Loss 0.276358                                        LR 0.000036    Time 0.027653    
2023-01-06 14:33:28,304 - Epoch: [47][  170/  246]    Overall Loss 0.276707    Objective Loss 0.276707                                        LR 0.000036    Time 0.027489    
2023-01-06 14:33:28,551 - Epoch: [47][  180/  246]    Overall Loss 0.276060    Objective Loss 0.276060                                        LR 0.000036    Time 0.027333    
2023-01-06 14:33:28,803 - Epoch: [47][  190/  246]    Overall Loss 0.275671    Objective Loss 0.275671                                        LR 0.000036    Time 0.027215    
2023-01-06 14:33:29,051 - Epoch: [47][  200/  246]    Overall Loss 0.275641    Objective Loss 0.275641                                        LR 0.000036    Time 0.027090    
2023-01-06 14:33:29,293 - Epoch: [47][  210/  246]    Overall Loss 0.276949    Objective Loss 0.276949                                        LR 0.000036    Time 0.026953    
2023-01-06 14:33:29,544 - Epoch: [47][  220/  246]    Overall Loss 0.276970    Objective Loss 0.276970                                        LR 0.000036    Time 0.026866    
2023-01-06 14:33:29,796 - Epoch: [47][  230/  246]    Overall Loss 0.277213    Objective Loss 0.277213                                        LR 0.000036    Time 0.026790    
2023-01-06 14:33:30,058 - Epoch: [47][  240/  246]    Overall Loss 0.276906    Objective Loss 0.276906                                        LR 0.000036    Time 0.026767    
2023-01-06 14:33:30,186 - Epoch: [47][  246/  246]    Overall Loss 0.276741    Objective Loss 0.276741    Top1 90.191388    LR 0.000036    Time 0.026634    
2023-01-06 14:33:30,324 - --- validate (epoch=47)-----------
2023-01-06 14:33:30,324 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:30,771 - Epoch: [47][   10/   28]    Loss 0.310525    Top1 88.867188    
2023-01-06 14:33:30,920 - Epoch: [47][   20/   28]    Loss 0.295336    Top1 89.550781    
2023-01-06 14:33:31,010 - Epoch: [47][   28/   28]    Loss 0.293579    Top1 89.722302    
2023-01-06 14:33:31,134 - ==> Top1: 89.722    Loss: 0.294

2023-01-06 14:33:31,134 - ==> Confusion:
[[ 211   10  218]
 [  14  225  363]
 [  57   56 5832]]

2023-01-06 14:33:31,135 - ==> Best [Top1: 89.851   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-06 14:33:31,135 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:31,145 - 

2023-01-06 14:33:31,145 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:31,901 - Epoch: [48][   10/  246]    Overall Loss 0.286538    Objective Loss 0.286538                                        LR 0.000036    Time 0.075477    
2023-01-06 14:33:32,156 - Epoch: [48][   20/  246]    Overall Loss 0.276797    Objective Loss 0.276797                                        LR 0.000036    Time 0.050444    
2023-01-06 14:33:32,396 - Epoch: [48][   30/  246]    Overall Loss 0.271132    Objective Loss 0.271132                                        LR 0.000036    Time 0.041612    
2023-01-06 14:33:32,638 - Epoch: [48][   40/  246]    Overall Loss 0.270349    Objective Loss 0.270349                                        LR 0.000036    Time 0.037248    
2023-01-06 14:33:32,882 - Epoch: [48][   50/  246]    Overall Loss 0.275476    Objective Loss 0.275476                                        LR 0.000036    Time 0.034666    
2023-01-06 14:33:33,129 - Epoch: [48][   60/  246]    Overall Loss 0.277008    Objective Loss 0.277008                                        LR 0.000036    Time 0.032990    
2023-01-06 14:33:33,379 - Epoch: [48][   70/  246]    Overall Loss 0.278997    Objective Loss 0.278997                                        LR 0.000036    Time 0.031830    
2023-01-06 14:33:33,628 - Epoch: [48][   80/  246]    Overall Loss 0.280098    Objective Loss 0.280098                                        LR 0.000036    Time 0.030963    
2023-01-06 14:33:33,884 - Epoch: [48][   90/  246]    Overall Loss 0.279545    Objective Loss 0.279545                                        LR 0.000036    Time 0.030364    
2023-01-06 14:33:34,139 - Epoch: [48][  100/  246]    Overall Loss 0.278509    Objective Loss 0.278509                                        LR 0.000036    Time 0.029874    
2023-01-06 14:33:34,387 - Epoch: [48][  110/  246]    Overall Loss 0.277981    Objective Loss 0.277981                                        LR 0.000036    Time 0.029406    
2023-01-06 14:33:34,636 - Epoch: [48][  120/  246]    Overall Loss 0.277894    Objective Loss 0.277894                                        LR 0.000036    Time 0.029029    
2023-01-06 14:33:34,892 - Epoch: [48][  130/  246]    Overall Loss 0.279288    Objective Loss 0.279288                                        LR 0.000036    Time 0.028762    
2023-01-06 14:33:35,138 - Epoch: [48][  140/  246]    Overall Loss 0.278294    Objective Loss 0.278294                                        LR 0.000036    Time 0.028463    
2023-01-06 14:33:35,392 - Epoch: [48][  150/  246]    Overall Loss 0.278750    Objective Loss 0.278750                                        LR 0.000036    Time 0.028255    
2023-01-06 14:33:35,642 - Epoch: [48][  160/  246]    Overall Loss 0.278743    Objective Loss 0.278743                                        LR 0.000036    Time 0.028046    
2023-01-06 14:33:35,892 - Epoch: [48][  170/  246]    Overall Loss 0.279570    Objective Loss 0.279570                                        LR 0.000036    Time 0.027868    
2023-01-06 14:33:36,141 - Epoch: [48][  180/  246]    Overall Loss 0.278558    Objective Loss 0.278558                                        LR 0.000036    Time 0.027699    
2023-01-06 14:33:36,392 - Epoch: [48][  190/  246]    Overall Loss 0.277932    Objective Loss 0.277932                                        LR 0.000036    Time 0.027559    
2023-01-06 14:33:36,638 - Epoch: [48][  200/  246]    Overall Loss 0.277658    Objective Loss 0.277658                                        LR 0.000036    Time 0.027411    
2023-01-06 14:33:36,880 - Epoch: [48][  210/  246]    Overall Loss 0.277997    Objective Loss 0.277997                                        LR 0.000036    Time 0.027255    
2023-01-06 14:33:37,123 - Epoch: [48][  220/  246]    Overall Loss 0.277996    Objective Loss 0.277996                                        LR 0.000036    Time 0.027118    
2023-01-06 14:33:37,366 - Epoch: [48][  230/  246]    Overall Loss 0.279309    Objective Loss 0.279309                                        LR 0.000036    Time 0.026994    
2023-01-06 14:33:37,626 - Epoch: [48][  240/  246]    Overall Loss 0.278708    Objective Loss 0.278708                                        LR 0.000036    Time 0.026949    
2023-01-06 14:33:37,755 - Epoch: [48][  246/  246]    Overall Loss 0.278336    Objective Loss 0.278336    Top1 92.583732    LR 0.000036    Time 0.026816    
2023-01-06 14:33:37,878 - --- validate (epoch=48)-----------
2023-01-06 14:33:37,878 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:38,330 - Epoch: [48][   10/   28]    Loss 0.298748    Top1 90.000000    
2023-01-06 14:33:38,471 - Epoch: [48][   20/   28]    Loss 0.286265    Top1 89.921875    
2023-01-06 14:33:38,564 - Epoch: [48][   28/   28]    Loss 0.287438    Top1 90.022903    
2023-01-06 14:33:38,733 - ==> Top1: 90.023    Loss: 0.287

2023-01-06 14:33:38,734 - ==> Confusion:
[[ 221    9  209]
 [  11  240  351]
 [  62   55 5828]]

2023-01-06 14:33:38,735 - ==> Best [Top1: 90.023   Sparsity:0.00   Params: 360896 on epoch: 48]
2023-01-06 14:33:38,735 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:38,759 - 

2023-01-06 14:33:38,760 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:39,510 - Epoch: [49][   10/  246]    Overall Loss 0.263767    Objective Loss 0.263767                                        LR 0.000036    Time 0.074996    
2023-01-06 14:33:39,761 - Epoch: [49][   20/  246]    Overall Loss 0.267335    Objective Loss 0.267335                                        LR 0.000036    Time 0.049993    
2023-01-06 14:33:40,007 - Epoch: [49][   30/  246]    Overall Loss 0.268733    Objective Loss 0.268733                                        LR 0.000036    Time 0.041532    
2023-01-06 14:33:40,251 - Epoch: [49][   40/  246]    Overall Loss 0.271181    Objective Loss 0.271181                                        LR 0.000036    Time 0.037232    
2023-01-06 14:33:40,496 - Epoch: [49][   50/  246]    Overall Loss 0.272206    Objective Loss 0.272206                                        LR 0.000036    Time 0.034682    
2023-01-06 14:33:40,744 - Epoch: [49][   60/  246]    Overall Loss 0.271796    Objective Loss 0.271796                                        LR 0.000036    Time 0.033037    
2023-01-06 14:33:40,992 - Epoch: [49][   70/  246]    Overall Loss 0.274392    Objective Loss 0.274392                                        LR 0.000036    Time 0.031852    
2023-01-06 14:33:41,240 - Epoch: [49][   80/  246]    Overall Loss 0.275384    Objective Loss 0.275384                                        LR 0.000036    Time 0.030968    
2023-01-06 14:33:41,485 - Epoch: [49][   90/  246]    Overall Loss 0.275029    Objective Loss 0.275029                                        LR 0.000036    Time 0.030239    
2023-01-06 14:33:41,728 - Epoch: [49][  100/  246]    Overall Loss 0.273299    Objective Loss 0.273299                                        LR 0.000036    Time 0.029644    
2023-01-06 14:33:41,972 - Epoch: [49][  110/  246]    Overall Loss 0.273930    Objective Loss 0.273930                                        LR 0.000036    Time 0.029164    
2023-01-06 14:33:42,215 - Epoch: [49][  120/  246]    Overall Loss 0.274644    Objective Loss 0.274644                                        LR 0.000036    Time 0.028752    
2023-01-06 14:33:42,458 - Epoch: [49][  130/  246]    Overall Loss 0.274636    Objective Loss 0.274636                                        LR 0.000036    Time 0.028411    
2023-01-06 14:33:42,715 - Epoch: [49][  140/  246]    Overall Loss 0.275021    Objective Loss 0.275021                                        LR 0.000036    Time 0.028209    
2023-01-06 14:33:42,959 - Epoch: [49][  150/  246]    Overall Loss 0.275539    Objective Loss 0.275539                                        LR 0.000036    Time 0.027954    
2023-01-06 14:33:43,207 - Epoch: [49][  160/  246]    Overall Loss 0.275856    Objective Loss 0.275856                                        LR 0.000036    Time 0.027757    
2023-01-06 14:33:43,454 - Epoch: [49][  170/  246]    Overall Loss 0.275532    Objective Loss 0.275532                                        LR 0.000036    Time 0.027575    
2023-01-06 14:33:43,700 - Epoch: [49][  180/  246]    Overall Loss 0.274794    Objective Loss 0.274794                                        LR 0.000036    Time 0.027408    
2023-01-06 14:33:43,949 - Epoch: [49][  190/  246]    Overall Loss 0.274404    Objective Loss 0.274404                                        LR 0.000036    Time 0.027269    
2023-01-06 14:33:44,202 - Epoch: [49][  200/  246]    Overall Loss 0.274589    Objective Loss 0.274589                                        LR 0.000036    Time 0.027169    
2023-01-06 14:33:44,457 - Epoch: [49][  210/  246]    Overall Loss 0.275633    Objective Loss 0.275633                                        LR 0.000036    Time 0.027089    
2023-01-06 14:33:44,720 - Epoch: [49][  220/  246]    Overall Loss 0.276297    Objective Loss 0.276297                                        LR 0.000036    Time 0.027049    
2023-01-06 14:33:44,983 - Epoch: [49][  230/  246]    Overall Loss 0.275680    Objective Loss 0.275680                                        LR 0.000036    Time 0.027017    
2023-01-06 14:33:45,247 - Epoch: [49][  240/  246]    Overall Loss 0.276845    Objective Loss 0.276845                                        LR 0.000036    Time 0.026991    
2023-01-06 14:33:45,379 - Epoch: [49][  246/  246]    Overall Loss 0.276290    Objective Loss 0.276290    Top1 93.062201    LR 0.000036    Time 0.026866    
2023-01-06 14:33:45,505 - --- validate (epoch=49)-----------
2023-01-06 14:33:45,505 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:45,948 - Epoch: [49][   10/   28]    Loss 0.270257    Top1 90.937500    
2023-01-06 14:33:46,091 - Epoch: [49][   20/   28]    Loss 0.283485    Top1 90.078125    
2023-01-06 14:33:46,183 - Epoch: [49][   28/   28]    Loss 0.285788    Top1 90.065846    
2023-01-06 14:33:46,317 - ==> Top1: 90.066    Loss: 0.286

2023-01-06 14:33:46,317 - ==> Confusion:
[[ 218   18  203]
 [   8  286  308]
 [  56  101 5788]]

2023-01-06 14:33:46,319 - ==> Best [Top1: 90.066   Sparsity:0.00   Params: 360896 on epoch: 49]
2023-01-06 14:33:46,319 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:46,348 - 

2023-01-06 14:33:46,349 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:46,963 - Epoch: [50][   10/  246]    Overall Loss 0.266619    Objective Loss 0.266619                                        LR 0.000036    Time 0.061270    
2023-01-06 14:33:47,214 - Epoch: [50][   20/  246]    Overall Loss 0.272010    Objective Loss 0.272010                                        LR 0.000036    Time 0.043205    
2023-01-06 14:33:47,467 - Epoch: [50][   30/  246]    Overall Loss 0.274991    Objective Loss 0.274991                                        LR 0.000036    Time 0.037196    
2023-01-06 14:33:47,720 - Epoch: [50][   40/  246]    Overall Loss 0.278013    Objective Loss 0.278013                                        LR 0.000036    Time 0.034222    
2023-01-06 14:33:47,968 - Epoch: [50][   50/  246]    Overall Loss 0.277392    Objective Loss 0.277392                                        LR 0.000036    Time 0.032342    
2023-01-06 14:33:48,220 - Epoch: [50][   60/  246]    Overall Loss 0.276119    Objective Loss 0.276119                                        LR 0.000036    Time 0.031145    
2023-01-06 14:33:48,483 - Epoch: [50][   70/  246]    Overall Loss 0.274071    Objective Loss 0.274071                                        LR 0.000036    Time 0.030441    
2023-01-06 14:33:48,758 - Epoch: [50][   80/  246]    Overall Loss 0.274434    Objective Loss 0.274434                                        LR 0.000036    Time 0.030060    
2023-01-06 14:33:48,989 - Epoch: [50][   90/  246]    Overall Loss 0.272060    Objective Loss 0.272060                                        LR 0.000036    Time 0.029284    
2023-01-06 14:33:49,214 - Epoch: [50][  100/  246]    Overall Loss 0.272970    Objective Loss 0.272970                                        LR 0.000036    Time 0.028600    
2023-01-06 14:33:49,448 - Epoch: [50][  110/  246]    Overall Loss 0.274009    Objective Loss 0.274009                                        LR 0.000036    Time 0.028126    
2023-01-06 14:33:49,709 - Epoch: [50][  120/  246]    Overall Loss 0.272676    Objective Loss 0.272676                                        LR 0.000036    Time 0.027953    
2023-01-06 14:33:49,963 - Epoch: [50][  130/  246]    Overall Loss 0.271407    Objective Loss 0.271407                                        LR 0.000036    Time 0.027757    
2023-01-06 14:33:50,209 - Epoch: [50][  140/  246]    Overall Loss 0.271326    Objective Loss 0.271326                                        LR 0.000036    Time 0.027529    
2023-01-06 14:33:50,463 - Epoch: [50][  150/  246]    Overall Loss 0.271790    Objective Loss 0.271790                                        LR 0.000036    Time 0.027380    
2023-01-06 14:33:50,717 - Epoch: [50][  160/  246]    Overall Loss 0.272769    Objective Loss 0.272769                                        LR 0.000036    Time 0.027259    
2023-01-06 14:33:50,976 - Epoch: [50][  170/  246]    Overall Loss 0.274023    Objective Loss 0.274023                                        LR 0.000036    Time 0.027175    
2023-01-06 14:33:51,207 - Epoch: [50][  180/  246]    Overall Loss 0.274171    Objective Loss 0.274171                                        LR 0.000036    Time 0.026945    
2023-01-06 14:33:51,432 - Epoch: [50][  190/  246]    Overall Loss 0.274645    Objective Loss 0.274645                                        LR 0.000036    Time 0.026709    
2023-01-06 14:33:51,664 - Epoch: [50][  200/  246]    Overall Loss 0.274767    Objective Loss 0.274767                                        LR 0.000036    Time 0.026535    
2023-01-06 14:33:51,889 - Epoch: [50][  210/  246]    Overall Loss 0.273894    Objective Loss 0.273894                                        LR 0.000036    Time 0.026338    
2023-01-06 14:33:52,101 - Epoch: [50][  220/  246]    Overall Loss 0.274803    Objective Loss 0.274803                                        LR 0.000036    Time 0.026105    
2023-01-06 14:33:52,317 - Epoch: [50][  230/  246]    Overall Loss 0.275130    Objective Loss 0.275130                                        LR 0.000036    Time 0.025905    
2023-01-06 14:33:52,543 - Epoch: [50][  240/  246]    Overall Loss 0.274581    Objective Loss 0.274581                                        LR 0.000036    Time 0.025767    
2023-01-06 14:33:52,667 - Epoch: [50][  246/  246]    Overall Loss 0.273992    Objective Loss 0.273992    Top1 91.148325    LR 0.000036    Time 0.025642    
2023-01-06 14:33:52,813 - --- validate (epoch=50)-----------
2023-01-06 14:33:52,813 - 6986 samples (256 per mini-batch)
2023-01-06 14:33:53,275 - Epoch: [50][   10/   28]    Loss 0.274861    Top1 90.000000    
2023-01-06 14:33:53,430 - Epoch: [50][   20/   28]    Loss 0.277596    Top1 89.882812    
2023-01-06 14:33:53,521 - Epoch: [50][   28/   28]    Loss 0.280224    Top1 89.894074    
2023-01-06 14:33:53,679 - ==> Top1: 89.894    Loss: 0.280

2023-01-06 14:33:53,679 - ==> Confusion:
[[ 220   15  204]
 [  15  252  335]
 [  70   67 5808]]

2023-01-06 14:33:53,680 - ==> Best [Top1: 90.066   Sparsity:0.00   Params: 360896 on epoch: 49]
2023-01-06 14:33:53,680 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:33:53,690 - 

2023-01-06 14:33:53,690 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:33:54,406 - Epoch: [51][   10/  246]    Overall Loss 0.264635    Objective Loss 0.264635                                        LR 0.000036    Time 0.071476    
2023-01-06 14:33:54,657 - Epoch: [51][   20/  246]    Overall Loss 0.264111    Objective Loss 0.264111                                        LR 0.000036    Time 0.048263    
2023-01-06 14:33:54,913 - Epoch: [51][   30/  246]    Overall Loss 0.267007    Objective Loss 0.267007                                        LR 0.000036    Time 0.040678    
2023-01-06 14:33:55,180 - Epoch: [51][   40/  246]    Overall Loss 0.269612    Objective Loss 0.269612                                        LR 0.000036    Time 0.037180    
2023-01-06 14:33:55,437 - Epoch: [51][   50/  246]    Overall Loss 0.272474    Objective Loss 0.272474                                        LR 0.000036    Time 0.034876    
2023-01-06 14:33:55,702 - Epoch: [51][   60/  246]    Overall Loss 0.275785    Objective Loss 0.275785                                        LR 0.000036    Time 0.033452    
2023-01-06 14:33:55,964 - Epoch: [51][   70/  246]    Overall Loss 0.273862    Objective Loss 0.273862                                        LR 0.000036    Time 0.032410    
2023-01-06 14:33:56,202 - Epoch: [51][   80/  246]    Overall Loss 0.272850    Objective Loss 0.272850                                        LR 0.000036    Time 0.031330    
2023-01-06 14:33:56,419 - Epoch: [51][   90/  246]    Overall Loss 0.272683    Objective Loss 0.272683                                        LR 0.000036    Time 0.030258    
2023-01-06 14:33:56,646 - Epoch: [51][  100/  246]    Overall Loss 0.274609    Objective Loss 0.274609                                        LR 0.000036    Time 0.029496    
2023-01-06 14:33:56,872 - Epoch: [51][  110/  246]    Overall Loss 0.274254    Objective Loss 0.274254                                        LR 0.000036    Time 0.028864    
2023-01-06 14:33:57,133 - Epoch: [51][  120/  246]    Overall Loss 0.275725    Objective Loss 0.275725                                        LR 0.000036    Time 0.028637    
2023-01-06 14:33:57,398 - Epoch: [51][  130/  246]    Overall Loss 0.275252    Objective Loss 0.275252                                        LR 0.000036    Time 0.028468    
2023-01-06 14:33:57,664 - Epoch: [51][  140/  246]    Overall Loss 0.274173    Objective Loss 0.274173                                        LR 0.000036    Time 0.028328    
2023-01-06 14:33:57,929 - Epoch: [51][  150/  246]    Overall Loss 0.274271    Objective Loss 0.274271                                        LR 0.000036    Time 0.028204    
2023-01-06 14:33:58,194 - Epoch: [51][  160/  246]    Overall Loss 0.275558    Objective Loss 0.275558                                        LR 0.000036    Time 0.028094    
2023-01-06 14:33:58,460 - Epoch: [51][  170/  246]    Overall Loss 0.274752    Objective Loss 0.274752                                        LR 0.000036    Time 0.028005    
2023-01-06 14:33:58,726 - Epoch: [51][  180/  246]    Overall Loss 0.275400    Objective Loss 0.275400                                        LR 0.000036    Time 0.027923    
2023-01-06 14:33:58,993 - Epoch: [51][  190/  246]    Overall Loss 0.275076    Objective Loss 0.275076                                        LR 0.000036    Time 0.027858    
2023-01-06 14:33:59,230 - Epoch: [51][  200/  246]    Overall Loss 0.274818    Objective Loss 0.274818                                        LR 0.000036    Time 0.027635    
2023-01-06 14:33:59,498 - Epoch: [51][  210/  246]    Overall Loss 0.275048    Objective Loss 0.275048                                        LR 0.000036    Time 0.027593    
2023-01-06 14:33:59,739 - Epoch: [51][  220/  246]    Overall Loss 0.273991    Objective Loss 0.273991                                        LR 0.000036    Time 0.027433    
2023-01-06 14:33:59,988 - Epoch: [51][  230/  246]    Overall Loss 0.274520    Objective Loss 0.274520                                        LR 0.000036    Time 0.027320    
2023-01-06 14:34:00,247 - Epoch: [51][  240/  246]    Overall Loss 0.274893    Objective Loss 0.274893                                        LR 0.000036    Time 0.027261    
2023-01-06 14:34:00,376 - Epoch: [51][  246/  246]    Overall Loss 0.274574    Objective Loss 0.274574    Top1 89.234450    LR 0.000036    Time 0.027117    
2023-01-06 14:34:00,511 - --- validate (epoch=51)-----------
2023-01-06 14:34:00,512 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:00,957 - Epoch: [51][   10/   28]    Loss 0.277292    Top1 89.960938    
2023-01-06 14:34:01,104 - Epoch: [51][   20/   28]    Loss 0.285137    Top1 89.960938    
2023-01-06 14:34:01,195 - Epoch: [51][   28/   28]    Loss 0.294220    Top1 89.894074    
2023-01-06 14:34:01,329 - ==> Top1: 89.894    Loss: 0.294

2023-01-06 14:34:01,329 - ==> Confusion:
[[ 200   13  226]
 [   8  251  343]
 [  53   63 5829]]

2023-01-06 14:34:01,330 - ==> Best [Top1: 90.066   Sparsity:0.00   Params: 360896 on epoch: 49]
2023-01-06 14:34:01,330 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:01,340 - 

2023-01-06 14:34:01,340 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:01,934 - Epoch: [52][   10/  246]    Overall Loss 0.270651    Objective Loss 0.270651                                        LR 0.000036    Time 0.059256    
2023-01-06 14:34:02,179 - Epoch: [52][   20/  246]    Overall Loss 0.278371    Objective Loss 0.278371                                        LR 0.000036    Time 0.041871    
2023-01-06 14:34:02,431 - Epoch: [52][   30/  246]    Overall Loss 0.272581    Objective Loss 0.272581                                        LR 0.000036    Time 0.036293    
2023-01-06 14:34:02,686 - Epoch: [52][   40/  246]    Overall Loss 0.272876    Objective Loss 0.272876                                        LR 0.000036    Time 0.033553    
2023-01-06 14:34:02,953 - Epoch: [52][   50/  246]    Overall Loss 0.271521    Objective Loss 0.271521                                        LR 0.000036    Time 0.032173    
2023-01-06 14:34:03,226 - Epoch: [52][   60/  246]    Overall Loss 0.272555    Objective Loss 0.272555                                        LR 0.000036    Time 0.031345    
2023-01-06 14:34:03,500 - Epoch: [52][   70/  246]    Overall Loss 0.274018    Objective Loss 0.274018                                        LR 0.000036    Time 0.030771    
2023-01-06 14:34:03,776 - Epoch: [52][   80/  246]    Overall Loss 0.274343    Objective Loss 0.274343                                        LR 0.000036    Time 0.030370    
2023-01-06 14:34:04,051 - Epoch: [52][   90/  246]    Overall Loss 0.273201    Objective Loss 0.273201                                        LR 0.000036    Time 0.030052    
2023-01-06 14:34:04,317 - Epoch: [52][  100/  246]    Overall Loss 0.275232    Objective Loss 0.275232                                        LR 0.000036    Time 0.029704    
2023-01-06 14:34:04,578 - Epoch: [52][  110/  246]    Overall Loss 0.276818    Objective Loss 0.276818                                        LR 0.000036    Time 0.029374    
2023-01-06 14:34:04,841 - Epoch: [52][  120/  246]    Overall Loss 0.276430    Objective Loss 0.276430                                        LR 0.000036    Time 0.029112    
2023-01-06 14:34:05,101 - Epoch: [52][  130/  246]    Overall Loss 0.276187    Objective Loss 0.276187                                        LR 0.000036    Time 0.028873    
2023-01-06 14:34:05,362 - Epoch: [52][  140/  246]    Overall Loss 0.276666    Objective Loss 0.276666                                        LR 0.000036    Time 0.028673    
2023-01-06 14:34:05,621 - Epoch: [52][  150/  246]    Overall Loss 0.275347    Objective Loss 0.275347                                        LR 0.000036    Time 0.028483    
2023-01-06 14:34:05,880 - Epoch: [52][  160/  246]    Overall Loss 0.276861    Objective Loss 0.276861                                        LR 0.000036    Time 0.028318    
2023-01-06 14:34:06,136 - Epoch: [52][  170/  246]    Overall Loss 0.276541    Objective Loss 0.276541                                        LR 0.000036    Time 0.028158    
2023-01-06 14:34:06,389 - Epoch: [52][  180/  246]    Overall Loss 0.277195    Objective Loss 0.277195                                        LR 0.000036    Time 0.027993    
2023-01-06 14:34:06,646 - Epoch: [52][  190/  246]    Overall Loss 0.278654    Objective Loss 0.278654                                        LR 0.000036    Time 0.027872    
2023-01-06 14:34:06,891 - Epoch: [52][  200/  246]    Overall Loss 0.277760    Objective Loss 0.277760                                        LR 0.000036    Time 0.027701    
2023-01-06 14:34:07,136 - Epoch: [52][  210/  246]    Overall Loss 0.276863    Objective Loss 0.276863                                        LR 0.000036    Time 0.027548    
2023-01-06 14:34:07,384 - Epoch: [52][  220/  246]    Overall Loss 0.277181    Objective Loss 0.277181                                        LR 0.000036    Time 0.027420    
2023-01-06 14:34:07,630 - Epoch: [52][  230/  246]    Overall Loss 0.276295    Objective Loss 0.276295                                        LR 0.000036    Time 0.027292    
2023-01-06 14:34:07,886 - Epoch: [52][  240/  246]    Overall Loss 0.275580    Objective Loss 0.275580                                        LR 0.000036    Time 0.027223    
2023-01-06 14:34:08,015 - Epoch: [52][  246/  246]    Overall Loss 0.275169    Objective Loss 0.275169    Top1 90.430622    LR 0.000036    Time 0.027082    
2023-01-06 14:34:08,146 - --- validate (epoch=52)-----------
2023-01-06 14:34:08,147 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:08,588 - Epoch: [52][   10/   28]    Loss 0.283044    Top1 89.843750    
2023-01-06 14:34:08,740 - Epoch: [52][   20/   28]    Loss 0.280321    Top1 89.980469    
2023-01-06 14:34:08,831 - Epoch: [52][   28/   28]    Loss 0.279903    Top1 89.793873    
2023-01-06 14:34:08,969 - ==> Top1: 89.794    Loss: 0.280

2023-01-06 14:34:08,969 - ==> Confusion:
[[ 214   12  213]
 [   8  237  357]
 [  60   63 5822]]

2023-01-06 14:34:08,970 - ==> Best [Top1: 90.066   Sparsity:0.00   Params: 360896 on epoch: 49]
2023-01-06 14:34:08,970 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:08,981 - 

2023-01-06 14:34:08,981 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:09,716 - Epoch: [53][   10/  246]    Overall Loss 0.271522    Objective Loss 0.271522                                        LR 0.000036    Time 0.073487    
2023-01-06 14:34:09,963 - Epoch: [53][   20/  246]    Overall Loss 0.287302    Objective Loss 0.287302                                        LR 0.000036    Time 0.049051    
2023-01-06 14:34:10,230 - Epoch: [53][   30/  246]    Overall Loss 0.281004    Objective Loss 0.281004                                        LR 0.000036    Time 0.041593    
2023-01-06 14:34:10,489 - Epoch: [53][   40/  246]    Overall Loss 0.282559    Objective Loss 0.282559                                        LR 0.000036    Time 0.037655    
2023-01-06 14:34:10,760 - Epoch: [53][   50/  246]    Overall Loss 0.280294    Objective Loss 0.280294                                        LR 0.000036    Time 0.035503    
2023-01-06 14:34:11,016 - Epoch: [53][   60/  246]    Overall Loss 0.276604    Objective Loss 0.276604                                        LR 0.000036    Time 0.033847    
2023-01-06 14:34:11,254 - Epoch: [53][   70/  246]    Overall Loss 0.276640    Objective Loss 0.276640                                        LR 0.000036    Time 0.032408    
2023-01-06 14:34:11,513 - Epoch: [53][   80/  246]    Overall Loss 0.276242    Objective Loss 0.276242                                        LR 0.000036    Time 0.031594    
2023-01-06 14:34:11,781 - Epoch: [53][   90/  246]    Overall Loss 0.275992    Objective Loss 0.275992                                        LR 0.000036    Time 0.031048    
2023-01-06 14:34:12,040 - Epoch: [53][  100/  246]    Overall Loss 0.274304    Objective Loss 0.274304                                        LR 0.000036    Time 0.030533    
2023-01-06 14:34:12,307 - Epoch: [53][  110/  246]    Overall Loss 0.274652    Objective Loss 0.274652                                        LR 0.000036    Time 0.030175    
2023-01-06 14:34:12,557 - Epoch: [53][  120/  246]    Overall Loss 0.274748    Objective Loss 0.274748                                        LR 0.000036    Time 0.029747    
2023-01-06 14:34:12,823 - Epoch: [53][  130/  246]    Overall Loss 0.274167    Objective Loss 0.274167                                        LR 0.000036    Time 0.029487    
2023-01-06 14:34:13,080 - Epoch: [53][  140/  246]    Overall Loss 0.272743    Objective Loss 0.272743                                        LR 0.000036    Time 0.029215    
2023-01-06 14:34:13,339 - Epoch: [53][  150/  246]    Overall Loss 0.273244    Objective Loss 0.273244                                        LR 0.000036    Time 0.028991    
2023-01-06 14:34:13,598 - Epoch: [53][  160/  246]    Overall Loss 0.273713    Objective Loss 0.273713                                        LR 0.000036    Time 0.028794    
2023-01-06 14:34:13,856 - Epoch: [53][  170/  246]    Overall Loss 0.275026    Objective Loss 0.275026                                        LR 0.000036    Time 0.028617    
2023-01-06 14:34:14,113 - Epoch: [53][  180/  246]    Overall Loss 0.274615    Objective Loss 0.274615                                        LR 0.000036    Time 0.028454    
2023-01-06 14:34:14,372 - Epoch: [53][  190/  246]    Overall Loss 0.274394    Objective Loss 0.274394                                        LR 0.000036    Time 0.028316    
2023-01-06 14:34:14,629 - Epoch: [53][  200/  246]    Overall Loss 0.273871    Objective Loss 0.273871                                        LR 0.000036    Time 0.028187    
2023-01-06 14:34:14,891 - Epoch: [53][  210/  246]    Overall Loss 0.273421    Objective Loss 0.273421                                        LR 0.000036    Time 0.028087    
2023-01-06 14:34:15,141 - Epoch: [53][  220/  246]    Overall Loss 0.273060    Objective Loss 0.273060                                        LR 0.000036    Time 0.027946    
2023-01-06 14:34:15,395 - Epoch: [53][  230/  246]    Overall Loss 0.273320    Objective Loss 0.273320                                        LR 0.000036    Time 0.027833    
2023-01-06 14:34:15,658 - Epoch: [53][  240/  246]    Overall Loss 0.272970    Objective Loss 0.272970                                        LR 0.000036    Time 0.027769    
2023-01-06 14:34:15,783 - Epoch: [53][  246/  246]    Overall Loss 0.272794    Objective Loss 0.272794    Top1 88.277512    LR 0.000036    Time 0.027597    
2023-01-06 14:34:15,907 - --- validate (epoch=53)-----------
2023-01-06 14:34:15,907 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:16,362 - Epoch: [53][   10/   28]    Loss 0.289402    Top1 89.726562    
2023-01-06 14:34:16,522 - Epoch: [53][   20/   28]    Loss 0.277962    Top1 90.195312    
2023-01-06 14:34:16,612 - Epoch: [53][   28/   28]    Loss 0.279158    Top1 90.266247    
2023-01-06 14:34:16,776 - ==> Top1: 90.266    Loss: 0.279

2023-01-06 14:34:16,776 - ==> Confusion:
[[ 255    8  176]
 [  11  246  345]
 [  85   55 5805]]

2023-01-06 14:34:16,778 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:16,778 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:16,799 - 

2023-01-06 14:34:16,799 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:17,381 - Epoch: [54][   10/  246]    Overall Loss 0.281506    Objective Loss 0.281506                                        LR 0.000036    Time 0.058138    
2023-01-06 14:34:17,624 - Epoch: [54][   20/  246]    Overall Loss 0.272743    Objective Loss 0.272743                                        LR 0.000036    Time 0.041194    
2023-01-06 14:34:17,857 - Epoch: [54][   30/  246]    Overall Loss 0.272561    Objective Loss 0.272561                                        LR 0.000036    Time 0.035210    
2023-01-06 14:34:18,081 - Epoch: [54][   40/  246]    Overall Loss 0.279701    Objective Loss 0.279701                                        LR 0.000036    Time 0.031995    
2023-01-06 14:34:18,303 - Epoch: [54][   50/  246]    Overall Loss 0.281879    Objective Loss 0.281879                                        LR 0.000036    Time 0.030038    
2023-01-06 14:34:18,524 - Epoch: [54][   60/  246]    Overall Loss 0.279306    Objective Loss 0.279306                                        LR 0.000036    Time 0.028701    
2023-01-06 14:34:18,740 - Epoch: [54][   70/  246]    Overall Loss 0.277781    Objective Loss 0.277781                                        LR 0.000036    Time 0.027687    
2023-01-06 14:34:18,954 - Epoch: [54][   80/  246]    Overall Loss 0.279011    Objective Loss 0.279011                                        LR 0.000036    Time 0.026898    
2023-01-06 14:34:19,170 - Epoch: [54][   90/  246]    Overall Loss 0.277624    Objective Loss 0.277624                                        LR 0.000036    Time 0.026304    
2023-01-06 14:34:19,385 - Epoch: [54][  100/  246]    Overall Loss 0.276787    Objective Loss 0.276787                                        LR 0.000036    Time 0.025824    
2023-01-06 14:34:19,602 - Epoch: [54][  110/  246]    Overall Loss 0.275147    Objective Loss 0.275147                                        LR 0.000036    Time 0.025446    
2023-01-06 14:34:19,815 - Epoch: [54][  120/  246]    Overall Loss 0.275371    Objective Loss 0.275371                                        LR 0.000036    Time 0.025097    
2023-01-06 14:34:20,037 - Epoch: [54][  130/  246]    Overall Loss 0.273807    Objective Loss 0.273807                                        LR 0.000036    Time 0.024876    
2023-01-06 14:34:20,279 - Epoch: [54][  140/  246]    Overall Loss 0.273047    Objective Loss 0.273047                                        LR 0.000036    Time 0.024820    
2023-01-06 14:34:20,519 - Epoch: [54][  150/  246]    Overall Loss 0.273677    Objective Loss 0.273677                                        LR 0.000036    Time 0.024767    
2023-01-06 14:34:20,761 - Epoch: [54][  160/  246]    Overall Loss 0.273675    Objective Loss 0.273675                                        LR 0.000036    Time 0.024725    
2023-01-06 14:34:21,001 - Epoch: [54][  170/  246]    Overall Loss 0.272971    Objective Loss 0.272971                                        LR 0.000036    Time 0.024682    
2023-01-06 14:34:21,254 - Epoch: [54][  180/  246]    Overall Loss 0.272171    Objective Loss 0.272171                                        LR 0.000036    Time 0.024715    
2023-01-06 14:34:21,508 - Epoch: [54][  190/  246]    Overall Loss 0.272014    Objective Loss 0.272014                                        LR 0.000036    Time 0.024751    
2023-01-06 14:34:21,773 - Epoch: [54][  200/  246]    Overall Loss 0.272461    Objective Loss 0.272461                                        LR 0.000036    Time 0.024832    
2023-01-06 14:34:22,019 - Epoch: [54][  210/  246]    Overall Loss 0.272574    Objective Loss 0.272574                                        LR 0.000036    Time 0.024821    
2023-01-06 14:34:22,254 - Epoch: [54][  220/  246]    Overall Loss 0.272573    Objective Loss 0.272573                                        LR 0.000036    Time 0.024760    
2023-01-06 14:34:22,496 - Epoch: [54][  230/  246]    Overall Loss 0.271925    Objective Loss 0.271925                                        LR 0.000036    Time 0.024735    
2023-01-06 14:34:22,751 - Epoch: [54][  240/  246]    Overall Loss 0.271988    Objective Loss 0.271988                                        LR 0.000036    Time 0.024765    
2023-01-06 14:34:22,880 - Epoch: [54][  246/  246]    Overall Loss 0.271342    Objective Loss 0.271342    Top1 93.779904    LR 0.000036    Time 0.024683    
2023-01-06 14:34:23,010 - --- validate (epoch=54)-----------
2023-01-06 14:34:23,011 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:23,476 - Epoch: [54][   10/   28]    Loss 0.298019    Top1 89.296875    
2023-01-06 14:34:23,637 - Epoch: [54][   20/   28]    Loss 0.296149    Top1 89.277344    
2023-01-06 14:34:23,729 - Epoch: [54][   28/   28]    Loss 0.290812    Top1 89.636416    
2023-01-06 14:34:23,878 - ==> Top1: 89.636    Loss: 0.291

2023-01-06 14:34:23,878 - ==> Confusion:
[[ 193   10  236]
 [   8  207  387]
 [  45   38 5862]]

2023-01-06 14:34:23,880 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:23,880 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:23,890 - 

2023-01-06 14:34:23,890 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:24,620 - Epoch: [55][   10/  246]    Overall Loss 0.256929    Objective Loss 0.256929                                        LR 0.000036    Time 0.072970    
2023-01-06 14:34:24,883 - Epoch: [55][   20/  246]    Overall Loss 0.260591    Objective Loss 0.260591                                        LR 0.000036    Time 0.049613    
2023-01-06 14:34:25,136 - Epoch: [55][   30/  246]    Overall Loss 0.259040    Objective Loss 0.259040                                        LR 0.000036    Time 0.041494    
2023-01-06 14:34:25,405 - Epoch: [55][   40/  246]    Overall Loss 0.266814    Objective Loss 0.266814                                        LR 0.000036    Time 0.037824    
2023-01-06 14:34:25,669 - Epoch: [55][   50/  246]    Overall Loss 0.262143    Objective Loss 0.262143                                        LR 0.000036    Time 0.035546    
2023-01-06 14:34:25,933 - Epoch: [55][   60/  246]    Overall Loss 0.264491    Objective Loss 0.264491                                        LR 0.000036    Time 0.034001    
2023-01-06 14:34:26,192 - Epoch: [55][   70/  246]    Overall Loss 0.262969    Objective Loss 0.262969                                        LR 0.000036    Time 0.032836    
2023-01-06 14:34:26,453 - Epoch: [55][   80/  246]    Overall Loss 0.265782    Objective Loss 0.265782                                        LR 0.000036    Time 0.031999    
2023-01-06 14:34:26,713 - Epoch: [55][   90/  246]    Overall Loss 0.268701    Objective Loss 0.268701                                        LR 0.000036    Time 0.031328    
2023-01-06 14:34:26,978 - Epoch: [55][  100/  246]    Overall Loss 0.270720    Objective Loss 0.270720                                        LR 0.000036    Time 0.030832    
2023-01-06 14:34:27,240 - Epoch: [55][  110/  246]    Overall Loss 0.271763    Objective Loss 0.271763                                        LR 0.000036    Time 0.030408    
2023-01-06 14:34:27,487 - Epoch: [55][  120/  246]    Overall Loss 0.272852    Objective Loss 0.272852                                        LR 0.000036    Time 0.029936    
2023-01-06 14:34:27,748 - Epoch: [55][  130/  246]    Overall Loss 0.271480    Objective Loss 0.271480                                        LR 0.000036    Time 0.029632    
2023-01-06 14:34:28,001 - Epoch: [55][  140/  246]    Overall Loss 0.272144    Objective Loss 0.272144                                        LR 0.000036    Time 0.029326    
2023-01-06 14:34:28,256 - Epoch: [55][  150/  246]    Overall Loss 0.270939    Objective Loss 0.270939                                        LR 0.000036    Time 0.029064    
2023-01-06 14:34:28,512 - Epoch: [55][  160/  246]    Overall Loss 0.271227    Objective Loss 0.271227                                        LR 0.000036    Time 0.028849    
2023-01-06 14:34:28,764 - Epoch: [55][  170/  246]    Overall Loss 0.271018    Objective Loss 0.271018                                        LR 0.000036    Time 0.028630    
2023-01-06 14:34:29,011 - Epoch: [55][  180/  246]    Overall Loss 0.271783    Objective Loss 0.271783                                        LR 0.000036    Time 0.028411    
2023-01-06 14:34:29,237 - Epoch: [55][  190/  246]    Overall Loss 0.271289    Objective Loss 0.271289                                        LR 0.000036    Time 0.028104    
2023-01-06 14:34:29,447 - Epoch: [55][  200/  246]    Overall Loss 0.270689    Objective Loss 0.270689                                        LR 0.000036    Time 0.027749    
2023-01-06 14:34:29,652 - Epoch: [55][  210/  246]    Overall Loss 0.270555    Objective Loss 0.270555                                        LR 0.000036    Time 0.027398    
2023-01-06 14:34:29,868 - Epoch: [55][  220/  246]    Overall Loss 0.269419    Objective Loss 0.269419                                        LR 0.000036    Time 0.027136    
2023-01-06 14:34:30,082 - Epoch: [55][  230/  246]    Overall Loss 0.268896    Objective Loss 0.268896                                        LR 0.000036    Time 0.026883    
2023-01-06 14:34:30,310 - Epoch: [55][  240/  246]    Overall Loss 0.268594    Objective Loss 0.268594                                        LR 0.000036    Time 0.026712    
2023-01-06 14:34:30,435 - Epoch: [55][  246/  246]    Overall Loss 0.269078    Objective Loss 0.269078    Top1 89.952153    LR 0.000036    Time 0.026567    
2023-01-06 14:34:30,575 - --- validate (epoch=55)-----------
2023-01-06 14:34:30,575 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:31,024 - Epoch: [55][   10/   28]    Loss 0.273671    Top1 90.234375    
2023-01-06 14:34:31,177 - Epoch: [55][   20/   28]    Loss 0.273509    Top1 90.429688    
2023-01-06 14:34:31,267 - Epoch: [55][   28/   28]    Loss 0.280913    Top1 90.051532    
2023-01-06 14:34:31,404 - ==> Top1: 90.052    Loss: 0.281

2023-01-06 14:34:31,404 - ==> Confusion:
[[ 240   13  186]
 [  10  249  343]
 [  78   65 5802]]

2023-01-06 14:34:31,406 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:31,406 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:31,415 - 

2023-01-06 14:34:31,415 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:32,146 - Epoch: [56][   10/  246]    Overall Loss 0.269850    Objective Loss 0.269850                                        LR 0.000036    Time 0.072957    
2023-01-06 14:34:32,390 - Epoch: [56][   20/  246]    Overall Loss 0.267934    Objective Loss 0.267934                                        LR 0.000036    Time 0.048659    
2023-01-06 14:34:32,637 - Epoch: [56][   30/  246]    Overall Loss 0.266356    Objective Loss 0.266356                                        LR 0.000036    Time 0.040659    
2023-01-06 14:34:32,884 - Epoch: [56][   40/  246]    Overall Loss 0.272563    Objective Loss 0.272563                                        LR 0.000036    Time 0.036657    
2023-01-06 14:34:33,129 - Epoch: [56][   50/  246]    Overall Loss 0.272886    Objective Loss 0.272886                                        LR 0.000036    Time 0.034237    
2023-01-06 14:34:33,352 - Epoch: [56][   60/  246]    Overall Loss 0.276699    Objective Loss 0.276699                                        LR 0.000036    Time 0.032237    
2023-01-06 14:34:33,576 - Epoch: [56][   70/  246]    Overall Loss 0.277120    Objective Loss 0.277120                                        LR 0.000036    Time 0.030831    
2023-01-06 14:34:33,795 - Epoch: [56][   80/  246]    Overall Loss 0.275436    Objective Loss 0.275436                                        LR 0.000036    Time 0.029702    
2023-01-06 14:34:34,015 - Epoch: [56][   90/  246]    Overall Loss 0.272005    Objective Loss 0.272005                                        LR 0.000036    Time 0.028850    
2023-01-06 14:34:34,237 - Epoch: [56][  100/  246]    Overall Loss 0.273648    Objective Loss 0.273648                                        LR 0.000036    Time 0.028183    
2023-01-06 14:34:34,469 - Epoch: [56][  110/  246]    Overall Loss 0.273305    Objective Loss 0.273305                                        LR 0.000036    Time 0.027727    
2023-01-06 14:34:34,723 - Epoch: [56][  120/  246]    Overall Loss 0.273981    Objective Loss 0.273981                                        LR 0.000036    Time 0.027522    
2023-01-06 14:34:34,979 - Epoch: [56][  130/  246]    Overall Loss 0.272888    Objective Loss 0.272888                                        LR 0.000036    Time 0.027378    
2023-01-06 14:34:35,211 - Epoch: [56][  140/  246]    Overall Loss 0.271197    Objective Loss 0.271197                                        LR 0.000036    Time 0.027077    
2023-01-06 14:34:35,466 - Epoch: [56][  150/  246]    Overall Loss 0.270433    Objective Loss 0.270433                                        LR 0.000036    Time 0.026967    
2023-01-06 14:34:35,721 - Epoch: [56][  160/  246]    Overall Loss 0.271146    Objective Loss 0.271146                                        LR 0.000036    Time 0.026867    
2023-01-06 14:34:35,978 - Epoch: [56][  170/  246]    Overall Loss 0.271063    Objective Loss 0.271063                                        LR 0.000036    Time 0.026794    
2023-01-06 14:34:36,201 - Epoch: [56][  180/  246]    Overall Loss 0.271647    Objective Loss 0.271647                                        LR 0.000036    Time 0.026536    
2023-01-06 14:34:36,420 - Epoch: [56][  190/  246]    Overall Loss 0.271385    Objective Loss 0.271385                                        LR 0.000036    Time 0.026292    
2023-01-06 14:34:36,639 - Epoch: [56][  200/  246]    Overall Loss 0.270871    Objective Loss 0.270871                                        LR 0.000036    Time 0.026071    
2023-01-06 14:34:36,857 - Epoch: [56][  210/  246]    Overall Loss 0.270476    Objective Loss 0.270476                                        LR 0.000036    Time 0.025866    
2023-01-06 14:34:37,077 - Epoch: [56][  220/  246]    Overall Loss 0.270456    Objective Loss 0.270456                                        LR 0.000036    Time 0.025680    
2023-01-06 14:34:37,295 - Epoch: [56][  230/  246]    Overall Loss 0.271007    Objective Loss 0.271007                                        LR 0.000036    Time 0.025511    
2023-01-06 14:34:37,519 - Epoch: [56][  240/  246]    Overall Loss 0.270093    Objective Loss 0.270093                                        LR 0.000036    Time 0.025381    
2023-01-06 14:34:37,634 - Epoch: [56][  246/  246]    Overall Loss 0.269801    Objective Loss 0.269801    Top1 92.344498    LR 0.000036    Time 0.025227    
2023-01-06 14:34:37,769 - --- validate (epoch=56)-----------
2023-01-06 14:34:37,769 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:38,222 - Epoch: [56][   10/   28]    Loss 0.265871    Top1 90.625000    
2023-01-06 14:34:38,365 - Epoch: [56][   20/   28]    Loss 0.276399    Top1 90.136719    
2023-01-06 14:34:38,458 - Epoch: [56][   28/   28]    Loss 0.283083    Top1 89.994274    
2023-01-06 14:34:38,581 - ==> Top1: 89.994    Loss: 0.283

2023-01-06 14:34:38,581 - ==> Confusion:
[[ 232   10  197]
 [  17  231  354]
 [  63   58 5824]]

2023-01-06 14:34:38,583 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:38,583 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:38,593 - 

2023-01-06 14:34:38,593 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:39,154 - Epoch: [57][   10/  246]    Overall Loss 0.278365    Objective Loss 0.278365                                        LR 0.000036    Time 0.056037    
2023-01-06 14:34:39,362 - Epoch: [57][   20/  246]    Overall Loss 0.268402    Objective Loss 0.268402                                        LR 0.000036    Time 0.038397    
2023-01-06 14:34:39,572 - Epoch: [57][   30/  246]    Overall Loss 0.266173    Objective Loss 0.266173                                        LR 0.000036    Time 0.032594    
2023-01-06 14:34:39,781 - Epoch: [57][   40/  246]    Overall Loss 0.267298    Objective Loss 0.267298                                        LR 0.000036    Time 0.029609    
2023-01-06 14:34:39,991 - Epoch: [57][   50/  246]    Overall Loss 0.267034    Objective Loss 0.267034                                        LR 0.000036    Time 0.027895    
2023-01-06 14:34:40,206 - Epoch: [57][   60/  246]    Overall Loss 0.265686    Objective Loss 0.265686                                        LR 0.000036    Time 0.026822    
2023-01-06 14:34:40,428 - Epoch: [57][   70/  246]    Overall Loss 0.264474    Objective Loss 0.264474                                        LR 0.000036    Time 0.026156    
2023-01-06 14:34:40,678 - Epoch: [57][   80/  246]    Overall Loss 0.268700    Objective Loss 0.268700                                        LR 0.000036    Time 0.026005    
2023-01-06 14:34:40,937 - Epoch: [57][   90/  246]    Overall Loss 0.270769    Objective Loss 0.270769                                        LR 0.000036    Time 0.025978    
2023-01-06 14:34:41,189 - Epoch: [57][  100/  246]    Overall Loss 0.270828    Objective Loss 0.270828                                        LR 0.000036    Time 0.025897    
2023-01-06 14:34:41,438 - Epoch: [57][  110/  246]    Overall Loss 0.270014    Objective Loss 0.270014                                        LR 0.000036    Time 0.025803    
2023-01-06 14:34:41,690 - Epoch: [57][  120/  246]    Overall Loss 0.269655    Objective Loss 0.269655                                        LR 0.000036    Time 0.025741    
2023-01-06 14:34:41,940 - Epoch: [57][  130/  246]    Overall Loss 0.269710    Objective Loss 0.269710                                        LR 0.000036    Time 0.025677    
2023-01-06 14:34:42,185 - Epoch: [57][  140/  246]    Overall Loss 0.268948    Objective Loss 0.268948                                        LR 0.000036    Time 0.025587    
2023-01-06 14:34:42,427 - Epoch: [57][  150/  246]    Overall Loss 0.269212    Objective Loss 0.269212                                        LR 0.000036    Time 0.025497    
2023-01-06 14:34:42,668 - Epoch: [57][  160/  246]    Overall Loss 0.268573    Objective Loss 0.268573                                        LR 0.000036    Time 0.025407    
2023-01-06 14:34:42,910 - Epoch: [57][  170/  246]    Overall Loss 0.270732    Objective Loss 0.270732                                        LR 0.000036    Time 0.025333    
2023-01-06 14:34:43,157 - Epoch: [57][  180/  246]    Overall Loss 0.273793    Objective Loss 0.273793                                        LR 0.000036    Time 0.025290    
2023-01-06 14:34:43,400 - Epoch: [57][  190/  246]    Overall Loss 0.273767    Objective Loss 0.273767                                        LR 0.000036    Time 0.025238    
2023-01-06 14:34:43,642 - Epoch: [57][  200/  246]    Overall Loss 0.273169    Objective Loss 0.273169                                        LR 0.000036    Time 0.025184    
2023-01-06 14:34:43,885 - Epoch: [57][  210/  246]    Overall Loss 0.273020    Objective Loss 0.273020                                        LR 0.000036    Time 0.025133    
2023-01-06 14:34:44,125 - Epoch: [57][  220/  246]    Overall Loss 0.272027    Objective Loss 0.272027                                        LR 0.000036    Time 0.025082    
2023-01-06 14:34:44,367 - Epoch: [57][  230/  246]    Overall Loss 0.271387    Objective Loss 0.271387                                        LR 0.000036    Time 0.025042    
2023-01-06 14:34:44,621 - Epoch: [57][  240/  246]    Overall Loss 0.270668    Objective Loss 0.270668                                        LR 0.000036    Time 0.025052    
2023-01-06 14:34:44,744 - Epoch: [57][  246/  246]    Overall Loss 0.270409    Objective Loss 0.270409    Top1 90.909091    LR 0.000036    Time 0.024941    
2023-01-06 14:34:44,879 - --- validate (epoch=57)-----------
2023-01-06 14:34:44,879 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:45,323 - Epoch: [57][   10/   28]    Loss 0.297067    Top1 88.515625    
2023-01-06 14:34:45,478 - Epoch: [57][   20/   28]    Loss 0.290566    Top1 89.218750    
2023-01-06 14:34:45,568 - Epoch: [57][   28/   28]    Loss 0.282519    Top1 89.779559    
2023-01-06 14:34:45,689 - ==> Top1: 89.780    Loss: 0.283

2023-01-06 14:34:45,690 - ==> Confusion:
[[ 222    9  208]
 [  14  225  363]
 [  61   59 5825]]

2023-01-06 14:34:45,691 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:45,691 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:45,701 - 

2023-01-06 14:34:45,701 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:46,409 - Epoch: [58][   10/  246]    Overall Loss 0.272234    Objective Loss 0.272234                                        LR 0.000036    Time 0.070679    
2023-01-06 14:34:46,623 - Epoch: [58][   20/  246]    Overall Loss 0.271629    Objective Loss 0.271629                                        LR 0.000036    Time 0.046027    
2023-01-06 14:34:46,841 - Epoch: [58][   30/  246]    Overall Loss 0.268436    Objective Loss 0.268436                                        LR 0.000036    Time 0.037929    
2023-01-06 14:34:47,061 - Epoch: [58][   40/  246]    Overall Loss 0.272496    Objective Loss 0.272496                                        LR 0.000036    Time 0.033950    
2023-01-06 14:34:47,280 - Epoch: [58][   50/  246]    Overall Loss 0.267988    Objective Loss 0.267988                                        LR 0.000036    Time 0.031515    
2023-01-06 14:34:47,512 - Epoch: [58][   60/  246]    Overall Loss 0.270485    Objective Loss 0.270485                                        LR 0.000036    Time 0.030132    
2023-01-06 14:34:47,737 - Epoch: [58][   70/  246]    Overall Loss 0.267536    Objective Loss 0.267536                                        LR 0.000036    Time 0.029032    
2023-01-06 14:34:47,965 - Epoch: [58][   80/  246]    Overall Loss 0.269264    Objective Loss 0.269264                                        LR 0.000036    Time 0.028247    
2023-01-06 14:34:48,194 - Epoch: [58][   90/  246]    Overall Loss 0.270832    Objective Loss 0.270832                                        LR 0.000036    Time 0.027651    
2023-01-06 14:34:48,430 - Epoch: [58][  100/  246]    Overall Loss 0.269261    Objective Loss 0.269261                                        LR 0.000036    Time 0.027243    
2023-01-06 14:34:48,670 - Epoch: [58][  110/  246]    Overall Loss 0.268337    Objective Loss 0.268337                                        LR 0.000036    Time 0.026946    
2023-01-06 14:34:48,911 - Epoch: [58][  120/  246]    Overall Loss 0.265231    Objective Loss 0.265231                                        LR 0.000036    Time 0.026707    
2023-01-06 14:34:49,150 - Epoch: [58][  130/  246]    Overall Loss 0.265011    Objective Loss 0.265011                                        LR 0.000036    Time 0.026483    
2023-01-06 14:34:49,389 - Epoch: [58][  140/  246]    Overall Loss 0.264607    Objective Loss 0.264607                                        LR 0.000036    Time 0.026298    
2023-01-06 14:34:49,629 - Epoch: [58][  150/  246]    Overall Loss 0.263794    Objective Loss 0.263794                                        LR 0.000036    Time 0.026141    
2023-01-06 14:34:49,871 - Epoch: [58][  160/  246]    Overall Loss 0.263029    Objective Loss 0.263029                                        LR 0.000036    Time 0.026021    
2023-01-06 14:34:50,104 - Epoch: [58][  170/  246]    Overall Loss 0.262370    Objective Loss 0.262370                                        LR 0.000036    Time 0.025856    
2023-01-06 14:34:50,346 - Epoch: [58][  180/  246]    Overall Loss 0.263455    Objective Loss 0.263455                                        LR 0.000036    Time 0.025756    
2023-01-06 14:34:50,586 - Epoch: [58][  190/  246]    Overall Loss 0.264218    Objective Loss 0.264218                                        LR 0.000036    Time 0.025659    
2023-01-06 14:34:50,822 - Epoch: [58][  200/  246]    Overall Loss 0.264688    Objective Loss 0.264688                                        LR 0.000036    Time 0.025554    
2023-01-06 14:34:51,059 - Epoch: [58][  210/  246]    Overall Loss 0.265399    Objective Loss 0.265399                                        LR 0.000036    Time 0.025463    
2023-01-06 14:34:51,297 - Epoch: [58][  220/  246]    Overall Loss 0.265525    Objective Loss 0.265525                                        LR 0.000036    Time 0.025387    
2023-01-06 14:34:51,536 - Epoch: [58][  230/  246]    Overall Loss 0.266671    Objective Loss 0.266671                                        LR 0.000036    Time 0.025321    
2023-01-06 14:34:51,791 - Epoch: [58][  240/  246]    Overall Loss 0.266583    Objective Loss 0.266583                                        LR 0.000036    Time 0.025325    
2023-01-06 14:34:51,920 - Epoch: [58][  246/  246]    Overall Loss 0.266638    Objective Loss 0.266638    Top1 90.909091    LR 0.000036    Time 0.025232    
2023-01-06 14:34:52,047 - --- validate (epoch=58)-----------
2023-01-06 14:34:52,047 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:52,490 - Epoch: [58][   10/   28]    Loss 0.284771    Top1 89.843750    
2023-01-06 14:34:52,632 - Epoch: [58][   20/   28]    Loss 0.280319    Top1 89.824219    
2023-01-06 14:34:52,724 - Epoch: [58][   28/   28]    Loss 0.279479    Top1 90.065846    
2023-01-06 14:34:52,855 - ==> Top1: 90.066    Loss: 0.279

2023-01-06 14:34:52,855 - ==> Confusion:
[[ 234   12  193]
 [  13  253  336]
 [  73   67 5805]]

2023-01-06 14:34:52,856 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:34:52,857 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:34:52,866 - 

2023-01-06 14:34:52,866 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:34:53,587 - Epoch: [59][   10/  246]    Overall Loss 0.254057    Objective Loss 0.254057                                        LR 0.000036    Time 0.071949    
2023-01-06 14:34:53,810 - Epoch: [59][   20/  246]    Overall Loss 0.263865    Objective Loss 0.263865                                        LR 0.000036    Time 0.047131    
2023-01-06 14:34:54,044 - Epoch: [59][   30/  246]    Overall Loss 0.263925    Objective Loss 0.263925                                        LR 0.000036    Time 0.039196    
2023-01-06 14:34:54,281 - Epoch: [59][   40/  246]    Overall Loss 0.260940    Objective Loss 0.260940                                        LR 0.000036    Time 0.035319    
2023-01-06 14:34:54,516 - Epoch: [59][   50/  246]    Overall Loss 0.258987    Objective Loss 0.258987                                        LR 0.000036    Time 0.032915    
2023-01-06 14:34:54,753 - Epoch: [59][   60/  246]    Overall Loss 0.260540    Objective Loss 0.260540                                        LR 0.000036    Time 0.031373    
2023-01-06 14:34:54,987 - Epoch: [59][   70/  246]    Overall Loss 0.261588    Objective Loss 0.261588                                        LR 0.000036    Time 0.030229    
2023-01-06 14:34:55,230 - Epoch: [59][   80/  246]    Overall Loss 0.259561    Objective Loss 0.259561                                        LR 0.000036    Time 0.029474    
2023-01-06 14:34:55,460 - Epoch: [59][   90/  246]    Overall Loss 0.263179    Objective Loss 0.263179                                        LR 0.000036    Time 0.028757    
2023-01-06 14:34:55,702 - Epoch: [59][  100/  246]    Overall Loss 0.264443    Objective Loss 0.264443                                        LR 0.000036    Time 0.028299    
2023-01-06 14:34:55,946 - Epoch: [59][  110/  246]    Overall Loss 0.265150    Objective Loss 0.265150                                        LR 0.000036    Time 0.027932    
2023-01-06 14:34:56,184 - Epoch: [59][  120/  246]    Overall Loss 0.263646    Objective Loss 0.263646                                        LR 0.000036    Time 0.027589    
2023-01-06 14:34:56,426 - Epoch: [59][  130/  246]    Overall Loss 0.264432    Objective Loss 0.264432                                        LR 0.000036    Time 0.027322    
2023-01-06 14:34:56,665 - Epoch: [59][  140/  246]    Overall Loss 0.265225    Objective Loss 0.265225                                        LR 0.000036    Time 0.027075    
2023-01-06 14:34:56,901 - Epoch: [59][  150/  246]    Overall Loss 0.265900    Objective Loss 0.265900                                        LR 0.000036    Time 0.026831    
2023-01-06 14:34:57,137 - Epoch: [59][  160/  246]    Overall Loss 0.266283    Objective Loss 0.266283                                        LR 0.000036    Time 0.026623    
2023-01-06 14:34:57,379 - Epoch: [59][  170/  246]    Overall Loss 0.265719    Objective Loss 0.265719                                        LR 0.000036    Time 0.026476    
2023-01-06 14:34:57,617 - Epoch: [59][  180/  246]    Overall Loss 0.264999    Objective Loss 0.264999                                        LR 0.000036    Time 0.026327    
2023-01-06 14:34:57,850 - Epoch: [59][  190/  246]    Overall Loss 0.264191    Objective Loss 0.264191                                        LR 0.000036    Time 0.026164    
2023-01-06 14:34:58,091 - Epoch: [59][  200/  246]    Overall Loss 0.264841    Objective Loss 0.264841                                        LR 0.000036    Time 0.026059    
2023-01-06 14:34:58,329 - Epoch: [59][  210/  246]    Overall Loss 0.264632    Objective Loss 0.264632                                        LR 0.000036    Time 0.025948    
2023-01-06 14:34:58,568 - Epoch: [59][  220/  246]    Overall Loss 0.266238    Objective Loss 0.266238                                        LR 0.000036    Time 0.025855    
2023-01-06 14:34:58,804 - Epoch: [59][  230/  246]    Overall Loss 0.266044    Objective Loss 0.266044                                        LR 0.000036    Time 0.025754    
2023-01-06 14:34:59,061 - Epoch: [59][  240/  246]    Overall Loss 0.266066    Objective Loss 0.266066                                        LR 0.000036    Time 0.025749    
2023-01-06 14:34:59,193 - Epoch: [59][  246/  246]    Overall Loss 0.266466    Objective Loss 0.266466    Top1 91.148325    LR 0.000036    Time 0.025657    
2023-01-06 14:34:59,329 - --- validate (epoch=59)-----------
2023-01-06 14:34:59,330 - 6986 samples (256 per mini-batch)
2023-01-06 14:34:59,766 - Epoch: [59][   10/   28]    Loss 0.294840    Top1 89.609375    
2023-01-06 14:34:59,909 - Epoch: [59][   20/   28]    Loss 0.296885    Top1 89.414062    
2023-01-06 14:35:00,003 - Epoch: [59][   28/   28]    Loss 0.292875    Top1 89.436015    
2023-01-06 14:35:00,105 - ==> Top1: 89.436    Loss: 0.293

2023-01-06 14:35:00,105 - ==> Confusion:
[[ 180    9  250]
 [   9  235  358]
 [  44   68 5833]]

2023-01-06 14:35:00,107 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:35:00,107 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:00,117 - 

2023-01-06 14:35:00,117 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:00,686 - Epoch: [60][   10/  246]    Overall Loss 0.296251    Objective Loss 0.296251                                        LR 0.000036    Time 0.056873    
2023-01-06 14:35:00,912 - Epoch: [60][   20/  246]    Overall Loss 0.276105    Objective Loss 0.276105                                        LR 0.000036    Time 0.039682    
2023-01-06 14:35:01,145 - Epoch: [60][   30/  246]    Overall Loss 0.265150    Objective Loss 0.265150                                        LR 0.000036    Time 0.034221    
2023-01-06 14:35:01,364 - Epoch: [60][   40/  246]    Overall Loss 0.265098    Objective Loss 0.265098                                        LR 0.000036    Time 0.031133    
2023-01-06 14:35:01,579 - Epoch: [60][   50/  246]    Overall Loss 0.266714    Objective Loss 0.266714                                        LR 0.000036    Time 0.029186    
2023-01-06 14:35:01,796 - Epoch: [60][   60/  246]    Overall Loss 0.267500    Objective Loss 0.267500                                        LR 0.000036    Time 0.027943    
2023-01-06 14:35:02,016 - Epoch: [60][   70/  246]    Overall Loss 0.265738    Objective Loss 0.265738                                        LR 0.000036    Time 0.027086    
2023-01-06 14:35:02,231 - Epoch: [60][   80/  246]    Overall Loss 0.265021    Objective Loss 0.265021                                        LR 0.000036    Time 0.026378    
2023-01-06 14:35:02,440 - Epoch: [60][   90/  246]    Overall Loss 0.266691    Objective Loss 0.266691                                        LR 0.000036    Time 0.025765    
2023-01-06 14:35:02,650 - Epoch: [60][  100/  246]    Overall Loss 0.266073    Objective Loss 0.266073                                        LR 0.000036    Time 0.025289    
2023-01-06 14:35:02,859 - Epoch: [60][  110/  246]    Overall Loss 0.265949    Objective Loss 0.265949                                        LR 0.000036    Time 0.024883    
2023-01-06 14:35:03,070 - Epoch: [60][  120/  246]    Overall Loss 0.264882    Objective Loss 0.264882                                        LR 0.000036    Time 0.024570    
2023-01-06 14:35:03,284 - Epoch: [60][  130/  246]    Overall Loss 0.263261    Objective Loss 0.263261                                        LR 0.000036    Time 0.024319    
2023-01-06 14:35:03,506 - Epoch: [60][  140/  246]    Overall Loss 0.263560    Objective Loss 0.263560                                        LR 0.000036    Time 0.024168    
2023-01-06 14:35:03,722 - Epoch: [60][  150/  246]    Overall Loss 0.263784    Objective Loss 0.263784                                        LR 0.000036    Time 0.023991    
2023-01-06 14:35:03,942 - Epoch: [60][  160/  246]    Overall Loss 0.263546    Objective Loss 0.263546                                        LR 0.000036    Time 0.023866    
2023-01-06 14:35:04,162 - Epoch: [60][  170/  246]    Overall Loss 0.263027    Objective Loss 0.263027                                        LR 0.000036    Time 0.023750    
2023-01-06 14:35:04,378 - Epoch: [60][  180/  246]    Overall Loss 0.264292    Objective Loss 0.264292                                        LR 0.000036    Time 0.023633    
2023-01-06 14:35:04,610 - Epoch: [60][  190/  246]    Overall Loss 0.265024    Objective Loss 0.265024                                        LR 0.000036    Time 0.023608    
2023-01-06 14:35:04,858 - Epoch: [60][  200/  246]    Overall Loss 0.264943    Objective Loss 0.264943                                        LR 0.000036    Time 0.023664    
2023-01-06 14:35:05,107 - Epoch: [60][  210/  246]    Overall Loss 0.265017    Objective Loss 0.265017                                        LR 0.000036    Time 0.023718    
2023-01-06 14:35:05,322 - Epoch: [60][  220/  246]    Overall Loss 0.264951    Objective Loss 0.264951                                        LR 0.000036    Time 0.023617    
2023-01-06 14:35:05,539 - Epoch: [60][  230/  246]    Overall Loss 0.264629    Objective Loss 0.264629                                        LR 0.000036    Time 0.023534    
2023-01-06 14:35:05,787 - Epoch: [60][  240/  246]    Overall Loss 0.265892    Objective Loss 0.265892                                        LR 0.000036    Time 0.023583    
2023-01-06 14:35:05,918 - Epoch: [60][  246/  246]    Overall Loss 0.266127    Objective Loss 0.266127    Top1 88.755981    LR 0.000036    Time 0.023538    
2023-01-06 14:35:06,093 - --- validate (epoch=60)-----------
2023-01-06 14:35:06,093 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:06,551 - Epoch: [60][   10/   28]    Loss 0.315974    Top1 88.242188    
2023-01-06 14:35:06,693 - Epoch: [60][   20/   28]    Loss 0.302105    Top1 89.101562    
2023-01-06 14:35:06,785 - Epoch: [60][   28/   28]    Loss 0.307290    Top1 88.949327    
2023-01-06 14:35:06,915 - ==> Top1: 88.949    Loss: 0.307

2023-01-06 14:35:06,915 - ==> Confusion:
[[ 167    6  266]
 [   9  175  418]
 [  33   40 5872]]

2023-01-06 14:35:06,917 - ==> Best [Top1: 90.266   Sparsity:0.00   Params: 360896 on epoch: 53]
2023-01-06 14:35:06,917 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:06,926 - 

2023-01-06 14:35:06,927 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:07,663 - Epoch: [61][   10/  246]    Overall Loss 0.288477    Objective Loss 0.288477                                        LR 0.000036    Time 0.073598    
2023-01-06 14:35:07,895 - Epoch: [61][   20/  246]    Overall Loss 0.277718    Objective Loss 0.277718                                        LR 0.000036    Time 0.048366    
2023-01-06 14:35:08,136 - Epoch: [61][   30/  246]    Overall Loss 0.268904    Objective Loss 0.268904                                        LR 0.000036    Time 0.040251    
2023-01-06 14:35:08,371 - Epoch: [61][   40/  246]    Overall Loss 0.260227    Objective Loss 0.260227                                        LR 0.000036    Time 0.036042    
2023-01-06 14:35:08,605 - Epoch: [61][   50/  246]    Overall Loss 0.265442    Objective Loss 0.265442                                        LR 0.000036    Time 0.033498    
2023-01-06 14:35:08,836 - Epoch: [61][   60/  246]    Overall Loss 0.265601    Objective Loss 0.265601                                        LR 0.000036    Time 0.031761    
2023-01-06 14:35:09,063 - Epoch: [61][   70/  246]    Overall Loss 0.265731    Objective Loss 0.265731                                        LR 0.000036    Time 0.030458    
2023-01-06 14:35:09,280 - Epoch: [61][   80/  246]    Overall Loss 0.264686    Objective Loss 0.264686                                        LR 0.000036    Time 0.029362    
2023-01-06 14:35:09,492 - Epoch: [61][   90/  246]    Overall Loss 0.264542    Objective Loss 0.264542                                        LR 0.000036    Time 0.028451    
2023-01-06 14:35:09,709 - Epoch: [61][  100/  246]    Overall Loss 0.263915    Objective Loss 0.263915                                        LR 0.000036    Time 0.027778    
2023-01-06 14:35:09,958 - Epoch: [61][  110/  246]    Overall Loss 0.263988    Objective Loss 0.263988                                        LR 0.000036    Time 0.027508    
2023-01-06 14:35:10,209 - Epoch: [61][  120/  246]    Overall Loss 0.263543    Objective Loss 0.263543                                        LR 0.000036    Time 0.027308    
2023-01-06 14:35:10,462 - Epoch: [61][  130/  246]    Overall Loss 0.262856    Objective Loss 0.262856                                        LR 0.000036    Time 0.027150    
2023-01-06 14:35:10,714 - Epoch: [61][  140/  246]    Overall Loss 0.263528    Objective Loss 0.263528                                        LR 0.000036    Time 0.027008    
2023-01-06 14:35:10,969 - Epoch: [61][  150/  246]    Overall Loss 0.263996    Objective Loss 0.263996                                        LR 0.000036    Time 0.026906    
2023-01-06 14:35:11,213 - Epoch: [61][  160/  246]    Overall Loss 0.263914    Objective Loss 0.263914                                        LR 0.000036    Time 0.026747    
2023-01-06 14:35:11,459 - Epoch: [61][  170/  246]    Overall Loss 0.263585    Objective Loss 0.263585                                        LR 0.000036    Time 0.026618    
2023-01-06 14:35:11,705 - Epoch: [61][  180/  246]    Overall Loss 0.264473    Objective Loss 0.264473                                        LR 0.000036    Time 0.026495    
2023-01-06 14:35:11,949 - Epoch: [61][  190/  246]    Overall Loss 0.265197    Objective Loss 0.265197                                        LR 0.000036    Time 0.026383    
2023-01-06 14:35:12,189 - Epoch: [61][  200/  246]    Overall Loss 0.265584    Objective Loss 0.265584                                        LR 0.000036    Time 0.026259    
2023-01-06 14:35:12,434 - Epoch: [61][  210/  246]    Overall Loss 0.265282    Objective Loss 0.265282                                        LR 0.000036    Time 0.026170    
2023-01-06 14:35:12,678 - Epoch: [61][  220/  246]    Overall Loss 0.265350    Objective Loss 0.265350                                        LR 0.000036    Time 0.026082    
2023-01-06 14:35:12,925 - Epoch: [61][  230/  246]    Overall Loss 0.265906    Objective Loss 0.265906                                        LR 0.000036    Time 0.026020    
2023-01-06 14:35:13,187 - Epoch: [61][  240/  246]    Overall Loss 0.266138    Objective Loss 0.266138                                        LR 0.000036    Time 0.026016    
2023-01-06 14:35:13,310 - Epoch: [61][  246/  246]    Overall Loss 0.265644    Objective Loss 0.265644    Top1 89.473684    LR 0.000036    Time 0.025882    
2023-01-06 14:35:13,451 - --- validate (epoch=61)-----------
2023-01-06 14:35:13,451 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:13,908 - Epoch: [61][   10/   28]    Loss 0.285432    Top1 90.039062    
2023-01-06 14:35:14,052 - Epoch: [61][   20/   28]    Loss 0.271064    Top1 90.625000    
2023-01-06 14:35:14,142 - Epoch: [61][   28/   28]    Loss 0.281708    Top1 90.337818    
2023-01-06 14:35:14,273 - ==> Top1: 90.338    Loss: 0.282

2023-01-06 14:35:14,274 - ==> Confusion:
[[ 240   11  188]
 [  12  229  361]
 [  63   40 5842]]

2023-01-06 14:35:14,275 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 360896 on epoch: 61]
2023-01-06 14:35:14,275 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:14,295 - 

2023-01-06 14:35:14,295 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:14,876 - Epoch: [62][   10/  246]    Overall Loss 0.236502    Objective Loss 0.236502                                        LR 0.000036    Time 0.058052    
2023-01-06 14:35:15,106 - Epoch: [62][   20/  246]    Overall Loss 0.253153    Objective Loss 0.253153                                        LR 0.000036    Time 0.040466    
2023-01-06 14:35:15,347 - Epoch: [62][   30/  246]    Overall Loss 0.257223    Objective Loss 0.257223                                        LR 0.000036    Time 0.034998    
2023-01-06 14:35:15,602 - Epoch: [62][   40/  246]    Overall Loss 0.261585    Objective Loss 0.261585                                        LR 0.000036    Time 0.032623    
2023-01-06 14:35:15,852 - Epoch: [62][   50/  246]    Overall Loss 0.264391    Objective Loss 0.264391                                        LR 0.000036    Time 0.031087    
2023-01-06 14:35:16,106 - Epoch: [62][   60/  246]    Overall Loss 0.264744    Objective Loss 0.264744                                        LR 0.000036    Time 0.030139    
2023-01-06 14:35:16,356 - Epoch: [62][   70/  246]    Overall Loss 0.260443    Objective Loss 0.260443                                        LR 0.000036    Time 0.029397    
2023-01-06 14:35:16,600 - Epoch: [62][   80/  246]    Overall Loss 0.262138    Objective Loss 0.262138                                        LR 0.000036    Time 0.028768    
2023-01-06 14:35:16,829 - Epoch: [62][   90/  246]    Overall Loss 0.261340    Objective Loss 0.261340                                        LR 0.000036    Time 0.028110    
2023-01-06 14:35:17,057 - Epoch: [62][  100/  246]    Overall Loss 0.260106    Objective Loss 0.260106                                        LR 0.000036    Time 0.027578    
2023-01-06 14:35:17,282 - Epoch: [62][  110/  246]    Overall Loss 0.259005    Objective Loss 0.259005                                        LR 0.000036    Time 0.027111    
2023-01-06 14:35:17,539 - Epoch: [62][  120/  246]    Overall Loss 0.259003    Objective Loss 0.259003                                        LR 0.000036    Time 0.026989    
2023-01-06 14:35:17,778 - Epoch: [62][  130/  246]    Overall Loss 0.259833    Objective Loss 0.259833                                        LR 0.000036    Time 0.026753    
2023-01-06 14:35:18,036 - Epoch: [62][  140/  246]    Overall Loss 0.260465    Objective Loss 0.260465                                        LR 0.000036    Time 0.026678    
2023-01-06 14:35:18,285 - Epoch: [62][  150/  246]    Overall Loss 0.261226    Objective Loss 0.261226                                        LR 0.000036    Time 0.026563    
2023-01-06 14:35:18,537 - Epoch: [62][  160/  246]    Overall Loss 0.261761    Objective Loss 0.261761                                        LR 0.000036    Time 0.026472    
2023-01-06 14:35:18,787 - Epoch: [62][  170/  246]    Overall Loss 0.262176    Objective Loss 0.262176                                        LR 0.000036    Time 0.026381    
2023-01-06 14:35:19,038 - Epoch: [62][  180/  246]    Overall Loss 0.262822    Objective Loss 0.262822                                        LR 0.000036    Time 0.026309    
2023-01-06 14:35:19,286 - Epoch: [62][  190/  246]    Overall Loss 0.262795    Objective Loss 0.262795                                        LR 0.000036    Time 0.026231    
2023-01-06 14:35:19,539 - Epoch: [62][  200/  246]    Overall Loss 0.262818    Objective Loss 0.262818                                        LR 0.000036    Time 0.026182    
2023-01-06 14:35:19,783 - Epoch: [62][  210/  246]    Overall Loss 0.263501    Objective Loss 0.263501                                        LR 0.000036    Time 0.026095    
2023-01-06 14:35:20,029 - Epoch: [62][  220/  246]    Overall Loss 0.263997    Objective Loss 0.263997                                        LR 0.000036    Time 0.026024    
2023-01-06 14:35:20,269 - Epoch: [62][  230/  246]    Overall Loss 0.264610    Objective Loss 0.264610                                        LR 0.000036    Time 0.025934    
2023-01-06 14:35:20,523 - Epoch: [62][  240/  246]    Overall Loss 0.264401    Objective Loss 0.264401                                        LR 0.000036    Time 0.025912    
2023-01-06 14:35:20,653 - Epoch: [62][  246/  246]    Overall Loss 0.264686    Objective Loss 0.264686    Top1 87.799043    LR 0.000036    Time 0.025808    
2023-01-06 14:35:20,789 - --- validate (epoch=62)-----------
2023-01-06 14:35:20,789 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:21,255 - Epoch: [62][   10/   28]    Loss 0.275994    Top1 90.156250    
2023-01-06 14:35:21,405 - Epoch: [62][   20/   28]    Loss 0.279832    Top1 89.941406    
2023-01-06 14:35:21,495 - Epoch: [62][   28/   28]    Loss 0.274754    Top1 90.137418    
2023-01-06 14:35:21,615 - ==> Top1: 90.137    Loss: 0.275

2023-01-06 14:35:21,616 - ==> Confusion:
[[ 226    9  204]
 [  15  255  332]
 [  55   74 5816]]

2023-01-06 14:35:21,617 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 360896 on epoch: 61]
2023-01-06 14:35:21,617 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:21,627 - 

2023-01-06 14:35:21,627 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:22,365 - Epoch: [63][   10/  246]    Overall Loss 0.259023    Objective Loss 0.259023                                        LR 0.000036    Time 0.073690    
2023-01-06 14:35:22,614 - Epoch: [63][   20/  246]    Overall Loss 0.261010    Objective Loss 0.261010                                        LR 0.000036    Time 0.049289    
2023-01-06 14:35:22,866 - Epoch: [63][   30/  246]    Overall Loss 0.267790    Objective Loss 0.267790                                        LR 0.000036    Time 0.041251    
2023-01-06 14:35:23,124 - Epoch: [63][   40/  246]    Overall Loss 0.269190    Objective Loss 0.269190                                        LR 0.000036    Time 0.037382    
2023-01-06 14:35:23,376 - Epoch: [63][   50/  246]    Overall Loss 0.270819    Objective Loss 0.270819                                        LR 0.000036    Time 0.034925    
2023-01-06 14:35:23,627 - Epoch: [63][   60/  246]    Overall Loss 0.272232    Objective Loss 0.272232                                        LR 0.000036    Time 0.033275    
2023-01-06 14:35:23,878 - Epoch: [63][   70/  246]    Overall Loss 0.272346    Objective Loss 0.272346                                        LR 0.000036    Time 0.032101    
2023-01-06 14:35:24,129 - Epoch: [63][   80/  246]    Overall Loss 0.272064    Objective Loss 0.272064                                        LR 0.000036    Time 0.031219    
2023-01-06 14:35:24,379 - Epoch: [63][   90/  246]    Overall Loss 0.270533    Objective Loss 0.270533                                        LR 0.000036    Time 0.030529    
2023-01-06 14:35:24,628 - Epoch: [63][  100/  246]    Overall Loss 0.268959    Objective Loss 0.268959                                        LR 0.000036    Time 0.029957    
2023-01-06 14:35:24,880 - Epoch: [63][  110/  246]    Overall Loss 0.266636    Objective Loss 0.266636                                        LR 0.000036    Time 0.029524    
2023-01-06 14:35:25,134 - Epoch: [63][  120/  246]    Overall Loss 0.265899    Objective Loss 0.265899                                        LR 0.000036    Time 0.029171    
2023-01-06 14:35:25,390 - Epoch: [63][  130/  246]    Overall Loss 0.265058    Objective Loss 0.265058                                        LR 0.000036    Time 0.028894    
2023-01-06 14:35:25,648 - Epoch: [63][  140/  246]    Overall Loss 0.266643    Objective Loss 0.266643                                        LR 0.000036    Time 0.028671    
2023-01-06 14:35:25,904 - Epoch: [63][  150/  246]    Overall Loss 0.267247    Objective Loss 0.267247                                        LR 0.000036    Time 0.028460    
2023-01-06 14:35:26,162 - Epoch: [63][  160/  246]    Overall Loss 0.266498    Objective Loss 0.266498                                        LR 0.000036    Time 0.028292    
2023-01-06 14:35:26,419 - Epoch: [63][  170/  246]    Overall Loss 0.266558    Objective Loss 0.266558                                        LR 0.000036    Time 0.028138    
2023-01-06 14:35:26,678 - Epoch: [63][  180/  246]    Overall Loss 0.265627    Objective Loss 0.265627                                        LR 0.000036    Time 0.028008    
2023-01-06 14:35:26,935 - Epoch: [63][  190/  246]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.000036    Time 0.027883    
2023-01-06 14:35:27,197 - Epoch: [63][  200/  246]    Overall Loss 0.263951    Objective Loss 0.263951                                        LR 0.000036    Time 0.027794    
2023-01-06 14:35:27,454 - Epoch: [63][  210/  246]    Overall Loss 0.263814    Objective Loss 0.263814                                        LR 0.000036    Time 0.027694    
2023-01-06 14:35:27,718 - Epoch: [63][  220/  246]    Overall Loss 0.264873    Objective Loss 0.264873                                        LR 0.000036    Time 0.027635    
2023-01-06 14:35:27,976 - Epoch: [63][  230/  246]    Overall Loss 0.264198    Objective Loss 0.264198                                        LR 0.000036    Time 0.027552    
2023-01-06 14:35:28,244 - Epoch: [63][  240/  246]    Overall Loss 0.263933    Objective Loss 0.263933                                        LR 0.000036    Time 0.027521    
2023-01-06 14:35:28,374 - Epoch: [63][  246/  246]    Overall Loss 0.263694    Objective Loss 0.263694    Top1 90.669856    LR 0.000036    Time 0.027377    
2023-01-06 14:35:28,511 - --- validate (epoch=63)-----------
2023-01-06 14:35:28,511 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:28,981 - Epoch: [63][   10/   28]    Loss 0.290204    Top1 89.687500    
2023-01-06 14:35:29,136 - Epoch: [63][   20/   28]    Loss 0.280269    Top1 89.882812    
2023-01-06 14:35:29,228 - Epoch: [63][   28/   28]    Loss 0.280808    Top1 89.636416    
2023-01-06 14:35:29,363 - ==> Top1: 89.636    Loss: 0.281

2023-01-06 14:35:29,363 - ==> Confusion:
[[ 252   10  177]
 [  18  269  315]
 [ 112   92 5741]]

2023-01-06 14:35:29,365 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 360896 on epoch: 61]
2023-01-06 14:35:29,365 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:29,375 - 

2023-01-06 14:35:29,375 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:30,112 - Epoch: [64][   10/  246]    Overall Loss 0.254822    Objective Loss 0.254822                                        LR 0.000036    Time 0.073606    
2023-01-06 14:35:30,357 - Epoch: [64][   20/  246]    Overall Loss 0.253331    Objective Loss 0.253331                                        LR 0.000036    Time 0.049038    
2023-01-06 14:35:30,604 - Epoch: [64][   30/  246]    Overall Loss 0.252733    Objective Loss 0.252733                                        LR 0.000036    Time 0.040865    
2023-01-06 14:35:30,850 - Epoch: [64][   40/  246]    Overall Loss 0.251789    Objective Loss 0.251789                                        LR 0.000036    Time 0.036803    
2023-01-06 14:35:31,095 - Epoch: [64][   50/  246]    Overall Loss 0.253734    Objective Loss 0.253734                                        LR 0.000036    Time 0.034330    
2023-01-06 14:35:31,342 - Epoch: [64][   60/  246]    Overall Loss 0.256996    Objective Loss 0.256996                                        LR 0.000036    Time 0.032715    
2023-01-06 14:35:31,585 - Epoch: [64][   70/  246]    Overall Loss 0.260239    Objective Loss 0.260239                                        LR 0.000036    Time 0.031509    
2023-01-06 14:35:31,830 - Epoch: [64][   80/  246]    Overall Loss 0.261696    Objective Loss 0.261696                                        LR 0.000036    Time 0.030625    
2023-01-06 14:35:32,076 - Epoch: [64][   90/  246]    Overall Loss 0.262266    Objective Loss 0.262266                                        LR 0.000036    Time 0.029955    
2023-01-06 14:35:32,321 - Epoch: [64][  100/  246]    Overall Loss 0.262040    Objective Loss 0.262040                                        LR 0.000036    Time 0.029407    
2023-01-06 14:35:32,567 - Epoch: [64][  110/  246]    Overall Loss 0.262855    Objective Loss 0.262855                                        LR 0.000036    Time 0.028965    
2023-01-06 14:35:32,812 - Epoch: [64][  120/  246]    Overall Loss 0.263722    Objective Loss 0.263722                                        LR 0.000036    Time 0.028591    
2023-01-06 14:35:33,056 - Epoch: [64][  130/  246]    Overall Loss 0.262973    Objective Loss 0.262973                                        LR 0.000036    Time 0.028262    
2023-01-06 14:35:33,297 - Epoch: [64][  140/  246]    Overall Loss 0.262846    Objective Loss 0.262846                                        LR 0.000036    Time 0.027965    
2023-01-06 14:35:33,544 - Epoch: [64][  150/  246]    Overall Loss 0.262031    Objective Loss 0.262031                                        LR 0.000036    Time 0.027744    
2023-01-06 14:35:33,798 - Epoch: [64][  160/  246]    Overall Loss 0.263725    Objective Loss 0.263725                                        LR 0.000036    Time 0.027593    
2023-01-06 14:35:34,054 - Epoch: [64][  170/  246]    Overall Loss 0.264246    Objective Loss 0.264246                                        LR 0.000036    Time 0.027471    
2023-01-06 14:35:34,302 - Epoch: [64][  180/  246]    Overall Loss 0.263430    Objective Loss 0.263430                                        LR 0.000036    Time 0.027323    
2023-01-06 14:35:34,552 - Epoch: [64][  190/  246]    Overall Loss 0.263197    Objective Loss 0.263197                                        LR 0.000036    Time 0.027196    
2023-01-06 14:35:34,800 - Epoch: [64][  200/  246]    Overall Loss 0.264315    Objective Loss 0.264315                                        LR 0.000036    Time 0.027076    
2023-01-06 14:35:35,051 - Epoch: [64][  210/  246]    Overall Loss 0.263317    Objective Loss 0.263317                                        LR 0.000036    Time 0.026980    
2023-01-06 14:35:35,302 - Epoch: [64][  220/  246]    Overall Loss 0.263551    Objective Loss 0.263551                                        LR 0.000036    Time 0.026890    
2023-01-06 14:35:35,549 - Epoch: [64][  230/  246]    Overall Loss 0.263312    Objective Loss 0.263312                                        LR 0.000036    Time 0.026793    
2023-01-06 14:35:35,807 - Epoch: [64][  240/  246]    Overall Loss 0.263740    Objective Loss 0.263740                                        LR 0.000036    Time 0.026750    
2023-01-06 14:35:35,936 - Epoch: [64][  246/  246]    Overall Loss 0.263363    Objective Loss 0.263363    Top1 90.909091    LR 0.000036    Time 0.026624    
2023-01-06 14:35:36,104 - --- validate (epoch=64)-----------
2023-01-06 14:35:36,105 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:36,574 - Epoch: [64][   10/   28]    Loss 0.296126    Top1 89.570312    
2023-01-06 14:35:36,713 - Epoch: [64][   20/   28]    Loss 0.283470    Top1 90.175781    
2023-01-06 14:35:36,804 - Epoch: [64][   28/   28]    Loss 0.270947    Top1 90.466648    
2023-01-06 14:35:36,940 - ==> Top1: 90.467    Loss: 0.271

2023-01-06 14:35:36,940 - ==> Confusion:
[[ 240   12  187]
 [  13  253  336]
 [  62   56 5827]]

2023-01-06 14:35:36,942 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:35:36,942 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:36,963 - 

2023-01-06 14:35:36,964 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:37,590 - Epoch: [65][   10/  246]    Overall Loss 0.283761    Objective Loss 0.283761                                        LR 0.000036    Time 0.062581    
2023-01-06 14:35:37,858 - Epoch: [65][   20/  246]    Overall Loss 0.274762    Objective Loss 0.274762                                        LR 0.000036    Time 0.044647    
2023-01-06 14:35:38,124 - Epoch: [65][   30/  246]    Overall Loss 0.276193    Objective Loss 0.276193                                        LR 0.000036    Time 0.038639    
2023-01-06 14:35:38,398 - Epoch: [65][   40/  246]    Overall Loss 0.267257    Objective Loss 0.267257                                        LR 0.000036    Time 0.035807    
2023-01-06 14:35:38,672 - Epoch: [65][   50/  246]    Overall Loss 0.264715    Objective Loss 0.264715                                        LR 0.000036    Time 0.034122    
2023-01-06 14:35:38,948 - Epoch: [65][   60/  246]    Overall Loss 0.266011    Objective Loss 0.266011                                        LR 0.000036    Time 0.033034    
2023-01-06 14:35:39,202 - Epoch: [65][   70/  246]    Overall Loss 0.265021    Objective Loss 0.265021                                        LR 0.000036    Time 0.031942    
2023-01-06 14:35:39,447 - Epoch: [65][   80/  246]    Overall Loss 0.265463    Objective Loss 0.265463                                        LR 0.000036    Time 0.031007    
2023-01-06 14:35:39,708 - Epoch: [65][   90/  246]    Overall Loss 0.262374    Objective Loss 0.262374                                        LR 0.000036    Time 0.030453    
2023-01-06 14:35:39,967 - Epoch: [65][  100/  246]    Overall Loss 0.262347    Objective Loss 0.262347                                        LR 0.000036    Time 0.030000    
2023-01-06 14:35:40,205 - Epoch: [65][  110/  246]    Overall Loss 0.264080    Objective Loss 0.264080                                        LR 0.000036    Time 0.029425    
2023-01-06 14:35:40,468 - Epoch: [65][  120/  246]    Overall Loss 0.264608    Objective Loss 0.264608                                        LR 0.000036    Time 0.029165    
2023-01-06 14:35:40,725 - Epoch: [65][  130/  246]    Overall Loss 0.264419    Objective Loss 0.264419                                        LR 0.000036    Time 0.028896    
2023-01-06 14:35:40,967 - Epoch: [65][  140/  246]    Overall Loss 0.264185    Objective Loss 0.264185                                        LR 0.000036    Time 0.028558    
2023-01-06 14:35:41,211 - Epoch: [65][  150/  246]    Overall Loss 0.265477    Objective Loss 0.265477                                        LR 0.000036    Time 0.028275    
2023-01-06 14:35:41,452 - Epoch: [65][  160/  246]    Overall Loss 0.265322    Objective Loss 0.265322                                        LR 0.000036    Time 0.028009    
2023-01-06 14:35:41,694 - Epoch: [65][  170/  246]    Overall Loss 0.265078    Objective Loss 0.265078                                        LR 0.000036    Time 0.027782    
2023-01-06 14:35:41,934 - Epoch: [65][  180/  246]    Overall Loss 0.265205    Objective Loss 0.265205                                        LR 0.000036    Time 0.027571    
2023-01-06 14:35:42,174 - Epoch: [65][  190/  246]    Overall Loss 0.264614    Objective Loss 0.264614                                        LR 0.000036    Time 0.027382    
2023-01-06 14:35:42,415 - Epoch: [65][  200/  246]    Overall Loss 0.264118    Objective Loss 0.264118                                        LR 0.000036    Time 0.027215    
2023-01-06 14:35:42,662 - Epoch: [65][  210/  246]    Overall Loss 0.263635    Objective Loss 0.263635                                        LR 0.000036    Time 0.027093    
2023-01-06 14:35:42,911 - Epoch: [65][  220/  246]    Overall Loss 0.264126    Objective Loss 0.264126                                        LR 0.000036    Time 0.026992    
2023-01-06 14:35:43,156 - Epoch: [65][  230/  246]    Overall Loss 0.264511    Objective Loss 0.264511                                        LR 0.000036    Time 0.026882    
2023-01-06 14:35:43,416 - Epoch: [65][  240/  246]    Overall Loss 0.264494    Objective Loss 0.264494                                        LR 0.000036    Time 0.026841    
2023-01-06 14:35:43,544 - Epoch: [65][  246/  246]    Overall Loss 0.265048    Objective Loss 0.265048    Top1 89.473684    LR 0.000036    Time 0.026706    
2023-01-06 14:35:43,680 - --- validate (epoch=65)-----------
2023-01-06 14:35:43,680 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:44,161 - Epoch: [65][   10/   28]    Loss 0.297239    Top1 89.296875    
2023-01-06 14:35:44,305 - Epoch: [65][   20/   28]    Loss 0.298057    Top1 89.492188    
2023-01-06 14:35:44,398 - Epoch: [65][   28/   28]    Loss 0.288864    Top1 89.765245    
2023-01-06 14:35:44,531 - ==> Top1: 89.765    Loss: 0.289

2023-01-06 14:35:44,531 - ==> Confusion:
[[ 239    7  193]
 [  17  259  326]
 [  76   96 5773]]

2023-01-06 14:35:44,533 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:35:44,533 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:44,543 - 

2023-01-06 14:35:44,543 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:45,287 - Epoch: [66][   10/  246]    Overall Loss 0.263974    Objective Loss 0.263974                                        LR 0.000036    Time 0.074315    
2023-01-06 14:35:45,541 - Epoch: [66][   20/  246]    Overall Loss 0.261775    Objective Loss 0.261775                                        LR 0.000036    Time 0.049838    
2023-01-06 14:35:45,798 - Epoch: [66][   30/  246]    Overall Loss 0.259578    Objective Loss 0.259578                                        LR 0.000036    Time 0.041792    
2023-01-06 14:35:46,044 - Epoch: [66][   40/  246]    Overall Loss 0.259658    Objective Loss 0.259658                                        LR 0.000036    Time 0.037485    
2023-01-06 14:35:46,291 - Epoch: [66][   50/  246]    Overall Loss 0.260658    Objective Loss 0.260658                                        LR 0.000036    Time 0.034918    
2023-01-06 14:35:46,539 - Epoch: [66][   60/  246]    Overall Loss 0.259917    Objective Loss 0.259917                                        LR 0.000036    Time 0.033228    
2023-01-06 14:35:46,787 - Epoch: [66][   70/  246]    Overall Loss 0.259068    Objective Loss 0.259068                                        LR 0.000036    Time 0.032014    
2023-01-06 14:35:47,035 - Epoch: [66][   80/  246]    Overall Loss 0.260228    Objective Loss 0.260228                                        LR 0.000036    Time 0.031104    
2023-01-06 14:35:47,284 - Epoch: [66][   90/  246]    Overall Loss 0.260956    Objective Loss 0.260956                                        LR 0.000036    Time 0.030414    
2023-01-06 14:35:47,534 - Epoch: [66][  100/  246]    Overall Loss 0.259890    Objective Loss 0.259890                                        LR 0.000036    Time 0.029865    
2023-01-06 14:35:47,783 - Epoch: [66][  110/  246]    Overall Loss 0.258349    Objective Loss 0.258349                                        LR 0.000036    Time 0.029410    
2023-01-06 14:35:48,033 - Epoch: [66][  120/  246]    Overall Loss 0.259145    Objective Loss 0.259145                                        LR 0.000036    Time 0.029033    
2023-01-06 14:35:48,282 - Epoch: [66][  130/  246]    Overall Loss 0.259820    Objective Loss 0.259820                                        LR 0.000036    Time 0.028716    
2023-01-06 14:35:48,532 - Epoch: [66][  140/  246]    Overall Loss 0.260122    Objective Loss 0.260122                                        LR 0.000036    Time 0.028444    
2023-01-06 14:35:48,782 - Epoch: [66][  150/  246]    Overall Loss 0.259809    Objective Loss 0.259809                                        LR 0.000036    Time 0.028211    
2023-01-06 14:35:49,032 - Epoch: [66][  160/  246]    Overall Loss 0.260134    Objective Loss 0.260134                                        LR 0.000036    Time 0.028007    
2023-01-06 14:35:49,282 - Epoch: [66][  170/  246]    Overall Loss 0.259891    Objective Loss 0.259891                                        LR 0.000036    Time 0.027829    
2023-01-06 14:35:49,532 - Epoch: [66][  180/  246]    Overall Loss 0.260703    Objective Loss 0.260703                                        LR 0.000036    Time 0.027668    
2023-01-06 14:35:49,782 - Epoch: [66][  190/  246]    Overall Loss 0.261099    Objective Loss 0.261099                                        LR 0.000036    Time 0.027527    
2023-01-06 14:35:50,033 - Epoch: [66][  200/  246]    Overall Loss 0.261651    Objective Loss 0.261651                                        LR 0.000036    Time 0.027400    
2023-01-06 14:35:50,280 - Epoch: [66][  210/  246]    Overall Loss 0.261740    Objective Loss 0.261740                                        LR 0.000036    Time 0.027269    
2023-01-06 14:35:50,529 - Epoch: [66][  220/  246]    Overall Loss 0.262068    Objective Loss 0.262068                                        LR 0.000036    Time 0.027161    
2023-01-06 14:35:50,777 - Epoch: [66][  230/  246]    Overall Loss 0.261962    Objective Loss 0.261962                                        LR 0.000036    Time 0.027058    
2023-01-06 14:35:51,038 - Epoch: [66][  240/  246]    Overall Loss 0.262010    Objective Loss 0.262010                                        LR 0.000036    Time 0.027015    
2023-01-06 14:35:51,168 - Epoch: [66][  246/  246]    Overall Loss 0.261695    Objective Loss 0.261695    Top1 91.387560    LR 0.000036    Time 0.026881    
2023-01-06 14:35:51,308 - --- validate (epoch=66)-----------
2023-01-06 14:35:51,308 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:51,747 - Epoch: [66][   10/   28]    Loss 0.279399    Top1 90.195312    
2023-01-06 14:35:51,897 - Epoch: [66][   20/   28]    Loss 0.289036    Top1 89.765625    
2023-01-06 14:35:51,987 - Epoch: [66][   28/   28]    Loss 0.287384    Top1 89.707987    
2023-01-06 14:35:52,125 - ==> Top1: 89.708    Loss: 0.287

2023-01-06 14:35:52,126 - ==> Confusion:
[[ 210    6  223]
 [  11  200  391]
 [  52   36 5857]]

2023-01-06 14:35:52,127 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:35:52,127 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:52,137 - 

2023-01-06 14:35:52,137 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:35:52,751 - Epoch: [67][   10/  246]    Overall Loss 0.254906    Objective Loss 0.254906                                        LR 0.000036    Time 0.061297    
2023-01-06 14:35:52,996 - Epoch: [67][   20/  246]    Overall Loss 0.258538    Objective Loss 0.258538                                        LR 0.000036    Time 0.042854    
2023-01-06 14:35:53,244 - Epoch: [67][   30/  246]    Overall Loss 0.258645    Objective Loss 0.258645                                        LR 0.000036    Time 0.036821    
2023-01-06 14:35:53,491 - Epoch: [67][   40/  246]    Overall Loss 0.258652    Objective Loss 0.258652                                        LR 0.000036    Time 0.033777    
2023-01-06 14:35:53,737 - Epoch: [67][   50/  246]    Overall Loss 0.258420    Objective Loss 0.258420                                        LR 0.000036    Time 0.031935    
2023-01-06 14:35:53,982 - Epoch: [67][   60/  246]    Overall Loss 0.258500    Objective Loss 0.258500                                        LR 0.000036    Time 0.030700    
2023-01-06 14:35:54,226 - Epoch: [67][   70/  246]    Overall Loss 0.260788    Objective Loss 0.260788                                        LR 0.000036    Time 0.029793    
2023-01-06 14:35:54,470 - Epoch: [67][   80/  246]    Overall Loss 0.257796    Objective Loss 0.257796                                        LR 0.000036    Time 0.029104    
2023-01-06 14:35:54,710 - Epoch: [67][   90/  246]    Overall Loss 0.257604    Objective Loss 0.257604                                        LR 0.000036    Time 0.028531    
2023-01-06 14:35:54,950 - Epoch: [67][  100/  246]    Overall Loss 0.258174    Objective Loss 0.258174                                        LR 0.000036    Time 0.028075    
2023-01-06 14:35:55,192 - Epoch: [67][  110/  246]    Overall Loss 0.258557    Objective Loss 0.258557                                        LR 0.000036    Time 0.027718    
2023-01-06 14:35:55,431 - Epoch: [67][  120/  246]    Overall Loss 0.258014    Objective Loss 0.258014                                        LR 0.000036    Time 0.027396    
2023-01-06 14:35:55,672 - Epoch: [67][  130/  246]    Overall Loss 0.258191    Objective Loss 0.258191                                        LR 0.000036    Time 0.027141    
2023-01-06 14:35:55,913 - Epoch: [67][  140/  246]    Overall Loss 0.258192    Objective Loss 0.258192                                        LR 0.000036    Time 0.026922    
2023-01-06 14:35:56,155 - Epoch: [67][  150/  246]    Overall Loss 0.258338    Objective Loss 0.258338                                        LR 0.000036    Time 0.026737    
2023-01-06 14:35:56,398 - Epoch: [67][  160/  246]    Overall Loss 0.258948    Objective Loss 0.258948                                        LR 0.000036    Time 0.026586    
2023-01-06 14:35:56,640 - Epoch: [67][  170/  246]    Overall Loss 0.258250    Objective Loss 0.258250                                        LR 0.000036    Time 0.026444    
2023-01-06 14:35:56,882 - Epoch: [67][  180/  246]    Overall Loss 0.258264    Objective Loss 0.258264                                        LR 0.000036    Time 0.026315    
2023-01-06 14:35:57,124 - Epoch: [67][  190/  246]    Overall Loss 0.257326    Objective Loss 0.257326                                        LR 0.000036    Time 0.026207    
2023-01-06 14:35:57,367 - Epoch: [67][  200/  246]    Overall Loss 0.257501    Objective Loss 0.257501                                        LR 0.000036    Time 0.026109    
2023-01-06 14:35:57,608 - Epoch: [67][  210/  246]    Overall Loss 0.257825    Objective Loss 0.257825                                        LR 0.000036    Time 0.026012    
2023-01-06 14:35:57,852 - Epoch: [67][  220/  246]    Overall Loss 0.258549    Objective Loss 0.258549                                        LR 0.000036    Time 0.025937    
2023-01-06 14:35:58,104 - Epoch: [67][  230/  246]    Overall Loss 0.258670    Objective Loss 0.258670                                        LR 0.000036    Time 0.025900    
2023-01-06 14:35:58,362 - Epoch: [67][  240/  246]    Overall Loss 0.259839    Objective Loss 0.259839                                        LR 0.000036    Time 0.025896    
2023-01-06 14:35:58,492 - Epoch: [67][  246/  246]    Overall Loss 0.260650    Objective Loss 0.260650    Top1 90.669856    LR 0.000036    Time 0.025791    
2023-01-06 14:35:58,621 - --- validate (epoch=67)-----------
2023-01-06 14:35:58,621 - 6986 samples (256 per mini-batch)
2023-01-06 14:35:59,054 - Epoch: [67][   10/   28]    Loss 0.279961    Top1 90.156250    
2023-01-06 14:35:59,194 - Epoch: [67][   20/   28]    Loss 0.277311    Top1 90.058594    
2023-01-06 14:35:59,286 - Epoch: [67][   28/   28]    Loss 0.273075    Top1 90.395076    
2023-01-06 14:35:59,425 - ==> Top1: 90.395    Loss: 0.273

2023-01-06 14:35:59,425 - ==> Confusion:
[[ 248   12  179]
 [  12  276  314]
 [  78   76 5791]]

2023-01-06 14:35:59,426 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:35:59,426 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:35:59,436 - 

2023-01-06 14:35:59,437 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:00,185 - Epoch: [68][   10/  246]    Overall Loss 0.248205    Objective Loss 0.248205                                        LR 0.000036    Time 0.074722    
2023-01-06 14:36:00,425 - Epoch: [68][   20/  246]    Overall Loss 0.251244    Objective Loss 0.251244                                        LR 0.000036    Time 0.049375    
2023-01-06 14:36:00,681 - Epoch: [68][   30/  246]    Overall Loss 0.259887    Objective Loss 0.259887                                        LR 0.000036    Time 0.041428    
2023-01-06 14:36:00,934 - Epoch: [68][   40/  246]    Overall Loss 0.260835    Objective Loss 0.260835                                        LR 0.000036    Time 0.037393    
2023-01-06 14:36:01,183 - Epoch: [68][   50/  246]    Overall Loss 0.260400    Objective Loss 0.260400                                        LR 0.000036    Time 0.034879    
2023-01-06 14:36:01,436 - Epoch: [68][   60/  246]    Overall Loss 0.259310    Objective Loss 0.259310                                        LR 0.000036    Time 0.033269    
2023-01-06 14:36:01,684 - Epoch: [68][   70/  246]    Overall Loss 0.259646    Objective Loss 0.259646                                        LR 0.000036    Time 0.032050    
2023-01-06 14:36:01,941 - Epoch: [68][   80/  246]    Overall Loss 0.261896    Objective Loss 0.261896                                        LR 0.000036    Time 0.031258    
2023-01-06 14:36:02,189 - Epoch: [68][   90/  246]    Overall Loss 0.261877    Objective Loss 0.261877                                        LR 0.000036    Time 0.030530    
2023-01-06 14:36:02,440 - Epoch: [68][  100/  246]    Overall Loss 0.261000    Objective Loss 0.261000                                        LR 0.000036    Time 0.029987    
2023-01-06 14:36:02,688 - Epoch: [68][  110/  246]    Overall Loss 0.260156    Objective Loss 0.260156                                        LR 0.000036    Time 0.029514    
2023-01-06 14:36:02,937 - Epoch: [68][  120/  246]    Overall Loss 0.259649    Objective Loss 0.259649                                        LR 0.000036    Time 0.029122    
2023-01-06 14:36:03,188 - Epoch: [68][  130/  246]    Overall Loss 0.258892    Objective Loss 0.258892                                        LR 0.000036    Time 0.028811    
2023-01-06 14:36:03,436 - Epoch: [68][  140/  246]    Overall Loss 0.260193    Objective Loss 0.260193                                        LR 0.000036    Time 0.028522    
2023-01-06 14:36:03,688 - Epoch: [68][  150/  246]    Overall Loss 0.259571    Objective Loss 0.259571                                        LR 0.000036    Time 0.028295    
2023-01-06 14:36:03,937 - Epoch: [68][  160/  246]    Overall Loss 0.260579    Objective Loss 0.260579                                        LR 0.000036    Time 0.028082    
2023-01-06 14:36:04,189 - Epoch: [68][  170/  246]    Overall Loss 0.260341    Objective Loss 0.260341                                        LR 0.000036    Time 0.027910    
2023-01-06 14:36:04,446 - Epoch: [68][  180/  246]    Overall Loss 0.259868    Objective Loss 0.259868                                        LR 0.000036    Time 0.027781    
2023-01-06 14:36:04,701 - Epoch: [68][  190/  246]    Overall Loss 0.259275    Objective Loss 0.259275                                        LR 0.000036    Time 0.027662    
2023-01-06 14:36:04,968 - Epoch: [68][  200/  246]    Overall Loss 0.259326    Objective Loss 0.259326                                        LR 0.000036    Time 0.027604    
2023-01-06 14:36:05,226 - Epoch: [68][  210/  246]    Overall Loss 0.259960    Objective Loss 0.259960                                        LR 0.000036    Time 0.027517    
2023-01-06 14:36:05,485 - Epoch: [68][  220/  246]    Overall Loss 0.259405    Objective Loss 0.259405                                        LR 0.000036    Time 0.027440    
2023-01-06 14:36:05,727 - Epoch: [68][  230/  246]    Overall Loss 0.259616    Objective Loss 0.259616                                        LR 0.000036    Time 0.027298    
2023-01-06 14:36:05,986 - Epoch: [68][  240/  246]    Overall Loss 0.259655    Objective Loss 0.259655                                        LR 0.000036    Time 0.027236    
2023-01-06 14:36:06,114 - Epoch: [68][  246/  246]    Overall Loss 0.260358    Objective Loss 0.260358    Top1 89.952153    LR 0.000036    Time 0.027093    
2023-01-06 14:36:06,247 - --- validate (epoch=68)-----------
2023-01-06 14:36:06,248 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:06,687 - Epoch: [68][   10/   28]    Loss 0.276573    Top1 90.117188    
2023-01-06 14:36:06,831 - Epoch: [68][   20/   28]    Loss 0.278474    Top1 90.175781    
2023-01-06 14:36:06,923 - Epoch: [68][   28/   28]    Loss 0.271191    Top1 90.438019    
2023-01-06 14:36:07,070 - ==> Top1: 90.438    Loss: 0.271

2023-01-06 14:36:07,071 - ==> Confusion:
[[ 237   13  189]
 [  15  270  317]
 [  59   75 5811]]

2023-01-06 14:36:07,072 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:36:07,072 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:07,082 - 

2023-01-06 14:36:07,082 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:07,687 - Epoch: [69][   10/  246]    Overall Loss 0.263991    Objective Loss 0.263991                                        LR 0.000036    Time 0.060365    
2023-01-06 14:36:07,931 - Epoch: [69][   20/  246]    Overall Loss 0.265678    Objective Loss 0.265678                                        LR 0.000036    Time 0.042369    
2023-01-06 14:36:08,180 - Epoch: [69][   30/  246]    Overall Loss 0.266724    Objective Loss 0.266724                                        LR 0.000036    Time 0.036545    
2023-01-06 14:36:08,431 - Epoch: [69][   40/  246]    Overall Loss 0.260994    Objective Loss 0.260994                                        LR 0.000036    Time 0.033679    
2023-01-06 14:36:08,678 - Epoch: [69][   50/  246]    Overall Loss 0.256440    Objective Loss 0.256440                                        LR 0.000036    Time 0.031864    
2023-01-06 14:36:08,928 - Epoch: [69][   60/  246]    Overall Loss 0.256793    Objective Loss 0.256793                                        LR 0.000036    Time 0.030721    
2023-01-06 14:36:09,174 - Epoch: [69][   70/  246]    Overall Loss 0.257161    Objective Loss 0.257161                                        LR 0.000036    Time 0.029831    
2023-01-06 14:36:09,429 - Epoch: [69][   80/  246]    Overall Loss 0.257261    Objective Loss 0.257261                                        LR 0.000036    Time 0.029294    
2023-01-06 14:36:09,671 - Epoch: [69][   90/  246]    Overall Loss 0.259385    Objective Loss 0.259385                                        LR 0.000036    Time 0.028718    
2023-01-06 14:36:09,914 - Epoch: [69][  100/  246]    Overall Loss 0.258592    Objective Loss 0.258592                                        LR 0.000036    Time 0.028273    
2023-01-06 14:36:10,156 - Epoch: [69][  110/  246]    Overall Loss 0.258576    Objective Loss 0.258576                                        LR 0.000036    Time 0.027902    
2023-01-06 14:36:10,399 - Epoch: [69][  120/  246]    Overall Loss 0.258160    Objective Loss 0.258160                                        LR 0.000036    Time 0.027592    
2023-01-06 14:36:10,640 - Epoch: [69][  130/  246]    Overall Loss 0.259984    Objective Loss 0.259984                                        LR 0.000036    Time 0.027325    
2023-01-06 14:36:10,882 - Epoch: [69][  140/  246]    Overall Loss 0.259108    Objective Loss 0.259108                                        LR 0.000036    Time 0.027095    
2023-01-06 14:36:11,119 - Epoch: [69][  150/  246]    Overall Loss 0.260011    Objective Loss 0.260011                                        LR 0.000036    Time 0.026869    
2023-01-06 14:36:11,369 - Epoch: [69][  160/  246]    Overall Loss 0.259827    Objective Loss 0.259827                                        LR 0.000036    Time 0.026748    
2023-01-06 14:36:11,617 - Epoch: [69][  170/  246]    Overall Loss 0.259023    Objective Loss 0.259023                                        LR 0.000036    Time 0.026629    
2023-01-06 14:36:11,867 - Epoch: [69][  180/  246]    Overall Loss 0.260110    Objective Loss 0.260110                                        LR 0.000036    Time 0.026534    
2023-01-06 14:36:12,111 - Epoch: [69][  190/  246]    Overall Loss 0.260406    Objective Loss 0.260406                                        LR 0.000036    Time 0.026418    
2023-01-06 14:36:12,360 - Epoch: [69][  200/  246]    Overall Loss 0.259864    Objective Loss 0.259864                                        LR 0.000036    Time 0.026343    
2023-01-06 14:36:12,607 - Epoch: [69][  210/  246]    Overall Loss 0.259354    Objective Loss 0.259354                                        LR 0.000036    Time 0.026260    
2023-01-06 14:36:12,858 - Epoch: [69][  220/  246]    Overall Loss 0.258479    Objective Loss 0.258479                                        LR 0.000036    Time 0.026208    
2023-01-06 14:36:13,101 - Epoch: [69][  230/  246]    Overall Loss 0.257395    Objective Loss 0.257395                                        LR 0.000036    Time 0.026122    
2023-01-06 14:36:13,368 - Epoch: [69][  240/  246]    Overall Loss 0.258392    Objective Loss 0.258392                                        LR 0.000036    Time 0.026136    
2023-01-06 14:36:13,500 - Epoch: [69][  246/  246]    Overall Loss 0.258350    Objective Loss 0.258350    Top1 88.995215    LR 0.000036    Time 0.026035    
2023-01-06 14:36:13,625 - --- validate (epoch=69)-----------
2023-01-06 14:36:13,626 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:14,076 - Epoch: [69][   10/   28]    Loss 0.279788    Top1 89.609375    
2023-01-06 14:36:14,219 - Epoch: [69][   20/   28]    Loss 0.275451    Top1 89.941406    
2023-01-06 14:36:14,311 - Epoch: [69][   28/   28]    Loss 0.278950    Top1 89.908388    
2023-01-06 14:36:14,453 - ==> Top1: 89.908    Loss: 0.279

2023-01-06 14:36:14,453 - ==> Confusion:
[[ 201   13  225]
 [   7  255  340]
 [  43   77 5825]]

2023-01-06 14:36:14,454 - ==> Best [Top1: 90.467   Sparsity:0.00   Params: 360896 on epoch: 64]
2023-01-06 14:36:14,454 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:14,464 - 

2023-01-06 14:36:14,464 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:15,198 - Epoch: [70][   10/  246]    Overall Loss 0.244903    Objective Loss 0.244903                                        LR 0.000022    Time 0.073327    
2023-01-06 14:36:15,443 - Epoch: [70][   20/  246]    Overall Loss 0.253533    Objective Loss 0.253533                                        LR 0.000022    Time 0.048857    
2023-01-06 14:36:15,689 - Epoch: [70][   30/  246]    Overall Loss 0.253105    Objective Loss 0.253105                                        LR 0.000022    Time 0.040707    
2023-01-06 14:36:15,932 - Epoch: [70][   40/  246]    Overall Loss 0.251529    Objective Loss 0.251529                                        LR 0.000022    Time 0.036594    
2023-01-06 14:36:16,178 - Epoch: [70][   50/  246]    Overall Loss 0.256570    Objective Loss 0.256570                                        LR 0.000022    Time 0.034163    
2023-01-06 14:36:16,420 - Epoch: [70][   60/  246]    Overall Loss 0.257799    Objective Loss 0.257799                                        LR 0.000022    Time 0.032489    
2023-01-06 14:36:16,673 - Epoch: [70][   70/  246]    Overall Loss 0.257763    Objective Loss 0.257763                                        LR 0.000022    Time 0.031439    
2023-01-06 14:36:16,917 - Epoch: [70][   80/  246]    Overall Loss 0.255901    Objective Loss 0.255901                                        LR 0.000022    Time 0.030551    
2023-01-06 14:36:17,165 - Epoch: [70][   90/  246]    Overall Loss 0.255150    Objective Loss 0.255150                                        LR 0.000022    Time 0.029889    
2023-01-06 14:36:17,414 - Epoch: [70][  100/  246]    Overall Loss 0.256124    Objective Loss 0.256124                                        LR 0.000022    Time 0.029377    
2023-01-06 14:36:17,662 - Epoch: [70][  110/  246]    Overall Loss 0.254325    Objective Loss 0.254325                                        LR 0.000022    Time 0.028952    
2023-01-06 14:36:17,907 - Epoch: [70][  120/  246]    Overall Loss 0.253297    Objective Loss 0.253297                                        LR 0.000022    Time 0.028577    
2023-01-06 14:36:18,156 - Epoch: [70][  130/  246]    Overall Loss 0.254311    Objective Loss 0.254311                                        LR 0.000022    Time 0.028284    
2023-01-06 14:36:18,401 - Epoch: [70][  140/  246]    Overall Loss 0.252888    Objective Loss 0.252888                                        LR 0.000022    Time 0.028011    
2023-01-06 14:36:18,650 - Epoch: [70][  150/  246]    Overall Loss 0.252723    Objective Loss 0.252723                                        LR 0.000022    Time 0.027800    
2023-01-06 14:36:18,897 - Epoch: [70][  160/  246]    Overall Loss 0.252029    Objective Loss 0.252029                                        LR 0.000022    Time 0.027604    
2023-01-06 14:36:19,143 - Epoch: [70][  170/  246]    Overall Loss 0.251962    Objective Loss 0.251962                                        LR 0.000022    Time 0.027427    
2023-01-06 14:36:19,398 - Epoch: [70][  180/  246]    Overall Loss 0.250825    Objective Loss 0.250825                                        LR 0.000022    Time 0.027318    
2023-01-06 14:36:19,648 - Epoch: [70][  190/  246]    Overall Loss 0.250666    Objective Loss 0.250666                                        LR 0.000022    Time 0.027192    
2023-01-06 14:36:19,900 - Epoch: [70][  200/  246]    Overall Loss 0.251119    Objective Loss 0.251119                                        LR 0.000022    Time 0.027090    
2023-01-06 14:36:20,139 - Epoch: [70][  210/  246]    Overall Loss 0.251888    Objective Loss 0.251888                                        LR 0.000022    Time 0.026938    
2023-01-06 14:36:20,382 - Epoch: [70][  220/  246]    Overall Loss 0.252010    Objective Loss 0.252010                                        LR 0.000022    Time 0.026817    
2023-01-06 14:36:20,627 - Epoch: [70][  230/  246]    Overall Loss 0.252289    Objective Loss 0.252289                                        LR 0.000022    Time 0.026713    
2023-01-06 14:36:20,882 - Epoch: [70][  240/  246]    Overall Loss 0.252500    Objective Loss 0.252500                                        LR 0.000022    Time 0.026663    
2023-01-06 14:36:21,011 - Epoch: [70][  246/  246]    Overall Loss 0.252189    Objective Loss 0.252189    Top1 91.387560    LR 0.000022    Time 0.026533    
2023-01-06 14:36:21,184 - --- validate (epoch=70)-----------
2023-01-06 14:36:21,184 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:21,641 - Epoch: [70][   10/   28]    Loss 0.267134    Top1 90.781250    
2023-01-06 14:36:21,786 - Epoch: [70][   20/   28]    Loss 0.281630    Top1 90.039062    
2023-01-06 14:36:21,878 - Epoch: [70][   28/   28]    Loss 0.271840    Top1 90.538219    
2023-01-06 14:36:22,009 - ==> Top1: 90.538    Loss: 0.272

2023-01-06 14:36:22,010 - ==> Confusion:
[[ 238   11  190]
 [  11  286  305]
 [  59   85 5801]]

2023-01-06 14:36:22,011 - ==> Best [Top1: 90.538   Sparsity:0.00   Params: 360896 on epoch: 70]
2023-01-06 14:36:22,011 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:22,036 - 

2023-01-06 14:36:22,036 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:22,792 - Epoch: [71][   10/  246]    Overall Loss 0.250351    Objective Loss 0.250351                                        LR 0.000022    Time 0.075549    
2023-01-06 14:36:23,042 - Epoch: [71][   20/  246]    Overall Loss 0.252391    Objective Loss 0.252391                                        LR 0.000022    Time 0.050247    
2023-01-06 14:36:23,291 - Epoch: [71][   30/  246]    Overall Loss 0.254693    Objective Loss 0.254693                                        LR 0.000022    Time 0.041746    
2023-01-06 14:36:23,541 - Epoch: [71][   40/  246]    Overall Loss 0.252393    Objective Loss 0.252393                                        LR 0.000022    Time 0.037515    
2023-01-06 14:36:23,793 - Epoch: [71][   50/  246]    Overall Loss 0.254195    Objective Loss 0.254195                                        LR 0.000022    Time 0.034997    
2023-01-06 14:36:24,044 - Epoch: [71][   60/  246]    Overall Loss 0.255851    Objective Loss 0.255851                                        LR 0.000022    Time 0.033349    
2023-01-06 14:36:24,295 - Epoch: [71][   70/  246]    Overall Loss 0.257897    Objective Loss 0.257897                                        LR 0.000022    Time 0.032161    
2023-01-06 14:36:24,539 - Epoch: [71][   80/  246]    Overall Loss 0.258291    Objective Loss 0.258291                                        LR 0.000022    Time 0.031185    
2023-01-06 14:36:24,780 - Epoch: [71][   90/  246]    Overall Loss 0.257181    Objective Loss 0.257181                                        LR 0.000022    Time 0.030385    
2023-01-06 14:36:25,026 - Epoch: [71][  100/  246]    Overall Loss 0.256414    Objective Loss 0.256414                                        LR 0.000022    Time 0.029806    
2023-01-06 14:36:25,270 - Epoch: [71][  110/  246]    Overall Loss 0.256498    Objective Loss 0.256498                                        LR 0.000022    Time 0.029303    
2023-01-06 14:36:25,516 - Epoch: [71][  120/  246]    Overall Loss 0.257127    Objective Loss 0.257127                                        LR 0.000022    Time 0.028914    
2023-01-06 14:36:25,764 - Epoch: [71][  130/  246]    Overall Loss 0.257603    Objective Loss 0.257603                                        LR 0.000022    Time 0.028591    
2023-01-06 14:36:26,008 - Epoch: [71][  140/  246]    Overall Loss 0.257100    Objective Loss 0.257100                                        LR 0.000022    Time 0.028289    
2023-01-06 14:36:26,254 - Epoch: [71][  150/  246]    Overall Loss 0.257239    Objective Loss 0.257239                                        LR 0.000022    Time 0.028041    
2023-01-06 14:36:26,503 - Epoch: [71][  160/  246]    Overall Loss 0.258163    Objective Loss 0.258163                                        LR 0.000022    Time 0.027831    
2023-01-06 14:36:26,752 - Epoch: [71][  170/  246]    Overall Loss 0.258341    Objective Loss 0.258341                                        LR 0.000022    Time 0.027659    
2023-01-06 14:36:27,001 - Epoch: [71][  180/  246]    Overall Loss 0.257740    Objective Loss 0.257740                                        LR 0.000022    Time 0.027501    
2023-01-06 14:36:27,240 - Epoch: [71][  190/  246]    Overall Loss 0.256776    Objective Loss 0.256776                                        LR 0.000022    Time 0.027314    
2023-01-06 14:36:27,489 - Epoch: [71][  200/  246]    Overall Loss 0.256096    Objective Loss 0.256096                                        LR 0.000022    Time 0.027187    
2023-01-06 14:36:27,732 - Epoch: [71][  210/  246]    Overall Loss 0.256098    Objective Loss 0.256098                                        LR 0.000022    Time 0.027048    
2023-01-06 14:36:27,975 - Epoch: [71][  220/  246]    Overall Loss 0.256012    Objective Loss 0.256012                                        LR 0.000022    Time 0.026924    
2023-01-06 14:36:28,222 - Epoch: [71][  230/  246]    Overall Loss 0.254634    Objective Loss 0.254634                                        LR 0.000022    Time 0.026824    
2023-01-06 14:36:28,477 - Epoch: [71][  240/  246]    Overall Loss 0.253729    Objective Loss 0.253729                                        LR 0.000022    Time 0.026770    
2023-01-06 14:36:28,606 - Epoch: [71][  246/  246]    Overall Loss 0.253548    Objective Loss 0.253548    Top1 91.148325    LR 0.000022    Time 0.026638    
2023-01-06 14:36:28,750 - --- validate (epoch=71)-----------
2023-01-06 14:36:28,751 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:29,217 - Epoch: [71][   10/   28]    Loss 0.278375    Top1 89.609375    
2023-01-06 14:36:29,364 - Epoch: [71][   20/   28]    Loss 0.273286    Top1 90.175781    
2023-01-06 14:36:29,456 - Epoch: [71][   28/   28]    Loss 0.266424    Top1 90.438019    
2023-01-06 14:36:29,600 - ==> Top1: 90.438    Loss: 0.266

2023-01-06 14:36:29,600 - ==> Confusion:
[[ 223   16  200]
 [   9  275  318]
 [  54   71 5820]]

2023-01-06 14:36:29,602 - ==> Best [Top1: 90.538   Sparsity:0.00   Params: 360896 on epoch: 70]
2023-01-06 14:36:29,602 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:29,612 - 

2023-01-06 14:36:29,612 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:30,208 - Epoch: [72][   10/  246]    Overall Loss 0.258348    Objective Loss 0.258348                                        LR 0.000022    Time 0.059549    
2023-01-06 14:36:30,455 - Epoch: [72][   20/  246]    Overall Loss 0.273899    Objective Loss 0.273899                                        LR 0.000022    Time 0.042102    
2023-01-06 14:36:30,712 - Epoch: [72][   30/  246]    Overall Loss 0.266914    Objective Loss 0.266914                                        LR 0.000022    Time 0.036618    
2023-01-06 14:36:30,973 - Epoch: [72][   40/  246]    Overall Loss 0.262616    Objective Loss 0.262616                                        LR 0.000022    Time 0.033977    
2023-01-06 14:36:31,238 - Epoch: [72][   50/  246]    Overall Loss 0.257315    Objective Loss 0.257315                                        LR 0.000022    Time 0.032474    
2023-01-06 14:36:31,499 - Epoch: [72][   60/  246]    Overall Loss 0.261212    Objective Loss 0.261212                                        LR 0.000022    Time 0.031405    
2023-01-06 14:36:31,754 - Epoch: [72][   70/  246]    Overall Loss 0.258630    Objective Loss 0.258630                                        LR 0.000022    Time 0.030558    
2023-01-06 14:36:32,007 - Epoch: [72][   80/  246]    Overall Loss 0.257436    Objective Loss 0.257436                                        LR 0.000022    Time 0.029894    
2023-01-06 14:36:32,265 - Epoch: [72][   90/  246]    Overall Loss 0.255985    Objective Loss 0.255985                                        LR 0.000022    Time 0.029439    
2023-01-06 14:36:32,511 - Epoch: [72][  100/  246]    Overall Loss 0.253809    Objective Loss 0.253809                                        LR 0.000022    Time 0.028950    
2023-01-06 14:36:32,761 - Epoch: [72][  110/  246]    Overall Loss 0.252714    Objective Loss 0.252714                                        LR 0.000022    Time 0.028570    
2023-01-06 14:36:33,004 - Epoch: [72][  120/  246]    Overall Loss 0.254248    Objective Loss 0.254248                                        LR 0.000022    Time 0.028196    
2023-01-06 14:36:33,259 - Epoch: [72][  130/  246]    Overall Loss 0.253223    Objective Loss 0.253223                                        LR 0.000022    Time 0.027989    
2023-01-06 14:36:33,526 - Epoch: [72][  140/  246]    Overall Loss 0.253761    Objective Loss 0.253761                                        LR 0.000022    Time 0.027875    
2023-01-06 14:36:33,787 - Epoch: [72][  150/  246]    Overall Loss 0.254732    Objective Loss 0.254732                                        LR 0.000022    Time 0.027744    
2023-01-06 14:36:34,056 - Epoch: [72][  160/  246]    Overall Loss 0.253784    Objective Loss 0.253784                                        LR 0.000022    Time 0.027684    
2023-01-06 14:36:34,323 - Epoch: [72][  170/  246]    Overall Loss 0.253222    Objective Loss 0.253222                                        LR 0.000022    Time 0.027623    
2023-01-06 14:36:34,568 - Epoch: [72][  180/  246]    Overall Loss 0.253342    Objective Loss 0.253342                                        LR 0.000022    Time 0.027448    
2023-01-06 14:36:34,797 - Epoch: [72][  190/  246]    Overall Loss 0.252987    Objective Loss 0.252987                                        LR 0.000022    Time 0.027208    
2023-01-06 14:36:35,020 - Epoch: [72][  200/  246]    Overall Loss 0.253134    Objective Loss 0.253134                                        LR 0.000022    Time 0.026957    
2023-01-06 14:36:35,244 - Epoch: [72][  210/  246]    Overall Loss 0.253005    Objective Loss 0.253005                                        LR 0.000022    Time 0.026743    
2023-01-06 14:36:35,469 - Epoch: [72][  220/  246]    Overall Loss 0.253663    Objective Loss 0.253663                                        LR 0.000022    Time 0.026547    
2023-01-06 14:36:35,694 - Epoch: [72][  230/  246]    Overall Loss 0.252997    Objective Loss 0.252997                                        LR 0.000022    Time 0.026367    
2023-01-06 14:36:35,949 - Epoch: [72][  240/  246]    Overall Loss 0.252968    Objective Loss 0.252968                                        LR 0.000022    Time 0.026331    
2023-01-06 14:36:36,073 - Epoch: [72][  246/  246]    Overall Loss 0.252859    Objective Loss 0.252859    Top1 90.430622    LR 0.000022    Time 0.026194    
2023-01-06 14:36:36,211 - --- validate (epoch=72)-----------
2023-01-06 14:36:36,211 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:36,673 - Epoch: [72][   10/   28]    Loss 0.251671    Top1 91.093750    
2023-01-06 14:36:36,832 - Epoch: [72][   20/   28]    Loss 0.270605    Top1 90.507812    
2023-01-06 14:36:36,922 - Epoch: [72][   28/   28]    Loss 0.265870    Top1 90.595477    
2023-01-06 14:36:37,054 - ==> Top1: 90.595    Loss: 0.266

2023-01-06 14:36:37,055 - ==> Confusion:
[[ 255    9  175]
 [  16  271  315]
 [  66   76 5803]]

2023-01-06 14:36:37,056 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 360896 on epoch: 72]
2023-01-06 14:36:37,056 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:37,077 - 

2023-01-06 14:36:37,078 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:37,823 - Epoch: [73][   10/  246]    Overall Loss 0.251126    Objective Loss 0.251126                                        LR 0.000022    Time 0.074486    
2023-01-06 14:36:38,073 - Epoch: [73][   20/  246]    Overall Loss 0.248971    Objective Loss 0.248971                                        LR 0.000022    Time 0.049705    
2023-01-06 14:36:38,315 - Epoch: [73][   30/  246]    Overall Loss 0.250078    Objective Loss 0.250078                                        LR 0.000022    Time 0.041204    
2023-01-06 14:36:38,568 - Epoch: [73][   40/  246]    Overall Loss 0.249855    Objective Loss 0.249855                                        LR 0.000022    Time 0.037210    
2023-01-06 14:36:38,815 - Epoch: [73][   50/  246]    Overall Loss 0.251315    Objective Loss 0.251315                                        LR 0.000022    Time 0.034707    
2023-01-06 14:36:39,064 - Epoch: [73][   60/  246]    Overall Loss 0.252053    Objective Loss 0.252053                                        LR 0.000022    Time 0.033058    
2023-01-06 14:36:39,297 - Epoch: [73][   70/  246]    Overall Loss 0.249932    Objective Loss 0.249932                                        LR 0.000022    Time 0.031664    
2023-01-06 14:36:39,549 - Epoch: [73][   80/  246]    Overall Loss 0.254555    Objective Loss 0.254555                                        LR 0.000022    Time 0.030846    
2023-01-06 14:36:39,799 - Epoch: [73][   90/  246]    Overall Loss 0.253710    Objective Loss 0.253710                                        LR 0.000022    Time 0.030193    
2023-01-06 14:36:40,051 - Epoch: [73][  100/  246]    Overall Loss 0.253446    Objective Loss 0.253446                                        LR 0.000022    Time 0.029685    
2023-01-06 14:36:40,306 - Epoch: [73][  110/  246]    Overall Loss 0.254127    Objective Loss 0.254127                                        LR 0.000022    Time 0.029308    
2023-01-06 14:36:40,557 - Epoch: [73][  120/  246]    Overall Loss 0.253871    Objective Loss 0.253871                                        LR 0.000022    Time 0.028950    
2023-01-06 14:36:40,809 - Epoch: [73][  130/  246]    Overall Loss 0.253030    Objective Loss 0.253030                                        LR 0.000022    Time 0.028663    
2023-01-06 14:36:41,060 - Epoch: [73][  140/  246]    Overall Loss 0.252998    Objective Loss 0.252998                                        LR 0.000022    Time 0.028402    
2023-01-06 14:36:41,313 - Epoch: [73][  150/  246]    Overall Loss 0.252905    Objective Loss 0.252905                                        LR 0.000022    Time 0.028198    
2023-01-06 14:36:41,562 - Epoch: [73][  160/  246]    Overall Loss 0.252983    Objective Loss 0.252983                                        LR 0.000022    Time 0.027991    
2023-01-06 14:36:41,817 - Epoch: [73][  170/  246]    Overall Loss 0.253266    Objective Loss 0.253266                                        LR 0.000022    Time 0.027839    
2023-01-06 14:36:42,066 - Epoch: [73][  180/  246]    Overall Loss 0.254373    Objective Loss 0.254373                                        LR 0.000022    Time 0.027674    
2023-01-06 14:36:42,319 - Epoch: [73][  190/  246]    Overall Loss 0.253007    Objective Loss 0.253007                                        LR 0.000022    Time 0.027550    
2023-01-06 14:36:42,570 - Epoch: [73][  200/  246]    Overall Loss 0.254077    Objective Loss 0.254077                                        LR 0.000022    Time 0.027423    
2023-01-06 14:36:42,822 - Epoch: [73][  210/  246]    Overall Loss 0.253433    Objective Loss 0.253433                                        LR 0.000022    Time 0.027320    
2023-01-06 14:36:43,072 - Epoch: [73][  220/  246]    Overall Loss 0.253619    Objective Loss 0.253619                                        LR 0.000022    Time 0.027212    
2023-01-06 14:36:43,324 - Epoch: [73][  230/  246]    Overall Loss 0.253816    Objective Loss 0.253816                                        LR 0.000022    Time 0.027121    
2023-01-06 14:36:43,583 - Epoch: [73][  240/  246]    Overall Loss 0.253241    Objective Loss 0.253241                                        LR 0.000022    Time 0.027068    
2023-01-06 14:36:43,713 - Epoch: [73][  246/  246]    Overall Loss 0.253498    Objective Loss 0.253498    Top1 90.191388    LR 0.000022    Time 0.026937    
2023-01-06 14:36:43,856 - --- validate (epoch=73)-----------
2023-01-06 14:36:43,856 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:44,305 - Epoch: [73][   10/   28]    Loss 0.270713    Top1 90.546875    
2023-01-06 14:36:44,454 - Epoch: [73][   20/   28]    Loss 0.274744    Top1 90.273438    
2023-01-06 14:36:44,546 - Epoch: [73][   28/   28]    Loss 0.280393    Top1 90.094475    
2023-01-06 14:36:44,681 - ==> Top1: 90.094    Loss: 0.280

2023-01-06 14:36:44,681 - ==> Confusion:
[[ 264   15  160]
 [  17  283  302]
 [  94  104 5747]]

2023-01-06 14:36:44,682 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 360896 on epoch: 72]
2023-01-06 14:36:44,683 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:44,692 - 

2023-01-06 14:36:44,693 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:45,294 - Epoch: [74][   10/  246]    Overall Loss 0.257994    Objective Loss 0.257994                                        LR 0.000022    Time 0.060095    
2023-01-06 14:36:45,547 - Epoch: [74][   20/  246]    Overall Loss 0.246877    Objective Loss 0.246877                                        LR 0.000022    Time 0.042678    
2023-01-06 14:36:45,804 - Epoch: [74][   30/  246]    Overall Loss 0.253248    Objective Loss 0.253248                                        LR 0.000022    Time 0.036993    
2023-01-06 14:36:46,059 - Epoch: [74][   40/  246]    Overall Loss 0.253781    Objective Loss 0.253781                                        LR 0.000022    Time 0.034119    
2023-01-06 14:36:46,311 - Epoch: [74][   50/  246]    Overall Loss 0.255127    Objective Loss 0.255127                                        LR 0.000022    Time 0.032327    
2023-01-06 14:36:46,574 - Epoch: [74][   60/  246]    Overall Loss 0.254048    Objective Loss 0.254048                                        LR 0.000022    Time 0.031305    
2023-01-06 14:36:46,830 - Epoch: [74][   70/  246]    Overall Loss 0.251265    Objective Loss 0.251265                                        LR 0.000022    Time 0.030491    
2023-01-06 14:36:47,086 - Epoch: [74][   80/  246]    Overall Loss 0.250874    Objective Loss 0.250874                                        LR 0.000022    Time 0.029862    
2023-01-06 14:36:47,338 - Epoch: [74][   90/  246]    Overall Loss 0.251630    Objective Loss 0.251630                                        LR 0.000022    Time 0.029345    
2023-01-06 14:36:47,590 - Epoch: [74][  100/  246]    Overall Loss 0.251617    Objective Loss 0.251617                                        LR 0.000022    Time 0.028922    
2023-01-06 14:36:47,841 - Epoch: [74][  110/  246]    Overall Loss 0.251675    Objective Loss 0.251675                                        LR 0.000022    Time 0.028572    
2023-01-06 14:36:48,093 - Epoch: [74][  120/  246]    Overall Loss 0.253387    Objective Loss 0.253387                                        LR 0.000022    Time 0.028285    
2023-01-06 14:36:48,349 - Epoch: [74][  130/  246]    Overall Loss 0.253036    Objective Loss 0.253036                                        LR 0.000022    Time 0.028074    
2023-01-06 14:36:48,609 - Epoch: [74][  140/  246]    Overall Loss 0.251930    Objective Loss 0.251930                                        LR 0.000022    Time 0.027925    
2023-01-06 14:36:48,865 - Epoch: [74][  150/  246]    Overall Loss 0.251729    Objective Loss 0.251729                                        LR 0.000022    Time 0.027768    
2023-01-06 14:36:49,118 - Epoch: [74][  160/  246]    Overall Loss 0.251330    Objective Loss 0.251330                                        LR 0.000022    Time 0.027617    
2023-01-06 14:36:49,364 - Epoch: [74][  170/  246]    Overall Loss 0.251538    Objective Loss 0.251538                                        LR 0.000022    Time 0.027436    
2023-01-06 14:36:49,616 - Epoch: [74][  180/  246]    Overall Loss 0.250172    Objective Loss 0.250172                                        LR 0.000022    Time 0.027308    
2023-01-06 14:36:49,864 - Epoch: [74][  190/  246]    Overall Loss 0.250746    Objective Loss 0.250746                                        LR 0.000022    Time 0.027174    
2023-01-06 14:36:50,115 - Epoch: [74][  200/  246]    Overall Loss 0.250672    Objective Loss 0.250672                                        LR 0.000022    Time 0.027071    
2023-01-06 14:36:50,364 - Epoch: [74][  210/  246]    Overall Loss 0.252073    Objective Loss 0.252073                                        LR 0.000022    Time 0.026968    
2023-01-06 14:36:50,615 - Epoch: [74][  220/  246]    Overall Loss 0.253120    Objective Loss 0.253120                                        LR 0.000022    Time 0.026879    
2023-01-06 14:36:50,861 - Epoch: [74][  230/  246]    Overall Loss 0.252463    Objective Loss 0.252463                                        LR 0.000022    Time 0.026780    
2023-01-06 14:36:51,122 - Epoch: [74][  240/  246]    Overall Loss 0.251955    Objective Loss 0.251955                                        LR 0.000022    Time 0.026749    
2023-01-06 14:36:51,252 - Epoch: [74][  246/  246]    Overall Loss 0.252166    Objective Loss 0.252166    Top1 90.909091    LR 0.000022    Time 0.026625    
2023-01-06 14:36:51,401 - --- validate (epoch=74)-----------
2023-01-06 14:36:51,401 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:51,843 - Epoch: [74][   10/   28]    Loss 0.254417    Top1 90.312500    
2023-01-06 14:36:51,985 - Epoch: [74][   20/   28]    Loss 0.255276    Top1 90.605469    
2023-01-06 14:36:52,076 - Epoch: [74][   28/   28]    Loss 0.267553    Top1 90.237618    
2023-01-06 14:36:52,210 - ==> Top1: 90.238    Loss: 0.268

2023-01-06 14:36:52,210 - ==> Confusion:
[[ 213   14  212]
 [   9  278  315]
 [  46   86 5813]]

2023-01-06 14:36:52,211 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 360896 on epoch: 72]
2023-01-06 14:36:52,212 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:52,221 - 

2023-01-06 14:36:52,222 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:36:52,923 - Epoch: [75][   10/  246]    Overall Loss 0.253971    Objective Loss 0.253971                                        LR 0.000022    Time 0.070109    
2023-01-06 14:36:53,161 - Epoch: [75][   20/  246]    Overall Loss 0.249251    Objective Loss 0.249251                                        LR 0.000022    Time 0.046892    
2023-01-06 14:36:53,407 - Epoch: [75][   30/  246]    Overall Loss 0.247899    Objective Loss 0.247899                                        LR 0.000022    Time 0.039465    
2023-01-06 14:36:53,650 - Epoch: [75][   40/  246]    Overall Loss 0.247370    Objective Loss 0.247370                                        LR 0.000022    Time 0.035672    
2023-01-06 14:36:53,898 - Epoch: [75][   50/  246]    Overall Loss 0.254125    Objective Loss 0.254125                                        LR 0.000022    Time 0.033490    
2023-01-06 14:36:54,148 - Epoch: [75][   60/  246]    Overall Loss 0.253789    Objective Loss 0.253789                                        LR 0.000022    Time 0.032055    
2023-01-06 14:36:54,400 - Epoch: [75][   70/  246]    Overall Loss 0.251928    Objective Loss 0.251928                                        LR 0.000022    Time 0.031081    
2023-01-06 14:36:54,649 - Epoch: [75][   80/  246]    Overall Loss 0.252635    Objective Loss 0.252635                                        LR 0.000022    Time 0.030296    
2023-01-06 14:36:54,898 - Epoch: [75][   90/  246]    Overall Loss 0.252536    Objective Loss 0.252536                                        LR 0.000022    Time 0.029700    
2023-01-06 14:36:55,148 - Epoch: [75][  100/  246]    Overall Loss 0.253543    Objective Loss 0.253543                                        LR 0.000022    Time 0.029220    
2023-01-06 14:36:55,397 - Epoch: [75][  110/  246]    Overall Loss 0.255026    Objective Loss 0.255026                                        LR 0.000022    Time 0.028826    
2023-01-06 14:36:55,647 - Epoch: [75][  120/  246]    Overall Loss 0.256219    Objective Loss 0.256219                                        LR 0.000022    Time 0.028498    
2023-01-06 14:36:55,897 - Epoch: [75][  130/  246]    Overall Loss 0.254824    Objective Loss 0.254824                                        LR 0.000022    Time 0.028224    
2023-01-06 14:36:56,146 - Epoch: [75][  140/  246]    Overall Loss 0.254311    Objective Loss 0.254311                                        LR 0.000022    Time 0.027988    
2023-01-06 14:36:56,395 - Epoch: [75][  150/  246]    Overall Loss 0.252000    Objective Loss 0.252000                                        LR 0.000022    Time 0.027778    
2023-01-06 14:36:56,645 - Epoch: [75][  160/  246]    Overall Loss 0.252203    Objective Loss 0.252203                                        LR 0.000022    Time 0.027600    
2023-01-06 14:36:56,894 - Epoch: [75][  170/  246]    Overall Loss 0.252755    Objective Loss 0.252755                                        LR 0.000022    Time 0.027441    
2023-01-06 14:36:57,144 - Epoch: [75][  180/  246]    Overall Loss 0.253574    Objective Loss 0.253574                                        LR 0.000022    Time 0.027300    
2023-01-06 14:36:57,393 - Epoch: [75][  190/  246]    Overall Loss 0.253274    Objective Loss 0.253274                                        LR 0.000022    Time 0.027173    
2023-01-06 14:36:57,643 - Epoch: [75][  200/  246]    Overall Loss 0.252821    Objective Loss 0.252821                                        LR 0.000022    Time 0.027060    
2023-01-06 14:36:57,893 - Epoch: [75][  210/  246]    Overall Loss 0.253404    Objective Loss 0.253404                                        LR 0.000022    Time 0.026958    
2023-01-06 14:36:58,142 - Epoch: [75][  220/  246]    Overall Loss 0.253149    Objective Loss 0.253149                                        LR 0.000022    Time 0.026864    
2023-01-06 14:36:58,392 - Epoch: [75][  230/  246]    Overall Loss 0.252992    Objective Loss 0.252992                                        LR 0.000022    Time 0.026781    
2023-01-06 14:36:58,654 - Epoch: [75][  240/  246]    Overall Loss 0.252711    Objective Loss 0.252711                                        LR 0.000022    Time 0.026753    
2023-01-06 14:36:58,782 - Epoch: [75][  246/  246]    Overall Loss 0.252486    Objective Loss 0.252486    Top1 90.909091    LR 0.000022    Time 0.026622    
2023-01-06 14:36:58,962 - --- validate (epoch=75)-----------
2023-01-06 14:36:58,962 - 6986 samples (256 per mini-batch)
2023-01-06 14:36:59,407 - Epoch: [75][   10/   28]    Loss 0.277747    Top1 89.648438    
2023-01-06 14:36:59,554 - Epoch: [75][   20/   28]    Loss 0.267884    Top1 90.273438    
2023-01-06 14:36:59,646 - Epoch: [75][   28/   28]    Loss 0.267686    Top1 90.452333    
2023-01-06 14:36:59,773 - ==> Top1: 90.452    Loss: 0.268

2023-01-06 14:36:59,774 - ==> Confusion:
[[ 245   17  177]
 [  13  284  305]
 [  71   84 5790]]

2023-01-06 14:36:59,775 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 360896 on epoch: 72]
2023-01-06 14:36:59,775 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:36:59,785 - 

2023-01-06 14:36:59,785 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:00,514 - Epoch: [76][   10/  246]    Overall Loss 0.253965    Objective Loss 0.253965                                        LR 0.000022    Time 0.072802    
2023-01-06 14:37:00,760 - Epoch: [76][   20/  246]    Overall Loss 0.254219    Objective Loss 0.254219                                        LR 0.000022    Time 0.048700    
2023-01-06 14:37:01,003 - Epoch: [76][   30/  246]    Overall Loss 0.255522    Objective Loss 0.255522                                        LR 0.000022    Time 0.040559    
2023-01-06 14:37:01,248 - Epoch: [76][   40/  246]    Overall Loss 0.252643    Objective Loss 0.252643                                        LR 0.000022    Time 0.036526    
2023-01-06 14:37:01,495 - Epoch: [76][   50/  246]    Overall Loss 0.253194    Objective Loss 0.253194                                        LR 0.000022    Time 0.034145    
2023-01-06 14:37:01,740 - Epoch: [76][   60/  246]    Overall Loss 0.252145    Objective Loss 0.252145                                        LR 0.000022    Time 0.032539    
2023-01-06 14:37:01,986 - Epoch: [76][   70/  246]    Overall Loss 0.254868    Objective Loss 0.254868                                        LR 0.000022    Time 0.031396    
2023-01-06 14:37:02,230 - Epoch: [76][   80/  246]    Overall Loss 0.255507    Objective Loss 0.255507                                        LR 0.000022    Time 0.030520    
2023-01-06 14:37:02,476 - Epoch: [76][   90/  246]    Overall Loss 0.255641    Objective Loss 0.255641                                        LR 0.000022    Time 0.029853    
2023-01-06 14:37:02,722 - Epoch: [76][  100/  246]    Overall Loss 0.255639    Objective Loss 0.255639                                        LR 0.000022    Time 0.029326    
2023-01-06 14:37:02,969 - Epoch: [76][  110/  246]    Overall Loss 0.254446    Objective Loss 0.254446                                        LR 0.000022    Time 0.028900    
2023-01-06 14:37:03,204 - Epoch: [76][  120/  246]    Overall Loss 0.254931    Objective Loss 0.254931                                        LR 0.000022    Time 0.028445    
2023-01-06 14:37:03,412 - Epoch: [76][  130/  246]    Overall Loss 0.254234    Objective Loss 0.254234                                        LR 0.000022    Time 0.027853    
2023-01-06 14:37:03,622 - Epoch: [76][  140/  246]    Overall Loss 0.252394    Objective Loss 0.252394                                        LR 0.000022    Time 0.027365    
2023-01-06 14:37:03,831 - Epoch: [76][  150/  246]    Overall Loss 0.252277    Objective Loss 0.252277                                        LR 0.000022    Time 0.026931    
2023-01-06 14:37:04,040 - Epoch: [76][  160/  246]    Overall Loss 0.251189    Objective Loss 0.251189                                        LR 0.000022    Time 0.026549    
2023-01-06 14:37:04,265 - Epoch: [76][  170/  246]    Overall Loss 0.250168    Objective Loss 0.250168                                        LR 0.000022    Time 0.026313    
2023-01-06 14:37:04,481 - Epoch: [76][  180/  246]    Overall Loss 0.251059    Objective Loss 0.251059                                        LR 0.000022    Time 0.026044    
2023-01-06 14:37:04,689 - Epoch: [76][  190/  246]    Overall Loss 0.251087    Objective Loss 0.251087                                        LR 0.000022    Time 0.025769    
2023-01-06 14:37:04,898 - Epoch: [76][  200/  246]    Overall Loss 0.250910    Objective Loss 0.250910                                        LR 0.000022    Time 0.025524    
2023-01-06 14:37:05,108 - Epoch: [76][  210/  246]    Overall Loss 0.250811    Objective Loss 0.250811                                        LR 0.000022    Time 0.025304    
2023-01-06 14:37:05,317 - Epoch: [76][  220/  246]    Overall Loss 0.250884    Objective Loss 0.250884                                        LR 0.000022    Time 0.025105    
2023-01-06 14:37:05,526 - Epoch: [76][  230/  246]    Overall Loss 0.250628    Objective Loss 0.250628                                        LR 0.000022    Time 0.024919    
2023-01-06 14:37:05,758 - Epoch: [76][  240/  246]    Overall Loss 0.250277    Objective Loss 0.250277                                        LR 0.000022    Time 0.024848    
2023-01-06 14:37:05,886 - Epoch: [76][  246/  246]    Overall Loss 0.250304    Objective Loss 0.250304    Top1 89.712919    LR 0.000022    Time 0.024761    
2023-01-06 14:37:06,021 - --- validate (epoch=76)-----------
2023-01-06 14:37:06,022 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:06,482 - Epoch: [76][   10/   28]    Loss 0.270100    Top1 90.390625    
2023-01-06 14:37:06,634 - Epoch: [76][   20/   28]    Loss 0.271355    Top1 90.449219    
2023-01-06 14:37:06,726 - Epoch: [76][   28/   28]    Loss 0.263593    Top1 90.609791    
2023-01-06 14:37:06,863 - ==> Top1: 90.610    Loss: 0.264

2023-01-06 14:37:06,864 - ==> Confusion:
[[ 227   12  200]
 [   7  288  307]
 [  56   74 5815]]

2023-01-06 14:37:06,866 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:06,866 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:06,887 - 

2023-01-06 14:37:06,887 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:07,469 - Epoch: [77][   10/  246]    Overall Loss 0.233096    Objective Loss 0.233096                                        LR 0.000022    Time 0.058035    
2023-01-06 14:37:07,720 - Epoch: [77][   20/  246]    Overall Loss 0.240113    Objective Loss 0.240113                                        LR 0.000022    Time 0.041546    
2023-01-06 14:37:07,961 - Epoch: [77][   30/  246]    Overall Loss 0.249866    Objective Loss 0.249866                                        LR 0.000022    Time 0.035700    
2023-01-06 14:37:08,208 - Epoch: [77][   40/  246]    Overall Loss 0.247315    Objective Loss 0.247315                                        LR 0.000022    Time 0.032946    
2023-01-06 14:37:08,450 - Epoch: [77][   50/  246]    Overall Loss 0.248823    Objective Loss 0.248823                                        LR 0.000022    Time 0.031181    
2023-01-06 14:37:08,699 - Epoch: [77][   60/  246]    Overall Loss 0.248638    Objective Loss 0.248638                                        LR 0.000022    Time 0.030120    
2023-01-06 14:37:08,950 - Epoch: [77][   70/  246]    Overall Loss 0.249228    Objective Loss 0.249228                                        LR 0.000022    Time 0.029399    
2023-01-06 14:37:09,197 - Epoch: [77][   80/  246]    Overall Loss 0.248354    Objective Loss 0.248354                                        LR 0.000022    Time 0.028808    
2023-01-06 14:37:09,443 - Epoch: [77][   90/  246]    Overall Loss 0.248947    Objective Loss 0.248947                                        LR 0.000022    Time 0.028338    
2023-01-06 14:37:09,689 - Epoch: [77][  100/  246]    Overall Loss 0.248277    Objective Loss 0.248277                                        LR 0.000022    Time 0.027957    
2023-01-06 14:37:09,940 - Epoch: [77][  110/  246]    Overall Loss 0.247429    Objective Loss 0.247429                                        LR 0.000022    Time 0.027697    
2023-01-06 14:37:10,194 - Epoch: [77][  120/  246]    Overall Loss 0.247588    Objective Loss 0.247588                                        LR 0.000022    Time 0.027501    
2023-01-06 14:37:10,445 - Epoch: [77][  130/  246]    Overall Loss 0.248813    Objective Loss 0.248813                                        LR 0.000022    Time 0.027312    
2023-01-06 14:37:10,700 - Epoch: [77][  140/  246]    Overall Loss 0.247994    Objective Loss 0.247994                                        LR 0.000022    Time 0.027180    
2023-01-06 14:37:10,957 - Epoch: [77][  150/  246]    Overall Loss 0.248325    Objective Loss 0.248325                                        LR 0.000022    Time 0.027074    
2023-01-06 14:37:11,215 - Epoch: [77][  160/  246]    Overall Loss 0.247902    Objective Loss 0.247902                                        LR 0.000022    Time 0.026993    
2023-01-06 14:37:11,470 - Epoch: [77][  170/  246]    Overall Loss 0.247897    Objective Loss 0.247897                                        LR 0.000022    Time 0.026900    
2023-01-06 14:37:11,724 - Epoch: [77][  180/  246]    Overall Loss 0.248117    Objective Loss 0.248117                                        LR 0.000022    Time 0.026815    
2023-01-06 14:37:11,977 - Epoch: [77][  190/  246]    Overall Loss 0.247328    Objective Loss 0.247328                                        LR 0.000022    Time 0.026734    
2023-01-06 14:37:12,230 - Epoch: [77][  200/  246]    Overall Loss 0.247272    Objective Loss 0.247272                                        LR 0.000022    Time 0.026661    
2023-01-06 14:37:12,492 - Epoch: [77][  210/  246]    Overall Loss 0.247810    Objective Loss 0.247810                                        LR 0.000022    Time 0.026636    
2023-01-06 14:37:12,751 - Epoch: [77][  220/  246]    Overall Loss 0.248127    Objective Loss 0.248127                                        LR 0.000022    Time 0.026601    
2023-01-06 14:37:13,008 - Epoch: [77][  230/  246]    Overall Loss 0.248738    Objective Loss 0.248738                                        LR 0.000022    Time 0.026555    
2023-01-06 14:37:13,271 - Epoch: [77][  240/  246]    Overall Loss 0.249168    Objective Loss 0.249168                                        LR 0.000022    Time 0.026541    
2023-01-06 14:37:13,399 - Epoch: [77][  246/  246]    Overall Loss 0.249242    Objective Loss 0.249242    Top1 88.516746    LR 0.000022    Time 0.026413    
2023-01-06 14:37:13,523 - --- validate (epoch=77)-----------
2023-01-06 14:37:13,524 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:13,966 - Epoch: [77][   10/   28]    Loss 0.269012    Top1 90.234375    
2023-01-06 14:37:14,108 - Epoch: [77][   20/   28]    Loss 0.266012    Top1 90.527344    
2023-01-06 14:37:14,200 - Epoch: [77][   28/   28]    Loss 0.267766    Top1 90.380762    
2023-01-06 14:37:14,325 - ==> Top1: 90.381    Loss: 0.268

2023-01-06 14:37:14,326 - ==> Confusion:
[[ 262    9  168]
 [  17  267  318]
 [  92   68 5785]]

2023-01-06 14:37:14,327 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:14,327 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:14,337 - 

2023-01-06 14:37:14,337 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:15,067 - Epoch: [78][   10/  246]    Overall Loss 0.240360    Objective Loss 0.240360                                        LR 0.000022    Time 0.072926    
2023-01-06 14:37:15,313 - Epoch: [78][   20/  246]    Overall Loss 0.243760    Objective Loss 0.243760                                        LR 0.000022    Time 0.048716    
2023-01-06 14:37:15,558 - Epoch: [78][   30/  246]    Overall Loss 0.245938    Objective Loss 0.245938                                        LR 0.000022    Time 0.040646    
2023-01-06 14:37:15,806 - Epoch: [78][   40/  246]    Overall Loss 0.245667    Objective Loss 0.245667                                        LR 0.000022    Time 0.036665    
2023-01-06 14:37:16,054 - Epoch: [78][   50/  246]    Overall Loss 0.242699    Objective Loss 0.242699                                        LR 0.000022    Time 0.034288    
2023-01-06 14:37:16,311 - Epoch: [78][   60/  246]    Overall Loss 0.242048    Objective Loss 0.242048                                        LR 0.000022    Time 0.032844    
2023-01-06 14:37:16,570 - Epoch: [78][   70/  246]    Overall Loss 0.242610    Objective Loss 0.242610                                        LR 0.000022    Time 0.031847    
2023-01-06 14:37:16,831 - Epoch: [78][   80/  246]    Overall Loss 0.243161    Objective Loss 0.243161                                        LR 0.000022    Time 0.031124    
2023-01-06 14:37:17,089 - Epoch: [78][   90/  246]    Overall Loss 0.243394    Objective Loss 0.243394                                        LR 0.000022    Time 0.030524    
2023-01-06 14:37:17,349 - Epoch: [78][  100/  246]    Overall Loss 0.244575    Objective Loss 0.244575                                        LR 0.000022    Time 0.030061    
2023-01-06 14:37:17,605 - Epoch: [78][  110/  246]    Overall Loss 0.244044    Objective Loss 0.244044                                        LR 0.000022    Time 0.029657    
2023-01-06 14:37:17,864 - Epoch: [78][  120/  246]    Overall Loss 0.244675    Objective Loss 0.244675                                        LR 0.000022    Time 0.029337    
2023-01-06 14:37:18,117 - Epoch: [78][  130/  246]    Overall Loss 0.247338    Objective Loss 0.247338                                        LR 0.000022    Time 0.029021    
2023-01-06 14:37:18,369 - Epoch: [78][  140/  246]    Overall Loss 0.248597    Objective Loss 0.248597                                        LR 0.000022    Time 0.028745    
2023-01-06 14:37:18,620 - Epoch: [78][  150/  246]    Overall Loss 0.247331    Objective Loss 0.247331                                        LR 0.000022    Time 0.028501    
2023-01-06 14:37:18,875 - Epoch: [78][  160/  246]    Overall Loss 0.246858    Objective Loss 0.246858                                        LR 0.000022    Time 0.028308    
2023-01-06 14:37:19,127 - Epoch: [78][  170/  246]    Overall Loss 0.246006    Objective Loss 0.246006                                        LR 0.000022    Time 0.028124    
2023-01-06 14:37:19,378 - Epoch: [78][  180/  246]    Overall Loss 0.246240    Objective Loss 0.246240                                        LR 0.000022    Time 0.027953    
2023-01-06 14:37:19,630 - Epoch: [78][  190/  246]    Overall Loss 0.245586    Objective Loss 0.245586                                        LR 0.000022    Time 0.027806    
2023-01-06 14:37:19,882 - Epoch: [78][  200/  246]    Overall Loss 0.246714    Objective Loss 0.246714                                        LR 0.000022    Time 0.027673    
2023-01-06 14:37:20,131 - Epoch: [78][  210/  246]    Overall Loss 0.247703    Objective Loss 0.247703                                        LR 0.000022    Time 0.027538    
2023-01-06 14:37:20,382 - Epoch: [78][  220/  246]    Overall Loss 0.247865    Objective Loss 0.247865                                        LR 0.000022    Time 0.027424    
2023-01-06 14:37:20,633 - Epoch: [78][  230/  246]    Overall Loss 0.247823    Objective Loss 0.247823                                        LR 0.000022    Time 0.027322    
2023-01-06 14:37:20,892 - Epoch: [78][  240/  246]    Overall Loss 0.248595    Objective Loss 0.248595                                        LR 0.000022    Time 0.027261    
2023-01-06 14:37:21,020 - Epoch: [78][  246/  246]    Overall Loss 0.248782    Objective Loss 0.248782    Top1 91.148325    LR 0.000022    Time 0.027115    
2023-01-06 14:37:21,154 - --- validate (epoch=78)-----------
2023-01-06 14:37:21,154 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:21,606 - Epoch: [78][   10/   28]    Loss 0.255941    Top1 90.781250    
2023-01-06 14:37:21,750 - Epoch: [78][   20/   28]    Loss 0.254463    Top1 90.644531    
2023-01-06 14:37:21,842 - Epoch: [78][   28/   28]    Loss 0.261195    Top1 90.509591    
2023-01-06 14:37:21,974 - ==> Top1: 90.510    Loss: 0.261

2023-01-06 14:37:21,974 - ==> Confusion:
[[ 237   12  190]
 [  10  269  323]
 [  56   72 5817]]

2023-01-06 14:37:21,976 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:21,976 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:21,986 - 

2023-01-06 14:37:21,986 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:22,737 - Epoch: [79][   10/  246]    Overall Loss 0.229548    Objective Loss 0.229548                                        LR 0.000022    Time 0.075083    
2023-01-06 14:37:22,984 - Epoch: [79][   20/  246]    Overall Loss 0.239446    Objective Loss 0.239446                                        LR 0.000022    Time 0.049863    
2023-01-06 14:37:23,237 - Epoch: [79][   30/  246]    Overall Loss 0.243737    Objective Loss 0.243737                                        LR 0.000022    Time 0.041651    
2023-01-06 14:37:23,482 - Epoch: [79][   40/  246]    Overall Loss 0.245867    Objective Loss 0.245867                                        LR 0.000022    Time 0.037345    
2023-01-06 14:37:23,725 - Epoch: [79][   50/  246]    Overall Loss 0.251436    Objective Loss 0.251436                                        LR 0.000022    Time 0.034738    
2023-01-06 14:37:23,971 - Epoch: [79][   60/  246]    Overall Loss 0.248699    Objective Loss 0.248699                                        LR 0.000022    Time 0.033040    
2023-01-06 14:37:24,215 - Epoch: [79][   70/  246]    Overall Loss 0.249758    Objective Loss 0.249758                                        LR 0.000022    Time 0.031791    
2023-01-06 14:37:24,460 - Epoch: [79][   80/  246]    Overall Loss 0.252465    Objective Loss 0.252465                                        LR 0.000022    Time 0.030871    
2023-01-06 14:37:24,705 - Epoch: [79][   90/  246]    Overall Loss 0.250246    Objective Loss 0.250246                                        LR 0.000022    Time 0.030158    
2023-01-06 14:37:24,947 - Epoch: [79][  100/  246]    Overall Loss 0.250121    Objective Loss 0.250121                                        LR 0.000022    Time 0.029563    
2023-01-06 14:37:25,191 - Epoch: [79][  110/  246]    Overall Loss 0.250293    Objective Loss 0.250293                                        LR 0.000022    Time 0.029086    
2023-01-06 14:37:25,433 - Epoch: [79][  120/  246]    Overall Loss 0.251230    Objective Loss 0.251230                                        LR 0.000022    Time 0.028671    
2023-01-06 14:37:25,675 - Epoch: [79][  130/  246]    Overall Loss 0.251184    Objective Loss 0.251184                                        LR 0.000022    Time 0.028323    
2023-01-06 14:37:25,920 - Epoch: [79][  140/  246]    Overall Loss 0.250746    Objective Loss 0.250746                                        LR 0.000022    Time 0.028048    
2023-01-06 14:37:26,177 - Epoch: [79][  150/  246]    Overall Loss 0.251038    Objective Loss 0.251038                                        LR 0.000022    Time 0.027889    
2023-01-06 14:37:26,433 - Epoch: [79][  160/  246]    Overall Loss 0.250804    Objective Loss 0.250804                                        LR 0.000022    Time 0.027740    
2023-01-06 14:37:26,691 - Epoch: [79][  170/  246]    Overall Loss 0.249989    Objective Loss 0.249989                                        LR 0.000022    Time 0.027624    
2023-01-06 14:37:26,947 - Epoch: [79][  180/  246]    Overall Loss 0.249754    Objective Loss 0.249754                                        LR 0.000022    Time 0.027512    
2023-01-06 14:37:27,205 - Epoch: [79][  190/  246]    Overall Loss 0.250070    Objective Loss 0.250070                                        LR 0.000022    Time 0.027419    
2023-01-06 14:37:27,460 - Epoch: [79][  200/  246]    Overall Loss 0.250521    Objective Loss 0.250521                                        LR 0.000022    Time 0.027319    
2023-01-06 14:37:27,720 - Epoch: [79][  210/  246]    Overall Loss 0.250181    Objective Loss 0.250181                                        LR 0.000022    Time 0.027256    
2023-01-06 14:37:27,975 - Epoch: [79][  220/  246]    Overall Loss 0.250792    Objective Loss 0.250792                                        LR 0.000022    Time 0.027172    
2023-01-06 14:37:28,236 - Epoch: [79][  230/  246]    Overall Loss 0.250312    Objective Loss 0.250312                                        LR 0.000022    Time 0.027124    
2023-01-06 14:37:28,501 - Epoch: [79][  240/  246]    Overall Loss 0.249800    Objective Loss 0.249800                                        LR 0.000022    Time 0.027097    
2023-01-06 14:37:28,631 - Epoch: [79][  246/  246]    Overall Loss 0.249595    Objective Loss 0.249595    Top1 91.626794    LR 0.000022    Time 0.026963    
2023-01-06 14:37:28,763 - --- validate (epoch=79)-----------
2023-01-06 14:37:28,763 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:29,210 - Epoch: [79][   10/   28]    Loss 0.274898    Top1 90.898438    
2023-01-06 14:37:29,347 - Epoch: [79][   20/   28]    Loss 0.277374    Top1 90.410156    
2023-01-06 14:37:29,438 - Epoch: [79][   28/   28]    Loss 0.272095    Top1 90.595477    
2023-01-06 14:37:29,576 - ==> Top1: 90.595    Loss: 0.272

2023-01-06 14:37:29,576 - ==> Confusion:
[[ 236   16  187]
 [  11  317  274]
 [  51  118 5776]]

2023-01-06 14:37:29,578 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:29,578 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:29,588 - 

2023-01-06 14:37:29,588 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:30,153 - Epoch: [80][   10/  246]    Overall Loss 0.235303    Objective Loss 0.235303                                        LR 0.000022    Time 0.056469    
2023-01-06 14:37:30,387 - Epoch: [80][   20/  246]    Overall Loss 0.244497    Objective Loss 0.244497                                        LR 0.000022    Time 0.039916    
2023-01-06 14:37:30,641 - Epoch: [80][   30/  246]    Overall Loss 0.249795    Objective Loss 0.249795                                        LR 0.000022    Time 0.035066    
2023-01-06 14:37:30,896 - Epoch: [80][   40/  246]    Overall Loss 0.249426    Objective Loss 0.249426                                        LR 0.000022    Time 0.032649    
2023-01-06 14:37:31,149 - Epoch: [80][   50/  246]    Overall Loss 0.252379    Objective Loss 0.252379                                        LR 0.000022    Time 0.031169    
2023-01-06 14:37:31,402 - Epoch: [80][   60/  246]    Overall Loss 0.255900    Objective Loss 0.255900                                        LR 0.000022    Time 0.030165    
2023-01-06 14:37:31,655 - Epoch: [80][   70/  246]    Overall Loss 0.253671    Objective Loss 0.253671                                        LR 0.000022    Time 0.029461    
2023-01-06 14:37:31,908 - Epoch: [80][   80/  246]    Overall Loss 0.253332    Objective Loss 0.253332                                        LR 0.000022    Time 0.028931    
2023-01-06 14:37:32,162 - Epoch: [80][   90/  246]    Overall Loss 0.252386    Objective Loss 0.252386                                        LR 0.000022    Time 0.028534    
2023-01-06 14:37:32,413 - Epoch: [80][  100/  246]    Overall Loss 0.251648    Objective Loss 0.251648                                        LR 0.000022    Time 0.028188    
2023-01-06 14:37:32,667 - Epoch: [80][  110/  246]    Overall Loss 0.251403    Objective Loss 0.251403                                        LR 0.000022    Time 0.027932    
2023-01-06 14:37:32,918 - Epoch: [80][  120/  246]    Overall Loss 0.251118    Objective Loss 0.251118                                        LR 0.000022    Time 0.027686    
2023-01-06 14:37:33,161 - Epoch: [80][  130/  246]    Overall Loss 0.250537    Objective Loss 0.250537                                        LR 0.000022    Time 0.027422    
2023-01-06 14:37:33,405 - Epoch: [80][  140/  246]    Overall Loss 0.249394    Objective Loss 0.249394                                        LR 0.000022    Time 0.027202    
2023-01-06 14:37:33,647 - Epoch: [80][  150/  246]    Overall Loss 0.249360    Objective Loss 0.249360                                        LR 0.000022    Time 0.027001    
2023-01-06 14:37:33,890 - Epoch: [80][  160/  246]    Overall Loss 0.248588    Objective Loss 0.248588                                        LR 0.000022    Time 0.026829    
2023-01-06 14:37:34,131 - Epoch: [80][  170/  246]    Overall Loss 0.248827    Objective Loss 0.248827                                        LR 0.000022    Time 0.026668    
2023-01-06 14:37:34,362 - Epoch: [80][  180/  246]    Overall Loss 0.250126    Objective Loss 0.250126                                        LR 0.000022    Time 0.026465    
2023-01-06 14:37:34,613 - Epoch: [80][  190/  246]    Overall Loss 0.249141    Objective Loss 0.249141                                        LR 0.000022    Time 0.026393    
2023-01-06 14:37:34,865 - Epoch: [80][  200/  246]    Overall Loss 0.249149    Objective Loss 0.249149                                        LR 0.000022    Time 0.026328    
2023-01-06 14:37:35,117 - Epoch: [80][  210/  246]    Overall Loss 0.249500    Objective Loss 0.249500                                        LR 0.000022    Time 0.026272    
2023-01-06 14:37:35,368 - Epoch: [80][  220/  246]    Overall Loss 0.249104    Objective Loss 0.249104                                        LR 0.000022    Time 0.026217    
2023-01-06 14:37:35,620 - Epoch: [80][  230/  246]    Overall Loss 0.248716    Objective Loss 0.248716                                        LR 0.000022    Time 0.026172    
2023-01-06 14:37:35,884 - Epoch: [80][  240/  246]    Overall Loss 0.248156    Objective Loss 0.248156                                        LR 0.000022    Time 0.026177    
2023-01-06 14:37:36,015 - Epoch: [80][  246/  246]    Overall Loss 0.248486    Objective Loss 0.248486    Top1 89.473684    LR 0.000022    Time 0.026070    
2023-01-06 14:37:36,142 - --- validate (epoch=80)-----------
2023-01-06 14:37:36,142 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:36,604 - Epoch: [80][   10/   28]    Loss 0.272992    Top1 90.156250    
2023-01-06 14:37:36,764 - Epoch: [80][   20/   28]    Loss 0.265349    Top1 90.449219    
2023-01-06 14:37:36,854 - Epoch: [80][   28/   28]    Loss 0.260429    Top1 90.566848    
2023-01-06 14:37:36,979 - ==> Top1: 90.567    Loss: 0.260

2023-01-06 14:37:36,979 - ==> Confusion:
[[ 256   12  171]
 [  14  295  293]
 [  83   86 5776]]

2023-01-06 14:37:36,980 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:36,980 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:36,991 - 

2023-01-06 14:37:36,991 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:37,714 - Epoch: [81][   10/  246]    Overall Loss 0.243708    Objective Loss 0.243708                                        LR 0.000022    Time 0.072287    
2023-01-06 14:37:37,963 - Epoch: [81][   20/  246]    Overall Loss 0.246888    Objective Loss 0.246888                                        LR 0.000022    Time 0.048569    
2023-01-06 14:37:38,226 - Epoch: [81][   30/  246]    Overall Loss 0.248555    Objective Loss 0.248555                                        LR 0.000022    Time 0.041127    
2023-01-06 14:37:38,490 - Epoch: [81][   40/  246]    Overall Loss 0.248760    Objective Loss 0.248760                                        LR 0.000022    Time 0.037438    
2023-01-06 14:37:38,743 - Epoch: [81][   50/  246]    Overall Loss 0.250385    Objective Loss 0.250385                                        LR 0.000022    Time 0.035005    
2023-01-06 14:37:38,999 - Epoch: [81][   60/  246]    Overall Loss 0.251051    Objective Loss 0.251051                                        LR 0.000022    Time 0.033402    
2023-01-06 14:37:39,245 - Epoch: [81][   70/  246]    Overall Loss 0.254126    Objective Loss 0.254126                                        LR 0.000022    Time 0.032141    
2023-01-06 14:37:39,510 - Epoch: [81][   80/  246]    Overall Loss 0.251639    Objective Loss 0.251639                                        LR 0.000022    Time 0.031422    
2023-01-06 14:37:39,768 - Epoch: [81][   90/  246]    Overall Loss 0.252522    Objective Loss 0.252522                                        LR 0.000022    Time 0.030793    
2023-01-06 14:37:40,027 - Epoch: [81][  100/  246]    Overall Loss 0.250763    Objective Loss 0.250763                                        LR 0.000022    Time 0.030302    
2023-01-06 14:37:40,278 - Epoch: [81][  110/  246]    Overall Loss 0.249288    Objective Loss 0.249288                                        LR 0.000022    Time 0.029820    
2023-01-06 14:37:40,533 - Epoch: [81][  120/  246]    Overall Loss 0.249821    Objective Loss 0.249821                                        LR 0.000022    Time 0.029456    
2023-01-06 14:37:40,787 - Epoch: [81][  130/  246]    Overall Loss 0.250956    Objective Loss 0.250956                                        LR 0.000022    Time 0.029139    
2023-01-06 14:37:41,041 - Epoch: [81][  140/  246]    Overall Loss 0.251067    Objective Loss 0.251067                                        LR 0.000022    Time 0.028873    
2023-01-06 14:37:41,290 - Epoch: [81][  150/  246]    Overall Loss 0.250693    Objective Loss 0.250693                                        LR 0.000022    Time 0.028602    
2023-01-06 14:37:41,549 - Epoch: [81][  160/  246]    Overall Loss 0.251836    Objective Loss 0.251836                                        LR 0.000022    Time 0.028433    
2023-01-06 14:37:41,797 - Epoch: [81][  170/  246]    Overall Loss 0.250595    Objective Loss 0.250595                                        LR 0.000022    Time 0.028212    
2023-01-06 14:37:42,048 - Epoch: [81][  180/  246]    Overall Loss 0.250023    Objective Loss 0.250023                                        LR 0.000022    Time 0.028040    
2023-01-06 14:37:42,296 - Epoch: [81][  190/  246]    Overall Loss 0.250812    Objective Loss 0.250812                                        LR 0.000022    Time 0.027864    
2023-01-06 14:37:42,548 - Epoch: [81][  200/  246]    Overall Loss 0.250849    Objective Loss 0.250849                                        LR 0.000022    Time 0.027728    
2023-01-06 14:37:42,791 - Epoch: [81][  210/  246]    Overall Loss 0.249656    Objective Loss 0.249656                                        LR 0.000022    Time 0.027563    
2023-01-06 14:37:43,036 - Epoch: [81][  220/  246]    Overall Loss 0.250534    Objective Loss 0.250534                                        LR 0.000022    Time 0.027421    
2023-01-06 14:37:43,277 - Epoch: [81][  230/  246]    Overall Loss 0.249836    Objective Loss 0.249836                                        LR 0.000022    Time 0.027276    
2023-01-06 14:37:43,534 - Epoch: [81][  240/  246]    Overall Loss 0.249346    Objective Loss 0.249346                                        LR 0.000022    Time 0.027211    
2023-01-06 14:37:43,666 - Epoch: [81][  246/  246]    Overall Loss 0.248767    Objective Loss 0.248767    Top1 92.105263    LR 0.000022    Time 0.027083    
2023-01-06 14:37:43,783 - --- validate (epoch=81)-----------
2023-01-06 14:37:43,783 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:44,233 - Epoch: [81][   10/   28]    Loss 0.280255    Top1 90.390625    
2023-01-06 14:37:44,373 - Epoch: [81][   20/   28]    Loss 0.273390    Top1 90.449219    
2023-01-06 14:37:44,464 - Epoch: [81][   28/   28]    Loss 0.263043    Top1 90.581162    
2023-01-06 14:37:44,603 - ==> Top1: 90.581    Loss: 0.263

2023-01-06 14:37:44,603 - ==> Confusion:
[[ 253   11  175]
 [  17  286  299]
 [  71   85 5789]]

2023-01-06 14:37:44,605 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 76]
2023-01-06 14:37:44,605 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:44,614 - 

2023-01-06 14:37:44,615 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:45,209 - Epoch: [82][   10/  246]    Overall Loss 0.251479    Objective Loss 0.251479                                        LR 0.000022    Time 0.059321    
2023-01-06 14:37:45,441 - Epoch: [82][   20/  246]    Overall Loss 0.249238    Objective Loss 0.249238                                        LR 0.000022    Time 0.041252    
2023-01-06 14:37:45,681 - Epoch: [82][   30/  246]    Overall Loss 0.254170    Objective Loss 0.254170                                        LR 0.000022    Time 0.035494    
2023-01-06 14:37:45,921 - Epoch: [82][   40/  246]    Overall Loss 0.251431    Objective Loss 0.251431                                        LR 0.000022    Time 0.032557    
2023-01-06 14:37:46,156 - Epoch: [82][   50/  246]    Overall Loss 0.253531    Objective Loss 0.253531                                        LR 0.000022    Time 0.030740    
2023-01-06 14:37:46,392 - Epoch: [82][   60/  246]    Overall Loss 0.251862    Objective Loss 0.251862                                        LR 0.000022    Time 0.029551    
2023-01-06 14:37:46,636 - Epoch: [82][   70/  246]    Overall Loss 0.249576    Objective Loss 0.249576                                        LR 0.000022    Time 0.028801    
2023-01-06 14:37:46,872 - Epoch: [82][   80/  246]    Overall Loss 0.249207    Objective Loss 0.249207                                        LR 0.000022    Time 0.028149    
2023-01-06 14:37:47,114 - Epoch: [82][   90/  246]    Overall Loss 0.247766    Objective Loss 0.247766                                        LR 0.000022    Time 0.027707    
2023-01-06 14:37:47,355 - Epoch: [82][  100/  246]    Overall Loss 0.247884    Objective Loss 0.247884                                        LR 0.000022    Time 0.027326    
2023-01-06 14:37:47,596 - Epoch: [82][  110/  246]    Overall Loss 0.249053    Objective Loss 0.249053                                        LR 0.000022    Time 0.027015    
2023-01-06 14:37:47,838 - Epoch: [82][  120/  246]    Overall Loss 0.249608    Objective Loss 0.249608                                        LR 0.000022    Time 0.026763    
2023-01-06 14:37:48,085 - Epoch: [82][  130/  246]    Overall Loss 0.250098    Objective Loss 0.250098                                        LR 0.000022    Time 0.026597    
2023-01-06 14:37:48,324 - Epoch: [82][  140/  246]    Overall Loss 0.251214    Objective Loss 0.251214                                        LR 0.000022    Time 0.026403    
2023-01-06 14:37:48,565 - Epoch: [82][  150/  246]    Overall Loss 0.250329    Objective Loss 0.250329                                        LR 0.000022    Time 0.026245    
2023-01-06 14:37:48,806 - Epoch: [82][  160/  246]    Overall Loss 0.248939    Objective Loss 0.248939                                        LR 0.000022    Time 0.026106    
2023-01-06 14:37:49,031 - Epoch: [82][  170/  246]    Overall Loss 0.248570    Objective Loss 0.248570                                        LR 0.000022    Time 0.025896    
2023-01-06 14:37:49,257 - Epoch: [82][  180/  246]    Overall Loss 0.247492    Objective Loss 0.247492                                        LR 0.000022    Time 0.025706    
2023-01-06 14:37:49,499 - Epoch: [82][  190/  246]    Overall Loss 0.248063    Objective Loss 0.248063                                        LR 0.000022    Time 0.025627    
2023-01-06 14:37:49,738 - Epoch: [82][  200/  246]    Overall Loss 0.248782    Objective Loss 0.248782                                        LR 0.000022    Time 0.025537    
2023-01-06 14:37:49,963 - Epoch: [82][  210/  246]    Overall Loss 0.248600    Objective Loss 0.248600                                        LR 0.000022    Time 0.025391    
2023-01-06 14:37:50,206 - Epoch: [82][  220/  246]    Overall Loss 0.247982    Objective Loss 0.247982                                        LR 0.000022    Time 0.025338    
2023-01-06 14:37:50,450 - Epoch: [82][  230/  246]    Overall Loss 0.248065    Objective Loss 0.248065                                        LR 0.000022    Time 0.025296    
2023-01-06 14:37:50,696 - Epoch: [82][  240/  246]    Overall Loss 0.247981    Objective Loss 0.247981                                        LR 0.000022    Time 0.025266    
2023-01-06 14:37:50,822 - Epoch: [82][  246/  246]    Overall Loss 0.247902    Objective Loss 0.247902    Top1 92.105263    LR 0.000022    Time 0.025163    
2023-01-06 14:37:50,975 - --- validate (epoch=82)-----------
2023-01-06 14:37:50,975 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:51,424 - Epoch: [82][   10/   28]    Loss 0.260515    Top1 90.742188    
2023-01-06 14:37:51,562 - Epoch: [82][   20/   28]    Loss 0.264597    Top1 90.664062    
2023-01-06 14:37:51,654 - Epoch: [82][   28/   28]    Loss 0.267703    Top1 90.609791    
2023-01-06 14:37:51,790 - ==> Top1: 90.610    Loss: 0.268

2023-01-06 14:37:51,791 - ==> Confusion:
[[ 240   17  182]
 [   9  320  273]
 [  61  114 5770]]

2023-01-06 14:37:51,792 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 82]
2023-01-06 14:37:51,792 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:51,813 - 

2023-01-06 14:37:51,813 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:37:52,513 - Epoch: [83][   10/  246]    Overall Loss 0.236209    Objective Loss 0.236209                                        LR 0.000022    Time 0.069847    
2023-01-06 14:37:52,730 - Epoch: [83][   20/  246]    Overall Loss 0.231810    Objective Loss 0.231810                                        LR 0.000022    Time 0.045778    
2023-01-06 14:37:52,971 - Epoch: [83][   30/  246]    Overall Loss 0.243090    Objective Loss 0.243090                                        LR 0.000022    Time 0.038530    
2023-01-06 14:37:53,214 - Epoch: [83][   40/  246]    Overall Loss 0.249201    Objective Loss 0.249201                                        LR 0.000022    Time 0.034952    
2023-01-06 14:37:53,463 - Epoch: [83][   50/  246]    Overall Loss 0.246965    Objective Loss 0.246965                                        LR 0.000022    Time 0.032945    
2023-01-06 14:37:53,709 - Epoch: [83][   60/  246]    Overall Loss 0.248947    Objective Loss 0.248947                                        LR 0.000022    Time 0.031551    
2023-01-06 14:37:53,955 - Epoch: [83][   70/  246]    Overall Loss 0.246461    Objective Loss 0.246461                                        LR 0.000022    Time 0.030541    
2023-01-06 14:37:54,200 - Epoch: [83][   80/  246]    Overall Loss 0.246532    Objective Loss 0.246532                                        LR 0.000022    Time 0.029788    
2023-01-06 14:37:54,444 - Epoch: [83][   90/  246]    Overall Loss 0.245342    Objective Loss 0.245342                                        LR 0.000022    Time 0.029183    
2023-01-06 14:37:54,689 - Epoch: [83][  100/  246]    Overall Loss 0.248788    Objective Loss 0.248788                                        LR 0.000022    Time 0.028707    
2023-01-06 14:37:54,908 - Epoch: [83][  110/  246]    Overall Loss 0.248725    Objective Loss 0.248725                                        LR 0.000022    Time 0.028088    
2023-01-06 14:37:55,166 - Epoch: [83][  120/  246]    Overall Loss 0.248912    Objective Loss 0.248912                                        LR 0.000022    Time 0.027899    
2023-01-06 14:37:55,429 - Epoch: [83][  130/  246]    Overall Loss 0.248643    Objective Loss 0.248643                                        LR 0.000022    Time 0.027772    
2023-01-06 14:37:55,682 - Epoch: [83][  140/  246]    Overall Loss 0.248382    Objective Loss 0.248382                                        LR 0.000022    Time 0.027589    
2023-01-06 14:37:55,943 - Epoch: [83][  150/  246]    Overall Loss 0.248275    Objective Loss 0.248275                                        LR 0.000022    Time 0.027488    
2023-01-06 14:37:56,193 - Epoch: [83][  160/  246]    Overall Loss 0.248385    Objective Loss 0.248385                                        LR 0.000022    Time 0.027325    
2023-01-06 14:37:56,436 - Epoch: [83][  170/  246]    Overall Loss 0.247373    Objective Loss 0.247373                                        LR 0.000022    Time 0.027148    
2023-01-06 14:37:56,679 - Epoch: [83][  180/  246]    Overall Loss 0.246489    Objective Loss 0.246489                                        LR 0.000022    Time 0.026988    
2023-01-06 14:37:56,921 - Epoch: [83][  190/  246]    Overall Loss 0.247408    Objective Loss 0.247408                                        LR 0.000022    Time 0.026841    
2023-01-06 14:37:57,166 - Epoch: [83][  200/  246]    Overall Loss 0.246966    Objective Loss 0.246966                                        LR 0.000022    Time 0.026721    
2023-01-06 14:37:57,408 - Epoch: [83][  210/  246]    Overall Loss 0.246920    Objective Loss 0.246920                                        LR 0.000022    Time 0.026597    
2023-01-06 14:37:57,649 - Epoch: [83][  220/  246]    Overall Loss 0.248224    Objective Loss 0.248224                                        LR 0.000022    Time 0.026485    
2023-01-06 14:37:57,892 - Epoch: [83][  230/  246]    Overall Loss 0.248001    Objective Loss 0.248001                                        LR 0.000022    Time 0.026385    
2023-01-06 14:37:58,147 - Epoch: [83][  240/  246]    Overall Loss 0.247929    Objective Loss 0.247929                                        LR 0.000022    Time 0.026348    
2023-01-06 14:37:58,279 - Epoch: [83][  246/  246]    Overall Loss 0.248051    Objective Loss 0.248051    Top1 90.191388    LR 0.000022    Time 0.026242    
2023-01-06 14:37:58,422 - --- validate (epoch=83)-----------
2023-01-06 14:37:58,423 - 6986 samples (256 per mini-batch)
2023-01-06 14:37:58,883 - Epoch: [83][   10/   28]    Loss 0.268060    Top1 90.390625    
2023-01-06 14:37:59,043 - Epoch: [83][   20/   28]    Loss 0.267035    Top1 90.468750    
2023-01-06 14:37:59,133 - Epoch: [83][   28/   28]    Loss 0.267526    Top1 90.452333    
2023-01-06 14:37:59,272 - ==> Top1: 90.452    Loss: 0.268

2023-01-06 14:37:59,272 - ==> Confusion:
[[ 221   14  204]
 [  12  279  311]
 [  46   80 5819]]

2023-01-06 14:37:59,274 - ==> Best [Top1: 90.610   Sparsity:0.00   Params: 360896 on epoch: 82]
2023-01-06 14:37:59,274 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:37:59,284 - 

2023-01-06 14:37:59,284 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:00,021 - Epoch: [84][   10/  246]    Overall Loss 0.244611    Objective Loss 0.244611                                        LR 0.000022    Time 0.073640    
2023-01-06 14:38:00,268 - Epoch: [84][   20/  246]    Overall Loss 0.234953    Objective Loss 0.234953                                        LR 0.000022    Time 0.049149    
2023-01-06 14:38:00,523 - Epoch: [84][   30/  246]    Overall Loss 0.234207    Objective Loss 0.234207                                        LR 0.000022    Time 0.041246    
2023-01-06 14:38:00,745 - Epoch: [84][   40/  246]    Overall Loss 0.241693    Objective Loss 0.241693                                        LR 0.000022    Time 0.036474    
2023-01-06 14:38:00,970 - Epoch: [84][   50/  246]    Overall Loss 0.244522    Objective Loss 0.244522                                        LR 0.000022    Time 0.033677    
2023-01-06 14:38:01,192 - Epoch: [84][   60/  246]    Overall Loss 0.245699    Objective Loss 0.245699                                        LR 0.000022    Time 0.031752    
2023-01-06 14:38:01,422 - Epoch: [84][   70/  246]    Overall Loss 0.245662    Objective Loss 0.245662                                        LR 0.000022    Time 0.030505    
2023-01-06 14:38:01,645 - Epoch: [84][   80/  246]    Overall Loss 0.246638    Objective Loss 0.246638                                        LR 0.000022    Time 0.029466    
2023-01-06 14:38:01,871 - Epoch: [84][   90/  246]    Overall Loss 0.246137    Objective Loss 0.246137                                        LR 0.000022    Time 0.028700    
2023-01-06 14:38:02,092 - Epoch: [84][  100/  246]    Overall Loss 0.245455    Objective Loss 0.245455                                        LR 0.000022    Time 0.028035    
2023-01-06 14:38:02,315 - Epoch: [84][  110/  246]    Overall Loss 0.245963    Objective Loss 0.245963                                        LR 0.000022    Time 0.027518    
2023-01-06 14:38:02,537 - Epoch: [84][  120/  246]    Overall Loss 0.246655    Objective Loss 0.246655                                        LR 0.000022    Time 0.027066    
2023-01-06 14:38:02,771 - Epoch: [84][  130/  246]    Overall Loss 0.246314    Objective Loss 0.246314                                        LR 0.000022    Time 0.026783    
2023-01-06 14:38:03,008 - Epoch: [84][  140/  246]    Overall Loss 0.244956    Objective Loss 0.244956                                        LR 0.000022    Time 0.026562    
2023-01-06 14:38:03,242 - Epoch: [84][  150/  246]    Overall Loss 0.244576    Objective Loss 0.244576                                        LR 0.000022    Time 0.026347    
2023-01-06 14:38:03,483 - Epoch: [84][  160/  246]    Overall Loss 0.244525    Objective Loss 0.244525                                        LR 0.000022    Time 0.026205    
2023-01-06 14:38:03,720 - Epoch: [84][  170/  246]    Overall Loss 0.245931    Objective Loss 0.245931                                        LR 0.000022    Time 0.026059    
2023-01-06 14:38:03,956 - Epoch: [84][  180/  246]    Overall Loss 0.246624    Objective Loss 0.246624                                        LR 0.000022    Time 0.025919    
2023-01-06 14:38:04,192 - Epoch: [84][  190/  246]    Overall Loss 0.247258    Objective Loss 0.247258                                        LR 0.000022    Time 0.025794    
2023-01-06 14:38:04,424 - Epoch: [84][  200/  246]    Overall Loss 0.246784    Objective Loss 0.246784                                        LR 0.000022    Time 0.025666    
2023-01-06 14:38:04,656 - Epoch: [84][  210/  246]    Overall Loss 0.246740    Objective Loss 0.246740                                        LR 0.000022    Time 0.025543    
2023-01-06 14:38:04,888 - Epoch: [84][  220/  246]    Overall Loss 0.247425    Objective Loss 0.247425                                        LR 0.000022    Time 0.025438    
2023-01-06 14:38:05,125 - Epoch: [84][  230/  246]    Overall Loss 0.247931    Objective Loss 0.247931                                        LR 0.000022    Time 0.025360    
2023-01-06 14:38:05,379 - Epoch: [84][  240/  246]    Overall Loss 0.247629    Objective Loss 0.247629                                        LR 0.000022    Time 0.025359    
2023-01-06 14:38:05,513 - Epoch: [84][  246/  246]    Overall Loss 0.247671    Objective Loss 0.247671    Top1 92.105263    LR 0.000022    Time 0.025284    
2023-01-06 14:38:05,646 - --- validate (epoch=84)-----------
2023-01-06 14:38:05,646 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:06,097 - Epoch: [84][   10/   28]    Loss 0.280972    Top1 89.921875    
2023-01-06 14:38:06,238 - Epoch: [84][   20/   28]    Loss 0.261580    Top1 90.546875    
2023-01-06 14:38:06,330 - Epoch: [84][   28/   28]    Loss 0.260738    Top1 90.709991    
2023-01-06 14:38:06,471 - ==> Top1: 90.710    Loss: 0.261

2023-01-06 14:38:06,471 - ==> Confusion:
[[ 239   12  188]
 [   9  287  306]
 [  51   83 5811]]

2023-01-06 14:38:06,472 - ==> Best [Top1: 90.710   Sparsity:0.00   Params: 360896 on epoch: 84]
2023-01-06 14:38:06,472 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:06,497 - 

2023-01-06 14:38:06,497 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:07,066 - Epoch: [85][   10/  246]    Overall Loss 0.229332    Objective Loss 0.229332                                        LR 0.000022    Time 0.056836    
2023-01-06 14:38:07,288 - Epoch: [85][   20/  246]    Overall Loss 0.229850    Objective Loss 0.229850                                        LR 0.000022    Time 0.039496    
2023-01-06 14:38:07,506 - Epoch: [85][   30/  246]    Overall Loss 0.243439    Objective Loss 0.243439                                        LR 0.000022    Time 0.033554    
2023-01-06 14:38:07,722 - Epoch: [85][   40/  246]    Overall Loss 0.239135    Objective Loss 0.239135                                        LR 0.000022    Time 0.030572    
2023-01-06 14:38:07,938 - Epoch: [85][   50/  246]    Overall Loss 0.240682    Objective Loss 0.240682                                        LR 0.000022    Time 0.028774    
2023-01-06 14:38:08,154 - Epoch: [85][   60/  246]    Overall Loss 0.239889    Objective Loss 0.239889                                        LR 0.000022    Time 0.027573    
2023-01-06 14:38:08,373 - Epoch: [85][   70/  246]    Overall Loss 0.240424    Objective Loss 0.240424                                        LR 0.000022    Time 0.026752    
2023-01-06 14:38:08,592 - Epoch: [85][   80/  246]    Overall Loss 0.239231    Objective Loss 0.239231                                        LR 0.000022    Time 0.026139    
2023-01-06 14:38:08,811 - Epoch: [85][   90/  246]    Overall Loss 0.240159    Objective Loss 0.240159                                        LR 0.000022    Time 0.025662    
2023-01-06 14:38:09,049 - Epoch: [85][  100/  246]    Overall Loss 0.241534    Objective Loss 0.241534                                        LR 0.000022    Time 0.025473    
2023-01-06 14:38:09,282 - Epoch: [85][  110/  246]    Overall Loss 0.241532    Objective Loss 0.241532                                        LR 0.000022    Time 0.025255    
2023-01-06 14:38:09,522 - Epoch: [85][  120/  246]    Overall Loss 0.242067    Objective Loss 0.242067                                        LR 0.000022    Time 0.025145    
2023-01-06 14:38:09,758 - Epoch: [85][  130/  246]    Overall Loss 0.244093    Objective Loss 0.244093                                        LR 0.000022    Time 0.025026    
2023-01-06 14:38:09,992 - Epoch: [85][  140/  246]    Overall Loss 0.244871    Objective Loss 0.244871                                        LR 0.000022    Time 0.024905    
2023-01-06 14:38:10,247 - Epoch: [85][  150/  246]    Overall Loss 0.244947    Objective Loss 0.244947                                        LR 0.000022    Time 0.024942    
2023-01-06 14:38:10,499 - Epoch: [85][  160/  246]    Overall Loss 0.246441    Objective Loss 0.246441                                        LR 0.000022    Time 0.024956    
2023-01-06 14:38:10,754 - Epoch: [85][  170/  246]    Overall Loss 0.246278    Objective Loss 0.246278                                        LR 0.000022    Time 0.024976    
2023-01-06 14:38:11,007 - Epoch: [85][  180/  246]    Overall Loss 0.246334    Objective Loss 0.246334                                        LR 0.000022    Time 0.024993    
2023-01-06 14:38:11,260 - Epoch: [85][  190/  246]    Overall Loss 0.246214    Objective Loss 0.246214                                        LR 0.000022    Time 0.025000    
2023-01-06 14:38:11,514 - Epoch: [85][  200/  246]    Overall Loss 0.245572    Objective Loss 0.245572                                        LR 0.000022    Time 0.025018    
2023-01-06 14:38:11,771 - Epoch: [85][  210/  246]    Overall Loss 0.244519    Objective Loss 0.244519                                        LR 0.000022    Time 0.025041    
2023-01-06 14:38:12,020 - Epoch: [85][  220/  246]    Overall Loss 0.244806    Objective Loss 0.244806                                        LR 0.000022    Time 0.025033    
2023-01-06 14:38:12,270 - Epoch: [85][  230/  246]    Overall Loss 0.245184    Objective Loss 0.245184                                        LR 0.000022    Time 0.025028    
2023-01-06 14:38:12,525 - Epoch: [85][  240/  246]    Overall Loss 0.245056    Objective Loss 0.245056                                        LR 0.000022    Time 0.025046    
2023-01-06 14:38:12,647 - Epoch: [85][  246/  246]    Overall Loss 0.245417    Objective Loss 0.245417    Top1 89.952153    LR 0.000022    Time 0.024933    
2023-01-06 14:38:12,763 - --- validate (epoch=85)-----------
2023-01-06 14:38:12,763 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:13,203 - Epoch: [85][   10/   28]    Loss 0.260177    Top1 90.468750    
2023-01-06 14:38:13,345 - Epoch: [85][   20/   28]    Loss 0.259153    Top1 90.488281    
2023-01-06 14:38:13,437 - Epoch: [85][   28/   28]    Loss 0.265566    Top1 90.294875    
2023-01-06 14:38:13,582 - ==> Top1: 90.295    Loss: 0.266

2023-01-06 14:38:13,582 - ==> Confusion:
[[ 221   17  201]
 [   8  270  324]
 [  54   74 5817]]

2023-01-06 14:38:13,583 - ==> Best [Top1: 90.710   Sparsity:0.00   Params: 360896 on epoch: 84]
2023-01-06 14:38:13,584 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:13,593 - 

2023-01-06 14:38:13,593 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:14,290 - Epoch: [86][   10/  246]    Overall Loss 0.251913    Objective Loss 0.251913                                        LR 0.000022    Time 0.069537    
2023-01-06 14:38:14,528 - Epoch: [86][   20/  246]    Overall Loss 0.253947    Objective Loss 0.253947                                        LR 0.000022    Time 0.046678    
2023-01-06 14:38:14,788 - Epoch: [86][   30/  246]    Overall Loss 0.246843    Objective Loss 0.246843                                        LR 0.000022    Time 0.039754    
2023-01-06 14:38:15,042 - Epoch: [86][   40/  246]    Overall Loss 0.246284    Objective Loss 0.246284                                        LR 0.000022    Time 0.036166    
2023-01-06 14:38:15,302 - Epoch: [86][   50/  246]    Overall Loss 0.247611    Objective Loss 0.247611                                        LR 0.000022    Time 0.034099    
2023-01-06 14:38:15,553 - Epoch: [86][   60/  246]    Overall Loss 0.244963    Objective Loss 0.244963                                        LR 0.000022    Time 0.032588    
2023-01-06 14:38:15,784 - Epoch: [86][   70/  246]    Overall Loss 0.247040    Objective Loss 0.247040                                        LR 0.000022    Time 0.031224    
2023-01-06 14:38:16,001 - Epoch: [86][   80/  246]    Overall Loss 0.247389    Objective Loss 0.247389                                        LR 0.000022    Time 0.030035    
2023-01-06 14:38:16,257 - Epoch: [86][   90/  246]    Overall Loss 0.244928    Objective Loss 0.244928                                        LR 0.000022    Time 0.029538    
2023-01-06 14:38:16,520 - Epoch: [86][  100/  246]    Overall Loss 0.245228    Objective Loss 0.245228                                        LR 0.000022    Time 0.029211    
2023-01-06 14:38:16,784 - Epoch: [86][  110/  246]    Overall Loss 0.246101    Objective Loss 0.246101                                        LR 0.000022    Time 0.028947    
2023-01-06 14:38:17,047 - Epoch: [86][  120/  246]    Overall Loss 0.247109    Objective Loss 0.247109                                        LR 0.000022    Time 0.028725    
2023-01-06 14:38:17,309 - Epoch: [86][  130/  246]    Overall Loss 0.247881    Objective Loss 0.247881                                        LR 0.000022    Time 0.028525    
2023-01-06 14:38:17,573 - Epoch: [86][  140/  246]    Overall Loss 0.246898    Objective Loss 0.246898                                        LR 0.000022    Time 0.028370    
2023-01-06 14:38:17,836 - Epoch: [86][  150/  246]    Overall Loss 0.247092    Objective Loss 0.247092                                        LR 0.000022    Time 0.028231    
2023-01-06 14:38:18,102 - Epoch: [86][  160/  246]    Overall Loss 0.247200    Objective Loss 0.247200                                        LR 0.000022    Time 0.028124    
2023-01-06 14:38:18,369 - Epoch: [86][  170/  246]    Overall Loss 0.247867    Objective Loss 0.247867                                        LR 0.000022    Time 0.028038    
2023-01-06 14:38:18,638 - Epoch: [86][  180/  246]    Overall Loss 0.248007    Objective Loss 0.248007                                        LR 0.000022    Time 0.027970    
2023-01-06 14:38:18,905 - Epoch: [86][  190/  246]    Overall Loss 0.247359    Objective Loss 0.247359                                        LR 0.000022    Time 0.027904    
2023-01-06 14:38:19,161 - Epoch: [86][  200/  246]    Overall Loss 0.246463    Objective Loss 0.246463                                        LR 0.000022    Time 0.027786    
2023-01-06 14:38:19,413 - Epoch: [86][  210/  246]    Overall Loss 0.246050    Objective Loss 0.246050                                        LR 0.000022    Time 0.027652    
2023-01-06 14:38:19,662 - Epoch: [86][  220/  246]    Overall Loss 0.246224    Objective Loss 0.246224                                        LR 0.000022    Time 0.027524    
2023-01-06 14:38:19,909 - Epoch: [86][  230/  246]    Overall Loss 0.247293    Objective Loss 0.247293                                        LR 0.000022    Time 0.027398    
2023-01-06 14:38:20,165 - Epoch: [86][  240/  246]    Overall Loss 0.246670    Objective Loss 0.246670                                        LR 0.000022    Time 0.027323    
2023-01-06 14:38:20,297 - Epoch: [86][  246/  246]    Overall Loss 0.246975    Objective Loss 0.246975    Top1 89.712919    LR 0.000022    Time 0.027190    
2023-01-06 14:38:20,429 - --- validate (epoch=86)-----------
2023-01-06 14:38:20,429 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:20,889 - Epoch: [86][   10/   28]    Loss 0.269140    Top1 89.843750    
2023-01-06 14:38:21,051 - Epoch: [86][   20/   28]    Loss 0.268452    Top1 90.449219    
2023-01-06 14:38:21,141 - Epoch: [86][   28/   28]    Loss 0.264753    Top1 90.423705    
2023-01-06 14:38:21,268 - ==> Top1: 90.424    Loss: 0.265

2023-01-06 14:38:21,268 - ==> Confusion:
[[ 212   18  209]
 [   4  271  327]
 [  47   64 5834]]

2023-01-06 14:38:21,270 - ==> Best [Top1: 90.710   Sparsity:0.00   Params: 360896 on epoch: 84]
2023-01-06 14:38:21,270 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:21,279 - 

2023-01-06 14:38:21,280 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:21,847 - Epoch: [87][   10/  246]    Overall Loss 0.254713    Objective Loss 0.254713                                        LR 0.000022    Time 0.056679    
2023-01-06 14:38:22,053 - Epoch: [87][   20/  246]    Overall Loss 0.242935    Objective Loss 0.242935                                        LR 0.000022    Time 0.038630    
2023-01-06 14:38:22,260 - Epoch: [87][   30/  246]    Overall Loss 0.242686    Objective Loss 0.242686                                        LR 0.000022    Time 0.032626    
2023-01-06 14:38:22,471 - Epoch: [87][   40/  246]    Overall Loss 0.245759    Objective Loss 0.245759                                        LR 0.000022    Time 0.029731    
2023-01-06 14:38:22,680 - Epoch: [87][   50/  246]    Overall Loss 0.246042    Objective Loss 0.246042                                        LR 0.000022    Time 0.027970    
2023-01-06 14:38:22,891 - Epoch: [87][   60/  246]    Overall Loss 0.246477    Objective Loss 0.246477                                        LR 0.000022    Time 0.026818    
2023-01-06 14:38:23,106 - Epoch: [87][   70/  246]    Overall Loss 0.246688    Objective Loss 0.246688                                        LR 0.000022    Time 0.026047    
2023-01-06 14:38:23,326 - Epoch: [87][   80/  246]    Overall Loss 0.249067    Objective Loss 0.249067                                        LR 0.000022    Time 0.025535    
2023-01-06 14:38:23,551 - Epoch: [87][   90/  246]    Overall Loss 0.248742    Objective Loss 0.248742                                        LR 0.000022    Time 0.025198    
2023-01-06 14:38:23,775 - Epoch: [87][  100/  246]    Overall Loss 0.246371    Objective Loss 0.246371                                        LR 0.000022    Time 0.024907    
2023-01-06 14:38:23,995 - Epoch: [87][  110/  246]    Overall Loss 0.248030    Objective Loss 0.248030                                        LR 0.000022    Time 0.024646    
2023-01-06 14:38:24,210 - Epoch: [87][  120/  246]    Overall Loss 0.248505    Objective Loss 0.248505                                        LR 0.000022    Time 0.024375    
2023-01-06 14:38:24,426 - Epoch: [87][  130/  246]    Overall Loss 0.245904    Objective Loss 0.245904                                        LR 0.000022    Time 0.024160    
2023-01-06 14:38:24,636 - Epoch: [87][  140/  246]    Overall Loss 0.246514    Objective Loss 0.246514                                        LR 0.000022    Time 0.023935    
2023-01-06 14:38:24,846 - Epoch: [87][  150/  246]    Overall Loss 0.246282    Objective Loss 0.246282                                        LR 0.000022    Time 0.023736    
2023-01-06 14:38:25,057 - Epoch: [87][  160/  246]    Overall Loss 0.246078    Objective Loss 0.246078                                        LR 0.000022    Time 0.023570    
2023-01-06 14:38:25,272 - Epoch: [87][  170/  246]    Overall Loss 0.245784    Objective Loss 0.245784                                        LR 0.000022    Time 0.023447    
2023-01-06 14:38:25,482 - Epoch: [87][  180/  246]    Overall Loss 0.245950    Objective Loss 0.245950                                        LR 0.000022    Time 0.023304    
2023-01-06 14:38:25,691 - Epoch: [87][  190/  246]    Overall Loss 0.247050    Objective Loss 0.247050                                        LR 0.000022    Time 0.023180    
2023-01-06 14:38:25,902 - Epoch: [87][  200/  246]    Overall Loss 0.246467    Objective Loss 0.246467                                        LR 0.000022    Time 0.023074    
2023-01-06 14:38:26,112 - Epoch: [87][  210/  246]    Overall Loss 0.245662    Objective Loss 0.245662                                        LR 0.000022    Time 0.022974    
2023-01-06 14:38:26,323 - Epoch: [87][  220/  246]    Overall Loss 0.245398    Objective Loss 0.245398                                        LR 0.000022    Time 0.022884    
2023-01-06 14:38:26,533 - Epoch: [87][  230/  246]    Overall Loss 0.245629    Objective Loss 0.245629                                        LR 0.000022    Time 0.022801    
2023-01-06 14:38:26,758 - Epoch: [87][  240/  246]    Overall Loss 0.245898    Objective Loss 0.245898                                        LR 0.000022    Time 0.022786    
2023-01-06 14:38:26,879 - Epoch: [87][  246/  246]    Overall Loss 0.245273    Objective Loss 0.245273    Top1 93.301435    LR 0.000022    Time 0.022721    
2023-01-06 14:38:27,021 - --- validate (epoch=87)-----------
2023-01-06 14:38:27,021 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:27,617 - Epoch: [87][   10/   28]    Loss 0.271791    Top1 90.468750    
2023-01-06 14:38:27,780 - Epoch: [87][   20/   28]    Loss 0.268406    Top1 90.703125    
2023-01-06 14:38:27,870 - Epoch: [87][   28/   28]    Loss 0.275877    Top1 90.438019    
2023-01-06 14:38:28,009 - ==> Top1: 90.438    Loss: 0.276

2023-01-06 14:38:28,009 - ==> Confusion:
[[ 254   12  173]
 [  17  260  325]
 [  80   61 5804]]

2023-01-06 14:38:28,011 - ==> Best [Top1: 90.710   Sparsity:0.00   Params: 360896 on epoch: 84]
2023-01-06 14:38:28,011 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:28,021 - 

2023-01-06 14:38:28,021 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:28,618 - Epoch: [88][   10/  246]    Overall Loss 0.235593    Objective Loss 0.235593                                        LR 0.000022    Time 0.059675    
2023-01-06 14:38:28,868 - Epoch: [88][   20/  246]    Overall Loss 0.247880    Objective Loss 0.247880                                        LR 0.000022    Time 0.042275    
2023-01-06 14:38:29,124 - Epoch: [88][   30/  246]    Overall Loss 0.255842    Objective Loss 0.255842                                        LR 0.000022    Time 0.036703    
2023-01-06 14:38:29,381 - Epoch: [88][   40/  246]    Overall Loss 0.251793    Objective Loss 0.251793                                        LR 0.000022    Time 0.033940    
2023-01-06 14:38:29,637 - Epoch: [88][   50/  246]    Overall Loss 0.248736    Objective Loss 0.248736                                        LR 0.000022    Time 0.032256    
2023-01-06 14:38:29,892 - Epoch: [88][   60/  246]    Overall Loss 0.247354    Objective Loss 0.247354                                        LR 0.000022    Time 0.031119    
2023-01-06 14:38:30,146 - Epoch: [88][   70/  246]    Overall Loss 0.242837    Objective Loss 0.242837                                        LR 0.000022    Time 0.030294    
2023-01-06 14:38:30,400 - Epoch: [88][   80/  246]    Overall Loss 0.240082    Objective Loss 0.240082                                        LR 0.000022    Time 0.029683    
2023-01-06 14:38:30,653 - Epoch: [88][   90/  246]    Overall Loss 0.240806    Objective Loss 0.240806                                        LR 0.000022    Time 0.029192    
2023-01-06 14:38:30,908 - Epoch: [88][  100/  246]    Overall Loss 0.241620    Objective Loss 0.241620                                        LR 0.000022    Time 0.028814    
2023-01-06 14:38:31,162 - Epoch: [88][  110/  246]    Overall Loss 0.241182    Objective Loss 0.241182                                        LR 0.000022    Time 0.028493    
2023-01-06 14:38:31,419 - Epoch: [88][  120/  246]    Overall Loss 0.240322    Objective Loss 0.240322                                        LR 0.000022    Time 0.028258    
2023-01-06 14:38:31,677 - Epoch: [88][  130/  246]    Overall Loss 0.239240    Objective Loss 0.239240                                        LR 0.000022    Time 0.028068    
2023-01-06 14:38:31,931 - Epoch: [88][  140/  246]    Overall Loss 0.240378    Objective Loss 0.240378                                        LR 0.000022    Time 0.027873    
2023-01-06 14:38:32,188 - Epoch: [88][  150/  246]    Overall Loss 0.240530    Objective Loss 0.240530                                        LR 0.000022    Time 0.027722    
2023-01-06 14:38:32,441 - Epoch: [88][  160/  246]    Overall Loss 0.241459    Objective Loss 0.241459                                        LR 0.000022    Time 0.027568    
2023-01-06 14:38:32,693 - Epoch: [88][  170/  246]    Overall Loss 0.242493    Objective Loss 0.242493                                        LR 0.000022    Time 0.027427    
2023-01-06 14:38:32,951 - Epoch: [88][  180/  246]    Overall Loss 0.242018    Objective Loss 0.242018                                        LR 0.000022    Time 0.027329    
2023-01-06 14:38:33,201 - Epoch: [88][  190/  246]    Overall Loss 0.242441    Objective Loss 0.242441                                        LR 0.000022    Time 0.027204    
2023-01-06 14:38:33,467 - Epoch: [88][  200/  246]    Overall Loss 0.243563    Objective Loss 0.243563                                        LR 0.000022    Time 0.027172    
2023-01-06 14:38:33,727 - Epoch: [88][  210/  246]    Overall Loss 0.243430    Objective Loss 0.243430                                        LR 0.000022    Time 0.027115    
2023-01-06 14:38:33,988 - Epoch: [88][  220/  246]    Overall Loss 0.243780    Objective Loss 0.243780                                        LR 0.000022    Time 0.027068    
2023-01-06 14:38:34,250 - Epoch: [88][  230/  246]    Overall Loss 0.244306    Objective Loss 0.244306                                        LR 0.000022    Time 0.027017    
2023-01-06 14:38:34,524 - Epoch: [88][  240/  246]    Overall Loss 0.244340    Objective Loss 0.244340                                        LR 0.000022    Time 0.027032    
2023-01-06 14:38:34,657 - Epoch: [88][  246/  246]    Overall Loss 0.244858    Objective Loss 0.244858    Top1 88.277512    LR 0.000022    Time 0.026912    
2023-01-06 14:38:34,792 - --- validate (epoch=88)-----------
2023-01-06 14:38:34,792 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:35,236 - Epoch: [88][   10/   28]    Loss 0.265078    Top1 90.664062    
2023-01-06 14:38:35,378 - Epoch: [88][   20/   28]    Loss 0.260324    Top1 90.742188    
2023-01-06 14:38:35,470 - Epoch: [88][   28/   28]    Loss 0.268166    Top1 90.509591    
2023-01-06 14:38:35,598 - ==> Top1: 90.510    Loss: 0.268

2023-01-06 14:38:35,599 - ==> Confusion:
[[ 253    9  177]
 [  15  283  304]
 [  75   83 5787]]

2023-01-06 14:38:35,600 - ==> Best [Top1: 90.710   Sparsity:0.00   Params: 360896 on epoch: 84]
2023-01-06 14:38:35,600 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:35,610 - 

2023-01-06 14:38:35,610 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:36,335 - Epoch: [89][   10/  246]    Overall Loss 0.252755    Objective Loss 0.252755                                        LR 0.000022    Time 0.072360    
2023-01-06 14:38:36,583 - Epoch: [89][   20/  246]    Overall Loss 0.246823    Objective Loss 0.246823                                        LR 0.000022    Time 0.048581    
2023-01-06 14:38:36,839 - Epoch: [89][   30/  246]    Overall Loss 0.250939    Objective Loss 0.250939                                        LR 0.000022    Time 0.040892    
2023-01-06 14:38:37,095 - Epoch: [89][   40/  246]    Overall Loss 0.247708    Objective Loss 0.247708                                        LR 0.000022    Time 0.037071    
2023-01-06 14:38:37,345 - Epoch: [89][   50/  246]    Overall Loss 0.248086    Objective Loss 0.248086                                        LR 0.000022    Time 0.034651    
2023-01-06 14:38:37,596 - Epoch: [89][   60/  246]    Overall Loss 0.243706    Objective Loss 0.243706                                        LR 0.000022    Time 0.033043    
2023-01-06 14:38:37,847 - Epoch: [89][   70/  246]    Overall Loss 0.241885    Objective Loss 0.241885                                        LR 0.000022    Time 0.031897    
2023-01-06 14:38:38,099 - Epoch: [89][   80/  246]    Overall Loss 0.241350    Objective Loss 0.241350                                        LR 0.000022    Time 0.031055    
2023-01-06 14:38:38,356 - Epoch: [89][   90/  246]    Overall Loss 0.244372    Objective Loss 0.244372                                        LR 0.000022    Time 0.030443    
2023-01-06 14:38:38,587 - Epoch: [89][  100/  246]    Overall Loss 0.244722    Objective Loss 0.244722                                        LR 0.000022    Time 0.029697    
2023-01-06 14:38:38,807 - Epoch: [89][  110/  246]    Overall Loss 0.244868    Objective Loss 0.244868                                        LR 0.000022    Time 0.028994    
2023-01-06 14:38:39,027 - Epoch: [89][  120/  246]    Overall Loss 0.244788    Objective Loss 0.244788                                        LR 0.000022    Time 0.028408    
2023-01-06 14:38:39,253 - Epoch: [89][  130/  246]    Overall Loss 0.245775    Objective Loss 0.245775                                        LR 0.000022    Time 0.027962    
2023-01-06 14:38:39,463 - Epoch: [89][  140/  246]    Overall Loss 0.245368    Objective Loss 0.245368                                        LR 0.000022    Time 0.027462    
2023-01-06 14:38:39,674 - Epoch: [89][  150/  246]    Overall Loss 0.245170    Objective Loss 0.245170                                        LR 0.000022    Time 0.027034    
2023-01-06 14:38:39,886 - Epoch: [89][  160/  246]    Overall Loss 0.245422    Objective Loss 0.245422                                        LR 0.000022    Time 0.026671    
2023-01-06 14:38:40,121 - Epoch: [89][  170/  246]    Overall Loss 0.244951    Objective Loss 0.244951                                        LR 0.000022    Time 0.026477    
2023-01-06 14:38:40,387 - Epoch: [89][  180/  246]    Overall Loss 0.245778    Objective Loss 0.245778                                        LR 0.000022    Time 0.026476    
2023-01-06 14:38:40,651 - Epoch: [89][  190/  246]    Overall Loss 0.246697    Objective Loss 0.246697                                        LR 0.000022    Time 0.026457    
2023-01-06 14:38:40,919 - Epoch: [89][  200/  246]    Overall Loss 0.246221    Objective Loss 0.246221                                        LR 0.000022    Time 0.026462    
2023-01-06 14:38:41,182 - Epoch: [89][  210/  246]    Overall Loss 0.246029    Objective Loss 0.246029                                        LR 0.000022    Time 0.026447    
2023-01-06 14:38:41,449 - Epoch: [89][  220/  246]    Overall Loss 0.246861    Objective Loss 0.246861                                        LR 0.000022    Time 0.026447    
2023-01-06 14:38:41,706 - Epoch: [89][  230/  246]    Overall Loss 0.246444    Objective Loss 0.246444                                        LR 0.000022    Time 0.026411    
2023-01-06 14:38:41,971 - Epoch: [89][  240/  246]    Overall Loss 0.245958    Objective Loss 0.245958                                        LR 0.000022    Time 0.026415    
2023-01-06 14:38:42,103 - Epoch: [89][  246/  246]    Overall Loss 0.245529    Objective Loss 0.245529    Top1 92.344498    LR 0.000022    Time 0.026308    
2023-01-06 14:38:42,244 - --- validate (epoch=89)-----------
2023-01-06 14:38:42,244 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:42,703 - Epoch: [89][   10/   28]    Loss 0.263196    Top1 91.250000    
2023-01-06 14:38:42,859 - Epoch: [89][   20/   28]    Loss 0.267653    Top1 91.035156    
2023-01-06 14:38:42,949 - Epoch: [89][   28/   28]    Loss 0.267988    Top1 90.738620    
2023-01-06 14:38:43,082 - ==> Top1: 90.739    Loss: 0.268

2023-01-06 14:38:43,082 - ==> Confusion:
[[ 266    9  164]
 [  18  272  312]
 [  75   69 5801]]

2023-01-06 14:38:43,084 - ==> Best [Top1: 90.739   Sparsity:0.00   Params: 360896 on epoch: 89]
2023-01-06 14:38:43,084 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:43,105 - 

2023-01-06 14:38:43,105 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:43,686 - Epoch: [90][   10/  246]    Overall Loss 0.243660    Objective Loss 0.243660                                        LR 0.000022    Time 0.058067    
2023-01-06 14:38:43,901 - Epoch: [90][   20/  246]    Overall Loss 0.253567    Objective Loss 0.253567                                        LR 0.000022    Time 0.039742    
2023-01-06 14:38:44,124 - Epoch: [90][   30/  246]    Overall Loss 0.250798    Objective Loss 0.250798                                        LR 0.000022    Time 0.033917    
2023-01-06 14:38:44,370 - Epoch: [90][   40/  246]    Overall Loss 0.242348    Objective Loss 0.242348                                        LR 0.000022    Time 0.031570    
2023-01-06 14:38:44,589 - Epoch: [90][   50/  246]    Overall Loss 0.240481    Objective Loss 0.240481                                        LR 0.000022    Time 0.029595    
2023-01-06 14:38:44,802 - Epoch: [90][   60/  246]    Overall Loss 0.235502    Objective Loss 0.235502                                        LR 0.000022    Time 0.028217    
2023-01-06 14:38:45,018 - Epoch: [90][   70/  246]    Overall Loss 0.238828    Objective Loss 0.238828                                        LR 0.000022    Time 0.027269    
2023-01-06 14:38:45,233 - Epoch: [90][   80/  246]    Overall Loss 0.240149    Objective Loss 0.240149                                        LR 0.000022    Time 0.026533    
2023-01-06 14:38:45,450 - Epoch: [90][   90/  246]    Overall Loss 0.239652    Objective Loss 0.239652                                        LR 0.000022    Time 0.026003    
2023-01-06 14:38:45,664 - Epoch: [90][  100/  246]    Overall Loss 0.241587    Objective Loss 0.241587                                        LR 0.000022    Time 0.025531    
2023-01-06 14:38:45,883 - Epoch: [90][  110/  246]    Overall Loss 0.240708    Objective Loss 0.240708                                        LR 0.000022    Time 0.025203    
2023-01-06 14:38:46,098 - Epoch: [90][  120/  246]    Overall Loss 0.241039    Objective Loss 0.241039                                        LR 0.000022    Time 0.024888    
2023-01-06 14:38:46,317 - Epoch: [90][  130/  246]    Overall Loss 0.241483    Objective Loss 0.241483                                        LR 0.000022    Time 0.024658    
2023-01-06 14:38:46,542 - Epoch: [90][  140/  246]    Overall Loss 0.242916    Objective Loss 0.242916                                        LR 0.000022    Time 0.024499    
2023-01-06 14:38:46,782 - Epoch: [90][  150/  246]    Overall Loss 0.242955    Objective Loss 0.242955                                        LR 0.000022    Time 0.024464    
2023-01-06 14:38:47,008 - Epoch: [90][  160/  246]    Overall Loss 0.244134    Objective Loss 0.244134                                        LR 0.000022    Time 0.024346    
2023-01-06 14:38:47,239 - Epoch: [90][  170/  246]    Overall Loss 0.244347    Objective Loss 0.244347                                        LR 0.000022    Time 0.024274    
2023-01-06 14:38:47,470 - Epoch: [90][  180/  246]    Overall Loss 0.244506    Objective Loss 0.244506                                        LR 0.000022    Time 0.024202    
2023-01-06 14:38:47,697 - Epoch: [90][  190/  246]    Overall Loss 0.244058    Objective Loss 0.244058                                        LR 0.000022    Time 0.024124    
2023-01-06 14:38:47,926 - Epoch: [90][  200/  246]    Overall Loss 0.243428    Objective Loss 0.243428                                        LR 0.000022    Time 0.024060    
2023-01-06 14:38:48,176 - Epoch: [90][  210/  246]    Overall Loss 0.243581    Objective Loss 0.243581                                        LR 0.000022    Time 0.024101    
2023-01-06 14:38:48,426 - Epoch: [90][  220/  246]    Overall Loss 0.244428    Objective Loss 0.244428                                        LR 0.000022    Time 0.024135    
2023-01-06 14:38:48,672 - Epoch: [90][  230/  246]    Overall Loss 0.244373    Objective Loss 0.244373                                        LR 0.000022    Time 0.024154    
2023-01-06 14:38:48,930 - Epoch: [90][  240/  246]    Overall Loss 0.244783    Objective Loss 0.244783                                        LR 0.000022    Time 0.024222    
2023-01-06 14:38:49,059 - Epoch: [90][  246/  246]    Overall Loss 0.243999    Objective Loss 0.243999    Top1 93.301435    LR 0.000022    Time 0.024151    
2023-01-06 14:38:49,190 - --- validate (epoch=90)-----------
2023-01-06 14:38:49,190 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:49,655 - Epoch: [90][   10/   28]    Loss 0.292497    Top1 90.234375    
2023-01-06 14:38:49,796 - Epoch: [90][   20/   28]    Loss 0.272891    Top1 90.957031    
2023-01-06 14:38:49,888 - Epoch: [90][   28/   28]    Loss 0.271606    Top1 90.695677    
2023-01-06 14:38:50,022 - ==> Top1: 90.696    Loss: 0.272

2023-01-06 14:38:50,022 - ==> Confusion:
[[ 232    9  198]
 [  14  253  335]
 [  46   48 5851]]

2023-01-06 14:38:50,024 - ==> Best [Top1: 90.739   Sparsity:0.00   Params: 360896 on epoch: 89]
2023-01-06 14:38:50,024 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:50,033 - 

2023-01-06 14:38:50,034 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:50,799 - Epoch: [91][   10/  246]    Overall Loss 0.247379    Objective Loss 0.247379                                        LR 0.000022    Time 0.076439    
2023-01-06 14:38:51,054 - Epoch: [91][   20/  246]    Overall Loss 0.238364    Objective Loss 0.238364                                        LR 0.000022    Time 0.050966    
2023-01-06 14:38:51,316 - Epoch: [91][   30/  246]    Overall Loss 0.239423    Objective Loss 0.239423                                        LR 0.000022    Time 0.042700    
2023-01-06 14:38:51,579 - Epoch: [91][   40/  246]    Overall Loss 0.240239    Objective Loss 0.240239                                        LR 0.000022    Time 0.038591    
2023-01-06 14:38:51,843 - Epoch: [91][   50/  246]    Overall Loss 0.241846    Objective Loss 0.241846                                        LR 0.000022    Time 0.036141    
2023-01-06 14:38:52,105 - Epoch: [91][   60/  246]    Overall Loss 0.242325    Objective Loss 0.242325                                        LR 0.000022    Time 0.034489    
2023-01-06 14:38:52,367 - Epoch: [91][   70/  246]    Overall Loss 0.238306    Objective Loss 0.238306                                        LR 0.000022    Time 0.033296    
2023-01-06 14:38:52,621 - Epoch: [91][   80/  246]    Overall Loss 0.236952    Objective Loss 0.236952                                        LR 0.000022    Time 0.032298    
2023-01-06 14:38:52,873 - Epoch: [91][   90/  246]    Overall Loss 0.236859    Objective Loss 0.236859                                        LR 0.000022    Time 0.031510    
2023-01-06 14:38:53,123 - Epoch: [91][  100/  246]    Overall Loss 0.238215    Objective Loss 0.238215                                        LR 0.000022    Time 0.030855    
2023-01-06 14:38:53,374 - Epoch: [91][  110/  246]    Overall Loss 0.239247    Objective Loss 0.239247                                        LR 0.000022    Time 0.030328    
2023-01-06 14:38:53,621 - Epoch: [91][  120/  246]    Overall Loss 0.239418    Objective Loss 0.239418                                        LR 0.000022    Time 0.029857    
2023-01-06 14:38:53,871 - Epoch: [91][  130/  246]    Overall Loss 0.239110    Objective Loss 0.239110                                        LR 0.000022    Time 0.029482    
2023-01-06 14:38:54,113 - Epoch: [91][  140/  246]    Overall Loss 0.238687    Objective Loss 0.238687                                        LR 0.000022    Time 0.029105    
2023-01-06 14:38:54,364 - Epoch: [91][  150/  246]    Overall Loss 0.238538    Objective Loss 0.238538                                        LR 0.000022    Time 0.028835    
2023-01-06 14:38:54,613 - Epoch: [91][  160/  246]    Overall Loss 0.239597    Objective Loss 0.239597                                        LR 0.000022    Time 0.028586    
2023-01-06 14:38:54,864 - Epoch: [91][  170/  246]    Overall Loss 0.241450    Objective Loss 0.241450                                        LR 0.000022    Time 0.028380    
2023-01-06 14:38:55,099 - Epoch: [91][  180/  246]    Overall Loss 0.242632    Objective Loss 0.242632                                        LR 0.000022    Time 0.028107    
2023-01-06 14:38:55,349 - Epoch: [91][  190/  246]    Overall Loss 0.243727    Objective Loss 0.243727                                        LR 0.000022    Time 0.027942    
2023-01-06 14:38:55,596 - Epoch: [91][  200/  246]    Overall Loss 0.244369    Objective Loss 0.244369                                        LR 0.000022    Time 0.027780    
2023-01-06 14:38:55,847 - Epoch: [91][  210/  246]    Overall Loss 0.245170    Objective Loss 0.245170                                        LR 0.000022    Time 0.027650    
2023-01-06 14:38:56,096 - Epoch: [91][  220/  246]    Overall Loss 0.245349    Objective Loss 0.245349                                        LR 0.000022    Time 0.027521    
2023-01-06 14:38:56,347 - Epoch: [91][  230/  246]    Overall Loss 0.244512    Objective Loss 0.244512                                        LR 0.000022    Time 0.027414    
2023-01-06 14:38:56,608 - Epoch: [91][  240/  246]    Overall Loss 0.244591    Objective Loss 0.244591                                        LR 0.000022    Time 0.027359    
2023-01-06 14:38:56,736 - Epoch: [91][  246/  246]    Overall Loss 0.244342    Objective Loss 0.244342    Top1 91.626794    LR 0.000022    Time 0.027212    
2023-01-06 14:38:56,860 - --- validate (epoch=91)-----------
2023-01-06 14:38:56,860 - 6986 samples (256 per mini-batch)
2023-01-06 14:38:57,318 - Epoch: [91][   10/   28]    Loss 0.252181    Top1 90.664062    
2023-01-06 14:38:57,458 - Epoch: [91][   20/   28]    Loss 0.249713    Top1 90.839844    
2023-01-06 14:38:57,550 - Epoch: [91][   28/   28]    Loss 0.258295    Top1 90.395076    
2023-01-06 14:38:57,695 - ==> Top1: 90.395    Loss: 0.258

2023-01-06 14:38:57,695 - ==> Confusion:
[[ 222   18  199]
 [  15  280  307]
 [  59   73 5813]]

2023-01-06 14:38:57,696 - ==> Best [Top1: 90.739   Sparsity:0.00   Params: 360896 on epoch: 89]
2023-01-06 14:38:57,696 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:38:57,706 - 

2023-01-06 14:38:57,706 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:38:58,469 - Epoch: [92][   10/  246]    Overall Loss 0.237817    Objective Loss 0.237817                                        LR 0.000022    Time 0.076218    
2023-01-06 14:38:58,728 - Epoch: [92][   20/  246]    Overall Loss 0.247218    Objective Loss 0.247218                                        LR 0.000022    Time 0.051048    
2023-01-06 14:38:58,985 - Epoch: [92][   30/  246]    Overall Loss 0.244650    Objective Loss 0.244650                                        LR 0.000022    Time 0.042576    
2023-01-06 14:38:59,245 - Epoch: [92][   40/  246]    Overall Loss 0.244962    Objective Loss 0.244962                                        LR 0.000022    Time 0.038433    
2023-01-06 14:38:59,505 - Epoch: [92][   50/  246]    Overall Loss 0.247778    Objective Loss 0.247778                                        LR 0.000022    Time 0.035929    
2023-01-06 14:38:59,771 - Epoch: [92][   60/  246]    Overall Loss 0.249160    Objective Loss 0.249160                                        LR 0.000022    Time 0.034365    
2023-01-06 14:39:00,031 - Epoch: [92][   70/  246]    Overall Loss 0.249008    Objective Loss 0.249008                                        LR 0.000022    Time 0.033166    
2023-01-06 14:39:00,282 - Epoch: [92][   80/  246]    Overall Loss 0.248742    Objective Loss 0.248742                                        LR 0.000022    Time 0.032157    
2023-01-06 14:39:00,540 - Epoch: [92][   90/  246]    Overall Loss 0.248694    Objective Loss 0.248694                                        LR 0.000022    Time 0.031450    
2023-01-06 14:39:00,795 - Epoch: [92][  100/  246]    Overall Loss 0.247469    Objective Loss 0.247469                                        LR 0.000022    Time 0.030849    
2023-01-06 14:39:01,025 - Epoch: [92][  110/  246]    Overall Loss 0.247095    Objective Loss 0.247095                                        LR 0.000022    Time 0.030129    
2023-01-06 14:39:01,240 - Epoch: [92][  120/  246]    Overall Loss 0.246806    Objective Loss 0.246806                                        LR 0.000022    Time 0.029406    
2023-01-06 14:39:01,456 - Epoch: [92][  130/  246]    Overall Loss 0.246791    Objective Loss 0.246791                                        LR 0.000022    Time 0.028806    
2023-01-06 14:39:01,671 - Epoch: [92][  140/  246]    Overall Loss 0.247724    Objective Loss 0.247724                                        LR 0.000022    Time 0.028282    
2023-01-06 14:39:01,882 - Epoch: [92][  150/  246]    Overall Loss 0.246859    Objective Loss 0.246859                                        LR 0.000022    Time 0.027798    
2023-01-06 14:39:02,100 - Epoch: [92][  160/  246]    Overall Loss 0.246428    Objective Loss 0.246428                                        LR 0.000022    Time 0.027426    
2023-01-06 14:39:02,311 - Epoch: [92][  170/  246]    Overall Loss 0.247112    Objective Loss 0.247112                                        LR 0.000022    Time 0.027048    
2023-01-06 14:39:02,533 - Epoch: [92][  180/  246]    Overall Loss 0.247217    Objective Loss 0.247217                                        LR 0.000022    Time 0.026776    
2023-01-06 14:39:02,745 - Epoch: [92][  190/  246]    Overall Loss 0.247123    Objective Loss 0.247123                                        LR 0.000022    Time 0.026473    
2023-01-06 14:39:02,965 - Epoch: [92][  200/  246]    Overall Loss 0.245258    Objective Loss 0.245258                                        LR 0.000022    Time 0.026251    
2023-01-06 14:39:03,177 - Epoch: [92][  210/  246]    Overall Loss 0.245329    Objective Loss 0.245329                                        LR 0.000022    Time 0.026008    
2023-01-06 14:39:03,393 - Epoch: [92][  220/  246]    Overall Loss 0.245564    Objective Loss 0.245564                                        LR 0.000022    Time 0.025803    
2023-01-06 14:39:03,607 - Epoch: [92][  230/  246]    Overall Loss 0.245034    Objective Loss 0.245034                                        LR 0.000022    Time 0.025614    
2023-01-06 14:39:03,833 - Epoch: [92][  240/  246]    Overall Loss 0.244821    Objective Loss 0.244821                                        LR 0.000022    Time 0.025487    
2023-01-06 14:39:03,949 - Epoch: [92][  246/  246]    Overall Loss 0.244840    Objective Loss 0.244840    Top1 90.430622    LR 0.000022    Time 0.025335    
2023-01-06 14:39:04,070 - --- validate (epoch=92)-----------
2023-01-06 14:39:04,070 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:04,519 - Epoch: [92][   10/   28]    Loss 0.245434    Top1 90.820312    
2023-01-06 14:39:04,659 - Epoch: [92][   20/   28]    Loss 0.258415    Top1 90.488281    
2023-01-06 14:39:04,749 - Epoch: [92][   28/   28]    Loss 0.258271    Top1 90.566848    
2023-01-06 14:39:04,898 - ==> Top1: 90.567    Loss: 0.258

2023-01-06 14:39:04,898 - ==> Confusion:
[[ 258    9  172]
 [  16  270  316]
 [  74   72 5799]]

2023-01-06 14:39:04,899 - ==> Best [Top1: 90.739   Sparsity:0.00   Params: 360896 on epoch: 89]
2023-01-06 14:39:04,900 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:04,909 - 

2023-01-06 14:39:04,909 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:05,496 - Epoch: [93][   10/  246]    Overall Loss 0.246709    Objective Loss 0.246709                                        LR 0.000022    Time 0.058558    
2023-01-06 14:39:05,735 - Epoch: [93][   20/  246]    Overall Loss 0.255242    Objective Loss 0.255242                                        LR 0.000022    Time 0.041209    
2023-01-06 14:39:05,982 - Epoch: [93][   30/  246]    Overall Loss 0.252508    Objective Loss 0.252508                                        LR 0.000022    Time 0.035651    
2023-01-06 14:39:06,230 - Epoch: [93][   40/  246]    Overall Loss 0.254870    Objective Loss 0.254870                                        LR 0.000022    Time 0.032912    
2023-01-06 14:39:06,479 - Epoch: [93][   50/  246]    Overall Loss 0.245794    Objective Loss 0.245794                                        LR 0.000022    Time 0.031305    
2023-01-06 14:39:06,724 - Epoch: [93][   60/  246]    Overall Loss 0.243940    Objective Loss 0.243940                                        LR 0.000022    Time 0.030163    
2023-01-06 14:39:06,971 - Epoch: [93][   70/  246]    Overall Loss 0.243231    Objective Loss 0.243231                                        LR 0.000022    Time 0.029351    
2023-01-06 14:39:07,220 - Epoch: [93][   80/  246]    Overall Loss 0.240600    Objective Loss 0.240600                                        LR 0.000022    Time 0.028793    
2023-01-06 14:39:07,447 - Epoch: [93][   90/  246]    Overall Loss 0.240053    Objective Loss 0.240053                                        LR 0.000022    Time 0.028107    
2023-01-06 14:39:07,658 - Epoch: [93][  100/  246]    Overall Loss 0.239828    Objective Loss 0.239828                                        LR 0.000022    Time 0.027401    
2023-01-06 14:39:07,868 - Epoch: [93][  110/  246]    Overall Loss 0.239258    Objective Loss 0.239258                                        LR 0.000022    Time 0.026817    
2023-01-06 14:39:08,077 - Epoch: [93][  120/  246]    Overall Loss 0.238637    Objective Loss 0.238637                                        LR 0.000022    Time 0.026327    
2023-01-06 14:39:08,287 - Epoch: [93][  130/  246]    Overall Loss 0.239362    Objective Loss 0.239362                                        LR 0.000022    Time 0.025910    
2023-01-06 14:39:08,497 - Epoch: [93][  140/  246]    Overall Loss 0.239598    Objective Loss 0.239598                                        LR 0.000022    Time 0.025556    
2023-01-06 14:39:08,708 - Epoch: [93][  150/  246]    Overall Loss 0.240012    Objective Loss 0.240012                                        LR 0.000022    Time 0.025260    
2023-01-06 14:39:08,919 - Epoch: [93][  160/  246]    Overall Loss 0.240979    Objective Loss 0.240979                                        LR 0.000022    Time 0.024997    
2023-01-06 14:39:09,128 - Epoch: [93][  170/  246]    Overall Loss 0.241274    Objective Loss 0.241274                                        LR 0.000022    Time 0.024754    
2023-01-06 14:39:09,342 - Epoch: [93][  180/  246]    Overall Loss 0.242262    Objective Loss 0.242262                                        LR 0.000022    Time 0.024564    
2023-01-06 14:39:09,571 - Epoch: [93][  190/  246]    Overall Loss 0.242736    Objective Loss 0.242736                                        LR 0.000022    Time 0.024474    
2023-01-06 14:39:09,781 - Epoch: [93][  200/  246]    Overall Loss 0.242402    Objective Loss 0.242402                                        LR 0.000022    Time 0.024297    
2023-01-06 14:39:09,993 - Epoch: [93][  210/  246]    Overall Loss 0.242531    Objective Loss 0.242531                                        LR 0.000022    Time 0.024150    
2023-01-06 14:39:10,230 - Epoch: [93][  220/  246]    Overall Loss 0.241963    Objective Loss 0.241963                                        LR 0.000022    Time 0.024127    
2023-01-06 14:39:10,480 - Epoch: [93][  230/  246]    Overall Loss 0.241476    Objective Loss 0.241476                                        LR 0.000022    Time 0.024162    
2023-01-06 14:39:10,740 - Epoch: [93][  240/  246]    Overall Loss 0.241910    Objective Loss 0.241910                                        LR 0.000022    Time 0.024237    
2023-01-06 14:39:10,870 - Epoch: [93][  246/  246]    Overall Loss 0.241557    Objective Loss 0.241557    Top1 90.909091    LR 0.000022    Time 0.024173    
2023-01-06 14:39:10,996 - --- validate (epoch=93)-----------
2023-01-06 14:39:10,996 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:11,466 - Epoch: [93][   10/   28]    Loss 0.276996    Top1 90.468750    
2023-01-06 14:39:11,610 - Epoch: [93][   20/   28]    Loss 0.260838    Top1 90.664062    
2023-01-06 14:39:11,700 - Epoch: [93][   28/   28]    Loss 0.263075    Top1 90.509591    
2023-01-06 14:39:11,819 - ==> Top1: 90.510    Loss: 0.263

2023-01-06 14:39:11,819 - ==> Confusion:
[[ 238   16  185]
 [  13  260  329]
 [  51   69 5825]]

2023-01-06 14:39:11,821 - ==> Best [Top1: 90.739   Sparsity:0.00   Params: 360896 on epoch: 89]
2023-01-06 14:39:11,821 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:11,831 - 

2023-01-06 14:39:11,831 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:12,556 - Epoch: [94][   10/  246]    Overall Loss 0.239668    Objective Loss 0.239668                                        LR 0.000022    Time 0.072440    
2023-01-06 14:39:12,791 - Epoch: [94][   20/  246]    Overall Loss 0.248631    Objective Loss 0.248631                                        LR 0.000022    Time 0.047961    
2023-01-06 14:39:13,046 - Epoch: [94][   30/  246]    Overall Loss 0.248784    Objective Loss 0.248784                                        LR 0.000022    Time 0.040468    
2023-01-06 14:39:13,300 - Epoch: [94][   40/  246]    Overall Loss 0.248347    Objective Loss 0.248347                                        LR 0.000022    Time 0.036689    
2023-01-06 14:39:13,565 - Epoch: [94][   50/  246]    Overall Loss 0.244343    Objective Loss 0.244343                                        LR 0.000022    Time 0.034636    
2023-01-06 14:39:13,818 - Epoch: [94][   60/  246]    Overall Loss 0.244354    Objective Loss 0.244354                                        LR 0.000022    Time 0.033065    
2023-01-06 14:39:14,067 - Epoch: [94][   70/  246]    Overall Loss 0.245709    Objective Loss 0.245709                                        LR 0.000022    Time 0.031898    
2023-01-06 14:39:14,311 - Epoch: [94][   80/  246]    Overall Loss 0.245500    Objective Loss 0.245500                                        LR 0.000022    Time 0.030953    
2023-01-06 14:39:14,560 - Epoch: [94][   90/  246]    Overall Loss 0.243734    Objective Loss 0.243734                                        LR 0.000022    Time 0.030282    
2023-01-06 14:39:14,805 - Epoch: [94][  100/  246]    Overall Loss 0.243709    Objective Loss 0.243709                                        LR 0.000022    Time 0.029693    
2023-01-06 14:39:15,053 - Epoch: [94][  110/  246]    Overall Loss 0.243799    Objective Loss 0.243799                                        LR 0.000022    Time 0.029250    
2023-01-06 14:39:15,301 - Epoch: [94][  120/  246]    Overall Loss 0.243447    Objective Loss 0.243447                                        LR 0.000022    Time 0.028873    
2023-01-06 14:39:15,551 - Epoch: [94][  130/  246]    Overall Loss 0.244633    Objective Loss 0.244633                                        LR 0.000022    Time 0.028568    
2023-01-06 14:39:15,804 - Epoch: [94][  140/  246]    Overall Loss 0.246315    Objective Loss 0.246315                                        LR 0.000022    Time 0.028334    
2023-01-06 14:39:16,058 - Epoch: [94][  150/  246]    Overall Loss 0.245494    Objective Loss 0.245494                                        LR 0.000022    Time 0.028134    
2023-01-06 14:39:16,310 - Epoch: [94][  160/  246]    Overall Loss 0.245455    Objective Loss 0.245455                                        LR 0.000022    Time 0.027950    
2023-01-06 14:39:16,567 - Epoch: [94][  170/  246]    Overall Loss 0.245210    Objective Loss 0.245210                                        LR 0.000022    Time 0.027814    
2023-01-06 14:39:16,820 - Epoch: [94][  180/  246]    Overall Loss 0.245661    Objective Loss 0.245661                                        LR 0.000022    Time 0.027671    
2023-01-06 14:39:17,073 - Epoch: [94][  190/  246]    Overall Loss 0.244941    Objective Loss 0.244941                                        LR 0.000022    Time 0.027545    
2023-01-06 14:39:17,320 - Epoch: [94][  200/  246]    Overall Loss 0.245347    Objective Loss 0.245347                                        LR 0.000022    Time 0.027398    
2023-01-06 14:39:17,572 - Epoch: [94][  210/  246]    Overall Loss 0.245087    Objective Loss 0.245087                                        LR 0.000022    Time 0.027292    
2023-01-06 14:39:17,817 - Epoch: [94][  220/  246]    Overall Loss 0.244785    Objective Loss 0.244785                                        LR 0.000022    Time 0.027164    
2023-01-06 14:39:18,063 - Epoch: [94][  230/  246]    Overall Loss 0.244250    Objective Loss 0.244250                                        LR 0.000022    Time 0.027051    
2023-01-06 14:39:18,322 - Epoch: [94][  240/  246]    Overall Loss 0.244255    Objective Loss 0.244255                                        LR 0.000022    Time 0.026999    
2023-01-06 14:39:18,450 - Epoch: [94][  246/  246]    Overall Loss 0.244272    Objective Loss 0.244272    Top1 90.430622    LR 0.000022    Time 0.026862    
2023-01-06 14:39:18,594 - --- validate (epoch=94)-----------
2023-01-06 14:39:18,594 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:19,041 - Epoch: [94][   10/   28]    Loss 0.272020    Top1 90.898438    
2023-01-06 14:39:19,180 - Epoch: [94][   20/   28]    Loss 0.264835    Top1 90.898438    
2023-01-06 14:39:19,272 - Epoch: [94][   28/   28]    Loss 0.261113    Top1 90.910392    
2023-01-06 14:39:19,435 - ==> Top1: 90.910    Loss: 0.261

2023-01-06 14:39:19,435 - ==> Confusion:
[[ 249   13  177]
 [   8  304  290]
 [  60   87 5798]]

2023-01-06 14:39:19,436 - ==> Best [Top1: 90.910   Sparsity:0.00   Params: 360896 on epoch: 94]
2023-01-06 14:39:19,436 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:19,457 - 

2023-01-06 14:39:19,457 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:20,044 - Epoch: [95][   10/  246]    Overall Loss 0.240476    Objective Loss 0.240476                                        LR 0.000022    Time 0.058610    
2023-01-06 14:39:20,288 - Epoch: [95][   20/  246]    Overall Loss 0.245481    Objective Loss 0.245481                                        LR 0.000022    Time 0.041493    
2023-01-06 14:39:20,533 - Epoch: [95][   30/  246]    Overall Loss 0.247870    Objective Loss 0.247870                                        LR 0.000022    Time 0.035808    
2023-01-06 14:39:20,782 - Epoch: [95][   40/  246]    Overall Loss 0.247664    Objective Loss 0.247664                                        LR 0.000022    Time 0.033066    
2023-01-06 14:39:21,024 - Epoch: [95][   50/  246]    Overall Loss 0.246665    Objective Loss 0.246665                                        LR 0.000022    Time 0.031295    
2023-01-06 14:39:21,270 - Epoch: [95][   60/  246]    Overall Loss 0.245134    Objective Loss 0.245134                                        LR 0.000022    Time 0.030152    
2023-01-06 14:39:21,512 - Epoch: [95][   70/  246]    Overall Loss 0.243945    Objective Loss 0.243945                                        LR 0.000022    Time 0.029298    
2023-01-06 14:39:21,756 - Epoch: [95][   80/  246]    Overall Loss 0.245840    Objective Loss 0.245840                                        LR 0.000022    Time 0.028677    
2023-01-06 14:39:21,997 - Epoch: [95][   90/  246]    Overall Loss 0.243943    Objective Loss 0.243943                                        LR 0.000022    Time 0.028163    
2023-01-06 14:39:22,241 - Epoch: [95][  100/  246]    Overall Loss 0.243655    Objective Loss 0.243655                                        LR 0.000022    Time 0.027781    
2023-01-06 14:39:22,489 - Epoch: [95][  110/  246]    Overall Loss 0.241358    Objective Loss 0.241358                                        LR 0.000022    Time 0.027507    
2023-01-06 14:39:22,739 - Epoch: [95][  120/  246]    Overall Loss 0.239761    Objective Loss 0.239761                                        LR 0.000022    Time 0.027292    
2023-01-06 14:39:22,987 - Epoch: [95][  130/  246]    Overall Loss 0.239321    Objective Loss 0.239321                                        LR 0.000022    Time 0.027099    
2023-01-06 14:39:23,236 - Epoch: [95][  140/  246]    Overall Loss 0.238717    Objective Loss 0.238717                                        LR 0.000022    Time 0.026938    
2023-01-06 14:39:23,479 - Epoch: [95][  150/  246]    Overall Loss 0.239699    Objective Loss 0.239699                                        LR 0.000022    Time 0.026761    
2023-01-06 14:39:23,729 - Epoch: [95][  160/  246]    Overall Loss 0.238933    Objective Loss 0.238933                                        LR 0.000022    Time 0.026645    
2023-01-06 14:39:23,972 - Epoch: [95][  170/  246]    Overall Loss 0.239174    Objective Loss 0.239174                                        LR 0.000022    Time 0.026506    
2023-01-06 14:39:24,215 - Epoch: [95][  180/  246]    Overall Loss 0.239020    Objective Loss 0.239020                                        LR 0.000022    Time 0.026379    
2023-01-06 14:39:24,457 - Epoch: [95][  190/  246]    Overall Loss 0.239852    Objective Loss 0.239852                                        LR 0.000022    Time 0.026265    
2023-01-06 14:39:24,699 - Epoch: [95][  200/  246]    Overall Loss 0.239541    Objective Loss 0.239541                                        LR 0.000022    Time 0.026157    
2023-01-06 14:39:24,940 - Epoch: [95][  210/  246]    Overall Loss 0.240657    Objective Loss 0.240657                                        LR 0.000022    Time 0.026060    
2023-01-06 14:39:25,184 - Epoch: [95][  220/  246]    Overall Loss 0.240836    Objective Loss 0.240836                                        LR 0.000022    Time 0.025978    
2023-01-06 14:39:25,426 - Epoch: [95][  230/  246]    Overall Loss 0.241128    Objective Loss 0.241128                                        LR 0.000022    Time 0.025900    
2023-01-06 14:39:25,682 - Epoch: [95][  240/  246]    Overall Loss 0.241185    Objective Loss 0.241185                                        LR 0.000022    Time 0.025886    
2023-01-06 14:39:25,812 - Epoch: [95][  246/  246]    Overall Loss 0.241298    Objective Loss 0.241298    Top1 88.277512    LR 0.000022    Time 0.025781    
2023-01-06 14:39:25,941 - --- validate (epoch=95)-----------
2023-01-06 14:39:25,941 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:26,386 - Epoch: [95][   10/   28]    Loss 0.222661    Top1 92.343750    
2023-01-06 14:39:26,533 - Epoch: [95][   20/   28]    Loss 0.241930    Top1 91.386719    
2023-01-06 14:39:26,625 - Epoch: [95][   28/   28]    Loss 0.255091    Top1 90.924707    
2023-01-06 14:39:26,750 - ==> Top1: 90.925    Loss: 0.255

2023-01-06 14:39:26,751 - ==> Confusion:
[[ 237   15  187]
 [  14  304  284]
 [  52   82 5811]]

2023-01-06 14:39:26,752 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:39:26,752 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:26,773 - 

2023-01-06 14:39:26,774 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:27,497 - Epoch: [96][   10/  246]    Overall Loss 0.209760    Objective Loss 0.209760                                        LR 0.000022    Time 0.072235    
2023-01-06 14:39:27,740 - Epoch: [96][   20/  246]    Overall Loss 0.228801    Objective Loss 0.228801                                        LR 0.000022    Time 0.048248    
2023-01-06 14:39:27,977 - Epoch: [96][   30/  246]    Overall Loss 0.230182    Objective Loss 0.230182                                        LR 0.000022    Time 0.040071    
2023-01-06 14:39:28,226 - Epoch: [96][   40/  246]    Overall Loss 0.230163    Objective Loss 0.230163                                        LR 0.000022    Time 0.036266    
2023-01-06 14:39:28,473 - Epoch: [96][   50/  246]    Overall Loss 0.232871    Objective Loss 0.232871                                        LR 0.000022    Time 0.033934    
2023-01-06 14:39:28,719 - Epoch: [96][   60/  246]    Overall Loss 0.234564    Objective Loss 0.234564                                        LR 0.000022    Time 0.032371    
2023-01-06 14:39:28,962 - Epoch: [96][   70/  246]    Overall Loss 0.236575    Objective Loss 0.236575                                        LR 0.000022    Time 0.031222    
2023-01-06 14:39:29,204 - Epoch: [96][   80/  246]    Overall Loss 0.235788    Objective Loss 0.235788                                        LR 0.000022    Time 0.030328    
2023-01-06 14:39:29,448 - Epoch: [96][   90/  246]    Overall Loss 0.236565    Objective Loss 0.236565                                        LR 0.000022    Time 0.029666    
2023-01-06 14:39:29,693 - Epoch: [96][  100/  246]    Overall Loss 0.239970    Objective Loss 0.239970                                        LR 0.000022    Time 0.029148    
2023-01-06 14:39:29,935 - Epoch: [96][  110/  246]    Overall Loss 0.240055    Objective Loss 0.240055                                        LR 0.000022    Time 0.028694    
2023-01-06 14:39:30,179 - Epoch: [96][  120/  246]    Overall Loss 0.241233    Objective Loss 0.241233                                        LR 0.000022    Time 0.028334    
2023-01-06 14:39:30,422 - Epoch: [96][  130/  246]    Overall Loss 0.240861    Objective Loss 0.240861                                        LR 0.000022    Time 0.028022    
2023-01-06 14:39:30,665 - Epoch: [96][  140/  246]    Overall Loss 0.240900    Objective Loss 0.240900                                        LR 0.000022    Time 0.027756    
2023-01-06 14:39:30,909 - Epoch: [96][  150/  246]    Overall Loss 0.240949    Objective Loss 0.240949                                        LR 0.000022    Time 0.027531    
2023-01-06 14:39:31,152 - Epoch: [96][  160/  246]    Overall Loss 0.240204    Objective Loss 0.240204                                        LR 0.000022    Time 0.027326    
2023-01-06 14:39:31,396 - Epoch: [96][  170/  246]    Overall Loss 0.241027    Objective Loss 0.241027                                        LR 0.000022    Time 0.027152    
2023-01-06 14:39:31,639 - Epoch: [96][  180/  246]    Overall Loss 0.240281    Objective Loss 0.240281                                        LR 0.000022    Time 0.026994    
2023-01-06 14:39:31,884 - Epoch: [96][  190/  246]    Overall Loss 0.241019    Objective Loss 0.241019                                        LR 0.000022    Time 0.026856    
2023-01-06 14:39:32,129 - Epoch: [96][  200/  246]    Overall Loss 0.240663    Objective Loss 0.240663                                        LR 0.000022    Time 0.026736    
2023-01-06 14:39:32,370 - Epoch: [96][  210/  246]    Overall Loss 0.240767    Objective Loss 0.240767                                        LR 0.000022    Time 0.026612    
2023-01-06 14:39:32,612 - Epoch: [96][  220/  246]    Overall Loss 0.240399    Objective Loss 0.240399                                        LR 0.000022    Time 0.026501    
2023-01-06 14:39:32,855 - Epoch: [96][  230/  246]    Overall Loss 0.240427    Objective Loss 0.240427                                        LR 0.000022    Time 0.026402    
2023-01-06 14:39:33,112 - Epoch: [96][  240/  246]    Overall Loss 0.239784    Objective Loss 0.239784                                        LR 0.000022    Time 0.026370    
2023-01-06 14:39:33,241 - Epoch: [96][  246/  246]    Overall Loss 0.239927    Objective Loss 0.239927    Top1 92.105263    LR 0.000022    Time 0.026252    
2023-01-06 14:39:33,374 - --- validate (epoch=96)-----------
2023-01-06 14:39:33,374 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:33,822 - Epoch: [96][   10/   28]    Loss 0.249499    Top1 91.210938    
2023-01-06 14:39:33,972 - Epoch: [96][   20/   28]    Loss 0.259681    Top1 90.664062    
2023-01-06 14:39:34,064 - Epoch: [96][   28/   28]    Loss 0.258509    Top1 90.838820    
2023-01-06 14:39:34,193 - ==> Top1: 90.839    Loss: 0.259

2023-01-06 14:39:34,193 - ==> Confusion:
[[ 245   20  174]
 [  11  314  277]
 [  52  106 5787]]

2023-01-06 14:39:34,195 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:39:34,195 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:34,205 - 

2023-01-06 14:39:34,205 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:34,958 - Epoch: [97][   10/  246]    Overall Loss 0.224287    Objective Loss 0.224287                                        LR 0.000022    Time 0.075282    
2023-01-06 14:39:35,208 - Epoch: [97][   20/  246]    Overall Loss 0.238073    Objective Loss 0.238073                                        LR 0.000022    Time 0.050113    
2023-01-06 14:39:35,447 - Epoch: [97][   30/  246]    Overall Loss 0.240688    Objective Loss 0.240688                                        LR 0.000022    Time 0.041357    
2023-01-06 14:39:35,696 - Epoch: [97][   40/  246]    Overall Loss 0.239968    Objective Loss 0.239968                                        LR 0.000022    Time 0.037227    
2023-01-06 14:39:35,948 - Epoch: [97][   50/  246]    Overall Loss 0.234644    Objective Loss 0.234644                                        LR 0.000022    Time 0.034826    
2023-01-06 14:39:36,202 - Epoch: [97][   60/  246]    Overall Loss 0.233488    Objective Loss 0.233488                                        LR 0.000022    Time 0.033245    
2023-01-06 14:39:36,443 - Epoch: [97][   70/  246]    Overall Loss 0.237697    Objective Loss 0.237697                                        LR 0.000022    Time 0.031933    
2023-01-06 14:39:36,695 - Epoch: [97][   80/  246]    Overall Loss 0.236392    Objective Loss 0.236392                                        LR 0.000022    Time 0.031086    
2023-01-06 14:39:36,950 - Epoch: [97][   90/  246]    Overall Loss 0.240479    Objective Loss 0.240479                                        LR 0.000022    Time 0.030459    
2023-01-06 14:39:37,203 - Epoch: [97][  100/  246]    Overall Loss 0.238030    Objective Loss 0.238030                                        LR 0.000022    Time 0.029939    
2023-01-06 14:39:37,454 - Epoch: [97][  110/  246]    Overall Loss 0.240199    Objective Loss 0.240199                                        LR 0.000022    Time 0.029491    
2023-01-06 14:39:37,708 - Epoch: [97][  120/  246]    Overall Loss 0.239344    Objective Loss 0.239344                                        LR 0.000022    Time 0.029147    
2023-01-06 14:39:37,961 - Epoch: [97][  130/  246]    Overall Loss 0.239641    Objective Loss 0.239641                                        LR 0.000022    Time 0.028846    
2023-01-06 14:39:38,211 - Epoch: [97][  140/  246]    Overall Loss 0.239958    Objective Loss 0.239958                                        LR 0.000022    Time 0.028568    
2023-01-06 14:39:38,465 - Epoch: [97][  150/  246]    Overall Loss 0.238990    Objective Loss 0.238990                                        LR 0.000022    Time 0.028354    
2023-01-06 14:39:38,719 - Epoch: [97][  160/  246]    Overall Loss 0.239476    Objective Loss 0.239476                                        LR 0.000022    Time 0.028168    
2023-01-06 14:39:38,972 - Epoch: [97][  170/  246]    Overall Loss 0.238946    Objective Loss 0.238946                                        LR 0.000022    Time 0.027995    
2023-01-06 14:39:39,236 - Epoch: [97][  180/  246]    Overall Loss 0.238815    Objective Loss 0.238815                                        LR 0.000022    Time 0.027902    
2023-01-06 14:39:39,496 - Epoch: [97][  190/  246]    Overall Loss 0.239397    Objective Loss 0.239397                                        LR 0.000022    Time 0.027800    
2023-01-06 14:39:39,716 - Epoch: [97][  200/  246]    Overall Loss 0.239101    Objective Loss 0.239101                                        LR 0.000022    Time 0.027512    
2023-01-06 14:39:39,932 - Epoch: [97][  210/  246]    Overall Loss 0.239944    Objective Loss 0.239944                                        LR 0.000022    Time 0.027227    
2023-01-06 14:39:40,151 - Epoch: [97][  220/  246]    Overall Loss 0.239536    Objective Loss 0.239536                                        LR 0.000022    Time 0.026983    
2023-01-06 14:39:40,368 - Epoch: [97][  230/  246]    Overall Loss 0.239590    Objective Loss 0.239590                                        LR 0.000022    Time 0.026752    
2023-01-06 14:39:40,597 - Epoch: [97][  240/  246]    Overall Loss 0.239622    Objective Loss 0.239622                                        LR 0.000022    Time 0.026588    
2023-01-06 14:39:40,720 - Epoch: [97][  246/  246]    Overall Loss 0.239778    Objective Loss 0.239778    Top1 89.712919    LR 0.000022    Time 0.026440    
2023-01-06 14:39:40,851 - --- validate (epoch=97)-----------
2023-01-06 14:39:40,851 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:41,297 - Epoch: [97][   10/   28]    Loss 0.278898    Top1 90.234375    
2023-01-06 14:39:41,435 - Epoch: [97][   20/   28]    Loss 0.276048    Top1 90.273438    
2023-01-06 14:39:41,527 - Epoch: [97][   28/   28]    Loss 0.263269    Top1 90.667048    
2023-01-06 14:39:41,684 - ==> Top1: 90.667    Loss: 0.263

2023-01-06 14:39:41,684 - ==> Confusion:
[[ 235   16  188]
 [  10  290  302]
 [  54   82 5809]]

2023-01-06 14:39:41,686 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:39:41,686 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:41,696 - 

2023-01-06 14:39:41,696 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:42,268 - Epoch: [98][   10/  246]    Overall Loss 0.232616    Objective Loss 0.232616                                        LR 0.000022    Time 0.057094    
2023-01-06 14:39:42,512 - Epoch: [98][   20/  246]    Overall Loss 0.240908    Objective Loss 0.240908                                        LR 0.000022    Time 0.040657    
2023-01-06 14:39:42,751 - Epoch: [98][   30/  246]    Overall Loss 0.241590    Objective Loss 0.241590                                        LR 0.000022    Time 0.035012    
2023-01-06 14:39:43,002 - Epoch: [98][   40/  246]    Overall Loss 0.238449    Objective Loss 0.238449                                        LR 0.000022    Time 0.032525    
2023-01-06 14:39:43,255 - Epoch: [98][   50/  246]    Overall Loss 0.237757    Objective Loss 0.237757                                        LR 0.000022    Time 0.031078    
2023-01-06 14:39:43,505 - Epoch: [98][   60/  246]    Overall Loss 0.239845    Objective Loss 0.239845                                        LR 0.000022    Time 0.030043    
2023-01-06 14:39:43,751 - Epoch: [98][   70/  246]    Overall Loss 0.239499    Objective Loss 0.239499                                        LR 0.000022    Time 0.029263    
2023-01-06 14:39:44,000 - Epoch: [98][   80/  246]    Overall Loss 0.239484    Objective Loss 0.239484                                        LR 0.000022    Time 0.028718    
2023-01-06 14:39:44,250 - Epoch: [98][   90/  246]    Overall Loss 0.236320    Objective Loss 0.236320                                        LR 0.000022    Time 0.028298    
2023-01-06 14:39:44,501 - Epoch: [98][  100/  246]    Overall Loss 0.237135    Objective Loss 0.237135                                        LR 0.000022    Time 0.027957    
2023-01-06 14:39:44,755 - Epoch: [98][  110/  246]    Overall Loss 0.236834    Objective Loss 0.236834                                        LR 0.000022    Time 0.027720    
2023-01-06 14:39:45,003 - Epoch: [98][  120/  246]    Overall Loss 0.236219    Objective Loss 0.236219                                        LR 0.000022    Time 0.027467    
2023-01-06 14:39:45,252 - Epoch: [98][  130/  246]    Overall Loss 0.238029    Objective Loss 0.238029                                        LR 0.000022    Time 0.027263    
2023-01-06 14:39:45,499 - Epoch: [98][  140/  246]    Overall Loss 0.237455    Objective Loss 0.237455                                        LR 0.000022    Time 0.027079    
2023-01-06 14:39:45,750 - Epoch: [98][  150/  246]    Overall Loss 0.237922    Objective Loss 0.237922                                        LR 0.000022    Time 0.026940    
2023-01-06 14:39:45,999 - Epoch: [98][  160/  246]    Overall Loss 0.237256    Objective Loss 0.237256                                        LR 0.000022    Time 0.026811    
2023-01-06 14:39:46,248 - Epoch: [98][  170/  246]    Overall Loss 0.238665    Objective Loss 0.238665                                        LR 0.000022    Time 0.026698    
2023-01-06 14:39:46,498 - Epoch: [98][  180/  246]    Overall Loss 0.238672    Objective Loss 0.238672                                        LR 0.000022    Time 0.026600    
2023-01-06 14:39:46,748 - Epoch: [98][  190/  246]    Overall Loss 0.240234    Objective Loss 0.240234                                        LR 0.000022    Time 0.026512    
2023-01-06 14:39:46,997 - Epoch: [98][  200/  246]    Overall Loss 0.240461    Objective Loss 0.240461                                        LR 0.000022    Time 0.026432    
2023-01-06 14:39:47,247 - Epoch: [98][  210/  246]    Overall Loss 0.239701    Objective Loss 0.239701                                        LR 0.000022    Time 0.026360    
2023-01-06 14:39:47,497 - Epoch: [98][  220/  246]    Overall Loss 0.239598    Objective Loss 0.239598                                        LR 0.000022    Time 0.026295    
2023-01-06 14:39:47,746 - Epoch: [98][  230/  246]    Overall Loss 0.239225    Objective Loss 0.239225                                        LR 0.000022    Time 0.026232    
2023-01-06 14:39:48,004 - Epoch: [98][  240/  246]    Overall Loss 0.240071    Objective Loss 0.240071                                        LR 0.000022    Time 0.026213    
2023-01-06 14:39:48,133 - Epoch: [98][  246/  246]    Overall Loss 0.239578    Objective Loss 0.239578    Top1 92.583732    LR 0.000022    Time 0.026095    
2023-01-06 14:39:48,247 - --- validate (epoch=98)-----------
2023-01-06 14:39:48,247 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:48,692 - Epoch: [98][   10/   28]    Loss 0.293234    Top1 89.648438    
2023-01-06 14:39:48,838 - Epoch: [98][   20/   28]    Loss 0.269243    Top1 90.117188    
2023-01-06 14:39:48,928 - Epoch: [98][   28/   28]    Loss 0.265081    Top1 90.466648    
2023-01-06 14:39:49,076 - ==> Top1: 90.467    Loss: 0.265

2023-01-06 14:39:49,077 - ==> Confusion:
[[ 245   16  178]
 [  17  278  307]
 [  73   75 5797]]

2023-01-06 14:39:49,078 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:39:49,078 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:49,088 - 

2023-01-06 14:39:49,088 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:49,832 - Epoch: [99][   10/  246]    Overall Loss 0.268670    Objective Loss 0.268670                                        LR 0.000022    Time 0.074338    
2023-01-06 14:39:50,082 - Epoch: [99][   20/  246]    Overall Loss 0.249567    Objective Loss 0.249567                                        LR 0.000022    Time 0.049642    
2023-01-06 14:39:50,333 - Epoch: [99][   30/  246]    Overall Loss 0.248594    Objective Loss 0.248594                                        LR 0.000022    Time 0.041439    
2023-01-06 14:39:50,583 - Epoch: [99][   40/  246]    Overall Loss 0.247193    Objective Loss 0.247193                                        LR 0.000022    Time 0.037323    
2023-01-06 14:39:50,833 - Epoch: [99][   50/  246]    Overall Loss 0.242743    Objective Loss 0.242743                                        LR 0.000022    Time 0.034809    
2023-01-06 14:39:51,079 - Epoch: [99][   60/  246]    Overall Loss 0.242775    Objective Loss 0.242775                                        LR 0.000022    Time 0.033076    
2023-01-06 14:39:51,322 - Epoch: [99][   70/  246]    Overall Loss 0.239407    Objective Loss 0.239407                                        LR 0.000022    Time 0.031819    
2023-01-06 14:39:51,565 - Epoch: [99][   80/  246]    Overall Loss 0.237125    Objective Loss 0.237125                                        LR 0.000022    Time 0.030875    
2023-01-06 14:39:51,814 - Epoch: [99][   90/  246]    Overall Loss 0.238829    Objective Loss 0.238829                                        LR 0.000022    Time 0.030203    
2023-01-06 14:39:52,063 - Epoch: [99][  100/  246]    Overall Loss 0.239232    Objective Loss 0.239232                                        LR 0.000022    Time 0.029671    
2023-01-06 14:39:52,312 - Epoch: [99][  110/  246]    Overall Loss 0.239587    Objective Loss 0.239587                                        LR 0.000022    Time 0.029215    
2023-01-06 14:39:52,561 - Epoch: [99][  120/  246]    Overall Loss 0.239038    Objective Loss 0.239038                                        LR 0.000022    Time 0.028852    
2023-01-06 14:39:52,812 - Epoch: [99][  130/  246]    Overall Loss 0.238681    Objective Loss 0.238681                                        LR 0.000022    Time 0.028542    
2023-01-06 14:39:53,062 - Epoch: [99][  140/  246]    Overall Loss 0.238202    Objective Loss 0.238202                                        LR 0.000022    Time 0.028288    
2023-01-06 14:39:53,308 - Epoch: [99][  150/  246]    Overall Loss 0.237918    Objective Loss 0.237918                                        LR 0.000022    Time 0.028027    
2023-01-06 14:39:53,559 - Epoch: [99][  160/  246]    Overall Loss 0.237087    Objective Loss 0.237087                                        LR 0.000022    Time 0.027840    
2023-01-06 14:39:53,811 - Epoch: [99][  170/  246]    Overall Loss 0.237456    Objective Loss 0.237456                                        LR 0.000022    Time 0.027668    
2023-01-06 14:39:54,060 - Epoch: [99][  180/  246]    Overall Loss 0.237944    Objective Loss 0.237944                                        LR 0.000022    Time 0.027517    
2023-01-06 14:39:54,311 - Epoch: [99][  190/  246]    Overall Loss 0.238661    Objective Loss 0.238661                                        LR 0.000022    Time 0.027373    
2023-01-06 14:39:54,561 - Epoch: [99][  200/  246]    Overall Loss 0.238496    Objective Loss 0.238496                                        LR 0.000022    Time 0.027251    
2023-01-06 14:39:54,813 - Epoch: [99][  210/  246]    Overall Loss 0.238038    Objective Loss 0.238038                                        LR 0.000022    Time 0.027145    
2023-01-06 14:39:55,040 - Epoch: [99][  220/  246]    Overall Loss 0.237614    Objective Loss 0.237614                                        LR 0.000022    Time 0.026943    
2023-01-06 14:39:55,269 - Epoch: [99][  230/  246]    Overall Loss 0.238599    Objective Loss 0.238599                                        LR 0.000022    Time 0.026765    
2023-01-06 14:39:55,507 - Epoch: [99][  240/  246]    Overall Loss 0.239124    Objective Loss 0.239124                                        LR 0.000022    Time 0.026641    
2023-01-06 14:39:55,626 - Epoch: [99][  246/  246]    Overall Loss 0.238785    Objective Loss 0.238785    Top1 93.301435    LR 0.000022    Time 0.026473    
2023-01-06 14:39:55,743 - --- validate (epoch=99)-----------
2023-01-06 14:39:55,744 - 6986 samples (256 per mini-batch)
2023-01-06 14:39:56,191 - Epoch: [99][   10/   28]    Loss 0.256485    Top1 90.468750    
2023-01-06 14:39:56,332 - Epoch: [99][   20/   28]    Loss 0.258214    Top1 90.546875    
2023-01-06 14:39:56,423 - Epoch: [99][   28/   28]    Loss 0.267369    Top1 90.466648    
2023-01-06 14:39:56,551 - ==> Top1: 90.467    Loss: 0.267

2023-01-06 14:39:56,552 - ==> Confusion:
[[ 262   12  165]
 [  18  250  334]
 [  85   52 5808]]

2023-01-06 14:39:56,553 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:39:56,553 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:39:56,563 - 

2023-01-06 14:39:56,563 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:39:57,130 - Epoch: [100][   10/  246]    Overall Loss 0.261651    Objective Loss 0.261651                                        LR 0.000013    Time 0.056595    
2023-01-06 14:39:57,345 - Epoch: [100][   20/  246]    Overall Loss 0.249561    Objective Loss 0.249561                                        LR 0.000013    Time 0.039020    
2023-01-06 14:39:57,566 - Epoch: [100][   30/  246]    Overall Loss 0.245702    Objective Loss 0.245702                                        LR 0.000013    Time 0.033362    
2023-01-06 14:39:57,787 - Epoch: [100][   40/  246]    Overall Loss 0.242288    Objective Loss 0.242288                                        LR 0.000013    Time 0.030556    
2023-01-06 14:39:58,018 - Epoch: [100][   50/  246]    Overall Loss 0.240487    Objective Loss 0.240487                                        LR 0.000013    Time 0.029058    
2023-01-06 14:39:58,250 - Epoch: [100][   60/  246]    Overall Loss 0.238228    Objective Loss 0.238228                                        LR 0.000013    Time 0.028061    
2023-01-06 14:39:58,488 - Epoch: [100][   70/  246]    Overall Loss 0.237676    Objective Loss 0.237676                                        LR 0.000013    Time 0.027455    
2023-01-06 14:39:58,725 - Epoch: [100][   80/  246]    Overall Loss 0.237007    Objective Loss 0.237007                                        LR 0.000013    Time 0.026984    
2023-01-06 14:39:58,974 - Epoch: [100][   90/  246]    Overall Loss 0.236559    Objective Loss 0.236559                                        LR 0.000013    Time 0.026740    
2023-01-06 14:39:59,206 - Epoch: [100][  100/  246]    Overall Loss 0.236505    Objective Loss 0.236505                                        LR 0.000013    Time 0.026390    
2023-01-06 14:39:59,449 - Epoch: [100][  110/  246]    Overall Loss 0.238192    Objective Loss 0.238192                                        LR 0.000013    Time 0.026194    
2023-01-06 14:39:59,683 - Epoch: [100][  120/  246]    Overall Loss 0.237321    Objective Loss 0.237321                                        LR 0.000013    Time 0.025958    
2023-01-06 14:39:59,917 - Epoch: [100][  130/  246]    Overall Loss 0.236655    Objective Loss 0.236655                                        LR 0.000013    Time 0.025761    
2023-01-06 14:40:00,143 - Epoch: [100][  140/  246]    Overall Loss 0.237115    Objective Loss 0.237115                                        LR 0.000013    Time 0.025530    
2023-01-06 14:40:00,386 - Epoch: [100][  150/  246]    Overall Loss 0.236525    Objective Loss 0.236525                                        LR 0.000013    Time 0.025444    
2023-01-06 14:40:00,617 - Epoch: [100][  160/  246]    Overall Loss 0.236558    Objective Loss 0.236558                                        LR 0.000013    Time 0.025296    
2023-01-06 14:40:00,858 - Epoch: [100][  170/  246]    Overall Loss 0.236125    Objective Loss 0.236125                                        LR 0.000013    Time 0.025220    
2023-01-06 14:40:01,095 - Epoch: [100][  180/  246]    Overall Loss 0.235744    Objective Loss 0.235744                                        LR 0.000013    Time 0.025138    
2023-01-06 14:40:01,342 - Epoch: [100][  190/  246]    Overall Loss 0.236465    Objective Loss 0.236465                                        LR 0.000013    Time 0.025113    
2023-01-06 14:40:01,578 - Epoch: [100][  200/  246]    Overall Loss 0.235950    Objective Loss 0.235950                                        LR 0.000013    Time 0.025035    
2023-01-06 14:40:01,824 - Epoch: [100][  210/  246]    Overall Loss 0.236109    Objective Loss 0.236109                                        LR 0.000013    Time 0.025013    
2023-01-06 14:40:02,065 - Epoch: [100][  220/  246]    Overall Loss 0.236366    Objective Loss 0.236366                                        LR 0.000013    Time 0.024969    
2023-01-06 14:40:02,307 - Epoch: [100][  230/  246]    Overall Loss 0.235918    Objective Loss 0.235918                                        LR 0.000013    Time 0.024931    
2023-01-06 14:40:02,557 - Epoch: [100][  240/  246]    Overall Loss 0.236190    Objective Loss 0.236190                                        LR 0.000013    Time 0.024934    
2023-01-06 14:40:02,679 - Epoch: [100][  246/  246]    Overall Loss 0.236212    Objective Loss 0.236212    Top1 90.430622    LR 0.000013    Time 0.024820    
2023-01-06 14:40:02,799 - --- validate (epoch=100)-----------
2023-01-06 14:40:02,800 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:03,245 - Epoch: [100][   10/   28]    Loss 0.248875    Top1 90.820312    
2023-01-06 14:40:03,387 - Epoch: [100][   20/   28]    Loss 0.260880    Top1 90.488281    
2023-01-06 14:40:03,481 - Epoch: [100][   28/   28]    Loss 0.260850    Top1 90.566848    
2023-01-06 14:40:03,618 - ==> Top1: 90.567    Loss: 0.261

2023-01-06 14:40:03,618 - ==> Confusion:
[[ 253   15  171]
 [  12  306  284]
 [  76  101 5768]]

2023-01-06 14:40:03,620 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:40:03,620 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:03,630 - 

2023-01-06 14:40:03,630 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:04,344 - Epoch: [101][   10/  246]    Overall Loss 0.224659    Objective Loss 0.224659                                        LR 0.000013    Time 0.071357    
2023-01-06 14:40:04,559 - Epoch: [101][   20/  246]    Overall Loss 0.227899    Objective Loss 0.227899                                        LR 0.000013    Time 0.046397    
2023-01-06 14:40:04,773 - Epoch: [101][   30/  246]    Overall Loss 0.224445    Objective Loss 0.224445                                        LR 0.000013    Time 0.038041    
2023-01-06 14:40:04,992 - Epoch: [101][   40/  246]    Overall Loss 0.227678    Objective Loss 0.227678                                        LR 0.000013    Time 0.033986    
2023-01-06 14:40:05,220 - Epoch: [101][   50/  246]    Overall Loss 0.229060    Objective Loss 0.229060                                        LR 0.000013    Time 0.031759    
2023-01-06 14:40:05,482 - Epoch: [101][   60/  246]    Overall Loss 0.230130    Objective Loss 0.230130                                        LR 0.000013    Time 0.030810    
2023-01-06 14:40:05,739 - Epoch: [101][   70/  246]    Overall Loss 0.232451    Objective Loss 0.232451                                        LR 0.000013    Time 0.030077    
2023-01-06 14:40:06,000 - Epoch: [101][   80/  246]    Overall Loss 0.229346    Objective Loss 0.229346                                        LR 0.000013    Time 0.029568    
2023-01-06 14:40:06,241 - Epoch: [101][   90/  246]    Overall Loss 0.230799    Objective Loss 0.230799                                        LR 0.000013    Time 0.028944    
2023-01-06 14:40:06,464 - Epoch: [101][  100/  246]    Overall Loss 0.232781    Objective Loss 0.232781                                        LR 0.000013    Time 0.028275    
2023-01-06 14:40:06,682 - Epoch: [101][  110/  246]    Overall Loss 0.231593    Objective Loss 0.231593                                        LR 0.000013    Time 0.027682    
2023-01-06 14:40:06,903 - Epoch: [101][  120/  246]    Overall Loss 0.231717    Objective Loss 0.231717                                        LR 0.000013    Time 0.027213    
2023-01-06 14:40:07,122 - Epoch: [101][  130/  246]    Overall Loss 0.231549    Objective Loss 0.231549                                        LR 0.000013    Time 0.026797    
2023-01-06 14:40:07,343 - Epoch: [101][  140/  246]    Overall Loss 0.231268    Objective Loss 0.231268                                        LR 0.000013    Time 0.026459    
2023-01-06 14:40:07,561 - Epoch: [101][  150/  246]    Overall Loss 0.231849    Objective Loss 0.231849                                        LR 0.000013    Time 0.026146    
2023-01-06 14:40:07,812 - Epoch: [101][  160/  246]    Overall Loss 0.232400    Objective Loss 0.232400                                        LR 0.000013    Time 0.026082    
2023-01-06 14:40:08,035 - Epoch: [101][  170/  246]    Overall Loss 0.233536    Objective Loss 0.233536                                        LR 0.000013    Time 0.025856    
2023-01-06 14:40:08,258 - Epoch: [101][  180/  246]    Overall Loss 0.233639    Objective Loss 0.233639                                        LR 0.000013    Time 0.025655    
2023-01-06 14:40:08,477 - Epoch: [101][  190/  246]    Overall Loss 0.234003    Objective Loss 0.234003                                        LR 0.000013    Time 0.025457    
2023-01-06 14:40:08,700 - Epoch: [101][  200/  246]    Overall Loss 0.233807    Objective Loss 0.233807                                        LR 0.000013    Time 0.025295    
2023-01-06 14:40:08,917 - Epoch: [101][  210/  246]    Overall Loss 0.234901    Objective Loss 0.234901                                        LR 0.000013    Time 0.025122    
2023-01-06 14:40:09,156 - Epoch: [101][  220/  246]    Overall Loss 0.234953    Objective Loss 0.234953                                        LR 0.000013    Time 0.025063    
2023-01-06 14:40:09,405 - Epoch: [101][  230/  246]    Overall Loss 0.235233    Objective Loss 0.235233                                        LR 0.000013    Time 0.025055    
2023-01-06 14:40:09,654 - Epoch: [101][  240/  246]    Overall Loss 0.234484    Objective Loss 0.234484                                        LR 0.000013    Time 0.025047    
2023-01-06 14:40:09,777 - Epoch: [101][  246/  246]    Overall Loss 0.234130    Objective Loss 0.234130    Top1 92.583732    LR 0.000013    Time 0.024936    
2023-01-06 14:40:09,917 - --- validate (epoch=101)-----------
2023-01-06 14:40:09,917 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:10,371 - Epoch: [101][   10/   28]    Loss 0.269771    Top1 89.882812    
2023-01-06 14:40:10,513 - Epoch: [101][   20/   28]    Loss 0.269060    Top1 90.195312    
2023-01-06 14:40:10,605 - Epoch: [101][   28/   28]    Loss 0.259915    Top1 90.566848    
2023-01-06 14:40:10,736 - ==> Top1: 90.567    Loss: 0.260

2023-01-06 14:40:10,736 - ==> Confusion:
[[ 240    9  190]
 [  17  262  323]
 [  57   63 5825]]

2023-01-06 14:40:10,738 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:40:10,738 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:10,748 - 

2023-01-06 14:40:10,748 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:11,351 - Epoch: [102][   10/  246]    Overall Loss 0.216228    Objective Loss 0.216228                                        LR 0.000013    Time 0.060293    
2023-01-06 14:40:11,590 - Epoch: [102][   20/  246]    Overall Loss 0.226350    Objective Loss 0.226350                                        LR 0.000013    Time 0.042052    
2023-01-06 14:40:11,834 - Epoch: [102][   30/  246]    Overall Loss 0.228204    Objective Loss 0.228204                                        LR 0.000013    Time 0.036135    
2023-01-06 14:40:12,074 - Epoch: [102][   40/  246]    Overall Loss 0.230609    Objective Loss 0.230609                                        LR 0.000013    Time 0.033076    
2023-01-06 14:40:12,317 - Epoch: [102][   50/  246]    Overall Loss 0.230761    Objective Loss 0.230761                                        LR 0.000013    Time 0.031323    
2023-01-06 14:40:12,555 - Epoch: [102][   60/  246]    Overall Loss 0.233176    Objective Loss 0.233176                                        LR 0.000013    Time 0.030064    
2023-01-06 14:40:12,816 - Epoch: [102][   70/  246]    Overall Loss 0.236554    Objective Loss 0.236554                                        LR 0.000013    Time 0.029482    
2023-01-06 14:40:13,082 - Epoch: [102][   80/  246]    Overall Loss 0.237118    Objective Loss 0.237118                                        LR 0.000013    Time 0.029108    
2023-01-06 14:40:13,343 - Epoch: [102][   90/  246]    Overall Loss 0.237016    Objective Loss 0.237016                                        LR 0.000013    Time 0.028769    
2023-01-06 14:40:13,604 - Epoch: [102][  100/  246]    Overall Loss 0.238694    Objective Loss 0.238694                                        LR 0.000013    Time 0.028501    
2023-01-06 14:40:13,863 - Epoch: [102][  110/  246]    Overall Loss 0.237693    Objective Loss 0.237693                                        LR 0.000013    Time 0.028265    
2023-01-06 14:40:14,128 - Epoch: [102][  120/  246]    Overall Loss 0.236676    Objective Loss 0.236676                                        LR 0.000013    Time 0.028101    
2023-01-06 14:40:14,384 - Epoch: [102][  130/  246]    Overall Loss 0.237272    Objective Loss 0.237272                                        LR 0.000013    Time 0.027903    
2023-01-06 14:40:14,632 - Epoch: [102][  140/  246]    Overall Loss 0.234990    Objective Loss 0.234990                                        LR 0.000013    Time 0.027678    
2023-01-06 14:40:14,886 - Epoch: [102][  150/  246]    Overall Loss 0.233544    Objective Loss 0.233544                                        LR 0.000013    Time 0.027524    
2023-01-06 14:40:15,121 - Epoch: [102][  160/  246]    Overall Loss 0.233488    Objective Loss 0.233488                                        LR 0.000013    Time 0.027271    
2023-01-06 14:40:15,361 - Epoch: [102][  170/  246]    Overall Loss 0.234269    Objective Loss 0.234269                                        LR 0.000013    Time 0.027077    
2023-01-06 14:40:15,599 - Epoch: [102][  180/  246]    Overall Loss 0.235241    Objective Loss 0.235241                                        LR 0.000013    Time 0.026891    
2023-01-06 14:40:15,844 - Epoch: [102][  190/  246]    Overall Loss 0.235804    Objective Loss 0.235804                                        LR 0.000013    Time 0.026765    
2023-01-06 14:40:16,092 - Epoch: [102][  200/  246]    Overall Loss 0.236108    Objective Loss 0.236108                                        LR 0.000013    Time 0.026663    
2023-01-06 14:40:16,331 - Epoch: [102][  210/  246]    Overall Loss 0.236578    Objective Loss 0.236578                                        LR 0.000013    Time 0.026529    
2023-01-06 14:40:16,578 - Epoch: [102][  220/  246]    Overall Loss 0.236268    Objective Loss 0.236268                                        LR 0.000013    Time 0.026447    
2023-01-06 14:40:16,828 - Epoch: [102][  230/  246]    Overall Loss 0.236903    Objective Loss 0.236903                                        LR 0.000013    Time 0.026380    
2023-01-06 14:40:17,075 - Epoch: [102][  240/  246]    Overall Loss 0.236639    Objective Loss 0.236639                                        LR 0.000013    Time 0.026308    
2023-01-06 14:40:17,206 - Epoch: [102][  246/  246]    Overall Loss 0.236613    Objective Loss 0.236613    Top1 91.626794    LR 0.000013    Time 0.026199    
2023-01-06 14:40:17,349 - --- validate (epoch=102)-----------
2023-01-06 14:40:17,349 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:17,800 - Epoch: [102][   10/   28]    Loss 0.239271    Top1 91.562500    
2023-01-06 14:40:17,949 - Epoch: [102][   20/   28]    Loss 0.252894    Top1 90.937500    
2023-01-06 14:40:18,041 - Epoch: [102][   28/   28]    Loss 0.260438    Top1 90.738620    
2023-01-06 14:40:18,148 - ==> Top1: 90.739    Loss: 0.260

2023-01-06 14:40:18,148 - ==> Confusion:
[[ 256   12  171]
 [  16  275  311]
 [  68   69 5808]]

2023-01-06 14:40:18,150 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:40:18,150 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:18,160 - 

2023-01-06 14:40:18,160 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:18,876 - Epoch: [103][   10/  246]    Overall Loss 0.218748    Objective Loss 0.218748                                        LR 0.000013    Time 0.071486    
2023-01-06 14:40:19,098 - Epoch: [103][   20/  246]    Overall Loss 0.220738    Objective Loss 0.220738                                        LR 0.000013    Time 0.046835    
2023-01-06 14:40:19,352 - Epoch: [103][   30/  246]    Overall Loss 0.225706    Objective Loss 0.225706                                        LR 0.000013    Time 0.039665    
2023-01-06 14:40:19,596 - Epoch: [103][   40/  246]    Overall Loss 0.226418    Objective Loss 0.226418                                        LR 0.000013    Time 0.035817    
2023-01-06 14:40:19,839 - Epoch: [103][   50/  246]    Overall Loss 0.226503    Objective Loss 0.226503                                        LR 0.000013    Time 0.033516    
2023-01-06 14:40:20,083 - Epoch: [103][   60/  246]    Overall Loss 0.226631    Objective Loss 0.226631                                        LR 0.000013    Time 0.031985    
2023-01-06 14:40:20,323 - Epoch: [103][   70/  246]    Overall Loss 0.228223    Objective Loss 0.228223                                        LR 0.000013    Time 0.030840    
2023-01-06 14:40:20,568 - Epoch: [103][   80/  246]    Overall Loss 0.228746    Objective Loss 0.228746                                        LR 0.000013    Time 0.030036    
2023-01-06 14:40:20,812 - Epoch: [103][   90/  246]    Overall Loss 0.228188    Objective Loss 0.228188                                        LR 0.000013    Time 0.029412    
2023-01-06 14:40:21,057 - Epoch: [103][  100/  246]    Overall Loss 0.228345    Objective Loss 0.228345                                        LR 0.000013    Time 0.028912    
2023-01-06 14:40:21,297 - Epoch: [103][  110/  246]    Overall Loss 0.230368    Objective Loss 0.230368                                        LR 0.000013    Time 0.028464    
2023-01-06 14:40:21,540 - Epoch: [103][  120/  246]    Overall Loss 0.231288    Objective Loss 0.231288                                        LR 0.000013    Time 0.028113    
2023-01-06 14:40:21,781 - Epoch: [103][  130/  246]    Overall Loss 0.232097    Objective Loss 0.232097                                        LR 0.000013    Time 0.027803    
2023-01-06 14:40:22,023 - Epoch: [103][  140/  246]    Overall Loss 0.233243    Objective Loss 0.233243                                        LR 0.000013    Time 0.027541    
2023-01-06 14:40:22,264 - Epoch: [103][  150/  246]    Overall Loss 0.232849    Objective Loss 0.232849                                        LR 0.000013    Time 0.027309    
2023-01-06 14:40:22,503 - Epoch: [103][  160/  246]    Overall Loss 0.234376    Objective Loss 0.234376                                        LR 0.000013    Time 0.027087    
2023-01-06 14:40:22,754 - Epoch: [103][  170/  246]    Overall Loss 0.234361    Objective Loss 0.234361                                        LR 0.000013    Time 0.026973    
2023-01-06 14:40:23,007 - Epoch: [103][  180/  246]    Overall Loss 0.234894    Objective Loss 0.234894                                        LR 0.000013    Time 0.026871    
2023-01-06 14:40:23,270 - Epoch: [103][  190/  246]    Overall Loss 0.234690    Objective Loss 0.234690                                        LR 0.000013    Time 0.026828    
2023-01-06 14:40:23,533 - Epoch: [103][  200/  246]    Overall Loss 0.234612    Objective Loss 0.234612                                        LR 0.000013    Time 0.026798    
2023-01-06 14:40:23,797 - Epoch: [103][  210/  246]    Overall Loss 0.234844    Objective Loss 0.234844                                        LR 0.000013    Time 0.026782    
2023-01-06 14:40:24,056 - Epoch: [103][  220/  246]    Overall Loss 0.235187    Objective Loss 0.235187                                        LR 0.000013    Time 0.026738    
2023-01-06 14:40:24,318 - Epoch: [103][  230/  246]    Overall Loss 0.235656    Objective Loss 0.235656                                        LR 0.000013    Time 0.026713    
2023-01-06 14:40:24,586 - Epoch: [103][  240/  246]    Overall Loss 0.236002    Objective Loss 0.236002                                        LR 0.000013    Time 0.026715    
2023-01-06 14:40:24,718 - Epoch: [103][  246/  246]    Overall Loss 0.236275    Objective Loss 0.236275    Top1 90.669856    LR 0.000013    Time 0.026600    
2023-01-06 14:40:24,853 - --- validate (epoch=103)-----------
2023-01-06 14:40:24,853 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:25,309 - Epoch: [103][   10/   28]    Loss 0.269568    Top1 90.664062    
2023-01-06 14:40:25,447 - Epoch: [103][   20/   28]    Loss 0.261370    Top1 90.839844    
2023-01-06 14:40:25,538 - Epoch: [103][   28/   28]    Loss 0.258499    Top1 90.681363    
2023-01-06 14:40:25,671 - ==> Top1: 90.681    Loss: 0.258

2023-01-06 14:40:25,671 - ==> Confusion:
[[ 227   18  194]
 [   9  273  320]
 [  47   63 5835]]

2023-01-06 14:40:25,673 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:40:25,673 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:25,682 - 

2023-01-06 14:40:25,682 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:26,427 - Epoch: [104][   10/  246]    Overall Loss 0.244406    Objective Loss 0.244406                                        LR 0.000013    Time 0.074420    
2023-01-06 14:40:26,675 - Epoch: [104][   20/  246]    Overall Loss 0.247415    Objective Loss 0.247415                                        LR 0.000013    Time 0.049553    
2023-01-06 14:40:26,926 - Epoch: [104][   30/  246]    Overall Loss 0.249356    Objective Loss 0.249356                                        LR 0.000013    Time 0.041399    
2023-01-06 14:40:27,173 - Epoch: [104][   40/  246]    Overall Loss 0.244369    Objective Loss 0.244369                                        LR 0.000013    Time 0.037191    
2023-01-06 14:40:27,416 - Epoch: [104][   50/  246]    Overall Loss 0.240196    Objective Loss 0.240196                                        LR 0.000013    Time 0.034607    
2023-01-06 14:40:27,665 - Epoch: [104][   60/  246]    Overall Loss 0.238550    Objective Loss 0.238550                                        LR 0.000013    Time 0.032986    
2023-01-06 14:40:27,907 - Epoch: [104][   70/  246]    Overall Loss 0.240391    Objective Loss 0.240391                                        LR 0.000013    Time 0.031733    
2023-01-06 14:40:28,151 - Epoch: [104][   80/  246]    Overall Loss 0.240434    Objective Loss 0.240434                                        LR 0.000013    Time 0.030804    
2023-01-06 14:40:28,394 - Epoch: [104][   90/  246]    Overall Loss 0.240352    Objective Loss 0.240352                                        LR 0.000013    Time 0.030077    
2023-01-06 14:40:28,636 - Epoch: [104][  100/  246]    Overall Loss 0.238782    Objective Loss 0.238782                                        LR 0.000013    Time 0.029489    
2023-01-06 14:40:28,878 - Epoch: [104][  110/  246]    Overall Loss 0.238676    Objective Loss 0.238676                                        LR 0.000013    Time 0.029007    
2023-01-06 14:40:29,123 - Epoch: [104][  120/  246]    Overall Loss 0.238516    Objective Loss 0.238516                                        LR 0.000013    Time 0.028625    
2023-01-06 14:40:29,364 - Epoch: [104][  130/  246]    Overall Loss 0.237058    Objective Loss 0.237058                                        LR 0.000013    Time 0.028275    
2023-01-06 14:40:29,606 - Epoch: [104][  140/  246]    Overall Loss 0.237755    Objective Loss 0.237755                                        LR 0.000013    Time 0.027981    
2023-01-06 14:40:29,857 - Epoch: [104][  150/  246]    Overall Loss 0.237353    Objective Loss 0.237353                                        LR 0.000013    Time 0.027784    
2023-01-06 14:40:30,108 - Epoch: [104][  160/  246]    Overall Loss 0.237328    Objective Loss 0.237328                                        LR 0.000013    Time 0.027619    
2023-01-06 14:40:30,359 - Epoch: [104][  170/  246]    Overall Loss 0.237018    Objective Loss 0.237018                                        LR 0.000013    Time 0.027469    
2023-01-06 14:40:30,609 - Epoch: [104][  180/  246]    Overall Loss 0.236806    Objective Loss 0.236806                                        LR 0.000013    Time 0.027330    
2023-01-06 14:40:30,863 - Epoch: [104][  190/  246]    Overall Loss 0.236623    Objective Loss 0.236623                                        LR 0.000013    Time 0.027226    
2023-01-06 14:40:31,097 - Epoch: [104][  200/  246]    Overall Loss 0.236829    Objective Loss 0.236829                                        LR 0.000013    Time 0.027033    
2023-01-06 14:40:31,318 - Epoch: [104][  210/  246]    Overall Loss 0.236206    Objective Loss 0.236206                                        LR 0.000013    Time 0.026795    
2023-01-06 14:40:31,535 - Epoch: [104][  220/  246]    Overall Loss 0.236102    Objective Loss 0.236102                                        LR 0.000013    Time 0.026562    
2023-01-06 14:40:31,753 - Epoch: [104][  230/  246]    Overall Loss 0.235630    Objective Loss 0.235630                                        LR 0.000013    Time 0.026353    
2023-01-06 14:40:31,983 - Epoch: [104][  240/  246]    Overall Loss 0.235660    Objective Loss 0.235660                                        LR 0.000013    Time 0.026213    
2023-01-06 14:40:32,107 - Epoch: [104][  246/  246]    Overall Loss 0.235050    Objective Loss 0.235050    Top1 92.583732    LR 0.000013    Time 0.026076    
2023-01-06 14:40:32,250 - --- validate (epoch=104)-----------
2023-01-06 14:40:32,250 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:32,704 - Epoch: [104][   10/   28]    Loss 0.278963    Top1 90.195312    
2023-01-06 14:40:32,843 - Epoch: [104][   20/   28]    Loss 0.255634    Top1 90.644531    
2023-01-06 14:40:32,935 - Epoch: [104][   28/   28]    Loss 0.258413    Top1 90.552534    
2023-01-06 14:40:33,070 - ==> Top1: 90.553    Loss: 0.258

2023-01-06 14:40:33,071 - ==> Confusion:
[[ 227   15  197]
 [  11  294  297]
 [  58   82 5805]]

2023-01-06 14:40:33,072 - ==> Best [Top1: 90.925   Sparsity:0.00   Params: 360896 on epoch: 95]
2023-01-06 14:40:33,072 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:33,082 - 

2023-01-06 14:40:33,082 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:33,678 - Epoch: [105][   10/  246]    Overall Loss 0.240462    Objective Loss 0.240462                                        LR 0.000013    Time 0.059466    
2023-01-06 14:40:33,925 - Epoch: [105][   20/  246]    Overall Loss 0.240060    Objective Loss 0.240060                                        LR 0.000013    Time 0.042070    
2023-01-06 14:40:34,180 - Epoch: [105][   30/  246]    Overall Loss 0.236572    Objective Loss 0.236572                                        LR 0.000013    Time 0.036540    
2023-01-06 14:40:34,440 - Epoch: [105][   40/  246]    Overall Loss 0.232176    Objective Loss 0.232176                                        LR 0.000013    Time 0.033832    
2023-01-06 14:40:34,707 - Epoch: [105][   50/  246]    Overall Loss 0.236026    Objective Loss 0.236026                                        LR 0.000013    Time 0.032404    
2023-01-06 14:40:34,978 - Epoch: [105][   60/  246]    Overall Loss 0.237220    Objective Loss 0.237220                                        LR 0.000013    Time 0.031514    
2023-01-06 14:40:35,241 - Epoch: [105][   70/  246]    Overall Loss 0.235862    Objective Loss 0.235862                                        LR 0.000013    Time 0.030765    
2023-01-06 14:40:35,507 - Epoch: [105][   80/  246]    Overall Loss 0.234185    Objective Loss 0.234185                                        LR 0.000013    Time 0.030234    
2023-01-06 14:40:35,779 - Epoch: [105][   90/  246]    Overall Loss 0.232433    Objective Loss 0.232433                                        LR 0.000013    Time 0.029884    
2023-01-06 14:40:36,049 - Epoch: [105][  100/  246]    Overall Loss 0.233652    Objective Loss 0.233652                                        LR 0.000013    Time 0.029588    
2023-01-06 14:40:36,300 - Epoch: [105][  110/  246]    Overall Loss 0.231195    Objective Loss 0.231195                                        LR 0.000013    Time 0.029177    
2023-01-06 14:40:36,553 - Epoch: [105][  120/  246]    Overall Loss 0.230836    Objective Loss 0.230836                                        LR 0.000013    Time 0.028848    
2023-01-06 14:40:36,804 - Epoch: [105][  130/  246]    Overall Loss 0.230330    Objective Loss 0.230330                                        LR 0.000013    Time 0.028548    
2023-01-06 14:40:37,052 - Epoch: [105][  140/  246]    Overall Loss 0.232396    Objective Loss 0.232396                                        LR 0.000013    Time 0.028277    
2023-01-06 14:40:37,296 - Epoch: [105][  150/  246]    Overall Loss 0.232916    Objective Loss 0.232916                                        LR 0.000013    Time 0.028014    
2023-01-06 14:40:37,537 - Epoch: [105][  160/  246]    Overall Loss 0.232577    Objective Loss 0.232577                                        LR 0.000013    Time 0.027767    
2023-01-06 14:40:37,777 - Epoch: [105][  170/  246]    Overall Loss 0.232996    Objective Loss 0.232996                                        LR 0.000013    Time 0.027538    
2023-01-06 14:40:38,023 - Epoch: [105][  180/  246]    Overall Loss 0.233252    Objective Loss 0.233252                                        LR 0.000013    Time 0.027364    
2023-01-06 14:40:38,267 - Epoch: [105][  190/  246]    Overall Loss 0.233732    Objective Loss 0.233732                                        LR 0.000013    Time 0.027205    
2023-01-06 14:40:38,518 - Epoch: [105][  200/  246]    Overall Loss 0.232932    Objective Loss 0.232932                                        LR 0.000013    Time 0.027100    
2023-01-06 14:40:38,763 - Epoch: [105][  210/  246]    Overall Loss 0.233753    Objective Loss 0.233753                                        LR 0.000013    Time 0.026975    
2023-01-06 14:40:39,014 - Epoch: [105][  220/  246]    Overall Loss 0.234049    Objective Loss 0.234049                                        LR 0.000013    Time 0.026887    
2023-01-06 14:40:39,264 - Epoch: [105][  230/  246]    Overall Loss 0.234687    Objective Loss 0.234687                                        LR 0.000013    Time 0.026805    
2023-01-06 14:40:39,531 - Epoch: [105][  240/  246]    Overall Loss 0.234994    Objective Loss 0.234994                                        LR 0.000013    Time 0.026795    
2023-01-06 14:40:39,661 - Epoch: [105][  246/  246]    Overall Loss 0.235237    Objective Loss 0.235237    Top1 90.669856    LR 0.000013    Time 0.026673    
2023-01-06 14:40:39,792 - --- validate (epoch=105)-----------
2023-01-06 14:40:39,792 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:40,250 - Epoch: [105][   10/   28]    Loss 0.243923    Top1 91.328125    
2023-01-06 14:40:40,402 - Epoch: [105][   20/   28]    Loss 0.250901    Top1 91.191406    
2023-01-06 14:40:40,493 - Epoch: [105][   28/   28]    Loss 0.253713    Top1 90.939021    
2023-01-06 14:40:40,635 - ==> Top1: 90.939    Loss: 0.254

2023-01-06 14:40:40,635 - ==> Confusion:
[[ 253   12  174]
 [  15  298  289]
 [  65   78 5802]]

2023-01-06 14:40:40,637 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:40:40,637 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:40,658 - 

2023-01-06 14:40:40,658 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:41,387 - Epoch: [106][   10/  246]    Overall Loss 0.218566    Objective Loss 0.218566                                        LR 0.000013    Time 0.072895    
2023-01-06 14:40:41,635 - Epoch: [106][   20/  246]    Overall Loss 0.232060    Objective Loss 0.232060                                        LR 0.000013    Time 0.048812    
2023-01-06 14:40:41,886 - Epoch: [106][   30/  246]    Overall Loss 0.227073    Objective Loss 0.227073                                        LR 0.000013    Time 0.040873    
2023-01-06 14:40:42,139 - Epoch: [106][   40/  246]    Overall Loss 0.229235    Objective Loss 0.229235                                        LR 0.000013    Time 0.036971    
2023-01-06 14:40:42,384 - Epoch: [106][   50/  246]    Overall Loss 0.228215    Objective Loss 0.228215                                        LR 0.000013    Time 0.034481    
2023-01-06 14:40:42,637 - Epoch: [106][   60/  246]    Overall Loss 0.226167    Objective Loss 0.226167                                        LR 0.000013    Time 0.032941    
2023-01-06 14:40:42,894 - Epoch: [106][   70/  246]    Overall Loss 0.225902    Objective Loss 0.225902                                        LR 0.000013    Time 0.031890    
2023-01-06 14:40:43,149 - Epoch: [106][   80/  246]    Overall Loss 0.227399    Objective Loss 0.227399                                        LR 0.000013    Time 0.031089    
2023-01-06 14:40:43,402 - Epoch: [106][   90/  246]    Overall Loss 0.228159    Objective Loss 0.228159                                        LR 0.000013    Time 0.030436    
2023-01-06 14:40:43,664 - Epoch: [106][  100/  246]    Overall Loss 0.229396    Objective Loss 0.229396                                        LR 0.000013    Time 0.030005    
2023-01-06 14:40:43,927 - Epoch: [106][  110/  246]    Overall Loss 0.229236    Objective Loss 0.229236                                        LR 0.000013    Time 0.029665    
2023-01-06 14:40:44,194 - Epoch: [106][  120/  246]    Overall Loss 0.228533    Objective Loss 0.228533                                        LR 0.000013    Time 0.029410    
2023-01-06 14:40:44,452 - Epoch: [106][  130/  246]    Overall Loss 0.229728    Objective Loss 0.229728                                        LR 0.000013    Time 0.029131    
2023-01-06 14:40:44,709 - Epoch: [106][  140/  246]    Overall Loss 0.230581    Objective Loss 0.230581                                        LR 0.000013    Time 0.028870    
2023-01-06 14:40:44,975 - Epoch: [106][  150/  246]    Overall Loss 0.231687    Objective Loss 0.231687                                        LR 0.000013    Time 0.028717    
2023-01-06 14:40:45,240 - Epoch: [106][  160/  246]    Overall Loss 0.231774    Objective Loss 0.231774                                        LR 0.000013    Time 0.028573    
2023-01-06 14:40:45,505 - Epoch: [106][  170/  246]    Overall Loss 0.233572    Objective Loss 0.233572                                        LR 0.000013    Time 0.028451    
2023-01-06 14:40:45,732 - Epoch: [106][  180/  246]    Overall Loss 0.235034    Objective Loss 0.235034                                        LR 0.000013    Time 0.028126    
2023-01-06 14:40:45,952 - Epoch: [106][  190/  246]    Overall Loss 0.234538    Objective Loss 0.234538                                        LR 0.000013    Time 0.027801    
2023-01-06 14:40:46,201 - Epoch: [106][  200/  246]    Overall Loss 0.235415    Objective Loss 0.235415                                        LR 0.000013    Time 0.027654    
2023-01-06 14:40:46,464 - Epoch: [106][  210/  246]    Overall Loss 0.236198    Objective Loss 0.236198                                        LR 0.000013    Time 0.027589    
2023-01-06 14:40:46,730 - Epoch: [106][  220/  246]    Overall Loss 0.235981    Objective Loss 0.235981                                        LR 0.000013    Time 0.027543    
2023-01-06 14:40:46,990 - Epoch: [106][  230/  246]    Overall Loss 0.236242    Objective Loss 0.236242                                        LR 0.000013    Time 0.027474    
2023-01-06 14:40:47,261 - Epoch: [106][  240/  246]    Overall Loss 0.235540    Objective Loss 0.235540                                        LR 0.000013    Time 0.027458    
2023-01-06 14:40:47,385 - Epoch: [106][  246/  246]    Overall Loss 0.234996    Objective Loss 0.234996    Top1 93.301435    LR 0.000013    Time 0.027292    
2023-01-06 14:40:47,514 - --- validate (epoch=106)-----------
2023-01-06 14:40:47,514 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:47,955 - Epoch: [106][   10/   28]    Loss 0.254914    Top1 90.312500    
2023-01-06 14:40:48,096 - Epoch: [106][   20/   28]    Loss 0.260710    Top1 90.332031    
2023-01-06 14:40:48,188 - Epoch: [106][   28/   28]    Loss 0.258727    Top1 90.323504    
2023-01-06 14:40:48,324 - ==> Top1: 90.324    Loss: 0.259

2023-01-06 14:40:48,325 - ==> Confusion:
[[ 237   15  187]
 [  21  280  301]
 [  72   80 5793]]

2023-01-06 14:40:48,326 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:40:48,326 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:48,336 - 

2023-01-06 14:40:48,337 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:48,941 - Epoch: [107][   10/  246]    Overall Loss 0.234320    Objective Loss 0.234320                                        LR 0.000013    Time 0.060416    
2023-01-06 14:40:49,195 - Epoch: [107][   20/  246]    Overall Loss 0.241254    Objective Loss 0.241254                                        LR 0.000013    Time 0.042879    
2023-01-06 14:40:49,452 - Epoch: [107][   30/  246]    Overall Loss 0.241623    Objective Loss 0.241623                                        LR 0.000013    Time 0.037116    
2023-01-06 14:40:49,714 - Epoch: [107][   40/  246]    Overall Loss 0.241181    Objective Loss 0.241181                                        LR 0.000013    Time 0.034380    
2023-01-06 14:40:49,969 - Epoch: [107][   50/  246]    Overall Loss 0.236868    Objective Loss 0.236868                                        LR 0.000013    Time 0.032596    
2023-01-06 14:40:50,220 - Epoch: [107][   60/  246]    Overall Loss 0.238435    Objective Loss 0.238435                                        LR 0.000013    Time 0.031345    
2023-01-06 14:40:50,481 - Epoch: [107][   70/  246]    Overall Loss 0.238673    Objective Loss 0.238673                                        LR 0.000013    Time 0.030582    
2023-01-06 14:40:50,732 - Epoch: [107][   80/  246]    Overall Loss 0.237987    Objective Loss 0.237987                                        LR 0.000013    Time 0.029896    
2023-01-06 14:40:51,002 - Epoch: [107][   90/  246]    Overall Loss 0.237216    Objective Loss 0.237216                                        LR 0.000013    Time 0.029575    
2023-01-06 14:40:51,254 - Epoch: [107][  100/  246]    Overall Loss 0.235504    Objective Loss 0.235504                                        LR 0.000013    Time 0.029127    
2023-01-06 14:40:51,515 - Epoch: [107][  110/  246]    Overall Loss 0.238217    Objective Loss 0.238217                                        LR 0.000013    Time 0.028832    
2023-01-06 14:40:51,763 - Epoch: [107][  120/  246]    Overall Loss 0.238821    Objective Loss 0.238821                                        LR 0.000013    Time 0.028479    
2023-01-06 14:40:51,983 - Epoch: [107][  130/  246]    Overall Loss 0.238981    Objective Loss 0.238981                                        LR 0.000013    Time 0.027979    
2023-01-06 14:40:52,220 - Epoch: [107][  140/  246]    Overall Loss 0.239214    Objective Loss 0.239214                                        LR 0.000013    Time 0.027669    
2023-01-06 14:40:52,455 - Epoch: [107][  150/  246]    Overall Loss 0.238958    Objective Loss 0.238958                                        LR 0.000013    Time 0.027389    
2023-01-06 14:40:52,690 - Epoch: [107][  160/  246]    Overall Loss 0.238019    Objective Loss 0.238019                                        LR 0.000013    Time 0.027142    
2023-01-06 14:40:52,922 - Epoch: [107][  170/  246]    Overall Loss 0.237644    Objective Loss 0.237644                                        LR 0.000013    Time 0.026913    
2023-01-06 14:40:53,153 - Epoch: [107][  180/  246]    Overall Loss 0.236516    Objective Loss 0.236516                                        LR 0.000013    Time 0.026694    
2023-01-06 14:40:53,386 - Epoch: [107][  190/  246]    Overall Loss 0.236680    Objective Loss 0.236680                                        LR 0.000013    Time 0.026517    
2023-01-06 14:40:53,622 - Epoch: [107][  200/  246]    Overall Loss 0.236179    Objective Loss 0.236179                                        LR 0.000013    Time 0.026368    
2023-01-06 14:40:53,850 - Epoch: [107][  210/  246]    Overall Loss 0.236341    Objective Loss 0.236341                                        LR 0.000013    Time 0.026194    
2023-01-06 14:40:54,084 - Epoch: [107][  220/  246]    Overall Loss 0.236088    Objective Loss 0.236088                                        LR 0.000013    Time 0.026067    
2023-01-06 14:40:54,312 - Epoch: [107][  230/  246]    Overall Loss 0.235428    Objective Loss 0.235428                                        LR 0.000013    Time 0.025922    
2023-01-06 14:40:54,559 - Epoch: [107][  240/  246]    Overall Loss 0.235235    Objective Loss 0.235235                                        LR 0.000013    Time 0.025869    
2023-01-06 14:40:54,683 - Epoch: [107][  246/  246]    Overall Loss 0.234898    Objective Loss 0.234898    Top1 92.105263    LR 0.000013    Time 0.025744    
2023-01-06 14:40:54,834 - --- validate (epoch=107)-----------
2023-01-06 14:40:54,834 - 6986 samples (256 per mini-batch)
2023-01-06 14:40:55,285 - Epoch: [107][   10/   28]    Loss 0.245110    Top1 90.507812    
2023-01-06 14:40:55,439 - Epoch: [107][   20/   28]    Loss 0.257518    Top1 90.820312    
2023-01-06 14:40:55,529 - Epoch: [107][   28/   28]    Loss 0.259920    Top1 90.781563    
2023-01-06 14:40:55,645 - ==> Top1: 90.782    Loss: 0.260

2023-01-06 14:40:55,645 - ==> Confusion:
[[ 242   16  181]
 [   7  290  305]
 [  66   69 5810]]

2023-01-06 14:40:55,647 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:40:55,647 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:40:55,657 - 

2023-01-06 14:40:55,657 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:40:56,395 - Epoch: [108][   10/  246]    Overall Loss 0.215052    Objective Loss 0.215052                                        LR 0.000013    Time 0.073760    
2023-01-06 14:40:56,651 - Epoch: [108][   20/  246]    Overall Loss 0.231517    Objective Loss 0.231517                                        LR 0.000013    Time 0.049648    
2023-01-06 14:40:56,905 - Epoch: [108][   30/  246]    Overall Loss 0.231923    Objective Loss 0.231923                                        LR 0.000013    Time 0.041552    
2023-01-06 14:40:57,157 - Epoch: [108][   40/  246]    Overall Loss 0.232100    Objective Loss 0.232100                                        LR 0.000013    Time 0.037453    
2023-01-06 14:40:57,408 - Epoch: [108][   50/  246]    Overall Loss 0.233261    Objective Loss 0.233261                                        LR 0.000013    Time 0.034972    
2023-01-06 14:40:57,657 - Epoch: [108][   60/  246]    Overall Loss 0.228797    Objective Loss 0.228797                                        LR 0.000013    Time 0.033283    
2023-01-06 14:40:57,907 - Epoch: [108][   70/  246]    Overall Loss 0.228352    Objective Loss 0.228352                                        LR 0.000013    Time 0.032094    
2023-01-06 14:40:58,166 - Epoch: [108][   80/  246]    Overall Loss 0.228593    Objective Loss 0.228593                                        LR 0.000013    Time 0.031300    
2023-01-06 14:40:58,423 - Epoch: [108][   90/  246]    Overall Loss 0.229373    Objective Loss 0.229373                                        LR 0.000013    Time 0.030657    
2023-01-06 14:40:58,682 - Epoch: [108][  100/  246]    Overall Loss 0.230372    Objective Loss 0.230372                                        LR 0.000013    Time 0.030175    
2023-01-06 14:40:58,941 - Epoch: [108][  110/  246]    Overall Loss 0.230650    Objective Loss 0.230650                                        LR 0.000013    Time 0.029777    
2023-01-06 14:40:59,198 - Epoch: [108][  120/  246]    Overall Loss 0.229969    Objective Loss 0.229969                                        LR 0.000013    Time 0.029434    
2023-01-06 14:40:59,455 - Epoch: [108][  130/  246]    Overall Loss 0.230819    Objective Loss 0.230819                                        LR 0.000013    Time 0.029131    
2023-01-06 14:40:59,714 - Epoch: [108][  140/  246]    Overall Loss 0.230479    Objective Loss 0.230479                                        LR 0.000013    Time 0.028896    
2023-01-06 14:40:59,975 - Epoch: [108][  150/  246]    Overall Loss 0.231862    Objective Loss 0.231862                                        LR 0.000013    Time 0.028705    
2023-01-06 14:41:00,250 - Epoch: [108][  160/  246]    Overall Loss 0.231856    Objective Loss 0.231856                                        LR 0.000013    Time 0.028627    
2023-01-06 14:41:00,524 - Epoch: [108][  170/  246]    Overall Loss 0.231385    Objective Loss 0.231385                                        LR 0.000013    Time 0.028554    
2023-01-06 14:41:00,813 - Epoch: [108][  180/  246]    Overall Loss 0.231764    Objective Loss 0.231764                                        LR 0.000013    Time 0.028571    
2023-01-06 14:41:01,095 - Epoch: [108][  190/  246]    Overall Loss 0.231680    Objective Loss 0.231680                                        LR 0.000013    Time 0.028548    
2023-01-06 14:41:01,359 - Epoch: [108][  200/  246]    Overall Loss 0.231984    Objective Loss 0.231984                                        LR 0.000013    Time 0.028438    
2023-01-06 14:41:01,619 - Epoch: [108][  210/  246]    Overall Loss 0.232642    Objective Loss 0.232642                                        LR 0.000013    Time 0.028320    
2023-01-06 14:41:01,860 - Epoch: [108][  220/  246]    Overall Loss 0.233764    Objective Loss 0.233764                                        LR 0.000013    Time 0.028127    
2023-01-06 14:41:02,094 - Epoch: [108][  230/  246]    Overall Loss 0.234278    Objective Loss 0.234278                                        LR 0.000013    Time 0.027921    
2023-01-06 14:41:02,342 - Epoch: [108][  240/  246]    Overall Loss 0.234816    Objective Loss 0.234816                                        LR 0.000013    Time 0.027791    
2023-01-06 14:41:02,471 - Epoch: [108][  246/  246]    Overall Loss 0.234433    Objective Loss 0.234433    Top1 93.062201    LR 0.000013    Time 0.027637    
2023-01-06 14:41:02,624 - --- validate (epoch=108)-----------
2023-01-06 14:41:02,624 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:03,093 - Epoch: [108][   10/   28]    Loss 0.264998    Top1 90.312500    
2023-01-06 14:41:03,243 - Epoch: [108][   20/   28]    Loss 0.265660    Top1 90.429688    
2023-01-06 14:41:03,333 - Epoch: [108][   28/   28]    Loss 0.252437    Top1 90.896078    
2023-01-06 14:41:03,467 - ==> Top1: 90.896    Loss: 0.252

2023-01-06 14:41:03,467 - ==> Confusion:
[[ 259   15  165]
 [  14  293  295]
 [  69   78 5798]]

2023-01-06 14:41:03,469 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:03,469 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:03,479 - 

2023-01-06 14:41:03,479 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:04,208 - Epoch: [109][   10/  246]    Overall Loss 0.243294    Objective Loss 0.243294                                        LR 0.000013    Time 0.072822    
2023-01-06 14:41:04,430 - Epoch: [109][   20/  246]    Overall Loss 0.238712    Objective Loss 0.238712                                        LR 0.000013    Time 0.047492    
2023-01-06 14:41:04,666 - Epoch: [109][   30/  246]    Overall Loss 0.237041    Objective Loss 0.237041                                        LR 0.000013    Time 0.039519    
2023-01-06 14:41:04,900 - Epoch: [109][   40/  246]    Overall Loss 0.236789    Objective Loss 0.236789                                        LR 0.000013    Time 0.035444    
2023-01-06 14:41:05,132 - Epoch: [109][   50/  246]    Overall Loss 0.236405    Objective Loss 0.236405                                        LR 0.000013    Time 0.032981    
2023-01-06 14:41:05,353 - Epoch: [109][   60/  246]    Overall Loss 0.236531    Objective Loss 0.236531                                        LR 0.000013    Time 0.031170    
2023-01-06 14:41:05,590 - Epoch: [109][   70/  246]    Overall Loss 0.234256    Objective Loss 0.234256                                        LR 0.000013    Time 0.030085    
2023-01-06 14:41:05,824 - Epoch: [109][   80/  246]    Overall Loss 0.237230    Objective Loss 0.237230                                        LR 0.000013    Time 0.029248    
2023-01-06 14:41:06,069 - Epoch: [109][   90/  246]    Overall Loss 0.236697    Objective Loss 0.236697                                        LR 0.000013    Time 0.028715    
2023-01-06 14:41:06,293 - Epoch: [109][  100/  246]    Overall Loss 0.234808    Objective Loss 0.234808                                        LR 0.000013    Time 0.028081    
2023-01-06 14:41:06,508 - Epoch: [109][  110/  246]    Overall Loss 0.234459    Objective Loss 0.234459                                        LR 0.000013    Time 0.027483    
2023-01-06 14:41:06,722 - Epoch: [109][  120/  246]    Overall Loss 0.233254    Objective Loss 0.233254                                        LR 0.000013    Time 0.026974    
2023-01-06 14:41:06,943 - Epoch: [109][  130/  246]    Overall Loss 0.233186    Objective Loss 0.233186                                        LR 0.000013    Time 0.026594    
2023-01-06 14:41:07,167 - Epoch: [109][  140/  246]    Overall Loss 0.233027    Objective Loss 0.233027                                        LR 0.000013    Time 0.026288    
2023-01-06 14:41:07,380 - Epoch: [109][  150/  246]    Overall Loss 0.233177    Objective Loss 0.233177                                        LR 0.000013    Time 0.025948    
2023-01-06 14:41:07,597 - Epoch: [109][  160/  246]    Overall Loss 0.233482    Objective Loss 0.233482                                        LR 0.000013    Time 0.025680    
2023-01-06 14:41:07,810 - Epoch: [109][  170/  246]    Overall Loss 0.233332    Objective Loss 0.233332                                        LR 0.000013    Time 0.025416    
2023-01-06 14:41:08,025 - Epoch: [109][  180/  246]    Overall Loss 0.234135    Objective Loss 0.234135                                        LR 0.000013    Time 0.025196    
2023-01-06 14:41:08,239 - Epoch: [109][  190/  246]    Overall Loss 0.233253    Objective Loss 0.233253                                        LR 0.000013    Time 0.024997    
2023-01-06 14:41:08,456 - Epoch: [109][  200/  246]    Overall Loss 0.232556    Objective Loss 0.232556                                        LR 0.000013    Time 0.024827    
2023-01-06 14:41:08,668 - Epoch: [109][  210/  246]    Overall Loss 0.232107    Objective Loss 0.232107                                        LR 0.000013    Time 0.024656    
2023-01-06 14:41:08,890 - Epoch: [109][  220/  246]    Overall Loss 0.231698    Objective Loss 0.231698                                        LR 0.000013    Time 0.024539    
2023-01-06 14:41:09,105 - Epoch: [109][  230/  246]    Overall Loss 0.231805    Objective Loss 0.231805                                        LR 0.000013    Time 0.024406    
2023-01-06 14:41:09,334 - Epoch: [109][  240/  246]    Overall Loss 0.231989    Objective Loss 0.231989                                        LR 0.000013    Time 0.024343    
2023-01-06 14:41:09,457 - Epoch: [109][  246/  246]    Overall Loss 0.232808    Objective Loss 0.232808    Top1 90.430622    LR 0.000013    Time 0.024250    
2023-01-06 14:41:09,590 - --- validate (epoch=109)-----------
2023-01-06 14:41:09,590 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:10,037 - Epoch: [109][   10/   28]    Loss 0.261361    Top1 90.546875    
2023-01-06 14:41:10,195 - Epoch: [109][   20/   28]    Loss 0.252976    Top1 91.171875    
2023-01-06 14:41:10,285 - Epoch: [109][   28/   28]    Loss 0.260624    Top1 90.896078    
2023-01-06 14:41:10,419 - ==> Top1: 90.896    Loss: 0.261

2023-01-06 14:41:10,419 - ==> Confusion:
[[ 265   10  164]
 [  17  312  273]
 [  74   98 5773]]

2023-01-06 14:41:10,421 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:10,421 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:10,431 - 

2023-01-06 14:41:10,431 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:11,006 - Epoch: [110][   10/  246]    Overall Loss 0.240161    Objective Loss 0.240161                                        LR 0.000013    Time 0.057499    
2023-01-06 14:41:11,227 - Epoch: [110][   20/  246]    Overall Loss 0.240476    Objective Loss 0.240476                                        LR 0.000013    Time 0.039744    
2023-01-06 14:41:11,444 - Epoch: [110][   30/  246]    Overall Loss 0.241151    Objective Loss 0.241151                                        LR 0.000013    Time 0.033711    
2023-01-06 14:41:11,660 - Epoch: [110][   40/  246]    Overall Loss 0.241603    Objective Loss 0.241603                                        LR 0.000013    Time 0.030693    
2023-01-06 14:41:11,877 - Epoch: [110][   50/  246]    Overall Loss 0.243872    Objective Loss 0.243872                                        LR 0.000013    Time 0.028887    
2023-01-06 14:41:12,105 - Epoch: [110][   60/  246]    Overall Loss 0.244957    Objective Loss 0.244957                                        LR 0.000013    Time 0.027861    
2023-01-06 14:41:12,339 - Epoch: [110][   70/  246]    Overall Loss 0.244385    Objective Loss 0.244385                                        LR 0.000013    Time 0.027217    
2023-01-06 14:41:12,571 - Epoch: [110][   80/  246]    Overall Loss 0.243274    Objective Loss 0.243274                                        LR 0.000013    Time 0.026706    
2023-01-06 14:41:12,797 - Epoch: [110][   90/  246]    Overall Loss 0.240492    Objective Loss 0.240492                                        LR 0.000013    Time 0.026246    
2023-01-06 14:41:13,023 - Epoch: [110][  100/  246]    Overall Loss 0.238796    Objective Loss 0.238796                                        LR 0.000013    Time 0.025879    
2023-01-06 14:41:13,254 - Epoch: [110][  110/  246]    Overall Loss 0.237779    Objective Loss 0.237779                                        LR 0.000013    Time 0.025623    
2023-01-06 14:41:13,505 - Epoch: [110][  120/  246]    Overall Loss 0.236792    Objective Loss 0.236792                                        LR 0.000013    Time 0.025574    
2023-01-06 14:41:13,750 - Epoch: [110][  130/  246]    Overall Loss 0.235066    Objective Loss 0.235066                                        LR 0.000013    Time 0.025493    
2023-01-06 14:41:13,996 - Epoch: [110][  140/  246]    Overall Loss 0.234283    Objective Loss 0.234283                                        LR 0.000013    Time 0.025424    
2023-01-06 14:41:14,242 - Epoch: [110][  150/  246]    Overall Loss 0.234537    Objective Loss 0.234537                                        LR 0.000013    Time 0.025367    
2023-01-06 14:41:14,477 - Epoch: [110][  160/  246]    Overall Loss 0.233583    Objective Loss 0.233583                                        LR 0.000013    Time 0.025252    
2023-01-06 14:41:14,716 - Epoch: [110][  170/  246]    Overall Loss 0.232900    Objective Loss 0.232900                                        LR 0.000013    Time 0.025169    
2023-01-06 14:41:14,962 - Epoch: [110][  180/  246]    Overall Loss 0.232740    Objective Loss 0.232740                                        LR 0.000013    Time 0.025124    
2023-01-06 14:41:15,181 - Epoch: [110][  190/  246]    Overall Loss 0.232396    Objective Loss 0.232396                                        LR 0.000013    Time 0.024955    
2023-01-06 14:41:15,408 - Epoch: [110][  200/  246]    Overall Loss 0.232980    Objective Loss 0.232980                                        LR 0.000013    Time 0.024837    
2023-01-06 14:41:15,639 - Epoch: [110][  210/  246]    Overall Loss 0.233113    Objective Loss 0.233113                                        LR 0.000013    Time 0.024754    
2023-01-06 14:41:15,872 - Epoch: [110][  220/  246]    Overall Loss 0.232773    Objective Loss 0.232773                                        LR 0.000013    Time 0.024684    
2023-01-06 14:41:16,106 - Epoch: [110][  230/  246]    Overall Loss 0.233007    Objective Loss 0.233007                                        LR 0.000013    Time 0.024627    
2023-01-06 14:41:16,357 - Epoch: [110][  240/  246]    Overall Loss 0.233112    Objective Loss 0.233112                                        LR 0.000013    Time 0.024648    
2023-01-06 14:41:16,486 - Epoch: [110][  246/  246]    Overall Loss 0.233422    Objective Loss 0.233422    Top1 92.105263    LR 0.000013    Time 0.024569    
2023-01-06 14:41:16,629 - --- validate (epoch=110)-----------
2023-01-06 14:41:16,630 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:17,072 - Epoch: [110][   10/   28]    Loss 0.244312    Top1 91.210938    
2023-01-06 14:41:17,215 - Epoch: [110][   20/   28]    Loss 0.241873    Top1 91.054688    
2023-01-06 14:41:17,305 - Epoch: [110][   28/   28]    Loss 0.254114    Top1 90.624105    
2023-01-06 14:41:17,442 - ==> Top1: 90.624    Loss: 0.254

2023-01-06 14:41:17,443 - ==> Confusion:
[[ 244   16  179]
 [  14  298  290]
 [  64   92 5789]]

2023-01-06 14:41:17,444 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:17,444 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:17,454 - 

2023-01-06 14:41:17,454 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:18,170 - Epoch: [111][   10/  246]    Overall Loss 0.218121    Objective Loss 0.218121                                        LR 0.000013    Time 0.071491    
2023-01-06 14:41:18,387 - Epoch: [111][   20/  246]    Overall Loss 0.233288    Objective Loss 0.233288                                        LR 0.000013    Time 0.046587    
2023-01-06 14:41:18,621 - Epoch: [111][   30/  246]    Overall Loss 0.232080    Objective Loss 0.232080                                        LR 0.000013    Time 0.038839    
2023-01-06 14:41:18,854 - Epoch: [111][   40/  246]    Overall Loss 0.235687    Objective Loss 0.235687                                        LR 0.000013    Time 0.034912    
2023-01-06 14:41:19,079 - Epoch: [111][   50/  246]    Overall Loss 0.236205    Objective Loss 0.236205                                        LR 0.000013    Time 0.032421    
2023-01-06 14:41:19,292 - Epoch: [111][   60/  246]    Overall Loss 0.236169    Objective Loss 0.236169                                        LR 0.000013    Time 0.030565    
2023-01-06 14:41:19,506 - Epoch: [111][   70/  246]    Overall Loss 0.235317    Objective Loss 0.235317                                        LR 0.000013    Time 0.029253    
2023-01-06 14:41:19,721 - Epoch: [111][   80/  246]    Overall Loss 0.234997    Objective Loss 0.234997                                        LR 0.000013    Time 0.028276    
2023-01-06 14:41:19,936 - Epoch: [111][   90/  246]    Overall Loss 0.233107    Objective Loss 0.233107                                        LR 0.000013    Time 0.027524    
2023-01-06 14:41:20,159 - Epoch: [111][  100/  246]    Overall Loss 0.233937    Objective Loss 0.233937                                        LR 0.000013    Time 0.026993    
2023-01-06 14:41:20,388 - Epoch: [111][  110/  246]    Overall Loss 0.233842    Objective Loss 0.233842                                        LR 0.000013    Time 0.026612    
2023-01-06 14:41:20,603 - Epoch: [111][  120/  246]    Overall Loss 0.232927    Objective Loss 0.232927                                        LR 0.000013    Time 0.026186    
2023-01-06 14:41:20,819 - Epoch: [111][  130/  246]    Overall Loss 0.233222    Objective Loss 0.233222                                        LR 0.000013    Time 0.025828    
2023-01-06 14:41:21,040 - Epoch: [111][  140/  246]    Overall Loss 0.232720    Objective Loss 0.232720                                        LR 0.000013    Time 0.025559    
2023-01-06 14:41:21,262 - Epoch: [111][  150/  246]    Overall Loss 0.232492    Objective Loss 0.232492                                        LR 0.000013    Time 0.025337    
2023-01-06 14:41:21,477 - Epoch: [111][  160/  246]    Overall Loss 0.232418    Objective Loss 0.232418                                        LR 0.000013    Time 0.025092    
2023-01-06 14:41:21,694 - Epoch: [111][  170/  246]    Overall Loss 0.232033    Objective Loss 0.232033                                        LR 0.000013    Time 0.024891    
2023-01-06 14:41:21,908 - Epoch: [111][  180/  246]    Overall Loss 0.232522    Objective Loss 0.232522                                        LR 0.000013    Time 0.024694    
2023-01-06 14:41:22,124 - Epoch: [111][  190/  246]    Overall Loss 0.233274    Objective Loss 0.233274                                        LR 0.000013    Time 0.024531    
2023-01-06 14:41:22,348 - Epoch: [111][  200/  246]    Overall Loss 0.233121    Objective Loss 0.233121                                        LR 0.000013    Time 0.024421    
2023-01-06 14:41:22,575 - Epoch: [111][  210/  246]    Overall Loss 0.232929    Objective Loss 0.232929                                        LR 0.000013    Time 0.024339    
2023-01-06 14:41:22,809 - Epoch: [111][  220/  246]    Overall Loss 0.232110    Objective Loss 0.232110                                        LR 0.000013    Time 0.024292    
2023-01-06 14:41:23,037 - Epoch: [111][  230/  246]    Overall Loss 0.232538    Objective Loss 0.232538                                        LR 0.000013    Time 0.024225    
2023-01-06 14:41:23,272 - Epoch: [111][  240/  246]    Overall Loss 0.233467    Objective Loss 0.233467                                        LR 0.000013    Time 0.024195    
2023-01-06 14:41:23,393 - Epoch: [111][  246/  246]    Overall Loss 0.234315    Objective Loss 0.234315    Top1 91.866029    LR 0.000013    Time 0.024097    
2023-01-06 14:41:23,564 - --- validate (epoch=111)-----------
2023-01-06 14:41:23,564 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:24,004 - Epoch: [111][   10/   28]    Loss 0.276807    Top1 89.882812    
2023-01-06 14:41:24,143 - Epoch: [111][   20/   28]    Loss 0.257358    Top1 90.742188    
2023-01-06 14:41:24,234 - Epoch: [111][   28/   28]    Loss 0.259134    Top1 90.695677    
2023-01-06 14:41:24,363 - ==> Top1: 90.696    Loss: 0.259

2023-01-06 14:41:24,364 - ==> Confusion:
[[ 251   11  177]
 [  20  294  288]
 [  60   94 5791]]

2023-01-06 14:41:24,365 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:24,365 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:24,375 - 

2023-01-06 14:41:24,375 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:24,954 - Epoch: [112][   10/  246]    Overall Loss 0.236187    Objective Loss 0.236187                                        LR 0.000013    Time 0.057844    
2023-01-06 14:41:25,172 - Epoch: [112][   20/  246]    Overall Loss 0.227708    Objective Loss 0.227708                                        LR 0.000013    Time 0.039790    
2023-01-06 14:41:25,394 - Epoch: [112][   30/  246]    Overall Loss 0.232902    Objective Loss 0.232902                                        LR 0.000013    Time 0.033899    
2023-01-06 14:41:25,609 - Epoch: [112][   40/  246]    Overall Loss 0.234585    Objective Loss 0.234585                                        LR 0.000013    Time 0.030787    
2023-01-06 14:41:25,836 - Epoch: [112][   50/  246]    Overall Loss 0.235012    Objective Loss 0.235012                                        LR 0.000013    Time 0.029175    
2023-01-06 14:41:26,053 - Epoch: [112][   60/  246]    Overall Loss 0.236201    Objective Loss 0.236201                                        LR 0.000013    Time 0.027918    
2023-01-06 14:41:26,275 - Epoch: [112][   70/  246]    Overall Loss 0.236423    Objective Loss 0.236423                                        LR 0.000013    Time 0.027095    
2023-01-06 14:41:26,490 - Epoch: [112][   80/  246]    Overall Loss 0.235947    Objective Loss 0.235947                                        LR 0.000013    Time 0.026396    
2023-01-06 14:41:26,712 - Epoch: [112][   90/  246]    Overall Loss 0.235507    Objective Loss 0.235507                                        LR 0.000013    Time 0.025918    
2023-01-06 14:41:26,926 - Epoch: [112][  100/  246]    Overall Loss 0.237584    Objective Loss 0.237584                                        LR 0.000013    Time 0.025468    
2023-01-06 14:41:27,154 - Epoch: [112][  110/  246]    Overall Loss 0.237990    Objective Loss 0.237990                                        LR 0.000013    Time 0.025221    
2023-01-06 14:41:27,368 - Epoch: [112][  120/  246]    Overall Loss 0.237532    Objective Loss 0.237532                                        LR 0.000013    Time 0.024902    
2023-01-06 14:41:27,587 - Epoch: [112][  130/  246]    Overall Loss 0.237257    Objective Loss 0.237257                                        LR 0.000013    Time 0.024664    
2023-01-06 14:41:27,798 - Epoch: [112][  140/  246]    Overall Loss 0.237598    Objective Loss 0.237598                                        LR 0.000013    Time 0.024413    
2023-01-06 14:41:28,016 - Epoch: [112][  150/  246]    Overall Loss 0.237994    Objective Loss 0.237994                                        LR 0.000013    Time 0.024232    
2023-01-06 14:41:28,229 - Epoch: [112][  160/  246]    Overall Loss 0.237663    Objective Loss 0.237663                                        LR 0.000013    Time 0.024047    
2023-01-06 14:41:28,447 - Epoch: [112][  170/  246]    Overall Loss 0.236588    Objective Loss 0.236588                                        LR 0.000013    Time 0.023910    
2023-01-06 14:41:28,664 - Epoch: [112][  180/  246]    Overall Loss 0.236472    Objective Loss 0.236472                                        LR 0.000013    Time 0.023786    
2023-01-06 14:41:28,885 - Epoch: [112][  190/  246]    Overall Loss 0.235765    Objective Loss 0.235765                                        LR 0.000013    Time 0.023695    
2023-01-06 14:41:29,111 - Epoch: [112][  200/  246]    Overall Loss 0.236500    Objective Loss 0.236500                                        LR 0.000013    Time 0.023639    
2023-01-06 14:41:29,334 - Epoch: [112][  210/  246]    Overall Loss 0.235834    Objective Loss 0.235834                                        LR 0.000013    Time 0.023567    
2023-01-06 14:41:29,583 - Epoch: [112][  220/  246]    Overall Loss 0.235636    Objective Loss 0.235636                                        LR 0.000013    Time 0.023627    
2023-01-06 14:41:29,833 - Epoch: [112][  230/  246]    Overall Loss 0.235128    Objective Loss 0.235128                                        LR 0.000013    Time 0.023681    
2023-01-06 14:41:30,098 - Epoch: [112][  240/  246]    Overall Loss 0.235134    Objective Loss 0.235134                                        LR 0.000013    Time 0.023796    
2023-01-06 14:41:30,227 - Epoch: [112][  246/  246]    Overall Loss 0.234802    Objective Loss 0.234802    Top1 89.952153    LR 0.000013    Time 0.023739    
2023-01-06 14:41:30,355 - --- validate (epoch=112)-----------
2023-01-06 14:41:30,355 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:30,811 - Epoch: [112][   10/   28]    Loss 0.255931    Top1 90.664062    
2023-01-06 14:41:30,956 - Epoch: [112][   20/   28]    Loss 0.255456    Top1 90.839844    
2023-01-06 14:41:31,049 - Epoch: [112][   28/   28]    Loss 0.256255    Top1 90.709991    
2023-01-06 14:41:31,189 - ==> Top1: 90.710    Loss: 0.256

2023-01-06 14:41:31,189 - ==> Confusion:
[[ 234   12  193]
 [  14  285  303]
 [  57   70 5818]]

2023-01-06 14:41:31,191 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:31,191 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:31,200 - 

2023-01-06 14:41:31,201 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:31,910 - Epoch: [113][   10/  246]    Overall Loss 0.202219    Objective Loss 0.202219                                        LR 0.000013    Time 0.070838    
2023-01-06 14:41:32,152 - Epoch: [113][   20/  246]    Overall Loss 0.217911    Objective Loss 0.217911                                        LR 0.000013    Time 0.047506    
2023-01-06 14:41:32,383 - Epoch: [113][   30/  246]    Overall Loss 0.226626    Objective Loss 0.226626                                        LR 0.000013    Time 0.039375    
2023-01-06 14:41:32,628 - Epoch: [113][   40/  246]    Overall Loss 0.226951    Objective Loss 0.226951                                        LR 0.000013    Time 0.035644    
2023-01-06 14:41:32,874 - Epoch: [113][   50/  246]    Overall Loss 0.229681    Objective Loss 0.229681                                        LR 0.000013    Time 0.033431    
2023-01-06 14:41:33,097 - Epoch: [113][   60/  246]    Overall Loss 0.229393    Objective Loss 0.229393                                        LR 0.000013    Time 0.031560    
2023-01-06 14:41:33,347 - Epoch: [113][   70/  246]    Overall Loss 0.227273    Objective Loss 0.227273                                        LR 0.000013    Time 0.030618    
2023-01-06 14:41:33,604 - Epoch: [113][   80/  246]    Overall Loss 0.229300    Objective Loss 0.229300                                        LR 0.000013    Time 0.029997    
2023-01-06 14:41:33,828 - Epoch: [113][   90/  246]    Overall Loss 0.230469    Objective Loss 0.230469                                        LR 0.000013    Time 0.029155    
2023-01-06 14:41:34,051 - Epoch: [113][  100/  246]    Overall Loss 0.230029    Objective Loss 0.230029                                        LR 0.000013    Time 0.028465    
2023-01-06 14:41:34,271 - Epoch: [113][  110/  246]    Overall Loss 0.231822    Objective Loss 0.231822                                        LR 0.000013    Time 0.027872    
2023-01-06 14:41:34,495 - Epoch: [113][  120/  246]    Overall Loss 0.230436    Objective Loss 0.230436                                        LR 0.000013    Time 0.027415    
2023-01-06 14:41:34,713 - Epoch: [113][  130/  246]    Overall Loss 0.230701    Objective Loss 0.230701                                        LR 0.000013    Time 0.026983    
2023-01-06 14:41:34,937 - Epoch: [113][  140/  246]    Overall Loss 0.231213    Objective Loss 0.231213                                        LR 0.000013    Time 0.026638    
2023-01-06 14:41:35,156 - Epoch: [113][  150/  246]    Overall Loss 0.232331    Objective Loss 0.232331                                        LR 0.000013    Time 0.026323    
2023-01-06 14:41:35,381 - Epoch: [113][  160/  246]    Overall Loss 0.232434    Objective Loss 0.232434                                        LR 0.000013    Time 0.026080    
2023-01-06 14:41:35,603 - Epoch: [113][  170/  246]    Overall Loss 0.232897    Objective Loss 0.232897                                        LR 0.000013    Time 0.025850    
2023-01-06 14:41:35,852 - Epoch: [113][  180/  246]    Overall Loss 0.233075    Objective Loss 0.233075                                        LR 0.000013    Time 0.025799    
2023-01-06 14:41:36,095 - Epoch: [113][  190/  246]    Overall Loss 0.232261    Objective Loss 0.232261                                        LR 0.000013    Time 0.025719    
2023-01-06 14:41:36,339 - Epoch: [113][  200/  246]    Overall Loss 0.232139    Objective Loss 0.232139                                        LR 0.000013    Time 0.025649    
2023-01-06 14:41:36,584 - Epoch: [113][  210/  246]    Overall Loss 0.232332    Objective Loss 0.232332                                        LR 0.000013    Time 0.025594    
2023-01-06 14:41:36,831 - Epoch: [113][  220/  246]    Overall Loss 0.231759    Objective Loss 0.231759                                        LR 0.000013    Time 0.025554    
2023-01-06 14:41:37,073 - Epoch: [113][  230/  246]    Overall Loss 0.231943    Objective Loss 0.231943                                        LR 0.000013    Time 0.025492    
2023-01-06 14:41:37,330 - Epoch: [113][  240/  246]    Overall Loss 0.232537    Objective Loss 0.232537                                        LR 0.000013    Time 0.025499    
2023-01-06 14:41:37,461 - Epoch: [113][  246/  246]    Overall Loss 0.231821    Objective Loss 0.231821    Top1 92.822967    LR 0.000013    Time 0.025411    
2023-01-06 14:41:37,591 - --- validate (epoch=113)-----------
2023-01-06 14:41:37,591 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:38,050 - Epoch: [113][   10/   28]    Loss 0.289000    Top1 89.648438    
2023-01-06 14:41:38,193 - Epoch: [113][   20/   28]    Loss 0.260473    Top1 90.976562    
2023-01-06 14:41:38,285 - Epoch: [113][   28/   28]    Loss 0.260141    Top1 90.681363    
2023-01-06 14:41:38,416 - ==> Top1: 90.681    Loss: 0.260

2023-01-06 14:41:38,416 - ==> Confusion:
[[ 255   17  167]
 [  14  274  314]
 [  66   73 5806]]

2023-01-06 14:41:38,418 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:38,418 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:38,435 - 

2023-01-06 14:41:38,435 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:39,158 - Epoch: [114][   10/  246]    Overall Loss 0.242909    Objective Loss 0.242909                                        LR 0.000013    Time 0.072170    
2023-01-06 14:41:39,393 - Epoch: [114][   20/  246]    Overall Loss 0.244169    Objective Loss 0.244169                                        LR 0.000013    Time 0.047797    
2023-01-06 14:41:39,610 - Epoch: [114][   30/  246]    Overall Loss 0.241015    Objective Loss 0.241015                                        LR 0.000013    Time 0.039082    
2023-01-06 14:41:39,828 - Epoch: [114][   40/  246]    Overall Loss 0.243066    Objective Loss 0.243066                                        LR 0.000013    Time 0.034771    
2023-01-06 14:41:40,047 - Epoch: [114][   50/  246]    Overall Loss 0.237540    Objective Loss 0.237540                                        LR 0.000013    Time 0.032191    
2023-01-06 14:41:40,264 - Epoch: [114][   60/  246]    Overall Loss 0.236414    Objective Loss 0.236414                                        LR 0.000013    Time 0.030430    
2023-01-06 14:41:40,481 - Epoch: [114][   70/  246]    Overall Loss 0.233417    Objective Loss 0.233417                                        LR 0.000013    Time 0.029171    
2023-01-06 14:41:40,697 - Epoch: [114][   80/  246]    Overall Loss 0.232558    Objective Loss 0.232558                                        LR 0.000013    Time 0.028224    
2023-01-06 14:41:40,913 - Epoch: [114][   90/  246]    Overall Loss 0.234289    Objective Loss 0.234289                                        LR 0.000013    Time 0.027486    
2023-01-06 14:41:41,129 - Epoch: [114][  100/  246]    Overall Loss 0.235216    Objective Loss 0.235216                                        LR 0.000013    Time 0.026893    
2023-01-06 14:41:41,354 - Epoch: [114][  110/  246]    Overall Loss 0.235101    Objective Loss 0.235101                                        LR 0.000013    Time 0.026489    
2023-01-06 14:41:41,579 - Epoch: [114][  120/  246]    Overall Loss 0.234904    Objective Loss 0.234904                                        LR 0.000013    Time 0.026151    
2023-01-06 14:41:41,803 - Epoch: [114][  130/  246]    Overall Loss 0.235015    Objective Loss 0.235015                                        LR 0.000013    Time 0.025861    
2023-01-06 14:41:42,027 - Epoch: [114][  140/  246]    Overall Loss 0.235682    Objective Loss 0.235682                                        LR 0.000013    Time 0.025613    
2023-01-06 14:41:42,254 - Epoch: [114][  150/  246]    Overall Loss 0.234240    Objective Loss 0.234240                                        LR 0.000013    Time 0.025414    
2023-01-06 14:41:42,476 - Epoch: [114][  160/  246]    Overall Loss 0.233098    Objective Loss 0.233098                                        LR 0.000013    Time 0.025210    
2023-01-06 14:41:42,696 - Epoch: [114][  170/  246]    Overall Loss 0.232969    Objective Loss 0.232969                                        LR 0.000013    Time 0.025018    
2023-01-06 14:41:42,918 - Epoch: [114][  180/  246]    Overall Loss 0.231227    Objective Loss 0.231227                                        LR 0.000013    Time 0.024862    
2023-01-06 14:41:43,139 - Epoch: [114][  190/  246]    Overall Loss 0.230346    Objective Loss 0.230346                                        LR 0.000013    Time 0.024711    
2023-01-06 14:41:43,362 - Epoch: [114][  200/  246]    Overall Loss 0.230791    Objective Loss 0.230791                                        LR 0.000013    Time 0.024590    
2023-01-06 14:41:43,581 - Epoch: [114][  210/  246]    Overall Loss 0.231333    Objective Loss 0.231333                                        LR 0.000013    Time 0.024458    
2023-01-06 14:41:43,804 - Epoch: [114][  220/  246]    Overall Loss 0.231885    Objective Loss 0.231885                                        LR 0.000013    Time 0.024362    
2023-01-06 14:41:44,025 - Epoch: [114][  230/  246]    Overall Loss 0.232508    Objective Loss 0.232508                                        LR 0.000013    Time 0.024259    
2023-01-06 14:41:44,258 - Epoch: [114][  240/  246]    Overall Loss 0.232273    Objective Loss 0.232273                                        LR 0.000013    Time 0.024218    
2023-01-06 14:41:44,372 - Epoch: [114][  246/  246]    Overall Loss 0.233003    Objective Loss 0.233003    Top1 89.234450    LR 0.000013    Time 0.024093    
2023-01-06 14:41:44,509 - --- validate (epoch=114)-----------
2023-01-06 14:41:44,509 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:44,964 - Epoch: [114][   10/   28]    Loss 0.257958    Top1 90.820312    
2023-01-06 14:41:45,107 - Epoch: [114][   20/   28]    Loss 0.255092    Top1 90.878906    
2023-01-06 14:41:45,198 - Epoch: [114][   28/   28]    Loss 0.256872    Top1 90.681363    
2023-01-06 14:41:45,345 - ==> Top1: 90.681    Loss: 0.257

2023-01-06 14:41:45,345 - ==> Confusion:
[[ 270   13  156]
 [  15  294  293]
 [  88   86 5771]]

2023-01-06 14:41:45,346 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:45,347 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:45,356 - 

2023-01-06 14:41:45,356 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:45,949 - Epoch: [115][   10/  246]    Overall Loss 0.217855    Objective Loss 0.217855                                        LR 0.000013    Time 0.059193    
2023-01-06 14:41:46,192 - Epoch: [115][   20/  246]    Overall Loss 0.221830    Objective Loss 0.221830                                        LR 0.000013    Time 0.041728    
2023-01-06 14:41:46,443 - Epoch: [115][   30/  246]    Overall Loss 0.221569    Objective Loss 0.221569                                        LR 0.000013    Time 0.036172    
2023-01-06 14:41:46,695 - Epoch: [115][   40/  246]    Overall Loss 0.227162    Objective Loss 0.227162                                        LR 0.000013    Time 0.033385    
2023-01-06 14:41:46,945 - Epoch: [115][   50/  246]    Overall Loss 0.227479    Objective Loss 0.227479                                        LR 0.000013    Time 0.031712    
2023-01-06 14:41:47,199 - Epoch: [115][   60/  246]    Overall Loss 0.227039    Objective Loss 0.227039                                        LR 0.000013    Time 0.030645    
2023-01-06 14:41:47,455 - Epoch: [115][   70/  246]    Overall Loss 0.228174    Objective Loss 0.228174                                        LR 0.000013    Time 0.029913    
2023-01-06 14:41:47,709 - Epoch: [115][   80/  246]    Overall Loss 0.227517    Objective Loss 0.227517                                        LR 0.000013    Time 0.029352    
2023-01-06 14:41:47,975 - Epoch: [115][   90/  246]    Overall Loss 0.227816    Objective Loss 0.227816                                        LR 0.000013    Time 0.029031    
2023-01-06 14:41:48,251 - Epoch: [115][  100/  246]    Overall Loss 0.227145    Objective Loss 0.227145                                        LR 0.000013    Time 0.028884    
2023-01-06 14:41:48,522 - Epoch: [115][  110/  246]    Overall Loss 0.227476    Objective Loss 0.227476                                        LR 0.000013    Time 0.028717    
2023-01-06 14:41:48,791 - Epoch: [115][  120/  246]    Overall Loss 0.228290    Objective Loss 0.228290                                        LR 0.000013    Time 0.028568    
2023-01-06 14:41:49,064 - Epoch: [115][  130/  246]    Overall Loss 0.228251    Objective Loss 0.228251                                        LR 0.000013    Time 0.028460    
2023-01-06 14:41:49,328 - Epoch: [115][  140/  246]    Overall Loss 0.228264    Objective Loss 0.228264                                        LR 0.000013    Time 0.028313    
2023-01-06 14:41:49,591 - Epoch: [115][  150/  246]    Overall Loss 0.227974    Objective Loss 0.227974                                        LR 0.000013    Time 0.028172    
2023-01-06 14:41:49,851 - Epoch: [115][  160/  246]    Overall Loss 0.228971    Objective Loss 0.228971                                        LR 0.000013    Time 0.028034    
2023-01-06 14:41:50,112 - Epoch: [115][  170/  246]    Overall Loss 0.229678    Objective Loss 0.229678                                        LR 0.000013    Time 0.027916    
2023-01-06 14:41:50,367 - Epoch: [115][  180/  246]    Overall Loss 0.230446    Objective Loss 0.230446                                        LR 0.000013    Time 0.027782    
2023-01-06 14:41:50,622 - Epoch: [115][  190/  246]    Overall Loss 0.230799    Objective Loss 0.230799                                        LR 0.000013    Time 0.027659    
2023-01-06 14:41:50,877 - Epoch: [115][  200/  246]    Overall Loss 0.231331    Objective Loss 0.231331                                        LR 0.000013    Time 0.027548    
2023-01-06 14:41:51,134 - Epoch: [115][  210/  246]    Overall Loss 0.231326    Objective Loss 0.231326                                        LR 0.000013    Time 0.027458    
2023-01-06 14:41:51,389 - Epoch: [115][  220/  246]    Overall Loss 0.231967    Objective Loss 0.231967                                        LR 0.000013    Time 0.027366    
2023-01-06 14:41:51,640 - Epoch: [115][  230/  246]    Overall Loss 0.232565    Objective Loss 0.232565                                        LR 0.000013    Time 0.027264    
2023-01-06 14:41:51,901 - Epoch: [115][  240/  246]    Overall Loss 0.232515    Objective Loss 0.232515                                        LR 0.000013    Time 0.027215    
2023-01-06 14:41:52,033 - Epoch: [115][  246/  246]    Overall Loss 0.232220    Objective Loss 0.232220    Top1 92.822967    LR 0.000013    Time 0.027088    
2023-01-06 14:41:52,203 - --- validate (epoch=115)-----------
2023-01-06 14:41:52,203 - 6986 samples (256 per mini-batch)
2023-01-06 14:41:52,656 - Epoch: [115][   10/   28]    Loss 0.250055    Top1 90.820312    
2023-01-06 14:41:52,805 - Epoch: [115][   20/   28]    Loss 0.251419    Top1 91.191406    
2023-01-06 14:41:52,895 - Epoch: [115][   28/   28]    Loss 0.256637    Top1 90.896078    
2023-01-06 14:41:53,030 - ==> Top1: 90.896    Loss: 0.257

2023-01-06 14:41:53,031 - ==> Confusion:
[[ 269   13  157]
 [  16  303  283]
 [  81   86 5778]]

2023-01-06 14:41:53,032 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:41:53,032 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:41:53,042 - 

2023-01-06 14:41:53,042 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:41:53,751 - Epoch: [116][   10/  246]    Overall Loss 0.243516    Objective Loss 0.243516                                        LR 0.000013    Time 0.070787    
2023-01-06 14:41:53,990 - Epoch: [116][   20/  246]    Overall Loss 0.244022    Objective Loss 0.244022                                        LR 0.000013    Time 0.047322    
2023-01-06 14:41:54,244 - Epoch: [116][   30/  246]    Overall Loss 0.242655    Objective Loss 0.242655                                        LR 0.000013    Time 0.040008    
2023-01-06 14:41:54,504 - Epoch: [116][   40/  246]    Overall Loss 0.239479    Objective Loss 0.239479                                        LR 0.000013    Time 0.036498    
2023-01-06 14:41:54,765 - Epoch: [116][   50/  246]    Overall Loss 0.236203    Objective Loss 0.236203                                        LR 0.000013    Time 0.034419    
2023-01-06 14:41:55,017 - Epoch: [116][   60/  246]    Overall Loss 0.231807    Objective Loss 0.231807                                        LR 0.000013    Time 0.032868    
2023-01-06 14:41:55,268 - Epoch: [116][   70/  246]    Overall Loss 0.236903    Objective Loss 0.236903                                        LR 0.000013    Time 0.031750    
2023-01-06 14:41:55,518 - Epoch: [116][   80/  246]    Overall Loss 0.235493    Objective Loss 0.235493                                        LR 0.000013    Time 0.030901    
2023-01-06 14:41:55,769 - Epoch: [116][   90/  246]    Overall Loss 0.235027    Objective Loss 0.235027                                        LR 0.000013    Time 0.030249    
2023-01-06 14:41:56,017 - Epoch: [116][  100/  246]    Overall Loss 0.235538    Objective Loss 0.235538                                        LR 0.000013    Time 0.029705    
2023-01-06 14:41:56,267 - Epoch: [116][  110/  246]    Overall Loss 0.233623    Objective Loss 0.233623                                        LR 0.000013    Time 0.029271    
2023-01-06 14:41:56,521 - Epoch: [116][  120/  246]    Overall Loss 0.233193    Objective Loss 0.233193                                        LR 0.000013    Time 0.028946    
2023-01-06 14:41:56,771 - Epoch: [116][  130/  246]    Overall Loss 0.234100    Objective Loss 0.234100                                        LR 0.000013    Time 0.028639    
2023-01-06 14:41:57,026 - Epoch: [116][  140/  246]    Overall Loss 0.233989    Objective Loss 0.233989                                        LR 0.000013    Time 0.028411    
2023-01-06 14:41:57,279 - Epoch: [116][  150/  246]    Overall Loss 0.234562    Objective Loss 0.234562                                        LR 0.000013    Time 0.028200    
2023-01-06 14:41:57,533 - Epoch: [116][  160/  246]    Overall Loss 0.234374    Objective Loss 0.234374                                        LR 0.000013    Time 0.028009    
2023-01-06 14:41:57,788 - Epoch: [116][  170/  246]    Overall Loss 0.234394    Objective Loss 0.234394                                        LR 0.000013    Time 0.027863    
2023-01-06 14:41:58,038 - Epoch: [116][  180/  246]    Overall Loss 0.234172    Objective Loss 0.234172                                        LR 0.000013    Time 0.027701    
2023-01-06 14:41:58,294 - Epoch: [116][  190/  246]    Overall Loss 0.233407    Objective Loss 0.233407                                        LR 0.000013    Time 0.027588    
2023-01-06 14:41:58,544 - Epoch: [116][  200/  246]    Overall Loss 0.233330    Objective Loss 0.233330                                        LR 0.000013    Time 0.027454    
2023-01-06 14:41:58,805 - Epoch: [116][  210/  246]    Overall Loss 0.233769    Objective Loss 0.233769                                        LR 0.000013    Time 0.027390    
2023-01-06 14:41:59,060 - Epoch: [116][  220/  246]    Overall Loss 0.234707    Objective Loss 0.234707                                        LR 0.000013    Time 0.027300    
2023-01-06 14:41:59,322 - Epoch: [116][  230/  246]    Overall Loss 0.234905    Objective Loss 0.234905                                        LR 0.000013    Time 0.027250    
2023-01-06 14:41:59,588 - Epoch: [116][  240/  246]    Overall Loss 0.234372    Objective Loss 0.234372                                        LR 0.000013    Time 0.027221    
2023-01-06 14:41:59,719 - Epoch: [116][  246/  246]    Overall Loss 0.233933    Objective Loss 0.233933    Top1 92.822967    LR 0.000013    Time 0.027090    
2023-01-06 14:41:59,855 - --- validate (epoch=116)-----------
2023-01-06 14:41:59,856 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:00,306 - Epoch: [116][   10/   28]    Loss 0.252280    Top1 91.875000    
2023-01-06 14:42:00,445 - Epoch: [116][   20/   28]    Loss 0.255777    Top1 91.171875    
2023-01-06 14:42:00,536 - Epoch: [116][   28/   28]    Loss 0.255774    Top1 90.881764    
2023-01-06 14:42:00,675 - ==> Top1: 90.882    Loss: 0.256

2023-01-06 14:42:00,675 - ==> Confusion:
[[ 253   13  173]
 [  13  276  313]
 [  62   63 5820]]

2023-01-06 14:42:00,676 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:00,677 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:00,686 - 

2023-01-06 14:42:00,686 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:01,431 - Epoch: [117][   10/  246]    Overall Loss 0.221300    Objective Loss 0.221300                                        LR 0.000013    Time 0.074372    
2023-01-06 14:42:01,673 - Epoch: [117][   20/  246]    Overall Loss 0.221000    Objective Loss 0.221000                                        LR 0.000013    Time 0.049259    
2023-01-06 14:42:01,921 - Epoch: [117][   30/  246]    Overall Loss 0.228698    Objective Loss 0.228698                                        LR 0.000013    Time 0.041102    
2023-01-06 14:42:02,167 - Epoch: [117][   40/  246]    Overall Loss 0.231751    Objective Loss 0.231751                                        LR 0.000013    Time 0.036941    
2023-01-06 14:42:02,412 - Epoch: [117][   50/  246]    Overall Loss 0.235359    Objective Loss 0.235359                                        LR 0.000013    Time 0.034437    
2023-01-06 14:42:02,656 - Epoch: [117][   60/  246]    Overall Loss 0.235673    Objective Loss 0.235673                                        LR 0.000013    Time 0.032739    
2023-01-06 14:42:02,901 - Epoch: [117][   70/  246]    Overall Loss 0.235466    Objective Loss 0.235466                                        LR 0.000013    Time 0.031552    
2023-01-06 14:42:03,162 - Epoch: [117][   80/  246]    Overall Loss 0.234715    Objective Loss 0.234715                                        LR 0.000013    Time 0.030862    
2023-01-06 14:42:03,421 - Epoch: [117][   90/  246]    Overall Loss 0.234994    Objective Loss 0.234994                                        LR 0.000013    Time 0.030286    
2023-01-06 14:42:03,678 - Epoch: [117][  100/  246]    Overall Loss 0.234890    Objective Loss 0.234890                                        LR 0.000013    Time 0.029819    
2023-01-06 14:42:03,902 - Epoch: [117][  110/  246]    Overall Loss 0.233382    Objective Loss 0.233382                                        LR 0.000013    Time 0.029134    
2023-01-06 14:42:04,149 - Epoch: [117][  120/  246]    Overall Loss 0.233665    Objective Loss 0.233665                                        LR 0.000013    Time 0.028755    
2023-01-06 14:42:04,387 - Epoch: [117][  130/  246]    Overall Loss 0.233454    Objective Loss 0.233454                                        LR 0.000013    Time 0.028377    
2023-01-06 14:42:04,627 - Epoch: [117][  140/  246]    Overall Loss 0.233453    Objective Loss 0.233453                                        LR 0.000013    Time 0.028058    
2023-01-06 14:42:04,854 - Epoch: [117][  150/  246]    Overall Loss 0.233829    Objective Loss 0.233829                                        LR 0.000013    Time 0.027693    
2023-01-06 14:42:05,096 - Epoch: [117][  160/  246]    Overall Loss 0.234160    Objective Loss 0.234160                                        LR 0.000013    Time 0.027473    
2023-01-06 14:42:05,337 - Epoch: [117][  170/  246]    Overall Loss 0.235129    Objective Loss 0.235129                                        LR 0.000013    Time 0.027264    
2023-01-06 14:42:05,575 - Epoch: [117][  180/  246]    Overall Loss 0.235793    Objective Loss 0.235793                                        LR 0.000013    Time 0.027067    
2023-01-06 14:42:05,815 - Epoch: [117][  190/  246]    Overall Loss 0.234925    Objective Loss 0.234925                                        LR 0.000013    Time 0.026899    
2023-01-06 14:42:06,056 - Epoch: [117][  200/  246]    Overall Loss 0.235579    Objective Loss 0.235579                                        LR 0.000013    Time 0.026753    
2023-01-06 14:42:06,297 - Epoch: [117][  210/  246]    Overall Loss 0.235598    Objective Loss 0.235598                                        LR 0.000013    Time 0.026619    
2023-01-06 14:42:06,535 - Epoch: [117][  220/  246]    Overall Loss 0.235947    Objective Loss 0.235947                                        LR 0.000013    Time 0.026491    
2023-01-06 14:42:06,789 - Epoch: [117][  230/  246]    Overall Loss 0.235599    Objective Loss 0.235599                                        LR 0.000013    Time 0.026435    
2023-01-06 14:42:07,057 - Epoch: [117][  240/  246]    Overall Loss 0.235176    Objective Loss 0.235176                                        LR 0.000013    Time 0.026444    
2023-01-06 14:42:07,173 - Epoch: [117][  246/  246]    Overall Loss 0.234014    Objective Loss 0.234014    Top1 91.626794    LR 0.000013    Time 0.026268    
2023-01-06 14:42:07,319 - --- validate (epoch=117)-----------
2023-01-06 14:42:07,319 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:07,782 - Epoch: [117][   10/   28]    Loss 0.274893    Top1 89.960938    
2023-01-06 14:42:07,923 - Epoch: [117][   20/   28]    Loss 0.264243    Top1 90.488281    
2023-01-06 14:42:08,014 - Epoch: [117][   28/   28]    Loss 0.256971    Top1 90.709991    
2023-01-06 14:42:08,146 - ==> Top1: 90.710    Loss: 0.257

2023-01-06 14:42:08,146 - ==> Confusion:
[[ 227   12  200]
 [  13  284  305]
 [  51   68 5826]]

2023-01-06 14:42:08,148 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:08,148 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:08,158 - 

2023-01-06 14:42:08,158 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:08,739 - Epoch: [118][   10/  246]    Overall Loss 0.241033    Objective Loss 0.241033                                        LR 0.000013    Time 0.058020    
2023-01-06 14:42:08,961 - Epoch: [118][   20/  246]    Overall Loss 0.235486    Objective Loss 0.235486                                        LR 0.000013    Time 0.040120    
2023-01-06 14:42:09,213 - Epoch: [118][   30/  246]    Overall Loss 0.234665    Objective Loss 0.234665                                        LR 0.000013    Time 0.035116    
2023-01-06 14:42:09,466 - Epoch: [118][   40/  246]    Overall Loss 0.236212    Objective Loss 0.236212                                        LR 0.000013    Time 0.032662    
2023-01-06 14:42:09,715 - Epoch: [118][   50/  246]    Overall Loss 0.234713    Objective Loss 0.234713                                        LR 0.000013    Time 0.031092    
2023-01-06 14:42:09,961 - Epoch: [118][   60/  246]    Overall Loss 0.235216    Objective Loss 0.235216                                        LR 0.000013    Time 0.029985    
2023-01-06 14:42:10,190 - Epoch: [118][   70/  246]    Overall Loss 0.233791    Objective Loss 0.233791                                        LR 0.000013    Time 0.028965    
2023-01-06 14:42:10,399 - Epoch: [118][   80/  246]    Overall Loss 0.234440    Objective Loss 0.234440                                        LR 0.000013    Time 0.027950    
2023-01-06 14:42:10,634 - Epoch: [118][   90/  246]    Overall Loss 0.233220    Objective Loss 0.233220                                        LR 0.000013    Time 0.027457    
2023-01-06 14:42:10,888 - Epoch: [118][  100/  246]    Overall Loss 0.232466    Objective Loss 0.232466                                        LR 0.000013    Time 0.027238    
2023-01-06 14:42:11,141 - Epoch: [118][  110/  246]    Overall Loss 0.232223    Objective Loss 0.232223                                        LR 0.000013    Time 0.027064    
2023-01-06 14:42:11,396 - Epoch: [118][  120/  246]    Overall Loss 0.232621    Objective Loss 0.232621                                        LR 0.000013    Time 0.026925    
2023-01-06 14:42:11,649 - Epoch: [118][  130/  246]    Overall Loss 0.232118    Objective Loss 0.232118                                        LR 0.000013    Time 0.026799    
2023-01-06 14:42:11,906 - Epoch: [118][  140/  246]    Overall Loss 0.231168    Objective Loss 0.231168                                        LR 0.000013    Time 0.026715    
2023-01-06 14:42:12,162 - Epoch: [118][  150/  246]    Overall Loss 0.230644    Objective Loss 0.230644                                        LR 0.000013    Time 0.026639    
2023-01-06 14:42:12,407 - Epoch: [118][  160/  246]    Overall Loss 0.231866    Objective Loss 0.231866                                        LR 0.000013    Time 0.026504    
2023-01-06 14:42:12,652 - Epoch: [118][  170/  246]    Overall Loss 0.232031    Objective Loss 0.232031                                        LR 0.000013    Time 0.026379    
2023-01-06 14:42:12,902 - Epoch: [118][  180/  246]    Overall Loss 0.231789    Objective Loss 0.231789                                        LR 0.000013    Time 0.026295    
2023-01-06 14:42:13,156 - Epoch: [118][  190/  246]    Overall Loss 0.231189    Objective Loss 0.231189                                        LR 0.000013    Time 0.026242    
2023-01-06 14:42:13,411 - Epoch: [118][  200/  246]    Overall Loss 0.231137    Objective Loss 0.231137                                        LR 0.000013    Time 0.026202    
2023-01-06 14:42:13,667 - Epoch: [118][  210/  246]    Overall Loss 0.231889    Objective Loss 0.231889                                        LR 0.000013    Time 0.026162    
2023-01-06 14:42:13,913 - Epoch: [118][  220/  246]    Overall Loss 0.231877    Objective Loss 0.231877                                        LR 0.000013    Time 0.026093    
2023-01-06 14:42:14,128 - Epoch: [118][  230/  246]    Overall Loss 0.231521    Objective Loss 0.231521                                        LR 0.000013    Time 0.025889    
2023-01-06 14:42:14,392 - Epoch: [118][  240/  246]    Overall Loss 0.231153    Objective Loss 0.231153                                        LR 0.000013    Time 0.025909    
2023-01-06 14:42:14,525 - Epoch: [118][  246/  246]    Overall Loss 0.230860    Objective Loss 0.230860    Top1 92.105263    LR 0.000013    Time 0.025815    
2023-01-06 14:42:14,695 - --- validate (epoch=118)-----------
2023-01-06 14:42:14,695 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:15,135 - Epoch: [118][   10/   28]    Loss 0.252708    Top1 90.351562    
2023-01-06 14:42:15,277 - Epoch: [118][   20/   28]    Loss 0.259707    Top1 90.195312    
2023-01-06 14:42:15,368 - Epoch: [118][   28/   28]    Loss 0.252805    Top1 90.466648    
2023-01-06 14:42:15,490 - ==> Top1: 90.467    Loss: 0.253

2023-01-06 14:42:15,491 - ==> Confusion:
[[ 245   17  177]
 [  11  296  295]
 [  76   90 5779]]

2023-01-06 14:42:15,492 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:15,492 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:15,502 - 

2023-01-06 14:42:15,502 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:16,232 - Epoch: [119][   10/  246]    Overall Loss 0.226672    Objective Loss 0.226672                                        LR 0.000013    Time 0.072925    
2023-01-06 14:42:16,475 - Epoch: [119][   20/  246]    Overall Loss 0.224588    Objective Loss 0.224588                                        LR 0.000013    Time 0.048579    
2023-01-06 14:42:16,739 - Epoch: [119][   30/  246]    Overall Loss 0.224931    Objective Loss 0.224931                                        LR 0.000013    Time 0.041171    
2023-01-06 14:42:16,998 - Epoch: [119][   40/  246]    Overall Loss 0.224682    Objective Loss 0.224682                                        LR 0.000013    Time 0.037323    
2023-01-06 14:42:17,227 - Epoch: [119][   50/  246]    Overall Loss 0.225969    Objective Loss 0.225969                                        LR 0.000013    Time 0.034415    
2023-01-06 14:42:17,466 - Epoch: [119][   60/  246]    Overall Loss 0.225801    Objective Loss 0.225801                                        LR 0.000013    Time 0.032641    
2023-01-06 14:42:17,696 - Epoch: [119][   70/  246]    Overall Loss 0.224130    Objective Loss 0.224130                                        LR 0.000013    Time 0.031270    
2023-01-06 14:42:17,935 - Epoch: [119][   80/  246]    Overall Loss 0.226981    Objective Loss 0.226981                                        LR 0.000013    Time 0.030333    
2023-01-06 14:42:18,173 - Epoch: [119][   90/  246]    Overall Loss 0.228365    Objective Loss 0.228365                                        LR 0.000013    Time 0.029596    
2023-01-06 14:42:18,400 - Epoch: [119][  100/  246]    Overall Loss 0.228098    Objective Loss 0.228098                                        LR 0.000013    Time 0.028901    
2023-01-06 14:42:18,632 - Epoch: [119][  110/  246]    Overall Loss 0.227964    Objective Loss 0.227964                                        LR 0.000013    Time 0.028379    
2023-01-06 14:42:18,869 - Epoch: [119][  120/  246]    Overall Loss 0.227227    Objective Loss 0.227227                                        LR 0.000013    Time 0.027983    
2023-01-06 14:42:19,093 - Epoch: [119][  130/  246]    Overall Loss 0.228125    Objective Loss 0.228125                                        LR 0.000013    Time 0.027554    
2023-01-06 14:42:19,321 - Epoch: [119][  140/  246]    Overall Loss 0.228809    Objective Loss 0.228809                                        LR 0.000013    Time 0.027213    
2023-01-06 14:42:19,559 - Epoch: [119][  150/  246]    Overall Loss 0.229257    Objective Loss 0.229257                                        LR 0.000013    Time 0.026981    
2023-01-06 14:42:19,792 - Epoch: [119][  160/  246]    Overall Loss 0.228995    Objective Loss 0.228995                                        LR 0.000013    Time 0.026743    
2023-01-06 14:42:20,028 - Epoch: [119][  170/  246]    Overall Loss 0.229644    Objective Loss 0.229644                                        LR 0.000013    Time 0.026559    
2023-01-06 14:42:20,262 - Epoch: [119][  180/  246]    Overall Loss 0.229422    Objective Loss 0.229422                                        LR 0.000013    Time 0.026377    
2023-01-06 14:42:20,495 - Epoch: [119][  190/  246]    Overall Loss 0.230317    Objective Loss 0.230317                                        LR 0.000013    Time 0.026215    
2023-01-06 14:42:20,724 - Epoch: [119][  200/  246]    Overall Loss 0.230195    Objective Loss 0.230195                                        LR 0.000013    Time 0.026045    
2023-01-06 14:42:20,951 - Epoch: [119][  210/  246]    Overall Loss 0.231404    Objective Loss 0.231404                                        LR 0.000013    Time 0.025888    
2023-01-06 14:42:21,177 - Epoch: [119][  220/  246]    Overall Loss 0.231896    Objective Loss 0.231896                                        LR 0.000013    Time 0.025732    
2023-01-06 14:42:21,406 - Epoch: [119][  230/  246]    Overall Loss 0.231926    Objective Loss 0.231926                                        LR 0.000013    Time 0.025610    
2023-01-06 14:42:21,661 - Epoch: [119][  240/  246]    Overall Loss 0.232276    Objective Loss 0.232276                                        LR 0.000013    Time 0.025601    
2023-01-06 14:42:21,793 - Epoch: [119][  246/  246]    Overall Loss 0.232389    Objective Loss 0.232389    Top1 90.909091    LR 0.000013    Time 0.025512    
2023-01-06 14:42:21,935 - --- validate (epoch=119)-----------
2023-01-06 14:42:21,936 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:22,372 - Epoch: [119][   10/   28]    Loss 0.248469    Top1 91.015625    
2023-01-06 14:42:22,514 - Epoch: [119][   20/   28]    Loss 0.250921    Top1 90.898438    
2023-01-06 14:42:22,606 - Epoch: [119][   28/   28]    Loss 0.263098    Top1 90.652734    
2023-01-06 14:42:22,751 - ==> Top1: 90.653    Loss: 0.263

2023-01-06 14:42:22,751 - ==> Confusion:
[[ 249   12  178]
 [  16  283  303]
 [  69   75 5801]]

2023-01-06 14:42:22,753 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:22,753 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:22,763 - 

2023-01-06 14:42:22,763 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:23,368 - Epoch: [120][   10/  246]    Overall Loss 0.233472    Objective Loss 0.233472                                        LR 0.000013    Time 0.060482    
2023-01-06 14:42:23,619 - Epoch: [120][   20/  246]    Overall Loss 0.233921    Objective Loss 0.233921                                        LR 0.000013    Time 0.042746    
2023-01-06 14:42:23,870 - Epoch: [120][   30/  246]    Overall Loss 0.235348    Objective Loss 0.235348                                        LR 0.000013    Time 0.036853    
2023-01-06 14:42:24,133 - Epoch: [120][   40/  246]    Overall Loss 0.231848    Objective Loss 0.231848                                        LR 0.000013    Time 0.034195    
2023-01-06 14:42:24,380 - Epoch: [120][   50/  246]    Overall Loss 0.233943    Objective Loss 0.233943                                        LR 0.000013    Time 0.032292    
2023-01-06 14:42:24,632 - Epoch: [120][   60/  246]    Overall Loss 0.231087    Objective Loss 0.231087                                        LR 0.000013    Time 0.031099    
2023-01-06 14:42:24,884 - Epoch: [120][   70/  246]    Overall Loss 0.229639    Objective Loss 0.229639                                        LR 0.000013    Time 0.030250    
2023-01-06 14:42:25,135 - Epoch: [120][   80/  246]    Overall Loss 0.230349    Objective Loss 0.230349                                        LR 0.000013    Time 0.029609    
2023-01-06 14:42:25,387 - Epoch: [120][   90/  246]    Overall Loss 0.230964    Objective Loss 0.230964                                        LR 0.000013    Time 0.029111    
2023-01-06 14:42:25,639 - Epoch: [120][  100/  246]    Overall Loss 0.231005    Objective Loss 0.231005                                        LR 0.000013    Time 0.028711    
2023-01-06 14:42:25,891 - Epoch: [120][  110/  246]    Overall Loss 0.232518    Objective Loss 0.232518                                        LR 0.000013    Time 0.028384    
2023-01-06 14:42:26,143 - Epoch: [120][  120/  246]    Overall Loss 0.232541    Objective Loss 0.232541                                        LR 0.000013    Time 0.028115    
2023-01-06 14:42:26,400 - Epoch: [120][  130/  246]    Overall Loss 0.230924    Objective Loss 0.230924                                        LR 0.000013    Time 0.027930    
2023-01-06 14:42:26,662 - Epoch: [120][  140/  246]    Overall Loss 0.231312    Objective Loss 0.231312                                        LR 0.000013    Time 0.027803    
2023-01-06 14:42:26,918 - Epoch: [120][  150/  246]    Overall Loss 0.232032    Objective Loss 0.232032                                        LR 0.000013    Time 0.027647    
2023-01-06 14:42:27,180 - Epoch: [120][  160/  246]    Overall Loss 0.231213    Objective Loss 0.231213                                        LR 0.000013    Time 0.027558    
2023-01-06 14:42:27,435 - Epoch: [120][  170/  246]    Overall Loss 0.231764    Objective Loss 0.231764                                        LR 0.000013    Time 0.027433    
2023-01-06 14:42:27,698 - Epoch: [120][  180/  246]    Overall Loss 0.232086    Objective Loss 0.232086                                        LR 0.000013    Time 0.027365    
2023-01-06 14:42:27,954 - Epoch: [120][  190/  246]    Overall Loss 0.231601    Objective Loss 0.231601                                        LR 0.000013    Time 0.027272    
2023-01-06 14:42:28,215 - Epoch: [120][  200/  246]    Overall Loss 0.231492    Objective Loss 0.231492                                        LR 0.000013    Time 0.027202    
2023-01-06 14:42:28,469 - Epoch: [120][  210/  246]    Overall Loss 0.231704    Objective Loss 0.231704                                        LR 0.000013    Time 0.027113    
2023-01-06 14:42:28,712 - Epoch: [120][  220/  246]    Overall Loss 0.232288    Objective Loss 0.232288                                        LR 0.000013    Time 0.026977    
2023-01-06 14:42:28,959 - Epoch: [120][  230/  246]    Overall Loss 0.231961    Objective Loss 0.231961                                        LR 0.000013    Time 0.026878    
2023-01-06 14:42:29,218 - Epoch: [120][  240/  246]    Overall Loss 0.231375    Objective Loss 0.231375                                        LR 0.000013    Time 0.026835    
2023-01-06 14:42:29,340 - Epoch: [120][  246/  246]    Overall Loss 0.231646    Objective Loss 0.231646    Top1 92.105263    LR 0.000013    Time 0.026677    
2023-01-06 14:42:29,482 - --- validate (epoch=120)-----------
2023-01-06 14:42:29,482 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:29,922 - Epoch: [120][   10/   28]    Loss 0.253209    Top1 90.976562    
2023-01-06 14:42:30,062 - Epoch: [120][   20/   28]    Loss 0.261827    Top1 90.429688    
2023-01-06 14:42:30,156 - Epoch: [120][   28/   28]    Loss 0.257842    Top1 90.423705    
2023-01-06 14:42:30,280 - ==> Top1: 90.424    Loss: 0.258

2023-01-06 14:42:30,281 - ==> Confusion:
[[ 227   17  195]
 [  13  295  294]
 [  68   82 5795]]

2023-01-06 14:42:30,282 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:30,282 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:30,292 - 

2023-01-06 14:42:30,292 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:31,013 - Epoch: [121][   10/  246]    Overall Loss 0.217801    Objective Loss 0.217801                                        LR 0.000013    Time 0.071942    
2023-01-06 14:42:31,259 - Epoch: [121][   20/  246]    Overall Loss 0.231806    Objective Loss 0.231806                                        LR 0.000013    Time 0.048286    
2023-01-06 14:42:31,512 - Epoch: [121][   30/  246]    Overall Loss 0.238435    Objective Loss 0.238435                                        LR 0.000013    Time 0.040592    
2023-01-06 14:42:31,765 - Epoch: [121][   40/  246]    Overall Loss 0.231896    Objective Loss 0.231896                                        LR 0.000013    Time 0.036754    
2023-01-06 14:42:31,999 - Epoch: [121][   50/  246]    Overall Loss 0.232990    Objective Loss 0.232990                                        LR 0.000013    Time 0.034086    
2023-01-06 14:42:32,238 - Epoch: [121][   60/  246]    Overall Loss 0.234716    Objective Loss 0.234716                                        LR 0.000013    Time 0.032377    
2023-01-06 14:42:32,474 - Epoch: [121][   70/  246]    Overall Loss 0.233297    Objective Loss 0.233297                                        LR 0.000013    Time 0.031110    
2023-01-06 14:42:32,717 - Epoch: [121][   80/  246]    Overall Loss 0.232011    Objective Loss 0.232011                                        LR 0.000013    Time 0.030253    
2023-01-06 14:42:32,957 - Epoch: [121][   90/  246]    Overall Loss 0.232360    Objective Loss 0.232360                                        LR 0.000013    Time 0.029561    
2023-01-06 14:42:33,193 - Epoch: [121][  100/  246]    Overall Loss 0.233021    Objective Loss 0.233021                                        LR 0.000013    Time 0.028955    
2023-01-06 14:42:33,439 - Epoch: [121][  110/  246]    Overall Loss 0.233801    Objective Loss 0.233801                                        LR 0.000013    Time 0.028553    
2023-01-06 14:42:33,675 - Epoch: [121][  120/  246]    Overall Loss 0.233568    Objective Loss 0.233568                                        LR 0.000013    Time 0.028142    
2023-01-06 14:42:33,906 - Epoch: [121][  130/  246]    Overall Loss 0.235310    Objective Loss 0.235310                                        LR 0.000013    Time 0.027748    
2023-01-06 14:42:34,118 - Epoch: [121][  140/  246]    Overall Loss 0.234439    Objective Loss 0.234439                                        LR 0.000013    Time 0.027282    
2023-01-06 14:42:34,351 - Epoch: [121][  150/  246]    Overall Loss 0.234987    Objective Loss 0.234987                                        LR 0.000013    Time 0.027012    
2023-01-06 14:42:34,604 - Epoch: [121][  160/  246]    Overall Loss 0.234869    Objective Loss 0.234869                                        LR 0.000013    Time 0.026905    
2023-01-06 14:42:34,844 - Epoch: [121][  170/  246]    Overall Loss 0.234235    Objective Loss 0.234235                                        LR 0.000013    Time 0.026731    
2023-01-06 14:42:35,066 - Epoch: [121][  180/  246]    Overall Loss 0.233426    Objective Loss 0.233426                                        LR 0.000013    Time 0.026474    
2023-01-06 14:42:35,288 - Epoch: [121][  190/  246]    Overall Loss 0.232985    Objective Loss 0.232985                                        LR 0.000013    Time 0.026249    
2023-01-06 14:42:35,517 - Epoch: [121][  200/  246]    Overall Loss 0.232941    Objective Loss 0.232941                                        LR 0.000013    Time 0.026079    
2023-01-06 14:42:35,752 - Epoch: [121][  210/  246]    Overall Loss 0.232148    Objective Loss 0.232148                                        LR 0.000013    Time 0.025956    
2023-01-06 14:42:35,994 - Epoch: [121][  220/  246]    Overall Loss 0.232966    Objective Loss 0.232966                                        LR 0.000013    Time 0.025872    
2023-01-06 14:42:36,224 - Epoch: [121][  230/  246]    Overall Loss 0.232278    Objective Loss 0.232278                                        LR 0.000013    Time 0.025747    
2023-01-06 14:42:36,470 - Epoch: [121][  240/  246]    Overall Loss 0.231957    Objective Loss 0.231957                                        LR 0.000013    Time 0.025695    
2023-01-06 14:42:36,592 - Epoch: [121][  246/  246]    Overall Loss 0.231844    Objective Loss 0.231844    Top1 90.909091    LR 0.000013    Time 0.025565    
2023-01-06 14:42:36,725 - --- validate (epoch=121)-----------
2023-01-06 14:42:36,725 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:37,163 - Epoch: [121][   10/   28]    Loss 0.242887    Top1 90.976562    
2023-01-06 14:42:37,301 - Epoch: [121][   20/   28]    Loss 0.257624    Top1 90.527344    
2023-01-06 14:42:37,393 - Epoch: [121][   28/   28]    Loss 0.256714    Top1 90.523905    
2023-01-06 14:42:37,505 - ==> Top1: 90.524    Loss: 0.257

2023-01-06 14:42:37,506 - ==> Confusion:
[[ 271   15  153]
 [  18  299  285]
 [  94   97 5754]]

2023-01-06 14:42:37,507 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:37,507 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:37,517 - 

2023-01-06 14:42:37,518 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:38,239 - Epoch: [122][   10/  246]    Overall Loss 0.238899    Objective Loss 0.238899                                        LR 0.000013    Time 0.072098    
2023-01-06 14:42:38,462 - Epoch: [122][   20/  246]    Overall Loss 0.235353    Objective Loss 0.235353                                        LR 0.000013    Time 0.047180    
2023-01-06 14:42:38,698 - Epoch: [122][   30/  246]    Overall Loss 0.232863    Objective Loss 0.232863                                        LR 0.000013    Time 0.039300    
2023-01-06 14:42:38,929 - Epoch: [122][   40/  246]    Overall Loss 0.229816    Objective Loss 0.229816                                        LR 0.000013    Time 0.035228    
2023-01-06 14:42:39,169 - Epoch: [122][   50/  246]    Overall Loss 0.225450    Objective Loss 0.225450                                        LR 0.000013    Time 0.032984    
2023-01-06 14:42:39,408 - Epoch: [122][   60/  246]    Overall Loss 0.223319    Objective Loss 0.223319                                        LR 0.000013    Time 0.031469    
2023-01-06 14:42:39,649 - Epoch: [122][   70/  246]    Overall Loss 0.221173    Objective Loss 0.221173                                        LR 0.000013    Time 0.030408    
2023-01-06 14:42:39,889 - Epoch: [122][   80/  246]    Overall Loss 0.223313    Objective Loss 0.223313                                        LR 0.000013    Time 0.029599    
2023-01-06 14:42:40,120 - Epoch: [122][   90/  246]    Overall Loss 0.223524    Objective Loss 0.223524                                        LR 0.000013    Time 0.028865    
2023-01-06 14:42:40,338 - Epoch: [122][  100/  246]    Overall Loss 0.225923    Objective Loss 0.225923                                        LR 0.000013    Time 0.028164    
2023-01-06 14:42:40,553 - Epoch: [122][  110/  246]    Overall Loss 0.227087    Objective Loss 0.227087                                        LR 0.000013    Time 0.027546    
2023-01-06 14:42:40,768 - Epoch: [122][  120/  246]    Overall Loss 0.226212    Objective Loss 0.226212                                        LR 0.000013    Time 0.027042    
2023-01-06 14:42:40,993 - Epoch: [122][  130/  246]    Overall Loss 0.226717    Objective Loss 0.226717                                        LR 0.000013    Time 0.026691    
2023-01-06 14:42:41,209 - Epoch: [122][  140/  246]    Overall Loss 0.226219    Objective Loss 0.226219                                        LR 0.000013    Time 0.026326    
2023-01-06 14:42:41,425 - Epoch: [122][  150/  246]    Overall Loss 0.226872    Objective Loss 0.226872                                        LR 0.000013    Time 0.026007    
2023-01-06 14:42:41,640 - Epoch: [122][  160/  246]    Overall Loss 0.226867    Objective Loss 0.226867                                        LR 0.000013    Time 0.025722    
2023-01-06 14:42:41,864 - Epoch: [122][  170/  246]    Overall Loss 0.227459    Objective Loss 0.227459                                        LR 0.000013    Time 0.025523    
2023-01-06 14:42:42,080 - Epoch: [122][  180/  246]    Overall Loss 0.227530    Objective Loss 0.227530                                        LR 0.000013    Time 0.025307    
2023-01-06 14:42:42,299 - Epoch: [122][  190/  246]    Overall Loss 0.229854    Objective Loss 0.229854                                        LR 0.000013    Time 0.025125    
2023-01-06 14:42:42,521 - Epoch: [122][  200/  246]    Overall Loss 0.230316    Objective Loss 0.230316                                        LR 0.000013    Time 0.024975    
2023-01-06 14:42:42,737 - Epoch: [122][  210/  246]    Overall Loss 0.230410    Objective Loss 0.230410                                        LR 0.000013    Time 0.024815    
2023-01-06 14:42:42,954 - Epoch: [122][  220/  246]    Overall Loss 0.230663    Objective Loss 0.230663                                        LR 0.000013    Time 0.024671    
2023-01-06 14:42:43,171 - Epoch: [122][  230/  246]    Overall Loss 0.230619    Objective Loss 0.230619                                        LR 0.000013    Time 0.024537    
2023-01-06 14:42:43,400 - Epoch: [122][  240/  246]    Overall Loss 0.231090    Objective Loss 0.231090                                        LR 0.000013    Time 0.024471    
2023-01-06 14:42:43,517 - Epoch: [122][  246/  246]    Overall Loss 0.232045    Objective Loss 0.232045    Top1 88.995215    LR 0.000013    Time 0.024348    
2023-01-06 14:42:43,671 - --- validate (epoch=122)-----------
2023-01-06 14:42:43,671 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:44,111 - Epoch: [122][   10/   28]    Loss 0.253433    Top1 91.132812    
2023-01-06 14:42:44,251 - Epoch: [122][   20/   28]    Loss 0.249365    Top1 91.074219    
2023-01-06 14:42:44,343 - Epoch: [122][   28/   28]    Loss 0.255199    Top1 90.838820    
2023-01-06 14:42:44,487 - ==> Top1: 90.839    Loss: 0.255

2023-01-06 14:42:44,488 - ==> Confusion:
[[ 259   15  165]
 [  14  298  290]
 [  68   88 5789]]

2023-01-06 14:42:44,489 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:44,489 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:44,499 - 

2023-01-06 14:42:44,499 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:45,097 - Epoch: [123][   10/  246]    Overall Loss 0.236576    Objective Loss 0.236576                                        LR 0.000013    Time 0.059752    
2023-01-06 14:42:45,320 - Epoch: [123][   20/  246]    Overall Loss 0.232588    Objective Loss 0.232588                                        LR 0.000013    Time 0.040968    
2023-01-06 14:42:45,544 - Epoch: [123][   30/  246]    Overall Loss 0.230007    Objective Loss 0.230007                                        LR 0.000013    Time 0.034777    
2023-01-06 14:42:45,766 - Epoch: [123][   40/  246]    Overall Loss 0.230932    Objective Loss 0.230932                                        LR 0.000013    Time 0.031629    
2023-01-06 14:42:45,997 - Epoch: [123][   50/  246]    Overall Loss 0.228697    Objective Loss 0.228697                                        LR 0.000013    Time 0.029912    
2023-01-06 14:42:46,229 - Epoch: [123][   60/  246]    Overall Loss 0.227463    Objective Loss 0.227463                                        LR 0.000013    Time 0.028781    
2023-01-06 14:42:46,454 - Epoch: [123][   70/  246]    Overall Loss 0.228730    Objective Loss 0.228730                                        LR 0.000013    Time 0.027886    
2023-01-06 14:42:46,691 - Epoch: [123][   80/  246]    Overall Loss 0.230159    Objective Loss 0.230159                                        LR 0.000013    Time 0.027349    
2023-01-06 14:42:46,929 - Epoch: [123][   90/  246]    Overall Loss 0.232013    Objective Loss 0.232013                                        LR 0.000013    Time 0.026957    
2023-01-06 14:42:47,159 - Epoch: [123][  100/  246]    Overall Loss 0.231625    Objective Loss 0.231625                                        LR 0.000013    Time 0.026556    
2023-01-06 14:42:47,419 - Epoch: [123][  110/  246]    Overall Loss 0.231306    Objective Loss 0.231306                                        LR 0.000013    Time 0.026496    
2023-01-06 14:42:47,679 - Epoch: [123][  120/  246]    Overall Loss 0.230810    Objective Loss 0.230810                                        LR 0.000013    Time 0.026454    
2023-01-06 14:42:47,938 - Epoch: [123][  130/  246]    Overall Loss 0.229550    Objective Loss 0.229550                                        LR 0.000013    Time 0.026404    
2023-01-06 14:42:48,198 - Epoch: [123][  140/  246]    Overall Loss 0.229915    Objective Loss 0.229915                                        LR 0.000013    Time 0.026375    
2023-01-06 14:42:48,457 - Epoch: [123][  150/  246]    Overall Loss 0.229255    Objective Loss 0.229255                                        LR 0.000013    Time 0.026339    
2023-01-06 14:42:48,720 - Epoch: [123][  160/  246]    Overall Loss 0.228898    Objective Loss 0.228898                                        LR 0.000013    Time 0.026331    
2023-01-06 14:42:48,977 - Epoch: [123][  170/  246]    Overall Loss 0.229723    Objective Loss 0.229723                                        LR 0.000013    Time 0.026294    
2023-01-06 14:42:49,218 - Epoch: [123][  180/  246]    Overall Loss 0.228651    Objective Loss 0.228651                                        LR 0.000013    Time 0.026167    
2023-01-06 14:42:49,455 - Epoch: [123][  190/  246]    Overall Loss 0.229202    Objective Loss 0.229202                                        LR 0.000013    Time 0.026037    
2023-01-06 14:42:49,695 - Epoch: [123][  200/  246]    Overall Loss 0.229339    Objective Loss 0.229339                                        LR 0.000013    Time 0.025934    
2023-01-06 14:42:49,941 - Epoch: [123][  210/  246]    Overall Loss 0.229463    Objective Loss 0.229463                                        LR 0.000013    Time 0.025867    
2023-01-06 14:42:50,177 - Epoch: [123][  220/  246]    Overall Loss 0.229478    Objective Loss 0.229478                                        LR 0.000013    Time 0.025764    
2023-01-06 14:42:50,409 - Epoch: [123][  230/  246]    Overall Loss 0.229613    Objective Loss 0.229613                                        LR 0.000013    Time 0.025649    
2023-01-06 14:42:50,662 - Epoch: [123][  240/  246]    Overall Loss 0.230134    Objective Loss 0.230134                                        LR 0.000013    Time 0.025634    
2023-01-06 14:42:50,790 - Epoch: [123][  246/  246]    Overall Loss 0.230749    Objective Loss 0.230749    Top1 91.626794    LR 0.000013    Time 0.025528    
2023-01-06 14:42:50,923 - --- validate (epoch=123)-----------
2023-01-06 14:42:50,923 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:51,362 - Epoch: [123][   10/   28]    Loss 0.256943    Top1 90.546875    
2023-01-06 14:42:51,504 - Epoch: [123][   20/   28]    Loss 0.257899    Top1 90.429688    
2023-01-06 14:42:51,595 - Epoch: [123][   28/   28]    Loss 0.255572    Top1 90.767249    
2023-01-06 14:42:51,743 - ==> Top1: 90.767    Loss: 0.256

2023-01-06 14:42:51,743 - ==> Confusion:
[[ 259   15  165]
 [  15  285  302]
 [  69   79 5797]]

2023-01-06 14:42:51,744 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:51,745 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:51,754 - 

2023-01-06 14:42:51,755 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:52,481 - Epoch: [124][   10/  246]    Overall Loss 0.232755    Objective Loss 0.232755                                        LR 0.000013    Time 0.072610    
2023-01-06 14:42:52,724 - Epoch: [124][   20/  246]    Overall Loss 0.227928    Objective Loss 0.227928                                        LR 0.000013    Time 0.048390    
2023-01-06 14:42:52,952 - Epoch: [124][   30/  246]    Overall Loss 0.228961    Objective Loss 0.228961                                        LR 0.000013    Time 0.039879    
2023-01-06 14:42:53,191 - Epoch: [124][   40/  246]    Overall Loss 0.230717    Objective Loss 0.230717                                        LR 0.000013    Time 0.035861    
2023-01-06 14:42:53,413 - Epoch: [124][   50/  246]    Overall Loss 0.229020    Objective Loss 0.229020                                        LR 0.000013    Time 0.033132    
2023-01-06 14:42:53,663 - Epoch: [124][   60/  246]    Overall Loss 0.231003    Objective Loss 0.231003                                        LR 0.000013    Time 0.031759    
2023-01-06 14:42:53,921 - Epoch: [124][   70/  246]    Overall Loss 0.229885    Objective Loss 0.229885                                        LR 0.000013    Time 0.030908    
2023-01-06 14:42:54,175 - Epoch: [124][   80/  246]    Overall Loss 0.227431    Objective Loss 0.227431                                        LR 0.000013    Time 0.030208    
2023-01-06 14:42:54,425 - Epoch: [124][   90/  246]    Overall Loss 0.227029    Objective Loss 0.227029                                        LR 0.000013    Time 0.029625    
2023-01-06 14:42:54,668 - Epoch: [124][  100/  246]    Overall Loss 0.226571    Objective Loss 0.226571                                        LR 0.000013    Time 0.029087    
2023-01-06 14:42:54,909 - Epoch: [124][  110/  246]    Overall Loss 0.228688    Objective Loss 0.228688                                        LR 0.000013    Time 0.028633    
2023-01-06 14:42:55,152 - Epoch: [124][  120/  246]    Overall Loss 0.227986    Objective Loss 0.227986                                        LR 0.000013    Time 0.028271    
2023-01-06 14:42:55,394 - Epoch: [124][  130/  246]    Overall Loss 0.228815    Objective Loss 0.228815                                        LR 0.000013    Time 0.027955    
2023-01-06 14:42:55,633 - Epoch: [124][  140/  246]    Overall Loss 0.229124    Objective Loss 0.229124                                        LR 0.000013    Time 0.027664    
2023-01-06 14:42:55,877 - Epoch: [124][  150/  246]    Overall Loss 0.229808    Objective Loss 0.229808                                        LR 0.000013    Time 0.027441    
2023-01-06 14:42:56,119 - Epoch: [124][  160/  246]    Overall Loss 0.229105    Objective Loss 0.229105                                        LR 0.000013    Time 0.027241    
2023-01-06 14:42:56,370 - Epoch: [124][  170/  246]    Overall Loss 0.229384    Objective Loss 0.229384                                        LR 0.000013    Time 0.027110    
2023-01-06 14:42:56,619 - Epoch: [124][  180/  246]    Overall Loss 0.229858    Objective Loss 0.229858                                        LR 0.000013    Time 0.026985    
2023-01-06 14:42:56,870 - Epoch: [124][  190/  246]    Overall Loss 0.230801    Objective Loss 0.230801                                        LR 0.000013    Time 0.026879    
2023-01-06 14:42:57,120 - Epoch: [124][  200/  246]    Overall Loss 0.231221    Objective Loss 0.231221                                        LR 0.000013    Time 0.026785    
2023-01-06 14:42:57,368 - Epoch: [124][  210/  246]    Overall Loss 0.230924    Objective Loss 0.230924                                        LR 0.000013    Time 0.026687    
2023-01-06 14:42:57,615 - Epoch: [124][  220/  246]    Overall Loss 0.230952    Objective Loss 0.230952                                        LR 0.000013    Time 0.026597    
2023-01-06 14:42:57,863 - Epoch: [124][  230/  246]    Overall Loss 0.231427    Objective Loss 0.231427                                        LR 0.000013    Time 0.026514    
2023-01-06 14:42:58,099 - Epoch: [124][  240/  246]    Overall Loss 0.231269    Objective Loss 0.231269                                        LR 0.000013    Time 0.026391    
2023-01-06 14:42:58,214 - Epoch: [124][  246/  246]    Overall Loss 0.231156    Objective Loss 0.231156    Top1 90.669856    LR 0.000013    Time 0.026216    
2023-01-06 14:42:58,333 - --- validate (epoch=124)-----------
2023-01-06 14:42:58,333 - 6986 samples (256 per mini-batch)
2023-01-06 14:42:58,769 - Epoch: [124][   10/   28]    Loss 0.259675    Top1 90.703125    
2023-01-06 14:42:58,908 - Epoch: [124][   20/   28]    Loss 0.258963    Top1 90.546875    
2023-01-06 14:42:58,999 - Epoch: [124][   28/   28]    Loss 0.256947    Top1 90.581162    
2023-01-06 14:42:59,134 - ==> Top1: 90.581    Loss: 0.257

2023-01-06 14:42:59,134 - ==> Confusion:
[[ 254    9  176]
 [  20  269  313]
 [  67   73 5805]]

2023-01-06 14:42:59,135 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:42:59,136 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:42:59,146 - 

2023-01-06 14:42:59,146 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:42:59,858 - Epoch: [125][   10/  246]    Overall Loss 0.235455    Objective Loss 0.235455                                        LR 0.000013    Time 0.071155    
2023-01-06 14:43:00,069 - Epoch: [125][   20/  246]    Overall Loss 0.241784    Objective Loss 0.241784                                        LR 0.000013    Time 0.046080    
2023-01-06 14:43:00,294 - Epoch: [125][   30/  246]    Overall Loss 0.236407    Objective Loss 0.236407                                        LR 0.000013    Time 0.038208    
2023-01-06 14:43:00,529 - Epoch: [125][   40/  246]    Overall Loss 0.238672    Objective Loss 0.238672                                        LR 0.000013    Time 0.034534    
2023-01-06 14:43:00,759 - Epoch: [125][   50/  246]    Overall Loss 0.240975    Objective Loss 0.240975                                        LR 0.000013    Time 0.032217    
2023-01-06 14:43:00,995 - Epoch: [125][   60/  246]    Overall Loss 0.241505    Objective Loss 0.241505                                        LR 0.000013    Time 0.030769    
2023-01-06 14:43:01,229 - Epoch: [125][   70/  246]    Overall Loss 0.238495    Objective Loss 0.238495                                        LR 0.000013    Time 0.029710    
2023-01-06 14:43:01,459 - Epoch: [125][   80/  246]    Overall Loss 0.240007    Objective Loss 0.240007                                        LR 0.000013    Time 0.028872    
2023-01-06 14:43:01,693 - Epoch: [125][   90/  246]    Overall Loss 0.237662    Objective Loss 0.237662                                        LR 0.000013    Time 0.028262    
2023-01-06 14:43:01,926 - Epoch: [125][  100/  246]    Overall Loss 0.236846    Objective Loss 0.236846                                        LR 0.000013    Time 0.027763    
2023-01-06 14:43:02,154 - Epoch: [125][  110/  246]    Overall Loss 0.237719    Objective Loss 0.237719                                        LR 0.000013    Time 0.027309    
2023-01-06 14:43:02,393 - Epoch: [125][  120/  246]    Overall Loss 0.236541    Objective Loss 0.236541                                        LR 0.000013    Time 0.027019    
2023-01-06 14:43:02,623 - Epoch: [125][  130/  246]    Overall Loss 0.233759    Objective Loss 0.233759                                        LR 0.000013    Time 0.026706    
2023-01-06 14:43:02,860 - Epoch: [125][  140/  246]    Overall Loss 0.234773    Objective Loss 0.234773                                        LR 0.000013    Time 0.026491    
2023-01-06 14:43:03,095 - Epoch: [125][  150/  246]    Overall Loss 0.233271    Objective Loss 0.233271                                        LR 0.000013    Time 0.026288    
2023-01-06 14:43:03,327 - Epoch: [125][  160/  246]    Overall Loss 0.233204    Objective Loss 0.233204                                        LR 0.000013    Time 0.026092    
2023-01-06 14:43:03,563 - Epoch: [125][  170/  246]    Overall Loss 0.232777    Objective Loss 0.232777                                        LR 0.000013    Time 0.025941    
2023-01-06 14:43:03,801 - Epoch: [125][  180/  246]    Overall Loss 0.233082    Objective Loss 0.233082                                        LR 0.000013    Time 0.025818    
2023-01-06 14:43:04,043 - Epoch: [125][  190/  246]    Overall Loss 0.230131    Objective Loss 0.230131                                        LR 0.000013    Time 0.025731    
2023-01-06 14:43:04,274 - Epoch: [125][  200/  246]    Overall Loss 0.230009    Objective Loss 0.230009                                        LR 0.000013    Time 0.025600    
2023-01-06 14:43:04,506 - Epoch: [125][  210/  246]    Overall Loss 0.229546    Objective Loss 0.229546                                        LR 0.000013    Time 0.025481    
2023-01-06 14:43:04,740 - Epoch: [125][  220/  246]    Overall Loss 0.229708    Objective Loss 0.229708                                        LR 0.000013    Time 0.025388    
2023-01-06 14:43:04,972 - Epoch: [125][  230/  246]    Overall Loss 0.229941    Objective Loss 0.229941                                        LR 0.000013    Time 0.025289    
2023-01-06 14:43:05,211 - Epoch: [125][  240/  246]    Overall Loss 0.229862    Objective Loss 0.229862                                        LR 0.000013    Time 0.025230    
2023-01-06 14:43:05,338 - Epoch: [125][  246/  246]    Overall Loss 0.229380    Objective Loss 0.229380    Top1 92.822967    LR 0.000013    Time 0.025133    
2023-01-06 14:43:05,480 - --- validate (epoch=125)-----------
2023-01-06 14:43:05,481 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:05,951 - Epoch: [125][   10/   28]    Loss 0.264044    Top1 90.820312    
2023-01-06 14:43:06,111 - Epoch: [125][   20/   28]    Loss 0.256629    Top1 90.859375    
2023-01-06 14:43:06,204 - Epoch: [125][   28/   28]    Loss 0.259961    Top1 90.695677    
2023-01-06 14:43:06,324 - ==> Top1: 90.696    Loss: 0.260

2023-01-06 14:43:06,324 - ==> Confusion:
[[ 245   13  181]
 [  13  285  304]
 [  72   67 5806]]

2023-01-06 14:43:06,326 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:06,326 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:06,336 - 

2023-01-06 14:43:06,336 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:06,931 - Epoch: [126][   10/  246]    Overall Loss 0.242560    Objective Loss 0.242560                                        LR 0.000013    Time 0.059396    
2023-01-06 14:43:07,160 - Epoch: [126][   20/  246]    Overall Loss 0.241137    Objective Loss 0.241137                                        LR 0.000013    Time 0.041099    
2023-01-06 14:43:07,395 - Epoch: [126][   30/  246]    Overall Loss 0.237714    Objective Loss 0.237714                                        LR 0.000013    Time 0.035245    
2023-01-06 14:43:07,635 - Epoch: [126][   40/  246]    Overall Loss 0.240560    Objective Loss 0.240560                                        LR 0.000013    Time 0.032410    
2023-01-06 14:43:07,865 - Epoch: [126][   50/  246]    Overall Loss 0.235462    Objective Loss 0.235462                                        LR 0.000013    Time 0.030515    
2023-01-06 14:43:08,096 - Epoch: [126][   60/  246]    Overall Loss 0.234647    Objective Loss 0.234647                                        LR 0.000013    Time 0.029276    
2023-01-06 14:43:08,327 - Epoch: [126][   70/  246]    Overall Loss 0.233058    Objective Loss 0.233058                                        LR 0.000013    Time 0.028397    
2023-01-06 14:43:08,560 - Epoch: [126][   80/  246]    Overall Loss 0.233444    Objective Loss 0.233444                                        LR 0.000013    Time 0.027749    
2023-01-06 14:43:08,810 - Epoch: [126][   90/  246]    Overall Loss 0.232447    Objective Loss 0.232447                                        LR 0.000013    Time 0.027443    
2023-01-06 14:43:09,062 - Epoch: [126][  100/  246]    Overall Loss 0.230951    Objective Loss 0.230951                                        LR 0.000013    Time 0.027214    
2023-01-06 14:43:09,312 - Epoch: [126][  110/  246]    Overall Loss 0.232950    Objective Loss 0.232950                                        LR 0.000013    Time 0.027011    
2023-01-06 14:43:09,560 - Epoch: [126][  120/  246]    Overall Loss 0.232394    Objective Loss 0.232394                                        LR 0.000013    Time 0.026824    
2023-01-06 14:43:09,800 - Epoch: [126][  130/  246]    Overall Loss 0.231514    Objective Loss 0.231514                                        LR 0.000013    Time 0.026602    
2023-01-06 14:43:10,040 - Epoch: [126][  140/  246]    Overall Loss 0.231728    Objective Loss 0.231728                                        LR 0.000013    Time 0.026414    
2023-01-06 14:43:10,276 - Epoch: [126][  150/  246]    Overall Loss 0.231858    Objective Loss 0.231858                                        LR 0.000013    Time 0.026222    
2023-01-06 14:43:10,509 - Epoch: [126][  160/  246]    Overall Loss 0.231580    Objective Loss 0.231580                                        LR 0.000013    Time 0.026037    
2023-01-06 14:43:10,742 - Epoch: [126][  170/  246]    Overall Loss 0.230618    Objective Loss 0.230618                                        LR 0.000013    Time 0.025873    
2023-01-06 14:43:10,977 - Epoch: [126][  180/  246]    Overall Loss 0.231687    Objective Loss 0.231687                                        LR 0.000013    Time 0.025741    
2023-01-06 14:43:11,211 - Epoch: [126][  190/  246]    Overall Loss 0.231389    Objective Loss 0.231389                                        LR 0.000013    Time 0.025614    
2023-01-06 14:43:11,433 - Epoch: [126][  200/  246]    Overall Loss 0.231439    Objective Loss 0.231439                                        LR 0.000013    Time 0.025440    
2023-01-06 14:43:11,649 - Epoch: [126][  210/  246]    Overall Loss 0.230579    Objective Loss 0.230579                                        LR 0.000013    Time 0.025254    
2023-01-06 14:43:11,867 - Epoch: [126][  220/  246]    Overall Loss 0.230749    Objective Loss 0.230749                                        LR 0.000013    Time 0.025098    
2023-01-06 14:43:12,094 - Epoch: [126][  230/  246]    Overall Loss 0.230019    Objective Loss 0.230019                                        LR 0.000013    Time 0.024992    
2023-01-06 14:43:12,338 - Epoch: [126][  240/  246]    Overall Loss 0.229797    Objective Loss 0.229797                                        LR 0.000013    Time 0.024965    
2023-01-06 14:43:12,461 - Epoch: [126][  246/  246]    Overall Loss 0.230464    Objective Loss 0.230464    Top1 90.430622    LR 0.000013    Time 0.024854    
2023-01-06 14:43:12,602 - --- validate (epoch=126)-----------
2023-01-06 14:43:12,602 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:13,058 - Epoch: [126][   10/   28]    Loss 0.250078    Top1 91.250000    
2023-01-06 14:43:13,218 - Epoch: [126][   20/   28]    Loss 0.255718    Top1 90.546875    
2023-01-06 14:43:13,312 - Epoch: [126][   28/   28]    Loss 0.257725    Top1 90.667048    
2023-01-06 14:43:13,439 - ==> Top1: 90.667    Loss: 0.258

2023-01-06 14:43:13,440 - ==> Confusion:
[[ 258   19  162]
 [  15  310  277]
 [  71  108 5766]]

2023-01-06 14:43:13,441 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:13,441 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:13,451 - 

2023-01-06 14:43:13,451 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:14,176 - Epoch: [127][   10/  246]    Overall Loss 0.254441    Objective Loss 0.254441                                        LR 0.000013    Time 0.072402    
2023-01-06 14:43:14,420 - Epoch: [127][   20/  246]    Overall Loss 0.246318    Objective Loss 0.246318                                        LR 0.000013    Time 0.048406    
2023-01-06 14:43:14,669 - Epoch: [127][   30/  246]    Overall Loss 0.238243    Objective Loss 0.238243                                        LR 0.000013    Time 0.040545    
2023-01-06 14:43:14,910 - Epoch: [127][   40/  246]    Overall Loss 0.237238    Objective Loss 0.237238                                        LR 0.000013    Time 0.036434    
2023-01-06 14:43:15,162 - Epoch: [127][   50/  246]    Overall Loss 0.239110    Objective Loss 0.239110                                        LR 0.000013    Time 0.034163    
2023-01-06 14:43:15,418 - Epoch: [127][   60/  246]    Overall Loss 0.236553    Objective Loss 0.236553                                        LR 0.000013    Time 0.032732    
2023-01-06 14:43:15,674 - Epoch: [127][   70/  246]    Overall Loss 0.234807    Objective Loss 0.234807                                        LR 0.000013    Time 0.031705    
2023-01-06 14:43:15,911 - Epoch: [127][   80/  246]    Overall Loss 0.235259    Objective Loss 0.235259                                        LR 0.000013    Time 0.030703    
2023-01-06 14:43:16,154 - Epoch: [127][   90/  246]    Overall Loss 0.234244    Objective Loss 0.234244                                        LR 0.000013    Time 0.029980    
2023-01-06 14:43:16,393 - Epoch: [127][  100/  246]    Overall Loss 0.232421    Objective Loss 0.232421                                        LR 0.000013    Time 0.029374    
2023-01-06 14:43:16,638 - Epoch: [127][  110/  246]    Overall Loss 0.230895    Objective Loss 0.230895                                        LR 0.000013    Time 0.028920    
2023-01-06 14:43:16,884 - Epoch: [127][  120/  246]    Overall Loss 0.229590    Objective Loss 0.229590                                        LR 0.000013    Time 0.028546    
2023-01-06 14:43:17,129 - Epoch: [127][  130/  246]    Overall Loss 0.228815    Objective Loss 0.228815                                        LR 0.000013    Time 0.028234    
2023-01-06 14:43:17,374 - Epoch: [127][  140/  246]    Overall Loss 0.228148    Objective Loss 0.228148                                        LR 0.000013    Time 0.027959    
2023-01-06 14:43:17,611 - Epoch: [127][  150/  246]    Overall Loss 0.228231    Objective Loss 0.228231                                        LR 0.000013    Time 0.027673    
2023-01-06 14:43:17,850 - Epoch: [127][  160/  246]    Overall Loss 0.227879    Objective Loss 0.227879                                        LR 0.000013    Time 0.027438    
2023-01-06 14:43:18,091 - Epoch: [127][  170/  246]    Overall Loss 0.228220    Objective Loss 0.228220                                        LR 0.000013    Time 0.027234    
2023-01-06 14:43:18,334 - Epoch: [127][  180/  246]    Overall Loss 0.228219    Objective Loss 0.228219                                        LR 0.000013    Time 0.027070    
2023-01-06 14:43:18,574 - Epoch: [127][  190/  246]    Overall Loss 0.227965    Objective Loss 0.227965                                        LR 0.000013    Time 0.026908    
2023-01-06 14:43:18,809 - Epoch: [127][  200/  246]    Overall Loss 0.228353    Objective Loss 0.228353                                        LR 0.000013    Time 0.026736    
2023-01-06 14:43:19,041 - Epoch: [127][  210/  246]    Overall Loss 0.227996    Objective Loss 0.227996                                        LR 0.000013    Time 0.026565    
2023-01-06 14:43:19,276 - Epoch: [127][  220/  246]    Overall Loss 0.227785    Objective Loss 0.227785                                        LR 0.000013    Time 0.026424    
2023-01-06 14:43:19,511 - Epoch: [127][  230/  246]    Overall Loss 0.228102    Objective Loss 0.228102                                        LR 0.000013    Time 0.026293    
2023-01-06 14:43:19,760 - Epoch: [127][  240/  246]    Overall Loss 0.228378    Objective Loss 0.228378                                        LR 0.000013    Time 0.026236    
2023-01-06 14:43:19,887 - Epoch: [127][  246/  246]    Overall Loss 0.228426    Objective Loss 0.228426    Top1 94.019139    LR 0.000013    Time 0.026109    
2023-01-06 14:43:20,042 - --- validate (epoch=127)-----------
2023-01-06 14:43:20,042 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:20,482 - Epoch: [127][   10/   28]    Loss 0.246452    Top1 91.250000    
2023-01-06 14:43:20,621 - Epoch: [127][   20/   28]    Loss 0.253482    Top1 90.761719    
2023-01-06 14:43:20,711 - Epoch: [127][   28/   28]    Loss 0.259158    Top1 90.523905    
2023-01-06 14:43:20,860 - ==> Top1: 90.524    Loss: 0.259

2023-01-06 14:43:20,860 - ==> Confusion:
[[ 222   18  199]
 [   9  286  307]
 [  46   83 5816]]

2023-01-06 14:43:20,862 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:20,862 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:20,872 - 

2023-01-06 14:43:20,872 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:21,451 - Epoch: [128][   10/  246]    Overall Loss 0.221268    Objective Loss 0.221268                                        LR 0.000013    Time 0.057751    
2023-01-06 14:43:21,673 - Epoch: [128][   20/  246]    Overall Loss 0.218826    Objective Loss 0.218826                                        LR 0.000013    Time 0.039960    
2023-01-06 14:43:21,904 - Epoch: [128][   30/  246]    Overall Loss 0.222896    Objective Loss 0.222896                                        LR 0.000013    Time 0.034344    
2023-01-06 14:43:22,155 - Epoch: [128][   40/  246]    Overall Loss 0.222968    Objective Loss 0.222968                                        LR 0.000013    Time 0.032012    
2023-01-06 14:43:22,395 - Epoch: [128][   50/  246]    Overall Loss 0.227368    Objective Loss 0.227368                                        LR 0.000013    Time 0.030407    
2023-01-06 14:43:22,638 - Epoch: [128][   60/  246]    Overall Loss 0.226495    Objective Loss 0.226495                                        LR 0.000013    Time 0.029385    
2023-01-06 14:43:22,872 - Epoch: [128][   70/  246]    Overall Loss 0.227566    Objective Loss 0.227566                                        LR 0.000013    Time 0.028522    
2023-01-06 14:43:23,123 - Epoch: [128][   80/  246]    Overall Loss 0.226351    Objective Loss 0.226351                                        LR 0.000013    Time 0.028082    
2023-01-06 14:43:23,367 - Epoch: [128][   90/  246]    Overall Loss 0.226449    Objective Loss 0.226449                                        LR 0.000013    Time 0.027668    
2023-01-06 14:43:23,610 - Epoch: [128][  100/  246]    Overall Loss 0.225645    Objective Loss 0.225645                                        LR 0.000013    Time 0.027332    
2023-01-06 14:43:23,858 - Epoch: [128][  110/  246]    Overall Loss 0.224457    Objective Loss 0.224457                                        LR 0.000013    Time 0.027091    
2023-01-06 14:43:24,105 - Epoch: [128][  120/  246]    Overall Loss 0.225014    Objective Loss 0.225014                                        LR 0.000013    Time 0.026892    
2023-01-06 14:43:24,360 - Epoch: [128][  130/  246]    Overall Loss 0.225270    Objective Loss 0.225270                                        LR 0.000013    Time 0.026766    
2023-01-06 14:43:24,613 - Epoch: [128][  140/  246]    Overall Loss 0.225704    Objective Loss 0.225704                                        LR 0.000013    Time 0.026662    
2023-01-06 14:43:24,866 - Epoch: [128][  150/  246]    Overall Loss 0.226246    Objective Loss 0.226246                                        LR 0.000013    Time 0.026563    
2023-01-06 14:43:25,121 - Epoch: [128][  160/  246]    Overall Loss 0.225126    Objective Loss 0.225126                                        LR 0.000013    Time 0.026493    
2023-01-06 14:43:25,362 - Epoch: [128][  170/  246]    Overall Loss 0.226756    Objective Loss 0.226756                                        LR 0.000013    Time 0.026354    
2023-01-06 14:43:25,601 - Epoch: [128][  180/  246]    Overall Loss 0.227272    Objective Loss 0.227272                                        LR 0.000013    Time 0.026215    
2023-01-06 14:43:25,845 - Epoch: [128][  190/  246]    Overall Loss 0.228007    Objective Loss 0.228007                                        LR 0.000013    Time 0.026104    
2023-01-06 14:43:26,094 - Epoch: [128][  200/  246]    Overall Loss 0.228818    Objective Loss 0.228818                                        LR 0.000013    Time 0.026045    
2023-01-06 14:43:26,339 - Epoch: [128][  210/  246]    Overall Loss 0.228912    Objective Loss 0.228912                                        LR 0.000013    Time 0.025971    
2023-01-06 14:43:26,587 - Epoch: [128][  220/  246]    Overall Loss 0.229166    Objective Loss 0.229166                                        LR 0.000013    Time 0.025917    
2023-01-06 14:43:26,835 - Epoch: [128][  230/  246]    Overall Loss 0.229330    Objective Loss 0.229330                                        LR 0.000013    Time 0.025866    
2023-01-06 14:43:27,090 - Epoch: [128][  240/  246]    Overall Loss 0.229433    Objective Loss 0.229433                                        LR 0.000013    Time 0.025848    
2023-01-06 14:43:27,211 - Epoch: [128][  246/  246]    Overall Loss 0.229835    Objective Loss 0.229835    Top1 91.866029    LR 0.000013    Time 0.025709    
2023-01-06 14:43:27,334 - --- validate (epoch=128)-----------
2023-01-06 14:43:27,334 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:27,808 - Epoch: [128][   10/   28]    Loss 0.258846    Top1 90.234375    
2023-01-06 14:43:27,962 - Epoch: [128][   20/   28]    Loss 0.248915    Top1 90.898438    
2023-01-06 14:43:28,052 - Epoch: [128][   28/   28]    Loss 0.253429    Top1 90.724306    
2023-01-06 14:43:28,183 - ==> Top1: 90.724    Loss: 0.253

2023-01-06 14:43:28,183 - ==> Confusion:
[[ 250   13  176]
 [  13  288  301]
 [  56   89 5800]]

2023-01-06 14:43:28,185 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:28,185 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:28,195 - 

2023-01-06 14:43:28,195 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:28,900 - Epoch: [129][   10/  246]    Overall Loss 0.213062    Objective Loss 0.213062                                        LR 0.000013    Time 0.070402    
2023-01-06 14:43:29,141 - Epoch: [129][   20/  246]    Overall Loss 0.232819    Objective Loss 0.232819                                        LR 0.000013    Time 0.047243    
2023-01-06 14:43:29,398 - Epoch: [129][   30/  246]    Overall Loss 0.224090    Objective Loss 0.224090                                        LR 0.000013    Time 0.039990    
2023-01-06 14:43:29,655 - Epoch: [129][   40/  246]    Overall Loss 0.223461    Objective Loss 0.223461                                        LR 0.000013    Time 0.036410    
2023-01-06 14:43:29,928 - Epoch: [129][   50/  246]    Overall Loss 0.223031    Objective Loss 0.223031                                        LR 0.000013    Time 0.034581    
2023-01-06 14:43:30,190 - Epoch: [129][   60/  246]    Overall Loss 0.223583    Objective Loss 0.223583                                        LR 0.000013    Time 0.033183    
2023-01-06 14:43:30,456 - Epoch: [129][   70/  246]    Overall Loss 0.225426    Objective Loss 0.225426                                        LR 0.000013    Time 0.032228    
2023-01-06 14:43:30,716 - Epoch: [129][   80/  246]    Overall Loss 0.225655    Objective Loss 0.225655                                        LR 0.000013    Time 0.031449    
2023-01-06 14:43:30,978 - Epoch: [129][   90/  246]    Overall Loss 0.225443    Objective Loss 0.225443                                        LR 0.000013    Time 0.030858    
2023-01-06 14:43:31,238 - Epoch: [129][  100/  246]    Overall Loss 0.225144    Objective Loss 0.225144                                        LR 0.000013    Time 0.030364    
2023-01-06 14:43:31,501 - Epoch: [129][  110/  246]    Overall Loss 0.226020    Objective Loss 0.226020                                        LR 0.000013    Time 0.029991    
2023-01-06 14:43:31,758 - Epoch: [129][  120/  246]    Overall Loss 0.227223    Objective Loss 0.227223                                        LR 0.000013    Time 0.029630    
2023-01-06 14:43:32,021 - Epoch: [129][  130/  246]    Overall Loss 0.227310    Objective Loss 0.227310                                        LR 0.000013    Time 0.029371    
2023-01-06 14:43:32,281 - Epoch: [129][  140/  246]    Overall Loss 0.230213    Objective Loss 0.230213                                        LR 0.000013    Time 0.029123    
2023-01-06 14:43:32,546 - Epoch: [129][  150/  246]    Overall Loss 0.229760    Objective Loss 0.229760                                        LR 0.000013    Time 0.028944    
2023-01-06 14:43:32,805 - Epoch: [129][  160/  246]    Overall Loss 0.229426    Objective Loss 0.229426                                        LR 0.000013    Time 0.028754    
2023-01-06 14:43:33,068 - Epoch: [129][  170/  246]    Overall Loss 0.229116    Objective Loss 0.229116                                        LR 0.000013    Time 0.028606    
2023-01-06 14:43:33,321 - Epoch: [129][  180/  246]    Overall Loss 0.228055    Objective Loss 0.228055                                        LR 0.000013    Time 0.028416    
2023-01-06 14:43:33,580 - Epoch: [129][  190/  246]    Overall Loss 0.228747    Objective Loss 0.228747                                        LR 0.000013    Time 0.028281    
2023-01-06 14:43:33,835 - Epoch: [129][  200/  246]    Overall Loss 0.227924    Objective Loss 0.227924                                        LR 0.000013    Time 0.028141    
2023-01-06 14:43:34,094 - Epoch: [129][  210/  246]    Overall Loss 0.228264    Objective Loss 0.228264                                        LR 0.000013    Time 0.028034    
2023-01-06 14:43:34,355 - Epoch: [129][  220/  246]    Overall Loss 0.229321    Objective Loss 0.229321                                        LR 0.000013    Time 0.027941    
2023-01-06 14:43:34,600 - Epoch: [129][  230/  246]    Overall Loss 0.230494    Objective Loss 0.230494                                        LR 0.000013    Time 0.027784    
2023-01-06 14:43:34,857 - Epoch: [129][  240/  246]    Overall Loss 0.230636    Objective Loss 0.230636                                        LR 0.000013    Time 0.027696    
2023-01-06 14:43:34,983 - Epoch: [129][  246/  246]    Overall Loss 0.230499    Objective Loss 0.230499    Top1 91.387560    LR 0.000013    Time 0.027530    
2023-01-06 14:43:35,109 - --- validate (epoch=129)-----------
2023-01-06 14:43:35,109 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:35,565 - Epoch: [129][   10/   28]    Loss 0.266991    Top1 90.390625    
2023-01-06 14:43:35,731 - Epoch: [129][   20/   28]    Loss 0.258462    Top1 90.820312    
2023-01-06 14:43:35,824 - Epoch: [129][   28/   28]    Loss 0.258223    Top1 90.853135    
2023-01-06 14:43:35,957 - ==> Top1: 90.853    Loss: 0.258

2023-01-06 14:43:35,957 - ==> Confusion:
[[ 254   10  175]
 [  13  294  295]
 [  69   77 5799]]

2023-01-06 14:43:35,959 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:35,959 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:35,969 - 

2023-01-06 14:43:35,969 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:36,708 - Epoch: [130][   10/  246]    Overall Loss 0.210757    Objective Loss 0.210757                                        LR 0.000013    Time 0.073793    
2023-01-06 14:43:36,944 - Epoch: [130][   20/  246]    Overall Loss 0.219251    Objective Loss 0.219251                                        LR 0.000013    Time 0.048674    
2023-01-06 14:43:37,179 - Epoch: [130][   30/  246]    Overall Loss 0.218030    Objective Loss 0.218030                                        LR 0.000013    Time 0.040276    
2023-01-06 14:43:37,413 - Epoch: [130][   40/  246]    Overall Loss 0.219452    Objective Loss 0.219452                                        LR 0.000013    Time 0.036040    
2023-01-06 14:43:37,633 - Epoch: [130][   50/  246]    Overall Loss 0.221273    Objective Loss 0.221273                                        LR 0.000013    Time 0.033224    
2023-01-06 14:43:37,864 - Epoch: [130][   60/  246]    Overall Loss 0.226274    Objective Loss 0.226274                                        LR 0.000013    Time 0.031534    
2023-01-06 14:43:38,102 - Epoch: [130][   70/  246]    Overall Loss 0.228675    Objective Loss 0.228675                                        LR 0.000013    Time 0.030416    
2023-01-06 14:43:38,338 - Epoch: [130][   80/  246]    Overall Loss 0.229691    Objective Loss 0.229691                                        LR 0.000013    Time 0.029556    
2023-01-06 14:43:38,579 - Epoch: [130][   90/  246]    Overall Loss 0.231338    Objective Loss 0.231338                                        LR 0.000013    Time 0.028955    
2023-01-06 14:43:38,823 - Epoch: [130][  100/  246]    Overall Loss 0.230286    Objective Loss 0.230286                                        LR 0.000013    Time 0.028493    
2023-01-06 14:43:39,060 - Epoch: [130][  110/  246]    Overall Loss 0.228998    Objective Loss 0.228998                                        LR 0.000013    Time 0.028056    
2023-01-06 14:43:39,297 - Epoch: [130][  120/  246]    Overall Loss 0.229239    Objective Loss 0.229239                                        LR 0.000013    Time 0.027686    
2023-01-06 14:43:39,541 - Epoch: [130][  130/  246]    Overall Loss 0.226981    Objective Loss 0.226981                                        LR 0.000013    Time 0.027431    
2023-01-06 14:43:39,796 - Epoch: [130][  140/  246]    Overall Loss 0.226902    Objective Loss 0.226902                                        LR 0.000013    Time 0.027279    
2023-01-06 14:43:40,048 - Epoch: [130][  150/  246]    Overall Loss 0.227705    Objective Loss 0.227705                                        LR 0.000013    Time 0.027133    
2023-01-06 14:43:40,286 - Epoch: [130][  160/  246]    Overall Loss 0.228018    Objective Loss 0.228018                                        LR 0.000013    Time 0.026926    
2023-01-06 14:43:40,530 - Epoch: [130][  170/  246]    Overall Loss 0.227025    Objective Loss 0.227025                                        LR 0.000013    Time 0.026772    
2023-01-06 14:43:40,783 - Epoch: [130][  180/  246]    Overall Loss 0.227838    Objective Loss 0.227838                                        LR 0.000013    Time 0.026681    
2023-01-06 14:43:41,032 - Epoch: [130][  190/  246]    Overall Loss 0.228102    Objective Loss 0.228102                                        LR 0.000013    Time 0.026582    
2023-01-06 14:43:41,287 - Epoch: [130][  200/  246]    Overall Loss 0.228626    Objective Loss 0.228626                                        LR 0.000013    Time 0.026520    
2023-01-06 14:43:41,541 - Epoch: [130][  210/  246]    Overall Loss 0.228494    Objective Loss 0.228494                                        LR 0.000013    Time 0.026462    
2023-01-06 14:43:41,794 - Epoch: [130][  220/  246]    Overall Loss 0.227795    Objective Loss 0.227795                                        LR 0.000013    Time 0.026408    
2023-01-06 14:43:42,044 - Epoch: [130][  230/  246]    Overall Loss 0.227464    Objective Loss 0.227464                                        LR 0.000013    Time 0.026347    
2023-01-06 14:43:42,308 - Epoch: [130][  240/  246]    Overall Loss 0.227361    Objective Loss 0.227361                                        LR 0.000013    Time 0.026348    
2023-01-06 14:43:42,437 - Epoch: [130][  246/  246]    Overall Loss 0.226914    Objective Loss 0.226914    Top1 92.344498    LR 0.000013    Time 0.026227    
2023-01-06 14:43:42,620 - --- validate (epoch=130)-----------
2023-01-06 14:43:42,620 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:43,076 - Epoch: [130][   10/   28]    Loss 0.261366    Top1 91.250000    
2023-01-06 14:43:43,217 - Epoch: [130][   20/   28]    Loss 0.255086    Top1 90.996094    
2023-01-06 14:43:43,310 - Epoch: [130][   28/   28]    Loss 0.254393    Top1 90.867449    
2023-01-06 14:43:43,449 - ==> Top1: 90.867    Loss: 0.254

2023-01-06 14:43:43,449 - ==> Confusion:
[[ 258   17  164]
 [  12  304  286]
 [  68   91 5786]]

2023-01-06 14:43:43,451 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:43,451 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:43,461 - 

2023-01-06 14:43:43,461 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:44,055 - Epoch: [131][   10/  246]    Overall Loss 0.225043    Objective Loss 0.225043                                        LR 0.000013    Time 0.059338    
2023-01-06 14:43:44,303 - Epoch: [131][   20/  246]    Overall Loss 0.231617    Objective Loss 0.231617                                        LR 0.000013    Time 0.042039    
2023-01-06 14:43:44,546 - Epoch: [131][   30/  246]    Overall Loss 0.226749    Objective Loss 0.226749                                        LR 0.000013    Time 0.036117    
2023-01-06 14:43:44,788 - Epoch: [131][   40/  246]    Overall Loss 0.228237    Objective Loss 0.228237                                        LR 0.000013    Time 0.033116    
2023-01-06 14:43:45,042 - Epoch: [131][   50/  246]    Overall Loss 0.232502    Objective Loss 0.232502                                        LR 0.000013    Time 0.031566    
2023-01-06 14:43:45,293 - Epoch: [131][   60/  246]    Overall Loss 0.228851    Objective Loss 0.228851                                        LR 0.000013    Time 0.030469    
2023-01-06 14:43:45,547 - Epoch: [131][   70/  246]    Overall Loss 0.226294    Objective Loss 0.226294                                        LR 0.000013    Time 0.029736    
2023-01-06 14:43:45,800 - Epoch: [131][   80/  246]    Overall Loss 0.224989    Objective Loss 0.224989                                        LR 0.000013    Time 0.029177    
2023-01-06 14:43:46,062 - Epoch: [131][   90/  246]    Overall Loss 0.227343    Objective Loss 0.227343                                        LR 0.000013    Time 0.028834    
2023-01-06 14:43:46,312 - Epoch: [131][  100/  246]    Overall Loss 0.226895    Objective Loss 0.226895                                        LR 0.000013    Time 0.028452    
2023-01-06 14:43:46,566 - Epoch: [131][  110/  246]    Overall Loss 0.226694    Objective Loss 0.226694                                        LR 0.000013    Time 0.028152    
2023-01-06 14:43:46,816 - Epoch: [131][  120/  246]    Overall Loss 0.225754    Objective Loss 0.225754                                        LR 0.000013    Time 0.027871    
2023-01-06 14:43:47,069 - Epoch: [131][  130/  246]    Overall Loss 0.227222    Objective Loss 0.227222                                        LR 0.000013    Time 0.027658    
2023-01-06 14:43:47,318 - Epoch: [131][  140/  246]    Overall Loss 0.227039    Objective Loss 0.227039                                        LR 0.000013    Time 0.027450    
2023-01-06 14:43:47,571 - Epoch: [131][  150/  246]    Overall Loss 0.227688    Objective Loss 0.227688                                        LR 0.000013    Time 0.027290    
2023-01-06 14:43:47,820 - Epoch: [131][  160/  246]    Overall Loss 0.228196    Objective Loss 0.228196                                        LR 0.000013    Time 0.027131    
2023-01-06 14:43:48,075 - Epoch: [131][  170/  246]    Overall Loss 0.228624    Objective Loss 0.228624                                        LR 0.000013    Time 0.027020    
2023-01-06 14:43:48,324 - Epoch: [131][  180/  246]    Overall Loss 0.229132    Objective Loss 0.229132                                        LR 0.000013    Time 0.026902    
2023-01-06 14:43:48,576 - Epoch: [131][  190/  246]    Overall Loss 0.228866    Objective Loss 0.228866                                        LR 0.000013    Time 0.026812    
2023-01-06 14:43:48,826 - Epoch: [131][  200/  246]    Overall Loss 0.229063    Objective Loss 0.229063                                        LR 0.000013    Time 0.026711    
2023-01-06 14:43:49,069 - Epoch: [131][  210/  246]    Overall Loss 0.229155    Objective Loss 0.229155                                        LR 0.000013    Time 0.026588    
2023-01-06 14:43:49,313 - Epoch: [131][  220/  246]    Overall Loss 0.229568    Objective Loss 0.229568                                        LR 0.000013    Time 0.026485    
2023-01-06 14:43:49,559 - Epoch: [131][  230/  246]    Overall Loss 0.229799    Objective Loss 0.229799                                        LR 0.000013    Time 0.026402    
2023-01-06 14:43:49,819 - Epoch: [131][  240/  246]    Overall Loss 0.228847    Objective Loss 0.228847                                        LR 0.000013    Time 0.026382    
2023-01-06 14:43:49,949 - Epoch: [131][  246/  246]    Overall Loss 0.228594    Objective Loss 0.228594    Top1 91.387560    LR 0.000013    Time 0.026267    
2023-01-06 14:43:50,074 - --- validate (epoch=131)-----------
2023-01-06 14:43:50,074 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:50,526 - Epoch: [131][   10/   28]    Loss 0.265666    Top1 90.507812    
2023-01-06 14:43:50,673 - Epoch: [131][   20/   28]    Loss 0.245644    Top1 91.132812    
2023-01-06 14:43:50,765 - Epoch: [131][   28/   28]    Loss 0.248562    Top1 90.795877    
2023-01-06 14:43:50,901 - ==> Top1: 90.796    Loss: 0.249

2023-01-06 14:43:50,901 - ==> Confusion:
[[ 259   12  168]
 [  15  298  289]
 [  70   89 5786]]

2023-01-06 14:43:50,902 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:50,902 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:50,912 - 

2023-01-06 14:43:50,912 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:51,674 - Epoch: [132][   10/  246]    Overall Loss 0.238882    Objective Loss 0.238882                                        LR 0.000013    Time 0.076067    
2023-01-06 14:43:51,928 - Epoch: [132][   20/  246]    Overall Loss 0.229574    Objective Loss 0.229574                                        LR 0.000013    Time 0.050737    
2023-01-06 14:43:52,185 - Epoch: [132][   30/  246]    Overall Loss 0.230620    Objective Loss 0.230620                                        LR 0.000013    Time 0.042334    
2023-01-06 14:43:52,433 - Epoch: [132][   40/  246]    Overall Loss 0.227196    Objective Loss 0.227196                                        LR 0.000013    Time 0.037923    
2023-01-06 14:43:52,698 - Epoch: [132][   50/  246]    Overall Loss 0.228407    Objective Loss 0.228407                                        LR 0.000013    Time 0.035609    
2023-01-06 14:43:52,962 - Epoch: [132][   60/  246]    Overall Loss 0.229274    Objective Loss 0.229274                                        LR 0.000013    Time 0.034069    
2023-01-06 14:43:53,223 - Epoch: [132][   70/  246]    Overall Loss 0.230696    Objective Loss 0.230696                                        LR 0.000013    Time 0.032937    
2023-01-06 14:43:53,485 - Epoch: [132][   80/  246]    Overall Loss 0.230118    Objective Loss 0.230118                                        LR 0.000013    Time 0.032084    
2023-01-06 14:43:53,739 - Epoch: [132][   90/  246]    Overall Loss 0.229553    Objective Loss 0.229553                                        LR 0.000013    Time 0.031342    
2023-01-06 14:43:53,989 - Epoch: [132][  100/  246]    Overall Loss 0.228652    Objective Loss 0.228652                                        LR 0.000013    Time 0.030705    
2023-01-06 14:43:54,255 - Epoch: [132][  110/  246]    Overall Loss 0.228398    Objective Loss 0.228398                                        LR 0.000013    Time 0.030325    
2023-01-06 14:43:54,517 - Epoch: [132][  120/  246]    Overall Loss 0.229500    Objective Loss 0.229500                                        LR 0.000013    Time 0.029980    
2023-01-06 14:43:54,771 - Epoch: [132][  130/  246]    Overall Loss 0.229112    Objective Loss 0.229112                                        LR 0.000013    Time 0.029625    
2023-01-06 14:43:55,025 - Epoch: [132][  140/  246]    Overall Loss 0.229708    Objective Loss 0.229708                                        LR 0.000013    Time 0.029316    
2023-01-06 14:43:55,281 - Epoch: [132][  150/  246]    Overall Loss 0.227913    Objective Loss 0.227913                                        LR 0.000013    Time 0.029068    
2023-01-06 14:43:55,536 - Epoch: [132][  160/  246]    Overall Loss 0.227274    Objective Loss 0.227274                                        LR 0.000013    Time 0.028842    
2023-01-06 14:43:55,790 - Epoch: [132][  170/  246]    Overall Loss 0.227392    Objective Loss 0.227392                                        LR 0.000013    Time 0.028640    
2023-01-06 14:43:56,044 - Epoch: [132][  180/  246]    Overall Loss 0.228263    Objective Loss 0.228263                                        LR 0.000013    Time 0.028459    
2023-01-06 14:43:56,299 - Epoch: [132][  190/  246]    Overall Loss 0.228569    Objective Loss 0.228569                                        LR 0.000013    Time 0.028298    
2023-01-06 14:43:56,555 - Epoch: [132][  200/  246]    Overall Loss 0.229568    Objective Loss 0.229568                                        LR 0.000013    Time 0.028161    
2023-01-06 14:43:56,809 - Epoch: [132][  210/  246]    Overall Loss 0.229311    Objective Loss 0.229311                                        LR 0.000013    Time 0.028028    
2023-01-06 14:43:57,065 - Epoch: [132][  220/  246]    Overall Loss 0.229628    Objective Loss 0.229628                                        LR 0.000013    Time 0.027917    
2023-01-06 14:43:57,317 - Epoch: [132][  230/  246]    Overall Loss 0.229556    Objective Loss 0.229556                                        LR 0.000013    Time 0.027794    
2023-01-06 14:43:57,579 - Epoch: [132][  240/  246]    Overall Loss 0.229688    Objective Loss 0.229688                                        LR 0.000013    Time 0.027727    
2023-01-06 14:43:57,709 - Epoch: [132][  246/  246]    Overall Loss 0.229184    Objective Loss 0.229184    Top1 92.105263    LR 0.000013    Time 0.027579    
2023-01-06 14:43:57,844 - --- validate (epoch=132)-----------
2023-01-06 14:43:57,844 - 6986 samples (256 per mini-batch)
2023-01-06 14:43:58,296 - Epoch: [132][   10/   28]    Loss 0.248469    Top1 91.523438    
2023-01-06 14:43:58,443 - Epoch: [132][   20/   28]    Loss 0.258330    Top1 90.820312    
2023-01-06 14:43:58,535 - Epoch: [132][   28/   28]    Loss 0.259643    Top1 90.738620    
2023-01-06 14:43:58,676 - ==> Top1: 90.739    Loss: 0.260

2023-01-06 14:43:58,676 - ==> Confusion:
[[ 227   10  202]
 [  12  267  323]
 [  44   56 5845]]

2023-01-06 14:43:58,678 - ==> Best [Top1: 90.939   Sparsity:0.00   Params: 360896 on epoch: 105]
2023-01-06 14:43:58,678 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:43:58,688 - 

2023-01-06 14:43:58,688 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:43:59,285 - Epoch: [133][   10/  246]    Overall Loss 0.240481    Objective Loss 0.240481                                        LR 0.000013    Time 0.059654    
2023-01-06 14:43:59,534 - Epoch: [133][   20/  246]    Overall Loss 0.230851    Objective Loss 0.230851                                        LR 0.000013    Time 0.042260    
2023-01-06 14:43:59,792 - Epoch: [133][   30/  246]    Overall Loss 0.233743    Objective Loss 0.233743                                        LR 0.000013    Time 0.036741    
2023-01-06 14:44:00,043 - Epoch: [133][   40/  246]    Overall Loss 0.234947    Objective Loss 0.234947                                        LR 0.000013    Time 0.033836    
2023-01-06 14:44:00,291 - Epoch: [133][   50/  246]    Overall Loss 0.230944    Objective Loss 0.230944                                        LR 0.000013    Time 0.032016    
2023-01-06 14:44:00,541 - Epoch: [133][   60/  246]    Overall Loss 0.231620    Objective Loss 0.231620                                        LR 0.000013    Time 0.030844    
2023-01-06 14:44:00,792 - Epoch: [133][   70/  246]    Overall Loss 0.230605    Objective Loss 0.230605                                        LR 0.000013    Time 0.030003    
2023-01-06 14:44:01,047 - Epoch: [133][   80/  246]    Overall Loss 0.231021    Objective Loss 0.231021                                        LR 0.000013    Time 0.029444    
2023-01-06 14:44:01,294 - Epoch: [133][   90/  246]    Overall Loss 0.231454    Objective Loss 0.231454                                        LR 0.000013    Time 0.028909    
2023-01-06 14:44:01,543 - Epoch: [133][  100/  246]    Overall Loss 0.231741    Objective Loss 0.231741                                        LR 0.000013    Time 0.028502    
2023-01-06 14:44:01,791 - Epoch: [133][  110/  246]    Overall Loss 0.233829    Objective Loss 0.233829                                        LR 0.000013    Time 0.028149    
2023-01-06 14:44:02,041 - Epoch: [133][  120/  246]    Overall Loss 0.233199    Objective Loss 0.233199                                        LR 0.000013    Time 0.027869    
2023-01-06 14:44:02,288 - Epoch: [133][  130/  246]    Overall Loss 0.234942    Objective Loss 0.234942                                        LR 0.000013    Time 0.027611    
2023-01-06 14:44:02,533 - Epoch: [133][  140/  246]    Overall Loss 0.234145    Objective Loss 0.234145                                        LR 0.000013    Time 0.027390    
2023-01-06 14:44:02,779 - Epoch: [133][  150/  246]    Overall Loss 0.231445    Objective Loss 0.231445                                        LR 0.000013    Time 0.027199    
2023-01-06 14:44:03,025 - Epoch: [133][  160/  246]    Overall Loss 0.231815    Objective Loss 0.231815                                        LR 0.000013    Time 0.027037    
2023-01-06 14:44:03,271 - Epoch: [133][  170/  246]    Overall Loss 0.232634    Objective Loss 0.232634                                        LR 0.000013    Time 0.026889    
2023-01-06 14:44:03,522 - Epoch: [133][  180/  246]    Overall Loss 0.232497    Objective Loss 0.232497                                        LR 0.000013    Time 0.026785    
2023-01-06 14:44:03,773 - Epoch: [133][  190/  246]    Overall Loss 0.231402    Objective Loss 0.231402                                        LR 0.000013    Time 0.026698    
2023-01-06 14:44:04,029 - Epoch: [133][  200/  246]    Overall Loss 0.230746    Objective Loss 0.230746                                        LR 0.000013    Time 0.026642    
2023-01-06 14:44:04,281 - Epoch: [133][  210/  246]    Overall Loss 0.230988    Objective Loss 0.230988                                        LR 0.000013    Time 0.026571    
2023-01-06 14:44:04,540 - Epoch: [133][  220/  246]    Overall Loss 0.230528    Objective Loss 0.230528                                        LR 0.000013    Time 0.026527    
2023-01-06 14:44:04,796 - Epoch: [133][  230/  246]    Overall Loss 0.230393    Objective Loss 0.230393                                        LR 0.000013    Time 0.026478    
2023-01-06 14:44:05,064 - Epoch: [133][  240/  246]    Overall Loss 0.230629    Objective Loss 0.230629                                        LR 0.000013    Time 0.026493    
2023-01-06 14:44:05,197 - Epoch: [133][  246/  246]    Overall Loss 0.230150    Objective Loss 0.230150    Top1 92.822967    LR 0.000013    Time 0.026384    
2023-01-06 14:44:05,341 - --- validate (epoch=133)-----------
2023-01-06 14:44:05,341 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:05,792 - Epoch: [133][   10/   28]    Loss 0.256473    Top1 91.093750    
2023-01-06 14:44:05,937 - Epoch: [133][   20/   28]    Loss 0.257659    Top1 90.937500    
2023-01-06 14:44:06,028 - Epoch: [133][   28/   28]    Loss 0.254210    Top1 90.953335    
2023-01-06 14:44:06,142 - ==> Top1: 90.953    Loss: 0.254

2023-01-06 14:44:06,142 - ==> Confusion:
[[ 256   15  168]
 [  12  311  279]
 [  66   92 5787]]

2023-01-06 14:44:06,144 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:06,144 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:06,169 - 

2023-01-06 14:44:06,169 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:06,874 - Epoch: [134][   10/  246]    Overall Loss 0.237213    Objective Loss 0.237213                                        LR 0.000013    Time 0.070463    
2023-01-06 14:44:07,132 - Epoch: [134][   20/  246]    Overall Loss 0.232081    Objective Loss 0.232081                                        LR 0.000013    Time 0.048091    
2023-01-06 14:44:07,385 - Epoch: [134][   30/  246]    Overall Loss 0.235255    Objective Loss 0.235255                                        LR 0.000013    Time 0.040468    
2023-01-06 14:44:07,643 - Epoch: [134][   40/  246]    Overall Loss 0.237884    Objective Loss 0.237884                                        LR 0.000013    Time 0.036809    
2023-01-06 14:44:07,904 - Epoch: [134][   50/  246]    Overall Loss 0.236538    Objective Loss 0.236538                                        LR 0.000013    Time 0.034649    
2023-01-06 14:44:08,172 - Epoch: [134][   60/  246]    Overall Loss 0.235324    Objective Loss 0.235324                                        LR 0.000013    Time 0.033341    
2023-01-06 14:44:08,421 - Epoch: [134][   70/  246]    Overall Loss 0.235702    Objective Loss 0.235702                                        LR 0.000013    Time 0.032124    
2023-01-06 14:44:08,676 - Epoch: [134][   80/  246]    Overall Loss 0.234266    Objective Loss 0.234266                                        LR 0.000013    Time 0.031297    
2023-01-06 14:44:08,937 - Epoch: [134][   90/  246]    Overall Loss 0.233320    Objective Loss 0.233320                                        LR 0.000013    Time 0.030714    
2023-01-06 14:44:09,178 - Epoch: [134][  100/  246]    Overall Loss 0.231979    Objective Loss 0.231979                                        LR 0.000013    Time 0.030047    
2023-01-06 14:44:09,413 - Epoch: [134][  110/  246]    Overall Loss 0.230555    Objective Loss 0.230555                                        LR 0.000013    Time 0.029449    
2023-01-06 14:44:09,650 - Epoch: [134][  120/  246]    Overall Loss 0.228489    Objective Loss 0.228489                                        LR 0.000013    Time 0.028962    
2023-01-06 14:44:09,903 - Epoch: [134][  130/  246]    Overall Loss 0.227676    Objective Loss 0.227676                                        LR 0.000013    Time 0.028677    
2023-01-06 14:44:10,127 - Epoch: [134][  140/  246]    Overall Loss 0.227995    Objective Loss 0.227995                                        LR 0.000013    Time 0.028225    
2023-01-06 14:44:10,347 - Epoch: [134][  150/  246]    Overall Loss 0.228885    Objective Loss 0.228885                                        LR 0.000013    Time 0.027802    
2023-01-06 14:44:10,588 - Epoch: [134][  160/  246]    Overall Loss 0.228960    Objective Loss 0.228960                                        LR 0.000013    Time 0.027574    
2023-01-06 14:44:10,851 - Epoch: [134][  170/  246]    Overall Loss 0.229072    Objective Loss 0.229072                                        LR 0.000013    Time 0.027491    
2023-01-06 14:44:11,099 - Epoch: [134][  180/  246]    Overall Loss 0.228746    Objective Loss 0.228746                                        LR 0.000013    Time 0.027339    
2023-01-06 14:44:11,365 - Epoch: [134][  190/  246]    Overall Loss 0.228935    Objective Loss 0.228935                                        LR 0.000013    Time 0.027296    
2023-01-06 14:44:11,625 - Epoch: [134][  200/  246]    Overall Loss 0.229326    Objective Loss 0.229326                                        LR 0.000013    Time 0.027227    
2023-01-06 14:44:11,848 - Epoch: [134][  210/  246]    Overall Loss 0.228802    Objective Loss 0.228802                                        LR 0.000013    Time 0.026993    
2023-01-06 14:44:12,059 - Epoch: [134][  220/  246]    Overall Loss 0.227011    Objective Loss 0.227011                                        LR 0.000013    Time 0.026723    
2023-01-06 14:44:12,272 - Epoch: [134][  230/  246]    Overall Loss 0.226567    Objective Loss 0.226567                                        LR 0.000013    Time 0.026485    
2023-01-06 14:44:12,501 - Epoch: [134][  240/  246]    Overall Loss 0.227283    Objective Loss 0.227283                                        LR 0.000013    Time 0.026332    
2023-01-06 14:44:12,621 - Epoch: [134][  246/  246]    Overall Loss 0.227324    Objective Loss 0.227324    Top1 90.430622    LR 0.000013    Time 0.026180    
2023-01-06 14:44:12,791 - --- validate (epoch=134)-----------
2023-01-06 14:44:12,791 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:13,252 - Epoch: [134][   10/   28]    Loss 0.243952    Top1 91.093750    
2023-01-06 14:44:13,409 - Epoch: [134][   20/   28]    Loss 0.253061    Top1 90.800781    
2023-01-06 14:44:13,500 - Epoch: [134][   28/   28]    Loss 0.254800    Top1 90.667048    
2023-01-06 14:44:13,638 - ==> Top1: 90.667    Loss: 0.255

2023-01-06 14:44:13,638 - ==> Confusion:
[[ 267   11  161]
 [  18  271  313]
 [  87   62 5796]]

2023-01-06 14:44:13,640 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:13,640 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:13,650 - 

2023-01-06 14:44:13,650 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:14,214 - Epoch: [135][   10/  246]    Overall Loss 0.230432    Objective Loss 0.230432                                        LR 0.000013    Time 0.056320    
2023-01-06 14:44:14,428 - Epoch: [135][   20/  246]    Overall Loss 0.226055    Objective Loss 0.226055                                        LR 0.000013    Time 0.038811    
2023-01-06 14:44:14,642 - Epoch: [135][   30/  246]    Overall Loss 0.221821    Objective Loss 0.221821                                        LR 0.000013    Time 0.033014    
2023-01-06 14:44:14,856 - Epoch: [135][   40/  246]    Overall Loss 0.220561    Objective Loss 0.220561                                        LR 0.000013    Time 0.030103    
2023-01-06 14:44:15,072 - Epoch: [135][   50/  246]    Overall Loss 0.221017    Objective Loss 0.221017                                        LR 0.000013    Time 0.028393    
2023-01-06 14:44:15,307 - Epoch: [135][   60/  246]    Overall Loss 0.223007    Objective Loss 0.223007                                        LR 0.000013    Time 0.027569    
2023-01-06 14:44:15,536 - Epoch: [135][   70/  246]    Overall Loss 0.221562    Objective Loss 0.221562                                        LR 0.000013    Time 0.026898    
2023-01-06 14:44:15,776 - Epoch: [135][   80/  246]    Overall Loss 0.225574    Objective Loss 0.225574                                        LR 0.000013    Time 0.026534    
2023-01-06 14:44:16,009 - Epoch: [135][   90/  246]    Overall Loss 0.222393    Objective Loss 0.222393                                        LR 0.000013    Time 0.026166    
2023-01-06 14:44:16,245 - Epoch: [135][  100/  246]    Overall Loss 0.223732    Objective Loss 0.223732                                        LR 0.000013    Time 0.025900    
2023-01-06 14:44:16,457 - Epoch: [135][  110/  246]    Overall Loss 0.224785    Objective Loss 0.224785                                        LR 0.000013    Time 0.025470    
2023-01-06 14:44:16,665 - Epoch: [135][  120/  246]    Overall Loss 0.225169    Objective Loss 0.225169                                        LR 0.000013    Time 0.025080    
2023-01-06 14:44:16,875 - Epoch: [135][  130/  246]    Overall Loss 0.225201    Objective Loss 0.225201                                        LR 0.000013    Time 0.024765    
2023-01-06 14:44:17,084 - Epoch: [135][  140/  246]    Overall Loss 0.223924    Objective Loss 0.223924                                        LR 0.000013    Time 0.024486    
2023-01-06 14:44:17,298 - Epoch: [135][  150/  246]    Overall Loss 0.224853    Objective Loss 0.224853                                        LR 0.000013    Time 0.024276    
2023-01-06 14:44:17,521 - Epoch: [135][  160/  246]    Overall Loss 0.226089    Objective Loss 0.226089                                        LR 0.000013    Time 0.024151    
2023-01-06 14:44:17,745 - Epoch: [135][  170/  246]    Overall Loss 0.226368    Objective Loss 0.226368                                        LR 0.000013    Time 0.024047    
2023-01-06 14:44:17,978 - Epoch: [135][  180/  246]    Overall Loss 0.226729    Objective Loss 0.226729                                        LR 0.000013    Time 0.024002    
2023-01-06 14:44:18,204 - Epoch: [135][  190/  246]    Overall Loss 0.225419    Objective Loss 0.225419                                        LR 0.000013    Time 0.023924    
2023-01-06 14:44:18,430 - Epoch: [135][  200/  246]    Overall Loss 0.225366    Objective Loss 0.225366                                        LR 0.000013    Time 0.023856    
2023-01-06 14:44:18,659 - Epoch: [135][  210/  246]    Overall Loss 0.225262    Objective Loss 0.225262                                        LR 0.000013    Time 0.023807    
2023-01-06 14:44:18,893 - Epoch: [135][  220/  246]    Overall Loss 0.225354    Objective Loss 0.225354                                        LR 0.000013    Time 0.023783    
2023-01-06 14:44:19,123 - Epoch: [135][  230/  246]    Overall Loss 0.226475    Objective Loss 0.226475                                        LR 0.000013    Time 0.023748    
2023-01-06 14:44:19,373 - Epoch: [135][  240/  246]    Overall Loss 0.227277    Objective Loss 0.227277                                        LR 0.000013    Time 0.023789    
2023-01-06 14:44:19,499 - Epoch: [135][  246/  246]    Overall Loss 0.227194    Objective Loss 0.227194    Top1 91.387560    LR 0.000013    Time 0.023721    
2023-01-06 14:44:19,623 - --- validate (epoch=135)-----------
2023-01-06 14:44:19,623 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:20,206 - Epoch: [135][   10/   28]    Loss 0.251836    Top1 90.898438    
2023-01-06 14:44:20,346 - Epoch: [135][   20/   28]    Loss 0.262200    Top1 90.546875    
2023-01-06 14:44:20,438 - Epoch: [135][   28/   28]    Loss 0.253247    Top1 90.824506    
2023-01-06 14:44:20,591 - ==> Top1: 90.825    Loss: 0.253

2023-01-06 14:44:20,592 - ==> Confusion:
[[ 242   15  182]
 [  10  282  310]
 [  58   66 5821]]

2023-01-06 14:44:20,593 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:20,593 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:20,603 - 

2023-01-06 14:44:20,603 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:21,187 - Epoch: [136][   10/  246]    Overall Loss 0.244342    Objective Loss 0.244342                                        LR 0.000013    Time 0.058263    
2023-01-06 14:44:21,419 - Epoch: [136][   20/  246]    Overall Loss 0.227482    Objective Loss 0.227482                                        LR 0.000013    Time 0.040722    
2023-01-06 14:44:21,657 - Epoch: [136][   30/  246]    Overall Loss 0.228738    Objective Loss 0.228738                                        LR 0.000013    Time 0.035075    
2023-01-06 14:44:21,896 - Epoch: [136][   40/  246]    Overall Loss 0.228326    Objective Loss 0.228326                                        LR 0.000013    Time 0.032264    
2023-01-06 14:44:22,125 - Epoch: [136][   50/  246]    Overall Loss 0.225038    Objective Loss 0.225038                                        LR 0.000013    Time 0.030385    
2023-01-06 14:44:22,356 - Epoch: [136][   60/  246]    Overall Loss 0.224475    Objective Loss 0.224475                                        LR 0.000013    Time 0.029155    
2023-01-06 14:44:22,587 - Epoch: [136][   70/  246]    Overall Loss 0.224900    Objective Loss 0.224900                                        LR 0.000013    Time 0.028289    
2023-01-06 14:44:22,824 - Epoch: [136][   80/  246]    Overall Loss 0.223652    Objective Loss 0.223652                                        LR 0.000013    Time 0.027713    
2023-01-06 14:44:23,064 - Epoch: [136][   90/  246]    Overall Loss 0.225592    Objective Loss 0.225592                                        LR 0.000013    Time 0.027289    
2023-01-06 14:44:23,301 - Epoch: [136][  100/  246]    Overall Loss 0.225202    Objective Loss 0.225202                                        LR 0.000013    Time 0.026931    
2023-01-06 14:44:23,547 - Epoch: [136][  110/  246]    Overall Loss 0.224798    Objective Loss 0.224798                                        LR 0.000013    Time 0.026700    
2023-01-06 14:44:23,789 - Epoch: [136][  120/  246]    Overall Loss 0.225018    Objective Loss 0.225018                                        LR 0.000013    Time 0.026486    
2023-01-06 14:44:24,021 - Epoch: [136][  130/  246]    Overall Loss 0.226124    Objective Loss 0.226124                                        LR 0.000013    Time 0.026229    
2023-01-06 14:44:24,261 - Epoch: [136][  140/  246]    Overall Loss 0.226555    Objective Loss 0.226555                                        LR 0.000013    Time 0.026067    
2023-01-06 14:44:24,495 - Epoch: [136][  150/  246]    Overall Loss 0.225235    Objective Loss 0.225235                                        LR 0.000013    Time 0.025888    
2023-01-06 14:44:24,729 - Epoch: [136][  160/  246]    Overall Loss 0.226575    Objective Loss 0.226575                                        LR 0.000013    Time 0.025730    
2023-01-06 14:44:24,960 - Epoch: [136][  170/  246]    Overall Loss 0.228055    Objective Loss 0.228055                                        LR 0.000013    Time 0.025570    
2023-01-06 14:44:25,195 - Epoch: [136][  180/  246]    Overall Loss 0.228613    Objective Loss 0.228613                                        LR 0.000013    Time 0.025452    
2023-01-06 14:44:25,428 - Epoch: [136][  190/  246]    Overall Loss 0.229616    Objective Loss 0.229616                                        LR 0.000013    Time 0.025340    
2023-01-06 14:44:25,670 - Epoch: [136][  200/  246]    Overall Loss 0.228937    Objective Loss 0.228937                                        LR 0.000013    Time 0.025280    
2023-01-06 14:44:25,904 - Epoch: [136][  210/  246]    Overall Loss 0.228368    Objective Loss 0.228368                                        LR 0.000013    Time 0.025186    
2023-01-06 14:44:26,129 - Epoch: [136][  220/  246]    Overall Loss 0.227786    Objective Loss 0.227786                                        LR 0.000013    Time 0.025063    
2023-01-06 14:44:26,371 - Epoch: [136][  230/  246]    Overall Loss 0.227351    Objective Loss 0.227351                                        LR 0.000013    Time 0.025024    
2023-01-06 14:44:26,620 - Epoch: [136][  240/  246]    Overall Loss 0.227021    Objective Loss 0.227021                                        LR 0.000013    Time 0.025020    
2023-01-06 14:44:26,744 - Epoch: [136][  246/  246]    Overall Loss 0.227257    Objective Loss 0.227257    Top1 92.822967    LR 0.000013    Time 0.024912    
2023-01-06 14:44:26,911 - --- validate (epoch=136)-----------
2023-01-06 14:44:26,911 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:27,351 - Epoch: [136][   10/   28]    Loss 0.266213    Top1 90.429688    
2023-01-06 14:44:27,492 - Epoch: [136][   20/   28]    Loss 0.266298    Top1 90.371094    
2023-01-06 14:44:27,584 - Epoch: [136][   28/   28]    Loss 0.259653    Top1 90.810192    
2023-01-06 14:44:27,715 - ==> Top1: 90.810    Loss: 0.260

2023-01-06 14:44:27,715 - ==> Confusion:
[[ 262   16  161]
 [  14  293  295]
 [  75   81 5789]]

2023-01-06 14:44:27,717 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:27,717 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:27,727 - 

2023-01-06 14:44:27,727 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:28,472 - Epoch: [137][   10/  246]    Overall Loss 0.238338    Objective Loss 0.238338                                        LR 0.000013    Time 0.074430    
2023-01-06 14:44:28,724 - Epoch: [137][   20/  246]    Overall Loss 0.231962    Objective Loss 0.231962                                        LR 0.000013    Time 0.049806    
2023-01-06 14:44:28,983 - Epoch: [137][   30/  246]    Overall Loss 0.231964    Objective Loss 0.231964                                        LR 0.000013    Time 0.041835    
2023-01-06 14:44:29,208 - Epoch: [137][   40/  246]    Overall Loss 0.231597    Objective Loss 0.231597                                        LR 0.000013    Time 0.036977    
2023-01-06 14:44:29,420 - Epoch: [137][   50/  246]    Overall Loss 0.230035    Objective Loss 0.230035                                        LR 0.000013    Time 0.033818    
2023-01-06 14:44:29,634 - Epoch: [137][   60/  246]    Overall Loss 0.229127    Objective Loss 0.229127                                        LR 0.000013    Time 0.031742    
2023-01-06 14:44:29,847 - Epoch: [137][   70/  246]    Overall Loss 0.226627    Objective Loss 0.226627                                        LR 0.000013    Time 0.030220    
2023-01-06 14:44:30,068 - Epoch: [137][   80/  246]    Overall Loss 0.229561    Objective Loss 0.229561                                        LR 0.000013    Time 0.029202    
2023-01-06 14:44:30,282 - Epoch: [137][   90/  246]    Overall Loss 0.227550    Objective Loss 0.227550                                        LR 0.000013    Time 0.028308    
2023-01-06 14:44:30,497 - Epoch: [137][  100/  246]    Overall Loss 0.228037    Objective Loss 0.228037                                        LR 0.000013    Time 0.027628    
2023-01-06 14:44:30,709 - Epoch: [137][  110/  246]    Overall Loss 0.227703    Objective Loss 0.227703                                        LR 0.000013    Time 0.027043    
2023-01-06 14:44:30,923 - Epoch: [137][  120/  246]    Overall Loss 0.228548    Objective Loss 0.228548                                        LR 0.000013    Time 0.026564    
2023-01-06 14:44:31,135 - Epoch: [137][  130/  246]    Overall Loss 0.228011    Objective Loss 0.228011                                        LR 0.000013    Time 0.026152    
2023-01-06 14:44:31,355 - Epoch: [137][  140/  246]    Overall Loss 0.228442    Objective Loss 0.228442                                        LR 0.000013    Time 0.025852    
2023-01-06 14:44:31,570 - Epoch: [137][  150/  246]    Overall Loss 0.229015    Objective Loss 0.229015                                        LR 0.000013    Time 0.025556    
2023-01-06 14:44:31,792 - Epoch: [137][  160/  246]    Overall Loss 0.228585    Objective Loss 0.228585                                        LR 0.000013    Time 0.025345    
2023-01-06 14:44:32,038 - Epoch: [137][  170/  246]    Overall Loss 0.227830    Objective Loss 0.227830                                        LR 0.000013    Time 0.025302    
2023-01-06 14:44:32,276 - Epoch: [137][  180/  246]    Overall Loss 0.227314    Objective Loss 0.227314                                        LR 0.000013    Time 0.025217    
2023-01-06 14:44:32,506 - Epoch: [137][  190/  246]    Overall Loss 0.226785    Objective Loss 0.226785                                        LR 0.000013    Time 0.025095    
2023-01-06 14:44:32,726 - Epoch: [137][  200/  246]    Overall Loss 0.226818    Objective Loss 0.226818                                        LR 0.000013    Time 0.024939    
2023-01-06 14:44:32,945 - Epoch: [137][  210/  246]    Overall Loss 0.227246    Objective Loss 0.227246                                        LR 0.000013    Time 0.024792    
2023-01-06 14:44:33,187 - Epoch: [137][  220/  246]    Overall Loss 0.227198    Objective Loss 0.227198                                        LR 0.000013    Time 0.024765    
2023-01-06 14:44:33,414 - Epoch: [137][  230/  246]    Overall Loss 0.227971    Objective Loss 0.227971                                        LR 0.000013    Time 0.024670    
2023-01-06 14:44:33,661 - Epoch: [137][  240/  246]    Overall Loss 0.228265    Objective Loss 0.228265                                        LR 0.000013    Time 0.024671    
2023-01-06 14:44:33,788 - Epoch: [137][  246/  246]    Overall Loss 0.227535    Objective Loss 0.227535    Top1 93.540670    LR 0.000013    Time 0.024586    
2023-01-06 14:44:33,939 - --- validate (epoch=137)-----------
2023-01-06 14:44:33,940 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:34,389 - Epoch: [137][   10/   28]    Loss 0.260672    Top1 90.312500    
2023-01-06 14:44:34,528 - Epoch: [137][   20/   28]    Loss 0.267841    Top1 90.175781    
2023-01-06 14:44:34,620 - Epoch: [137][   28/   28]    Loss 0.259709    Top1 90.423705    
2023-01-06 14:44:34,757 - ==> Top1: 90.424    Loss: 0.260

2023-01-06 14:44:34,758 - ==> Confusion:
[[ 215   14  210]
 [  13  251  338]
 [  44   50 5851]]

2023-01-06 14:44:34,759 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:34,759 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:34,769 - 

2023-01-06 14:44:34,769 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:35,342 - Epoch: [138][   10/  246]    Overall Loss 0.231455    Objective Loss 0.231455                                        LR 0.000013    Time 0.057168    
2023-01-06 14:44:35,551 - Epoch: [138][   20/  246]    Overall Loss 0.237549    Objective Loss 0.237549                                        LR 0.000013    Time 0.039013    
2023-01-06 14:44:35,767 - Epoch: [138][   30/  246]    Overall Loss 0.237855    Objective Loss 0.237855                                        LR 0.000013    Time 0.033213    
2023-01-06 14:44:35,990 - Epoch: [138][   40/  246]    Overall Loss 0.230774    Objective Loss 0.230774                                        LR 0.000013    Time 0.030470    
2023-01-06 14:44:36,212 - Epoch: [138][   50/  246]    Overall Loss 0.232867    Objective Loss 0.232867                                        LR 0.000013    Time 0.028812    
2023-01-06 14:44:36,429 - Epoch: [138][   60/  246]    Overall Loss 0.231730    Objective Loss 0.231730                                        LR 0.000013    Time 0.027617    
2023-01-06 14:44:36,655 - Epoch: [138][   70/  246]    Overall Loss 0.230353    Objective Loss 0.230353                                        LR 0.000013    Time 0.026897    
2023-01-06 14:44:36,865 - Epoch: [138][   80/  246]    Overall Loss 0.230741    Objective Loss 0.230741                                        LR 0.000013    Time 0.026160    
2023-01-06 14:44:37,089 - Epoch: [138][   90/  246]    Overall Loss 0.230165    Objective Loss 0.230165                                        LR 0.000013    Time 0.025737    
2023-01-06 14:44:37,320 - Epoch: [138][  100/  246]    Overall Loss 0.231160    Objective Loss 0.231160                                        LR 0.000013    Time 0.025464    
2023-01-06 14:44:37,547 - Epoch: [138][  110/  246]    Overall Loss 0.230277    Objective Loss 0.230277                                        LR 0.000013    Time 0.025212    
2023-01-06 14:44:37,778 - Epoch: [138][  120/  246]    Overall Loss 0.230009    Objective Loss 0.230009                                        LR 0.000013    Time 0.025037    
2023-01-06 14:44:38,022 - Epoch: [138][  130/  246]    Overall Loss 0.228776    Objective Loss 0.228776                                        LR 0.000013    Time 0.024979    
2023-01-06 14:44:38,262 - Epoch: [138][  140/  246]    Overall Loss 0.228359    Objective Loss 0.228359                                        LR 0.000013    Time 0.024904    
2023-01-06 14:44:38,518 - Epoch: [138][  150/  246]    Overall Loss 0.227407    Objective Loss 0.227407                                        LR 0.000013    Time 0.024951    
2023-01-06 14:44:38,771 - Epoch: [138][  160/  246]    Overall Loss 0.228175    Objective Loss 0.228175                                        LR 0.000013    Time 0.024968    
2023-01-06 14:44:39,027 - Epoch: [138][  170/  246]    Overall Loss 0.227889    Objective Loss 0.227889                                        LR 0.000013    Time 0.025006    
2023-01-06 14:44:39,274 - Epoch: [138][  180/  246]    Overall Loss 0.227347    Objective Loss 0.227347                                        LR 0.000013    Time 0.024986    
2023-01-06 14:44:39,538 - Epoch: [138][  190/  246]    Overall Loss 0.226953    Objective Loss 0.226953                                        LR 0.000013    Time 0.025060    
2023-01-06 14:44:39,804 - Epoch: [138][  200/  246]    Overall Loss 0.226179    Objective Loss 0.226179                                        LR 0.000013    Time 0.025135    
2023-01-06 14:44:40,070 - Epoch: [138][  210/  246]    Overall Loss 0.226825    Objective Loss 0.226825                                        LR 0.000013    Time 0.025202    
2023-01-06 14:44:40,337 - Epoch: [138][  220/  246]    Overall Loss 0.226683    Objective Loss 0.226683                                        LR 0.000013    Time 0.025266    
2023-01-06 14:44:40,601 - Epoch: [138][  230/  246]    Overall Loss 0.226949    Objective Loss 0.226949                                        LR 0.000013    Time 0.025315    
2023-01-06 14:44:40,874 - Epoch: [138][  240/  246]    Overall Loss 0.225983    Objective Loss 0.225983                                        LR 0.000013    Time 0.025397    
2023-01-06 14:44:41,002 - Epoch: [138][  246/  246]    Overall Loss 0.225861    Objective Loss 0.225861    Top1 92.583732    LR 0.000013    Time 0.025298    
2023-01-06 14:44:41,141 - --- validate (epoch=138)-----------
2023-01-06 14:44:41,141 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:41,586 - Epoch: [138][   10/   28]    Loss 0.247562    Top1 90.703125    
2023-01-06 14:44:41,728 - Epoch: [138][   20/   28]    Loss 0.260558    Top1 90.585938    
2023-01-06 14:44:41,820 - Epoch: [138][   28/   28]    Loss 0.252106    Top1 90.781563    
2023-01-06 14:44:41,953 - ==> Top1: 90.782    Loss: 0.252

2023-01-06 14:44:41,954 - ==> Confusion:
[[ 272   14  153]
 [  14  310  278]
 [  83  102 5760]]

2023-01-06 14:44:41,955 - ==> Best [Top1: 90.953   Sparsity:0.00   Params: 360896 on epoch: 133]
2023-01-06 14:44:41,955 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:41,965 - 

2023-01-06 14:44:41,965 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:42,689 - Epoch: [139][   10/  246]    Overall Loss 0.222266    Objective Loss 0.222266                                        LR 0.000013    Time 0.072266    
2023-01-06 14:44:42,932 - Epoch: [139][   20/  246]    Overall Loss 0.220822    Objective Loss 0.220822                                        LR 0.000013    Time 0.048288    
2023-01-06 14:44:43,176 - Epoch: [139][   30/  246]    Overall Loss 0.226161    Objective Loss 0.226161                                        LR 0.000013    Time 0.040290    
2023-01-06 14:44:43,409 - Epoch: [139][   40/  246]    Overall Loss 0.225423    Objective Loss 0.225423                                        LR 0.000013    Time 0.036055    
2023-01-06 14:44:43,640 - Epoch: [139][   50/  246]    Overall Loss 0.230452    Objective Loss 0.230452                                        LR 0.000013    Time 0.033449    
2023-01-06 14:44:43,884 - Epoch: [139][   60/  246]    Overall Loss 0.226481    Objective Loss 0.226481                                        LR 0.000013    Time 0.031935    
2023-01-06 14:44:44,127 - Epoch: [139][   70/  246]    Overall Loss 0.226172    Objective Loss 0.226172                                        LR 0.000013    Time 0.030837    
2023-01-06 14:44:44,368 - Epoch: [139][   80/  246]    Overall Loss 0.227639    Objective Loss 0.227639                                        LR 0.000013    Time 0.029989    
2023-01-06 14:44:44,606 - Epoch: [139][   90/  246]    Overall Loss 0.226232    Objective Loss 0.226232                                        LR 0.000013    Time 0.029298    
2023-01-06 14:44:44,851 - Epoch: [139][  100/  246]    Overall Loss 0.226486    Objective Loss 0.226486                                        LR 0.000013    Time 0.028815    
2023-01-06 14:44:45,093 - Epoch: [139][  110/  246]    Overall Loss 0.226779    Objective Loss 0.226779                                        LR 0.000013    Time 0.028395    
2023-01-06 14:44:45,335 - Epoch: [139][  120/  246]    Overall Loss 0.226056    Objective Loss 0.226056                                        LR 0.000013    Time 0.028036    
2023-01-06 14:44:45,576 - Epoch: [139][  130/  246]    Overall Loss 0.226243    Objective Loss 0.226243                                        LR 0.000013    Time 0.027733    
2023-01-06 14:44:45,821 - Epoch: [139][  140/  246]    Overall Loss 0.226122    Objective Loss 0.226122                                        LR 0.000013    Time 0.027497    
2023-01-06 14:44:46,063 - Epoch: [139][  150/  246]    Overall Loss 0.225470    Objective Loss 0.225470                                        LR 0.000013    Time 0.027279    
2023-01-06 14:44:46,310 - Epoch: [139][  160/  246]    Overall Loss 0.225427    Objective Loss 0.225427                                        LR 0.000013    Time 0.027110    
2023-01-06 14:44:46,556 - Epoch: [139][  170/  246]    Overall Loss 0.225666    Objective Loss 0.225666                                        LR 0.000013    Time 0.026959    
2023-01-06 14:44:46,800 - Epoch: [139][  180/  246]    Overall Loss 0.226523    Objective Loss 0.226523                                        LR 0.000013    Time 0.026817    
2023-01-06 14:44:47,044 - Epoch: [139][  190/  246]    Overall Loss 0.226547    Objective Loss 0.226547                                        LR 0.000013    Time 0.026685    
2023-01-06 14:44:47,288 - Epoch: [139][  200/  246]    Overall Loss 0.226651    Objective Loss 0.226651                                        LR 0.000013    Time 0.026569    
2023-01-06 14:44:47,529 - Epoch: [139][  210/  246]    Overall Loss 0.226860    Objective Loss 0.226860                                        LR 0.000013    Time 0.026450    
2023-01-06 14:44:47,771 - Epoch: [139][  220/  246]    Overall Loss 0.226708    Objective Loss 0.226708                                        LR 0.000013    Time 0.026348    
2023-01-06 14:44:48,010 - Epoch: [139][  230/  246]    Overall Loss 0.227271    Objective Loss 0.227271                                        LR 0.000013    Time 0.026238    
2023-01-06 14:44:48,242 - Epoch: [139][  240/  246]    Overall Loss 0.226692    Objective Loss 0.226692                                        LR 0.000013    Time 0.026107    
2023-01-06 14:44:48,357 - Epoch: [139][  246/  246]    Overall Loss 0.226992    Objective Loss 0.226992    Top1 90.669856    LR 0.000013    Time 0.025937    
2023-01-06 14:44:48,494 - --- validate (epoch=139)-----------
2023-01-06 14:44:48,494 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:48,960 - Epoch: [139][   10/   28]    Loss 0.248625    Top1 91.171875    
2023-01-06 14:44:49,119 - Epoch: [139][   20/   28]    Loss 0.263991    Top1 90.527344    
2023-01-06 14:44:49,208 - Epoch: [139][   28/   28]    Loss 0.257174    Top1 90.981964    
2023-01-06 14:44:49,392 - ==> Top1: 90.982    Loss: 0.257

2023-01-06 14:44:49,392 - ==> Confusion:
[[ 241   11  187]
 [  10  280  312]
 [  52   58 5835]]

2023-01-06 14:44:49,394 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:44:49,394 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:49,415 - 

2023-01-06 14:44:49,415 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:50,212 - Epoch: [140][   10/  246]    Overall Loss 0.227633    Objective Loss 0.227633                                        LR 0.000008    Time 0.079619    
2023-01-06 14:44:50,498 - Epoch: [140][   20/  246]    Overall Loss 0.222934    Objective Loss 0.222934                                        LR 0.000008    Time 0.053986    
2023-01-06 14:44:50,783 - Epoch: [140][   30/  246]    Overall Loss 0.218949    Objective Loss 0.218949                                        LR 0.000008    Time 0.045483    
2023-01-06 14:44:51,055 - Epoch: [140][   40/  246]    Overall Loss 0.224013    Objective Loss 0.224013                                        LR 0.000008    Time 0.040910    
2023-01-06 14:44:51,303 - Epoch: [140][   50/  246]    Overall Loss 0.222861    Objective Loss 0.222861                                        LR 0.000008    Time 0.037666    
2023-01-06 14:44:51,542 - Epoch: [140][   60/  246]    Overall Loss 0.222864    Objective Loss 0.222864                                        LR 0.000008    Time 0.035370    
2023-01-06 14:44:51,780 - Epoch: [140][   70/  246]    Overall Loss 0.222019    Objective Loss 0.222019                                        LR 0.000008    Time 0.033718    
2023-01-06 14:44:52,028 - Epoch: [140][   80/  246]    Overall Loss 0.221525    Objective Loss 0.221525                                        LR 0.000008    Time 0.032597    
2023-01-06 14:44:52,279 - Epoch: [140][   90/  246]    Overall Loss 0.222263    Objective Loss 0.222263                                        LR 0.000008    Time 0.031752    
2023-01-06 14:44:52,528 - Epoch: [140][  100/  246]    Overall Loss 0.223987    Objective Loss 0.223987                                        LR 0.000008    Time 0.031067    
2023-01-06 14:44:52,764 - Epoch: [140][  110/  246]    Overall Loss 0.224694    Objective Loss 0.224694                                        LR 0.000008    Time 0.030384    
2023-01-06 14:44:53,009 - Epoch: [140][  120/  246]    Overall Loss 0.225443    Objective Loss 0.225443                                        LR 0.000008    Time 0.029896    
2023-01-06 14:44:53,266 - Epoch: [140][  130/  246]    Overall Loss 0.226058    Objective Loss 0.226058                                        LR 0.000008    Time 0.029565    
2023-01-06 14:44:53,525 - Epoch: [140][  140/  246]    Overall Loss 0.225905    Objective Loss 0.225905                                        LR 0.000008    Time 0.029299    
2023-01-06 14:44:53,776 - Epoch: [140][  150/  246]    Overall Loss 0.226500    Objective Loss 0.226500                                        LR 0.000008    Time 0.029019    
2023-01-06 14:44:54,029 - Epoch: [140][  160/  246]    Overall Loss 0.226572    Objective Loss 0.226572                                        LR 0.000008    Time 0.028783    
2023-01-06 14:44:54,274 - Epoch: [140][  170/  246]    Overall Loss 0.225876    Objective Loss 0.225876                                        LR 0.000008    Time 0.028525    
2023-01-06 14:44:54,517 - Epoch: [140][  180/  246]    Overall Loss 0.225864    Objective Loss 0.225864                                        LR 0.000008    Time 0.028289    
2023-01-06 14:44:54,764 - Epoch: [140][  190/  246]    Overall Loss 0.224949    Objective Loss 0.224949                                        LR 0.000008    Time 0.028095    
2023-01-06 14:44:55,017 - Epoch: [140][  200/  246]    Overall Loss 0.224500    Objective Loss 0.224500                                        LR 0.000008    Time 0.027957    
2023-01-06 14:44:55,270 - Epoch: [140][  210/  246]    Overall Loss 0.224997    Objective Loss 0.224997                                        LR 0.000008    Time 0.027825    
2023-01-06 14:44:55,520 - Epoch: [140][  220/  246]    Overall Loss 0.224817    Objective Loss 0.224817                                        LR 0.000008    Time 0.027690    
2023-01-06 14:44:55,773 - Epoch: [140][  230/  246]    Overall Loss 0.225300    Objective Loss 0.225300                                        LR 0.000008    Time 0.027585    
2023-01-06 14:44:56,037 - Epoch: [140][  240/  246]    Overall Loss 0.225180    Objective Loss 0.225180                                        LR 0.000008    Time 0.027531    
2023-01-06 14:44:56,167 - Epoch: [140][  246/  246]    Overall Loss 0.225088    Objective Loss 0.225088    Top1 92.583732    LR 0.000008    Time 0.027388    
2023-01-06 14:44:56,311 - --- validate (epoch=140)-----------
2023-01-06 14:44:56,312 - 6986 samples (256 per mini-batch)
2023-01-06 14:44:56,759 - Epoch: [140][   10/   28]    Loss 0.267421    Top1 90.546875    
2023-01-06 14:44:56,899 - Epoch: [140][   20/   28]    Loss 0.265468    Top1 90.546875    
2023-01-06 14:44:56,991 - Epoch: [140][   28/   28]    Loss 0.255282    Top1 90.824506    
2023-01-06 14:44:57,147 - ==> Top1: 90.825    Loss: 0.255

2023-01-06 14:44:57,147 - ==> Confusion:
[[ 247   11  181]
 [  17  287  298]
 [  65   69 5811]]

2023-01-06 14:44:57,149 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:44:57,149 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:44:57,159 - 

2023-01-06 14:44:57,159 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:44:57,783 - Epoch: [141][   10/  246]    Overall Loss 0.230598    Objective Loss 0.230598                                        LR 0.000008    Time 0.062343    
2023-01-06 14:44:58,052 - Epoch: [141][   20/  246]    Overall Loss 0.229201    Objective Loss 0.229201                                        LR 0.000008    Time 0.044583    
2023-01-06 14:44:58,324 - Epoch: [141][   30/  246]    Overall Loss 0.239685    Objective Loss 0.239685                                        LR 0.000008    Time 0.038774    
2023-01-06 14:44:58,596 - Epoch: [141][   40/  246]    Overall Loss 0.239607    Objective Loss 0.239607                                        LR 0.000008    Time 0.035880    
2023-01-06 14:44:58,868 - Epoch: [141][   50/  246]    Overall Loss 0.234622    Objective Loss 0.234622                                        LR 0.000008    Time 0.034136    
2023-01-06 14:44:59,139 - Epoch: [141][   60/  246]    Overall Loss 0.233688    Objective Loss 0.233688                                        LR 0.000008    Time 0.032949    
2023-01-06 14:44:59,407 - Epoch: [141][   70/  246]    Overall Loss 0.233519    Objective Loss 0.233519                                        LR 0.000008    Time 0.032073    
2023-01-06 14:44:59,676 - Epoch: [141][   80/  246]    Overall Loss 0.233547    Objective Loss 0.233547                                        LR 0.000008    Time 0.031417    
2023-01-06 14:44:59,941 - Epoch: [141][   90/  246]    Overall Loss 0.233161    Objective Loss 0.233161                                        LR 0.000008    Time 0.030861    
2023-01-06 14:45:00,198 - Epoch: [141][  100/  246]    Overall Loss 0.232727    Objective Loss 0.232727                                        LR 0.000008    Time 0.030344    
2023-01-06 14:45:00,447 - Epoch: [141][  110/  246]    Overall Loss 0.232597    Objective Loss 0.232597                                        LR 0.000008    Time 0.029851    
2023-01-06 14:45:00,699 - Epoch: [141][  120/  246]    Overall Loss 0.230719    Objective Loss 0.230719                                        LR 0.000008    Time 0.029460    
2023-01-06 14:45:00,948 - Epoch: [141][  130/  246]    Overall Loss 0.227844    Objective Loss 0.227844                                        LR 0.000008    Time 0.029107    
2023-01-06 14:45:01,200 - Epoch: [141][  140/  246]    Overall Loss 0.227299    Objective Loss 0.227299                                        LR 0.000008    Time 0.028826    
2023-01-06 14:45:01,450 - Epoch: [141][  150/  246]    Overall Loss 0.227471    Objective Loss 0.227471                                        LR 0.000008    Time 0.028569    
2023-01-06 14:45:01,701 - Epoch: [141][  160/  246]    Overall Loss 0.226439    Objective Loss 0.226439                                        LR 0.000008    Time 0.028348    
2023-01-06 14:45:01,950 - Epoch: [141][  170/  246]    Overall Loss 0.226715    Objective Loss 0.226715                                        LR 0.000008    Time 0.028140    
2023-01-06 14:45:02,178 - Epoch: [141][  180/  246]    Overall Loss 0.226940    Objective Loss 0.226940                                        LR 0.000008    Time 0.027843    
2023-01-06 14:45:02,412 - Epoch: [141][  190/  246]    Overall Loss 0.225680    Objective Loss 0.225680                                        LR 0.000008    Time 0.027607    
2023-01-06 14:45:02,652 - Epoch: [141][  200/  246]    Overall Loss 0.225841    Objective Loss 0.225841                                        LR 0.000008    Time 0.027424    
2023-01-06 14:45:02,881 - Epoch: [141][  210/  246]    Overall Loss 0.225430    Objective Loss 0.225430                                        LR 0.000008    Time 0.027210    
2023-01-06 14:45:03,121 - Epoch: [141][  220/  246]    Overall Loss 0.225486    Objective Loss 0.225486                                        LR 0.000008    Time 0.027059    
2023-01-06 14:45:03,361 - Epoch: [141][  230/  246]    Overall Loss 0.225450    Objective Loss 0.225450                                        LR 0.000008    Time 0.026926    
2023-01-06 14:45:03,612 - Epoch: [141][  240/  246]    Overall Loss 0.224901    Objective Loss 0.224901                                        LR 0.000008    Time 0.026849    
2023-01-06 14:45:03,734 - Epoch: [141][  246/  246]    Overall Loss 0.224936    Objective Loss 0.224936    Top1 94.258373    LR 0.000008    Time 0.026688    
2023-01-06 14:45:03,880 - --- validate (epoch=141)-----------
2023-01-06 14:45:03,881 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:04,323 - Epoch: [141][   10/   28]    Loss 0.264682    Top1 90.546875    
2023-01-06 14:45:04,469 - Epoch: [141][   20/   28]    Loss 0.252278    Top1 91.093750    
2023-01-06 14:45:04,561 - Epoch: [141][   28/   28]    Loss 0.247338    Top1 90.953335    
2023-01-06 14:45:04,697 - ==> Top1: 90.953    Loss: 0.247

2023-01-06 14:45:04,698 - ==> Confusion:
[[ 254   14  171]
 [  11  306  285]
 [  64   87 5794]]

2023-01-06 14:45:04,699 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:04,699 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:04,709 - 

2023-01-06 14:45:04,709 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:05,449 - Epoch: [142][   10/  246]    Overall Loss 0.233975    Objective Loss 0.233975                                        LR 0.000008    Time 0.073835    
2023-01-06 14:45:05,695 - Epoch: [142][   20/  246]    Overall Loss 0.231990    Objective Loss 0.231990                                        LR 0.000008    Time 0.049222    
2023-01-06 14:45:05,948 - Epoch: [142][   30/  246]    Overall Loss 0.223618    Objective Loss 0.223618                                        LR 0.000008    Time 0.041218    
2023-01-06 14:45:06,199 - Epoch: [142][   40/  246]    Overall Loss 0.224635    Objective Loss 0.224635                                        LR 0.000008    Time 0.037190    
2023-01-06 14:45:06,449 - Epoch: [142][   50/  246]    Overall Loss 0.224551    Objective Loss 0.224551                                        LR 0.000008    Time 0.034707    
2023-01-06 14:45:06,695 - Epoch: [142][   60/  246]    Overall Loss 0.222478    Objective Loss 0.222478                                        LR 0.000008    Time 0.033016    
2023-01-06 14:45:06,944 - Epoch: [142][   70/  246]    Overall Loss 0.221692    Objective Loss 0.221692                                        LR 0.000008    Time 0.031828    
2023-01-06 14:45:07,190 - Epoch: [142][   80/  246]    Overall Loss 0.220828    Objective Loss 0.220828                                        LR 0.000008    Time 0.030917    
2023-01-06 14:45:07,439 - Epoch: [142][   90/  246]    Overall Loss 0.221451    Objective Loss 0.221451                                        LR 0.000008    Time 0.030242    
2023-01-06 14:45:07,698 - Epoch: [142][  100/  246]    Overall Loss 0.221509    Objective Loss 0.221509                                        LR 0.000008    Time 0.029793    
2023-01-06 14:45:07,955 - Epoch: [142][  110/  246]    Overall Loss 0.222942    Objective Loss 0.222942                                        LR 0.000008    Time 0.029404    
2023-01-06 14:45:08,202 - Epoch: [142][  120/  246]    Overall Loss 0.223512    Objective Loss 0.223512                                        LR 0.000008    Time 0.029011    
2023-01-06 14:45:08,454 - Epoch: [142][  130/  246]    Overall Loss 0.223206    Objective Loss 0.223206                                        LR 0.000008    Time 0.028705    
2023-01-06 14:45:08,695 - Epoch: [142][  140/  246]    Overall Loss 0.222502    Objective Loss 0.222502                                        LR 0.000008    Time 0.028369    
2023-01-06 14:45:08,912 - Epoch: [142][  150/  246]    Overall Loss 0.222543    Objective Loss 0.222543                                        LR 0.000008    Time 0.027924    
2023-01-06 14:45:09,128 - Epoch: [142][  160/  246]    Overall Loss 0.223602    Objective Loss 0.223602                                        LR 0.000008    Time 0.027528    
2023-01-06 14:45:09,355 - Epoch: [142][  170/  246]    Overall Loss 0.223570    Objective Loss 0.223570                                        LR 0.000008    Time 0.027238    
2023-01-06 14:45:09,571 - Epoch: [142][  180/  246]    Overall Loss 0.224013    Objective Loss 0.224013                                        LR 0.000008    Time 0.026928    
2023-01-06 14:45:09,818 - Epoch: [142][  190/  246]    Overall Loss 0.223303    Objective Loss 0.223303                                        LR 0.000008    Time 0.026807    
2023-01-06 14:45:10,075 - Epoch: [142][  200/  246]    Overall Loss 0.224104    Objective Loss 0.224104                                        LR 0.000008    Time 0.026747    
2023-01-06 14:45:10,336 - Epoch: [142][  210/  246]    Overall Loss 0.224663    Objective Loss 0.224663                                        LR 0.000008    Time 0.026704    
2023-01-06 14:45:10,590 - Epoch: [142][  220/  246]    Overall Loss 0.224432    Objective Loss 0.224432                                        LR 0.000008    Time 0.026644    
2023-01-06 14:45:10,850 - Epoch: [142][  230/  246]    Overall Loss 0.224212    Objective Loss 0.224212                                        LR 0.000008    Time 0.026611    
2023-01-06 14:45:11,115 - Epoch: [142][  240/  246]    Overall Loss 0.224379    Objective Loss 0.224379                                        LR 0.000008    Time 0.026605    
2023-01-06 14:45:11,246 - Epoch: [142][  246/  246]    Overall Loss 0.224025    Objective Loss 0.224025    Top1 93.062201    LR 0.000008    Time 0.026486    
2023-01-06 14:45:11,364 - --- validate (epoch=142)-----------
2023-01-06 14:45:11,365 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:11,826 - Epoch: [142][   10/   28]    Loss 0.252728    Top1 90.859375    
2023-01-06 14:45:11,969 - Epoch: [142][   20/   28]    Loss 0.256741    Top1 90.761719    
2023-01-06 14:45:12,063 - Epoch: [142][   28/   28]    Loss 0.251454    Top1 90.939021    
2023-01-06 14:45:12,211 - ==> Top1: 90.939    Loss: 0.251

2023-01-06 14:45:12,211 - ==> Confusion:
[[ 278   13  148]
 [  16  315  271]
 [  73  112 5760]]

2023-01-06 14:45:12,212 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:12,213 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:12,222 - 

2023-01-06 14:45:12,223 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:12,797 - Epoch: [143][   10/  246]    Overall Loss 0.218302    Objective Loss 0.218302                                        LR 0.000008    Time 0.057349    
2023-01-06 14:45:13,032 - Epoch: [143][   20/  246]    Overall Loss 0.225870    Objective Loss 0.225870                                        LR 0.000008    Time 0.040392    
2023-01-06 14:45:13,268 - Epoch: [143][   30/  246]    Overall Loss 0.226774    Objective Loss 0.226774                                        LR 0.000008    Time 0.034795    
2023-01-06 14:45:13,508 - Epoch: [143][   40/  246]    Overall Loss 0.223064    Objective Loss 0.223064                                        LR 0.000008    Time 0.032097    
2023-01-06 14:45:13,748 - Epoch: [143][   50/  246]    Overall Loss 0.223142    Objective Loss 0.223142                                        LR 0.000008    Time 0.030469    
2023-01-06 14:45:13,993 - Epoch: [143][   60/  246]    Overall Loss 0.223890    Objective Loss 0.223890                                        LR 0.000008    Time 0.029467    
2023-01-06 14:45:14,238 - Epoch: [143][   70/  246]    Overall Loss 0.225371    Objective Loss 0.225371                                        LR 0.000008    Time 0.028746    
2023-01-06 14:45:14,479 - Epoch: [143][   80/  246]    Overall Loss 0.225242    Objective Loss 0.225242                                        LR 0.000008    Time 0.028170    
2023-01-06 14:45:14,700 - Epoch: [143][   90/  246]    Overall Loss 0.223523    Objective Loss 0.223523                                        LR 0.000008    Time 0.027484    
2023-01-06 14:45:14,925 - Epoch: [143][  100/  246]    Overall Loss 0.221876    Objective Loss 0.221876                                        LR 0.000008    Time 0.026982    
2023-01-06 14:45:15,144 - Epoch: [143][  110/  246]    Overall Loss 0.222674    Objective Loss 0.222674                                        LR 0.000008    Time 0.026520    
2023-01-06 14:45:15,368 - Epoch: [143][  120/  246]    Overall Loss 0.223591    Objective Loss 0.223591                                        LR 0.000008    Time 0.026174    
2023-01-06 14:45:15,587 - Epoch: [143][  130/  246]    Overall Loss 0.223920    Objective Loss 0.223920                                        LR 0.000008    Time 0.025847    
2023-01-06 14:45:15,811 - Epoch: [143][  140/  246]    Overall Loss 0.223457    Objective Loss 0.223457                                        LR 0.000008    Time 0.025598    
2023-01-06 14:45:16,031 - Epoch: [143][  150/  246]    Overall Loss 0.222974    Objective Loss 0.222974                                        LR 0.000008    Time 0.025356    
2023-01-06 14:45:16,268 - Epoch: [143][  160/  246]    Overall Loss 0.223786    Objective Loss 0.223786                                        LR 0.000008    Time 0.025251    
2023-01-06 14:45:16,513 - Epoch: [143][  170/  246]    Overall Loss 0.223223    Objective Loss 0.223223                                        LR 0.000008    Time 0.025206    
2023-01-06 14:45:16,757 - Epoch: [143][  180/  246]    Overall Loss 0.223588    Objective Loss 0.223588                                        LR 0.000008    Time 0.025157    
2023-01-06 14:45:17,000 - Epoch: [143][  190/  246]    Overall Loss 0.222698    Objective Loss 0.222698                                        LR 0.000008    Time 0.025109    
2023-01-06 14:45:17,245 - Epoch: [143][  200/  246]    Overall Loss 0.223600    Objective Loss 0.223600                                        LR 0.000008    Time 0.025073    
2023-01-06 14:45:17,488 - Epoch: [143][  210/  246]    Overall Loss 0.223913    Objective Loss 0.223913                                        LR 0.000008    Time 0.025031    
2023-01-06 14:45:17,726 - Epoch: [143][  220/  246]    Overall Loss 0.223704    Objective Loss 0.223704                                        LR 0.000008    Time 0.024973    
2023-01-06 14:45:17,968 - Epoch: [143][  230/  246]    Overall Loss 0.225242    Objective Loss 0.225242                                        LR 0.000008    Time 0.024942    
2023-01-06 14:45:18,226 - Epoch: [143][  240/  246]    Overall Loss 0.224978    Objective Loss 0.224978                                        LR 0.000008    Time 0.024974    
2023-01-06 14:45:18,351 - Epoch: [143][  246/  246]    Overall Loss 0.224943    Objective Loss 0.224943    Top1 89.473684    LR 0.000008    Time 0.024871    
2023-01-06 14:45:18,466 - --- validate (epoch=143)-----------
2023-01-06 14:45:18,466 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:18,908 - Epoch: [143][   10/   28]    Loss 0.250812    Top1 91.250000    
2023-01-06 14:45:19,051 - Epoch: [143][   20/   28]    Loss 0.259577    Top1 90.625000    
2023-01-06 14:45:19,143 - Epoch: [143][   28/   28]    Loss 0.256875    Top1 90.853135    
2023-01-06 14:45:19,264 - ==> Top1: 90.853    Loss: 0.257

2023-01-06 14:45:19,264 - ==> Confusion:
[[ 229   24  186]
 [   8  302  292]
 [  57   72 5816]]

2023-01-06 14:45:19,266 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:19,266 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:19,276 - 

2023-01-06 14:45:19,276 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:19,986 - Epoch: [144][   10/  246]    Overall Loss 0.223983    Objective Loss 0.223983                                        LR 0.000008    Time 0.070983    
2023-01-06 14:45:20,202 - Epoch: [144][   20/  246]    Overall Loss 0.231774    Objective Loss 0.231774                                        LR 0.000008    Time 0.046227    
2023-01-06 14:45:20,420 - Epoch: [144][   30/  246]    Overall Loss 0.231009    Objective Loss 0.231009                                        LR 0.000008    Time 0.038096    
2023-01-06 14:45:20,638 - Epoch: [144][   40/  246]    Overall Loss 0.228283    Objective Loss 0.228283                                        LR 0.000008    Time 0.034002    
2023-01-06 14:45:20,855 - Epoch: [144][   50/  246]    Overall Loss 0.228629    Objective Loss 0.228629                                        LR 0.000008    Time 0.031531    
2023-01-06 14:45:21,082 - Epoch: [144][   60/  246]    Overall Loss 0.231381    Objective Loss 0.231381                                        LR 0.000008    Time 0.030054    
2023-01-06 14:45:21,303 - Epoch: [144][   70/  246]    Overall Loss 0.229701    Objective Loss 0.229701                                        LR 0.000008    Time 0.028914    
2023-01-06 14:45:21,523 - Epoch: [144][   80/  246]    Overall Loss 0.229337    Objective Loss 0.229337                                        LR 0.000008    Time 0.028040    
2023-01-06 14:45:21,741 - Epoch: [144][   90/  246]    Overall Loss 0.229725    Objective Loss 0.229725                                        LR 0.000008    Time 0.027351    
2023-01-06 14:45:21,963 - Epoch: [144][  100/  246]    Overall Loss 0.228722    Objective Loss 0.228722                                        LR 0.000008    Time 0.026832    
2023-01-06 14:45:22,185 - Epoch: [144][  110/  246]    Overall Loss 0.230072    Objective Loss 0.230072                                        LR 0.000008    Time 0.026404    
2023-01-06 14:45:22,407 - Epoch: [144][  120/  246]    Overall Loss 0.229038    Objective Loss 0.229038                                        LR 0.000008    Time 0.026047    
2023-01-06 14:45:22,626 - Epoch: [144][  130/  246]    Overall Loss 0.228781    Objective Loss 0.228781                                        LR 0.000008    Time 0.025730    
2023-01-06 14:45:22,844 - Epoch: [144][  140/  246]    Overall Loss 0.227870    Objective Loss 0.227870                                        LR 0.000008    Time 0.025448    
2023-01-06 14:45:23,067 - Epoch: [144][  150/  246]    Overall Loss 0.227436    Objective Loss 0.227436                                        LR 0.000008    Time 0.025231    
2023-01-06 14:45:23,290 - Epoch: [144][  160/  246]    Overall Loss 0.226973    Objective Loss 0.226973                                        LR 0.000008    Time 0.025044    
2023-01-06 14:45:23,516 - Epoch: [144][  170/  246]    Overall Loss 0.226341    Objective Loss 0.226341                                        LR 0.000008    Time 0.024899    
2023-01-06 14:45:23,738 - Epoch: [144][  180/  246]    Overall Loss 0.226166    Objective Loss 0.226166                                        LR 0.000008    Time 0.024749    
2023-01-06 14:45:23,959 - Epoch: [144][  190/  246]    Overall Loss 0.226105    Objective Loss 0.226105                                        LR 0.000008    Time 0.024604    
2023-01-06 14:45:24,181 - Epoch: [144][  200/  246]    Overall Loss 0.226186    Objective Loss 0.226186                                        LR 0.000008    Time 0.024482    
2023-01-06 14:45:24,392 - Epoch: [144][  210/  246]    Overall Loss 0.226669    Objective Loss 0.226669                                        LR 0.000008    Time 0.024319    
2023-01-06 14:45:24,602 - Epoch: [144][  220/  246]    Overall Loss 0.226795    Objective Loss 0.226795                                        LR 0.000008    Time 0.024168    
2023-01-06 14:45:24,827 - Epoch: [144][  230/  246]    Overall Loss 0.226437    Objective Loss 0.226437                                        LR 0.000008    Time 0.024094    
2023-01-06 14:45:25,086 - Epoch: [144][  240/  246]    Overall Loss 0.226035    Objective Loss 0.226035                                        LR 0.000008    Time 0.024169    
2023-01-06 14:45:25,219 - Epoch: [144][  246/  246]    Overall Loss 0.225602    Objective Loss 0.225602    Top1 92.105263    LR 0.000008    Time 0.024117    
2023-01-06 14:45:25,336 - --- validate (epoch=144)-----------
2023-01-06 14:45:25,337 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:25,789 - Epoch: [144][   10/   28]    Loss 0.238956    Top1 91.328125    
2023-01-06 14:45:25,944 - Epoch: [144][   20/   28]    Loss 0.248179    Top1 90.917969    
2023-01-06 14:45:26,035 - Epoch: [144][   28/   28]    Loss 0.256284    Top1 90.667048    
2023-01-06 14:45:26,150 - ==> Top1: 90.667    Loss: 0.256

2023-01-06 14:45:26,150 - ==> Confusion:
[[ 258   10  171]
 [  15  281  306]
 [  71   79 5795]]

2023-01-06 14:45:26,152 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:26,152 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:26,162 - 

2023-01-06 14:45:26,162 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:26,922 - Epoch: [145][   10/  246]    Overall Loss 0.236336    Objective Loss 0.236336                                        LR 0.000008    Time 0.075901    
2023-01-06 14:45:27,173 - Epoch: [145][   20/  246]    Overall Loss 0.224117    Objective Loss 0.224117                                        LR 0.000008    Time 0.050478    
2023-01-06 14:45:27,427 - Epoch: [145][   30/  246]    Overall Loss 0.224038    Objective Loss 0.224038                                        LR 0.000008    Time 0.042127    
2023-01-06 14:45:27,667 - Epoch: [145][   40/  246]    Overall Loss 0.225107    Objective Loss 0.225107                                        LR 0.000008    Time 0.037567    
2023-01-06 14:45:27,885 - Epoch: [145][   50/  246]    Overall Loss 0.224234    Objective Loss 0.224234                                        LR 0.000008    Time 0.034416    
2023-01-06 14:45:28,101 - Epoch: [145][   60/  246]    Overall Loss 0.224233    Objective Loss 0.224233                                        LR 0.000008    Time 0.032275    
2023-01-06 14:45:28,350 - Epoch: [145][   70/  246]    Overall Loss 0.224841    Objective Loss 0.224841                                        LR 0.000008    Time 0.031206    
2023-01-06 14:45:28,601 - Epoch: [145][   80/  246]    Overall Loss 0.222984    Objective Loss 0.222984                                        LR 0.000008    Time 0.030437    
2023-01-06 14:45:28,850 - Epoch: [145][   90/  246]    Overall Loss 0.225044    Objective Loss 0.225044                                        LR 0.000008    Time 0.029823    
2023-01-06 14:45:29,104 - Epoch: [145][  100/  246]    Overall Loss 0.227021    Objective Loss 0.227021                                        LR 0.000008    Time 0.029356    
2023-01-06 14:45:29,356 - Epoch: [145][  110/  246]    Overall Loss 0.225903    Objective Loss 0.225903                                        LR 0.000008    Time 0.028971    
2023-01-06 14:45:29,607 - Epoch: [145][  120/  246]    Overall Loss 0.225718    Objective Loss 0.225718                                        LR 0.000008    Time 0.028652    
2023-01-06 14:45:29,859 - Epoch: [145][  130/  246]    Overall Loss 0.225942    Objective Loss 0.225942                                        LR 0.000008    Time 0.028377    
2023-01-06 14:45:30,111 - Epoch: [145][  140/  246]    Overall Loss 0.227281    Objective Loss 0.227281                                        LR 0.000008    Time 0.028151    
2023-01-06 14:45:30,363 - Epoch: [145][  150/  246]    Overall Loss 0.227852    Objective Loss 0.227852                                        LR 0.000008    Time 0.027951    
2023-01-06 14:45:30,608 - Epoch: [145][  160/  246]    Overall Loss 0.227408    Objective Loss 0.227408                                        LR 0.000008    Time 0.027729    
2023-01-06 14:45:30,860 - Epoch: [145][  170/  246]    Overall Loss 0.226823    Objective Loss 0.226823                                        LR 0.000008    Time 0.027582    
2023-01-06 14:45:31,111 - Epoch: [145][  180/  246]    Overall Loss 0.226200    Objective Loss 0.226200                                        LR 0.000008    Time 0.027438    
2023-01-06 14:45:31,356 - Epoch: [145][  190/  246]    Overall Loss 0.226763    Objective Loss 0.226763                                        LR 0.000008    Time 0.027283    
2023-01-06 14:45:31,605 - Epoch: [145][  200/  246]    Overall Loss 0.226582    Objective Loss 0.226582                                        LR 0.000008    Time 0.027160    
2023-01-06 14:45:31,851 - Epoch: [145][  210/  246]    Overall Loss 0.226896    Objective Loss 0.226896                                        LR 0.000008    Time 0.027037    
2023-01-06 14:45:32,092 - Epoch: [145][  220/  246]    Overall Loss 0.226428    Objective Loss 0.226428                                        LR 0.000008    Time 0.026902    
2023-01-06 14:45:32,345 - Epoch: [145][  230/  246]    Overall Loss 0.226026    Objective Loss 0.226026                                        LR 0.000008    Time 0.026830    
2023-01-06 14:45:32,603 - Epoch: [145][  240/  246]    Overall Loss 0.225087    Objective Loss 0.225087                                        LR 0.000008    Time 0.026784    
2023-01-06 14:45:32,731 - Epoch: [145][  246/  246]    Overall Loss 0.225423    Objective Loss 0.225423    Top1 91.148325    LR 0.000008    Time 0.026650    
2023-01-06 14:45:32,882 - --- validate (epoch=145)-----------
2023-01-06 14:45:32,882 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:33,327 - Epoch: [145][   10/   28]    Loss 0.278742    Top1 89.804688    
2023-01-06 14:45:33,469 - Epoch: [145][   20/   28]    Loss 0.257519    Top1 90.761719    
2023-01-06 14:45:33,562 - Epoch: [145][   28/   28]    Loss 0.261320    Top1 90.724306    
2023-01-06 14:45:33,696 - ==> Top1: 90.724    Loss: 0.261

2023-01-06 14:45:33,696 - ==> Confusion:
[[ 239   21  179]
 [   8  318  276]
 [  65   99 5781]]

2023-01-06 14:45:33,698 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:33,698 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:33,708 - 

2023-01-06 14:45:33,708 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:34,320 - Epoch: [146][   10/  246]    Overall Loss 0.220648    Objective Loss 0.220648                                        LR 0.000008    Time 0.061114    
2023-01-06 14:45:34,560 - Epoch: [146][   20/  246]    Overall Loss 0.232062    Objective Loss 0.232062                                        LR 0.000008    Time 0.042537    
2023-01-06 14:45:34,805 - Epoch: [146][   30/  246]    Overall Loss 0.227052    Objective Loss 0.227052                                        LR 0.000008    Time 0.036526    
2023-01-06 14:45:35,055 - Epoch: [146][   40/  246]    Overall Loss 0.225414    Objective Loss 0.225414                                        LR 0.000008    Time 0.033618    
2023-01-06 14:45:35,305 - Epoch: [146][   50/  246]    Overall Loss 0.225713    Objective Loss 0.225713                                        LR 0.000008    Time 0.031896    
2023-01-06 14:45:35,555 - Epoch: [146][   60/  246]    Overall Loss 0.223335    Objective Loss 0.223335                                        LR 0.000008    Time 0.030741    
2023-01-06 14:45:35,801 - Epoch: [146][   70/  246]    Overall Loss 0.225400    Objective Loss 0.225400                                        LR 0.000008    Time 0.029859    
2023-01-06 14:45:36,049 - Epoch: [146][   80/  246]    Overall Loss 0.224677    Objective Loss 0.224677                                        LR 0.000008    Time 0.029216    
2023-01-06 14:45:36,303 - Epoch: [146][   90/  246]    Overall Loss 0.225124    Objective Loss 0.225124                                        LR 0.000008    Time 0.028784    
2023-01-06 14:45:36,554 - Epoch: [146][  100/  246]    Overall Loss 0.225105    Objective Loss 0.225105                                        LR 0.000008    Time 0.028418    
2023-01-06 14:45:36,805 - Epoch: [146][  110/  246]    Overall Loss 0.225398    Objective Loss 0.225398                                        LR 0.000008    Time 0.028109    
2023-01-06 14:45:37,055 - Epoch: [146][  120/  246]    Overall Loss 0.227996    Objective Loss 0.227996                                        LR 0.000008    Time 0.027846    
2023-01-06 14:45:37,304 - Epoch: [146][  130/  246]    Overall Loss 0.228248    Objective Loss 0.228248                                        LR 0.000008    Time 0.027616    
2023-01-06 14:45:37,556 - Epoch: [146][  140/  246]    Overall Loss 0.227771    Objective Loss 0.227771                                        LR 0.000008    Time 0.027435    
2023-01-06 14:45:37,805 - Epoch: [146][  150/  246]    Overall Loss 0.228521    Objective Loss 0.228521                                        LR 0.000008    Time 0.027255    
2023-01-06 14:45:38,057 - Epoch: [146][  160/  246]    Overall Loss 0.228840    Objective Loss 0.228840                                        LR 0.000008    Time 0.027127    
2023-01-06 14:45:38,305 - Epoch: [146][  170/  246]    Overall Loss 0.228469    Objective Loss 0.228469                                        LR 0.000008    Time 0.026987    
2023-01-06 14:45:38,555 - Epoch: [146][  180/  246]    Overall Loss 0.227050    Objective Loss 0.227050                                        LR 0.000008    Time 0.026874    
2023-01-06 14:45:38,807 - Epoch: [146][  190/  246]    Overall Loss 0.226881    Objective Loss 0.226881                                        LR 0.000008    Time 0.026782    
2023-01-06 14:45:39,060 - Epoch: [146][  200/  246]    Overall Loss 0.227936    Objective Loss 0.227936                                        LR 0.000008    Time 0.026706    
2023-01-06 14:45:39,323 - Epoch: [146][  210/  246]    Overall Loss 0.227547    Objective Loss 0.227547                                        LR 0.000008    Time 0.026687    
2023-01-06 14:45:39,585 - Epoch: [146][  220/  246]    Overall Loss 0.226659    Objective Loss 0.226659                                        LR 0.000008    Time 0.026663    
2023-01-06 14:45:39,835 - Epoch: [146][  230/  246]    Overall Loss 0.226742    Objective Loss 0.226742                                        LR 0.000008    Time 0.026590    
2023-01-06 14:45:40,097 - Epoch: [146][  240/  246]    Overall Loss 0.226016    Objective Loss 0.226016                                        LR 0.000008    Time 0.026562    
2023-01-06 14:45:40,227 - Epoch: [146][  246/  246]    Overall Loss 0.225854    Objective Loss 0.225854    Top1 90.669856    LR 0.000008    Time 0.026443    
2023-01-06 14:45:40,403 - --- validate (epoch=146)-----------
2023-01-06 14:45:40,403 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:40,851 - Epoch: [146][   10/   28]    Loss 0.271808    Top1 90.429688    
2023-01-06 14:45:40,991 - Epoch: [146][   20/   28]    Loss 0.261060    Top1 90.449219    
2023-01-06 14:45:41,083 - Epoch: [146][   28/   28]    Loss 0.251772    Top1 90.853135    
2023-01-06 14:45:41,215 - ==> Top1: 90.853    Loss: 0.252

2023-01-06 14:45:41,215 - ==> Confusion:
[[ 248   15  176]
 [   9  303  290]
 [  58   91 5796]]

2023-01-06 14:45:41,217 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:41,217 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:41,227 - 

2023-01-06 14:45:41,227 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:41,980 - Epoch: [147][   10/  246]    Overall Loss 0.213273    Objective Loss 0.213273                                        LR 0.000008    Time 0.075203    
2023-01-06 14:45:42,239 - Epoch: [147][   20/  246]    Overall Loss 0.217266    Objective Loss 0.217266                                        LR 0.000008    Time 0.050482    
2023-01-06 14:45:42,505 - Epoch: [147][   30/  246]    Overall Loss 0.221082    Objective Loss 0.221082                                        LR 0.000008    Time 0.042494    
2023-01-06 14:45:42,775 - Epoch: [147][   40/  246]    Overall Loss 0.223017    Objective Loss 0.223017                                        LR 0.000008    Time 0.038617    
2023-01-06 14:45:43,038 - Epoch: [147][   50/  246]    Overall Loss 0.218010    Objective Loss 0.218010                                        LR 0.000008    Time 0.036105    
2023-01-06 14:45:43,300 - Epoch: [147][   60/  246]    Overall Loss 0.219069    Objective Loss 0.219069                                        LR 0.000008    Time 0.034435    
2023-01-06 14:45:43,556 - Epoch: [147][   70/  246]    Overall Loss 0.220717    Objective Loss 0.220717                                        LR 0.000008    Time 0.033166    
2023-01-06 14:45:43,807 - Epoch: [147][   80/  246]    Overall Loss 0.221353    Objective Loss 0.221353                                        LR 0.000008    Time 0.032144    
2023-01-06 14:45:44,049 - Epoch: [147][   90/  246]    Overall Loss 0.222806    Objective Loss 0.222806                                        LR 0.000008    Time 0.031259    
2023-01-06 14:45:44,300 - Epoch: [147][  100/  246]    Overall Loss 0.223309    Objective Loss 0.223309                                        LR 0.000008    Time 0.030639    
2023-01-06 14:45:44,547 - Epoch: [147][  110/  246]    Overall Loss 0.223611    Objective Loss 0.223611                                        LR 0.000008    Time 0.030073    
2023-01-06 14:45:44,803 - Epoch: [147][  120/  246]    Overall Loss 0.223362    Objective Loss 0.223362                                        LR 0.000008    Time 0.029696    
2023-01-06 14:45:45,057 - Epoch: [147][  130/  246]    Overall Loss 0.223519    Objective Loss 0.223519                                        LR 0.000008    Time 0.029361    
2023-01-06 14:45:45,312 - Epoch: [147][  140/  246]    Overall Loss 0.222485    Objective Loss 0.222485                                        LR 0.000008    Time 0.029080    
2023-01-06 14:45:45,539 - Epoch: [147][  150/  246]    Overall Loss 0.222703    Objective Loss 0.222703                                        LR 0.000008    Time 0.028653    
2023-01-06 14:45:45,763 - Epoch: [147][  160/  246]    Overall Loss 0.223192    Objective Loss 0.223192                                        LR 0.000008    Time 0.028262    
2023-01-06 14:45:45,988 - Epoch: [147][  170/  246]    Overall Loss 0.222694    Objective Loss 0.222694                                        LR 0.000008    Time 0.027918    
2023-01-06 14:45:46,212 - Epoch: [147][  180/  246]    Overall Loss 0.222368    Objective Loss 0.222368                                        LR 0.000008    Time 0.027612    
2023-01-06 14:45:46,435 - Epoch: [147][  190/  246]    Overall Loss 0.223014    Objective Loss 0.223014                                        LR 0.000008    Time 0.027327    
2023-01-06 14:45:46,658 - Epoch: [147][  200/  246]    Overall Loss 0.223464    Objective Loss 0.223464                                        LR 0.000008    Time 0.027076    
2023-01-06 14:45:46,882 - Epoch: [147][  210/  246]    Overall Loss 0.223296    Objective Loss 0.223296                                        LR 0.000008    Time 0.026848    
2023-01-06 14:45:47,104 - Epoch: [147][  220/  246]    Overall Loss 0.223923    Objective Loss 0.223923                                        LR 0.000008    Time 0.026638    
2023-01-06 14:45:47,326 - Epoch: [147][  230/  246]    Overall Loss 0.224839    Objective Loss 0.224839                                        LR 0.000008    Time 0.026444    
2023-01-06 14:45:47,555 - Epoch: [147][  240/  246]    Overall Loss 0.225146    Objective Loss 0.225146                                        LR 0.000008    Time 0.026293    
2023-01-06 14:45:47,670 - Epoch: [147][  246/  246]    Overall Loss 0.225028    Objective Loss 0.225028    Top1 89.712919    LR 0.000008    Time 0.026120    
2023-01-06 14:45:47,814 - --- validate (epoch=147)-----------
2023-01-06 14:45:47,814 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:48,275 - Epoch: [147][   10/   28]    Loss 0.253415    Top1 90.976562    
2023-01-06 14:45:48,416 - Epoch: [147][   20/   28]    Loss 0.242071    Top1 91.582031    
2023-01-06 14:45:48,507 - Epoch: [147][   28/   28]    Loss 0.258945    Top1 90.967650    
2023-01-06 14:45:48,656 - ==> Top1: 90.968    Loss: 0.259

2023-01-06 14:45:48,656 - ==> Confusion:
[[ 242   11  186]
 [  11  270  321]
 [  49   53 5843]]

2023-01-06 14:45:48,658 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:48,658 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:48,668 - 

2023-01-06 14:45:48,668 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:49,272 - Epoch: [148][   10/  246]    Overall Loss 0.209320    Objective Loss 0.209320                                        LR 0.000008    Time 0.060388    
2023-01-06 14:45:49,523 - Epoch: [148][   20/  246]    Overall Loss 0.223698    Objective Loss 0.223698                                        LR 0.000008    Time 0.042684    
2023-01-06 14:45:49,770 - Epoch: [148][   30/  246]    Overall Loss 0.219849    Objective Loss 0.219849                                        LR 0.000008    Time 0.036693    
2023-01-06 14:45:50,017 - Epoch: [148][   40/  246]    Overall Loss 0.222237    Objective Loss 0.222237                                        LR 0.000008    Time 0.033683    
2023-01-06 14:45:50,265 - Epoch: [148][   50/  246]    Overall Loss 0.222314    Objective Loss 0.222314                                        LR 0.000008    Time 0.031891    
2023-01-06 14:45:50,513 - Epoch: [148][   60/  246]    Overall Loss 0.223529    Objective Loss 0.223529                                        LR 0.000008    Time 0.030710    
2023-01-06 14:45:50,763 - Epoch: [148][   70/  246]    Overall Loss 0.224732    Objective Loss 0.224732                                        LR 0.000008    Time 0.029879    
2023-01-06 14:45:51,011 - Epoch: [148][   80/  246]    Overall Loss 0.225885    Objective Loss 0.225885                                        LR 0.000008    Time 0.029241    
2023-01-06 14:45:51,255 - Epoch: [148][   90/  246]    Overall Loss 0.224913    Objective Loss 0.224913                                        LR 0.000008    Time 0.028692    
2023-01-06 14:45:51,505 - Epoch: [148][  100/  246]    Overall Loss 0.224896    Objective Loss 0.224896                                        LR 0.000008    Time 0.028322    
2023-01-06 14:45:51,757 - Epoch: [148][  110/  246]    Overall Loss 0.226119    Objective Loss 0.226119                                        LR 0.000008    Time 0.028034    
2023-01-06 14:45:52,009 - Epoch: [148][  120/  246]    Overall Loss 0.225737    Objective Loss 0.225737                                        LR 0.000008    Time 0.027795    
2023-01-06 14:45:52,260 - Epoch: [148][  130/  246]    Overall Loss 0.225952    Objective Loss 0.225952                                        LR 0.000008    Time 0.027579    
2023-01-06 14:45:52,511 - Epoch: [148][  140/  246]    Overall Loss 0.224976    Objective Loss 0.224976                                        LR 0.000008    Time 0.027401    
2023-01-06 14:45:52,762 - Epoch: [148][  150/  246]    Overall Loss 0.223976    Objective Loss 0.223976                                        LR 0.000008    Time 0.027243    
2023-01-06 14:45:53,012 - Epoch: [148][  160/  246]    Overall Loss 0.224258    Objective Loss 0.224258                                        LR 0.000008    Time 0.027098    
2023-01-06 14:45:53,262 - Epoch: [148][  170/  246]    Overall Loss 0.223208    Objective Loss 0.223208                                        LR 0.000008    Time 0.026973    
2023-01-06 14:45:53,511 - Epoch: [148][  180/  246]    Overall Loss 0.222697    Objective Loss 0.222697                                        LR 0.000008    Time 0.026859    
2023-01-06 14:45:53,763 - Epoch: [148][  190/  246]    Overall Loss 0.223458    Objective Loss 0.223458                                        LR 0.000008    Time 0.026765    
2023-01-06 14:45:54,014 - Epoch: [148][  200/  246]    Overall Loss 0.223982    Objective Loss 0.223982                                        LR 0.000008    Time 0.026681    
2023-01-06 14:45:54,266 - Epoch: [148][  210/  246]    Overall Loss 0.224236    Objective Loss 0.224236                                        LR 0.000008    Time 0.026606    
2023-01-06 14:45:54,518 - Epoch: [148][  220/  246]    Overall Loss 0.223726    Objective Loss 0.223726                                        LR 0.000008    Time 0.026541    
2023-01-06 14:45:54,768 - Epoch: [148][  230/  246]    Overall Loss 0.224391    Objective Loss 0.224391                                        LR 0.000008    Time 0.026472    
2023-01-06 14:45:55,029 - Epoch: [148][  240/  246]    Overall Loss 0.223084    Objective Loss 0.223084                                        LR 0.000008    Time 0.026456    
2023-01-06 14:45:55,159 - Epoch: [148][  246/  246]    Overall Loss 0.223095    Objective Loss 0.223095    Top1 92.344498    LR 0.000008    Time 0.026339    
2023-01-06 14:45:55,301 - --- validate (epoch=148)-----------
2023-01-06 14:45:55,301 - 6986 samples (256 per mini-batch)
2023-01-06 14:45:55,756 - Epoch: [148][   10/   28]    Loss 0.229037    Top1 91.601562    
2023-01-06 14:45:55,896 - Epoch: [148][   20/   28]    Loss 0.254830    Top1 90.664062    
2023-01-06 14:45:55,989 - Epoch: [148][   28/   28]    Loss 0.250923    Top1 90.724306    
2023-01-06 14:45:56,117 - ==> Top1: 90.724    Loss: 0.251

2023-01-06 14:45:56,117 - ==> Confusion:
[[ 238   16  185]
 [  11  294  297]
 [  59   80 5806]]

2023-01-06 14:45:56,119 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 139]
2023-01-06 14:45:56,120 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:45:56,136 - 

2023-01-06 14:45:56,136 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:45:56,863 - Epoch: [149][   10/  246]    Overall Loss 0.214867    Objective Loss 0.214867                                        LR 0.000008    Time 0.072585    
2023-01-06 14:45:57,099 - Epoch: [149][   20/  246]    Overall Loss 0.221932    Objective Loss 0.221932                                        LR 0.000008    Time 0.048060    
2023-01-06 14:45:57,344 - Epoch: [149][   30/  246]    Overall Loss 0.224024    Objective Loss 0.224024                                        LR 0.000008    Time 0.040169    
2023-01-06 14:45:57,589 - Epoch: [149][   40/  246]    Overall Loss 0.226472    Objective Loss 0.226472                                        LR 0.000008    Time 0.036255    
2023-01-06 14:45:57,834 - Epoch: [149][   50/  246]    Overall Loss 0.226187    Objective Loss 0.226187                                        LR 0.000008    Time 0.033886    
2023-01-06 14:45:58,079 - Epoch: [149][   60/  246]    Overall Loss 0.221875    Objective Loss 0.221875                                        LR 0.000008    Time 0.032313    
2023-01-06 14:45:58,323 - Epoch: [149][   70/  246]    Overall Loss 0.222754    Objective Loss 0.222754                                        LR 0.000008    Time 0.031179    
2023-01-06 14:45:58,568 - Epoch: [149][   80/  246]    Overall Loss 0.221299    Objective Loss 0.221299                                        LR 0.000008    Time 0.030332    
2023-01-06 14:45:58,812 - Epoch: [149][   90/  246]    Overall Loss 0.222326    Objective Loss 0.222326                                        LR 0.000008    Time 0.029672    
2023-01-06 14:45:59,057 - Epoch: [149][  100/  246]    Overall Loss 0.223599    Objective Loss 0.223599                                        LR 0.000008    Time 0.029153    
2023-01-06 14:45:59,300 - Epoch: [149][  110/  246]    Overall Loss 0.224244    Objective Loss 0.224244                                        LR 0.000008    Time 0.028700    
2023-01-06 14:45:59,546 - Epoch: [149][  120/  246]    Overall Loss 0.224629    Objective Loss 0.224629                                        LR 0.000008    Time 0.028356    
2023-01-06 14:45:59,793 - Epoch: [149][  130/  246]    Overall Loss 0.224890    Objective Loss 0.224890                                        LR 0.000008    Time 0.028069    
2023-01-06 14:46:00,037 - Epoch: [149][  140/  246]    Overall Loss 0.224364    Objective Loss 0.224364                                        LR 0.000008    Time 0.027805    
2023-01-06 14:46:00,281 - Epoch: [149][  150/  246]    Overall Loss 0.225344    Objective Loss 0.225344                                        LR 0.000008    Time 0.027572    
2023-01-06 14:46:00,526 - Epoch: [149][  160/  246]    Overall Loss 0.224643    Objective Loss 0.224643                                        LR 0.000008    Time 0.027383    
2023-01-06 14:46:00,771 - Epoch: [149][  170/  246]    Overall Loss 0.224508    Objective Loss 0.224508                                        LR 0.000008    Time 0.027205    
2023-01-06 14:46:01,015 - Epoch: [149][  180/  246]    Overall Loss 0.225537    Objective Loss 0.225537                                        LR 0.000008    Time 0.027046    
2023-01-06 14:46:01,258 - Epoch: [149][  190/  246]    Overall Loss 0.225099    Objective Loss 0.225099                                        LR 0.000008    Time 0.026902    
2023-01-06 14:46:01,502 - Epoch: [149][  200/  246]    Overall Loss 0.224763    Objective Loss 0.224763                                        LR 0.000008    Time 0.026773    
2023-01-06 14:46:01,750 - Epoch: [149][  210/  246]    Overall Loss 0.224597    Objective Loss 0.224597                                        LR 0.000008    Time 0.026676    
2023-01-06 14:46:01,995 - Epoch: [149][  220/  246]    Overall Loss 0.224490    Objective Loss 0.224490                                        LR 0.000008    Time 0.026575    
2023-01-06 14:46:02,238 - Epoch: [149][  230/  246]    Overall Loss 0.224089    Objective Loss 0.224089                                        LR 0.000008    Time 0.026476    
2023-01-06 14:46:02,500 - Epoch: [149][  240/  246]    Overall Loss 0.223195    Objective Loss 0.223195                                        LR 0.000008    Time 0.026459    
2023-01-06 14:46:02,629 - Epoch: [149][  246/  246]    Overall Loss 0.223314    Objective Loss 0.223314    Top1 90.909091    LR 0.000008    Time 0.026340    
2023-01-06 14:46:02,752 - --- validate (epoch=149)-----------
2023-01-06 14:46:02,753 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:03,203 - Epoch: [149][   10/   28]    Loss 0.248189    Top1 91.328125    
2023-01-06 14:46:03,345 - Epoch: [149][   20/   28]    Loss 0.247767    Top1 91.015625    
2023-01-06 14:46:03,436 - Epoch: [149][   28/   28]    Loss 0.250671    Top1 90.981964    
2023-01-06 14:46:03,576 - ==> Top1: 90.982    Loss: 0.251

2023-01-06 14:46:03,576 - ==> Confusion:
[[ 246   15  178]
 [  14  296  292]
 [  49   82 5814]]

2023-01-06 14:46:03,578 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 149]
2023-01-06 14:46:03,578 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:03,599 - 

2023-01-06 14:46:03,599 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:04,334 - Epoch: [150][   10/  246]    Overall Loss 0.216653    Objective Loss 0.216653                                        LR 0.000008    Time 0.073397    
2023-01-06 14:46:04,587 - Epoch: [150][   20/  246]    Overall Loss 0.214820    Objective Loss 0.214820                                        LR 0.000008    Time 0.049345    
2023-01-06 14:46:04,830 - Epoch: [150][   30/  246]    Overall Loss 0.213615    Objective Loss 0.213615                                        LR 0.000008    Time 0.040994    
2023-01-06 14:46:05,076 - Epoch: [150][   40/  246]    Overall Loss 0.220368    Objective Loss 0.220368                                        LR 0.000008    Time 0.036879    
2023-01-06 14:46:05,328 - Epoch: [150][   50/  246]    Overall Loss 0.221765    Objective Loss 0.221765                                        LR 0.000008    Time 0.034538    
2023-01-06 14:46:05,586 - Epoch: [150][   60/  246]    Overall Loss 0.223720    Objective Loss 0.223720                                        LR 0.000008    Time 0.033067    
2023-01-06 14:46:05,832 - Epoch: [150][   70/  246]    Overall Loss 0.228833    Objective Loss 0.228833                                        LR 0.000008    Time 0.031847    
2023-01-06 14:46:06,088 - Epoch: [150][   80/  246]    Overall Loss 0.229343    Objective Loss 0.229343                                        LR 0.000008    Time 0.031060    
2023-01-06 14:46:06,342 - Epoch: [150][   90/  246]    Overall Loss 0.229118    Objective Loss 0.229118                                        LR 0.000008    Time 0.030435    
2023-01-06 14:46:06,599 - Epoch: [150][  100/  246]    Overall Loss 0.227533    Objective Loss 0.227533                                        LR 0.000008    Time 0.029956    
2023-01-06 14:46:06,853 - Epoch: [150][  110/  246]    Overall Loss 0.225273    Objective Loss 0.225273                                        LR 0.000008    Time 0.029532    
2023-01-06 14:46:07,109 - Epoch: [150][  120/  246]    Overall Loss 0.224905    Objective Loss 0.224905                                        LR 0.000008    Time 0.029202    
2023-01-06 14:46:07,362 - Epoch: [150][  130/  246]    Overall Loss 0.225667    Objective Loss 0.225667                                        LR 0.000008    Time 0.028900    
2023-01-06 14:46:07,618 - Epoch: [150][  140/  246]    Overall Loss 0.224252    Objective Loss 0.224252                                        LR 0.000008    Time 0.028660    
2023-01-06 14:46:07,871 - Epoch: [150][  150/  246]    Overall Loss 0.224382    Objective Loss 0.224382                                        LR 0.000008    Time 0.028432    
2023-01-06 14:46:08,121 - Epoch: [150][  160/  246]    Overall Loss 0.223076    Objective Loss 0.223076                                        LR 0.000008    Time 0.028216    
2023-01-06 14:46:08,369 - Epoch: [150][  170/  246]    Overall Loss 0.222366    Objective Loss 0.222366                                        LR 0.000008    Time 0.028011    
2023-01-06 14:46:08,615 - Epoch: [150][  180/  246]    Overall Loss 0.223215    Objective Loss 0.223215                                        LR 0.000008    Time 0.027822    
2023-01-06 14:46:08,864 - Epoch: [150][  190/  246]    Overall Loss 0.223506    Objective Loss 0.223506                                        LR 0.000008    Time 0.027663    
2023-01-06 14:46:09,113 - Epoch: [150][  200/  246]    Overall Loss 0.223827    Objective Loss 0.223827                                        LR 0.000008    Time 0.027521    
2023-01-06 14:46:09,363 - Epoch: [150][  210/  246]    Overall Loss 0.224493    Objective Loss 0.224493                                        LR 0.000008    Time 0.027402    
2023-01-06 14:46:09,612 - Epoch: [150][  220/  246]    Overall Loss 0.223922    Objective Loss 0.223922                                        LR 0.000008    Time 0.027285    
2023-01-06 14:46:09,862 - Epoch: [150][  230/  246]    Overall Loss 0.223595    Objective Loss 0.223595                                        LR 0.000008    Time 0.027181    
2023-01-06 14:46:10,126 - Epoch: [150][  240/  246]    Overall Loss 0.224373    Objective Loss 0.224373                                        LR 0.000008    Time 0.027150    
2023-01-06 14:46:10,257 - Epoch: [150][  246/  246]    Overall Loss 0.224441    Objective Loss 0.224441    Top1 92.105263    LR 0.000008    Time 0.027017    
2023-01-06 14:46:10,391 - --- validate (epoch=150)-----------
2023-01-06 14:46:10,391 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:10,850 - Epoch: [150][   10/   28]    Loss 0.253429    Top1 91.015625    
2023-01-06 14:46:10,989 - Epoch: [150][   20/   28]    Loss 0.259342    Top1 90.859375    
2023-01-06 14:46:11,079 - Epoch: [150][   28/   28]    Loss 0.251976    Top1 90.838820    
2023-01-06 14:46:11,216 - ==> Top1: 90.839    Loss: 0.252

2023-01-06 14:46:11,217 - ==> Confusion:
[[ 268   14  157]
 [  17  295  290]
 [  72   90 5783]]

2023-01-06 14:46:11,218 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 149]
2023-01-06 14:46:11,218 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:11,228 - 

2023-01-06 14:46:11,228 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:11,821 - Epoch: [151][   10/  246]    Overall Loss 0.214024    Objective Loss 0.214024                                        LR 0.000008    Time 0.059152    
2023-01-06 14:46:12,064 - Epoch: [151][   20/  246]    Overall Loss 0.213178    Objective Loss 0.213178                                        LR 0.000008    Time 0.041728    
2023-01-06 14:46:12,307 - Epoch: [151][   30/  246]    Overall Loss 0.216323    Objective Loss 0.216323                                        LR 0.000008    Time 0.035913    
2023-01-06 14:46:12,551 - Epoch: [151][   40/  246]    Overall Loss 0.220458    Objective Loss 0.220458                                        LR 0.000008    Time 0.033019    
2023-01-06 14:46:12,794 - Epoch: [151][   50/  246]    Overall Loss 0.224275    Objective Loss 0.224275                                        LR 0.000008    Time 0.031263    
2023-01-06 14:46:13,015 - Epoch: [151][   60/  246]    Overall Loss 0.225691    Objective Loss 0.225691                                        LR 0.000008    Time 0.029735    
2023-01-06 14:46:13,253 - Epoch: [151][   70/  246]    Overall Loss 0.224983    Objective Loss 0.224983                                        LR 0.000008    Time 0.028883    
2023-01-06 14:46:13,495 - Epoch: [151][   80/  246]    Overall Loss 0.226047    Objective Loss 0.226047                                        LR 0.000008    Time 0.028292    
2023-01-06 14:46:13,738 - Epoch: [151][   90/  246]    Overall Loss 0.225261    Objective Loss 0.225261                                        LR 0.000008    Time 0.027836    
2023-01-06 14:46:13,988 - Epoch: [151][  100/  246]    Overall Loss 0.226077    Objective Loss 0.226077                                        LR 0.000008    Time 0.027545    
2023-01-06 14:46:14,245 - Epoch: [151][  110/  246]    Overall Loss 0.224016    Objective Loss 0.224016                                        LR 0.000008    Time 0.027379    
2023-01-06 14:46:14,503 - Epoch: [151][  120/  246]    Overall Loss 0.222557    Objective Loss 0.222557                                        LR 0.000008    Time 0.027240    
2023-01-06 14:46:14,750 - Epoch: [151][  130/  246]    Overall Loss 0.221828    Objective Loss 0.221828                                        LR 0.000008    Time 0.027044    
2023-01-06 14:46:15,004 - Epoch: [151][  140/  246]    Overall Loss 0.222034    Objective Loss 0.222034                                        LR 0.000008    Time 0.026927    
2023-01-06 14:46:15,252 - Epoch: [151][  150/  246]    Overall Loss 0.221971    Objective Loss 0.221971                                        LR 0.000008    Time 0.026781    
2023-01-06 14:46:15,505 - Epoch: [151][  160/  246]    Overall Loss 0.223429    Objective Loss 0.223429                                        LR 0.000008    Time 0.026687    
2023-01-06 14:46:15,752 - Epoch: [151][  170/  246]    Overall Loss 0.223968    Objective Loss 0.223968                                        LR 0.000008    Time 0.026569    
2023-01-06 14:46:16,005 - Epoch: [151][  180/  246]    Overall Loss 0.224219    Objective Loss 0.224219                                        LR 0.000008    Time 0.026496    
2023-01-06 14:46:16,252 - Epoch: [151][  190/  246]    Overall Loss 0.224001    Objective Loss 0.224001                                        LR 0.000008    Time 0.026399    
2023-01-06 14:46:16,505 - Epoch: [151][  200/  246]    Overall Loss 0.223474    Objective Loss 0.223474                                        LR 0.000008    Time 0.026341    
2023-01-06 14:46:16,720 - Epoch: [151][  210/  246]    Overall Loss 0.222845    Objective Loss 0.222845                                        LR 0.000008    Time 0.026109    
2023-01-06 14:46:16,935 - Epoch: [151][  220/  246]    Overall Loss 0.223861    Objective Loss 0.223861                                        LR 0.000008    Time 0.025902    
2023-01-06 14:46:17,153 - Epoch: [151][  230/  246]    Overall Loss 0.224334    Objective Loss 0.224334                                        LR 0.000008    Time 0.025721    
2023-01-06 14:46:17,392 - Epoch: [151][  240/  246]    Overall Loss 0.224441    Objective Loss 0.224441                                        LR 0.000008    Time 0.025631    
2023-01-06 14:46:17,507 - Epoch: [151][  246/  246]    Overall Loss 0.224190    Objective Loss 0.224190    Top1 92.105263    LR 0.000008    Time 0.025471    
2023-01-06 14:46:17,653 - --- validate (epoch=151)-----------
2023-01-06 14:46:17,653 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:18,098 - Epoch: [151][   10/   28]    Loss 0.262739    Top1 90.273438    
2023-01-06 14:46:18,238 - Epoch: [151][   20/   28]    Loss 0.255587    Top1 90.625000    
2023-01-06 14:46:18,329 - Epoch: [151][   28/   28]    Loss 0.251902    Top1 90.853135    
2023-01-06 14:46:18,451 - ==> Top1: 90.853    Loss: 0.252

2023-01-06 14:46:18,452 - ==> Confusion:
[[ 252   16  171]
 [  11  299  292]
 [  64   85 5796]]

2023-01-06 14:46:18,453 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 149]
2023-01-06 14:46:18,453 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:18,463 - 

2023-01-06 14:46:18,463 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:19,213 - Epoch: [152][   10/  246]    Overall Loss 0.223153    Objective Loss 0.223153                                        LR 0.000008    Time 0.074965    
2023-01-06 14:46:19,461 - Epoch: [152][   20/  246]    Overall Loss 0.219713    Objective Loss 0.219713                                        LR 0.000008    Time 0.049857    
2023-01-06 14:46:19,704 - Epoch: [152][   30/  246]    Overall Loss 0.223941    Objective Loss 0.223941                                        LR 0.000008    Time 0.041308    
2023-01-06 14:46:19,948 - Epoch: [152][   40/  246]    Overall Loss 0.224283    Objective Loss 0.224283                                        LR 0.000008    Time 0.037031    
2023-01-06 14:46:20,194 - Epoch: [152][   50/  246]    Overall Loss 0.221875    Objective Loss 0.221875                                        LR 0.000008    Time 0.034509    
2023-01-06 14:46:20,441 - Epoch: [152][   60/  246]    Overall Loss 0.220637    Objective Loss 0.220637                                        LR 0.000008    Time 0.032862    
2023-01-06 14:46:20,688 - Epoch: [152][   70/  246]    Overall Loss 0.219931    Objective Loss 0.219931                                        LR 0.000008    Time 0.031680    
2023-01-06 14:46:20,933 - Epoch: [152][   80/  246]    Overall Loss 0.220006    Objective Loss 0.220006                                        LR 0.000008    Time 0.030758    
2023-01-06 14:46:21,183 - Epoch: [152][   90/  246]    Overall Loss 0.220245    Objective Loss 0.220245                                        LR 0.000008    Time 0.030094    
2023-01-06 14:46:21,433 - Epoch: [152][  100/  246]    Overall Loss 0.220081    Objective Loss 0.220081                                        LR 0.000008    Time 0.029574    
2023-01-06 14:46:21,682 - Epoch: [152][  110/  246]    Overall Loss 0.219325    Objective Loss 0.219325                                        LR 0.000008    Time 0.029147    
2023-01-06 14:46:21,930 - Epoch: [152][  120/  246]    Overall Loss 0.219555    Objective Loss 0.219555                                        LR 0.000008    Time 0.028782    
2023-01-06 14:46:22,182 - Epoch: [152][  130/  246]    Overall Loss 0.220366    Objective Loss 0.220366                                        LR 0.000008    Time 0.028487    
2023-01-06 14:46:22,431 - Epoch: [152][  140/  246]    Overall Loss 0.221456    Objective Loss 0.221456                                        LR 0.000008    Time 0.028232    
2023-01-06 14:46:22,681 - Epoch: [152][  150/  246]    Overall Loss 0.221588    Objective Loss 0.221588                                        LR 0.000008    Time 0.028012    
2023-01-06 14:46:22,930 - Epoch: [152][  160/  246]    Overall Loss 0.221991    Objective Loss 0.221991                                        LR 0.000008    Time 0.027818    
2023-01-06 14:46:23,181 - Epoch: [152][  170/  246]    Overall Loss 0.222407    Objective Loss 0.222407                                        LR 0.000008    Time 0.027651    
2023-01-06 14:46:23,428 - Epoch: [152][  180/  246]    Overall Loss 0.221461    Objective Loss 0.221461                                        LR 0.000008    Time 0.027486    
2023-01-06 14:46:23,679 - Epoch: [152][  190/  246]    Overall Loss 0.220205    Objective Loss 0.220205                                        LR 0.000008    Time 0.027349    
2023-01-06 14:46:23,929 - Epoch: [152][  200/  246]    Overall Loss 0.220053    Objective Loss 0.220053                                        LR 0.000008    Time 0.027230    
2023-01-06 14:46:24,179 - Epoch: [152][  210/  246]    Overall Loss 0.221504    Objective Loss 0.221504                                        LR 0.000008    Time 0.027120    
2023-01-06 14:46:24,427 - Epoch: [152][  220/  246]    Overall Loss 0.222816    Objective Loss 0.222816                                        LR 0.000008    Time 0.027013    
2023-01-06 14:46:24,678 - Epoch: [152][  230/  246]    Overall Loss 0.223065    Objective Loss 0.223065                                        LR 0.000008    Time 0.026920    
2023-01-06 14:46:24,936 - Epoch: [152][  240/  246]    Overall Loss 0.222999    Objective Loss 0.222999                                        LR 0.000008    Time 0.026874    
2023-01-06 14:46:25,064 - Epoch: [152][  246/  246]    Overall Loss 0.222967    Objective Loss 0.222967    Top1 91.148325    LR 0.000008    Time 0.026738    
2023-01-06 14:46:25,195 - --- validate (epoch=152)-----------
2023-01-06 14:46:25,196 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:25,635 - Epoch: [152][   10/   28]    Loss 0.239192    Top1 90.976562    
2023-01-06 14:46:25,777 - Epoch: [152][   20/   28]    Loss 0.246245    Top1 91.093750    
2023-01-06 14:46:25,868 - Epoch: [152][   28/   28]    Loss 0.251098    Top1 90.838820    
2023-01-06 14:46:26,006 - ==> Top1: 90.839    Loss: 0.251

2023-01-06 14:46:26,006 - ==> Confusion:
[[ 266   10  163]
 [  22  284  296]
 [  79   70 5796]]

2023-01-06 14:46:26,007 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 149]
2023-01-06 14:46:26,008 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:26,017 - 

2023-01-06 14:46:26,018 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:26,740 - Epoch: [153][   10/  246]    Overall Loss 0.200876    Objective Loss 0.200876                                        LR 0.000008    Time 0.072173    
2023-01-06 14:46:26,987 - Epoch: [153][   20/  246]    Overall Loss 0.217419    Objective Loss 0.217419                                        LR 0.000008    Time 0.048436    
2023-01-06 14:46:27,245 - Epoch: [153][   30/  246]    Overall Loss 0.219011    Objective Loss 0.219011                                        LR 0.000008    Time 0.040855    
2023-01-06 14:46:27,510 - Epoch: [153][   40/  246]    Overall Loss 0.224113    Objective Loss 0.224113                                        LR 0.000008    Time 0.037260    
2023-01-06 14:46:27,763 - Epoch: [153][   50/  246]    Overall Loss 0.220066    Objective Loss 0.220066                                        LR 0.000008    Time 0.034871    
2023-01-06 14:46:28,025 - Epoch: [153][   60/  246]    Overall Loss 0.219612    Objective Loss 0.219612                                        LR 0.000008    Time 0.033382    
2023-01-06 14:46:28,285 - Epoch: [153][   70/  246]    Overall Loss 0.218436    Objective Loss 0.218436                                        LR 0.000008    Time 0.032323    
2023-01-06 14:46:28,546 - Epoch: [153][   80/  246]    Overall Loss 0.219564    Objective Loss 0.219564                                        LR 0.000008    Time 0.031543    
2023-01-06 14:46:28,802 - Epoch: [153][   90/  246]    Overall Loss 0.221036    Objective Loss 0.221036                                        LR 0.000008    Time 0.030881    
2023-01-06 14:46:29,063 - Epoch: [153][  100/  246]    Overall Loss 0.221852    Objective Loss 0.221852                                        LR 0.000008    Time 0.030379    
2023-01-06 14:46:29,319 - Epoch: [153][  110/  246]    Overall Loss 0.221524    Objective Loss 0.221524                                        LR 0.000008    Time 0.029945    
2023-01-06 14:46:29,578 - Epoch: [153][  120/  246]    Overall Loss 0.219767    Objective Loss 0.219767                                        LR 0.000008    Time 0.029593    
2023-01-06 14:46:29,833 - Epoch: [153][  130/  246]    Overall Loss 0.218898    Objective Loss 0.218898                                        LR 0.000008    Time 0.029278    
2023-01-06 14:46:30,091 - Epoch: [153][  140/  246]    Overall Loss 0.218423    Objective Loss 0.218423                                        LR 0.000008    Time 0.029018    
2023-01-06 14:46:30,348 - Epoch: [153][  150/  246]    Overall Loss 0.219927    Objective Loss 0.219927                                        LR 0.000008    Time 0.028790    
2023-01-06 14:46:30,607 - Epoch: [153][  160/  246]    Overall Loss 0.221068    Objective Loss 0.221068                                        LR 0.000008    Time 0.028597    
2023-01-06 14:46:30,863 - Epoch: [153][  170/  246]    Overall Loss 0.221522    Objective Loss 0.221522                                        LR 0.000008    Time 0.028419    
2023-01-06 14:46:31,122 - Epoch: [153][  180/  246]    Overall Loss 0.221559    Objective Loss 0.221559                                        LR 0.000008    Time 0.028271    
2023-01-06 14:46:31,376 - Epoch: [153][  190/  246]    Overall Loss 0.221559    Objective Loss 0.221559                                        LR 0.000008    Time 0.028119    
2023-01-06 14:46:31,636 - Epoch: [153][  200/  246]    Overall Loss 0.221592    Objective Loss 0.221592                                        LR 0.000008    Time 0.028001    
2023-01-06 14:46:31,894 - Epoch: [153][  210/  246]    Overall Loss 0.221453    Objective Loss 0.221453                                        LR 0.000008    Time 0.027895    
2023-01-06 14:46:32,153 - Epoch: [153][  220/  246]    Overall Loss 0.221765    Objective Loss 0.221765                                        LR 0.000008    Time 0.027804    
2023-01-06 14:46:32,409 - Epoch: [153][  230/  246]    Overall Loss 0.221715    Objective Loss 0.221715                                        LR 0.000008    Time 0.027708    
2023-01-06 14:46:32,675 - Epoch: [153][  240/  246]    Overall Loss 0.222015    Objective Loss 0.222015                                        LR 0.000008    Time 0.027651    
2023-01-06 14:46:32,805 - Epoch: [153][  246/  246]    Overall Loss 0.222033    Objective Loss 0.222033    Top1 91.626794    LR 0.000008    Time 0.027504    
2023-01-06 14:46:32,926 - --- validate (epoch=153)-----------
2023-01-06 14:46:32,926 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:33,394 - Epoch: [153][   10/   28]    Loss 0.239185    Top1 91.367188    
2023-01-06 14:46:33,549 - Epoch: [153][   20/   28]    Loss 0.256159    Top1 91.015625    
2023-01-06 14:46:33,641 - Epoch: [153][   28/   28]    Loss 0.257400    Top1 90.967650    
2023-01-06 14:46:33,773 - ==> Top1: 90.968    Loss: 0.257

2023-01-06 14:46:33,773 - ==> Confusion:
[[ 248   16  175]
 [  10  307  285]
 [  55   90 5800]]

2023-01-06 14:46:33,775 - ==> Best [Top1: 90.982   Sparsity:0.00   Params: 360896 on epoch: 149]
2023-01-06 14:46:33,775 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:33,785 - 

2023-01-06 14:46:33,785 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:34,373 - Epoch: [154][   10/  246]    Overall Loss 0.217027    Objective Loss 0.217027                                        LR 0.000008    Time 0.058747    
2023-01-06 14:46:34,619 - Epoch: [154][   20/  246]    Overall Loss 0.219407    Objective Loss 0.219407                                        LR 0.000008    Time 0.041629    
2023-01-06 14:46:34,872 - Epoch: [154][   30/  246]    Overall Loss 0.218136    Objective Loss 0.218136                                        LR 0.000008    Time 0.036180    
2023-01-06 14:46:35,129 - Epoch: [154][   40/  246]    Overall Loss 0.216990    Objective Loss 0.216990                                        LR 0.000008    Time 0.033553    
2023-01-06 14:46:35,388 - Epoch: [154][   50/  246]    Overall Loss 0.222042    Objective Loss 0.222042                                        LR 0.000008    Time 0.032013    
2023-01-06 14:46:35,637 - Epoch: [154][   60/  246]    Overall Loss 0.222257    Objective Loss 0.222257                                        LR 0.000008    Time 0.030822    
2023-01-06 14:46:35,886 - Epoch: [154][   70/  246]    Overall Loss 0.221629    Objective Loss 0.221629                                        LR 0.000008    Time 0.029966    
2023-01-06 14:46:36,135 - Epoch: [154][   80/  246]    Overall Loss 0.221997    Objective Loss 0.221997                                        LR 0.000008    Time 0.029331    
2023-01-06 14:46:36,384 - Epoch: [154][   90/  246]    Overall Loss 0.220846    Objective Loss 0.220846                                        LR 0.000008    Time 0.028824    
2023-01-06 14:46:36,624 - Epoch: [154][  100/  246]    Overall Loss 0.222750    Objective Loss 0.222750                                        LR 0.000008    Time 0.028343    
2023-01-06 14:46:36,879 - Epoch: [154][  110/  246]    Overall Loss 0.223933    Objective Loss 0.223933                                        LR 0.000008    Time 0.028074    
2023-01-06 14:46:37,142 - Epoch: [154][  120/  246]    Overall Loss 0.223892    Objective Loss 0.223892                                        LR 0.000008    Time 0.027926    
2023-01-06 14:46:37,399 - Epoch: [154][  130/  246]    Overall Loss 0.223254    Objective Loss 0.223254                                        LR 0.000008    Time 0.027749    
2023-01-06 14:46:37,660 - Epoch: [154][  140/  246]    Overall Loss 0.223212    Objective Loss 0.223212                                        LR 0.000008    Time 0.027626    
2023-01-06 14:46:37,921 - Epoch: [154][  150/  246]    Overall Loss 0.223436    Objective Loss 0.223436                                        LR 0.000008    Time 0.027525    
2023-01-06 14:46:38,181 - Epoch: [154][  160/  246]    Overall Loss 0.224030    Objective Loss 0.224030                                        LR 0.000008    Time 0.027427    
2023-01-06 14:46:38,437 - Epoch: [154][  170/  246]    Overall Loss 0.223753    Objective Loss 0.223753                                        LR 0.000008    Time 0.027312    
2023-01-06 14:46:38,692 - Epoch: [154][  180/  246]    Overall Loss 0.223029    Objective Loss 0.223029                                        LR 0.000008    Time 0.027201    
2023-01-06 14:46:38,953 - Epoch: [154][  190/  246]    Overall Loss 0.223269    Objective Loss 0.223269                                        LR 0.000008    Time 0.027138    
2023-01-06 14:46:39,197 - Epoch: [154][  200/  246]    Overall Loss 0.223396    Objective Loss 0.223396                                        LR 0.000008    Time 0.027001    
2023-01-06 14:46:39,452 - Epoch: [154][  210/  246]    Overall Loss 0.223650    Objective Loss 0.223650                                        LR 0.000008    Time 0.026931    
2023-01-06 14:46:39,702 - Epoch: [154][  220/  246]    Overall Loss 0.223952    Objective Loss 0.223952                                        LR 0.000008    Time 0.026841    
2023-01-06 14:46:39,959 - Epoch: [154][  230/  246]    Overall Loss 0.224011    Objective Loss 0.224011                                        LR 0.000008    Time 0.026788    
2023-01-06 14:46:40,216 - Epoch: [154][  240/  246]    Overall Loss 0.223142    Objective Loss 0.223142                                        LR 0.000008    Time 0.026743    
2023-01-06 14:46:40,346 - Epoch: [154][  246/  246]    Overall Loss 0.223237    Objective Loss 0.223237    Top1 90.430622    LR 0.000008    Time 0.026618    
2023-01-06 14:46:40,473 - --- validate (epoch=154)-----------
2023-01-06 14:46:40,473 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:40,928 - Epoch: [154][   10/   28]    Loss 0.252946    Top1 90.781250    
2023-01-06 14:46:41,082 - Epoch: [154][   20/   28]    Loss 0.245251    Top1 91.152344    
2023-01-06 14:46:41,173 - Epoch: [154][   28/   28]    Loss 0.250026    Top1 91.024907    
2023-01-06 14:46:41,312 - ==> Top1: 91.025    Loss: 0.250

2023-01-06 14:46:41,313 - ==> Confusion:
[[ 252   17  170]
 [  11  295  296]
 [  59   74 5812]]

2023-01-06 14:46:41,314 - ==> Best [Top1: 91.025   Sparsity:0.00   Params: 360896 on epoch: 154]
2023-01-06 14:46:41,314 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:41,335 - 

2023-01-06 14:46:41,335 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:42,100 - Epoch: [155][   10/  246]    Overall Loss 0.224231    Objective Loss 0.224231                                        LR 0.000008    Time 0.076350    
2023-01-06 14:46:42,343 - Epoch: [155][   20/  246]    Overall Loss 0.215573    Objective Loss 0.215573                                        LR 0.000008    Time 0.050325    
2023-01-06 14:46:42,593 - Epoch: [155][   30/  246]    Overall Loss 0.213062    Objective Loss 0.213062                                        LR 0.000008    Time 0.041868    
2023-01-06 14:46:42,837 - Epoch: [155][   40/  246]    Overall Loss 0.220848    Objective Loss 0.220848                                        LR 0.000008    Time 0.037476    
2023-01-06 14:46:43,086 - Epoch: [155][   50/  246]    Overall Loss 0.222386    Objective Loss 0.222386                                        LR 0.000008    Time 0.034960    
2023-01-06 14:46:43,346 - Epoch: [155][   60/  246]    Overall Loss 0.223415    Objective Loss 0.223415                                        LR 0.000008    Time 0.033470    
2023-01-06 14:46:43,614 - Epoch: [155][   70/  246]    Overall Loss 0.223846    Objective Loss 0.223846                                        LR 0.000008    Time 0.032506    
2023-01-06 14:46:43,873 - Epoch: [155][   80/  246]    Overall Loss 0.222527    Objective Loss 0.222527                                        LR 0.000008    Time 0.031671    
2023-01-06 14:46:44,140 - Epoch: [155][   90/  246]    Overall Loss 0.223404    Objective Loss 0.223404                                        LR 0.000008    Time 0.031121    
2023-01-06 14:46:44,398 - Epoch: [155][  100/  246]    Overall Loss 0.222996    Objective Loss 0.222996                                        LR 0.000008    Time 0.030581    
2023-01-06 14:46:44,667 - Epoch: [155][  110/  246]    Overall Loss 0.222210    Objective Loss 0.222210                                        LR 0.000008    Time 0.030246    
2023-01-06 14:46:44,926 - Epoch: [155][  120/  246]    Overall Loss 0.220924    Objective Loss 0.220924                                        LR 0.000008    Time 0.029880    
2023-01-06 14:46:45,184 - Epoch: [155][  130/  246]    Overall Loss 0.221516    Objective Loss 0.221516                                        LR 0.000008    Time 0.029561    
2023-01-06 14:46:45,438 - Epoch: [155][  140/  246]    Overall Loss 0.220896    Objective Loss 0.220896                                        LR 0.000008    Time 0.029263    
2023-01-06 14:46:45,695 - Epoch: [155][  150/  246]    Overall Loss 0.220943    Objective Loss 0.220943                                        LR 0.000008    Time 0.029020    
2023-01-06 14:46:45,949 - Epoch: [155][  160/  246]    Overall Loss 0.220373    Objective Loss 0.220373                                        LR 0.000008    Time 0.028790    
2023-01-06 14:46:46,200 - Epoch: [155][  170/  246]    Overall Loss 0.221693    Objective Loss 0.221693                                        LR 0.000008    Time 0.028572    
2023-01-06 14:46:46,450 - Epoch: [155][  180/  246]    Overall Loss 0.221792    Objective Loss 0.221792                                        LR 0.000008    Time 0.028370    
2023-01-06 14:46:46,698 - Epoch: [155][  190/  246]    Overall Loss 0.221382    Objective Loss 0.221382                                        LR 0.000008    Time 0.028180    
2023-01-06 14:46:46,947 - Epoch: [155][  200/  246]    Overall Loss 0.221700    Objective Loss 0.221700                                        LR 0.000008    Time 0.028013    
2023-01-06 14:46:47,195 - Epoch: [155][  210/  246]    Overall Loss 0.221984    Objective Loss 0.221984                                        LR 0.000008    Time 0.027859    
2023-01-06 14:46:47,441 - Epoch: [155][  220/  246]    Overall Loss 0.221428    Objective Loss 0.221428                                        LR 0.000008    Time 0.027706    
2023-01-06 14:46:47,687 - Epoch: [155][  230/  246]    Overall Loss 0.221831    Objective Loss 0.221831                                        LR 0.000008    Time 0.027571    
2023-01-06 14:46:47,947 - Epoch: [155][  240/  246]    Overall Loss 0.222023    Objective Loss 0.222023                                        LR 0.000008    Time 0.027505    
2023-01-06 14:46:48,077 - Epoch: [155][  246/  246]    Overall Loss 0.221812    Objective Loss 0.221812    Top1 91.387560    LR 0.000008    Time 0.027362    
2023-01-06 14:46:48,214 - --- validate (epoch=155)-----------
2023-01-06 14:46:48,214 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:48,657 - Epoch: [155][   10/   28]    Loss 0.251844    Top1 90.820312    
2023-01-06 14:46:48,795 - Epoch: [155][   20/   28]    Loss 0.258138    Top1 90.566406    
2023-01-06 14:46:48,887 - Epoch: [155][   28/   28]    Loss 0.251172    Top1 91.010593    
2023-01-06 14:46:49,023 - ==> Top1: 91.011    Loss: 0.251

2023-01-06 14:46:49,023 - ==> Confusion:
[[ 270   16  153]
 [  18  304  280]
 [  73   88 5784]]

2023-01-06 14:46:49,025 - ==> Best [Top1: 91.025   Sparsity:0.00   Params: 360896 on epoch: 154]
2023-01-06 14:46:49,025 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:49,034 - 

2023-01-06 14:46:49,035 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:49,625 - Epoch: [156][   10/  246]    Overall Loss 0.218241    Objective Loss 0.218241                                        LR 0.000008    Time 0.058931    
2023-01-06 14:46:49,870 - Epoch: [156][   20/  246]    Overall Loss 0.221136    Objective Loss 0.221136                                        LR 0.000008    Time 0.041701    
2023-01-06 14:46:50,118 - Epoch: [156][   30/  246]    Overall Loss 0.223680    Objective Loss 0.223680                                        LR 0.000008    Time 0.036051    
2023-01-06 14:46:50,366 - Epoch: [156][   40/  246]    Overall Loss 0.221576    Objective Loss 0.221576                                        LR 0.000008    Time 0.033230    
2023-01-06 14:46:50,614 - Epoch: [156][   50/  246]    Overall Loss 0.220108    Objective Loss 0.220108                                        LR 0.000008    Time 0.031543    
2023-01-06 14:46:50,868 - Epoch: [156][   60/  246]    Overall Loss 0.219013    Objective Loss 0.219013                                        LR 0.000008    Time 0.030504    
2023-01-06 14:46:51,119 - Epoch: [156][   70/  246]    Overall Loss 0.220082    Objective Loss 0.220082                                        LR 0.000008    Time 0.029720    
2023-01-06 14:46:51,371 - Epoch: [156][   80/  246]    Overall Loss 0.220785    Objective Loss 0.220785                                        LR 0.000008    Time 0.029154    
2023-01-06 14:46:51,623 - Epoch: [156][   90/  246]    Overall Loss 0.219255    Objective Loss 0.219255                                        LR 0.000008    Time 0.028708    
2023-01-06 14:46:51,874 - Epoch: [156][  100/  246]    Overall Loss 0.220500    Objective Loss 0.220500                                        LR 0.000008    Time 0.028344    
2023-01-06 14:46:52,125 - Epoch: [156][  110/  246]    Overall Loss 0.220851    Objective Loss 0.220851                                        LR 0.000008    Time 0.028044    
2023-01-06 14:46:52,374 - Epoch: [156][  120/  246]    Overall Loss 0.221238    Objective Loss 0.221238                                        LR 0.000008    Time 0.027776    
2023-01-06 14:46:52,622 - Epoch: [156][  130/  246]    Overall Loss 0.220828    Objective Loss 0.220828                                        LR 0.000008    Time 0.027539    
2023-01-06 14:46:52,873 - Epoch: [156][  140/  246]    Overall Loss 0.221399    Objective Loss 0.221399                                        LR 0.000008    Time 0.027360    
2023-01-06 14:46:53,119 - Epoch: [156][  150/  246]    Overall Loss 0.220566    Objective Loss 0.220566                                        LR 0.000008    Time 0.027178    
2023-01-06 14:46:53,372 - Epoch: [156][  160/  246]    Overall Loss 0.220737    Objective Loss 0.220737                                        LR 0.000008    Time 0.027053    
2023-01-06 14:46:53,620 - Epoch: [156][  170/  246]    Overall Loss 0.220325    Objective Loss 0.220325                                        LR 0.000008    Time 0.026922    
2023-01-06 14:46:53,871 - Epoch: [156][  180/  246]    Overall Loss 0.220659    Objective Loss 0.220659                                        LR 0.000008    Time 0.026818    
2023-01-06 14:46:54,118 - Epoch: [156][  190/  246]    Overall Loss 0.220383    Objective Loss 0.220383                                        LR 0.000008    Time 0.026699    
2023-01-06 14:46:54,369 - Epoch: [156][  200/  246]    Overall Loss 0.220579    Objective Loss 0.220579                                        LR 0.000008    Time 0.026617    
2023-01-06 14:46:54,620 - Epoch: [156][  210/  246]    Overall Loss 0.220613    Objective Loss 0.220613                                        LR 0.000008    Time 0.026543    
2023-01-06 14:46:54,871 - Epoch: [156][  220/  246]    Overall Loss 0.220719    Objective Loss 0.220719                                        LR 0.000008    Time 0.026473    
2023-01-06 14:46:55,119 - Epoch: [156][  230/  246]    Overall Loss 0.221392    Objective Loss 0.221392                                        LR 0.000008    Time 0.026399    
2023-01-06 14:46:55,378 - Epoch: [156][  240/  246]    Overall Loss 0.221307    Objective Loss 0.221307                                        LR 0.000008    Time 0.026377    
2023-01-06 14:46:55,508 - Epoch: [156][  246/  246]    Overall Loss 0.221462    Objective Loss 0.221462    Top1 91.387560    LR 0.000008    Time 0.026260    
2023-01-06 14:46:55,667 - --- validate (epoch=156)-----------
2023-01-06 14:46:55,668 - 6986 samples (256 per mini-batch)
2023-01-06 14:46:56,107 - Epoch: [156][   10/   28]    Loss 0.243951    Top1 91.367188    
2023-01-06 14:46:56,250 - Epoch: [156][   20/   28]    Loss 0.238252    Top1 91.601562    
2023-01-06 14:46:56,342 - Epoch: [156][   28/   28]    Loss 0.245884    Top1 91.125107    
2023-01-06 14:46:56,480 - ==> Top1: 91.125    Loss: 0.246

2023-01-06 14:46:56,481 - ==> Confusion:
[[ 264   10  165]
 [  15  301  286]
 [  66   78 5801]]

2023-01-06 14:46:56,482 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 156]
2023-01-06 14:46:56,482 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:46:56,506 - 

2023-01-06 14:46:56,506 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:46:57,235 - Epoch: [157][   10/  246]    Overall Loss 0.207678    Objective Loss 0.207678                                        LR 0.000008    Time 0.072877    
2023-01-06 14:46:57,480 - Epoch: [157][   20/  246]    Overall Loss 0.212621    Objective Loss 0.212621                                        LR 0.000008    Time 0.048650    
2023-01-06 14:46:57,738 - Epoch: [157][   30/  246]    Overall Loss 0.209356    Objective Loss 0.209356                                        LR 0.000008    Time 0.041011    
2023-01-06 14:46:57,992 - Epoch: [157][   40/  246]    Overall Loss 0.211232    Objective Loss 0.211232                                        LR 0.000008    Time 0.037072    
2023-01-06 14:46:58,246 - Epoch: [157][   50/  246]    Overall Loss 0.213418    Objective Loss 0.213418                                        LR 0.000008    Time 0.034736    
2023-01-06 14:46:58,512 - Epoch: [157][   60/  246]    Overall Loss 0.214186    Objective Loss 0.214186                                        LR 0.000008    Time 0.033373    
2023-01-06 14:46:58,774 - Epoch: [157][   70/  246]    Overall Loss 0.215951    Objective Loss 0.215951                                        LR 0.000008    Time 0.032338    
2023-01-06 14:46:59,038 - Epoch: [157][   80/  246]    Overall Loss 0.216162    Objective Loss 0.216162                                        LR 0.000008    Time 0.031591    
2023-01-06 14:46:59,301 - Epoch: [157][   90/  246]    Overall Loss 0.215975    Objective Loss 0.215975                                        LR 0.000008    Time 0.031007    
2023-01-06 14:46:59,557 - Epoch: [157][  100/  246]    Overall Loss 0.217270    Objective Loss 0.217270                                        LR 0.000008    Time 0.030446    
2023-01-06 14:46:59,817 - Epoch: [157][  110/  246]    Overall Loss 0.218329    Objective Loss 0.218329                                        LR 0.000008    Time 0.030031    
2023-01-06 14:47:00,073 - Epoch: [157][  120/  246]    Overall Loss 0.218657    Objective Loss 0.218657                                        LR 0.000008    Time 0.029656    
2023-01-06 14:47:00,332 - Epoch: [157][  130/  246]    Overall Loss 0.219432    Objective Loss 0.219432                                        LR 0.000008    Time 0.029363    
2023-01-06 14:47:00,584 - Epoch: [157][  140/  246]    Overall Loss 0.219051    Objective Loss 0.219051                                        LR 0.000008    Time 0.029063    
2023-01-06 14:47:00,838 - Epoch: [157][  150/  246]    Overall Loss 0.218784    Objective Loss 0.218784                                        LR 0.000008    Time 0.028812    
2023-01-06 14:47:01,090 - Epoch: [157][  160/  246]    Overall Loss 0.219338    Objective Loss 0.219338                                        LR 0.000008    Time 0.028586    
2023-01-06 14:47:01,349 - Epoch: [157][  170/  246]    Overall Loss 0.218629    Objective Loss 0.218629                                        LR 0.000008    Time 0.028427    
2023-01-06 14:47:01,605 - Epoch: [157][  180/  246]    Overall Loss 0.218700    Objective Loss 0.218700                                        LR 0.000008    Time 0.028264    
2023-01-06 14:47:01,866 - Epoch: [157][  190/  246]    Overall Loss 0.219707    Objective Loss 0.219707                                        LR 0.000008    Time 0.028148    
2023-01-06 14:47:02,134 - Epoch: [157][  200/  246]    Overall Loss 0.218986    Objective Loss 0.218986                                        LR 0.000008    Time 0.028069    
2023-01-06 14:47:02,399 - Epoch: [157][  210/  246]    Overall Loss 0.219686    Objective Loss 0.219686                                        LR 0.000008    Time 0.027994    
2023-01-06 14:47:02,668 - Epoch: [157][  220/  246]    Overall Loss 0.219349    Objective Loss 0.219349                                        LR 0.000008    Time 0.027931    
2023-01-06 14:47:02,930 - Epoch: [157][  230/  246]    Overall Loss 0.220139    Objective Loss 0.220139                                        LR 0.000008    Time 0.027856    
2023-01-06 14:47:03,204 - Epoch: [157][  240/  246]    Overall Loss 0.220061    Objective Loss 0.220061                                        LR 0.000008    Time 0.027837    
2023-01-06 14:47:03,332 - Epoch: [157][  246/  246]    Overall Loss 0.220529    Objective Loss 0.220529    Top1 92.105263    LR 0.000008    Time 0.027678    
2023-01-06 14:47:03,466 - --- validate (epoch=157)-----------
2023-01-06 14:47:03,466 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:03,941 - Epoch: [157][   10/   28]    Loss 0.236331    Top1 91.992188    
2023-01-06 14:47:04,101 - Epoch: [157][   20/   28]    Loss 0.252428    Top1 91.171875    
2023-01-06 14:47:04,192 - Epoch: [157][   28/   28]    Loss 0.249196    Top1 91.096479    
2023-01-06 14:47:04,323 - ==> Top1: 91.096    Loss: 0.249

2023-01-06 14:47:04,326 - ==> Confusion:
[[ 259   19  161]
 [  13  308  281]
 [  66   82 5797]]

2023-01-06 14:47:04,327 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 156]
2023-01-06 14:47:04,327 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:04,337 - 

2023-01-06 14:47:04,337 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:04,957 - Epoch: [158][   10/  246]    Overall Loss 0.223308    Objective Loss 0.223308                                        LR 0.000008    Time 0.061882    
2023-01-06 14:47:05,221 - Epoch: [158][   20/  246]    Overall Loss 0.231821    Objective Loss 0.231821                                        LR 0.000008    Time 0.044136    
2023-01-06 14:47:05,480 - Epoch: [158][   30/  246]    Overall Loss 0.228398    Objective Loss 0.228398                                        LR 0.000008    Time 0.038044    
2023-01-06 14:47:05,732 - Epoch: [158][   40/  246]    Overall Loss 0.226533    Objective Loss 0.226533                                        LR 0.000008    Time 0.034801    
2023-01-06 14:47:05,988 - Epoch: [158][   50/  246]    Overall Loss 0.223482    Objective Loss 0.223482                                        LR 0.000008    Time 0.032970    
2023-01-06 14:47:06,239 - Epoch: [158][   60/  246]    Overall Loss 0.223036    Objective Loss 0.223036                                        LR 0.000008    Time 0.031640    
2023-01-06 14:47:06,500 - Epoch: [158][   70/  246]    Overall Loss 0.220397    Objective Loss 0.220397                                        LR 0.000008    Time 0.030819    
2023-01-06 14:47:06,757 - Epoch: [158][   80/  246]    Overall Loss 0.218914    Objective Loss 0.218914                                        LR 0.000008    Time 0.030171    
2023-01-06 14:47:07,012 - Epoch: [158][   90/  246]    Overall Loss 0.219742    Objective Loss 0.219742                                        LR 0.000008    Time 0.029656    
2023-01-06 14:47:07,257 - Epoch: [158][  100/  246]    Overall Loss 0.220741    Objective Loss 0.220741                                        LR 0.000008    Time 0.029135    
2023-01-06 14:47:07,502 - Epoch: [158][  110/  246]    Overall Loss 0.221098    Objective Loss 0.221098                                        LR 0.000008    Time 0.028690    
2023-01-06 14:47:07,744 - Epoch: [158][  120/  246]    Overall Loss 0.220769    Objective Loss 0.220769                                        LR 0.000008    Time 0.028319    
2023-01-06 14:47:07,990 - Epoch: [158][  130/  246]    Overall Loss 0.221529    Objective Loss 0.221529                                        LR 0.000008    Time 0.028011    
2023-01-06 14:47:08,231 - Epoch: [158][  140/  246]    Overall Loss 0.222479    Objective Loss 0.222479                                        LR 0.000008    Time 0.027735    
2023-01-06 14:47:08,484 - Epoch: [158][  150/  246]    Overall Loss 0.222538    Objective Loss 0.222538                                        LR 0.000008    Time 0.027558    
2023-01-06 14:47:08,734 - Epoch: [158][  160/  246]    Overall Loss 0.222872    Objective Loss 0.222872                                        LR 0.000008    Time 0.027390    
2023-01-06 14:47:08,983 - Epoch: [158][  170/  246]    Overall Loss 0.221649    Objective Loss 0.221649                                        LR 0.000008    Time 0.027241    
2023-01-06 14:47:09,236 - Epoch: [158][  180/  246]    Overall Loss 0.220463    Objective Loss 0.220463                                        LR 0.000008    Time 0.027132    
2023-01-06 14:47:09,490 - Epoch: [158][  190/  246]    Overall Loss 0.220207    Objective Loss 0.220207                                        LR 0.000008    Time 0.027042    
2023-01-06 14:47:09,743 - Epoch: [158][  200/  246]    Overall Loss 0.220778    Objective Loss 0.220778                                        LR 0.000008    Time 0.026950    
2023-01-06 14:47:09,998 - Epoch: [158][  210/  246]    Overall Loss 0.220892    Objective Loss 0.220892                                        LR 0.000008    Time 0.026878    
2023-01-06 14:47:10,250 - Epoch: [158][  220/  246]    Overall Loss 0.220464    Objective Loss 0.220464                                        LR 0.000008    Time 0.026803    
2023-01-06 14:47:10,505 - Epoch: [158][  230/  246]    Overall Loss 0.220195    Objective Loss 0.220195                                        LR 0.000008    Time 0.026745    
2023-01-06 14:47:10,770 - Epoch: [158][  240/  246]    Overall Loss 0.220994    Objective Loss 0.220994                                        LR 0.000008    Time 0.026732    
2023-01-06 14:47:10,900 - Epoch: [158][  246/  246]    Overall Loss 0.220830    Objective Loss 0.220830    Top1 92.583732    LR 0.000008    Time 0.026607    
2023-01-06 14:47:11,036 - --- validate (epoch=158)-----------
2023-01-06 14:47:11,036 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:11,500 - Epoch: [158][   10/   28]    Loss 0.266557    Top1 90.625000    
2023-01-06 14:47:11,648 - Epoch: [158][   20/   28]    Loss 0.253194    Top1 91.113281    
2023-01-06 14:47:11,738 - Epoch: [158][   28/   28]    Loss 0.253324    Top1 91.125107    
2023-01-06 14:47:11,891 - ==> Top1: 91.125    Loss: 0.253

2023-01-06 14:47:11,891 - ==> Confusion:
[[ 269   10  160]
 [  15  291  296]
 [  77   62 5806]]

2023-01-06 14:47:11,893 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 158]
2023-01-06 14:47:11,893 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:11,914 - 

2023-01-06 14:47:11,914 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:12,663 - Epoch: [159][   10/  246]    Overall Loss 0.223721    Objective Loss 0.223721                                        LR 0.000008    Time 0.074836    
2023-01-06 14:47:12,926 - Epoch: [159][   20/  246]    Overall Loss 0.219306    Objective Loss 0.219306                                        LR 0.000008    Time 0.050528    
2023-01-06 14:47:13,189 - Epoch: [159][   30/  246]    Overall Loss 0.220516    Objective Loss 0.220516                                        LR 0.000008    Time 0.042447    
2023-01-06 14:47:13,451 - Epoch: [159][   40/  246]    Overall Loss 0.218941    Objective Loss 0.218941                                        LR 0.000008    Time 0.038374    
2023-01-06 14:47:13,714 - Epoch: [159][   50/  246]    Overall Loss 0.220166    Objective Loss 0.220166                                        LR 0.000008    Time 0.035948    
2023-01-06 14:47:13,976 - Epoch: [159][   60/  246]    Overall Loss 0.219394    Objective Loss 0.219394                                        LR 0.000008    Time 0.034326    
2023-01-06 14:47:14,242 - Epoch: [159][   70/  246]    Overall Loss 0.221035    Objective Loss 0.221035                                        LR 0.000008    Time 0.033225    
2023-01-06 14:47:14,505 - Epoch: [159][   80/  246]    Overall Loss 0.220440    Objective Loss 0.220440                                        LR 0.000008    Time 0.032349    
2023-01-06 14:47:14,766 - Epoch: [159][   90/  246]    Overall Loss 0.220418    Objective Loss 0.220418                                        LR 0.000008    Time 0.031657    
2023-01-06 14:47:15,027 - Epoch: [159][  100/  246]    Overall Loss 0.218466    Objective Loss 0.218466                                        LR 0.000008    Time 0.031099    
2023-01-06 14:47:15,288 - Epoch: [159][  110/  246]    Overall Loss 0.219386    Objective Loss 0.219386                                        LR 0.000008    Time 0.030638    
2023-01-06 14:47:15,550 - Epoch: [159][  120/  246]    Overall Loss 0.218781    Objective Loss 0.218781                                        LR 0.000008    Time 0.030263    
2023-01-06 14:47:15,810 - Epoch: [159][  130/  246]    Overall Loss 0.220858    Objective Loss 0.220858                                        LR 0.000008    Time 0.029936    
2023-01-06 14:47:16,070 - Epoch: [159][  140/  246]    Overall Loss 0.220648    Objective Loss 0.220648                                        LR 0.000008    Time 0.029652    
2023-01-06 14:47:16,331 - Epoch: [159][  150/  246]    Overall Loss 0.220593    Objective Loss 0.220593                                        LR 0.000008    Time 0.029415    
2023-01-06 14:47:16,591 - Epoch: [159][  160/  246]    Overall Loss 0.221948    Objective Loss 0.221948                                        LR 0.000008    Time 0.029198    
2023-01-06 14:47:16,835 - Epoch: [159][  170/  246]    Overall Loss 0.222138    Objective Loss 0.222138                                        LR 0.000008    Time 0.028915    
2023-01-06 14:47:17,074 - Epoch: [159][  180/  246]    Overall Loss 0.222504    Objective Loss 0.222504                                        LR 0.000008    Time 0.028632    
2023-01-06 14:47:17,314 - Epoch: [159][  190/  246]    Overall Loss 0.221808    Objective Loss 0.221808                                        LR 0.000008    Time 0.028389    
2023-01-06 14:47:17,553 - Epoch: [159][  200/  246]    Overall Loss 0.221959    Objective Loss 0.221959                                        LR 0.000008    Time 0.028164    
2023-01-06 14:47:17,793 - Epoch: [159][  210/  246]    Overall Loss 0.222738    Objective Loss 0.222738                                        LR 0.000008    Time 0.027966    
2023-01-06 14:47:18,036 - Epoch: [159][  220/  246]    Overall Loss 0.221928    Objective Loss 0.221928                                        LR 0.000008    Time 0.027795    
2023-01-06 14:47:18,277 - Epoch: [159][  230/  246]    Overall Loss 0.222104    Objective Loss 0.222104                                        LR 0.000008    Time 0.027631    
2023-01-06 14:47:18,529 - Epoch: [159][  240/  246]    Overall Loss 0.222545    Objective Loss 0.222545                                        LR 0.000008    Time 0.027530    
2023-01-06 14:47:18,657 - Epoch: [159][  246/  246]    Overall Loss 0.222122    Objective Loss 0.222122    Top1 94.736842    LR 0.000008    Time 0.027380    
2023-01-06 14:47:18,798 - --- validate (epoch=159)-----------
2023-01-06 14:47:18,798 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:19,263 - Epoch: [159][   10/   28]    Loss 0.259364    Top1 90.625000    
2023-01-06 14:47:19,425 - Epoch: [159][   20/   28]    Loss 0.256737    Top1 90.996094    
2023-01-06 14:47:19,515 - Epoch: [159][   28/   28]    Loss 0.250159    Top1 91.067850    
2023-01-06 14:47:19,653 - ==> Top1: 91.068    Loss: 0.250

2023-01-06 14:47:19,653 - ==> Confusion:
[[ 238   20  181]
 [   6  293  303]
 [  42   72 5831]]

2023-01-06 14:47:19,655 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 158]
2023-01-06 14:47:19,655 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:19,665 - 

2023-01-06 14:47:19,665 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:20,418 - Epoch: [160][   10/  246]    Overall Loss 0.233012    Objective Loss 0.233012                                        LR 0.000008    Time 0.075224    
2023-01-06 14:47:20,666 - Epoch: [160][   20/  246]    Overall Loss 0.227269    Objective Loss 0.227269                                        LR 0.000008    Time 0.050020    
2023-01-06 14:47:20,914 - Epoch: [160][   30/  246]    Overall Loss 0.223987    Objective Loss 0.223987                                        LR 0.000008    Time 0.041579    
2023-01-06 14:47:21,161 - Epoch: [160][   40/  246]    Overall Loss 0.222019    Objective Loss 0.222019                                        LR 0.000008    Time 0.037347    
2023-01-06 14:47:21,405 - Epoch: [160][   50/  246]    Overall Loss 0.220628    Objective Loss 0.220628                                        LR 0.000008    Time 0.034756    
2023-01-06 14:47:21,649 - Epoch: [160][   60/  246]    Overall Loss 0.220949    Objective Loss 0.220949                                        LR 0.000008    Time 0.033027    
2023-01-06 14:47:21,889 - Epoch: [160][   70/  246]    Overall Loss 0.223102    Objective Loss 0.223102                                        LR 0.000008    Time 0.031721    
2023-01-06 14:47:22,128 - Epoch: [160][   80/  246]    Overall Loss 0.224572    Objective Loss 0.224572                                        LR 0.000008    Time 0.030742    
2023-01-06 14:47:22,369 - Epoch: [160][   90/  246]    Overall Loss 0.223449    Objective Loss 0.223449                                        LR 0.000008    Time 0.029996    
2023-01-06 14:47:22,613 - Epoch: [160][  100/  246]    Overall Loss 0.223355    Objective Loss 0.223355                                        LR 0.000008    Time 0.029434    
2023-01-06 14:47:22,844 - Epoch: [160][  110/  246]    Overall Loss 0.222779    Objective Loss 0.222779                                        LR 0.000008    Time 0.028859    
2023-01-06 14:47:23,088 - Epoch: [160][  120/  246]    Overall Loss 0.222342    Objective Loss 0.222342                                        LR 0.000008    Time 0.028482    
2023-01-06 14:47:23,337 - Epoch: [160][  130/  246]    Overall Loss 0.222813    Objective Loss 0.222813                                        LR 0.000008    Time 0.028192    
2023-01-06 14:47:23,584 - Epoch: [160][  140/  246]    Overall Loss 0.222769    Objective Loss 0.222769                                        LR 0.000008    Time 0.027936    
2023-01-06 14:47:23,829 - Epoch: [160][  150/  246]    Overall Loss 0.222525    Objective Loss 0.222525                                        LR 0.000008    Time 0.027694    
2023-01-06 14:47:24,075 - Epoch: [160][  160/  246]    Overall Loss 0.223495    Objective Loss 0.223495                                        LR 0.000008    Time 0.027496    
2023-01-06 14:47:24,320 - Epoch: [160][  170/  246]    Overall Loss 0.222815    Objective Loss 0.222815                                        LR 0.000008    Time 0.027310    
2023-01-06 14:47:24,567 - Epoch: [160][  180/  246]    Overall Loss 0.222651    Objective Loss 0.222651                                        LR 0.000008    Time 0.027160    
2023-01-06 14:47:24,815 - Epoch: [160][  190/  246]    Overall Loss 0.222681    Objective Loss 0.222681                                        LR 0.000008    Time 0.027023    
2023-01-06 14:47:25,061 - Epoch: [160][  200/  246]    Overall Loss 0.222184    Objective Loss 0.222184                                        LR 0.000008    Time 0.026904    
2023-01-06 14:47:25,309 - Epoch: [160][  210/  246]    Overall Loss 0.222349    Objective Loss 0.222349                                        LR 0.000008    Time 0.026791    
2023-01-06 14:47:25,556 - Epoch: [160][  220/  246]    Overall Loss 0.221934    Objective Loss 0.221934                                        LR 0.000008    Time 0.026694    
2023-01-06 14:47:25,814 - Epoch: [160][  230/  246]    Overall Loss 0.221526    Objective Loss 0.221526                                        LR 0.000008    Time 0.026647    
2023-01-06 14:47:26,074 - Epoch: [160][  240/  246]    Overall Loss 0.221470    Objective Loss 0.221470                                        LR 0.000008    Time 0.026617    
2023-01-06 14:47:26,203 - Epoch: [160][  246/  246]    Overall Loss 0.221933    Objective Loss 0.221933    Top1 91.387560    LR 0.000008    Time 0.026495    
2023-01-06 14:47:26,332 - --- validate (epoch=160)-----------
2023-01-06 14:47:26,332 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:26,785 - Epoch: [160][   10/   28]    Loss 0.268478    Top1 90.234375    
2023-01-06 14:47:26,925 - Epoch: [160][   20/   28]    Loss 0.254350    Top1 90.546875    
2023-01-06 14:47:27,015 - Epoch: [160][   28/   28]    Loss 0.256582    Top1 90.624105    
2023-01-06 14:47:27,142 - ==> Top1: 90.624    Loss: 0.257

2023-01-06 14:47:27,142 - ==> Confusion:
[[ 247   13  179]
 [  18  286  298]
 [  75   72 5798]]

2023-01-06 14:47:27,144 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 158]
2023-01-06 14:47:27,144 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:27,161 - 

2023-01-06 14:47:27,161 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:27,767 - Epoch: [161][   10/  246]    Overall Loss 0.227440    Objective Loss 0.227440                                        LR 0.000008    Time 0.060537    
2023-01-06 14:47:28,011 - Epoch: [161][   20/  246]    Overall Loss 0.212369    Objective Loss 0.212369                                        LR 0.000008    Time 0.042441    
2023-01-06 14:47:28,257 - Epoch: [161][   30/  246]    Overall Loss 0.218153    Objective Loss 0.218153                                        LR 0.000008    Time 0.036466    
2023-01-06 14:47:28,503 - Epoch: [161][   40/  246]    Overall Loss 0.221051    Objective Loss 0.221051                                        LR 0.000008    Time 0.033493    
2023-01-06 14:47:28,746 - Epoch: [161][   50/  246]    Overall Loss 0.222303    Objective Loss 0.222303                                        LR 0.000008    Time 0.031641    
2023-01-06 14:47:28,991 - Epoch: [161][   60/  246]    Overall Loss 0.222668    Objective Loss 0.222668                                        LR 0.000008    Time 0.030454    
2023-01-06 14:47:29,238 - Epoch: [161][   70/  246]    Overall Loss 0.224784    Objective Loss 0.224784                                        LR 0.000008    Time 0.029620    
2023-01-06 14:47:29,482 - Epoch: [161][   80/  246]    Overall Loss 0.223981    Objective Loss 0.223981                                        LR 0.000008    Time 0.028968    
2023-01-06 14:47:29,725 - Epoch: [161][   90/  246]    Overall Loss 0.225570    Objective Loss 0.225570                                        LR 0.000008    Time 0.028446    
2023-01-06 14:47:29,969 - Epoch: [161][  100/  246]    Overall Loss 0.223867    Objective Loss 0.223867                                        LR 0.000008    Time 0.028031    
2023-01-06 14:47:30,209 - Epoch: [161][  110/  246]    Overall Loss 0.225342    Objective Loss 0.225342                                        LR 0.000008    Time 0.027662    
2023-01-06 14:47:30,451 - Epoch: [161][  120/  246]    Overall Loss 0.224864    Objective Loss 0.224864                                        LR 0.000008    Time 0.027373    
2023-01-06 14:47:30,693 - Epoch: [161][  130/  246]    Overall Loss 0.225896    Objective Loss 0.225896                                        LR 0.000008    Time 0.027130    
2023-01-06 14:47:30,936 - Epoch: [161][  140/  246]    Overall Loss 0.225176    Objective Loss 0.225176                                        LR 0.000008    Time 0.026921    
2023-01-06 14:47:31,180 - Epoch: [161][  150/  246]    Overall Loss 0.225665    Objective Loss 0.225665                                        LR 0.000008    Time 0.026754    
2023-01-06 14:47:31,422 - Epoch: [161][  160/  246]    Overall Loss 0.225594    Objective Loss 0.225594                                        LR 0.000008    Time 0.026592    
2023-01-06 14:47:31,665 - Epoch: [161][  170/  246]    Overall Loss 0.224928    Objective Loss 0.224928                                        LR 0.000008    Time 0.026460    
2023-01-06 14:47:31,916 - Epoch: [161][  180/  246]    Overall Loss 0.224394    Objective Loss 0.224394                                        LR 0.000008    Time 0.026383    
2023-01-06 14:47:32,159 - Epoch: [161][  190/  246]    Overall Loss 0.224166    Objective Loss 0.224166                                        LR 0.000008    Time 0.026269    
2023-01-06 14:47:32,401 - Epoch: [161][  200/  246]    Overall Loss 0.223984    Objective Loss 0.223984                                        LR 0.000008    Time 0.026165    
2023-01-06 14:47:32,643 - Epoch: [161][  210/  246]    Overall Loss 0.222982    Objective Loss 0.222982                                        LR 0.000008    Time 0.026071    
2023-01-06 14:47:32,887 - Epoch: [161][  220/  246]    Overall Loss 0.223254    Objective Loss 0.223254                                        LR 0.000008    Time 0.025990    
2023-01-06 14:47:33,128 - Epoch: [161][  230/  246]    Overall Loss 0.223101    Objective Loss 0.223101                                        LR 0.000008    Time 0.025910    
2023-01-06 14:47:33,390 - Epoch: [161][  240/  246]    Overall Loss 0.222631    Objective Loss 0.222631                                        LR 0.000008    Time 0.025912    
2023-01-06 14:47:33,522 - Epoch: [161][  246/  246]    Overall Loss 0.222465    Objective Loss 0.222465    Top1 91.626794    LR 0.000008    Time 0.025816    
2023-01-06 14:47:33,660 - --- validate (epoch=161)-----------
2023-01-06 14:47:33,661 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:34,104 - Epoch: [161][   10/   28]    Loss 0.271944    Top1 90.664062    
2023-01-06 14:47:34,249 - Epoch: [161][   20/   28]    Loss 0.260581    Top1 90.996094    
2023-01-06 14:47:34,340 - Epoch: [161][   28/   28]    Loss 0.254930    Top1 90.967650    
2023-01-06 14:47:34,493 - ==> Top1: 90.968    Loss: 0.255

2023-01-06 14:47:34,493 - ==> Confusion:
[[ 256   13  170]
 [  13  308  281]
 [  69   85 5791]]

2023-01-06 14:47:34,495 - ==> Best [Top1: 91.125   Sparsity:0.00   Params: 360896 on epoch: 158]
2023-01-06 14:47:34,495 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:34,504 - 

2023-01-06 14:47:34,505 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:35,226 - Epoch: [162][   10/  246]    Overall Loss 0.222898    Objective Loss 0.222898                                        LR 0.000008    Time 0.072063    
2023-01-06 14:47:35,445 - Epoch: [162][   20/  246]    Overall Loss 0.227803    Objective Loss 0.227803                                        LR 0.000008    Time 0.046963    
2023-01-06 14:47:35,664 - Epoch: [162][   30/  246]    Overall Loss 0.224507    Objective Loss 0.224507                                        LR 0.000008    Time 0.038593    
2023-01-06 14:47:35,886 - Epoch: [162][   40/  246]    Overall Loss 0.219691    Objective Loss 0.219691                                        LR 0.000008    Time 0.034474    
2023-01-06 14:47:36,104 - Epoch: [162][   50/  246]    Overall Loss 0.221271    Objective Loss 0.221271                                        LR 0.000008    Time 0.031940    
2023-01-06 14:47:36,321 - Epoch: [162][   60/  246]    Overall Loss 0.220149    Objective Loss 0.220149                                        LR 0.000008    Time 0.030229    
2023-01-06 14:47:36,577 - Epoch: [162][   70/  246]    Overall Loss 0.219475    Objective Loss 0.219475                                        LR 0.000008    Time 0.029567    
2023-01-06 14:47:36,833 - Epoch: [162][   80/  246]    Overall Loss 0.221781    Objective Loss 0.221781                                        LR 0.000008    Time 0.029037    
2023-01-06 14:47:37,088 - Epoch: [162][   90/  246]    Overall Loss 0.221131    Objective Loss 0.221131                                        LR 0.000008    Time 0.028630    
2023-01-06 14:47:37,344 - Epoch: [162][  100/  246]    Overall Loss 0.219836    Objective Loss 0.219836                                        LR 0.000008    Time 0.028299    
2023-01-06 14:47:37,597 - Epoch: [162][  110/  246]    Overall Loss 0.220599    Objective Loss 0.220599                                        LR 0.000008    Time 0.028028    
2023-01-06 14:47:37,855 - Epoch: [162][  120/  246]    Overall Loss 0.220654    Objective Loss 0.220654                                        LR 0.000008    Time 0.027824    
2023-01-06 14:47:38,100 - Epoch: [162][  130/  246]    Overall Loss 0.220814    Objective Loss 0.220814                                        LR 0.000008    Time 0.027565    
2023-01-06 14:47:38,311 - Epoch: [162][  140/  246]    Overall Loss 0.220941    Objective Loss 0.220941                                        LR 0.000008    Time 0.027102    
2023-01-06 14:47:38,558 - Epoch: [162][  150/  246]    Overall Loss 0.221159    Objective Loss 0.221159                                        LR 0.000008    Time 0.026935    
2023-01-06 14:47:38,811 - Epoch: [162][  160/  246]    Overall Loss 0.221875    Objective Loss 0.221875                                        LR 0.000008    Time 0.026835    
2023-01-06 14:47:39,062 - Epoch: [162][  170/  246]    Overall Loss 0.222082    Objective Loss 0.222082                                        LR 0.000008    Time 0.026727    
2023-01-06 14:47:39,317 - Epoch: [162][  180/  246]    Overall Loss 0.222710    Objective Loss 0.222710                                        LR 0.000008    Time 0.026658    
2023-01-06 14:47:39,566 - Epoch: [162][  190/  246]    Overall Loss 0.222350    Objective Loss 0.222350                                        LR 0.000008    Time 0.026563    
2023-01-06 14:47:39,820 - Epoch: [162][  200/  246]    Overall Loss 0.221943    Objective Loss 0.221943                                        LR 0.000008    Time 0.026505    
2023-01-06 14:47:40,068 - Epoch: [162][  210/  246]    Overall Loss 0.221780    Objective Loss 0.221780                                        LR 0.000008    Time 0.026420    
2023-01-06 14:47:40,321 - Epoch: [162][  220/  246]    Overall Loss 0.221828    Objective Loss 0.221828                                        LR 0.000008    Time 0.026363    
2023-01-06 14:47:40,567 - Epoch: [162][  230/  246]    Overall Loss 0.222005    Objective Loss 0.222005                                        LR 0.000008    Time 0.026287    
2023-01-06 14:47:40,825 - Epoch: [162][  240/  246]    Overall Loss 0.222618    Objective Loss 0.222618                                        LR 0.000008    Time 0.026258    
2023-01-06 14:47:40,951 - Epoch: [162][  246/  246]    Overall Loss 0.222846    Objective Loss 0.222846    Top1 90.909091    LR 0.000008    Time 0.026129    
2023-01-06 14:47:41,095 - --- validate (epoch=162)-----------
2023-01-06 14:47:41,095 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:41,551 - Epoch: [162][   10/   28]    Loss 0.253953    Top1 91.015625    
2023-01-06 14:47:41,700 - Epoch: [162][   20/   28]    Loss 0.258687    Top1 90.898438    
2023-01-06 14:47:41,793 - Epoch: [162][   28/   28]    Loss 0.251416    Top1 91.153736    
2023-01-06 14:47:41,924 - ==> Top1: 91.154    Loss: 0.251

2023-01-06 14:47:41,924 - ==> Confusion:
[[ 278   11  150]
 [  20  305  277]
 [  73   87 5785]]

2023-01-06 14:47:41,926 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:47:41,926 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:41,953 - 

2023-01-06 14:47:41,953 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:42,700 - Epoch: [163][   10/  246]    Overall Loss 0.203915    Objective Loss 0.203915                                        LR 0.000008    Time 0.074643    
2023-01-06 14:47:42,946 - Epoch: [163][   20/  246]    Overall Loss 0.214772    Objective Loss 0.214772                                        LR 0.000008    Time 0.049614    
2023-01-06 14:47:43,176 - Epoch: [163][   30/  246]    Overall Loss 0.217039    Objective Loss 0.217039                                        LR 0.000008    Time 0.040717    
2023-01-06 14:47:43,397 - Epoch: [163][   40/  246]    Overall Loss 0.223933    Objective Loss 0.223933                                        LR 0.000008    Time 0.036060    
2023-01-06 14:47:43,615 - Epoch: [163][   50/  246]    Overall Loss 0.218886    Objective Loss 0.218886                                        LR 0.000008    Time 0.033189    
2023-01-06 14:47:43,827 - Epoch: [163][   60/  246]    Overall Loss 0.216817    Objective Loss 0.216817                                        LR 0.000008    Time 0.031197    
2023-01-06 14:47:44,057 - Epoch: [163][   70/  246]    Overall Loss 0.216207    Objective Loss 0.216207                                        LR 0.000008    Time 0.030016    
2023-01-06 14:47:44,273 - Epoch: [163][   80/  246]    Overall Loss 0.216825    Objective Loss 0.216825                                        LR 0.000008    Time 0.028956    
2023-01-06 14:47:44,497 - Epoch: [163][   90/  246]    Overall Loss 0.216919    Objective Loss 0.216919                                        LR 0.000008    Time 0.028228    
2023-01-06 14:47:44,718 - Epoch: [163][  100/  246]    Overall Loss 0.215671    Objective Loss 0.215671                                        LR 0.000008    Time 0.027616    
2023-01-06 14:47:44,945 - Epoch: [163][  110/  246]    Overall Loss 0.217363    Objective Loss 0.217363                                        LR 0.000008    Time 0.027165    
2023-01-06 14:47:45,182 - Epoch: [163][  120/  246]    Overall Loss 0.218033    Objective Loss 0.218033                                        LR 0.000008    Time 0.026875    
2023-01-06 14:47:45,404 - Epoch: [163][  130/  246]    Overall Loss 0.217392    Objective Loss 0.217392                                        LR 0.000008    Time 0.026512    
2023-01-06 14:47:45,644 - Epoch: [163][  140/  246]    Overall Loss 0.218231    Objective Loss 0.218231                                        LR 0.000008    Time 0.026329    
2023-01-06 14:47:45,872 - Epoch: [163][  150/  246]    Overall Loss 0.219623    Objective Loss 0.219623                                        LR 0.000008    Time 0.026088    
2023-01-06 14:47:46,104 - Epoch: [163][  160/  246]    Overall Loss 0.220730    Objective Loss 0.220730                                        LR 0.000008    Time 0.025904    
2023-01-06 14:47:46,334 - Epoch: [163][  170/  246]    Overall Loss 0.221539    Objective Loss 0.221539                                        LR 0.000008    Time 0.025732    
2023-01-06 14:47:46,580 - Epoch: [163][  180/  246]    Overall Loss 0.221370    Objective Loss 0.221370                                        LR 0.000008    Time 0.025668    
2023-01-06 14:47:46,822 - Epoch: [163][  190/  246]    Overall Loss 0.222326    Objective Loss 0.222326                                        LR 0.000008    Time 0.025588    
2023-01-06 14:47:47,065 - Epoch: [163][  200/  246]    Overall Loss 0.222342    Objective Loss 0.222342                                        LR 0.000008    Time 0.025522    
2023-01-06 14:47:47,305 - Epoch: [163][  210/  246]    Overall Loss 0.223359    Objective Loss 0.223359                                        LR 0.000008    Time 0.025447    
2023-01-06 14:47:47,549 - Epoch: [163][  220/  246]    Overall Loss 0.223568    Objective Loss 0.223568                                        LR 0.000008    Time 0.025401    
2023-01-06 14:47:47,789 - Epoch: [163][  230/  246]    Overall Loss 0.223409    Objective Loss 0.223409                                        LR 0.000008    Time 0.025336    
2023-01-06 14:47:48,049 - Epoch: [163][  240/  246]    Overall Loss 0.223297    Objective Loss 0.223297                                        LR 0.000008    Time 0.025362    
2023-01-06 14:47:48,179 - Epoch: [163][  246/  246]    Overall Loss 0.222788    Objective Loss 0.222788    Top1 93.301435    LR 0.000008    Time 0.025270    
2023-01-06 14:47:48,299 - --- validate (epoch=163)-----------
2023-01-06 14:47:48,299 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:48,751 - Epoch: [163][   10/   28]    Loss 0.273895    Top1 90.781250    
2023-01-06 14:47:48,896 - Epoch: [163][   20/   28]    Loss 0.260986    Top1 91.113281    
2023-01-06 14:47:48,987 - Epoch: [163][   28/   28]    Loss 0.256117    Top1 91.067850    
2023-01-06 14:47:49,101 - ==> Top1: 91.068    Loss: 0.256

2023-01-06 14:47:49,101 - ==> Confusion:
[[ 246   20  173]
 [  12  310  280]
 [  58   81 5806]]

2023-01-06 14:47:49,102 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:47:49,103 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:49,112 - 

2023-01-06 14:47:49,113 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:49,706 - Epoch: [164][   10/  246]    Overall Loss 0.225717    Objective Loss 0.225717                                        LR 0.000008    Time 0.059278    
2023-01-06 14:47:49,938 - Epoch: [164][   20/  246]    Overall Loss 0.219775    Objective Loss 0.219775                                        LR 0.000008    Time 0.041220    
2023-01-06 14:47:50,179 - Epoch: [164][   30/  246]    Overall Loss 0.220736    Objective Loss 0.220736                                        LR 0.000008    Time 0.035477    
2023-01-06 14:47:50,436 - Epoch: [164][   40/  246]    Overall Loss 0.227683    Objective Loss 0.227683                                        LR 0.000008    Time 0.033035    
2023-01-06 14:47:50,689 - Epoch: [164][   50/  246]    Overall Loss 0.228049    Objective Loss 0.228049                                        LR 0.000008    Time 0.031488    
2023-01-06 14:47:50,943 - Epoch: [164][   60/  246]    Overall Loss 0.226659    Objective Loss 0.226659                                        LR 0.000008    Time 0.030464    
2023-01-06 14:47:51,199 - Epoch: [164][   70/  246]    Overall Loss 0.225147    Objective Loss 0.225147                                        LR 0.000008    Time 0.029759    
2023-01-06 14:47:51,453 - Epoch: [164][   80/  246]    Overall Loss 0.222945    Objective Loss 0.222945                                        LR 0.000008    Time 0.029209    
2023-01-06 14:47:51,704 - Epoch: [164][   90/  246]    Overall Loss 0.221342    Objective Loss 0.221342                                        LR 0.000008    Time 0.028748    
2023-01-06 14:47:51,959 - Epoch: [164][  100/  246]    Overall Loss 0.221990    Objective Loss 0.221990                                        LR 0.000008    Time 0.028419    
2023-01-06 14:47:52,204 - Epoch: [164][  110/  246]    Overall Loss 0.221890    Objective Loss 0.221890                                        LR 0.000008    Time 0.028062    
2023-01-06 14:47:52,448 - Epoch: [164][  120/  246]    Overall Loss 0.220259    Objective Loss 0.220259                                        LR 0.000008    Time 0.027749    
2023-01-06 14:47:52,694 - Epoch: [164][  130/  246]    Overall Loss 0.220066    Objective Loss 0.220066                                        LR 0.000008    Time 0.027499    
2023-01-06 14:47:52,938 - Epoch: [164][  140/  246]    Overall Loss 0.221006    Objective Loss 0.221006                                        LR 0.000008    Time 0.027270    
2023-01-06 14:47:53,180 - Epoch: [164][  150/  246]    Overall Loss 0.220097    Objective Loss 0.220097                                        LR 0.000008    Time 0.027062    
2023-01-06 14:47:53,420 - Epoch: [164][  160/  246]    Overall Loss 0.220302    Objective Loss 0.220302                                        LR 0.000008    Time 0.026864    
2023-01-06 14:47:53,660 - Epoch: [164][  170/  246]    Overall Loss 0.220348    Objective Loss 0.220348                                        LR 0.000008    Time 0.026697    
2023-01-06 14:47:53,904 - Epoch: [164][  180/  246]    Overall Loss 0.221030    Objective Loss 0.221030                                        LR 0.000008    Time 0.026565    
2023-01-06 14:47:54,147 - Epoch: [164][  190/  246]    Overall Loss 0.221434    Objective Loss 0.221434                                        LR 0.000008    Time 0.026443    
2023-01-06 14:47:54,388 - Epoch: [164][  200/  246]    Overall Loss 0.222143    Objective Loss 0.222143                                        LR 0.000008    Time 0.026325    
2023-01-06 14:47:54,629 - Epoch: [164][  210/  246]    Overall Loss 0.222539    Objective Loss 0.222539                                        LR 0.000008    Time 0.026218    
2023-01-06 14:47:54,872 - Epoch: [164][  220/  246]    Overall Loss 0.222472    Objective Loss 0.222472                                        LR 0.000008    Time 0.026129    
2023-01-06 14:47:55,113 - Epoch: [164][  230/  246]    Overall Loss 0.222281    Objective Loss 0.222281                                        LR 0.000008    Time 0.026039    
2023-01-06 14:47:55,367 - Epoch: [164][  240/  246]    Overall Loss 0.221750    Objective Loss 0.221750                                        LR 0.000008    Time 0.026014    
2023-01-06 14:47:55,501 - Epoch: [164][  246/  246]    Overall Loss 0.221638    Objective Loss 0.221638    Top1 91.866029    LR 0.000008    Time 0.025921    
2023-01-06 14:47:55,629 - --- validate (epoch=164)-----------
2023-01-06 14:47:55,630 - 6986 samples (256 per mini-batch)
2023-01-06 14:47:56,087 - Epoch: [164][   10/   28]    Loss 0.250456    Top1 90.898438    
2023-01-06 14:47:56,227 - Epoch: [164][   20/   28]    Loss 0.248076    Top1 90.937500    
2023-01-06 14:47:56,321 - Epoch: [164][   28/   28]    Loss 0.248557    Top1 90.981964    
2023-01-06 14:47:56,462 - ==> Top1: 90.982    Loss: 0.249

2023-01-06 14:47:56,462 - ==> Confusion:
[[ 259   24  156]
 [  11  318  273]
 [  69   97 5779]]

2023-01-06 14:47:56,464 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:47:56,464 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:47:56,480 - 

2023-01-06 14:47:56,480 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:47:57,225 - Epoch: [165][   10/  246]    Overall Loss 0.212215    Objective Loss 0.212215                                        LR 0.000008    Time 0.074390    
2023-01-06 14:47:57,457 - Epoch: [165][   20/  246]    Overall Loss 0.214573    Objective Loss 0.214573                                        LR 0.000008    Time 0.048761    
2023-01-06 14:47:57,707 - Epoch: [165][   30/  246]    Overall Loss 0.214214    Objective Loss 0.214214                                        LR 0.000008    Time 0.040834    
2023-01-06 14:47:57,948 - Epoch: [165][   40/  246]    Overall Loss 0.214133    Objective Loss 0.214133                                        LR 0.000008    Time 0.036648    
2023-01-06 14:47:58,197 - Epoch: [165][   50/  246]    Overall Loss 0.214815    Objective Loss 0.214815                                        LR 0.000008    Time 0.034283    
2023-01-06 14:47:58,436 - Epoch: [165][   60/  246]    Overall Loss 0.213335    Objective Loss 0.213335                                        LR 0.000008    Time 0.032554    
2023-01-06 14:47:58,684 - Epoch: [165][   70/  246]    Overall Loss 0.213015    Objective Loss 0.213015                                        LR 0.000008    Time 0.031431    
2023-01-06 14:47:58,920 - Epoch: [165][   80/  246]    Overall Loss 0.213759    Objective Loss 0.213759                                        LR 0.000008    Time 0.030450    
2023-01-06 14:47:59,168 - Epoch: [165][   90/  246]    Overall Loss 0.213916    Objective Loss 0.213916                                        LR 0.000008    Time 0.029812    
2023-01-06 14:47:59,406 - Epoch: [165][  100/  246]    Overall Loss 0.214575    Objective Loss 0.214575                                        LR 0.000008    Time 0.029191    
2023-01-06 14:47:59,647 - Epoch: [165][  110/  246]    Overall Loss 0.216342    Objective Loss 0.216342                                        LR 0.000008    Time 0.028715    
2023-01-06 14:47:59,892 - Epoch: [165][  120/  246]    Overall Loss 0.217725    Objective Loss 0.217725                                        LR 0.000008    Time 0.028355    
2023-01-06 14:48:00,130 - Epoch: [165][  130/  246]    Overall Loss 0.218485    Objective Loss 0.218485                                        LR 0.000008    Time 0.028003    
2023-01-06 14:48:00,365 - Epoch: [165][  140/  246]    Overall Loss 0.219754    Objective Loss 0.219754                                        LR 0.000008    Time 0.027683    
2023-01-06 14:48:00,605 - Epoch: [165][  150/  246]    Overall Loss 0.220574    Objective Loss 0.220574                                        LR 0.000008    Time 0.027431    
2023-01-06 14:48:00,846 - Epoch: [165][  160/  246]    Overall Loss 0.220640    Objective Loss 0.220640                                        LR 0.000008    Time 0.027211    
2023-01-06 14:48:01,089 - Epoch: [165][  170/  246]    Overall Loss 0.220844    Objective Loss 0.220844                                        LR 0.000008    Time 0.027035    
2023-01-06 14:48:01,331 - Epoch: [165][  180/  246]    Overall Loss 0.219621    Objective Loss 0.219621                                        LR 0.000008    Time 0.026870    
2023-01-06 14:48:01,565 - Epoch: [165][  190/  246]    Overall Loss 0.218872    Objective Loss 0.218872                                        LR 0.000008    Time 0.026686    
2023-01-06 14:48:01,799 - Epoch: [165][  200/  246]    Overall Loss 0.219298    Objective Loss 0.219298                                        LR 0.000008    Time 0.026518    
2023-01-06 14:48:02,028 - Epoch: [165][  210/  246]    Overall Loss 0.220549    Objective Loss 0.220549                                        LR 0.000008    Time 0.026344    
2023-01-06 14:48:02,253 - Epoch: [165][  220/  246]    Overall Loss 0.220471    Objective Loss 0.220471                                        LR 0.000008    Time 0.026166    
2023-01-06 14:48:02,488 - Epoch: [165][  230/  246]    Overall Loss 0.221413    Objective Loss 0.221413                                        LR 0.000008    Time 0.026048    
2023-01-06 14:48:02,732 - Epoch: [165][  240/  246]    Overall Loss 0.222151    Objective Loss 0.222151                                        LR 0.000008    Time 0.025978    
2023-01-06 14:48:02,854 - Epoch: [165][  246/  246]    Overall Loss 0.222530    Objective Loss 0.222530    Top1 91.387560    LR 0.000008    Time 0.025841    
2023-01-06 14:48:03,030 - --- validate (epoch=165)-----------
2023-01-06 14:48:03,030 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:03,484 - Epoch: [165][   10/   28]    Loss 0.265194    Top1 90.429688    
2023-01-06 14:48:03,628 - Epoch: [165][   20/   28]    Loss 0.255056    Top1 90.859375    
2023-01-06 14:48:03,721 - Epoch: [165][   28/   28]    Loss 0.250672    Top1 91.067850    
2023-01-06 14:48:03,850 - ==> Top1: 91.068    Loss: 0.251

2023-01-06 14:48:03,850 - ==> Confusion:
[[ 267   14  158]
 [  16  300  286]
 [  72   78 5795]]

2023-01-06 14:48:03,852 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:03,852 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:03,861 - 

2023-01-06 14:48:03,862 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:04,460 - Epoch: [166][   10/  246]    Overall Loss 0.234645    Objective Loss 0.234645                                        LR 0.000008    Time 0.059724    
2023-01-06 14:48:04,705 - Epoch: [166][   20/  246]    Overall Loss 0.230947    Objective Loss 0.230947                                        LR 0.000008    Time 0.042089    
2023-01-06 14:48:04,961 - Epoch: [166][   30/  246]    Overall Loss 0.231019    Objective Loss 0.231019                                        LR 0.000008    Time 0.036547    
2023-01-06 14:48:05,232 - Epoch: [166][   40/  246]    Overall Loss 0.231086    Objective Loss 0.231086                                        LR 0.000008    Time 0.034164    
2023-01-06 14:48:05,502 - Epoch: [166][   50/  246]    Overall Loss 0.229205    Objective Loss 0.229205                                        LR 0.000008    Time 0.032729    
2023-01-06 14:48:05,770 - Epoch: [166][   60/  246]    Overall Loss 0.227712    Objective Loss 0.227712                                        LR 0.000008    Time 0.031737    
2023-01-06 14:48:06,035 - Epoch: [166][   70/  246]    Overall Loss 0.222652    Objective Loss 0.222652                                        LR 0.000008    Time 0.030981    
2023-01-06 14:48:06,295 - Epoch: [166][   80/  246]    Overall Loss 0.221051    Objective Loss 0.221051                                        LR 0.000008    Time 0.030346    
2023-01-06 14:48:06,556 - Epoch: [166][   90/  246]    Overall Loss 0.222148    Objective Loss 0.222148                                        LR 0.000008    Time 0.029875    
2023-01-06 14:48:06,817 - Epoch: [166][  100/  246]    Overall Loss 0.222703    Objective Loss 0.222703                                        LR 0.000008    Time 0.029497    
2023-01-06 14:48:07,079 - Epoch: [166][  110/  246]    Overall Loss 0.222440    Objective Loss 0.222440                                        LR 0.000008    Time 0.029194    
2023-01-06 14:48:07,343 - Epoch: [166][  120/  246]    Overall Loss 0.221623    Objective Loss 0.221623                                        LR 0.000008    Time 0.028960    
2023-01-06 14:48:07,611 - Epoch: [166][  130/  246]    Overall Loss 0.221812    Objective Loss 0.221812                                        LR 0.000008    Time 0.028787    
2023-01-06 14:48:07,872 - Epoch: [166][  140/  246]    Overall Loss 0.221207    Objective Loss 0.221207                                        LR 0.000008    Time 0.028595    
2023-01-06 14:48:08,130 - Epoch: [166][  150/  246]    Overall Loss 0.220116    Objective Loss 0.220116                                        LR 0.000008    Time 0.028394    
2023-01-06 14:48:08,395 - Epoch: [166][  160/  246]    Overall Loss 0.219236    Objective Loss 0.219236                                        LR 0.000008    Time 0.028270    
2023-01-06 14:48:08,661 - Epoch: [166][  170/  246]    Overall Loss 0.219860    Objective Loss 0.219860                                        LR 0.000008    Time 0.028169    
2023-01-06 14:48:08,923 - Epoch: [166][  180/  246]    Overall Loss 0.220667    Objective Loss 0.220667                                        LR 0.000008    Time 0.028062    
2023-01-06 14:48:09,177 - Epoch: [166][  190/  246]    Overall Loss 0.219624    Objective Loss 0.219624                                        LR 0.000008    Time 0.027918    
2023-01-06 14:48:09,432 - Epoch: [166][  200/  246]    Overall Loss 0.220205    Objective Loss 0.220205                                        LR 0.000008    Time 0.027795    
2023-01-06 14:48:09,699 - Epoch: [166][  210/  246]    Overall Loss 0.220766    Objective Loss 0.220766                                        LR 0.000008    Time 0.027736    
2023-01-06 14:48:09,964 - Epoch: [166][  220/  246]    Overall Loss 0.220909    Objective Loss 0.220909                                        LR 0.000008    Time 0.027679    
2023-01-06 14:48:10,234 - Epoch: [166][  230/  246]    Overall Loss 0.221017    Objective Loss 0.221017                                        LR 0.000008    Time 0.027642    
2023-01-06 14:48:10,506 - Epoch: [166][  240/  246]    Overall Loss 0.221307    Objective Loss 0.221307                                        LR 0.000008    Time 0.027621    
2023-01-06 14:48:10,639 - Epoch: [166][  246/  246]    Overall Loss 0.221410    Objective Loss 0.221410    Top1 91.387560    LR 0.000008    Time 0.027488    
2023-01-06 14:48:10,789 - --- validate (epoch=166)-----------
2023-01-06 14:48:10,789 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:11,251 - Epoch: [166][   10/   28]    Loss 0.241071    Top1 90.664062    
2023-01-06 14:48:11,397 - Epoch: [166][   20/   28]    Loss 0.252988    Top1 90.761719    
2023-01-06 14:48:11,488 - Epoch: [166][   28/   28]    Loss 0.248214    Top1 91.010593    
2023-01-06 14:48:11,645 - ==> Top1: 91.011    Loss: 0.248

2023-01-06 14:48:11,645 - ==> Confusion:
[[ 278   15  146]
 [  15  306  281]
 [  77   94 5774]]

2023-01-06 14:48:11,647 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:11,647 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:11,664 - 

2023-01-06 14:48:11,664 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:12,370 - Epoch: [167][   10/  246]    Overall Loss 0.213705    Objective Loss 0.213705                                        LR 0.000008    Time 0.070517    
2023-01-06 14:48:12,616 - Epoch: [167][   20/  246]    Overall Loss 0.209964    Objective Loss 0.209964                                        LR 0.000008    Time 0.047549    
2023-01-06 14:48:12,876 - Epoch: [167][   30/  246]    Overall Loss 0.216412    Objective Loss 0.216412                                        LR 0.000008    Time 0.040341    
2023-01-06 14:48:13,124 - Epoch: [167][   40/  246]    Overall Loss 0.219629    Objective Loss 0.219629                                        LR 0.000008    Time 0.036451    
2023-01-06 14:48:13,374 - Epoch: [167][   50/  246]    Overall Loss 0.221002    Objective Loss 0.221002                                        LR 0.000008    Time 0.034156    
2023-01-06 14:48:13,631 - Epoch: [167][   60/  246]    Overall Loss 0.221291    Objective Loss 0.221291                                        LR 0.000008    Time 0.032734    
2023-01-06 14:48:13,886 - Epoch: [167][   70/  246]    Overall Loss 0.219294    Objective Loss 0.219294                                        LR 0.000008    Time 0.031695    
2023-01-06 14:48:14,147 - Epoch: [167][   80/  246]    Overall Loss 0.221093    Objective Loss 0.221093                                        LR 0.000008    Time 0.030984    
2023-01-06 14:48:14,404 - Epoch: [167][   90/  246]    Overall Loss 0.219968    Objective Loss 0.219968                                        LR 0.000008    Time 0.030392    
2023-01-06 14:48:14,663 - Epoch: [167][  100/  246]    Overall Loss 0.217138    Objective Loss 0.217138                                        LR 0.000008    Time 0.029939    
2023-01-06 14:48:14,919 - Epoch: [167][  110/  246]    Overall Loss 0.217255    Objective Loss 0.217255                                        LR 0.000008    Time 0.029542    
2023-01-06 14:48:15,175 - Epoch: [167][  120/  246]    Overall Loss 0.217128    Objective Loss 0.217128                                        LR 0.000008    Time 0.029208    
2023-01-06 14:48:15,434 - Epoch: [167][  130/  246]    Overall Loss 0.218491    Objective Loss 0.218491                                        LR 0.000008    Time 0.028949    
2023-01-06 14:48:15,692 - Epoch: [167][  140/  246]    Overall Loss 0.219277    Objective Loss 0.219277                                        LR 0.000008    Time 0.028718    
2023-01-06 14:48:15,951 - Epoch: [167][  150/  246]    Overall Loss 0.219110    Objective Loss 0.219110                                        LR 0.000008    Time 0.028525    
2023-01-06 14:48:16,209 - Epoch: [167][  160/  246]    Overall Loss 0.218497    Objective Loss 0.218497                                        LR 0.000008    Time 0.028350    
2023-01-06 14:48:16,467 - Epoch: [167][  170/  246]    Overall Loss 0.218662    Objective Loss 0.218662                                        LR 0.000008    Time 0.028202    
2023-01-06 14:48:16,726 - Epoch: [167][  180/  246]    Overall Loss 0.218392    Objective Loss 0.218392                                        LR 0.000008    Time 0.028067    
2023-01-06 14:48:16,981 - Epoch: [167][  190/  246]    Overall Loss 0.218817    Objective Loss 0.218817                                        LR 0.000008    Time 0.027933    
2023-01-06 14:48:17,240 - Epoch: [167][  200/  246]    Overall Loss 0.218885    Objective Loss 0.218885                                        LR 0.000008    Time 0.027825    
2023-01-06 14:48:17,491 - Epoch: [167][  210/  246]    Overall Loss 0.219447    Objective Loss 0.219447                                        LR 0.000008    Time 0.027695    
2023-01-06 14:48:17,745 - Epoch: [167][  220/  246]    Overall Loss 0.219757    Objective Loss 0.219757                                        LR 0.000008    Time 0.027589    
2023-01-06 14:48:17,997 - Epoch: [167][  230/  246]    Overall Loss 0.220339    Objective Loss 0.220339                                        LR 0.000008    Time 0.027481    
2023-01-06 14:48:18,263 - Epoch: [167][  240/  246]    Overall Loss 0.220493    Objective Loss 0.220493                                        LR 0.000008    Time 0.027444    
2023-01-06 14:48:18,394 - Epoch: [167][  246/  246]    Overall Loss 0.220374    Objective Loss 0.220374    Top1 92.822967    LR 0.000008    Time 0.027305    
2023-01-06 14:48:18,535 - --- validate (epoch=167)-----------
2023-01-06 14:48:18,535 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:19,000 - Epoch: [167][   10/   28]    Loss 0.251051    Top1 91.562500    
2023-01-06 14:48:19,141 - Epoch: [167][   20/   28]    Loss 0.256858    Top1 90.742188    
2023-01-06 14:48:19,233 - Epoch: [167][   28/   28]    Loss 0.250346    Top1 90.767249    
2023-01-06 14:48:19,372 - ==> Top1: 90.767    Loss: 0.250

2023-01-06 14:48:19,372 - ==> Confusion:
[[ 270   16  153]
 [  15  307  280]
 [  80  101 5764]]

2023-01-06 14:48:19,374 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:19,374 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:19,384 - 

2023-01-06 14:48:19,384 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:19,988 - Epoch: [168][   10/  246]    Overall Loss 0.227686    Objective Loss 0.227686                                        LR 0.000008    Time 0.060286    
2023-01-06 14:48:20,235 - Epoch: [168][   20/  246]    Overall Loss 0.219142    Objective Loss 0.219142                                        LR 0.000008    Time 0.042467    
2023-01-06 14:48:20,480 - Epoch: [168][   30/  246]    Overall Loss 0.218237    Objective Loss 0.218237                                        LR 0.000008    Time 0.036477    
2023-01-06 14:48:20,724 - Epoch: [168][   40/  246]    Overall Loss 0.220283    Objective Loss 0.220283                                        LR 0.000008    Time 0.033455    
2023-01-06 14:48:20,969 - Epoch: [168][   50/  246]    Overall Loss 0.219604    Objective Loss 0.219604                                        LR 0.000008    Time 0.031665    
2023-01-06 14:48:21,215 - Epoch: [168][   60/  246]    Overall Loss 0.216482    Objective Loss 0.216482                                        LR 0.000008    Time 0.030467    
2023-01-06 14:48:21,457 - Epoch: [168][   70/  246]    Overall Loss 0.216200    Objective Loss 0.216200                                        LR 0.000008    Time 0.029579    
2023-01-06 14:48:21,700 - Epoch: [168][   80/  246]    Overall Loss 0.216540    Objective Loss 0.216540                                        LR 0.000008    Time 0.028910    
2023-01-06 14:48:21,943 - Epoch: [168][   90/  246]    Overall Loss 0.215914    Objective Loss 0.215914                                        LR 0.000008    Time 0.028394    
2023-01-06 14:48:22,184 - Epoch: [168][  100/  246]    Overall Loss 0.216809    Objective Loss 0.216809                                        LR 0.000008    Time 0.027966    
2023-01-06 14:48:22,426 - Epoch: [168][  110/  246]    Overall Loss 0.217849    Objective Loss 0.217849                                        LR 0.000008    Time 0.027623    
2023-01-06 14:48:22,667 - Epoch: [168][  120/  246]    Overall Loss 0.217491    Objective Loss 0.217491                                        LR 0.000008    Time 0.027327    
2023-01-06 14:48:22,908 - Epoch: [168][  130/  246]    Overall Loss 0.216056    Objective Loss 0.216056                                        LR 0.000008    Time 0.027077    
2023-01-06 14:48:23,149 - Epoch: [168][  140/  246]    Overall Loss 0.215943    Objective Loss 0.215943                                        LR 0.000008    Time 0.026861    
2023-01-06 14:48:23,391 - Epoch: [168][  150/  246]    Overall Loss 0.216540    Objective Loss 0.216540                                        LR 0.000008    Time 0.026679    
2023-01-06 14:48:23,631 - Epoch: [168][  160/  246]    Overall Loss 0.217766    Objective Loss 0.217766                                        LR 0.000008    Time 0.026512    
2023-01-06 14:48:23,873 - Epoch: [168][  170/  246]    Overall Loss 0.218315    Objective Loss 0.218315                                        LR 0.000008    Time 0.026375    
2023-01-06 14:48:24,115 - Epoch: [168][  180/  246]    Overall Loss 0.219148    Objective Loss 0.219148                                        LR 0.000008    Time 0.026252    
2023-01-06 14:48:24,357 - Epoch: [168][  190/  246]    Overall Loss 0.219061    Objective Loss 0.219061                                        LR 0.000008    Time 0.026143    
2023-01-06 14:48:24,598 - Epoch: [168][  200/  246]    Overall Loss 0.219484    Objective Loss 0.219484                                        LR 0.000008    Time 0.026040    
2023-01-06 14:48:24,840 - Epoch: [168][  210/  246]    Overall Loss 0.220236    Objective Loss 0.220236                                        LR 0.000008    Time 0.025950    
2023-01-06 14:48:25,082 - Epoch: [168][  220/  246]    Overall Loss 0.220285    Objective Loss 0.220285                                        LR 0.000008    Time 0.025871    
2023-01-06 14:48:25,324 - Epoch: [168][  230/  246]    Overall Loss 0.221159    Objective Loss 0.221159                                        LR 0.000008    Time 0.025794    
2023-01-06 14:48:25,584 - Epoch: [168][  240/  246]    Overall Loss 0.221499    Objective Loss 0.221499                                        LR 0.000008    Time 0.025802    
2023-01-06 14:48:25,713 - Epoch: [168][  246/  246]    Overall Loss 0.222071    Objective Loss 0.222071    Top1 89.712919    LR 0.000008    Time 0.025699    
2023-01-06 14:48:25,842 - --- validate (epoch=168)-----------
2023-01-06 14:48:25,842 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:26,294 - Epoch: [168][   10/   28]    Loss 0.240508    Top1 91.445312    
2023-01-06 14:48:26,433 - Epoch: [168][   20/   28]    Loss 0.248050    Top1 90.996094    
2023-01-06 14:48:26,525 - Epoch: [168][   28/   28]    Loss 0.250448    Top1 90.724306    
2023-01-06 14:48:26,658 - ==> Top1: 90.724    Loss: 0.250

2023-01-06 14:48:26,659 - ==> Confusion:
[[ 273   18  148]
 [  20  300  282]
 [  95   85 5765]]

2023-01-06 14:48:26,660 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:26,660 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:26,670 - 

2023-01-06 14:48:26,670 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:27,392 - Epoch: [169][   10/  246]    Overall Loss 0.214857    Objective Loss 0.214857                                        LR 0.000008    Time 0.072135    
2023-01-06 14:48:27,638 - Epoch: [169][   20/  246]    Overall Loss 0.214145    Objective Loss 0.214145                                        LR 0.000008    Time 0.048357    
2023-01-06 14:48:27,886 - Epoch: [169][   30/  246]    Overall Loss 0.218611    Objective Loss 0.218611                                        LR 0.000008    Time 0.040463    
2023-01-06 14:48:28,144 - Epoch: [169][   40/  246]    Overall Loss 0.221405    Objective Loss 0.221405                                        LR 0.000008    Time 0.036791    
2023-01-06 14:48:28,398 - Epoch: [169][   50/  246]    Overall Loss 0.220312    Objective Loss 0.220312                                        LR 0.000008    Time 0.034517    
2023-01-06 14:48:28,656 - Epoch: [169][   60/  246]    Overall Loss 0.223137    Objective Loss 0.223137                                        LR 0.000008    Time 0.033055    
2023-01-06 14:48:28,909 - Epoch: [169][   70/  246]    Overall Loss 0.222405    Objective Loss 0.222405                                        LR 0.000008    Time 0.031938    
2023-01-06 14:48:29,166 - Epoch: [169][   80/  246]    Overall Loss 0.222862    Objective Loss 0.222862                                        LR 0.000008    Time 0.031159    
2023-01-06 14:48:29,419 - Epoch: [169][   90/  246]    Overall Loss 0.222170    Objective Loss 0.222170                                        LR 0.000008    Time 0.030503    
2023-01-06 14:48:29,671 - Epoch: [169][  100/  246]    Overall Loss 0.221633    Objective Loss 0.221633                                        LR 0.000008    Time 0.029975    
2023-01-06 14:48:29,920 - Epoch: [169][  110/  246]    Overall Loss 0.220851    Objective Loss 0.220851                                        LR 0.000008    Time 0.029506    
2023-01-06 14:48:30,157 - Epoch: [169][  120/  246]    Overall Loss 0.221004    Objective Loss 0.221004                                        LR 0.000008    Time 0.029023    
2023-01-06 14:48:30,399 - Epoch: [169][  130/  246]    Overall Loss 0.222715    Objective Loss 0.222715                                        LR 0.000008    Time 0.028647    
2023-01-06 14:48:30,632 - Epoch: [169][  140/  246]    Overall Loss 0.222174    Objective Loss 0.222174                                        LR 0.000008    Time 0.028262    
2023-01-06 14:48:30,868 - Epoch: [169][  150/  246]    Overall Loss 0.222998    Objective Loss 0.222998                                        LR 0.000008    Time 0.027953    
2023-01-06 14:48:31,100 - Epoch: [169][  160/  246]    Overall Loss 0.223498    Objective Loss 0.223498                                        LR 0.000008    Time 0.027654    
2023-01-06 14:48:31,340 - Epoch: [169][  170/  246]    Overall Loss 0.222841    Objective Loss 0.222841                                        LR 0.000008    Time 0.027436    
2023-01-06 14:48:31,578 - Epoch: [169][  180/  246]    Overall Loss 0.222855    Objective Loss 0.222855                                        LR 0.000008    Time 0.027232    
2023-01-06 14:48:31,809 - Epoch: [169][  190/  246]    Overall Loss 0.222644    Objective Loss 0.222644                                        LR 0.000008    Time 0.027012    
2023-01-06 14:48:32,033 - Epoch: [169][  200/  246]    Overall Loss 0.223221    Objective Loss 0.223221                                        LR 0.000008    Time 0.026780    
2023-01-06 14:48:32,265 - Epoch: [169][  210/  246]    Overall Loss 0.222209    Objective Loss 0.222209                                        LR 0.000008    Time 0.026609    
2023-01-06 14:48:32,498 - Epoch: [169][  220/  246]    Overall Loss 0.221824    Objective Loss 0.221824                                        LR 0.000008    Time 0.026460    
2023-01-06 14:48:32,725 - Epoch: [169][  230/  246]    Overall Loss 0.221987    Objective Loss 0.221987                                        LR 0.000008    Time 0.026295    
2023-01-06 14:48:32,955 - Epoch: [169][  240/  246]    Overall Loss 0.221813    Objective Loss 0.221813                                        LR 0.000008    Time 0.026156    
2023-01-06 14:48:33,083 - Epoch: [169][  246/  246]    Overall Loss 0.221957    Objective Loss 0.221957    Top1 91.866029    LR 0.000008    Time 0.026036    
2023-01-06 14:48:33,209 - --- validate (epoch=169)-----------
2023-01-06 14:48:33,209 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:33,671 - Epoch: [169][   10/   28]    Loss 0.256177    Top1 90.742188    
2023-01-06 14:48:33,814 - Epoch: [169][   20/   28]    Loss 0.255349    Top1 90.761719    
2023-01-06 14:48:33,905 - Epoch: [169][   28/   28]    Loss 0.250778    Top1 90.667048    
2023-01-06 14:48:34,032 - ==> Top1: 90.667    Loss: 0.251

2023-01-06 14:48:34,032 - ==> Confusion:
[[ 259   14  166]
 [  19  281  302]
 [  73   78 5794]]

2023-01-06 14:48:34,034 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:34,034 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:34,044 - 

2023-01-06 14:48:34,044 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:34,781 - Epoch: [170][   10/  246]    Overall Loss 0.218103    Objective Loss 0.218103                                        LR 0.000008    Time 0.073592    
2023-01-06 14:48:35,040 - Epoch: [170][   20/  246]    Overall Loss 0.220489    Objective Loss 0.220489                                        LR 0.000008    Time 0.049723    
2023-01-06 14:48:35,305 - Epoch: [170][   30/  246]    Overall Loss 0.221568    Objective Loss 0.221568                                        LR 0.000008    Time 0.041973    
2023-01-06 14:48:35,571 - Epoch: [170][   40/  246]    Overall Loss 0.227001    Objective Loss 0.227001                                        LR 0.000008    Time 0.038118    
2023-01-06 14:48:35,836 - Epoch: [170][   50/  246]    Overall Loss 0.231326    Objective Loss 0.231326                                        LR 0.000008    Time 0.035762    
2023-01-06 14:48:36,102 - Epoch: [170][   60/  246]    Overall Loss 0.230116    Objective Loss 0.230116                                        LR 0.000008    Time 0.034219    
2023-01-06 14:48:36,369 - Epoch: [170][   70/  246]    Overall Loss 0.226546    Objective Loss 0.226546                                        LR 0.000008    Time 0.033113    
2023-01-06 14:48:36,636 - Epoch: [170][   80/  246]    Overall Loss 0.226893    Objective Loss 0.226893                                        LR 0.000008    Time 0.032316    
2023-01-06 14:48:36,909 - Epoch: [170][   90/  246]    Overall Loss 0.225413    Objective Loss 0.225413                                        LR 0.000008    Time 0.031732    
2023-01-06 14:48:37,175 - Epoch: [170][  100/  246]    Overall Loss 0.224473    Objective Loss 0.224473                                        LR 0.000008    Time 0.031219    
2023-01-06 14:48:37,440 - Epoch: [170][  110/  246]    Overall Loss 0.224401    Objective Loss 0.224401                                        LR 0.000008    Time 0.030769    
2023-01-06 14:48:37,728 - Epoch: [170][  120/  246]    Overall Loss 0.223657    Objective Loss 0.223657                                        LR 0.000008    Time 0.030598    
2023-01-06 14:48:38,013 - Epoch: [170][  130/  246]    Overall Loss 0.224291    Objective Loss 0.224291                                        LR 0.000008    Time 0.030437    
2023-01-06 14:48:38,295 - Epoch: [170][  140/  246]    Overall Loss 0.222549    Objective Loss 0.222549                                        LR 0.000008    Time 0.030261    
2023-01-06 14:48:38,571 - Epoch: [170][  150/  246]    Overall Loss 0.223188    Objective Loss 0.223188                                        LR 0.000008    Time 0.030075    
2023-01-06 14:48:38,852 - Epoch: [170][  160/  246]    Overall Loss 0.222999    Objective Loss 0.222999                                        LR 0.000008    Time 0.029953    
2023-01-06 14:48:39,141 - Epoch: [170][  170/  246]    Overall Loss 0.221546    Objective Loss 0.221546                                        LR 0.000008    Time 0.029888    
2023-01-06 14:48:39,397 - Epoch: [170][  180/  246]    Overall Loss 0.221877    Objective Loss 0.221877                                        LR 0.000008    Time 0.029639    
2023-01-06 14:48:39,624 - Epoch: [170][  190/  246]    Overall Loss 0.221522    Objective Loss 0.221522                                        LR 0.000008    Time 0.029268    
2023-01-06 14:48:39,833 - Epoch: [170][  200/  246]    Overall Loss 0.221553    Objective Loss 0.221553                                        LR 0.000008    Time 0.028849    
2023-01-06 14:48:40,073 - Epoch: [170][  210/  246]    Overall Loss 0.221086    Objective Loss 0.221086                                        LR 0.000008    Time 0.028616    
2023-01-06 14:48:40,323 - Epoch: [170][  220/  246]    Overall Loss 0.220989    Objective Loss 0.220989                                        LR 0.000008    Time 0.028447    
2023-01-06 14:48:40,566 - Epoch: [170][  230/  246]    Overall Loss 0.220436    Objective Loss 0.220436                                        LR 0.000008    Time 0.028265    
2023-01-06 14:48:40,823 - Epoch: [170][  240/  246]    Overall Loss 0.220365    Objective Loss 0.220365                                        LR 0.000008    Time 0.028156    
2023-01-06 14:48:40,952 - Epoch: [170][  246/  246]    Overall Loss 0.220234    Objective Loss 0.220234    Top1 94.019139    LR 0.000008    Time 0.027995    
2023-01-06 14:48:41,067 - --- validate (epoch=170)-----------
2023-01-06 14:48:41,067 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:41,521 - Epoch: [170][   10/   28]    Loss 0.253951    Top1 90.546875    
2023-01-06 14:48:41,670 - Epoch: [170][   20/   28]    Loss 0.250537    Top1 90.839844    
2023-01-06 14:48:41,763 - Epoch: [170][   28/   28]    Loss 0.250392    Top1 90.967650    
2023-01-06 14:48:41,892 - ==> Top1: 90.968    Loss: 0.250

2023-01-06 14:48:41,893 - ==> Confusion:
[[ 269   17  153]
 [  16  311  275]
 [  86   84 5775]]

2023-01-06 14:48:41,894 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:41,894 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:41,904 - 

2023-01-06 14:48:41,904 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:42,514 - Epoch: [171][   10/  246]    Overall Loss 0.210794    Objective Loss 0.210794                                        LR 0.000008    Time 0.060880    
2023-01-06 14:48:42,772 - Epoch: [171][   20/  246]    Overall Loss 0.219236    Objective Loss 0.219236                                        LR 0.000008    Time 0.043315    
2023-01-06 14:48:43,028 - Epoch: [171][   30/  246]    Overall Loss 0.219256    Objective Loss 0.219256                                        LR 0.000008    Time 0.037397    
2023-01-06 14:48:43,279 - Epoch: [171][   40/  246]    Overall Loss 0.221472    Objective Loss 0.221472                                        LR 0.000008    Time 0.034325    
2023-01-06 14:48:43,545 - Epoch: [171][   50/  246]    Overall Loss 0.220430    Objective Loss 0.220430                                        LR 0.000008    Time 0.032774    
2023-01-06 14:48:43,811 - Epoch: [171][   60/  246]    Overall Loss 0.221089    Objective Loss 0.221089                                        LR 0.000008    Time 0.031724    
2023-01-06 14:48:44,076 - Epoch: [171][   70/  246]    Overall Loss 0.223996    Objective Loss 0.223996                                        LR 0.000008    Time 0.030983    
2023-01-06 14:48:44,329 - Epoch: [171][   80/  246]    Overall Loss 0.222670    Objective Loss 0.222670                                        LR 0.000008    Time 0.030265    
2023-01-06 14:48:44,579 - Epoch: [171][   90/  246]    Overall Loss 0.223227    Objective Loss 0.223227                                        LR 0.000008    Time 0.029678    
2023-01-06 14:48:44,823 - Epoch: [171][  100/  246]    Overall Loss 0.223176    Objective Loss 0.223176                                        LR 0.000008    Time 0.029143    
2023-01-06 14:48:45,071 - Epoch: [171][  110/  246]    Overall Loss 0.223760    Objective Loss 0.223760                                        LR 0.000008    Time 0.028745    
2023-01-06 14:48:45,315 - Epoch: [171][  120/  246]    Overall Loss 0.221846    Objective Loss 0.221846                                        LR 0.000008    Time 0.028381    
2023-01-06 14:48:45,558 - Epoch: [171][  130/  246]    Overall Loss 0.222329    Objective Loss 0.222329                                        LR 0.000008    Time 0.028056    
2023-01-06 14:48:45,803 - Epoch: [171][  140/  246]    Overall Loss 0.221298    Objective Loss 0.221298                                        LR 0.000008    Time 0.027796    
2023-01-06 14:48:46,054 - Epoch: [171][  150/  246]    Overall Loss 0.221209    Objective Loss 0.221209                                        LR 0.000008    Time 0.027609    
2023-01-06 14:48:46,301 - Epoch: [171][  160/  246]    Overall Loss 0.222415    Objective Loss 0.222415                                        LR 0.000008    Time 0.027423    
2023-01-06 14:48:46,550 - Epoch: [171][  170/  246]    Overall Loss 0.221931    Objective Loss 0.221931                                        LR 0.000008    Time 0.027273    
2023-01-06 14:48:46,803 - Epoch: [171][  180/  246]    Overall Loss 0.222668    Objective Loss 0.222668                                        LR 0.000008    Time 0.027160    
2023-01-06 14:48:47,054 - Epoch: [171][  190/  246]    Overall Loss 0.223275    Objective Loss 0.223275                                        LR 0.000008    Time 0.027051    
2023-01-06 14:48:47,311 - Epoch: [171][  200/  246]    Overall Loss 0.223980    Objective Loss 0.223980                                        LR 0.000008    Time 0.026982    
2023-01-06 14:48:47,569 - Epoch: [171][  210/  246]    Overall Loss 0.222731    Objective Loss 0.222731                                        LR 0.000008    Time 0.026923    
2023-01-06 14:48:47,824 - Epoch: [171][  220/  246]    Overall Loss 0.222432    Objective Loss 0.222432                                        LR 0.000008    Time 0.026857    
2023-01-06 14:48:48,084 - Epoch: [171][  230/  246]    Overall Loss 0.221646    Objective Loss 0.221646                                        LR 0.000008    Time 0.026811    
2023-01-06 14:48:48,350 - Epoch: [171][  240/  246]    Overall Loss 0.221614    Objective Loss 0.221614                                        LR 0.000008    Time 0.026800    
2023-01-06 14:48:48,480 - Epoch: [171][  246/  246]    Overall Loss 0.220799    Objective Loss 0.220799    Top1 94.019139    LR 0.000008    Time 0.026675    
2023-01-06 14:48:48,616 - --- validate (epoch=171)-----------
2023-01-06 14:48:48,616 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:49,065 - Epoch: [171][   10/   28]    Loss 0.254025    Top1 90.312500    
2023-01-06 14:48:49,208 - Epoch: [171][   20/   28]    Loss 0.255922    Top1 90.664062    
2023-01-06 14:48:49,300 - Epoch: [171][   28/   28]    Loss 0.252099    Top1 90.824506    
2023-01-06 14:48:49,427 - ==> Top1: 90.825    Loss: 0.252

2023-01-06 14:48:49,427 - ==> Confusion:
[[ 262   18  159]
 [  15  302  285]
 [  80   84 5781]]

2023-01-06 14:48:49,429 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:49,429 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:49,439 - 

2023-01-06 14:48:49,439 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:50,150 - Epoch: [172][   10/  246]    Overall Loss 0.227488    Objective Loss 0.227488                                        LR 0.000008    Time 0.071022    
2023-01-06 14:48:50,394 - Epoch: [172][   20/  246]    Overall Loss 0.212874    Objective Loss 0.212874                                        LR 0.000008    Time 0.047689    
2023-01-06 14:48:50,648 - Epoch: [172][   30/  246]    Overall Loss 0.216687    Objective Loss 0.216687                                        LR 0.000008    Time 0.040255    
2023-01-06 14:48:50,896 - Epoch: [172][   40/  246]    Overall Loss 0.216233    Objective Loss 0.216233                                        LR 0.000008    Time 0.036391    
2023-01-06 14:48:51,139 - Epoch: [172][   50/  246]    Overall Loss 0.216653    Objective Loss 0.216653                                        LR 0.000008    Time 0.033955    
2023-01-06 14:48:51,385 - Epoch: [172][   60/  246]    Overall Loss 0.214064    Objective Loss 0.214064                                        LR 0.000008    Time 0.032396    
2023-01-06 14:48:51,638 - Epoch: [172][   70/  246]    Overall Loss 0.215595    Objective Loss 0.215595                                        LR 0.000008    Time 0.031351    
2023-01-06 14:48:51,854 - Epoch: [172][   80/  246]    Overall Loss 0.213501    Objective Loss 0.213501                                        LR 0.000008    Time 0.030130    
2023-01-06 14:48:52,106 - Epoch: [172][   90/  246]    Overall Loss 0.214303    Objective Loss 0.214303                                        LR 0.000008    Time 0.029572    
2023-01-06 14:48:52,370 - Epoch: [172][  100/  246]    Overall Loss 0.215453    Objective Loss 0.215453                                        LR 0.000008    Time 0.029247    
2023-01-06 14:48:52,586 - Epoch: [172][  110/  246]    Overall Loss 0.215411    Objective Loss 0.215411                                        LR 0.000008    Time 0.028544    
2023-01-06 14:48:52,801 - Epoch: [172][  120/  246]    Overall Loss 0.215723    Objective Loss 0.215723                                        LR 0.000008    Time 0.027949    
2023-01-06 14:48:53,015 - Epoch: [172][  130/  246]    Overall Loss 0.216314    Objective Loss 0.216314                                        LR 0.000008    Time 0.027445    
2023-01-06 14:48:53,229 - Epoch: [172][  140/  246]    Overall Loss 0.217227    Objective Loss 0.217227                                        LR 0.000008    Time 0.027010    
2023-01-06 14:48:53,442 - Epoch: [172][  150/  246]    Overall Loss 0.216902    Objective Loss 0.216902                                        LR 0.000008    Time 0.026628    
2023-01-06 14:48:53,657 - Epoch: [172][  160/  246]    Overall Loss 0.216881    Objective Loss 0.216881                                        LR 0.000008    Time 0.026305    
2023-01-06 14:48:53,879 - Epoch: [172][  170/  246]    Overall Loss 0.218107    Objective Loss 0.218107                                        LR 0.000008    Time 0.026060    
2023-01-06 14:48:54,097 - Epoch: [172][  180/  246]    Overall Loss 0.218575    Objective Loss 0.218575                                        LR 0.000008    Time 0.025822    
2023-01-06 14:48:54,312 - Epoch: [172][  190/  246]    Overall Loss 0.218688    Objective Loss 0.218688                                        LR 0.000008    Time 0.025591    
2023-01-06 14:48:54,527 - Epoch: [172][  200/  246]    Overall Loss 0.219081    Objective Loss 0.219081                                        LR 0.000008    Time 0.025383    
2023-01-06 14:48:54,737 - Epoch: [172][  210/  246]    Overall Loss 0.218919    Objective Loss 0.218919                                        LR 0.000008    Time 0.025174    
2023-01-06 14:48:54,952 - Epoch: [172][  220/  246]    Overall Loss 0.218723    Objective Loss 0.218723                                        LR 0.000008    Time 0.025003    
2023-01-06 14:48:55,168 - Epoch: [172][  230/  246]    Overall Loss 0.218857    Objective Loss 0.218857                                        LR 0.000008    Time 0.024853    
2023-01-06 14:48:55,422 - Epoch: [172][  240/  246]    Overall Loss 0.219366    Objective Loss 0.219366                                        LR 0.000008    Time 0.024877    
2023-01-06 14:48:55,550 - Epoch: [172][  246/  246]    Overall Loss 0.219777    Objective Loss 0.219777    Top1 92.344498    LR 0.000008    Time 0.024788    
2023-01-06 14:48:55,692 - --- validate (epoch=172)-----------
2023-01-06 14:48:55,692 - 6986 samples (256 per mini-batch)
2023-01-06 14:48:56,129 - Epoch: [172][   10/   28]    Loss 0.242706    Top1 90.664062    
2023-01-06 14:48:56,271 - Epoch: [172][   20/   28]    Loss 0.240683    Top1 91.132812    
2023-01-06 14:48:56,363 - Epoch: [172][   28/   28]    Loss 0.252811    Top1 90.910392    
2023-01-06 14:48:56,502 - ==> Top1: 90.910    Loss: 0.253

2023-01-06 14:48:56,503 - ==> Confusion:
[[ 257   19  163]
 [  16  287  299]
 [  68   70 5807]]

2023-01-06 14:48:56,504 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:48:56,505 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:48:56,515 - 

2023-01-06 14:48:56,515 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:48:57,109 - Epoch: [173][   10/  246]    Overall Loss 0.223296    Objective Loss 0.223296                                        LR 0.000008    Time 0.059309    
2023-01-06 14:48:57,355 - Epoch: [173][   20/  246]    Overall Loss 0.222741    Objective Loss 0.222741                                        LR 0.000008    Time 0.041967    
2023-01-06 14:48:57,604 - Epoch: [173][   30/  246]    Overall Loss 0.222424    Objective Loss 0.222424                                        LR 0.000008    Time 0.036252    
2023-01-06 14:48:57,851 - Epoch: [173][   40/  246]    Overall Loss 0.220663    Objective Loss 0.220663                                        LR 0.000008    Time 0.033329    
2023-01-06 14:48:58,099 - Epoch: [173][   50/  246]    Overall Loss 0.218372    Objective Loss 0.218372                                        LR 0.000008    Time 0.031619    
2023-01-06 14:48:58,350 - Epoch: [173][   60/  246]    Overall Loss 0.218765    Objective Loss 0.218765                                        LR 0.000008    Time 0.030524    
2023-01-06 14:48:58,602 - Epoch: [173][   70/  246]    Overall Loss 0.217992    Objective Loss 0.217992                                        LR 0.000008    Time 0.029733    
2023-01-06 14:48:58,855 - Epoch: [173][   80/  246]    Overall Loss 0.218976    Objective Loss 0.218976                                        LR 0.000008    Time 0.029164    
2023-01-06 14:48:59,101 - Epoch: [173][   90/  246]    Overall Loss 0.219703    Objective Loss 0.219703                                        LR 0.000008    Time 0.028639    
2023-01-06 14:48:59,353 - Epoch: [173][  100/  246]    Overall Loss 0.219846    Objective Loss 0.219846                                        LR 0.000008    Time 0.028292    
2023-01-06 14:48:59,605 - Epoch: [173][  110/  246]    Overall Loss 0.220962    Objective Loss 0.220962                                        LR 0.000008    Time 0.028003    
2023-01-06 14:48:59,858 - Epoch: [173][  120/  246]    Overall Loss 0.219184    Objective Loss 0.219184                                        LR 0.000008    Time 0.027764    
2023-01-06 14:49:00,111 - Epoch: [173][  130/  246]    Overall Loss 0.219920    Objective Loss 0.219920                                        LR 0.000008    Time 0.027562    
2023-01-06 14:49:00,363 - Epoch: [173][  140/  246]    Overall Loss 0.218369    Objective Loss 0.218369                                        LR 0.000008    Time 0.027381    
2023-01-06 14:49:00,616 - Epoch: [173][  150/  246]    Overall Loss 0.219444    Objective Loss 0.219444                                        LR 0.000008    Time 0.027233    
2023-01-06 14:49:00,869 - Epoch: [173][  160/  246]    Overall Loss 0.219542    Objective Loss 0.219542                                        LR 0.000008    Time 0.027098    
2023-01-06 14:49:01,121 - Epoch: [173][  170/  246]    Overall Loss 0.219281    Objective Loss 0.219281                                        LR 0.000008    Time 0.026976    
2023-01-06 14:49:01,372 - Epoch: [173][  180/  246]    Overall Loss 0.219616    Objective Loss 0.219616                                        LR 0.000008    Time 0.026866    
2023-01-06 14:49:01,623 - Epoch: [173][  190/  246]    Overall Loss 0.218966    Objective Loss 0.218966                                        LR 0.000008    Time 0.026766    
2023-01-06 14:49:01,875 - Epoch: [173][  200/  246]    Overall Loss 0.219258    Objective Loss 0.219258                                        LR 0.000008    Time 0.026680    
2023-01-06 14:49:02,112 - Epoch: [173][  210/  246]    Overall Loss 0.219776    Objective Loss 0.219776                                        LR 0.000008    Time 0.026531    
2023-01-06 14:49:02,359 - Epoch: [173][  220/  246]    Overall Loss 0.220300    Objective Loss 0.220300                                        LR 0.000008    Time 0.026440    
2023-01-06 14:49:02,602 - Epoch: [173][  230/  246]    Overall Loss 0.220274    Objective Loss 0.220274                                        LR 0.000008    Time 0.026343    
2023-01-06 14:49:02,858 - Epoch: [173][  240/  246]    Overall Loss 0.219779    Objective Loss 0.219779                                        LR 0.000008    Time 0.026313    
2023-01-06 14:49:02,988 - Epoch: [173][  246/  246]    Overall Loss 0.219778    Objective Loss 0.219778    Top1 92.583732    LR 0.000008    Time 0.026196    
2023-01-06 14:49:03,125 - --- validate (epoch=173)-----------
2023-01-06 14:49:03,125 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:03,576 - Epoch: [173][   10/   28]    Loss 0.243655    Top1 91.679688    
2023-01-06 14:49:03,723 - Epoch: [173][   20/   28]    Loss 0.250842    Top1 91.269531    
2023-01-06 14:49:03,813 - Epoch: [173][   28/   28]    Loss 0.246555    Top1 91.110793    
2023-01-06 14:49:03,946 - ==> Top1: 91.111    Loss: 0.247

2023-01-06 14:49:03,947 - ==> Confusion:
[[ 266   15  158]
 [  13  303  286]
 [  68   81 5796]]

2023-01-06 14:49:03,949 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:03,950 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:03,963 - 

2023-01-06 14:49:03,963 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:04,675 - Epoch: [174][   10/  246]    Overall Loss 0.205264    Objective Loss 0.205264                                        LR 0.000008    Time 0.071068    
2023-01-06 14:49:04,908 - Epoch: [174][   20/  246]    Overall Loss 0.204353    Objective Loss 0.204353                                        LR 0.000008    Time 0.047186    
2023-01-06 14:49:05,165 - Epoch: [174][   30/  246]    Overall Loss 0.210362    Objective Loss 0.210362                                        LR 0.000008    Time 0.039982    
2023-01-06 14:49:05,420 - Epoch: [174][   40/  246]    Overall Loss 0.209212    Objective Loss 0.209212                                        LR 0.000008    Time 0.036356    
2023-01-06 14:49:05,675 - Epoch: [174][   50/  246]    Overall Loss 0.208346    Objective Loss 0.208346                                        LR 0.000008    Time 0.034190    
2023-01-06 14:49:05,942 - Epoch: [174][   60/  246]    Overall Loss 0.208100    Objective Loss 0.208100                                        LR 0.000008    Time 0.032932    
2023-01-06 14:49:06,207 - Epoch: [174][   70/  246]    Overall Loss 0.210286    Objective Loss 0.210286                                        LR 0.000008    Time 0.032005    
2023-01-06 14:49:06,474 - Epoch: [174][   80/  246]    Overall Loss 0.211280    Objective Loss 0.211280                                        LR 0.000008    Time 0.031308    
2023-01-06 14:49:06,731 - Epoch: [174][   90/  246]    Overall Loss 0.212332    Objective Loss 0.212332                                        LR 0.000008    Time 0.030688    
2023-01-06 14:49:06,990 - Epoch: [174][  100/  246]    Overall Loss 0.214371    Objective Loss 0.214371                                        LR 0.000008    Time 0.030196    
2023-01-06 14:49:07,246 - Epoch: [174][  110/  246]    Overall Loss 0.215088    Objective Loss 0.215088                                        LR 0.000008    Time 0.029776    
2023-01-06 14:49:07,501 - Epoch: [174][  120/  246]    Overall Loss 0.215560    Objective Loss 0.215560                                        LR 0.000008    Time 0.029407    
2023-01-06 14:49:07,758 - Epoch: [174][  130/  246]    Overall Loss 0.214546    Objective Loss 0.214546                                        LR 0.000008    Time 0.029115    
2023-01-06 14:49:08,021 - Epoch: [174][  140/  246]    Overall Loss 0.216338    Objective Loss 0.216338                                        LR 0.000008    Time 0.028895    
2023-01-06 14:49:08,276 - Epoch: [174][  150/  246]    Overall Loss 0.217352    Objective Loss 0.217352                                        LR 0.000008    Time 0.028667    
2023-01-06 14:49:08,528 - Epoch: [174][  160/  246]    Overall Loss 0.217953    Objective Loss 0.217953                                        LR 0.000008    Time 0.028448    
2023-01-06 14:49:08,781 - Epoch: [174][  170/  246]    Overall Loss 0.218638    Objective Loss 0.218638                                        LR 0.000008    Time 0.028261    
2023-01-06 14:49:09,032 - Epoch: [174][  180/  246]    Overall Loss 0.218798    Objective Loss 0.218798                                        LR 0.000008    Time 0.028080    
2023-01-06 14:49:09,286 - Epoch: [174][  190/  246]    Overall Loss 0.218296    Objective Loss 0.218296                                        LR 0.000008    Time 0.027935    
2023-01-06 14:49:09,539 - Epoch: [174][  200/  246]    Overall Loss 0.218847    Objective Loss 0.218847                                        LR 0.000008    Time 0.027803    
2023-01-06 14:49:09,789 - Epoch: [174][  210/  246]    Overall Loss 0.218530    Objective Loss 0.218530                                        LR 0.000008    Time 0.027669    
2023-01-06 14:49:10,039 - Epoch: [174][  220/  246]    Overall Loss 0.218641    Objective Loss 0.218641                                        LR 0.000008    Time 0.027544    
2023-01-06 14:49:10,288 - Epoch: [174][  230/  246]    Overall Loss 0.219050    Objective Loss 0.219050                                        LR 0.000008    Time 0.027429    
2023-01-06 14:49:10,551 - Epoch: [174][  240/  246]    Overall Loss 0.218688    Objective Loss 0.218688                                        LR 0.000008    Time 0.027379    
2023-01-06 14:49:10,671 - Epoch: [174][  246/  246]    Overall Loss 0.218775    Objective Loss 0.218775    Top1 93.301435    LR 0.000008    Time 0.027199    
2023-01-06 14:49:10,809 - --- validate (epoch=174)-----------
2023-01-06 14:49:10,809 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:11,269 - Epoch: [174][   10/   28]    Loss 0.247743    Top1 90.781250    
2023-01-06 14:49:11,413 - Epoch: [174][   20/   28]    Loss 0.248500    Top1 90.996094    
2023-01-06 14:49:11,510 - Epoch: [174][   28/   28]    Loss 0.249266    Top1 90.924707    
2023-01-06 14:49:11,618 - ==> Top1: 90.925    Loss: 0.249

2023-01-06 14:49:11,618 - ==> Confusion:
[[ 267   15  157]
 [  16  301  285]
 [  82   79 5784]]

2023-01-06 14:49:11,620 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:11,620 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:11,630 - 

2023-01-06 14:49:11,630 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:12,371 - Epoch: [175][   10/  246]    Overall Loss 0.211917    Objective Loss 0.211917                                        LR 0.000008    Time 0.074077    
2023-01-06 14:49:12,617 - Epoch: [175][   20/  246]    Overall Loss 0.211504    Objective Loss 0.211504                                        LR 0.000008    Time 0.049314    
2023-01-06 14:49:12,865 - Epoch: [175][   30/  246]    Overall Loss 0.216141    Objective Loss 0.216141                                        LR 0.000008    Time 0.041122    
2023-01-06 14:49:13,113 - Epoch: [175][   40/  246]    Overall Loss 0.220959    Objective Loss 0.220959                                        LR 0.000008    Time 0.037043    
2023-01-06 14:49:13,362 - Epoch: [175][   50/  246]    Overall Loss 0.222440    Objective Loss 0.222440                                        LR 0.000008    Time 0.034589    
2023-01-06 14:49:13,609 - Epoch: [175][   60/  246]    Overall Loss 0.222795    Objective Loss 0.222795                                        LR 0.000008    Time 0.032932    
2023-01-06 14:49:13,859 - Epoch: [175][   70/  246]    Overall Loss 0.221901    Objective Loss 0.221901                                        LR 0.000008    Time 0.031799    
2023-01-06 14:49:14,112 - Epoch: [175][   80/  246]    Overall Loss 0.221840    Objective Loss 0.221840                                        LR 0.000008    Time 0.030980    
2023-01-06 14:49:14,357 - Epoch: [175][   90/  246]    Overall Loss 0.221573    Objective Loss 0.221573                                        LR 0.000008    Time 0.030253    
2023-01-06 14:49:14,611 - Epoch: [175][  100/  246]    Overall Loss 0.221836    Objective Loss 0.221836                                        LR 0.000008    Time 0.029760    
2023-01-06 14:49:14,865 - Epoch: [175][  110/  246]    Overall Loss 0.222074    Objective Loss 0.222074                                        LR 0.000008    Time 0.029359    
2023-01-06 14:49:15,118 - Epoch: [175][  120/  246]    Overall Loss 0.222185    Objective Loss 0.222185                                        LR 0.000008    Time 0.029013    
2023-01-06 14:49:15,369 - Epoch: [175][  130/  246]    Overall Loss 0.220474    Objective Loss 0.220474                                        LR 0.000008    Time 0.028713    
2023-01-06 14:49:15,614 - Epoch: [175][  140/  246]    Overall Loss 0.221015    Objective Loss 0.221015                                        LR 0.000008    Time 0.028408    
2023-01-06 14:49:15,852 - Epoch: [175][  150/  246]    Overall Loss 0.221413    Objective Loss 0.221413                                        LR 0.000008    Time 0.028097    
2023-01-06 14:49:16,094 - Epoch: [175][  160/  246]    Overall Loss 0.221478    Objective Loss 0.221478                                        LR 0.000008    Time 0.027847    
2023-01-06 14:49:16,345 - Epoch: [175][  170/  246]    Overall Loss 0.219820    Objective Loss 0.219820                                        LR 0.000008    Time 0.027684    
2023-01-06 14:49:16,596 - Epoch: [175][  180/  246]    Overall Loss 0.219918    Objective Loss 0.219918                                        LR 0.000008    Time 0.027539    
2023-01-06 14:49:16,849 - Epoch: [175][  190/  246]    Overall Loss 0.219676    Objective Loss 0.219676                                        LR 0.000008    Time 0.027419    
2023-01-06 14:49:17,101 - Epoch: [175][  200/  246]    Overall Loss 0.219831    Objective Loss 0.219831                                        LR 0.000008    Time 0.027302    
2023-01-06 14:49:17,352 - Epoch: [175][  210/  246]    Overall Loss 0.220114    Objective Loss 0.220114                                        LR 0.000008    Time 0.027197    
2023-01-06 14:49:17,603 - Epoch: [175][  220/  246]    Overall Loss 0.220762    Objective Loss 0.220762                                        LR 0.000008    Time 0.027100    
2023-01-06 14:49:17,856 - Epoch: [175][  230/  246]    Overall Loss 0.220717    Objective Loss 0.220717                                        LR 0.000008    Time 0.027016    
2023-01-06 14:49:18,120 - Epoch: [175][  240/  246]    Overall Loss 0.220410    Objective Loss 0.220410                                        LR 0.000008    Time 0.026988    
2023-01-06 14:49:18,248 - Epoch: [175][  246/  246]    Overall Loss 0.220221    Objective Loss 0.220221    Top1 89.952153    LR 0.000008    Time 0.026850    
2023-01-06 14:49:18,419 - --- validate (epoch=175)-----------
2023-01-06 14:49:18,419 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:18,861 - Epoch: [175][   10/   28]    Loss 0.255374    Top1 90.234375    
2023-01-06 14:49:19,007 - Epoch: [175][   20/   28]    Loss 0.244132    Top1 90.859375    
2023-01-06 14:49:19,099 - Epoch: [175][   28/   28]    Loss 0.249055    Top1 90.838820    
2023-01-06 14:49:19,236 - ==> Top1: 90.839    Loss: 0.249

2023-01-06 14:49:19,236 - ==> Confusion:
[[ 244   17  178]
 [  14  297  291]
 [  63   77 5805]]

2023-01-06 14:49:19,238 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:19,238 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:19,248 - 

2023-01-06 14:49:19,248 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:19,860 - Epoch: [176][   10/  246]    Overall Loss 0.195381    Objective Loss 0.195381                                        LR 0.000008    Time 0.061111    
2023-01-06 14:49:20,112 - Epoch: [176][   20/  246]    Overall Loss 0.212631    Objective Loss 0.212631                                        LR 0.000008    Time 0.043155    
2023-01-06 14:49:20,370 - Epoch: [176][   30/  246]    Overall Loss 0.219005    Objective Loss 0.219005                                        LR 0.000008    Time 0.037359    
2023-01-06 14:49:20,620 - Epoch: [176][   40/  246]    Overall Loss 0.216546    Objective Loss 0.216546                                        LR 0.000008    Time 0.034267    
2023-01-06 14:49:20,873 - Epoch: [176][   50/  246]    Overall Loss 0.217201    Objective Loss 0.217201                                        LR 0.000008    Time 0.032451    
2023-01-06 14:49:21,119 - Epoch: [176][   60/  246]    Overall Loss 0.215103    Objective Loss 0.215103                                        LR 0.000008    Time 0.031137    
2023-01-06 14:49:21,359 - Epoch: [176][   70/  246]    Overall Loss 0.215271    Objective Loss 0.215271                                        LR 0.000008    Time 0.030111    
2023-01-06 14:49:21,608 - Epoch: [176][   80/  246]    Overall Loss 0.216632    Objective Loss 0.216632                                        LR 0.000008    Time 0.029459    
2023-01-06 14:49:21,856 - Epoch: [176][   90/  246]    Overall Loss 0.217408    Objective Loss 0.217408                                        LR 0.000008    Time 0.028933    
2023-01-06 14:49:22,111 - Epoch: [176][  100/  246]    Overall Loss 0.217312    Objective Loss 0.217312                                        LR 0.000008    Time 0.028586    
2023-01-06 14:49:22,362 - Epoch: [176][  110/  246]    Overall Loss 0.217086    Objective Loss 0.217086                                        LR 0.000008    Time 0.028263    
2023-01-06 14:49:22,627 - Epoch: [176][  120/  246]    Overall Loss 0.219305    Objective Loss 0.219305                                        LR 0.000008    Time 0.028116    
2023-01-06 14:49:22,896 - Epoch: [176][  130/  246]    Overall Loss 0.217453    Objective Loss 0.217453                                        LR 0.000008    Time 0.028019    
2023-01-06 14:49:23,177 - Epoch: [176][  140/  246]    Overall Loss 0.217667    Objective Loss 0.217667                                        LR 0.000008    Time 0.028023    
2023-01-06 14:49:23,398 - Epoch: [176][  150/  246]    Overall Loss 0.216931    Objective Loss 0.216931                                        LR 0.000008    Time 0.027627    
2023-01-06 14:49:23,614 - Epoch: [176][  160/  246]    Overall Loss 0.216098    Objective Loss 0.216098                                        LR 0.000008    Time 0.027244    
2023-01-06 14:49:23,834 - Epoch: [176][  170/  246]    Overall Loss 0.215882    Objective Loss 0.215882                                        LR 0.000008    Time 0.026938    
2023-01-06 14:49:24,066 - Epoch: [176][  180/  246]    Overall Loss 0.215837    Objective Loss 0.215837                                        LR 0.000008    Time 0.026726    
2023-01-06 14:49:24,295 - Epoch: [176][  190/  246]    Overall Loss 0.217265    Objective Loss 0.217265                                        LR 0.000008    Time 0.026524    
2023-01-06 14:49:24,516 - Epoch: [176][  200/  246]    Overall Loss 0.218305    Objective Loss 0.218305                                        LR 0.000008    Time 0.026299    
2023-01-06 14:49:24,745 - Epoch: [176][  210/  246]    Overall Loss 0.218750    Objective Loss 0.218750                                        LR 0.000008    Time 0.026137    
2023-01-06 14:49:24,963 - Epoch: [176][  220/  246]    Overall Loss 0.219759    Objective Loss 0.219759                                        LR 0.000008    Time 0.025939    
2023-01-06 14:49:25,195 - Epoch: [176][  230/  246]    Overall Loss 0.219198    Objective Loss 0.219198                                        LR 0.000008    Time 0.025814    
2023-01-06 14:49:25,435 - Epoch: [176][  240/  246]    Overall Loss 0.219968    Objective Loss 0.219968                                        LR 0.000008    Time 0.025737    
2023-01-06 14:49:25,562 - Epoch: [176][  246/  246]    Overall Loss 0.219788    Objective Loss 0.219788    Top1 93.301435    LR 0.000008    Time 0.025625    
2023-01-06 14:49:25,722 - --- validate (epoch=176)-----------
2023-01-06 14:49:25,722 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:26,152 - Epoch: [176][   10/   28]    Loss 0.258354    Top1 90.820312    
2023-01-06 14:49:26,292 - Epoch: [176][   20/   28]    Loss 0.250814    Top1 91.035156    
2023-01-06 14:49:26,383 - Epoch: [176][   28/   28]    Loss 0.252708    Top1 90.853135    
2023-01-06 14:49:26,557 - ==> Top1: 90.853    Loss: 0.253

2023-01-06 14:49:26,558 - ==> Confusion:
[[ 277   16  146]
 [  20  308  274]
 [  81  102 5762]]

2023-01-06 14:49:26,559 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:26,560 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:26,577 - 

2023-01-06 14:49:26,577 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:27,290 - Epoch: [177][   10/  246]    Overall Loss 0.220331    Objective Loss 0.220331                                        LR 0.000008    Time 0.071263    
2023-01-06 14:49:27,536 - Epoch: [177][   20/  246]    Overall Loss 0.215811    Objective Loss 0.215811                                        LR 0.000008    Time 0.047918    
2023-01-06 14:49:27,780 - Epoch: [177][   30/  246]    Overall Loss 0.214186    Objective Loss 0.214186                                        LR 0.000008    Time 0.040052    
2023-01-06 14:49:28,027 - Epoch: [177][   40/  246]    Overall Loss 0.213588    Objective Loss 0.213588                                        LR 0.000008    Time 0.036198    
2023-01-06 14:49:28,283 - Epoch: [177][   50/  246]    Overall Loss 0.222441    Objective Loss 0.222441                                        LR 0.000008    Time 0.034044    
2023-01-06 14:49:28,535 - Epoch: [177][   60/  246]    Overall Loss 0.223811    Objective Loss 0.223811                                        LR 0.000008    Time 0.032565    
2023-01-06 14:49:28,783 - Epoch: [177][   70/  246]    Overall Loss 0.223625    Objective Loss 0.223625                                        LR 0.000008    Time 0.031439    
2023-01-06 14:49:29,020 - Epoch: [177][   80/  246]    Overall Loss 0.222108    Objective Loss 0.222108                                        LR 0.000008    Time 0.030468    
2023-01-06 14:49:29,257 - Epoch: [177][   90/  246]    Overall Loss 0.221817    Objective Loss 0.221817                                        LR 0.000008    Time 0.029719    
2023-01-06 14:49:29,497 - Epoch: [177][  100/  246]    Overall Loss 0.220734    Objective Loss 0.220734                                        LR 0.000008    Time 0.029142    
2023-01-06 14:49:29,744 - Epoch: [177][  110/  246]    Overall Loss 0.221455    Objective Loss 0.221455                                        LR 0.000008    Time 0.028714    
2023-01-06 14:49:29,980 - Epoch: [177][  120/  246]    Overall Loss 0.221371    Objective Loss 0.221371                                        LR 0.000008    Time 0.028287    
2023-01-06 14:49:30,211 - Epoch: [177][  130/  246]    Overall Loss 0.220672    Objective Loss 0.220672                                        LR 0.000008    Time 0.027886    
2023-01-06 14:49:30,435 - Epoch: [177][  140/  246]    Overall Loss 0.219771    Objective Loss 0.219771                                        LR 0.000008    Time 0.027491    
2023-01-06 14:49:30,668 - Epoch: [177][  150/  246]    Overall Loss 0.218778    Objective Loss 0.218778                                        LR 0.000008    Time 0.027210    
2023-01-06 14:49:30,893 - Epoch: [177][  160/  246]    Overall Loss 0.217503    Objective Loss 0.217503                                        LR 0.000008    Time 0.026911    
2023-01-06 14:49:31,122 - Epoch: [177][  170/  246]    Overall Loss 0.216756    Objective Loss 0.216756                                        LR 0.000008    Time 0.026675    
2023-01-06 14:49:31,354 - Epoch: [177][  180/  246]    Overall Loss 0.216280    Objective Loss 0.216280                                        LR 0.000008    Time 0.026479    
2023-01-06 14:49:31,577 - Epoch: [177][  190/  246]    Overall Loss 0.216301    Objective Loss 0.216301                                        LR 0.000008    Time 0.026260    
2023-01-06 14:49:31,811 - Epoch: [177][  200/  246]    Overall Loss 0.216229    Objective Loss 0.216229                                        LR 0.000008    Time 0.026114    
2023-01-06 14:49:32,039 - Epoch: [177][  210/  246]    Overall Loss 0.217129    Objective Loss 0.217129                                        LR 0.000008    Time 0.025954    
2023-01-06 14:49:32,277 - Epoch: [177][  220/  246]    Overall Loss 0.217437    Objective Loss 0.217437                                        LR 0.000008    Time 0.025852    
2023-01-06 14:49:32,496 - Epoch: [177][  230/  246]    Overall Loss 0.217925    Objective Loss 0.217925                                        LR 0.000008    Time 0.025680    
2023-01-06 14:49:32,738 - Epoch: [177][  240/  246]    Overall Loss 0.218608    Objective Loss 0.218608                                        LR 0.000008    Time 0.025617    
2023-01-06 14:49:32,860 - Epoch: [177][  246/  246]    Overall Loss 0.219260    Objective Loss 0.219260    Top1 91.387560    LR 0.000008    Time 0.025487    
2023-01-06 14:49:33,019 - --- validate (epoch=177)-----------
2023-01-06 14:49:33,020 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:33,472 - Epoch: [177][   10/   28]    Loss 0.250044    Top1 91.210938    
2023-01-06 14:49:33,617 - Epoch: [177][   20/   28]    Loss 0.251686    Top1 90.839844    
2023-01-06 14:49:33,709 - Epoch: [177][   28/   28]    Loss 0.253671    Top1 90.967650    
2023-01-06 14:49:33,846 - ==> Top1: 90.968    Loss: 0.254

2023-01-06 14:49:33,847 - ==> Confusion:
[[ 253   17  169]
 [  13  296  293]
 [  58   81 5806]]

2023-01-06 14:49:33,848 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:33,849 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:33,858 - 

2023-01-06 14:49:33,858 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:34,581 - Epoch: [178][   10/  246]    Overall Loss 0.212134    Objective Loss 0.212134                                        LR 0.000008    Time 0.072127    
2023-01-06 14:49:34,843 - Epoch: [178][   20/  246]    Overall Loss 0.221154    Objective Loss 0.221154                                        LR 0.000008    Time 0.049164    
2023-01-06 14:49:35,110 - Epoch: [178][   30/  246]    Overall Loss 0.222257    Objective Loss 0.222257                                        LR 0.000008    Time 0.041655    
2023-01-06 14:49:35,370 - Epoch: [178][   40/  246]    Overall Loss 0.228243    Objective Loss 0.228243                                        LR 0.000008    Time 0.037705    
2023-01-06 14:49:35,630 - Epoch: [178][   50/  246]    Overall Loss 0.225983    Objective Loss 0.225983                                        LR 0.000008    Time 0.035342    
2023-01-06 14:49:35,897 - Epoch: [178][   60/  246]    Overall Loss 0.225113    Objective Loss 0.225113                                        LR 0.000008    Time 0.033900    
2023-01-06 14:49:36,156 - Epoch: [178][   70/  246]    Overall Loss 0.227433    Objective Loss 0.227433                                        LR 0.000008    Time 0.032746    
2023-01-06 14:49:36,415 - Epoch: [178][   80/  246]    Overall Loss 0.228146    Objective Loss 0.228146                                        LR 0.000008    Time 0.031883    
2023-01-06 14:49:36,674 - Epoch: [178][   90/  246]    Overall Loss 0.225617    Objective Loss 0.225617                                        LR 0.000008    Time 0.031213    
2023-01-06 14:49:36,935 - Epoch: [178][  100/  246]    Overall Loss 0.223163    Objective Loss 0.223163                                        LR 0.000008    Time 0.030679    
2023-01-06 14:49:37,193 - Epoch: [178][  110/  246]    Overall Loss 0.222831    Objective Loss 0.222831                                        LR 0.000008    Time 0.030236    
2023-01-06 14:49:37,457 - Epoch: [178][  120/  246]    Overall Loss 0.222126    Objective Loss 0.222126                                        LR 0.000008    Time 0.029894    
2023-01-06 14:49:37,715 - Epoch: [178][  130/  246]    Overall Loss 0.221630    Objective Loss 0.221630                                        LR 0.000008    Time 0.029574    
2023-01-06 14:49:37,975 - Epoch: [178][  140/  246]    Overall Loss 0.221709    Objective Loss 0.221709                                        LR 0.000008    Time 0.029318    
2023-01-06 14:49:38,232 - Epoch: [178][  150/  246]    Overall Loss 0.220394    Objective Loss 0.220394                                        LR 0.000008    Time 0.029072    
2023-01-06 14:49:38,489 - Epoch: [178][  160/  246]    Overall Loss 0.221795    Objective Loss 0.221795                                        LR 0.000008    Time 0.028858    
2023-01-06 14:49:38,750 - Epoch: [178][  170/  246]    Overall Loss 0.220868    Objective Loss 0.220868                                        LR 0.000008    Time 0.028692    
2023-01-06 14:49:39,010 - Epoch: [178][  180/  246]    Overall Loss 0.221638    Objective Loss 0.221638                                        LR 0.000008    Time 0.028537    
2023-01-06 14:49:39,273 - Epoch: [178][  190/  246]    Overall Loss 0.220604    Objective Loss 0.220604                                        LR 0.000008    Time 0.028421    
2023-01-06 14:49:39,531 - Epoch: [178][  200/  246]    Overall Loss 0.221546    Objective Loss 0.221546                                        LR 0.000008    Time 0.028283    
2023-01-06 14:49:39,789 - Epoch: [178][  210/  246]    Overall Loss 0.221717    Objective Loss 0.221717                                        LR 0.000008    Time 0.028163    
2023-01-06 14:49:40,045 - Epoch: [178][  220/  246]    Overall Loss 0.221524    Objective Loss 0.221524                                        LR 0.000008    Time 0.028038    
2023-01-06 14:49:40,304 - Epoch: [178][  230/  246]    Overall Loss 0.220677    Objective Loss 0.220677                                        LR 0.000008    Time 0.027944    
2023-01-06 14:49:40,566 - Epoch: [178][  240/  246]    Overall Loss 0.220771    Objective Loss 0.220771                                        LR 0.000008    Time 0.027869    
2023-01-06 14:49:40,697 - Epoch: [178][  246/  246]    Overall Loss 0.221356    Objective Loss 0.221356    Top1 89.473684    LR 0.000008    Time 0.027723    
2023-01-06 14:49:40,827 - --- validate (epoch=178)-----------
2023-01-06 14:49:40,827 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:41,284 - Epoch: [178][   10/   28]    Loss 0.247615    Top1 91.054688    
2023-01-06 14:49:41,443 - Epoch: [178][   20/   28]    Loss 0.245116    Top1 90.898438    
2023-01-06 14:49:41,533 - Epoch: [178][   28/   28]    Loss 0.251603    Top1 90.781563    
2023-01-06 14:49:41,697 - ==> Top1: 90.782    Loss: 0.252

2023-01-06 14:49:41,698 - ==> Confusion:
[[ 253   18  168]
 [  12  318  272]
 [  72  102 5771]]

2023-01-06 14:49:41,699 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:41,699 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:41,709 - 

2023-01-06 14:49:41,709 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:42,302 - Epoch: [179][   10/  246]    Overall Loss 0.218735    Objective Loss 0.218735                                        LR 0.000008    Time 0.059196    
2023-01-06 14:49:42,545 - Epoch: [179][   20/  246]    Overall Loss 0.224280    Objective Loss 0.224280                                        LR 0.000008    Time 0.041734    
2023-01-06 14:49:42,811 - Epoch: [179][   30/  246]    Overall Loss 0.222073    Objective Loss 0.222073                                        LR 0.000008    Time 0.036669    
2023-01-06 14:49:43,037 - Epoch: [179][   40/  246]    Overall Loss 0.217184    Objective Loss 0.217184                                        LR 0.000008    Time 0.033100    
2023-01-06 14:49:43,259 - Epoch: [179][   50/  246]    Overall Loss 0.216185    Objective Loss 0.216185                                        LR 0.000008    Time 0.030907    
2023-01-06 14:49:43,479 - Epoch: [179][   60/  246]    Overall Loss 0.218231    Objective Loss 0.218231                                        LR 0.000008    Time 0.029423    
2023-01-06 14:49:43,691 - Epoch: [179][   70/  246]    Overall Loss 0.218615    Objective Loss 0.218615                                        LR 0.000008    Time 0.028248    
2023-01-06 14:49:43,902 - Epoch: [179][   80/  246]    Overall Loss 0.217913    Objective Loss 0.217913                                        LR 0.000008    Time 0.027351    
2023-01-06 14:49:44,113 - Epoch: [179][   90/  246]    Overall Loss 0.218250    Objective Loss 0.218250                                        LR 0.000008    Time 0.026646    
2023-01-06 14:49:44,323 - Epoch: [179][  100/  246]    Overall Loss 0.220399    Objective Loss 0.220399                                        LR 0.000008    Time 0.026078    
2023-01-06 14:49:44,540 - Epoch: [179][  110/  246]    Overall Loss 0.221057    Objective Loss 0.221057                                        LR 0.000008    Time 0.025682    
2023-01-06 14:49:44,760 - Epoch: [179][  120/  246]    Overall Loss 0.221429    Objective Loss 0.221429                                        LR 0.000008    Time 0.025368    
2023-01-06 14:49:44,977 - Epoch: [179][  130/  246]    Overall Loss 0.221704    Objective Loss 0.221704                                        LR 0.000008    Time 0.025086    
2023-01-06 14:49:45,195 - Epoch: [179][  140/  246]    Overall Loss 0.220786    Objective Loss 0.220786                                        LR 0.000008    Time 0.024844    
2023-01-06 14:49:45,413 - Epoch: [179][  150/  246]    Overall Loss 0.219871    Objective Loss 0.219871                                        LR 0.000008    Time 0.024640    
2023-01-06 14:49:45,630 - Epoch: [179][  160/  246]    Overall Loss 0.219004    Objective Loss 0.219004                                        LR 0.000008    Time 0.024451    
2023-01-06 14:49:45,840 - Epoch: [179][  170/  246]    Overall Loss 0.219068    Objective Loss 0.219068                                        LR 0.000008    Time 0.024245    
2023-01-06 14:49:46,055 - Epoch: [179][  180/  246]    Overall Loss 0.219728    Objective Loss 0.219728                                        LR 0.000008    Time 0.024091    
2023-01-06 14:49:46,276 - Epoch: [179][  190/  246]    Overall Loss 0.219665    Objective Loss 0.219665                                        LR 0.000008    Time 0.023984    
2023-01-06 14:49:46,494 - Epoch: [179][  200/  246]    Overall Loss 0.220009    Objective Loss 0.220009                                        LR 0.000008    Time 0.023875    
2023-01-06 14:49:46,713 - Epoch: [179][  210/  246]    Overall Loss 0.220377    Objective Loss 0.220377                                        LR 0.000008    Time 0.023777    
2023-01-06 14:49:46,932 - Epoch: [179][  220/  246]    Overall Loss 0.220460    Objective Loss 0.220460                                        LR 0.000008    Time 0.023689    
2023-01-06 14:49:47,153 - Epoch: [179][  230/  246]    Overall Loss 0.220619    Objective Loss 0.220619                                        LR 0.000008    Time 0.023619    
2023-01-06 14:49:47,386 - Epoch: [179][  240/  246]    Overall Loss 0.220249    Objective Loss 0.220249                                        LR 0.000008    Time 0.023606    
2023-01-06 14:49:47,514 - Epoch: [179][  246/  246]    Overall Loss 0.220632    Objective Loss 0.220632    Top1 92.105263    LR 0.000008    Time 0.023548    
2023-01-06 14:49:47,633 - --- validate (epoch=179)-----------
2023-01-06 14:49:47,633 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:48,090 - Epoch: [179][   10/   28]    Loss 0.265297    Top1 90.351562    
2023-01-06 14:49:48,239 - Epoch: [179][   20/   28]    Loss 0.248744    Top1 90.878906    
2023-01-06 14:49:48,329 - Epoch: [179][   28/   28]    Loss 0.250567    Top1 90.996278    
2023-01-06 14:49:48,461 - ==> Top1: 90.996    Loss: 0.251

2023-01-06 14:49:48,462 - ==> Confusion:
[[ 257   13  169]
 [  13  299  290]
 [  70   74 5801]]

2023-01-06 14:49:48,463 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:48,463 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:48,473 - 

2023-01-06 14:49:48,473 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:49,214 - Epoch: [180][   10/  246]    Overall Loss 0.223588    Objective Loss 0.223588                                        LR 0.000005    Time 0.074010    
2023-01-06 14:49:49,483 - Epoch: [180][   20/  246]    Overall Loss 0.219034    Objective Loss 0.219034                                        LR 0.000005    Time 0.050442    
2023-01-06 14:49:49,749 - Epoch: [180][   30/  246]    Overall Loss 0.223519    Objective Loss 0.223519                                        LR 0.000005    Time 0.042485    
2023-01-06 14:49:50,016 - Epoch: [180][   40/  246]    Overall Loss 0.217855    Objective Loss 0.217855                                        LR 0.000005    Time 0.038522    
2023-01-06 14:49:50,282 - Epoch: [180][   50/  246]    Overall Loss 0.216430    Objective Loss 0.216430                                        LR 0.000005    Time 0.036121    
2023-01-06 14:49:50,554 - Epoch: [180][   60/  246]    Overall Loss 0.214819    Objective Loss 0.214819                                        LR 0.000005    Time 0.034639    
2023-01-06 14:49:50,828 - Epoch: [180][   70/  246]    Overall Loss 0.216921    Objective Loss 0.216921                                        LR 0.000005    Time 0.033590    
2023-01-06 14:49:51,095 - Epoch: [180][   80/  246]    Overall Loss 0.215039    Objective Loss 0.215039                                        LR 0.000005    Time 0.032710    
2023-01-06 14:49:51,350 - Epoch: [180][   90/  246]    Overall Loss 0.214609    Objective Loss 0.214609                                        LR 0.000005    Time 0.031897    
2023-01-06 14:49:51,614 - Epoch: [180][  100/  246]    Overall Loss 0.216960    Objective Loss 0.216960                                        LR 0.000005    Time 0.031350    
2023-01-06 14:49:51,871 - Epoch: [180][  110/  246]    Overall Loss 0.218765    Objective Loss 0.218765                                        LR 0.000005    Time 0.030826    
2023-01-06 14:49:52,131 - Epoch: [180][  120/  246]    Overall Loss 0.217929    Objective Loss 0.217929                                        LR 0.000005    Time 0.030417    
2023-01-06 14:49:52,387 - Epoch: [180][  130/  246]    Overall Loss 0.217256    Objective Loss 0.217256                                        LR 0.000005    Time 0.030043    
2023-01-06 14:49:52,648 - Epoch: [180][  140/  246]    Overall Loss 0.218231    Objective Loss 0.218231                                        LR 0.000005    Time 0.029759    
2023-01-06 14:49:52,905 - Epoch: [180][  150/  246]    Overall Loss 0.219496    Objective Loss 0.219496                                        LR 0.000005    Time 0.029488    
2023-01-06 14:49:53,166 - Epoch: [180][  160/  246]    Overall Loss 0.219528    Objective Loss 0.219528                                        LR 0.000005    Time 0.029272    
2023-01-06 14:49:53,426 - Epoch: [180][  170/  246]    Overall Loss 0.219651    Objective Loss 0.219651                                        LR 0.000005    Time 0.029072    
2023-01-06 14:49:53,685 - Epoch: [180][  180/  246]    Overall Loss 0.219977    Objective Loss 0.219977                                        LR 0.000005    Time 0.028898    
2023-01-06 14:49:53,944 - Epoch: [180][  190/  246]    Overall Loss 0.219858    Objective Loss 0.219858                                        LR 0.000005    Time 0.028735    
2023-01-06 14:49:54,173 - Epoch: [180][  200/  246]    Overall Loss 0.219649    Objective Loss 0.219649                                        LR 0.000005    Time 0.028443    
2023-01-06 14:49:54,405 - Epoch: [180][  210/  246]    Overall Loss 0.219054    Objective Loss 0.219054                                        LR 0.000005    Time 0.028187    
2023-01-06 14:49:54,633 - Epoch: [180][  220/  246]    Overall Loss 0.219617    Objective Loss 0.219617                                        LR 0.000005    Time 0.027941    
2023-01-06 14:49:54,866 - Epoch: [180][  230/  246]    Overall Loss 0.218940    Objective Loss 0.218940                                        LR 0.000005    Time 0.027737    
2023-01-06 14:49:55,111 - Epoch: [180][  240/  246]    Overall Loss 0.219089    Objective Loss 0.219089                                        LR 0.000005    Time 0.027603    
2023-01-06 14:49:55,235 - Epoch: [180][  246/  246]    Overall Loss 0.219258    Objective Loss 0.219258    Top1 91.148325    LR 0.000005    Time 0.027431    
2023-01-06 14:49:55,381 - --- validate (epoch=180)-----------
2023-01-06 14:49:55,382 - 6986 samples (256 per mini-batch)
2023-01-06 14:49:55,834 - Epoch: [180][   10/   28]    Loss 0.269481    Top1 90.273438    
2023-01-06 14:49:55,974 - Epoch: [180][   20/   28]    Loss 0.260227    Top1 90.605469    
2023-01-06 14:49:56,064 - Epoch: [180][   28/   28]    Loss 0.251868    Top1 90.924707    
2023-01-06 14:49:56,217 - ==> Top1: 90.925    Loss: 0.252

2023-01-06 14:49:56,217 - ==> Confusion:
[[ 252   16  171]
 [  13  307  282]
 [  65   87 5793]]

2023-01-06 14:49:56,219 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:49:56,219 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:49:56,229 - 

2023-01-06 14:49:56,229 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:49:56,797 - Epoch: [181][   10/  246]    Overall Loss 0.212334    Objective Loss 0.212334                                        LR 0.000005    Time 0.056720    
2023-01-06 14:49:57,026 - Epoch: [181][   20/  246]    Overall Loss 0.215381    Objective Loss 0.215381                                        LR 0.000005    Time 0.039784    
2023-01-06 14:49:57,258 - Epoch: [181][   30/  246]    Overall Loss 0.214218    Objective Loss 0.214218                                        LR 0.000005    Time 0.034229    
2023-01-06 14:49:57,484 - Epoch: [181][   40/  246]    Overall Loss 0.211797    Objective Loss 0.211797                                        LR 0.000005    Time 0.031312    
2023-01-06 14:49:57,719 - Epoch: [181][   50/  246]    Overall Loss 0.211340    Objective Loss 0.211340                                        LR 0.000005    Time 0.029740    
2023-01-06 14:49:57,952 - Epoch: [181][   60/  246]    Overall Loss 0.212728    Objective Loss 0.212728                                        LR 0.000005    Time 0.028666    
2023-01-06 14:49:58,182 - Epoch: [181][   70/  246]    Overall Loss 0.211007    Objective Loss 0.211007                                        LR 0.000005    Time 0.027851    
2023-01-06 14:49:58,413 - Epoch: [181][   80/  246]    Overall Loss 0.210129    Objective Loss 0.210129                                        LR 0.000005    Time 0.027232    
2023-01-06 14:49:58,643 - Epoch: [181][   90/  246]    Overall Loss 0.209898    Objective Loss 0.209898                                        LR 0.000005    Time 0.026756    
2023-01-06 14:49:58,874 - Epoch: [181][  100/  246]    Overall Loss 0.211605    Objective Loss 0.211605                                        LR 0.000005    Time 0.026394    
2023-01-06 14:49:59,103 - Epoch: [181][  110/  246]    Overall Loss 0.212667    Objective Loss 0.212667                                        LR 0.000005    Time 0.026066    
2023-01-06 14:49:59,337 - Epoch: [181][  120/  246]    Overall Loss 0.213595    Objective Loss 0.213595                                        LR 0.000005    Time 0.025841    
2023-01-06 14:49:59,576 - Epoch: [181][  130/  246]    Overall Loss 0.215304    Objective Loss 0.215304                                        LR 0.000005    Time 0.025686    
2023-01-06 14:49:59,800 - Epoch: [181][  140/  246]    Overall Loss 0.216473    Objective Loss 0.216473                                        LR 0.000005    Time 0.025453    
2023-01-06 14:50:00,033 - Epoch: [181][  150/  246]    Overall Loss 0.217117    Objective Loss 0.217117                                        LR 0.000005    Time 0.025308    
2023-01-06 14:50:00,279 - Epoch: [181][  160/  246]    Overall Loss 0.216866    Objective Loss 0.216866                                        LR 0.000005    Time 0.025260    
2023-01-06 14:50:00,527 - Epoch: [181][  170/  246]    Overall Loss 0.217396    Objective Loss 0.217396                                        LR 0.000005    Time 0.025224    
2023-01-06 14:50:00,777 - Epoch: [181][  180/  246]    Overall Loss 0.218373    Objective Loss 0.218373                                        LR 0.000005    Time 0.025198    
2023-01-06 14:50:01,026 - Epoch: [181][  190/  246]    Overall Loss 0.218446    Objective Loss 0.218446                                        LR 0.000005    Time 0.025177    
2023-01-06 14:50:01,266 - Epoch: [181][  200/  246]    Overall Loss 0.218545    Objective Loss 0.218545                                        LR 0.000005    Time 0.025111    
2023-01-06 14:50:01,529 - Epoch: [181][  210/  246]    Overall Loss 0.219046    Objective Loss 0.219046                                        LR 0.000005    Time 0.025159    
2023-01-06 14:50:01,769 - Epoch: [181][  220/  246]    Overall Loss 0.218207    Objective Loss 0.218207                                        LR 0.000005    Time 0.025105    
2023-01-06 14:50:01,986 - Epoch: [181][  230/  246]    Overall Loss 0.217941    Objective Loss 0.217941                                        LR 0.000005    Time 0.024957    
2023-01-06 14:50:02,220 - Epoch: [181][  240/  246]    Overall Loss 0.217987    Objective Loss 0.217987                                        LR 0.000005    Time 0.024891    
2023-01-06 14:50:02,336 - Epoch: [181][  246/  246]    Overall Loss 0.218167    Objective Loss 0.218167    Top1 91.387560    LR 0.000005    Time 0.024752    
2023-01-06 14:50:02,445 - --- validate (epoch=181)-----------
2023-01-06 14:50:02,445 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:02,890 - Epoch: [181][   10/   28]    Loss 0.227927    Top1 91.757812    
2023-01-06 14:50:03,031 - Epoch: [181][   20/   28]    Loss 0.240959    Top1 91.308594    
2023-01-06 14:50:03,122 - Epoch: [181][   28/   28]    Loss 0.249761    Top1 91.067850    
2023-01-06 14:50:03,259 - ==> Top1: 91.068    Loss: 0.250

2023-01-06 14:50:03,259 - ==> Confusion:
[[ 253    9  177]
 [  13  297  292]
 [  59   74 5812]]

2023-01-06 14:50:03,261 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:03,261 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:03,271 - 

2023-01-06 14:50:03,271 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:03,974 - Epoch: [182][   10/  246]    Overall Loss 0.216190    Objective Loss 0.216190                                        LR 0.000005    Time 0.070229    
2023-01-06 14:50:04,189 - Epoch: [182][   20/  246]    Overall Loss 0.215816    Objective Loss 0.215816                                        LR 0.000005    Time 0.045839    
2023-01-06 14:50:04,420 - Epoch: [182][   30/  246]    Overall Loss 0.217863    Objective Loss 0.217863                                        LR 0.000005    Time 0.038277    
2023-01-06 14:50:04,646 - Epoch: [182][   40/  246]    Overall Loss 0.214117    Objective Loss 0.214117                                        LR 0.000005    Time 0.034335    
2023-01-06 14:50:04,881 - Epoch: [182][   50/  246]    Overall Loss 0.218495    Objective Loss 0.218495                                        LR 0.000005    Time 0.032161    
2023-01-06 14:50:05,105 - Epoch: [182][   60/  246]    Overall Loss 0.217718    Objective Loss 0.217718                                        LR 0.000005    Time 0.030523    
2023-01-06 14:50:05,336 - Epoch: [182][   70/  246]    Overall Loss 0.219415    Objective Loss 0.219415                                        LR 0.000005    Time 0.029466    
2023-01-06 14:50:05,584 - Epoch: [182][   80/  246]    Overall Loss 0.217800    Objective Loss 0.217800                                        LR 0.000005    Time 0.028876    
2023-01-06 14:50:05,847 - Epoch: [182][   90/  246]    Overall Loss 0.216760    Objective Loss 0.216760                                        LR 0.000005    Time 0.028590    
2023-01-06 14:50:06,108 - Epoch: [182][  100/  246]    Overall Loss 0.217007    Objective Loss 0.217007                                        LR 0.000005    Time 0.028332    
2023-01-06 14:50:06,362 - Epoch: [182][  110/  246]    Overall Loss 0.217191    Objective Loss 0.217191                                        LR 0.000005    Time 0.028063    
2023-01-06 14:50:06,618 - Epoch: [182][  120/  246]    Overall Loss 0.217555    Objective Loss 0.217555                                        LR 0.000005    Time 0.027845    
2023-01-06 14:50:06,871 - Epoch: [182][  130/  246]    Overall Loss 0.216461    Objective Loss 0.216461                                        LR 0.000005    Time 0.027648    
2023-01-06 14:50:07,132 - Epoch: [182][  140/  246]    Overall Loss 0.215881    Objective Loss 0.215881                                        LR 0.000005    Time 0.027519    
2023-01-06 14:50:07,381 - Epoch: [182][  150/  246]    Overall Loss 0.215280    Objective Loss 0.215280                                        LR 0.000005    Time 0.027347    
2023-01-06 14:50:07,633 - Epoch: [182][  160/  246]    Overall Loss 0.215908    Objective Loss 0.215908                                        LR 0.000005    Time 0.027209    
2023-01-06 14:50:07,882 - Epoch: [182][  170/  246]    Overall Loss 0.215764    Objective Loss 0.215764                                        LR 0.000005    Time 0.027069    
2023-01-06 14:50:08,133 - Epoch: [182][  180/  246]    Overall Loss 0.216185    Objective Loss 0.216185                                        LR 0.000005    Time 0.026955    
2023-01-06 14:50:08,381 - Epoch: [182][  190/  246]    Overall Loss 0.216798    Objective Loss 0.216798                                        LR 0.000005    Time 0.026841    
2023-01-06 14:50:08,632 - Epoch: [182][  200/  246]    Overall Loss 0.217072    Objective Loss 0.217072                                        LR 0.000005    Time 0.026746    
2023-01-06 14:50:08,880 - Epoch: [182][  210/  246]    Overall Loss 0.218048    Objective Loss 0.218048                                        LR 0.000005    Time 0.026650    
2023-01-06 14:50:09,132 - Epoch: [182][  220/  246]    Overall Loss 0.217387    Objective Loss 0.217387                                        LR 0.000005    Time 0.026579    
2023-01-06 14:50:09,379 - Epoch: [182][  230/  246]    Overall Loss 0.218509    Objective Loss 0.218509                                        LR 0.000005    Time 0.026497    
2023-01-06 14:50:09,645 - Epoch: [182][  240/  246]    Overall Loss 0.217899    Objective Loss 0.217899                                        LR 0.000005    Time 0.026497    
2023-01-06 14:50:09,775 - Epoch: [182][  246/  246]    Overall Loss 0.218507    Objective Loss 0.218507    Top1 91.148325    LR 0.000005    Time 0.026380    
2023-01-06 14:50:09,904 - --- validate (epoch=182)-----------
2023-01-06 14:50:09,905 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:10,348 - Epoch: [182][   10/   28]    Loss 0.246343    Top1 91.093750    
2023-01-06 14:50:10,500 - Epoch: [182][   20/   28]    Loss 0.253718    Top1 90.507812    
2023-01-06 14:50:10,590 - Epoch: [182][   28/   28]    Loss 0.253059    Top1 90.624105    
2023-01-06 14:50:10,702 - ==> Top1: 90.624    Loss: 0.253

2023-01-06 14:50:10,703 - ==> Confusion:
[[ 266   16  157]
 [  12  301  289]
 [  76  105 5764]]

2023-01-06 14:50:10,704 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:10,704 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:10,714 - 

2023-01-06 14:50:10,714 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:11,325 - Epoch: [183][   10/  246]    Overall Loss 0.211307    Objective Loss 0.211307                                        LR 0.000005    Time 0.060949    
2023-01-06 14:50:11,577 - Epoch: [183][   20/  246]    Overall Loss 0.211153    Objective Loss 0.211153                                        LR 0.000005    Time 0.043092    
2023-01-06 14:50:11,823 - Epoch: [183][   30/  246]    Overall Loss 0.214300    Objective Loss 0.214300                                        LR 0.000005    Time 0.036907    
2023-01-06 14:50:12,033 - Epoch: [183][   40/  246]    Overall Loss 0.217498    Objective Loss 0.217498                                        LR 0.000005    Time 0.032912    
2023-01-06 14:50:12,270 - Epoch: [183][   50/  246]    Overall Loss 0.220363    Objective Loss 0.220363                                        LR 0.000005    Time 0.031064    
2023-01-06 14:50:12,516 - Epoch: [183][   60/  246]    Overall Loss 0.221042    Objective Loss 0.221042                                        LR 0.000005    Time 0.029986    
2023-01-06 14:50:12,737 - Epoch: [183][   70/  246]    Overall Loss 0.221894    Objective Loss 0.221894                                        LR 0.000005    Time 0.028842    
2023-01-06 14:50:12,943 - Epoch: [183][   80/  246]    Overall Loss 0.221804    Objective Loss 0.221804                                        LR 0.000005    Time 0.027811    
2023-01-06 14:50:13,149 - Epoch: [183][   90/  246]    Overall Loss 0.219587    Objective Loss 0.219587                                        LR 0.000005    Time 0.027006    
2023-01-06 14:50:13,389 - Epoch: [183][  100/  246]    Overall Loss 0.220768    Objective Loss 0.220768                                        LR 0.000005    Time 0.026705    
2023-01-06 14:50:13,630 - Epoch: [183][  110/  246]    Overall Loss 0.222578    Objective Loss 0.222578                                        LR 0.000005    Time 0.026449    
2023-01-06 14:50:13,859 - Epoch: [183][  120/  246]    Overall Loss 0.220567    Objective Loss 0.220567                                        LR 0.000005    Time 0.026146    
2023-01-06 14:50:14,105 - Epoch: [183][  130/  246]    Overall Loss 0.219844    Objective Loss 0.219844                                        LR 0.000005    Time 0.026013    
2023-01-06 14:50:14,353 - Epoch: [183][  140/  246]    Overall Loss 0.220629    Objective Loss 0.220629                                        LR 0.000005    Time 0.025926    
2023-01-06 14:50:14,604 - Epoch: [183][  150/  246]    Overall Loss 0.220469    Objective Loss 0.220469                                        LR 0.000005    Time 0.025857    
2023-01-06 14:50:14,852 - Epoch: [183][  160/  246]    Overall Loss 0.220782    Objective Loss 0.220782                                        LR 0.000005    Time 0.025781    
2023-01-06 14:50:15,084 - Epoch: [183][  170/  246]    Overall Loss 0.221467    Objective Loss 0.221467                                        LR 0.000005    Time 0.025620    
2023-01-06 14:50:15,326 - Epoch: [183][  180/  246]    Overall Loss 0.222223    Objective Loss 0.222223                                        LR 0.000005    Time 0.025542    
2023-01-06 14:50:15,575 - Epoch: [183][  190/  246]    Overall Loss 0.221379    Objective Loss 0.221379                                        LR 0.000005    Time 0.025504    
2023-01-06 14:50:15,824 - Epoch: [183][  200/  246]    Overall Loss 0.220452    Objective Loss 0.220452                                        LR 0.000005    Time 0.025470    
2023-01-06 14:50:16,075 - Epoch: [183][  210/  246]    Overall Loss 0.220542    Objective Loss 0.220542                                        LR 0.000005    Time 0.025452    
2023-01-06 14:50:16,332 - Epoch: [183][  220/  246]    Overall Loss 0.221043    Objective Loss 0.221043                                        LR 0.000005    Time 0.025460    
2023-01-06 14:50:16,596 - Epoch: [183][  230/  246]    Overall Loss 0.219535    Objective Loss 0.219535                                        LR 0.000005    Time 0.025498    
2023-01-06 14:50:16,867 - Epoch: [183][  240/  246]    Overall Loss 0.218977    Objective Loss 0.218977                                        LR 0.000005    Time 0.025564    
2023-01-06 14:50:16,997 - Epoch: [183][  246/  246]    Overall Loss 0.219015    Objective Loss 0.219015    Top1 90.909091    LR 0.000005    Time 0.025471    
2023-01-06 14:50:17,132 - --- validate (epoch=183)-----------
2023-01-06 14:50:17,132 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:17,717 - Epoch: [183][   10/   28]    Loss 0.249349    Top1 91.093750    
2023-01-06 14:50:17,856 - Epoch: [183][   20/   28]    Loss 0.250133    Top1 90.917969    
2023-01-06 14:50:17,945 - Epoch: [183][   28/   28]    Loss 0.252098    Top1 90.795877    
2023-01-06 14:50:18,073 - ==> Top1: 90.796    Loss: 0.252

2023-01-06 14:50:18,073 - ==> Confusion:
[[ 268   18  153]
 [  19  310  273]
 [  83   97 5765]]

2023-01-06 14:50:18,075 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:18,075 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:18,085 - 

2023-01-06 14:50:18,085 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:18,670 - Epoch: [184][   10/  246]    Overall Loss 0.215646    Objective Loss 0.215646                                        LR 0.000005    Time 0.058476    
2023-01-06 14:50:18,915 - Epoch: [184][   20/  246]    Overall Loss 0.219021    Objective Loss 0.219021                                        LR 0.000005    Time 0.041436    
2023-01-06 14:50:19,176 - Epoch: [184][   30/  246]    Overall Loss 0.221843    Objective Loss 0.221843                                        LR 0.000005    Time 0.036307    
2023-01-06 14:50:19,440 - Epoch: [184][   40/  246]    Overall Loss 0.217521    Objective Loss 0.217521                                        LR 0.000005    Time 0.033826    
2023-01-06 14:50:19,702 - Epoch: [184][   50/  246]    Overall Loss 0.216141    Objective Loss 0.216141                                        LR 0.000005    Time 0.032289    
2023-01-06 14:50:19,966 - Epoch: [184][   60/  246]    Overall Loss 0.216442    Objective Loss 0.216442                                        LR 0.000005    Time 0.031274    
2023-01-06 14:50:20,228 - Epoch: [184][   70/  246]    Overall Loss 0.216553    Objective Loss 0.216553                                        LR 0.000005    Time 0.030543    
2023-01-06 14:50:20,491 - Epoch: [184][   80/  246]    Overall Loss 0.220529    Objective Loss 0.220529                                        LR 0.000005    Time 0.029993    
2023-01-06 14:50:20,753 - Epoch: [184][   90/  246]    Overall Loss 0.219430    Objective Loss 0.219430                                        LR 0.000005    Time 0.029563    
2023-01-06 14:50:21,015 - Epoch: [184][  100/  246]    Overall Loss 0.218766    Objective Loss 0.218766                                        LR 0.000005    Time 0.029208    
2023-01-06 14:50:21,277 - Epoch: [184][  110/  246]    Overall Loss 0.218378    Objective Loss 0.218378                                        LR 0.000005    Time 0.028936    
2023-01-06 14:50:21,537 - Epoch: [184][  120/  246]    Overall Loss 0.217950    Objective Loss 0.217950                                        LR 0.000005    Time 0.028675    
2023-01-06 14:50:21,781 - Epoch: [184][  130/  246]    Overall Loss 0.218261    Objective Loss 0.218261                                        LR 0.000005    Time 0.028345    
2023-01-06 14:50:22,032 - Epoch: [184][  140/  246]    Overall Loss 0.218351    Objective Loss 0.218351                                        LR 0.000005    Time 0.028107    
2023-01-06 14:50:22,268 - Epoch: [184][  150/  246]    Overall Loss 0.217960    Objective Loss 0.217960                                        LR 0.000005    Time 0.027808    
2023-01-06 14:50:22,507 - Epoch: [184][  160/  246]    Overall Loss 0.218876    Objective Loss 0.218876                                        LR 0.000005    Time 0.027554    
2023-01-06 14:50:22,769 - Epoch: [184][  170/  246]    Overall Loss 0.218985    Objective Loss 0.218985                                        LR 0.000005    Time 0.027469    
2023-01-06 14:50:23,050 - Epoch: [184][  180/  246]    Overall Loss 0.218750    Objective Loss 0.218750                                        LR 0.000005    Time 0.027500    
2023-01-06 14:50:23,318 - Epoch: [184][  190/  246]    Overall Loss 0.218664    Objective Loss 0.218664                                        LR 0.000005    Time 0.027462    
2023-01-06 14:50:23,588 - Epoch: [184][  200/  246]    Overall Loss 0.219076    Objective Loss 0.219076                                        LR 0.000005    Time 0.027431    
2023-01-06 14:50:23,848 - Epoch: [184][  210/  246]    Overall Loss 0.219024    Objective Loss 0.219024                                        LR 0.000005    Time 0.027361    
2023-01-06 14:50:24,120 - Epoch: [184][  220/  246]    Overall Loss 0.219257    Objective Loss 0.219257                                        LR 0.000005    Time 0.027344    
2023-01-06 14:50:24,375 - Epoch: [184][  230/  246]    Overall Loss 0.218734    Objective Loss 0.218734                                        LR 0.000005    Time 0.027262    
2023-01-06 14:50:24,641 - Epoch: [184][  240/  246]    Overall Loss 0.218350    Objective Loss 0.218350                                        LR 0.000005    Time 0.027233    
2023-01-06 14:50:24,771 - Epoch: [184][  246/  246]    Overall Loss 0.217990    Objective Loss 0.217990    Top1 92.105263    LR 0.000005    Time 0.027098    
2023-01-06 14:50:24,908 - --- validate (epoch=184)-----------
2023-01-06 14:50:24,908 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:25,384 - Epoch: [184][   10/   28]    Loss 0.237883    Top1 91.250000    
2023-01-06 14:50:25,527 - Epoch: [184][   20/   28]    Loss 0.245536    Top1 90.800781    
2023-01-06 14:50:25,619 - Epoch: [184][   28/   28]    Loss 0.252114    Top1 90.767249    
2023-01-06 14:50:25,758 - ==> Top1: 90.767    Loss: 0.252

2023-01-06 14:50:25,758 - ==> Confusion:
[[ 258   18  163]
 [  17  305  280]
 [  72   95 5778]]

2023-01-06 14:50:25,760 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:25,760 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:25,770 - 

2023-01-06 14:50:25,770 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:26,496 - Epoch: [185][   10/  246]    Overall Loss 0.224554    Objective Loss 0.224554                                        LR 0.000005    Time 0.072505    
2023-01-06 14:50:26,738 - Epoch: [185][   20/  246]    Overall Loss 0.226933    Objective Loss 0.226933                                        LR 0.000005    Time 0.048325    
2023-01-06 14:50:26,995 - Epoch: [185][   30/  246]    Overall Loss 0.228032    Objective Loss 0.228032                                        LR 0.000005    Time 0.040790    
2023-01-06 14:50:27,251 - Epoch: [185][   40/  246]    Overall Loss 0.227279    Objective Loss 0.227279                                        LR 0.000005    Time 0.036968    
2023-01-06 14:50:27,509 - Epoch: [185][   50/  246]    Overall Loss 0.226711    Objective Loss 0.226711                                        LR 0.000005    Time 0.034730    
2023-01-06 14:50:27,767 - Epoch: [185][   60/  246]    Overall Loss 0.226830    Objective Loss 0.226830                                        LR 0.000005    Time 0.033234    
2023-01-06 14:50:28,030 - Epoch: [185][   70/  246]    Overall Loss 0.222205    Objective Loss 0.222205                                        LR 0.000005    Time 0.032226    
2023-01-06 14:50:28,286 - Epoch: [185][   80/  246]    Overall Loss 0.221506    Objective Loss 0.221506                                        LR 0.000005    Time 0.031394    
2023-01-06 14:50:28,539 - Epoch: [185][   90/  246]    Overall Loss 0.222239    Objective Loss 0.222239                                        LR 0.000005    Time 0.030715    
2023-01-06 14:50:28,779 - Epoch: [185][  100/  246]    Overall Loss 0.223101    Objective Loss 0.223101                                        LR 0.000005    Time 0.030041    
2023-01-06 14:50:29,022 - Epoch: [185][  110/  246]    Overall Loss 0.222118    Objective Loss 0.222118                                        LR 0.000005    Time 0.029508    
2023-01-06 14:50:29,263 - Epoch: [185][  120/  246]    Overall Loss 0.221192    Objective Loss 0.221192                                        LR 0.000005    Time 0.029055    
2023-01-06 14:50:29,504 - Epoch: [185][  130/  246]    Overall Loss 0.222338    Objective Loss 0.222338                                        LR 0.000005    Time 0.028675    
2023-01-06 14:50:29,754 - Epoch: [185][  140/  246]    Overall Loss 0.220958    Objective Loss 0.220958                                        LR 0.000005    Time 0.028409    
2023-01-06 14:50:30,007 - Epoch: [185][  150/  246]    Overall Loss 0.220158    Objective Loss 0.220158                                        LR 0.000005    Time 0.028196    
2023-01-06 14:50:30,260 - Epoch: [185][  160/  246]    Overall Loss 0.219253    Objective Loss 0.219253                                        LR 0.000005    Time 0.028013    
2023-01-06 14:50:30,515 - Epoch: [185][  170/  246]    Overall Loss 0.219017    Objective Loss 0.219017                                        LR 0.000005    Time 0.027860    
2023-01-06 14:50:30,768 - Epoch: [185][  180/  246]    Overall Loss 0.218724    Objective Loss 0.218724                                        LR 0.000005    Time 0.027704    
2023-01-06 14:50:31,023 - Epoch: [185][  190/  246]    Overall Loss 0.219501    Objective Loss 0.219501                                        LR 0.000005    Time 0.027587    
2023-01-06 14:50:31,276 - Epoch: [185][  200/  246]    Overall Loss 0.218741    Objective Loss 0.218741                                        LR 0.000005    Time 0.027467    
2023-01-06 14:50:31,528 - Epoch: [185][  210/  246]    Overall Loss 0.217592    Objective Loss 0.217592                                        LR 0.000005    Time 0.027357    
2023-01-06 14:50:31,771 - Epoch: [185][  220/  246]    Overall Loss 0.217983    Objective Loss 0.217983                                        LR 0.000005    Time 0.027216    
2023-01-06 14:50:32,016 - Epoch: [185][  230/  246]    Overall Loss 0.217600    Objective Loss 0.217600                                        LR 0.000005    Time 0.027096    
2023-01-06 14:50:32,268 - Epoch: [185][  240/  246]    Overall Loss 0.217978    Objective Loss 0.217978                                        LR 0.000005    Time 0.027017    
2023-01-06 14:50:32,398 - Epoch: [185][  246/  246]    Overall Loss 0.217478    Objective Loss 0.217478    Top1 92.344498    LR 0.000005    Time 0.026886    
2023-01-06 14:50:32,534 - --- validate (epoch=185)-----------
2023-01-06 14:50:32,534 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:32,988 - Epoch: [185][   10/   28]    Loss 0.236215    Top1 91.054688    
2023-01-06 14:50:33,133 - Epoch: [185][   20/   28]    Loss 0.244814    Top1 91.054688    
2023-01-06 14:50:33,226 - Epoch: [185][   28/   28]    Loss 0.248232    Top1 90.939021    
2023-01-06 14:50:33,361 - ==> Top1: 90.939    Loss: 0.248

2023-01-06 14:50:33,361 - ==> Confusion:
[[ 258   13  168]
 [  17  304  281]
 [  73   81 5791]]

2023-01-06 14:50:33,363 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:33,363 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:33,372 - 

2023-01-06 14:50:33,373 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:33,966 - Epoch: [186][   10/  246]    Overall Loss 0.210422    Objective Loss 0.210422                                        LR 0.000005    Time 0.059298    
2023-01-06 14:50:34,215 - Epoch: [186][   20/  246]    Overall Loss 0.203664    Objective Loss 0.203664                                        LR 0.000005    Time 0.042031    
2023-01-06 14:50:34,479 - Epoch: [186][   30/  246]    Overall Loss 0.205527    Objective Loss 0.205527                                        LR 0.000005    Time 0.036811    
2023-01-06 14:50:34,732 - Epoch: [186][   40/  246]    Overall Loss 0.208965    Objective Loss 0.208965                                        LR 0.000005    Time 0.033915    
2023-01-06 14:50:34,999 - Epoch: [186][   50/  246]    Overall Loss 0.206648    Objective Loss 0.206648                                        LR 0.000005    Time 0.032465    
2023-01-06 14:50:35,253 - Epoch: [186][   60/  246]    Overall Loss 0.207960    Objective Loss 0.207960                                        LR 0.000005    Time 0.031277    
2023-01-06 14:50:35,509 - Epoch: [186][   70/  246]    Overall Loss 0.211628    Objective Loss 0.211628                                        LR 0.000005    Time 0.030452    
2023-01-06 14:50:35,759 - Epoch: [186][   80/  246]    Overall Loss 0.212258    Objective Loss 0.212258                                        LR 0.000005    Time 0.029753    
2023-01-06 14:50:36,017 - Epoch: [186][   90/  246]    Overall Loss 0.213480    Objective Loss 0.213480                                        LR 0.000005    Time 0.029305    
2023-01-06 14:50:36,233 - Epoch: [186][  100/  246]    Overall Loss 0.213568    Objective Loss 0.213568                                        LR 0.000005    Time 0.028525    
2023-01-06 14:50:36,452 - Epoch: [186][  110/  246]    Overall Loss 0.213184    Objective Loss 0.213184                                        LR 0.000005    Time 0.027928    
2023-01-06 14:50:36,698 - Epoch: [186][  120/  246]    Overall Loss 0.214817    Objective Loss 0.214817                                        LR 0.000005    Time 0.027647    
2023-01-06 14:50:36,936 - Epoch: [186][  130/  246]    Overall Loss 0.214686    Objective Loss 0.214686                                        LR 0.000005    Time 0.027343    
2023-01-06 14:50:37,178 - Epoch: [186][  140/  246]    Overall Loss 0.216094    Objective Loss 0.216094                                        LR 0.000005    Time 0.027117    
2023-01-06 14:50:37,422 - Epoch: [186][  150/  246]    Overall Loss 0.216426    Objective Loss 0.216426                                        LR 0.000005    Time 0.026932    
2023-01-06 14:50:37,689 - Epoch: [186][  160/  246]    Overall Loss 0.216800    Objective Loss 0.216800                                        LR 0.000005    Time 0.026905    
2023-01-06 14:50:37,949 - Epoch: [186][  170/  246]    Overall Loss 0.215851    Objective Loss 0.215851                                        LR 0.000005    Time 0.026853    
2023-01-06 14:50:38,184 - Epoch: [186][  180/  246]    Overall Loss 0.215654    Objective Loss 0.215654                                        LR 0.000005    Time 0.026653    
2023-01-06 14:50:38,418 - Epoch: [186][  190/  246]    Overall Loss 0.216163    Objective Loss 0.216163                                        LR 0.000005    Time 0.026481    
2023-01-06 14:50:38,659 - Epoch: [186][  200/  246]    Overall Loss 0.217077    Objective Loss 0.217077                                        LR 0.000005    Time 0.026357    
2023-01-06 14:50:38,899 - Epoch: [186][  210/  246]    Overall Loss 0.217375    Objective Loss 0.217375                                        LR 0.000005    Time 0.026242    
2023-01-06 14:50:39,140 - Epoch: [186][  220/  246]    Overall Loss 0.217095    Objective Loss 0.217095                                        LR 0.000005    Time 0.026147    
2023-01-06 14:50:39,383 - Epoch: [186][  230/  246]    Overall Loss 0.217680    Objective Loss 0.217680                                        LR 0.000005    Time 0.026063    
2023-01-06 14:50:39,651 - Epoch: [186][  240/  246]    Overall Loss 0.217587    Objective Loss 0.217587                                        LR 0.000005    Time 0.026093    
2023-01-06 14:50:39,780 - Epoch: [186][  246/  246]    Overall Loss 0.218096    Objective Loss 0.218096    Top1 92.344498    LR 0.000005    Time 0.025977    
2023-01-06 14:50:39,925 - --- validate (epoch=186)-----------
2023-01-06 14:50:39,926 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:40,365 - Epoch: [186][   10/   28]    Loss 0.245237    Top1 90.820312    
2023-01-06 14:50:40,504 - Epoch: [186][   20/   28]    Loss 0.249907    Top1 90.664062    
2023-01-06 14:50:40,594 - Epoch: [186][   28/   28]    Loss 0.252886    Top1 90.881764    
2023-01-06 14:50:40,734 - ==> Top1: 90.882    Loss: 0.253

2023-01-06 14:50:40,734 - ==> Confusion:
[[ 262   13  164]
 [  17  294  291]
 [  71   81 5793]]

2023-01-06 14:50:40,736 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:40,736 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:40,746 - 

2023-01-06 14:50:40,746 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:41,448 - Epoch: [187][   10/  246]    Overall Loss 0.201768    Objective Loss 0.201768                                        LR 0.000005    Time 0.070137    
2023-01-06 14:50:41,684 - Epoch: [187][   20/  246]    Overall Loss 0.210242    Objective Loss 0.210242                                        LR 0.000005    Time 0.046828    
2023-01-06 14:50:41,938 - Epoch: [187][   30/  246]    Overall Loss 0.214677    Objective Loss 0.214677                                        LR 0.000005    Time 0.039616    
2023-01-06 14:50:42,203 - Epoch: [187][   40/  246]    Overall Loss 0.209479    Objective Loss 0.209479                                        LR 0.000005    Time 0.036321    
2023-01-06 14:50:42,455 - Epoch: [187][   50/  246]    Overall Loss 0.213345    Objective Loss 0.213345                                        LR 0.000005    Time 0.034101    
2023-01-06 14:50:42,709 - Epoch: [187][   60/  246]    Overall Loss 0.215876    Objective Loss 0.215876                                        LR 0.000005    Time 0.032639    
2023-01-06 14:50:42,963 - Epoch: [187][   70/  246]    Overall Loss 0.218690    Objective Loss 0.218690                                        LR 0.000005    Time 0.031598    
2023-01-06 14:50:43,215 - Epoch: [187][   80/  246]    Overall Loss 0.220320    Objective Loss 0.220320                                        LR 0.000005    Time 0.030786    
2023-01-06 14:50:43,465 - Epoch: [187][   90/  246]    Overall Loss 0.219675    Objective Loss 0.219675                                        LR 0.000005    Time 0.030147    
2023-01-06 14:50:43,716 - Epoch: [187][  100/  246]    Overall Loss 0.218590    Objective Loss 0.218590                                        LR 0.000005    Time 0.029639    
2023-01-06 14:50:43,971 - Epoch: [187][  110/  246]    Overall Loss 0.217508    Objective Loss 0.217508                                        LR 0.000005    Time 0.029238    
2023-01-06 14:50:44,214 - Epoch: [187][  120/  246]    Overall Loss 0.218777    Objective Loss 0.218777                                        LR 0.000005    Time 0.028824    
2023-01-06 14:50:44,467 - Epoch: [187][  130/  246]    Overall Loss 0.219148    Objective Loss 0.219148                                        LR 0.000005    Time 0.028547    
2023-01-06 14:50:44,716 - Epoch: [187][  140/  246]    Overall Loss 0.219476    Objective Loss 0.219476                                        LR 0.000005    Time 0.028276    
2023-01-06 14:50:44,968 - Epoch: [187][  150/  246]    Overall Loss 0.219795    Objective Loss 0.219795                                        LR 0.000005    Time 0.028055    
2023-01-06 14:50:45,215 - Epoch: [187][  160/  246]    Overall Loss 0.220229    Objective Loss 0.220229                                        LR 0.000005    Time 0.027844    
2023-01-06 14:50:45,467 - Epoch: [187][  170/  246]    Overall Loss 0.219980    Objective Loss 0.219980                                        LR 0.000005    Time 0.027672    
2023-01-06 14:50:45,715 - Epoch: [187][  180/  246]    Overall Loss 0.219606    Objective Loss 0.219606                                        LR 0.000005    Time 0.027510    
2023-01-06 14:50:45,965 - Epoch: [187][  190/  246]    Overall Loss 0.219560    Objective Loss 0.219560                                        LR 0.000005    Time 0.027371    
2023-01-06 14:50:46,217 - Epoch: [187][  200/  246]    Overall Loss 0.219340    Objective Loss 0.219340                                        LR 0.000005    Time 0.027258    
2023-01-06 14:50:46,466 - Epoch: [187][  210/  246]    Overall Loss 0.219068    Objective Loss 0.219068                                        LR 0.000005    Time 0.027143    
2023-01-06 14:50:46,714 - Epoch: [187][  220/  246]    Overall Loss 0.219153    Objective Loss 0.219153                                        LR 0.000005    Time 0.027027    
2023-01-06 14:50:46,959 - Epoch: [187][  230/  246]    Overall Loss 0.217761    Objective Loss 0.217761                                        LR 0.000005    Time 0.026917    
2023-01-06 14:50:47,220 - Epoch: [187][  240/  246]    Overall Loss 0.217617    Objective Loss 0.217617                                        LR 0.000005    Time 0.026881    
2023-01-06 14:50:47,351 - Epoch: [187][  246/  246]    Overall Loss 0.217791    Objective Loss 0.217791    Top1 92.344498    LR 0.000005    Time 0.026757    
2023-01-06 14:50:47,478 - --- validate (epoch=187)-----------
2023-01-06 14:50:47,478 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:47,918 - Epoch: [187][   10/   28]    Loss 0.263606    Top1 90.078125    
2023-01-06 14:50:48,060 - Epoch: [187][   20/   28]    Loss 0.253910    Top1 91.035156    
2023-01-06 14:50:48,150 - Epoch: [187][   28/   28]    Loss 0.248276    Top1 91.067850    
2023-01-06 14:50:48,321 - ==> Top1: 91.068    Loss: 0.248

2023-01-06 14:50:48,322 - ==> Confusion:
[[ 242   18  179]
 [  13  299  290]
 [  52   72 5821]]

2023-01-06 14:50:48,323 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:48,323 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:48,333 - 

2023-01-06 14:50:48,333 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:49,087 - Epoch: [188][   10/  246]    Overall Loss 0.221623    Objective Loss 0.221623                                        LR 0.000005    Time 0.075279    
2023-01-06 14:50:49,330 - Epoch: [188][   20/  246]    Overall Loss 0.211445    Objective Loss 0.211445                                        LR 0.000005    Time 0.049794    
2023-01-06 14:50:49,573 - Epoch: [188][   30/  246]    Overall Loss 0.227072    Objective Loss 0.227072                                        LR 0.000005    Time 0.041265    
2023-01-06 14:50:49,821 - Epoch: [188][   40/  246]    Overall Loss 0.228658    Objective Loss 0.228658                                        LR 0.000005    Time 0.037144    
2023-01-06 14:50:50,064 - Epoch: [188][   50/  246]    Overall Loss 0.226483    Objective Loss 0.226483                                        LR 0.000005    Time 0.034572    
2023-01-06 14:50:50,311 - Epoch: [188][   60/  246]    Overall Loss 0.225057    Objective Loss 0.225057                                        LR 0.000005    Time 0.032919    
2023-01-06 14:50:50,564 - Epoch: [188][   70/  246]    Overall Loss 0.223662    Objective Loss 0.223662                                        LR 0.000005    Time 0.031822    
2023-01-06 14:50:50,819 - Epoch: [188][   80/  246]    Overall Loss 0.222312    Objective Loss 0.222312                                        LR 0.000005    Time 0.031027    
2023-01-06 14:50:51,076 - Epoch: [188][   90/  246]    Overall Loss 0.220901    Objective Loss 0.220901                                        LR 0.000005    Time 0.030425    
2023-01-06 14:50:51,330 - Epoch: [188][  100/  246]    Overall Loss 0.220896    Objective Loss 0.220896                                        LR 0.000005    Time 0.029924    
2023-01-06 14:50:51,586 - Epoch: [188][  110/  246]    Overall Loss 0.221811    Objective Loss 0.221811                                        LR 0.000005    Time 0.029529    
2023-01-06 14:50:51,840 - Epoch: [188][  120/  246]    Overall Loss 0.220627    Objective Loss 0.220627                                        LR 0.000005    Time 0.029174    
2023-01-06 14:50:52,095 - Epoch: [188][  130/  246]    Overall Loss 0.220350    Objective Loss 0.220350                                        LR 0.000005    Time 0.028890    
2023-01-06 14:50:52,343 - Epoch: [188][  140/  246]    Overall Loss 0.222325    Objective Loss 0.222325                                        LR 0.000005    Time 0.028601    
2023-01-06 14:50:52,585 - Epoch: [188][  150/  246]    Overall Loss 0.221840    Objective Loss 0.221840                                        LR 0.000005    Time 0.028303    
2023-01-06 14:50:52,835 - Epoch: [188][  160/  246]    Overall Loss 0.221161    Objective Loss 0.221161                                        LR 0.000005    Time 0.028093    
2023-01-06 14:50:53,087 - Epoch: [188][  170/  246]    Overall Loss 0.219849    Objective Loss 0.219849                                        LR 0.000005    Time 0.027920    
2023-01-06 14:50:53,338 - Epoch: [188][  180/  246]    Overall Loss 0.219136    Objective Loss 0.219136                                        LR 0.000005    Time 0.027763    
2023-01-06 14:50:53,593 - Epoch: [188][  190/  246]    Overall Loss 0.219415    Objective Loss 0.219415                                        LR 0.000005    Time 0.027639    
2023-01-06 14:50:53,846 - Epoch: [188][  200/  246]    Overall Loss 0.218603    Objective Loss 0.218603                                        LR 0.000005    Time 0.027520    
2023-01-06 14:50:54,093 - Epoch: [188][  210/  246]    Overall Loss 0.218279    Objective Loss 0.218279                                        LR 0.000005    Time 0.027388    
2023-01-06 14:50:54,341 - Epoch: [188][  220/  246]    Overall Loss 0.217645    Objective Loss 0.217645                                        LR 0.000005    Time 0.027266    
2023-01-06 14:50:54,593 - Epoch: [188][  230/  246]    Overall Loss 0.218406    Objective Loss 0.218406                                        LR 0.000005    Time 0.027173    
2023-01-06 14:50:54,851 - Epoch: [188][  240/  246]    Overall Loss 0.217466    Objective Loss 0.217466                                        LR 0.000005    Time 0.027114    
2023-01-06 14:50:54,979 - Epoch: [188][  246/  246]    Overall Loss 0.217457    Objective Loss 0.217457    Top1 92.583732    LR 0.000005    Time 0.026976    
2023-01-06 14:50:55,111 - --- validate (epoch=188)-----------
2023-01-06 14:50:55,112 - 6986 samples (256 per mini-batch)
2023-01-06 14:50:55,581 - Epoch: [188][   10/   28]    Loss 0.240207    Top1 91.757812    
2023-01-06 14:50:55,736 - Epoch: [188][   20/   28]    Loss 0.243220    Top1 91.503906    
2023-01-06 14:50:55,826 - Epoch: [188][   28/   28]    Loss 0.250009    Top1 91.039221    
2023-01-06 14:50:55,965 - ==> Top1: 91.039    Loss: 0.250

2023-01-06 14:50:55,965 - ==> Confusion:
[[ 275   14  150]
 [  18  310  274]
 [  75   95 5775]]

2023-01-06 14:50:55,967 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:50:55,967 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:50:55,977 - 

2023-01-06 14:50:55,977 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:50:56,563 - Epoch: [189][   10/  246]    Overall Loss 0.215315    Objective Loss 0.215315                                        LR 0.000005    Time 0.058511    
2023-01-06 14:50:56,806 - Epoch: [189][   20/  246]    Overall Loss 0.216736    Objective Loss 0.216736                                        LR 0.000005    Time 0.041400    
2023-01-06 14:50:57,057 - Epoch: [189][   30/  246]    Overall Loss 0.211823    Objective Loss 0.211823                                        LR 0.000005    Time 0.035952    
2023-01-06 14:50:57,307 - Epoch: [189][   40/  246]    Overall Loss 0.215983    Objective Loss 0.215983                                        LR 0.000005    Time 0.033203    
2023-01-06 14:50:57,554 - Epoch: [189][   50/  246]    Overall Loss 0.212951    Objective Loss 0.212951                                        LR 0.000005    Time 0.031501    
2023-01-06 14:50:57,807 - Epoch: [189][   60/  246]    Overall Loss 0.213413    Objective Loss 0.213413                                        LR 0.000005    Time 0.030460    
2023-01-06 14:50:58,066 - Epoch: [189][   70/  246]    Overall Loss 0.217286    Objective Loss 0.217286                                        LR 0.000005    Time 0.029794    
2023-01-06 14:50:58,315 - Epoch: [189][   80/  246]    Overall Loss 0.217597    Objective Loss 0.217597                                        LR 0.000005    Time 0.029184    
2023-01-06 14:50:58,570 - Epoch: [189][   90/  246]    Overall Loss 0.215879    Objective Loss 0.215879                                        LR 0.000005    Time 0.028742    
2023-01-06 14:50:58,821 - Epoch: [189][  100/  246]    Overall Loss 0.215827    Objective Loss 0.215827                                        LR 0.000005    Time 0.028374    
2023-01-06 14:50:59,071 - Epoch: [189][  110/  246]    Overall Loss 0.217379    Objective Loss 0.217379                                        LR 0.000005    Time 0.028071    
2023-01-06 14:50:59,323 - Epoch: [189][  120/  246]    Overall Loss 0.215865    Objective Loss 0.215865                                        LR 0.000005    Time 0.027827    
2023-01-06 14:50:59,574 - Epoch: [189][  130/  246]    Overall Loss 0.215036    Objective Loss 0.215036                                        LR 0.000005    Time 0.027612    
2023-01-06 14:50:59,817 - Epoch: [189][  140/  246]    Overall Loss 0.216042    Objective Loss 0.216042                                        LR 0.000005    Time 0.027375    
2023-01-06 14:51:00,064 - Epoch: [189][  150/  246]    Overall Loss 0.215258    Objective Loss 0.215258                                        LR 0.000005    Time 0.027192    
2023-01-06 14:51:00,314 - Epoch: [189][  160/  246]    Overall Loss 0.215747    Objective Loss 0.215747                                        LR 0.000005    Time 0.027057    
2023-01-06 14:51:00,565 - Epoch: [189][  170/  246]    Overall Loss 0.215734    Objective Loss 0.215734                                        LR 0.000005    Time 0.026935    
2023-01-06 14:51:00,815 - Epoch: [189][  180/  246]    Overall Loss 0.215735    Objective Loss 0.215735                                        LR 0.000005    Time 0.026830    
2023-01-06 14:51:01,066 - Epoch: [189][  190/  246]    Overall Loss 0.216117    Objective Loss 0.216117                                        LR 0.000005    Time 0.026735    
2023-01-06 14:51:01,318 - Epoch: [189][  200/  246]    Overall Loss 0.216806    Objective Loss 0.216806                                        LR 0.000005    Time 0.026655    
2023-01-06 14:51:01,562 - Epoch: [189][  210/  246]    Overall Loss 0.217323    Objective Loss 0.217323                                        LR 0.000005    Time 0.026545    
2023-01-06 14:51:01,804 - Epoch: [189][  220/  246]    Overall Loss 0.217617    Objective Loss 0.217617                                        LR 0.000005    Time 0.026438    
2023-01-06 14:51:02,044 - Epoch: [189][  230/  246]    Overall Loss 0.217592    Objective Loss 0.217592                                        LR 0.000005    Time 0.026333    
2023-01-06 14:51:02,297 - Epoch: [189][  240/  246]    Overall Loss 0.217601    Objective Loss 0.217601                                        LR 0.000005    Time 0.026288    
2023-01-06 14:51:02,427 - Epoch: [189][  246/  246]    Overall Loss 0.217754    Objective Loss 0.217754    Top1 91.387560    LR 0.000005    Time 0.026174    
2023-01-06 14:51:02,563 - --- validate (epoch=189)-----------
2023-01-06 14:51:02,564 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:03,006 - Epoch: [189][   10/   28]    Loss 0.228213    Top1 91.914062    
2023-01-06 14:51:03,154 - Epoch: [189][   20/   28]    Loss 0.250372    Top1 91.074219    
2023-01-06 14:51:03,246 - Epoch: [189][   28/   28]    Loss 0.249402    Top1 90.896078    
2023-01-06 14:51:03,379 - ==> Top1: 90.896    Loss: 0.249

2023-01-06 14:51:03,379 - ==> Confusion:
[[ 253   16  170]
 [  14  303  285]
 [  69   82 5794]]

2023-01-06 14:51:03,381 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:03,381 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:03,391 - 

2023-01-06 14:51:03,391 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:04,112 - Epoch: [190][   10/  246]    Overall Loss 0.209897    Objective Loss 0.209897                                        LR 0.000003    Time 0.072059    
2023-01-06 14:51:04,354 - Epoch: [190][   20/  246]    Overall Loss 0.218758    Objective Loss 0.218758                                        LR 0.000003    Time 0.048129    
2023-01-06 14:51:04,619 - Epoch: [190][   30/  246]    Overall Loss 0.222187    Objective Loss 0.222187                                        LR 0.000003    Time 0.040901    
2023-01-06 14:51:04,878 - Epoch: [190][   40/  246]    Overall Loss 0.226036    Objective Loss 0.226036                                        LR 0.000003    Time 0.037134    
2023-01-06 14:51:05,143 - Epoch: [190][   50/  246]    Overall Loss 0.220886    Objective Loss 0.220886                                        LR 0.000003    Time 0.034972    
2023-01-06 14:51:05,401 - Epoch: [190][   60/  246]    Overall Loss 0.219388    Objective Loss 0.219388                                        LR 0.000003    Time 0.033433    
2023-01-06 14:51:05,663 - Epoch: [190][   70/  246]    Overall Loss 0.220989    Objective Loss 0.220989                                        LR 0.000003    Time 0.032388    
2023-01-06 14:51:05,916 - Epoch: [190][   80/  246]    Overall Loss 0.221739    Objective Loss 0.221739                                        LR 0.000003    Time 0.031507    
2023-01-06 14:51:06,183 - Epoch: [190][   90/  246]    Overall Loss 0.221412    Objective Loss 0.221412                                        LR 0.000003    Time 0.030950    
2023-01-06 14:51:06,443 - Epoch: [190][  100/  246]    Overall Loss 0.221054    Objective Loss 0.221054                                        LR 0.000003    Time 0.030453    
2023-01-06 14:51:06,708 - Epoch: [190][  110/  246]    Overall Loss 0.220959    Objective Loss 0.220959                                        LR 0.000003    Time 0.030090    
2023-01-06 14:51:06,968 - Epoch: [190][  120/  246]    Overall Loss 0.220605    Objective Loss 0.220605                                        LR 0.000003    Time 0.029745    
2023-01-06 14:51:07,225 - Epoch: [190][  130/  246]    Overall Loss 0.220086    Objective Loss 0.220086                                        LR 0.000003    Time 0.029432    
2023-01-06 14:51:07,475 - Epoch: [190][  140/  246]    Overall Loss 0.219076    Objective Loss 0.219076                                        LR 0.000003    Time 0.029114    
2023-01-06 14:51:07,727 - Epoch: [190][  150/  246]    Overall Loss 0.218569    Objective Loss 0.218569                                        LR 0.000003    Time 0.028847    
2023-01-06 14:51:07,978 - Epoch: [190][  160/  246]    Overall Loss 0.216635    Objective Loss 0.216635                                        LR 0.000003    Time 0.028613    
2023-01-06 14:51:08,230 - Epoch: [190][  170/  246]    Overall Loss 0.216337    Objective Loss 0.216337                                        LR 0.000003    Time 0.028412    
2023-01-06 14:51:08,487 - Epoch: [190][  180/  246]    Overall Loss 0.218178    Objective Loss 0.218178                                        LR 0.000003    Time 0.028258    
2023-01-06 14:51:08,742 - Epoch: [190][  190/  246]    Overall Loss 0.218382    Objective Loss 0.218382                                        LR 0.000003    Time 0.028113    
2023-01-06 14:51:09,001 - Epoch: [190][  200/  246]    Overall Loss 0.218330    Objective Loss 0.218330                                        LR 0.000003    Time 0.027998    
2023-01-06 14:51:09,258 - Epoch: [190][  210/  246]    Overall Loss 0.218034    Objective Loss 0.218034                                        LR 0.000003    Time 0.027889    
2023-01-06 14:51:09,514 - Epoch: [190][  220/  246]    Overall Loss 0.217083    Objective Loss 0.217083                                        LR 0.000003    Time 0.027781    
2023-01-06 14:51:09,770 - Epoch: [190][  230/  246]    Overall Loss 0.217568    Objective Loss 0.217568                                        LR 0.000003    Time 0.027677    
2023-01-06 14:51:10,036 - Epoch: [190][  240/  246]    Overall Loss 0.217049    Objective Loss 0.217049                                        LR 0.000003    Time 0.027632    
2023-01-06 14:51:10,164 - Epoch: [190][  246/  246]    Overall Loss 0.217108    Objective Loss 0.217108    Top1 92.583732    LR 0.000003    Time 0.027474    
2023-01-06 14:51:10,289 - --- validate (epoch=190)-----------
2023-01-06 14:51:10,290 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:10,732 - Epoch: [190][   10/   28]    Loss 0.251290    Top1 90.585938    
2023-01-06 14:51:10,872 - Epoch: [190][   20/   28]    Loss 0.242000    Top1 91.093750    
2023-01-06 14:51:10,963 - Epoch: [190][   28/   28]    Loss 0.249918    Top1 90.853135    
2023-01-06 14:51:11,089 - ==> Top1: 90.853    Loss: 0.250

2023-01-06 14:51:11,090 - ==> Confusion:
[[ 258   20  161]
 [  14  301  287]
 [  73   84 5788]]

2023-01-06 14:51:11,091 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:11,091 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:11,101 - 

2023-01-06 14:51:11,101 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:11,694 - Epoch: [191][   10/  246]    Overall Loss 0.201312    Objective Loss 0.201312                                        LR 0.000003    Time 0.059237    
2023-01-06 14:51:11,938 - Epoch: [191][   20/  246]    Overall Loss 0.207898    Objective Loss 0.207898                                        LR 0.000003    Time 0.041787    
2023-01-06 14:51:12,187 - Epoch: [191][   30/  246]    Overall Loss 0.212063    Objective Loss 0.212063                                        LR 0.000003    Time 0.036135    
2023-01-06 14:51:12,442 - Epoch: [191][   40/  246]    Overall Loss 0.214796    Objective Loss 0.214796                                        LR 0.000003    Time 0.033488    
2023-01-06 14:51:12,700 - Epoch: [191][   50/  246]    Overall Loss 0.215980    Objective Loss 0.215980                                        LR 0.000003    Time 0.031937    
2023-01-06 14:51:12,956 - Epoch: [191][   60/  246]    Overall Loss 0.212766    Objective Loss 0.212766                                        LR 0.000003    Time 0.030862    
2023-01-06 14:51:13,205 - Epoch: [191][   70/  246]    Overall Loss 0.212288    Objective Loss 0.212288                                        LR 0.000003    Time 0.030014    
2023-01-06 14:51:13,457 - Epoch: [191][   80/  246]    Overall Loss 0.213876    Objective Loss 0.213876                                        LR 0.000003    Time 0.029403    
2023-01-06 14:51:13,707 - Epoch: [191][   90/  246]    Overall Loss 0.214600    Objective Loss 0.214600                                        LR 0.000003    Time 0.028912    
2023-01-06 14:51:13,958 - Epoch: [191][  100/  246]    Overall Loss 0.213605    Objective Loss 0.213605                                        LR 0.000003    Time 0.028521    
2023-01-06 14:51:14,209 - Epoch: [191][  110/  246]    Overall Loss 0.214343    Objective Loss 0.214343                                        LR 0.000003    Time 0.028208    
2023-01-06 14:51:14,459 - Epoch: [191][  120/  246]    Overall Loss 0.213866    Objective Loss 0.213866                                        LR 0.000003    Time 0.027938    
2023-01-06 14:51:14,709 - Epoch: [191][  130/  246]    Overall Loss 0.213661    Objective Loss 0.213661                                        LR 0.000003    Time 0.027707    
2023-01-06 14:51:14,958 - Epoch: [191][  140/  246]    Overall Loss 0.214503    Objective Loss 0.214503                                        LR 0.000003    Time 0.027500    
2023-01-06 14:51:15,220 - Epoch: [191][  150/  246]    Overall Loss 0.214792    Objective Loss 0.214792                                        LR 0.000003    Time 0.027411    
2023-01-06 14:51:15,479 - Epoch: [191][  160/  246]    Overall Loss 0.214672    Objective Loss 0.214672                                        LR 0.000003    Time 0.027317    
2023-01-06 14:51:15,739 - Epoch: [191][  170/  246]    Overall Loss 0.215077    Objective Loss 0.215077                                        LR 0.000003    Time 0.027237    
2023-01-06 14:51:15,997 - Epoch: [191][  180/  246]    Overall Loss 0.215998    Objective Loss 0.215998                                        LR 0.000003    Time 0.027155    
2023-01-06 14:51:16,255 - Epoch: [191][  190/  246]    Overall Loss 0.215529    Objective Loss 0.215529                                        LR 0.000003    Time 0.027083    
2023-01-06 14:51:16,512 - Epoch: [191][  200/  246]    Overall Loss 0.216265    Objective Loss 0.216265                                        LR 0.000003    Time 0.027014    
2023-01-06 14:51:16,771 - Epoch: [191][  210/  246]    Overall Loss 0.214958    Objective Loss 0.214958                                        LR 0.000003    Time 0.026960    
2023-01-06 14:51:17,031 - Epoch: [191][  220/  246]    Overall Loss 0.214496    Objective Loss 0.214496                                        LR 0.000003    Time 0.026916    
2023-01-06 14:51:17,290 - Epoch: [191][  230/  246]    Overall Loss 0.213922    Objective Loss 0.213922                                        LR 0.000003    Time 0.026867    
2023-01-06 14:51:17,557 - Epoch: [191][  240/  246]    Overall Loss 0.214062    Objective Loss 0.214062                                        LR 0.000003    Time 0.026861    
2023-01-06 14:51:17,687 - Epoch: [191][  246/  246]    Overall Loss 0.214657    Objective Loss 0.214657    Top1 90.669856    LR 0.000003    Time 0.026731    
2023-01-06 14:51:17,844 - --- validate (epoch=191)-----------
2023-01-06 14:51:17,844 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:18,305 - Epoch: [191][   10/   28]    Loss 0.269999    Top1 90.468750    
2023-01-06 14:51:18,453 - Epoch: [191][   20/   28]    Loss 0.261253    Top1 90.761719    
2023-01-06 14:51:18,545 - Epoch: [191][   28/   28]    Loss 0.252654    Top1 90.939021    
2023-01-06 14:51:18,677 - ==> Top1: 90.939    Loss: 0.253

2023-01-06 14:51:18,678 - ==> Confusion:
[[ 268   14  157]
 [  17  298  287]
 [  83   75 5787]]

2023-01-06 14:51:18,679 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:18,679 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:18,689 - 

2023-01-06 14:51:18,689 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:19,423 - Epoch: [192][   10/  246]    Overall Loss 0.212447    Objective Loss 0.212447                                        LR 0.000003    Time 0.073256    
2023-01-06 14:51:19,665 - Epoch: [192][   20/  246]    Overall Loss 0.202695    Objective Loss 0.202695                                        LR 0.000003    Time 0.048747    
2023-01-06 14:51:19,911 - Epoch: [192][   30/  246]    Overall Loss 0.214621    Objective Loss 0.214621                                        LR 0.000003    Time 0.040657    
2023-01-06 14:51:20,158 - Epoch: [192][   40/  246]    Overall Loss 0.213167    Objective Loss 0.213167                                        LR 0.000003    Time 0.036672    
2023-01-06 14:51:20,406 - Epoch: [192][   50/  246]    Overall Loss 0.215404    Objective Loss 0.215404                                        LR 0.000003    Time 0.034285    
2023-01-06 14:51:20,653 - Epoch: [192][   60/  246]    Overall Loss 0.218369    Objective Loss 0.218369                                        LR 0.000003    Time 0.032673    
2023-01-06 14:51:20,896 - Epoch: [192][   70/  246]    Overall Loss 0.216176    Objective Loss 0.216176                                        LR 0.000003    Time 0.031482    
2023-01-06 14:51:21,144 - Epoch: [192][   80/  246]    Overall Loss 0.216980    Objective Loss 0.216980                                        LR 0.000003    Time 0.030641    
2023-01-06 14:51:21,388 - Epoch: [192][   90/  246]    Overall Loss 0.215963    Objective Loss 0.215963                                        LR 0.000003    Time 0.029939    
2023-01-06 14:51:21,635 - Epoch: [192][  100/  246]    Overall Loss 0.216246    Objective Loss 0.216246                                        LR 0.000003    Time 0.029409    
2023-01-06 14:51:21,878 - Epoch: [192][  110/  246]    Overall Loss 0.216306    Objective Loss 0.216306                                        LR 0.000003    Time 0.028940    
2023-01-06 14:51:22,122 - Epoch: [192][  120/  246]    Overall Loss 0.216529    Objective Loss 0.216529                                        LR 0.000003    Time 0.028563    
2023-01-06 14:51:22,363 - Epoch: [192][  130/  246]    Overall Loss 0.217738    Objective Loss 0.217738                                        LR 0.000003    Time 0.028214    
2023-01-06 14:51:22,608 - Epoch: [192][  140/  246]    Overall Loss 0.217195    Objective Loss 0.217195                                        LR 0.000003    Time 0.027945    
2023-01-06 14:51:22,848 - Epoch: [192][  150/  246]    Overall Loss 0.216879    Objective Loss 0.216879                                        LR 0.000003    Time 0.027681    
2023-01-06 14:51:23,093 - Epoch: [192][  160/  246]    Overall Loss 0.216863    Objective Loss 0.216863                                        LR 0.000003    Time 0.027482    
2023-01-06 14:51:23,335 - Epoch: [192][  170/  246]    Overall Loss 0.216114    Objective Loss 0.216114                                        LR 0.000003    Time 0.027285    
2023-01-06 14:51:23,577 - Epoch: [192][  180/  246]    Overall Loss 0.215006    Objective Loss 0.215006                                        LR 0.000003    Time 0.027112    
2023-01-06 14:51:23,819 - Epoch: [192][  190/  246]    Overall Loss 0.214982    Objective Loss 0.214982                                        LR 0.000003    Time 0.026955    
2023-01-06 14:51:24,059 - Epoch: [192][  200/  246]    Overall Loss 0.215276    Objective Loss 0.215276                                        LR 0.000003    Time 0.026804    
2023-01-06 14:51:24,305 - Epoch: [192][  210/  246]    Overall Loss 0.215924    Objective Loss 0.215924                                        LR 0.000003    Time 0.026701    
2023-01-06 14:51:24,556 - Epoch: [192][  220/  246]    Overall Loss 0.216663    Objective Loss 0.216663                                        LR 0.000003    Time 0.026623    
2023-01-06 14:51:24,804 - Epoch: [192][  230/  246]    Overall Loss 0.217203    Objective Loss 0.217203                                        LR 0.000003    Time 0.026543    
2023-01-06 14:51:25,062 - Epoch: [192][  240/  246]    Overall Loss 0.217196    Objective Loss 0.217196                                        LR 0.000003    Time 0.026511    
2023-01-06 14:51:25,190 - Epoch: [192][  246/  246]    Overall Loss 0.217198    Objective Loss 0.217198    Top1 95.215311    LR 0.000003    Time 0.026385    
2023-01-06 14:51:25,329 - --- validate (epoch=192)-----------
2023-01-06 14:51:25,329 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:25,772 - Epoch: [192][   10/   28]    Loss 0.247466    Top1 90.898438    
2023-01-06 14:51:25,912 - Epoch: [192][   20/   28]    Loss 0.259668    Top1 90.664062    
2023-01-06 14:51:26,003 - Epoch: [192][   28/   28]    Loss 0.251145    Top1 90.896078    
2023-01-06 14:51:26,135 - ==> Top1: 90.896    Loss: 0.251

2023-01-06 14:51:26,135 - ==> Confusion:
[[ 247   19  173]
 [  13  308  281]
 [  62   88 5795]]

2023-01-06 14:51:26,137 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:26,137 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:26,147 - 

2023-01-06 14:51:26,147 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:26,874 - Epoch: [193][   10/  246]    Overall Loss 0.206723    Objective Loss 0.206723                                        LR 0.000003    Time 0.072641    
2023-01-06 14:51:27,107 - Epoch: [193][   20/  246]    Overall Loss 0.207559    Objective Loss 0.207559                                        LR 0.000003    Time 0.047942    
2023-01-06 14:51:27,366 - Epoch: [193][   30/  246]    Overall Loss 0.206927    Objective Loss 0.206927                                        LR 0.000003    Time 0.040565    
2023-01-06 14:51:27,622 - Epoch: [193][   40/  246]    Overall Loss 0.206969    Objective Loss 0.206969                                        LR 0.000003    Time 0.036811    
2023-01-06 14:51:27,879 - Epoch: [193][   50/  246]    Overall Loss 0.204114    Objective Loss 0.204114                                        LR 0.000003    Time 0.034590    
2023-01-06 14:51:28,131 - Epoch: [193][   60/  246]    Overall Loss 0.207241    Objective Loss 0.207241                                        LR 0.000003    Time 0.033007    
2023-01-06 14:51:28,390 - Epoch: [193][   70/  246]    Overall Loss 0.208743    Objective Loss 0.208743                                        LR 0.000003    Time 0.031993    
2023-01-06 14:51:28,658 - Epoch: [193][   80/  246]    Overall Loss 0.208877    Objective Loss 0.208877                                        LR 0.000003    Time 0.031328    
2023-01-06 14:51:28,924 - Epoch: [193][   90/  246]    Overall Loss 0.209579    Objective Loss 0.209579                                        LR 0.000003    Time 0.030802    
2023-01-06 14:51:29,176 - Epoch: [193][  100/  246]    Overall Loss 0.209915    Objective Loss 0.209915                                        LR 0.000003    Time 0.030233    
2023-01-06 14:51:29,442 - Epoch: [193][  110/  246]    Overall Loss 0.209276    Objective Loss 0.209276                                        LR 0.000003    Time 0.029901    
2023-01-06 14:51:29,698 - Epoch: [193][  120/  246]    Overall Loss 0.211651    Objective Loss 0.211651                                        LR 0.000003    Time 0.029536    
2023-01-06 14:51:29,964 - Epoch: [193][  130/  246]    Overall Loss 0.212720    Objective Loss 0.212720                                        LR 0.000003    Time 0.029305    
2023-01-06 14:51:30,192 - Epoch: [193][  140/  246]    Overall Loss 0.214002    Objective Loss 0.214002                                        LR 0.000003    Time 0.028835    
2023-01-06 14:51:30,414 - Epoch: [193][  150/  246]    Overall Loss 0.213698    Objective Loss 0.213698                                        LR 0.000003    Time 0.028393    
2023-01-06 14:51:30,655 - Epoch: [193][  160/  246]    Overall Loss 0.214045    Objective Loss 0.214045                                        LR 0.000003    Time 0.028122    
2023-01-06 14:51:30,924 - Epoch: [193][  170/  246]    Overall Loss 0.214127    Objective Loss 0.214127                                        LR 0.000003    Time 0.028049    
2023-01-06 14:51:31,185 - Epoch: [193][  180/  246]    Overall Loss 0.214588    Objective Loss 0.214588                                        LR 0.000003    Time 0.027934    
2023-01-06 14:51:31,446 - Epoch: [193][  190/  246]    Overall Loss 0.213663    Objective Loss 0.213663                                        LR 0.000003    Time 0.027837    
2023-01-06 14:51:31,707 - Epoch: [193][  200/  246]    Overall Loss 0.213500    Objective Loss 0.213500                                        LR 0.000003    Time 0.027746    
2023-01-06 14:51:31,976 - Epoch: [193][  210/  246]    Overall Loss 0.214624    Objective Loss 0.214624                                        LR 0.000003    Time 0.027697    
2023-01-06 14:51:32,234 - Epoch: [193][  220/  246]    Overall Loss 0.214982    Objective Loss 0.214982                                        LR 0.000003    Time 0.027606    
2023-01-06 14:51:32,481 - Epoch: [193][  230/  246]    Overall Loss 0.216349    Objective Loss 0.216349                                        LR 0.000003    Time 0.027481    
2023-01-06 14:51:32,746 - Epoch: [193][  240/  246]    Overall Loss 0.216174    Objective Loss 0.216174                                        LR 0.000003    Time 0.027436    
2023-01-06 14:51:32,874 - Epoch: [193][  246/  246]    Overall Loss 0.215941    Objective Loss 0.215941    Top1 92.344498    LR 0.000003    Time 0.027288    
2023-01-06 14:51:33,020 - --- validate (epoch=193)-----------
2023-01-06 14:51:33,020 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:33,471 - Epoch: [193][   10/   28]    Loss 0.234288    Top1 91.835938    
2023-01-06 14:51:33,624 - Epoch: [193][   20/   28]    Loss 0.242354    Top1 91.308594    
2023-01-06 14:51:33,720 - Epoch: [193][   28/   28]    Loss 0.248722    Top1 91.024907    
2023-01-06 14:51:33,826 - ==> Top1: 91.025    Loss: 0.249

2023-01-06 14:51:33,826 - ==> Confusion:
[[ 262   13  164]
 [  14  299  289]
 [  66   81 5798]]

2023-01-06 14:51:33,828 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:33,828 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:33,839 - 

2023-01-06 14:51:33,839 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:34,476 - Epoch: [194][   10/  246]    Overall Loss 0.213310    Objective Loss 0.213310                                        LR 0.000003    Time 0.063666    
2023-01-06 14:51:34,733 - Epoch: [194][   20/  246]    Overall Loss 0.216948    Objective Loss 0.216948                                        LR 0.000003    Time 0.044660    
2023-01-06 14:51:34,986 - Epoch: [194][   30/  246]    Overall Loss 0.218406    Objective Loss 0.218406                                        LR 0.000003    Time 0.038176    
2023-01-06 14:51:35,234 - Epoch: [194][   40/  246]    Overall Loss 0.222803    Objective Loss 0.222803                                        LR 0.000003    Time 0.034832    
2023-01-06 14:51:35,479 - Epoch: [194][   50/  246]    Overall Loss 0.222444    Objective Loss 0.222444                                        LR 0.000003    Time 0.032762    
2023-01-06 14:51:35,729 - Epoch: [194][   60/  246]    Overall Loss 0.219524    Objective Loss 0.219524                                        LR 0.000003    Time 0.031454    
2023-01-06 14:51:35,979 - Epoch: [194][   70/  246]    Overall Loss 0.217170    Objective Loss 0.217170                                        LR 0.000003    Time 0.030518    
2023-01-06 14:51:36,231 - Epoch: [194][   80/  246]    Overall Loss 0.216623    Objective Loss 0.216623                                        LR 0.000003    Time 0.029845    
2023-01-06 14:51:36,478 - Epoch: [194][   90/  246]    Overall Loss 0.216150    Objective Loss 0.216150                                        LR 0.000003    Time 0.029275    
2023-01-06 14:51:36,721 - Epoch: [194][  100/  246]    Overall Loss 0.216180    Objective Loss 0.216180                                        LR 0.000003    Time 0.028773    
2023-01-06 14:51:36,962 - Epoch: [194][  110/  246]    Overall Loss 0.216371    Objective Loss 0.216371                                        LR 0.000003    Time 0.028345    
2023-01-06 14:51:37,205 - Epoch: [194][  120/  246]    Overall Loss 0.215878    Objective Loss 0.215878                                        LR 0.000003    Time 0.028004    
2023-01-06 14:51:37,448 - Epoch: [194][  130/  246]    Overall Loss 0.215459    Objective Loss 0.215459                                        LR 0.000003    Time 0.027710    
2023-01-06 14:51:37,684 - Epoch: [194][  140/  246]    Overall Loss 0.216933    Objective Loss 0.216933                                        LR 0.000003    Time 0.027404    
2023-01-06 14:51:37,929 - Epoch: [194][  150/  246]    Overall Loss 0.218469    Objective Loss 0.218469                                        LR 0.000003    Time 0.027207    
2023-01-06 14:51:38,181 - Epoch: [194][  160/  246]    Overall Loss 0.217997    Objective Loss 0.217997                                        LR 0.000003    Time 0.027080    
2023-01-06 14:51:38,436 - Epoch: [194][  170/  246]    Overall Loss 0.217655    Objective Loss 0.217655                                        LR 0.000003    Time 0.026986    
2023-01-06 14:51:38,689 - Epoch: [194][  180/  246]    Overall Loss 0.217416    Objective Loss 0.217416                                        LR 0.000003    Time 0.026887    
2023-01-06 14:51:38,944 - Epoch: [194][  190/  246]    Overall Loss 0.218118    Objective Loss 0.218118                                        LR 0.000003    Time 0.026812    
2023-01-06 14:51:39,199 - Epoch: [194][  200/  246]    Overall Loss 0.217385    Objective Loss 0.217385                                        LR 0.000003    Time 0.026743    
2023-01-06 14:51:39,448 - Epoch: [194][  210/  246]    Overall Loss 0.217281    Objective Loss 0.217281                                        LR 0.000003    Time 0.026652    
2023-01-06 14:51:39,700 - Epoch: [194][  220/  246]    Overall Loss 0.217364    Objective Loss 0.217364                                        LR 0.000003    Time 0.026584    
2023-01-06 14:51:39,948 - Epoch: [194][  230/  246]    Overall Loss 0.216444    Objective Loss 0.216444                                        LR 0.000003    Time 0.026508    
2023-01-06 14:51:40,212 - Epoch: [194][  240/  246]    Overall Loss 0.216321    Objective Loss 0.216321                                        LR 0.000003    Time 0.026493    
2023-01-06 14:51:40,342 - Epoch: [194][  246/  246]    Overall Loss 0.216415    Objective Loss 0.216415    Top1 92.344498    LR 0.000003    Time 0.026376    
2023-01-06 14:51:40,463 - --- validate (epoch=194)-----------
2023-01-06 14:51:40,463 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:40,911 - Epoch: [194][   10/   28]    Loss 0.233707    Top1 91.093750    
2023-01-06 14:51:41,052 - Epoch: [194][   20/   28]    Loss 0.251757    Top1 90.605469    
2023-01-06 14:51:41,144 - Epoch: [194][   28/   28]    Loss 0.248287    Top1 90.881764    
2023-01-06 14:51:41,276 - ==> Top1: 90.882    Loss: 0.248

2023-01-06 14:51:41,276 - ==> Confusion:
[[ 267   12  160]
 [  17  297  288]
 [  80   80 5785]]

2023-01-06 14:51:41,278 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:41,278 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:41,288 - 

2023-01-06 14:51:41,288 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:41,994 - Epoch: [195][   10/  246]    Overall Loss 0.233497    Objective Loss 0.233497                                        LR 0.000003    Time 0.070459    
2023-01-06 14:51:42,248 - Epoch: [195][   20/  246]    Overall Loss 0.225646    Objective Loss 0.225646                                        LR 0.000003    Time 0.047950    
2023-01-06 14:51:42,511 - Epoch: [195][   30/  246]    Overall Loss 0.226138    Objective Loss 0.226138                                        LR 0.000003    Time 0.040706    
2023-01-06 14:51:42,775 - Epoch: [195][   40/  246]    Overall Loss 0.222219    Objective Loss 0.222219                                        LR 0.000003    Time 0.037128    
2023-01-06 14:51:43,011 - Epoch: [195][   50/  246]    Overall Loss 0.219919    Objective Loss 0.219919                                        LR 0.000003    Time 0.034407    
2023-01-06 14:51:43,224 - Epoch: [195][   60/  246]    Overall Loss 0.218183    Objective Loss 0.218183                                        LR 0.000003    Time 0.032219    
2023-01-06 14:51:43,444 - Epoch: [195][   70/  246]    Overall Loss 0.218734    Objective Loss 0.218734                                        LR 0.000003    Time 0.030755    
2023-01-06 14:51:43,667 - Epoch: [195][   80/  246]    Overall Loss 0.221186    Objective Loss 0.221186                                        LR 0.000003    Time 0.029695    
2023-01-06 14:51:43,887 - Epoch: [195][   90/  246]    Overall Loss 0.219826    Objective Loss 0.219826                                        LR 0.000003    Time 0.028829    
2023-01-06 14:51:44,108 - Epoch: [195][  100/  246]    Overall Loss 0.219848    Objective Loss 0.219848                                        LR 0.000003    Time 0.028151    
2023-01-06 14:51:44,330 - Epoch: [195][  110/  246]    Overall Loss 0.219959    Objective Loss 0.219959                                        LR 0.000003    Time 0.027607    
2023-01-06 14:51:44,550 - Epoch: [195][  120/  246]    Overall Loss 0.220118    Objective Loss 0.220118                                        LR 0.000003    Time 0.027141    
2023-01-06 14:51:44,786 - Epoch: [195][  130/  246]    Overall Loss 0.218085    Objective Loss 0.218085                                        LR 0.000003    Time 0.026862    
2023-01-06 14:51:45,051 - Epoch: [195][  140/  246]    Overall Loss 0.218202    Objective Loss 0.218202                                        LR 0.000003    Time 0.026821    
2023-01-06 14:51:45,304 - Epoch: [195][  150/  246]    Overall Loss 0.217160    Objective Loss 0.217160                                        LR 0.000003    Time 0.026718    
2023-01-06 14:51:45,560 - Epoch: [195][  160/  246]    Overall Loss 0.218171    Objective Loss 0.218171                                        LR 0.000003    Time 0.026642    
2023-01-06 14:51:45,825 - Epoch: [195][  170/  246]    Overall Loss 0.216798    Objective Loss 0.216798                                        LR 0.000003    Time 0.026631    
2023-01-06 14:51:46,087 - Epoch: [195][  180/  246]    Overall Loss 0.216739    Objective Loss 0.216739                                        LR 0.000003    Time 0.026607    
2023-01-06 14:51:46,352 - Epoch: [195][  190/  246]    Overall Loss 0.216775    Objective Loss 0.216775                                        LR 0.000003    Time 0.026595    
2023-01-06 14:51:46,613 - Epoch: [195][  200/  246]    Overall Loss 0.216714    Objective Loss 0.216714                                        LR 0.000003    Time 0.026571    
2023-01-06 14:51:46,877 - Epoch: [195][  210/  246]    Overall Loss 0.216602    Objective Loss 0.216602                                        LR 0.000003    Time 0.026559    
2023-01-06 14:51:47,138 - Epoch: [195][  220/  246]    Overall Loss 0.216459    Objective Loss 0.216459                                        LR 0.000003    Time 0.026537    
2023-01-06 14:51:47,401 - Epoch: [195][  230/  246]    Overall Loss 0.216624    Objective Loss 0.216624                                        LR 0.000003    Time 0.026525    
2023-01-06 14:51:47,671 - Epoch: [195][  240/  246]    Overall Loss 0.216148    Objective Loss 0.216148                                        LR 0.000003    Time 0.026541    
2023-01-06 14:51:47,803 - Epoch: [195][  246/  246]    Overall Loss 0.216539    Objective Loss 0.216539    Top1 91.148325    LR 0.000003    Time 0.026431    
2023-01-06 14:51:47,955 - --- validate (epoch=195)-----------
2023-01-06 14:51:47,956 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:48,409 - Epoch: [195][   10/   28]    Loss 0.232568    Top1 91.679688    
2023-01-06 14:51:48,553 - Epoch: [195][   20/   28]    Loss 0.247575    Top1 91.152344    
2023-01-06 14:51:48,645 - Epoch: [195][   28/   28]    Loss 0.246763    Top1 91.125107    
2023-01-06 14:51:48,781 - ==> Top1: 91.125    Loss: 0.247

2023-01-06 14:51:48,781 - ==> Confusion:
[[ 261   15  163]
 [  16  304  282]
 [  70   74 5801]]

2023-01-06 14:51:48,783 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:48,783 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:48,793 - 

2023-01-06 14:51:48,793 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:49,394 - Epoch: [196][   10/  246]    Overall Loss 0.225370    Objective Loss 0.225370                                        LR 0.000003    Time 0.060062    
2023-01-06 14:51:49,642 - Epoch: [196][   20/  246]    Overall Loss 0.219300    Objective Loss 0.219300                                        LR 0.000003    Time 0.042406    
2023-01-06 14:51:49,908 - Epoch: [196][   30/  246]    Overall Loss 0.215538    Objective Loss 0.215538                                        LR 0.000003    Time 0.037113    
2023-01-06 14:51:50,154 - Epoch: [196][   40/  246]    Overall Loss 0.214215    Objective Loss 0.214215                                        LR 0.000003    Time 0.033966    
2023-01-06 14:51:50,420 - Epoch: [196][   50/  246]    Overall Loss 0.213920    Objective Loss 0.213920                                        LR 0.000003    Time 0.032482    
2023-01-06 14:51:50,685 - Epoch: [196][   60/  246]    Overall Loss 0.213541    Objective Loss 0.213541                                        LR 0.000003    Time 0.031479    
2023-01-06 14:51:50,961 - Epoch: [196][   70/  246]    Overall Loss 0.213298    Objective Loss 0.213298                                        LR 0.000003    Time 0.030932    
2023-01-06 14:51:51,225 - Epoch: [196][   80/  246]    Overall Loss 0.213698    Objective Loss 0.213698                                        LR 0.000003    Time 0.030350    
2023-01-06 14:51:51,491 - Epoch: [196][   90/  246]    Overall Loss 0.215936    Objective Loss 0.215936                                        LR 0.000003    Time 0.029934    
2023-01-06 14:51:51,762 - Epoch: [196][  100/  246]    Overall Loss 0.217092    Objective Loss 0.217092                                        LR 0.000003    Time 0.029625    
2023-01-06 14:51:52,025 - Epoch: [196][  110/  246]    Overall Loss 0.218407    Objective Loss 0.218407                                        LR 0.000003    Time 0.029320    
2023-01-06 14:51:52,281 - Epoch: [196][  120/  246]    Overall Loss 0.218247    Objective Loss 0.218247                                        LR 0.000003    Time 0.029006    
2023-01-06 14:51:52,528 - Epoch: [196][  130/  246]    Overall Loss 0.217154    Objective Loss 0.217154                                        LR 0.000003    Time 0.028667    
2023-01-06 14:51:52,778 - Epoch: [196][  140/  246]    Overall Loss 0.216672    Objective Loss 0.216672                                        LR 0.000003    Time 0.028393    
2023-01-06 14:51:53,021 - Epoch: [196][  150/  246]    Overall Loss 0.216974    Objective Loss 0.216974                                        LR 0.000003    Time 0.028116    
2023-01-06 14:51:53,274 - Epoch: [196][  160/  246]    Overall Loss 0.217908    Objective Loss 0.217908                                        LR 0.000003    Time 0.027942    
2023-01-06 14:51:53,516 - Epoch: [196][  170/  246]    Overall Loss 0.218133    Objective Loss 0.218133                                        LR 0.000003    Time 0.027718    
2023-01-06 14:51:53,768 - Epoch: [196][  180/  246]    Overall Loss 0.217792    Objective Loss 0.217792                                        LR 0.000003    Time 0.027572    
2023-01-06 14:51:54,021 - Epoch: [196][  190/  246]    Overall Loss 0.216968    Objective Loss 0.216968                                        LR 0.000003    Time 0.027453    
2023-01-06 14:51:54,292 - Epoch: [196][  200/  246]    Overall Loss 0.217216    Objective Loss 0.217216                                        LR 0.000003    Time 0.027431    
2023-01-06 14:51:54,560 - Epoch: [196][  210/  246]    Overall Loss 0.217025    Objective Loss 0.217025                                        LR 0.000003    Time 0.027400    
2023-01-06 14:51:54,834 - Epoch: [196][  220/  246]    Overall Loss 0.216701    Objective Loss 0.216701                                        LR 0.000003    Time 0.027397    
2023-01-06 14:51:55,102 - Epoch: [196][  230/  246]    Overall Loss 0.216421    Objective Loss 0.216421                                        LR 0.000003    Time 0.027371    
2023-01-06 14:51:55,382 - Epoch: [196][  240/  246]    Overall Loss 0.215691    Objective Loss 0.215691                                        LR 0.000003    Time 0.027389    
2023-01-06 14:51:55,513 - Epoch: [196][  246/  246]    Overall Loss 0.215363    Objective Loss 0.215363    Top1 92.583732    LR 0.000003    Time 0.027250    
2023-01-06 14:51:55,646 - --- validate (epoch=196)-----------
2023-01-06 14:51:55,646 - 6986 samples (256 per mini-batch)
2023-01-06 14:51:56,125 - Epoch: [196][   10/   28]    Loss 0.239951    Top1 91.406250    
2023-01-06 14:51:56,261 - Epoch: [196][   20/   28]    Loss 0.238759    Top1 91.367188    
2023-01-06 14:51:56,351 - Epoch: [196][   28/   28]    Loss 0.248296    Top1 91.096479    
2023-01-06 14:51:56,496 - ==> Top1: 91.096    Loss: 0.248

2023-01-06 14:51:56,496 - ==> Confusion:
[[ 267   14  158]
 [  17  305  280]
 [  61   92 5792]]

2023-01-06 14:51:56,498 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:51:56,498 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:51:56,508 - 

2023-01-06 14:51:56,508 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:51:57,243 - Epoch: [197][   10/  246]    Overall Loss 0.208227    Objective Loss 0.208227                                        LR 0.000003    Time 0.073491    
2023-01-06 14:51:57,503 - Epoch: [197][   20/  246]    Overall Loss 0.209871    Objective Loss 0.209871                                        LR 0.000003    Time 0.049690    
2023-01-06 14:51:57,758 - Epoch: [197][   30/  246]    Overall Loss 0.208829    Objective Loss 0.208829                                        LR 0.000003    Time 0.041632    
2023-01-06 14:51:58,027 - Epoch: [197][   40/  246]    Overall Loss 0.213984    Objective Loss 0.213984                                        LR 0.000003    Time 0.037930    
2023-01-06 14:51:58,292 - Epoch: [197][   50/  246]    Overall Loss 0.212245    Objective Loss 0.212245                                        LR 0.000003    Time 0.035642    
2023-01-06 14:51:58,571 - Epoch: [197][   60/  246]    Overall Loss 0.211140    Objective Loss 0.211140                                        LR 0.000003    Time 0.034329    
2023-01-06 14:51:58,847 - Epoch: [197][   70/  246]    Overall Loss 0.209451    Objective Loss 0.209451                                        LR 0.000003    Time 0.033375    
2023-01-06 14:51:59,120 - Epoch: [197][   80/  246]    Overall Loss 0.211669    Objective Loss 0.211669                                        LR 0.000003    Time 0.032582    
2023-01-06 14:51:59,389 - Epoch: [197][   90/  246]    Overall Loss 0.210305    Objective Loss 0.210305                                        LR 0.000003    Time 0.031951    
2023-01-06 14:51:59,661 - Epoch: [197][  100/  246]    Overall Loss 0.209663    Objective Loss 0.209663                                        LR 0.000003    Time 0.031472    
2023-01-06 14:51:59,935 - Epoch: [197][  110/  246]    Overall Loss 0.213014    Objective Loss 0.213014                                        LR 0.000003    Time 0.031093    
2023-01-06 14:52:00,204 - Epoch: [197][  120/  246]    Overall Loss 0.215923    Objective Loss 0.215923                                        LR 0.000003    Time 0.030744    
2023-01-06 14:52:00,474 - Epoch: [197][  130/  246]    Overall Loss 0.215532    Objective Loss 0.215532                                        LR 0.000003    Time 0.030447    
2023-01-06 14:52:00,744 - Epoch: [197][  140/  246]    Overall Loss 0.215332    Objective Loss 0.215332                                        LR 0.000003    Time 0.030188    
2023-01-06 14:52:00,993 - Epoch: [197][  150/  246]    Overall Loss 0.215186    Objective Loss 0.215186                                        LR 0.000003    Time 0.029833    
2023-01-06 14:52:01,248 - Epoch: [197][  160/  246]    Overall Loss 0.215649    Objective Loss 0.215649                                        LR 0.000003    Time 0.029559    
2023-01-06 14:52:01,505 - Epoch: [197][  170/  246]    Overall Loss 0.216669    Objective Loss 0.216669                                        LR 0.000003    Time 0.029332    
2023-01-06 14:52:01,777 - Epoch: [197][  180/  246]    Overall Loss 0.217083    Objective Loss 0.217083                                        LR 0.000003    Time 0.029209    
2023-01-06 14:52:02,035 - Epoch: [197][  190/  246]    Overall Loss 0.216968    Objective Loss 0.216968                                        LR 0.000003    Time 0.029026    
2023-01-06 14:52:02,276 - Epoch: [197][  200/  246]    Overall Loss 0.218097    Objective Loss 0.218097                                        LR 0.000003    Time 0.028776    
2023-01-06 14:52:02,514 - Epoch: [197][  210/  246]    Overall Loss 0.217519    Objective Loss 0.217519                                        LR 0.000003    Time 0.028537    
2023-01-06 14:52:02,757 - Epoch: [197][  220/  246]    Overall Loss 0.217457    Objective Loss 0.217457                                        LR 0.000003    Time 0.028344    
2023-01-06 14:52:02,979 - Epoch: [197][  230/  246]    Overall Loss 0.217114    Objective Loss 0.217114                                        LR 0.000003    Time 0.028077    
2023-01-06 14:52:03,219 - Epoch: [197][  240/  246]    Overall Loss 0.217095    Objective Loss 0.217095                                        LR 0.000003    Time 0.027906    
2023-01-06 14:52:03,335 - Epoch: [197][  246/  246]    Overall Loss 0.216719    Objective Loss 0.216719    Top1 91.148325    LR 0.000003    Time 0.027693    
2023-01-06 14:52:03,454 - --- validate (epoch=197)-----------
2023-01-06 14:52:03,454 - 6986 samples (256 per mini-batch)
2023-01-06 14:52:03,910 - Epoch: [197][   10/   28]    Loss 0.255676    Top1 90.976562    
2023-01-06 14:52:04,061 - Epoch: [197][   20/   28]    Loss 0.255909    Top1 90.781250    
2023-01-06 14:52:04,151 - Epoch: [197][   28/   28]    Loss 0.251920    Top1 91.039221    
2023-01-06 14:52:04,279 - ==> Top1: 91.039    Loss: 0.252

2023-01-06 14:52:04,279 - ==> Confusion:
[[ 259   17  163]
 [  14  298  290]
 [  68   74 5803]]

2023-01-06 14:52:04,281 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:52:04,281 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:52:04,291 - 

2023-01-06 14:52:04,291 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:52:05,036 - Epoch: [198][   10/  246]    Overall Loss 0.218823    Objective Loss 0.218823                                        LR 0.000003    Time 0.074445    
2023-01-06 14:52:05,276 - Epoch: [198][   20/  246]    Overall Loss 0.233600    Objective Loss 0.233600                                        LR 0.000003    Time 0.049191    
2023-01-06 14:52:05,519 - Epoch: [198][   30/  246]    Overall Loss 0.223104    Objective Loss 0.223104                                        LR 0.000003    Time 0.040891    
2023-01-06 14:52:05,777 - Epoch: [198][   40/  246]    Overall Loss 0.219971    Objective Loss 0.219971                                        LR 0.000003    Time 0.037065    
2023-01-06 14:52:06,043 - Epoch: [198][   50/  246]    Overall Loss 0.215614    Objective Loss 0.215614                                        LR 0.000003    Time 0.034957    
2023-01-06 14:52:06,267 - Epoch: [198][   60/  246]    Overall Loss 0.214917    Objective Loss 0.214917                                        LR 0.000003    Time 0.032857    
2023-01-06 14:52:06,484 - Epoch: [198][   70/  246]    Overall Loss 0.215176    Objective Loss 0.215176                                        LR 0.000003    Time 0.031257    
2023-01-06 14:52:06,714 - Epoch: [198][   80/  246]    Overall Loss 0.215519    Objective Loss 0.215519                                        LR 0.000003    Time 0.030223    
2023-01-06 14:52:06,939 - Epoch: [198][   90/  246]    Overall Loss 0.217736    Objective Loss 0.217736                                        LR 0.000003    Time 0.029358    
2023-01-06 14:52:07,165 - Epoch: [198][  100/  246]    Overall Loss 0.217361    Objective Loss 0.217361                                        LR 0.000003    Time 0.028678    
2023-01-06 14:52:07,404 - Epoch: [198][  110/  246]    Overall Loss 0.216823    Objective Loss 0.216823                                        LR 0.000003    Time 0.028242    
2023-01-06 14:52:07,667 - Epoch: [198][  120/  246]    Overall Loss 0.216428    Objective Loss 0.216428                                        LR 0.000003    Time 0.028064    
2023-01-06 14:52:07,922 - Epoch: [198][  130/  246]    Overall Loss 0.216005    Objective Loss 0.216005                                        LR 0.000003    Time 0.027857    
2023-01-06 14:52:08,181 - Epoch: [198][  140/  246]    Overall Loss 0.216521    Objective Loss 0.216521                                        LR 0.000003    Time 0.027708    
2023-01-06 14:52:08,443 - Epoch: [198][  150/  246]    Overall Loss 0.216830    Objective Loss 0.216830                                        LR 0.000003    Time 0.027592    
2023-01-06 14:52:08,703 - Epoch: [198][  160/  246]    Overall Loss 0.217648    Objective Loss 0.217648                                        LR 0.000003    Time 0.027480    
2023-01-06 14:52:08,964 - Epoch: [198][  170/  246]    Overall Loss 0.217381    Objective Loss 0.217381                                        LR 0.000003    Time 0.027388    
2023-01-06 14:52:09,226 - Epoch: [198][  180/  246]    Overall Loss 0.216664    Objective Loss 0.216664                                        LR 0.000003    Time 0.027323    
2023-01-06 14:52:09,488 - Epoch: [198][  190/  246]    Overall Loss 0.216247    Objective Loss 0.216247                                        LR 0.000003    Time 0.027261    
2023-01-06 14:52:09,749 - Epoch: [198][  200/  246]    Overall Loss 0.216495    Objective Loss 0.216495                                        LR 0.000003    Time 0.027201    
2023-01-06 14:52:09,975 - Epoch: [198][  210/  246]    Overall Loss 0.216134    Objective Loss 0.216134                                        LR 0.000003    Time 0.026980    
2023-01-06 14:52:10,207 - Epoch: [198][  220/  246]    Overall Loss 0.215184    Objective Loss 0.215184                                        LR 0.000003    Time 0.026804    
2023-01-06 14:52:10,432 - Epoch: [198][  230/  246]    Overall Loss 0.214832    Objective Loss 0.214832                                        LR 0.000003    Time 0.026615    
2023-01-06 14:52:10,668 - Epoch: [198][  240/  246]    Overall Loss 0.214804    Objective Loss 0.214804                                        LR 0.000003    Time 0.026489    
2023-01-06 14:52:10,784 - Epoch: [198][  246/  246]    Overall Loss 0.215215    Objective Loss 0.215215    Top1 89.473684    LR 0.000003    Time 0.026310    
2023-01-06 14:52:10,912 - --- validate (epoch=198)-----------
2023-01-06 14:52:10,912 - 6986 samples (256 per mini-batch)
2023-01-06 14:52:11,381 - Epoch: [198][   10/   28]    Loss 0.244346    Top1 91.210938    
2023-01-06 14:52:11,542 - Epoch: [198][   20/   28]    Loss 0.245272    Top1 91.015625    
2023-01-06 14:52:11,633 - Epoch: [198][   28/   28]    Loss 0.250000    Top1 90.967650    
2023-01-06 14:52:11,767 - ==> Top1: 90.968    Loss: 0.250

2023-01-06 14:52:11,768 - ==> Confusion:
[[ 262    9  168]
 [  19  299  284]
 [  70   81 5794]]

2023-01-06 14:52:11,769 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:52:11,769 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:52:11,786 - 

2023-01-06 14:52:11,787 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 14:52:12,387 - Epoch: [199][   10/  246]    Overall Loss 0.233083    Objective Loss 0.233083                                        LR 0.000003    Time 0.059966    
2023-01-06 14:52:12,633 - Epoch: [199][   20/  246]    Overall Loss 0.225469    Objective Loss 0.225469                                        LR 0.000003    Time 0.042263    
2023-01-06 14:52:12,867 - Epoch: [199][   30/  246]    Overall Loss 0.220673    Objective Loss 0.220673                                        LR 0.000003    Time 0.035966    
2023-01-06 14:52:13,119 - Epoch: [199][   40/  246]    Overall Loss 0.216813    Objective Loss 0.216813                                        LR 0.000003    Time 0.033240    
2023-01-06 14:52:13,378 - Epoch: [199][   50/  246]    Overall Loss 0.217291    Objective Loss 0.217291                                        LR 0.000003    Time 0.031765    
2023-01-06 14:52:13,649 - Epoch: [199][   60/  246]    Overall Loss 0.221876    Objective Loss 0.221876                                        LR 0.000003    Time 0.030988    
2023-01-06 14:52:13,905 - Epoch: [199][   70/  246]    Overall Loss 0.219100    Objective Loss 0.219100                                        LR 0.000003    Time 0.030191    
2023-01-06 14:52:14,158 - Epoch: [199][   80/  246]    Overall Loss 0.220141    Objective Loss 0.220141                                        LR 0.000003    Time 0.029567    
2023-01-06 14:52:14,417 - Epoch: [199][   90/  246]    Overall Loss 0.220625    Objective Loss 0.220625                                        LR 0.000003    Time 0.029157    
2023-01-06 14:52:14,676 - Epoch: [199][  100/  246]    Overall Loss 0.219316    Objective Loss 0.219316                                        LR 0.000003    Time 0.028824    
2023-01-06 14:52:14,929 - Epoch: [199][  110/  246]    Overall Loss 0.219569    Objective Loss 0.219569                                        LR 0.000003    Time 0.028499    
2023-01-06 14:52:15,183 - Epoch: [199][  120/  246]    Overall Loss 0.217914    Objective Loss 0.217914                                        LR 0.000003    Time 0.028238    
2023-01-06 14:52:15,434 - Epoch: [199][  130/  246]    Overall Loss 0.216283    Objective Loss 0.216283                                        LR 0.000003    Time 0.027994    
2023-01-06 14:52:15,687 - Epoch: [199][  140/  246]    Overall Loss 0.216340    Objective Loss 0.216340                                        LR 0.000003    Time 0.027797    
2023-01-06 14:52:15,934 - Epoch: [199][  150/  246]    Overall Loss 0.216733    Objective Loss 0.216733                                        LR 0.000003    Time 0.027588    
2023-01-06 14:52:16,185 - Epoch: [199][  160/  246]    Overall Loss 0.217762    Objective Loss 0.217762                                        LR 0.000003    Time 0.027430    
2023-01-06 14:52:16,439 - Epoch: [199][  170/  246]    Overall Loss 0.217384    Objective Loss 0.217384                                        LR 0.000003    Time 0.027312    
2023-01-06 14:52:16,695 - Epoch: [199][  180/  246]    Overall Loss 0.216413    Objective Loss 0.216413                                        LR 0.000003    Time 0.027215    
2023-01-06 14:52:16,926 - Epoch: [199][  190/  246]    Overall Loss 0.217076    Objective Loss 0.217076                                        LR 0.000003    Time 0.026993    
2023-01-06 14:52:17,141 - Epoch: [199][  200/  246]    Overall Loss 0.217084    Objective Loss 0.217084                                        LR 0.000003    Time 0.026718    
2023-01-06 14:52:17,358 - Epoch: [199][  210/  246]    Overall Loss 0.216509    Objective Loss 0.216509                                        LR 0.000003    Time 0.026475    
2023-01-06 14:52:17,573 - Epoch: [199][  220/  246]    Overall Loss 0.217138    Objective Loss 0.217138                                        LR 0.000003    Time 0.026249    
2023-01-06 14:52:17,789 - Epoch: [199][  230/  246]    Overall Loss 0.217211    Objective Loss 0.217211                                        LR 0.000003    Time 0.026048    
2023-01-06 14:52:18,020 - Epoch: [199][  240/  246]    Overall Loss 0.216638    Objective Loss 0.216638                                        LR 0.000003    Time 0.025921    
2023-01-06 14:52:18,134 - Epoch: [199][  246/  246]    Overall Loss 0.216440    Objective Loss 0.216440    Top1 94.736842    LR 0.000003    Time 0.025754    
2023-01-06 14:52:18,263 - --- validate (epoch=199)-----------
2023-01-06 14:52:18,264 - 6986 samples (256 per mini-batch)
2023-01-06 14:52:18,725 - Epoch: [199][   10/   28]    Loss 0.240698    Top1 91.210938    
2023-01-06 14:52:18,868 - Epoch: [199][   20/   28]    Loss 0.245250    Top1 91.210938    
2023-01-06 14:52:18,958 - Epoch: [199][   28/   28]    Loss 0.249517    Top1 91.053536    
2023-01-06 14:52:19,118 - ==> Top1: 91.054    Loss: 0.250

2023-01-06 14:52:19,118 - ==> Confusion:
[[ 273   12  154]
 [  16  312  274]
 [  74   95 5776]]

2023-01-06 14:52:19,120 - ==> Best [Top1: 91.154   Sparsity:0.00   Params: 360896 on epoch: 162]
2023-01-06 14:52:19,120 - Saving checkpoint to: logs/2023.01.06-142639/qat_checkpoint.pth.tar
2023-01-06 14:52:19,130 - --- test ---------------------
2023-01-06 14:52:19,130 - 13117 samples (256 per mini-batch)
2023-01-06 14:52:19,581 - Test: [   10/   52]    Loss 0.196054    Top1 93.046875    
2023-01-06 14:52:19,728 - Test: [   20/   52]    Loss 0.200156    Top1 92.636719    
2023-01-06 14:52:19,867 - Test: [   30/   52]    Loss 0.201418    Top1 92.721354    
2023-01-06 14:52:20,014 - Test: [   40/   52]    Loss 0.212565    Top1 92.304688    
2023-01-06 14:52:20,145 - Test: [   50/   52]    Loss 0.210411    Top1 92.414062    
2023-01-06 14:52:20,165 - Test: [   52/   52]    Loss 0.208435    Top1 92.460166    
2023-01-06 14:52:20,278 - ==> Top1: 92.460    Loss: 0.208

2023-01-06 14:52:20,278 - ==> Confusion:
[[  302    27   232]
 [   29   388   339]
 [  147   215 11438]]

2023-01-06 14:52:20,352 - 
2023-01-06 14:52:20,352 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-142639/2023.01.06-142639.log
