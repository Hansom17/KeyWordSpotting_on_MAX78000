2023-01-06 15:50:14,200 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-155014/2023.01.06-155014.log
2023-01-06 15:50:16,264 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 15:50:16,264 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 15:50:23,468 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-06 15:50:23,469 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 15:50:23,471 - 

2023-01-06 15:50:23,471 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:23,987 - Epoch: [0][   10/   37]    Overall Loss 1.092183    Objective Loss 1.092183                                        LR 0.000060    Time 0.051511    
2023-01-06 15:50:24,098 - Epoch: [0][   20/   37]    Overall Loss 1.082442    Objective Loss 1.082442                                        LR 0.000060    Time 0.031289    
2023-01-06 15:50:24,216 - Epoch: [0][   30/   37]    Overall Loss 1.067656    Objective Loss 1.067656                                        LR 0.000060    Time 0.024759    
2023-01-06 15:50:24,276 - Epoch: [0][   37/   37]    Overall Loss 1.052355    Objective Loss 1.052355    Top1 56.276151    LR 0.000060    Time 0.021692    
2023-01-06 15:50:24,352 - --- validate (epoch=0)-----------
2023-01-06 15:50:24,352 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:24,550 - Epoch: [0][    5/    5]    Loss 0.951322    Top1 59.064885    
2023-01-06 15:50:24,610 - ==> Top1: 59.065    Loss: 0.951

2023-01-06 15:50:24,610 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:24,611 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 0]
2023-01-06 15:50:24,611 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:24,617 - 

2023-01-06 15:50:24,617 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:24,922 - Epoch: [1][   10/   37]    Overall Loss 0.919765    Objective Loss 0.919765                                        LR 0.000060    Time 0.030447    
2023-01-06 15:50:25,025 - Epoch: [1][   20/   37]    Overall Loss 0.890362    Objective Loss 0.890362                                        LR 0.000060    Time 0.020353    
2023-01-06 15:50:25,140 - Epoch: [1][   30/   37]    Overall Loss 0.869323    Objective Loss 0.869323                                        LR 0.000060    Time 0.017393    
2023-01-06 15:50:25,202 - Epoch: [1][   37/   37]    Overall Loss 0.857877    Objective Loss 0.857877    Top1 56.903766    LR 0.000060    Time 0.015766    
2023-01-06 15:50:25,277 - --- validate (epoch=1)-----------
2023-01-06 15:50:25,277 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:25,483 - Epoch: [1][    5/    5]    Loss 0.789230    Top1 59.064885    
2023-01-06 15:50:25,541 - ==> Top1: 59.065    Loss: 0.789

2023-01-06 15:50:25,542 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:25,543 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 1]
2023-01-06 15:50:25,543 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:25,549 - 

2023-01-06 15:50:25,549 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:25,848 - Epoch: [2][   10/   37]    Overall Loss 0.795889    Objective Loss 0.795889                                        LR 0.000060    Time 0.029776    
2023-01-06 15:50:25,946 - Epoch: [2][   20/   37]    Overall Loss 0.788691    Objective Loss 0.788691                                        LR 0.000060    Time 0.019771    
2023-01-06 15:50:26,056 - Epoch: [2][   30/   37]    Overall Loss 0.781903    Objective Loss 0.781903                                        LR 0.000060    Time 0.016846    
2023-01-06 15:50:26,118 - Epoch: [2][   37/   37]    Overall Loss 0.778457    Objective Loss 0.778457    Top1 58.368201    LR 0.000060    Time 0.015330    
2023-01-06 15:50:26,188 - --- validate (epoch=2)-----------
2023-01-06 15:50:26,189 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:26,393 - Epoch: [2][    5/    5]    Loss 0.763660    Top1 59.064885    
2023-01-06 15:50:26,459 - ==> Top1: 59.065    Loss: 0.764

2023-01-06 15:50:26,459 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:26,460 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 2]
2023-01-06 15:50:26,460 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:26,466 - 

2023-01-06 15:50:26,466 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:26,761 - Epoch: [3][   10/   37]    Overall Loss 0.761769    Objective Loss 0.761769                                        LR 0.000060    Time 0.029410    
2023-01-06 15:50:26,867 - Epoch: [3][   20/   37]    Overall Loss 0.757313    Objective Loss 0.757313                                        LR 0.000060    Time 0.019955    
2023-01-06 15:50:26,978 - Epoch: [3][   30/   37]    Overall Loss 0.755851    Objective Loss 0.755851                                        LR 0.000060    Time 0.016959    
2023-01-06 15:50:27,037 - Epoch: [3][   37/   37]    Overall Loss 0.752407    Objective Loss 0.752407    Top1 58.786611    LR 0.000060    Time 0.015345    
2023-01-06 15:50:27,114 - --- validate (epoch=3)-----------
2023-01-06 15:50:27,115 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:27,325 - Epoch: [3][    5/    5]    Loss 0.731735    Top1 59.064885    
2023-01-06 15:50:27,391 - ==> Top1: 59.065    Loss: 0.732

2023-01-06 15:50:27,391 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:27,392 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 3]
2023-01-06 15:50:27,392 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:27,399 - 

2023-01-06 15:50:27,399 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:27,686 - Epoch: [4][   10/   37]    Overall Loss 0.744185    Objective Loss 0.744185                                        LR 0.000060    Time 0.028668    
2023-01-06 15:50:27,774 - Epoch: [4][   20/   37]    Overall Loss 0.741369    Objective Loss 0.741369                                        LR 0.000060    Time 0.018686    
2023-01-06 15:50:27,863 - Epoch: [4][   30/   37]    Overall Loss 0.738351    Objective Loss 0.738351                                        LR 0.000060    Time 0.015432    
2023-01-06 15:50:27,918 - Epoch: [4][   37/   37]    Overall Loss 0.736971    Objective Loss 0.736971    Top1 55.439331    LR 0.000060    Time 0.013991    
2023-01-06 15:50:27,986 - --- validate (epoch=4)-----------
2023-01-06 15:50:27,986 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:28,191 - Epoch: [4][    5/    5]    Loss 0.714592    Top1 59.064885    
2023-01-06 15:50:28,262 - ==> Top1: 59.065    Loss: 0.715

2023-01-06 15:50:28,262 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:28,263 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 4]
2023-01-06 15:50:28,263 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:28,269 - 

2023-01-06 15:50:28,269 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:28,565 - Epoch: [5][   10/   37]    Overall Loss 0.735297    Objective Loss 0.735297                                        LR 0.000060    Time 0.029462    
2023-01-06 15:50:28,665 - Epoch: [5][   20/   37]    Overall Loss 0.732072    Objective Loss 0.732072                                        LR 0.000060    Time 0.019706    
2023-01-06 15:50:28,763 - Epoch: [5][   30/   37]    Overall Loss 0.728642    Objective Loss 0.728642                                        LR 0.000060    Time 0.016339    
2023-01-06 15:50:28,815 - Epoch: [5][   37/   37]    Overall Loss 0.726235    Objective Loss 0.726235    Top1 58.158996    LR 0.000060    Time 0.014670    
2023-01-06 15:50:28,885 - --- validate (epoch=5)-----------
2023-01-06 15:50:28,886 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:29,089 - Epoch: [5][    5/    5]    Loss 0.703436    Top1 59.064885    
2023-01-06 15:50:29,153 - ==> Top1: 59.065    Loss: 0.703

2023-01-06 15:50:29,154 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:29,154 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 5]
2023-01-06 15:50:29,155 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:29,161 - 

2023-01-06 15:50:29,161 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:29,445 - Epoch: [6][   10/   37]    Overall Loss 0.720903    Objective Loss 0.720903                                        LR 0.000060    Time 0.028374    
2023-01-06 15:50:29,533 - Epoch: [6][   20/   37]    Overall Loss 0.716732    Objective Loss 0.716732                                        LR 0.000060    Time 0.018573    
2023-01-06 15:50:29,630 - Epoch: [6][   30/   37]    Overall Loss 0.717300    Objective Loss 0.717300                                        LR 0.000060    Time 0.015575    
2023-01-06 15:50:29,683 - Epoch: [6][   37/   37]    Overall Loss 0.717546    Objective Loss 0.717546    Top1 52.719665    LR 0.000060    Time 0.014057    
2023-01-06 15:50:29,754 - --- validate (epoch=6)-----------
2023-01-06 15:50:29,754 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:29,967 - Epoch: [6][    5/    5]    Loss 0.707359    Top1 59.064885    
2023-01-06 15:50:30,032 - ==> Top1: 59.065    Loss: 0.707

2023-01-06 15:50:30,032 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:30,033 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 6]
2023-01-06 15:50:30,033 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:30,039 - 

2023-01-06 15:50:30,040 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:30,353 - Epoch: [7][   10/   37]    Overall Loss 0.714872    Objective Loss 0.714872                                        LR 0.000060    Time 0.031310    
2023-01-06 15:50:30,474 - Epoch: [7][   20/   37]    Overall Loss 0.712948    Objective Loss 0.712948                                        LR 0.000060    Time 0.021675    
2023-01-06 15:50:30,602 - Epoch: [7][   30/   37]    Overall Loss 0.711528    Objective Loss 0.711528                                        LR 0.000060    Time 0.018675    
2023-01-06 15:50:30,672 - Epoch: [7][   37/   37]    Overall Loss 0.710030    Objective Loss 0.710030    Top1 59.623431    LR 0.000060    Time 0.017035    
2023-01-06 15:50:30,746 - --- validate (epoch=7)-----------
2023-01-06 15:50:30,746 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:30,958 - Epoch: [7][    5/    5]    Loss 0.697985    Top1 59.064885    
2023-01-06 15:50:31,027 - ==> Top1: 59.065    Loss: 0.698

2023-01-06 15:50:31,027 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:31,028 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 7]
2023-01-06 15:50:31,028 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:31,034 - 

2023-01-06 15:50:31,034 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:31,463 - Epoch: [8][   10/   37]    Overall Loss 0.707325    Objective Loss 0.707325                                        LR 0.000060    Time 0.042791    
2023-01-06 15:50:31,557 - Epoch: [8][   20/   37]    Overall Loss 0.705380    Objective Loss 0.705380                                        LR 0.000060    Time 0.026100    
2023-01-06 15:50:31,656 - Epoch: [8][   30/   37]    Overall Loss 0.704092    Objective Loss 0.704092                                        LR 0.000060    Time 0.020692    
2023-01-06 15:50:31,717 - Epoch: [8][   37/   37]    Overall Loss 0.703377    Objective Loss 0.703377    Top1 56.066946    LR 0.000060    Time 0.018421    
2023-01-06 15:50:31,806 - --- validate (epoch=8)-----------
2023-01-06 15:50:31,806 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:32,012 - Epoch: [8][    5/    5]    Loss 0.704430    Top1 59.064885    
2023-01-06 15:50:32,081 - ==> Top1: 59.065    Loss: 0.704

2023-01-06 15:50:32,081 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:32,082 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 8]
2023-01-06 15:50:32,082 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:32,089 - 

2023-01-06 15:50:32,089 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:32,405 - Epoch: [9][   10/   37]    Overall Loss 0.698670    Objective Loss 0.698670                                        LR 0.000060    Time 0.031546    
2023-01-06 15:50:32,500 - Epoch: [9][   20/   37]    Overall Loss 0.700707    Objective Loss 0.700707                                        LR 0.000060    Time 0.020517    
2023-01-06 15:50:32,601 - Epoch: [9][   30/   37]    Overall Loss 0.700204    Objective Loss 0.700204                                        LR 0.000060    Time 0.017028    
2023-01-06 15:50:32,658 - Epoch: [9][   37/   37]    Overall Loss 0.698877    Objective Loss 0.698877    Top1 55.230126    LR 0.000060    Time 0.015344    
2023-01-06 15:50:32,729 - --- validate (epoch=9)-----------
2023-01-06 15:50:32,729 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:32,936 - Epoch: [9][    5/    5]    Loss 0.697981    Top1 59.064885    
2023-01-06 15:50:33,005 - ==> Top1: 59.065    Loss: 0.698

2023-01-06 15:50:33,005 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:33,006 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 9]
2023-01-06 15:50:33,006 - Saving checkpoint to: logs/2023.01.06-155014/checkpoint.pth.tar
2023-01-06 15:50:33,022 - 

2023-01-06 15:50:33,022 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:33,364 - Epoch: [10][   10/   37]    Overall Loss 0.793728    Objective Loss 0.793728                                        LR 0.000060    Time 0.034120    
2023-01-06 15:50:33,500 - Epoch: [10][   20/   37]    Overall Loss 0.748116    Objective Loss 0.748116                                        LR 0.000060    Time 0.023835    
2023-01-06 15:50:33,638 - Epoch: [10][   30/   37]    Overall Loss 0.730365    Objective Loss 0.730365                                        LR 0.000060    Time 0.020489    
2023-01-06 15:50:33,713 - Epoch: [10][   37/   37]    Overall Loss 0.720620    Objective Loss 0.720620    Top1 60.251046    LR 0.000060    Time 0.018635    
2023-01-06 15:50:33,790 - --- validate (epoch=10)-----------
2023-01-06 15:50:33,790 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:34,008 - Epoch: [10][    5/    5]    Loss 0.681714    Top1 59.064885    
2023-01-06 15:50:34,078 - ==> Top1: 59.065    Loss: 0.682

2023-01-06 15:50:34,078 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:34,079 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 10]
2023-01-06 15:50:34,079 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:34,085 - 

2023-01-06 15:50:34,085 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:34,426 - Epoch: [11][   10/   37]    Overall Loss 0.689478    Objective Loss 0.689478                                        LR 0.000060    Time 0.034040    
2023-01-06 15:50:34,556 - Epoch: [11][   20/   37]    Overall Loss 0.683587    Objective Loss 0.683587                                        LR 0.000060    Time 0.023478    
2023-01-06 15:50:34,686 - Epoch: [11][   30/   37]    Overall Loss 0.683941    Objective Loss 0.683941                                        LR 0.000060    Time 0.019984    
2023-01-06 15:50:34,764 - Epoch: [11][   37/   37]    Overall Loss 0.683044    Objective Loss 0.683044    Top1 58.158996    LR 0.000060    Time 0.018315    
2023-01-06 15:50:34,837 - --- validate (epoch=11)-----------
2023-01-06 15:50:34,837 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:35,053 - Epoch: [11][    5/    5]    Loss 0.673805    Top1 59.064885    
2023-01-06 15:50:35,111 - ==> Top1: 59.065    Loss: 0.674

2023-01-06 15:50:35,111 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:35,112 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 11]
2023-01-06 15:50:35,112 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:35,118 - 

2023-01-06 15:50:35,118 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:35,437 - Epoch: [12][   10/   37]    Overall Loss 0.680359    Objective Loss 0.680359                                        LR 0.000060    Time 0.031849    
2023-01-06 15:50:35,561 - Epoch: [12][   20/   37]    Overall Loss 0.679073    Objective Loss 0.679073                                        LR 0.000060    Time 0.022093    
2023-01-06 15:50:35,680 - Epoch: [12][   30/   37]    Overall Loss 0.679271    Objective Loss 0.679271                                        LR 0.000060    Time 0.018697    
2023-01-06 15:50:35,751 - Epoch: [12][   37/   37]    Overall Loss 0.678640    Objective Loss 0.678640    Top1 56.903766    LR 0.000060    Time 0.017066    
2023-01-06 15:50:35,825 - --- validate (epoch=12)-----------
2023-01-06 15:50:35,825 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:36,043 - Epoch: [12][    5/    5]    Loss 0.676496    Top1 59.064885    
2023-01-06 15:50:36,120 - ==> Top1: 59.065    Loss: 0.676

2023-01-06 15:50:36,120 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:36,121 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 12]
2023-01-06 15:50:36,121 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:36,127 - 

2023-01-06 15:50:36,128 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:36,470 - Epoch: [13][   10/   37]    Overall Loss 0.677366    Objective Loss 0.677366                                        LR 0.000060    Time 0.034184    
2023-01-06 15:50:36,618 - Epoch: [13][   20/   37]    Overall Loss 0.676111    Objective Loss 0.676111                                        LR 0.000060    Time 0.024488    
2023-01-06 15:50:36,771 - Epoch: [13][   30/   37]    Overall Loss 0.675532    Objective Loss 0.675532                                        LR 0.000060    Time 0.021407    
2023-01-06 15:50:36,851 - Epoch: [13][   37/   37]    Overall Loss 0.675753    Objective Loss 0.675753    Top1 57.531381    LR 0.000060    Time 0.019494    
2023-01-06 15:50:36,925 - --- validate (epoch=13)-----------
2023-01-06 15:50:36,925 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:37,155 - Epoch: [13][    5/    5]    Loss 0.662157    Top1 59.064885    
2023-01-06 15:50:37,230 - ==> Top1: 59.065    Loss: 0.662

2023-01-06 15:50:37,230 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:37,231 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 13]
2023-01-06 15:50:37,231 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:37,238 - 

2023-01-06 15:50:37,238 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:37,574 - Epoch: [14][   10/   37]    Overall Loss 0.674213    Objective Loss 0.674213                                        LR 0.000060    Time 0.033562    
2023-01-06 15:50:37,715 - Epoch: [14][   20/   37]    Overall Loss 0.675384    Objective Loss 0.675384                                        LR 0.000060    Time 0.023803    
2023-01-06 15:50:37,855 - Epoch: [14][   30/   37]    Overall Loss 0.673778    Objective Loss 0.673778                                        LR 0.000060    Time 0.020526    
2023-01-06 15:50:37,935 - Epoch: [14][   37/   37]    Overall Loss 0.672484    Objective Loss 0.672484    Top1 58.786611    LR 0.000060    Time 0.018788    
2023-01-06 15:50:38,010 - --- validate (epoch=14)-----------
2023-01-06 15:50:38,010 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:38,229 - Epoch: [14][    5/    5]    Loss 0.661060    Top1 59.064885    
2023-01-06 15:50:38,301 - ==> Top1: 59.065    Loss: 0.661

2023-01-06 15:50:38,302 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:38,302 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 14]
2023-01-06 15:50:38,302 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:38,309 - 

2023-01-06 15:50:38,309 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:38,653 - Epoch: [15][   10/   37]    Overall Loss 0.665299    Objective Loss 0.665299                                        LR 0.000060    Time 0.034375    
2023-01-06 15:50:38,791 - Epoch: [15][   20/   37]    Overall Loss 0.670181    Objective Loss 0.670181                                        LR 0.000060    Time 0.024071    
2023-01-06 15:50:38,939 - Epoch: [15][   30/   37]    Overall Loss 0.670462    Objective Loss 0.670462                                        LR 0.000060    Time 0.020940    
2023-01-06 15:50:39,020 - Epoch: [15][   37/   37]    Overall Loss 0.669171    Objective Loss 0.669171    Top1 56.485356    LR 0.000060    Time 0.019165    
2023-01-06 15:50:39,093 - --- validate (epoch=15)-----------
2023-01-06 15:50:39,093 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:39,319 - Epoch: [15][    5/    5]    Loss 0.661201    Top1 59.064885    
2023-01-06 15:50:39,388 - ==> Top1: 59.065    Loss: 0.661

2023-01-06 15:50:39,389 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 15:50:39,390 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 151104 on epoch: 15]
2023-01-06 15:50:39,390 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:39,396 - 

2023-01-06 15:50:39,397 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:39,746 - Epoch: [16][   10/   37]    Overall Loss 0.661860    Objective Loss 0.661860                                        LR 0.000060    Time 0.034912    
2023-01-06 15:50:39,880 - Epoch: [16][   20/   37]    Overall Loss 0.660031    Objective Loss 0.660031                                        LR 0.000060    Time 0.024125    
2023-01-06 15:50:40,026 - Epoch: [16][   30/   37]    Overall Loss 0.659039    Objective Loss 0.659039                                        LR 0.000060    Time 0.020915    
2023-01-06 15:50:40,106 - Epoch: [16][   37/   37]    Overall Loss 0.659063    Objective Loss 0.659063    Top1 57.322176    LR 0.000060    Time 0.019123    
2023-01-06 15:50:40,179 - --- validate (epoch=16)-----------
2023-01-06 15:50:40,179 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:40,402 - Epoch: [16][    5/    5]    Loss 0.644822    Top1 61.545802    
2023-01-06 15:50:40,461 - ==> Top1: 61.546    Loss: 0.645

2023-01-06 15:50:40,462 - ==> Confusion:
[[ 29 400   0]
 [  3 616   0]
 [  0   0   0]]

2023-01-06 15:50:40,463 - ==> Best [Top1: 61.546   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 15:50:40,463 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:40,469 - 

2023-01-06 15:50:40,469 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:40,792 - Epoch: [17][   10/   37]    Overall Loss 0.653788    Objective Loss 0.653788                                        LR 0.000060    Time 0.032238    
2023-01-06 15:50:40,911 - Epoch: [17][   20/   37]    Overall Loss 0.652944    Objective Loss 0.652944                                        LR 0.000060    Time 0.022040    
2023-01-06 15:50:41,033 - Epoch: [17][   30/   37]    Overall Loss 0.651065    Objective Loss 0.651065                                        LR 0.000060    Time 0.018732    
2023-01-06 15:50:41,101 - Epoch: [17][   37/   37]    Overall Loss 0.649027    Objective Loss 0.649027    Top1 59.832636    LR 0.000060    Time 0.017032    
2023-01-06 15:50:41,171 - --- validate (epoch=17)-----------
2023-01-06 15:50:41,172 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:41,396 - Epoch: [17][    5/    5]    Loss 0.637499    Top1 62.309160    
2023-01-06 15:50:41,462 - ==> Top1: 62.309    Loss: 0.637

2023-01-06 15:50:41,462 - ==> Confusion:
[[ 43 386   0]
 [  9 610   0]
 [  0   0   0]]

2023-01-06 15:50:41,463 - ==> Best [Top1: 62.309   Sparsity:0.00   Params: 151104 on epoch: 17]
2023-01-06 15:50:41,463 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:41,470 - 

2023-01-06 15:50:41,470 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:41,828 - Epoch: [18][   10/   37]    Overall Loss 0.633514    Objective Loss 0.633514                                        LR 0.000060    Time 0.035756    
2023-01-06 15:50:41,962 - Epoch: [18][   20/   37]    Overall Loss 0.638430    Objective Loss 0.638430                                        LR 0.000060    Time 0.024554    
2023-01-06 15:50:42,106 - Epoch: [18][   30/   37]    Overall Loss 0.637734    Objective Loss 0.637734                                        LR 0.000060    Time 0.021131    
2023-01-06 15:50:42,181 - Epoch: [18][   37/   37]    Overall Loss 0.637123    Objective Loss 0.637123    Top1 67.573222    LR 0.000060    Time 0.019151    
2023-01-06 15:50:42,259 - --- validate (epoch=18)-----------
2023-01-06 15:50:42,259 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:42,484 - Epoch: [18][    5/    5]    Loss 0.633093    Top1 71.183206    
2023-01-06 15:50:42,546 - ==> Top1: 71.183    Loss: 0.633

2023-01-06 15:50:42,547 - ==> Confusion:
[[203 226   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:50:42,548 - ==> Best [Top1: 71.183   Sparsity:0.00   Params: 151104 on epoch: 18]
2023-01-06 15:50:42,548 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:42,554 - 

2023-01-06 15:50:42,554 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:42,898 - Epoch: [19][   10/   37]    Overall Loss 0.629133    Objective Loss 0.629133                                        LR 0.000060    Time 0.034286    
2023-01-06 15:50:43,027 - Epoch: [19][   20/   37]    Overall Loss 0.626593    Objective Loss 0.626593                                        LR 0.000060    Time 0.023574    
2023-01-06 15:50:43,157 - Epoch: [19][   30/   37]    Overall Loss 0.623645    Objective Loss 0.623645                                        LR 0.000060    Time 0.020049    
2023-01-06 15:50:43,236 - Epoch: [19][   37/   37]    Overall Loss 0.622470    Objective Loss 0.622470    Top1 65.271967    LR 0.000060    Time 0.018371    
2023-01-06 15:50:43,312 - --- validate (epoch=19)-----------
2023-01-06 15:50:43,312 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:43,541 - Epoch: [19][    5/    5]    Loss 0.600679    Top1 67.652672    
2023-01-06 15:50:43,610 - ==> Top1: 67.653    Loss: 0.601

2023-01-06 15:50:43,610 - ==> Confusion:
[[128 301   0]
 [ 38 581   0]
 [  0   0   0]]

2023-01-06 15:50:43,611 - ==> Best [Top1: 71.183   Sparsity:0.00   Params: 151104 on epoch: 18]
2023-01-06 15:50:43,611 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:43,616 - 

2023-01-06 15:50:43,617 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:43,946 - Epoch: [20][   10/   37]    Overall Loss 0.614321    Objective Loss 0.614321                                        LR 0.000060    Time 0.032846    
2023-01-06 15:50:44,068 - Epoch: [20][   20/   37]    Overall Loss 0.610244    Objective Loss 0.610244                                        LR 0.000060    Time 0.022502    
2023-01-06 15:50:44,194 - Epoch: [20][   30/   37]    Overall Loss 0.606465    Objective Loss 0.606465                                        LR 0.000060    Time 0.019187    
2023-01-06 15:50:44,271 - Epoch: [20][   37/   37]    Overall Loss 0.605754    Objective Loss 0.605754    Top1 66.945607    LR 0.000060    Time 0.017651    
2023-01-06 15:50:44,349 - --- validate (epoch=20)-----------
2023-01-06 15:50:44,350 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:44,581 - Epoch: [20][    5/    5]    Loss 0.583828    Top1 67.080153    
2023-01-06 15:50:44,639 - ==> Top1: 67.080    Loss: 0.584

2023-01-06 15:50:44,639 - ==> Confusion:
[[120 309   0]
 [ 36 583   0]
 [  0   0   0]]

2023-01-06 15:50:44,640 - ==> Best [Top1: 71.183   Sparsity:0.00   Params: 151104 on epoch: 18]
2023-01-06 15:50:44,640 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:44,645 - 

2023-01-06 15:50:44,645 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:44,976 - Epoch: [21][   10/   37]    Overall Loss 0.589269    Objective Loss 0.589269                                        LR 0.000060    Time 0.032991    
2023-01-06 15:50:45,094 - Epoch: [21][   20/   37]    Overall Loss 0.591740    Objective Loss 0.591740                                        LR 0.000060    Time 0.022381    
2023-01-06 15:50:45,215 - Epoch: [21][   30/   37]    Overall Loss 0.591469    Objective Loss 0.591469                                        LR 0.000060    Time 0.018940    
2023-01-06 15:50:45,293 - Epoch: [21][   37/   37]    Overall Loss 0.591676    Objective Loss 0.591676    Top1 69.456067    LR 0.000060    Time 0.017448    
2023-01-06 15:50:45,370 - --- validate (epoch=21)-----------
2023-01-06 15:50:45,370 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:45,594 - Epoch: [21][    5/    5]    Loss 0.581336    Top1 68.416031    
2023-01-06 15:50:45,656 - ==> Top1: 68.416    Loss: 0.581

2023-01-06 15:50:45,656 - ==> Confusion:
[[355  74   0]
 [257 362   0]
 [  0   0   0]]

2023-01-06 15:50:45,657 - ==> Best [Top1: 71.183   Sparsity:0.00   Params: 151104 on epoch: 18]
2023-01-06 15:50:45,657 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:45,662 - 

2023-01-06 15:50:45,662 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:45,992 - Epoch: [22][   10/   37]    Overall Loss 0.582849    Objective Loss 0.582849                                        LR 0.000060    Time 0.032992    
2023-01-06 15:50:46,112 - Epoch: [22][   20/   37]    Overall Loss 0.586464    Objective Loss 0.586464                                        LR 0.000060    Time 0.022444    
2023-01-06 15:50:46,234 - Epoch: [22][   30/   37]    Overall Loss 0.582743    Objective Loss 0.582743                                        LR 0.000060    Time 0.019015    
2023-01-06 15:50:46,304 - Epoch: [22][   37/   37]    Overall Loss 0.582243    Objective Loss 0.582243    Top1 69.246862    LR 0.000060    Time 0.017295    
2023-01-06 15:50:46,379 - --- validate (epoch=22)-----------
2023-01-06 15:50:46,379 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:46,597 - Epoch: [22][    5/    5]    Loss 0.576264    Top1 67.843511    
2023-01-06 15:50:46,660 - ==> Top1: 67.844    Loss: 0.576

2023-01-06 15:50:46,661 - ==> Confusion:
[[126 303   0]
 [ 34 585   0]
 [  0   0   0]]

2023-01-06 15:50:46,661 - ==> Best [Top1: 71.183   Sparsity:0.00   Params: 151104 on epoch: 18]
2023-01-06 15:50:46,662 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:46,666 - 

2023-01-06 15:50:46,666 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:47,137 - Epoch: [23][   10/   37]    Overall Loss 0.572045    Objective Loss 0.572045                                        LR 0.000060    Time 0.047003    
2023-01-06 15:50:47,292 - Epoch: [23][   20/   37]    Overall Loss 0.568615    Objective Loss 0.568615                                        LR 0.000060    Time 0.031241    
2023-01-06 15:50:47,438 - Epoch: [23][   30/   37]    Overall Loss 0.569785    Objective Loss 0.569785                                        LR 0.000060    Time 0.025650    
2023-01-06 15:50:47,522 - Epoch: [23][   37/   37]    Overall Loss 0.569318    Objective Loss 0.569318    Top1 74.686192    LR 0.000060    Time 0.023067    
2023-01-06 15:50:47,598 - --- validate (epoch=23)-----------
2023-01-06 15:50:47,598 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:47,820 - Epoch: [23][    5/    5]    Loss 0.545751    Top1 71.660305    
2023-01-06 15:50:47,890 - ==> Top1: 71.660    Loss: 0.546

2023-01-06 15:50:47,890 - ==> Confusion:
[[187 242   0]
 [ 55 564   0]
 [  0   0   0]]

2023-01-06 15:50:47,891 - ==> Best [Top1: 71.660   Sparsity:0.00   Params: 151104 on epoch: 23]
2023-01-06 15:50:47,891 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:47,898 - 

2023-01-06 15:50:47,898 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:48,233 - Epoch: [24][   10/   37]    Overall Loss 0.564113    Objective Loss 0.564113                                        LR 0.000060    Time 0.033422    
2023-01-06 15:50:48,363 - Epoch: [24][   20/   37]    Overall Loss 0.565607    Objective Loss 0.565607                                        LR 0.000060    Time 0.023186    
2023-01-06 15:50:48,519 - Epoch: [24][   30/   37]    Overall Loss 0.565625    Objective Loss 0.565625                                        LR 0.000060    Time 0.020661    
2023-01-06 15:50:48,594 - Epoch: [24][   37/   37]    Overall Loss 0.563779    Objective Loss 0.563779    Top1 74.058577    LR 0.000060    Time 0.018766    
2023-01-06 15:50:48,665 - --- validate (epoch=24)-----------
2023-01-06 15:50:48,666 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:48,883 - Epoch: [24][    5/    5]    Loss 0.534780    Top1 72.137405    
2023-01-06 15:50:48,948 - ==> Top1: 72.137    Loss: 0.535

2023-01-06 15:50:48,949 - ==> Confusion:
[[223 206   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 15:50:48,950 - ==> Best [Top1: 72.137   Sparsity:0.00   Params: 151104 on epoch: 24]
2023-01-06 15:50:48,950 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:48,956 - 

2023-01-06 15:50:48,956 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:49,299 - Epoch: [25][   10/   37]    Overall Loss 0.566167    Objective Loss 0.566167                                        LR 0.000060    Time 0.034247    
2023-01-06 15:50:49,441 - Epoch: [25][   20/   37]    Overall Loss 0.562132    Objective Loss 0.562132                                        LR 0.000060    Time 0.024172    
2023-01-06 15:50:49,560 - Epoch: [25][   30/   37]    Overall Loss 0.555257    Objective Loss 0.555257                                        LR 0.000060    Time 0.020093    
2023-01-06 15:50:49,631 - Epoch: [25][   37/   37]    Overall Loss 0.556426    Objective Loss 0.556426    Top1 70.083682    LR 0.000060    Time 0.018189    
2023-01-06 15:50:49,700 - --- validate (epoch=25)-----------
2023-01-06 15:50:49,700 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:49,919 - Epoch: [25][    5/    5]    Loss 0.548406    Top1 68.416031    
2023-01-06 15:50:49,978 - ==> Top1: 68.416    Loss: 0.548

2023-01-06 15:50:49,979 - ==> Confusion:
[[365  64   0]
 [267 352   0]
 [  0   0   0]]

2023-01-06 15:50:49,979 - ==> Best [Top1: 72.137   Sparsity:0.00   Params: 151104 on epoch: 24]
2023-01-06 15:50:49,980 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:49,985 - 

2023-01-06 15:50:49,985 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:50,302 - Epoch: [26][   10/   37]    Overall Loss 0.546185    Objective Loss 0.546185                                        LR 0.000060    Time 0.031711    
2023-01-06 15:50:50,415 - Epoch: [26][   20/   37]    Overall Loss 0.550258    Objective Loss 0.550258                                        LR 0.000060    Time 0.021463    
2023-01-06 15:50:50,529 - Epoch: [26][   30/   37]    Overall Loss 0.545729    Objective Loss 0.545729                                        LR 0.000060    Time 0.018082    
2023-01-06 15:50:50,605 - Epoch: [26][   37/   37]    Overall Loss 0.549317    Objective Loss 0.549317    Top1 69.665272    LR 0.000060    Time 0.016721    
2023-01-06 15:50:50,679 - --- validate (epoch=26)-----------
2023-01-06 15:50:50,680 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:50,900 - Epoch: [26][    5/    5]    Loss 0.547039    Top1 75.954198    
2023-01-06 15:50:50,972 - ==> Top1: 75.954    Loss: 0.547

2023-01-06 15:50:50,973 - ==> Confusion:
[[312 117   0]
 [135 484   0]
 [  0   0   0]]

2023-01-06 15:50:50,974 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:50,974 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:50,983 - 

2023-01-06 15:50:50,983 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:51,319 - Epoch: [27][   10/   37]    Overall Loss 0.540149    Objective Loss 0.540149                                        LR 0.000060    Time 0.033504    
2023-01-06 15:50:51,455 - Epoch: [27][   20/   37]    Overall Loss 0.544595    Objective Loss 0.544595                                        LR 0.000060    Time 0.023510    
2023-01-06 15:50:51,593 - Epoch: [27][   30/   37]    Overall Loss 0.548407    Objective Loss 0.548407                                        LR 0.000060    Time 0.020272    
2023-01-06 15:50:51,682 - Epoch: [27][   37/   37]    Overall Loss 0.545439    Objective Loss 0.545439    Top1 71.129707    LR 0.000060    Time 0.018839    
2023-01-06 15:50:51,755 - --- validate (epoch=27)-----------
2023-01-06 15:50:51,755 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:51,989 - Epoch: [27][    5/    5]    Loss 0.518150    Top1 72.519084    
2023-01-06 15:50:52,053 - ==> Top1: 72.519    Loss: 0.518

2023-01-06 15:50:52,053 - ==> Confusion:
[[191 238   0]
 [ 50 569   0]
 [  0   0   0]]

2023-01-06 15:50:52,054 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:52,054 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:52,059 - 

2023-01-06 15:50:52,059 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:52,411 - Epoch: [28][   10/   37]    Overall Loss 0.538101    Objective Loss 0.538101                                        LR 0.000060    Time 0.035165    
2023-01-06 15:50:52,562 - Epoch: [28][   20/   37]    Overall Loss 0.540455    Objective Loss 0.540455                                        LR 0.000060    Time 0.025079    
2023-01-06 15:50:52,721 - Epoch: [28][   30/   37]    Overall Loss 0.540826    Objective Loss 0.540826                                        LR 0.000060    Time 0.022028    
2023-01-06 15:50:52,801 - Epoch: [28][   37/   37]    Overall Loss 0.541493    Objective Loss 0.541493    Top1 75.523013    LR 0.000060    Time 0.020014    
2023-01-06 15:50:52,878 - --- validate (epoch=28)-----------
2023-01-06 15:50:52,878 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:53,101 - Epoch: [28][    5/    5]    Loss 0.508171    Top1 73.282443    
2023-01-06 15:50:53,161 - ==> Top1: 73.282    Loss: 0.508

2023-01-06 15:50:53,162 - ==> Confusion:
[[220 209   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:50:53,163 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:53,163 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:53,168 - 

2023-01-06 15:50:53,168 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:53,533 - Epoch: [29][   10/   37]    Overall Loss 0.536886    Objective Loss 0.536886                                        LR 0.000060    Time 0.036437    
2023-01-06 15:50:53,675 - Epoch: [29][   20/   37]    Overall Loss 0.538003    Objective Loss 0.538003                                        LR 0.000060    Time 0.025304    
2023-01-06 15:50:53,818 - Epoch: [29][   30/   37]    Overall Loss 0.537216    Objective Loss 0.537216                                        LR 0.000060    Time 0.021627    
2023-01-06 15:50:53,897 - Epoch: [29][   37/   37]    Overall Loss 0.534554    Objective Loss 0.534554    Top1 76.569038    LR 0.000060    Time 0.019655    
2023-01-06 15:50:53,970 - --- validate (epoch=29)-----------
2023-01-06 15:50:53,971 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:54,192 - Epoch: [29][    5/    5]    Loss 0.533784    Top1 74.332061    
2023-01-06 15:50:54,264 - ==> Top1: 74.332    Loss: 0.534

2023-01-06 15:50:54,265 - ==> Confusion:
[[351  78   0]
 [191 428   0]
 [  0   0   0]]

2023-01-06 15:50:54,266 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:54,266 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:54,271 - 

2023-01-06 15:50:54,271 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:54,617 - Epoch: [30][   10/   37]    Overall Loss 0.534522    Objective Loss 0.534522                                        LR 0.000060    Time 0.034519    
2023-01-06 15:50:54,744 - Epoch: [30][   20/   37]    Overall Loss 0.531459    Objective Loss 0.531459                                        LR 0.000060    Time 0.023589    
2023-01-06 15:50:54,868 - Epoch: [30][   30/   37]    Overall Loss 0.529304    Objective Loss 0.529304                                        LR 0.000060    Time 0.019851    
2023-01-06 15:50:54,939 - Epoch: [30][   37/   37]    Overall Loss 0.533869    Objective Loss 0.533869    Top1 69.037657    LR 0.000060    Time 0.017999    
2023-01-06 15:50:55,015 - --- validate (epoch=30)-----------
2023-01-06 15:50:55,015 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:55,238 - Epoch: [30][    5/    5]    Loss 0.492759    Top1 75.190840    
2023-01-06 15:50:55,302 - ==> Top1: 75.191    Loss: 0.493

2023-01-06 15:50:55,302 - ==> Confusion:
[[244 185   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:50:55,303 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:55,303 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:55,308 - 

2023-01-06 15:50:55,308 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:55,640 - Epoch: [31][   10/   37]    Overall Loss 0.535043    Objective Loss 0.535043                                        LR 0.000060    Time 0.033126    
2023-01-06 15:50:55,763 - Epoch: [31][   20/   37]    Overall Loss 0.532513    Objective Loss 0.532513                                        LR 0.000060    Time 0.022711    
2023-01-06 15:50:55,888 - Epoch: [31][   30/   37]    Overall Loss 0.532557    Objective Loss 0.532557                                        LR 0.000060    Time 0.019282    
2023-01-06 15:50:55,964 - Epoch: [31][   37/   37]    Overall Loss 0.526708    Objective Loss 0.526708    Top1 78.870293    LR 0.000060    Time 0.017682    
2023-01-06 15:50:56,034 - --- validate (epoch=31)-----------
2023-01-06 15:50:56,034 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:56,271 - Epoch: [31][    5/    5]    Loss 0.492588    Top1 73.950382    
2023-01-06 15:50:56,343 - ==> Top1: 73.950    Loss: 0.493

2023-01-06 15:50:56,343 - ==> Confusion:
[[203 226   0]
 [ 47 572   0]
 [  0   0   0]]

2023-01-06 15:50:56,344 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:56,344 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:56,349 - 

2023-01-06 15:50:56,349 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:56,696 - Epoch: [32][   10/   37]    Overall Loss 0.526846    Objective Loss 0.526846                                        LR 0.000060    Time 0.034583    
2023-01-06 15:50:56,817 - Epoch: [32][   20/   37]    Overall Loss 0.527066    Objective Loss 0.527066                                        LR 0.000060    Time 0.023355    
2023-01-06 15:50:56,959 - Epoch: [32][   30/   37]    Overall Loss 0.523764    Objective Loss 0.523764                                        LR 0.000060    Time 0.020294    
2023-01-06 15:50:57,037 - Epoch: [32][   37/   37]    Overall Loss 0.522278    Objective Loss 0.522278    Top1 72.803347    LR 0.000060    Time 0.018541    
2023-01-06 15:50:57,120 - --- validate (epoch=32)-----------
2023-01-06 15:50:57,120 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:57,349 - Epoch: [32][    5/    5]    Loss 0.504672    Top1 72.137405    
2023-01-06 15:50:57,409 - ==> Top1: 72.137    Loss: 0.505

2023-01-06 15:50:57,410 - ==> Confusion:
[[171 258   0]
 [ 34 585   0]
 [  0   0   0]]

2023-01-06 15:50:57,411 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:57,411 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:57,416 - 

2023-01-06 15:50:57,416 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:57,756 - Epoch: [33][   10/   37]    Overall Loss 0.524253    Objective Loss 0.524253                                        LR 0.000060    Time 0.033937    
2023-01-06 15:50:57,903 - Epoch: [33][   20/   37]    Overall Loss 0.521156    Objective Loss 0.521156                                        LR 0.000060    Time 0.024319    
2023-01-06 15:50:58,045 - Epoch: [33][   30/   37]    Overall Loss 0.517494    Objective Loss 0.517494                                        LR 0.000060    Time 0.020907    
2023-01-06 15:50:58,128 - Epoch: [33][   37/   37]    Overall Loss 0.516864    Objective Loss 0.516864    Top1 74.058577    LR 0.000060    Time 0.019188    
2023-01-06 15:50:58,203 - --- validate (epoch=33)-----------
2023-01-06 15:50:58,204 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:58,428 - Epoch: [33][    5/    5]    Loss 0.502917    Top1 75.667939    
2023-01-06 15:50:58,498 - ==> Top1: 75.668    Loss: 0.503

2023-01-06 15:50:58,498 - ==> Confusion:
[[244 185   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:50:58,499 - ==> Best [Top1: 75.954   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 15:50:58,499 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:58,504 - 

2023-01-06 15:50:58,505 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:58,871 - Epoch: [34][   10/   37]    Overall Loss 0.513303    Objective Loss 0.513303                                        LR 0.000060    Time 0.036586    
2023-01-06 15:50:58,995 - Epoch: [34][   20/   37]    Overall Loss 0.515370    Objective Loss 0.515370                                        LR 0.000060    Time 0.024475    
2023-01-06 15:50:59,128 - Epoch: [34][   30/   37]    Overall Loss 0.510994    Objective Loss 0.510994                                        LR 0.000060    Time 0.020720    
2023-01-06 15:50:59,197 - Epoch: [34][   37/   37]    Overall Loss 0.506459    Objective Loss 0.506459    Top1 78.661088    LR 0.000060    Time 0.018656    
2023-01-06 15:50:59,277 - --- validate (epoch=34)-----------
2023-01-06 15:50:59,277 - 1048 samples (256 per mini-batch)
2023-01-06 15:50:59,507 - Epoch: [34][    5/    5]    Loss 0.483901    Top1 76.431298    
2023-01-06 15:50:59,576 - ==> Top1: 76.431    Loss: 0.484

2023-01-06 15:50:59,576 - ==> Confusion:
[[286 143   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 15:50:59,577 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 34]
2023-01-06 15:50:59,577 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:50:59,584 - 

2023-01-06 15:50:59,584 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:50:59,924 - Epoch: [35][   10/   37]    Overall Loss 0.500613    Objective Loss 0.500613                                        LR 0.000060    Time 0.033938    
2023-01-06 15:51:00,060 - Epoch: [35][   20/   37]    Overall Loss 0.498082    Objective Loss 0.498082                                        LR 0.000060    Time 0.023747    
2023-01-06 15:51:00,215 - Epoch: [35][   30/   37]    Overall Loss 0.503781    Objective Loss 0.503781                                        LR 0.000060    Time 0.020995    
2023-01-06 15:51:00,301 - Epoch: [35][   37/   37]    Overall Loss 0.503802    Objective Loss 0.503802    Top1 74.267782    LR 0.000060    Time 0.019326    
2023-01-06 15:51:00,375 - --- validate (epoch=35)-----------
2023-01-06 15:51:00,375 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:00,594 - Epoch: [35][    5/    5]    Loss 0.488671    Top1 74.427481    
2023-01-06 15:51:00,655 - ==> Top1: 74.427    Loss: 0.489

2023-01-06 15:51:00,655 - ==> Confusion:
[[365  64   0]
 [204 415   0]
 [  0   0   0]]

2023-01-06 15:51:00,656 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 34]
2023-01-06 15:51:00,656 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:00,661 - 

2023-01-06 15:51:00,661 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:00,994 - Epoch: [36][   10/   37]    Overall Loss 0.500207    Objective Loss 0.500207                                        LR 0.000060    Time 0.033216    
2023-01-06 15:51:01,117 - Epoch: [36][   20/   37]    Overall Loss 0.498008    Objective Loss 0.498008                                        LR 0.000060    Time 0.022767    
2023-01-06 15:51:01,238 - Epoch: [36][   30/   37]    Overall Loss 0.499072    Objective Loss 0.499072                                        LR 0.000060    Time 0.019182    
2023-01-06 15:51:01,316 - Epoch: [36][   37/   37]    Overall Loss 0.496954    Objective Loss 0.496954    Top1 75.104603    LR 0.000060    Time 0.017648    
2023-01-06 15:51:01,385 - --- validate (epoch=36)-----------
2023-01-06 15:51:01,385 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:01,613 - Epoch: [36][    5/    5]    Loss 0.496951    Top1 76.335878    
2023-01-06 15:51:01,681 - ==> Top1: 76.336    Loss: 0.497

2023-01-06 15:51:01,681 - ==> Confusion:
[[251 178   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:51:01,682 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 34]
2023-01-06 15:51:01,682 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:01,687 - 

2023-01-06 15:51:01,688 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:02,030 - Epoch: [37][   10/   37]    Overall Loss 0.484043    Objective Loss 0.484043                                        LR 0.000060    Time 0.034187    
2023-01-06 15:51:02,165 - Epoch: [37][   20/   37]    Overall Loss 0.487049    Objective Loss 0.487049                                        LR 0.000060    Time 0.023789    
2023-01-06 15:51:02,299 - Epoch: [37][   30/   37]    Overall Loss 0.489505    Objective Loss 0.489505                                        LR 0.000060    Time 0.020329    
2023-01-06 15:51:02,387 - Epoch: [37][   37/   37]    Overall Loss 0.489574    Objective Loss 0.489574    Top1 74.267782    LR 0.000060    Time 0.018844    
2023-01-06 15:51:02,458 - --- validate (epoch=37)-----------
2023-01-06 15:51:02,458 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:02,690 - Epoch: [37][    5/    5]    Loss 0.469258    Top1 76.431298    
2023-01-06 15:51:02,761 - ==> Top1: 76.431    Loss: 0.469

2023-01-06 15:51:02,761 - ==> Confusion:
[[253 176   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:51:02,762 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 37]
2023-01-06 15:51:02,762 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:02,769 - 

2023-01-06 15:51:02,769 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:03,278 - Epoch: [38][   10/   37]    Overall Loss 0.499082    Objective Loss 0.499082                                        LR 0.000060    Time 0.050817    
2023-01-06 15:51:03,442 - Epoch: [38][   20/   37]    Overall Loss 0.491356    Objective Loss 0.491356                                        LR 0.000060    Time 0.033616    
2023-01-06 15:51:03,598 - Epoch: [38][   30/   37]    Overall Loss 0.489425    Objective Loss 0.489425                                        LR 0.000060    Time 0.027526    
2023-01-06 15:51:03,678 - Epoch: [38][   37/   37]    Overall Loss 0.487635    Objective Loss 0.487635    Top1 78.033473    LR 0.000060    Time 0.024466    
2023-01-06 15:51:03,754 - --- validate (epoch=38)-----------
2023-01-06 15:51:03,754 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:03,979 - Epoch: [38][    5/    5]    Loss 0.508676    Top1 75.381679    
2023-01-06 15:51:04,041 - ==> Top1: 75.382    Loss: 0.509

2023-01-06 15:51:04,041 - ==> Confusion:
[[374  55   0]
 [203 416   0]
 [  0   0   0]]

2023-01-06 15:51:04,042 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 37]
2023-01-06 15:51:04,042 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:04,047 - 

2023-01-06 15:51:04,048 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:04,389 - Epoch: [39][   10/   37]    Overall Loss 0.477816    Objective Loss 0.477816                                        LR 0.000060    Time 0.034111    
2023-01-06 15:51:04,539 - Epoch: [39][   20/   37]    Overall Loss 0.485898    Objective Loss 0.485898                                        LR 0.000060    Time 0.024516    
2023-01-06 15:51:04,694 - Epoch: [39][   30/   37]    Overall Loss 0.485503    Objective Loss 0.485503                                        LR 0.000060    Time 0.021465    
2023-01-06 15:51:04,775 - Epoch: [39][   37/   37]    Overall Loss 0.480987    Objective Loss 0.480987    Top1 78.870293    LR 0.000060    Time 0.019578    
2023-01-06 15:51:04,844 - --- validate (epoch=39)-----------
2023-01-06 15:51:04,844 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:05,070 - Epoch: [39][    5/    5]    Loss 0.515185    Top1 73.473282    
2023-01-06 15:51:05,149 - ==> Top1: 73.473    Loss: 0.515

2023-01-06 15:51:05,149 - ==> Confusion:
[[379  50   0]
 [228 391   0]
 [  0   0   0]]

2023-01-06 15:51:05,150 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 151104 on epoch: 37]
2023-01-06 15:51:05,150 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:05,155 - 

2023-01-06 15:51:05,155 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:05,518 - Epoch: [40][   10/   37]    Overall Loss 0.475312    Objective Loss 0.475312                                        LR 0.000036    Time 0.036204    
2023-01-06 15:51:05,668 - Epoch: [40][   20/   37]    Overall Loss 0.480027    Objective Loss 0.480027                                        LR 0.000036    Time 0.025588    
2023-01-06 15:51:05,818 - Epoch: [40][   30/   37]    Overall Loss 0.479868    Objective Loss 0.479868                                        LR 0.000036    Time 0.022030    
2023-01-06 15:51:05,900 - Epoch: [40][   37/   37]    Overall Loss 0.475327    Objective Loss 0.475327    Top1 78.033473    LR 0.000036    Time 0.020070    
2023-01-06 15:51:05,979 - --- validate (epoch=40)-----------
2023-01-06 15:51:05,980 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:06,202 - Epoch: [40][    5/    5]    Loss 0.472101    Top1 78.148855    
2023-01-06 15:51:06,273 - ==> Top1: 78.149    Loss: 0.472

2023-01-06 15:51:06,273 - ==> Confusion:
[[286 143   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 15:51:06,274 - ==> Best [Top1: 78.149   Sparsity:0.00   Params: 151104 on epoch: 40]
2023-01-06 15:51:06,274 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:06,280 - 

2023-01-06 15:51:06,280 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:06,620 - Epoch: [41][   10/   37]    Overall Loss 0.464709    Objective Loss 0.464709                                        LR 0.000036    Time 0.033890    
2023-01-06 15:51:06,766 - Epoch: [41][   20/   37]    Overall Loss 0.470098    Objective Loss 0.470098                                        LR 0.000036    Time 0.024209    
2023-01-06 15:51:06,905 - Epoch: [41][   30/   37]    Overall Loss 0.468099    Objective Loss 0.468099                                        LR 0.000036    Time 0.020774    
2023-01-06 15:51:06,986 - Epoch: [41][   37/   37]    Overall Loss 0.469760    Objective Loss 0.469760    Top1 78.451883    LR 0.000036    Time 0.019034    
2023-01-06 15:51:07,061 - --- validate (epoch=41)-----------
2023-01-06 15:51:07,061 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:07,284 - Epoch: [41][    5/    5]    Loss 0.451294    Top1 79.007634    
2023-01-06 15:51:07,351 - ==> Top1: 79.008    Loss: 0.451

2023-01-06 15:51:07,351 - ==> Confusion:
[[333  96   0]
 [124 495   0]
 [  0   0   0]]

2023-01-06 15:51:07,352 - ==> Best [Top1: 79.008   Sparsity:0.00   Params: 151104 on epoch: 41]
2023-01-06 15:51:07,352 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:07,359 - 

2023-01-06 15:51:07,359 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:07,686 - Epoch: [42][   10/   37]    Overall Loss 0.473786    Objective Loss 0.473786                                        LR 0.000036    Time 0.032623    
2023-01-06 15:51:07,806 - Epoch: [42][   20/   37]    Overall Loss 0.477626    Objective Loss 0.477626                                        LR 0.000036    Time 0.022318    
2023-01-06 15:51:07,934 - Epoch: [42][   30/   37]    Overall Loss 0.470354    Objective Loss 0.470354                                        LR 0.000036    Time 0.019120    
2023-01-06 15:51:08,010 - Epoch: [42][   37/   37]    Overall Loss 0.468403    Objective Loss 0.468403    Top1 81.171548    LR 0.000036    Time 0.017548    
2023-01-06 15:51:08,091 - --- validate (epoch=42)-----------
2023-01-06 15:51:08,092 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:08,329 - Epoch: [42][    5/    5]    Loss 0.461124    Top1 77.576336    
2023-01-06 15:51:08,391 - ==> Top1: 77.576    Loss: 0.461

2023-01-06 15:51:08,391 - ==> Confusion:
[[287 142   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-06 15:51:08,392 - ==> Best [Top1: 79.008   Sparsity:0.00   Params: 151104 on epoch: 41]
2023-01-06 15:51:08,392 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:08,399 - 

2023-01-06 15:51:08,400 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:08,747 - Epoch: [43][   10/   37]    Overall Loss 0.469722    Objective Loss 0.469722                                        LR 0.000036    Time 0.034685    
2023-01-06 15:51:08,879 - Epoch: [43][   20/   37]    Overall Loss 0.466695    Objective Loss 0.466695                                        LR 0.000036    Time 0.023915    
2023-01-06 15:51:09,014 - Epoch: [43][   30/   37]    Overall Loss 0.469303    Objective Loss 0.469303                                        LR 0.000036    Time 0.020416    
2023-01-06 15:51:09,093 - Epoch: [43][   37/   37]    Overall Loss 0.466321    Objective Loss 0.466321    Top1 80.962343    LR 0.000036    Time 0.018690    
2023-01-06 15:51:09,166 - --- validate (epoch=43)-----------
2023-01-06 15:51:09,166 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:09,395 - Epoch: [43][    5/    5]    Loss 0.451591    Top1 77.099237    
2023-01-06 15:51:09,457 - ==> Top1: 77.099    Loss: 0.452

2023-01-06 15:51:09,457 - ==> Confusion:
[[264 165   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:51:09,458 - ==> Best [Top1: 79.008   Sparsity:0.00   Params: 151104 on epoch: 41]
2023-01-06 15:51:09,458 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:09,463 - 

2023-01-06 15:51:09,463 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:09,801 - Epoch: [44][   10/   37]    Overall Loss 0.457237    Objective Loss 0.457237                                        LR 0.000036    Time 0.033648    
2023-01-06 15:51:09,921 - Epoch: [44][   20/   37]    Overall Loss 0.467335    Objective Loss 0.467335                                        LR 0.000036    Time 0.022812    
2023-01-06 15:51:10,052 - Epoch: [44][   30/   37]    Overall Loss 0.462244    Objective Loss 0.462244                                        LR 0.000036    Time 0.019567    
2023-01-06 15:51:10,126 - Epoch: [44][   37/   37]    Overall Loss 0.463397    Objective Loss 0.463397    Top1 81.589958    LR 0.000036    Time 0.017867    
2023-01-06 15:51:10,197 - --- validate (epoch=44)-----------
2023-01-06 15:51:10,197 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:10,422 - Epoch: [44][    5/    5]    Loss 0.450124    Top1 79.866412    
2023-01-06 15:51:10,484 - ==> Top1: 79.866    Loss: 0.450

2023-01-06 15:51:10,485 - ==> Confusion:
[[325 104   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 15:51:10,486 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:10,486 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:10,492 - 

2023-01-06 15:51:10,493 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:10,837 - Epoch: [45][   10/   37]    Overall Loss 0.467586    Objective Loss 0.467586                                        LR 0.000036    Time 0.034376    
2023-01-06 15:51:10,961 - Epoch: [45][   20/   37]    Overall Loss 0.465154    Objective Loss 0.465154                                        LR 0.000036    Time 0.023370    
2023-01-06 15:51:11,091 - Epoch: [45][   30/   37]    Overall Loss 0.461282    Objective Loss 0.461282                                        LR 0.000036    Time 0.019902    
2023-01-06 15:51:11,164 - Epoch: [45][   37/   37]    Overall Loss 0.458699    Objective Loss 0.458699    Top1 77.824268    LR 0.000036    Time 0.018090    
2023-01-06 15:51:11,235 - --- validate (epoch=45)-----------
2023-01-06 15:51:11,235 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:11,471 - Epoch: [45][    5/    5]    Loss 0.460962    Top1 76.049618    
2023-01-06 15:51:11,534 - ==> Top1: 76.050    Loss: 0.461

2023-01-06 15:51:11,534 - ==> Confusion:
[[218 211   0]
 [ 40 579   0]
 [  0   0   0]]

2023-01-06 15:51:11,535 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:11,535 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:11,540 - 

2023-01-06 15:51:11,540 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:11,898 - Epoch: [46][   10/   37]    Overall Loss 0.455890    Objective Loss 0.455890                                        LR 0.000036    Time 0.035770    
2023-01-06 15:51:12,045 - Epoch: [46][   20/   37]    Overall Loss 0.460067    Objective Loss 0.460067                                        LR 0.000036    Time 0.025203    
2023-01-06 15:51:12,187 - Epoch: [46][   30/   37]    Overall Loss 0.461427    Objective Loss 0.461427                                        LR 0.000036    Time 0.021518    
2023-01-06 15:51:12,264 - Epoch: [46][   37/   37]    Overall Loss 0.459679    Objective Loss 0.459679    Top1 76.778243    LR 0.000036    Time 0.019523    
2023-01-06 15:51:12,347 - --- validate (epoch=46)-----------
2023-01-06 15:51:12,347 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:12,568 - Epoch: [46][    5/    5]    Loss 0.462120    Top1 77.385496    
2023-01-06 15:51:12,626 - ==> Top1: 77.385    Loss: 0.462

2023-01-06 15:51:12,626 - ==> Confusion:
[[360  69   0]
 [168 451   0]
 [  0   0   0]]

2023-01-06 15:51:12,627 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:12,627 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:12,632 - 

2023-01-06 15:51:12,632 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:12,966 - Epoch: [47][   10/   37]    Overall Loss 0.462867    Objective Loss 0.462867                                        LR 0.000036    Time 0.033374    
2023-01-06 15:51:13,107 - Epoch: [47][   20/   37]    Overall Loss 0.460642    Objective Loss 0.460642                                        LR 0.000036    Time 0.023694    
2023-01-06 15:51:13,243 - Epoch: [47][   30/   37]    Overall Loss 0.457649    Objective Loss 0.457649                                        LR 0.000036    Time 0.020305    
2023-01-06 15:51:13,315 - Epoch: [47][   37/   37]    Overall Loss 0.454786    Objective Loss 0.454786    Top1 79.497908    LR 0.000036    Time 0.018416    
2023-01-06 15:51:13,380 - --- validate (epoch=47)-----------
2023-01-06 15:51:13,380 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:13,604 - Epoch: [47][    5/    5]    Loss 0.429520    Top1 78.625954    
2023-01-06 15:51:13,679 - ==> Top1: 78.626    Loss: 0.430

2023-01-06 15:51:13,680 - ==> Confusion:
[[292 137   0]
 [ 87 532   0]
 [  0   0   0]]

2023-01-06 15:51:13,681 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:13,681 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:13,686 - 

2023-01-06 15:51:13,686 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:14,021 - Epoch: [48][   10/   37]    Overall Loss 0.450371    Objective Loss 0.450371                                        LR 0.000036    Time 0.033408    
2023-01-06 15:51:14,151 - Epoch: [48][   20/   37]    Overall Loss 0.446888    Objective Loss 0.446888                                        LR 0.000036    Time 0.023204    
2023-01-06 15:51:14,286 - Epoch: [48][   30/   37]    Overall Loss 0.451508    Objective Loss 0.451508                                        LR 0.000036    Time 0.019947    
2023-01-06 15:51:14,360 - Epoch: [48][   37/   37]    Overall Loss 0.451914    Objective Loss 0.451914    Top1 76.150628    LR 0.000036    Time 0.018158    
2023-01-06 15:51:14,431 - --- validate (epoch=48)-----------
2023-01-06 15:51:14,431 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:14,649 - Epoch: [48][    5/    5]    Loss 0.433886    Top1 77.767176    
2023-01-06 15:51:14,712 - ==> Top1: 77.767    Loss: 0.434

2023-01-06 15:51:14,712 - ==> Confusion:
[[269 160   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 15:51:14,713 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:14,713 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:14,718 - 

2023-01-06 15:51:14,718 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:15,048 - Epoch: [49][   10/   37]    Overall Loss 0.463506    Objective Loss 0.463506                                        LR 0.000036    Time 0.032910    
2023-01-06 15:51:15,166 - Epoch: [49][   20/   37]    Overall Loss 0.453713    Objective Loss 0.453713                                        LR 0.000036    Time 0.022336    
2023-01-06 15:51:15,297 - Epoch: [49][   30/   37]    Overall Loss 0.447606    Objective Loss 0.447606                                        LR 0.000036    Time 0.019236    
2023-01-06 15:51:15,367 - Epoch: [49][   37/   37]    Overall Loss 0.449849    Objective Loss 0.449849    Top1 82.008368    LR 0.000036    Time 0.017488    
2023-01-06 15:51:15,434 - --- validate (epoch=49)-----------
2023-01-06 15:51:15,434 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:15,664 - Epoch: [49][    5/    5]    Loss 0.444114    Top1 78.625954    
2023-01-06 15:51:15,741 - ==> Top1: 78.626    Loss: 0.444

2023-01-06 15:51:15,741 - ==> Confusion:
[[297 132   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 15:51:15,742 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:15,742 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:15,747 - 

2023-01-06 15:51:15,747 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:16,082 - Epoch: [50][   10/   37]    Overall Loss 0.430951    Objective Loss 0.430951                                        LR 0.000036    Time 0.033432    
2023-01-06 15:51:16,215 - Epoch: [50][   20/   37]    Overall Loss 0.438703    Objective Loss 0.438703                                        LR 0.000036    Time 0.023346    
2023-01-06 15:51:16,337 - Epoch: [50][   30/   37]    Overall Loss 0.446131    Objective Loss 0.446131                                        LR 0.000036    Time 0.019622    
2023-01-06 15:51:16,406 - Epoch: [50][   37/   37]    Overall Loss 0.445175    Objective Loss 0.445175    Top1 77.196653    LR 0.000036    Time 0.017765    
2023-01-06 15:51:16,488 - --- validate (epoch=50)-----------
2023-01-06 15:51:16,488 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:16,708 - Epoch: [50][    5/    5]    Loss 0.470418    Top1 78.148855    
2023-01-06 15:51:16,774 - ==> Top1: 78.149    Loss: 0.470

2023-01-06 15:51:16,774 - ==> Confusion:
[[269 160   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 15:51:16,775 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:16,775 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:16,780 - 

2023-01-06 15:51:16,780 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:17,108 - Epoch: [51][   10/   37]    Overall Loss 0.444436    Objective Loss 0.444436                                        LR 0.000036    Time 0.032693    
2023-01-06 15:51:17,226 - Epoch: [51][   20/   37]    Overall Loss 0.442016    Objective Loss 0.442016                                        LR 0.000036    Time 0.022264    
2023-01-06 15:51:17,347 - Epoch: [51][   30/   37]    Overall Loss 0.442697    Objective Loss 0.442697                                        LR 0.000036    Time 0.018842    
2023-01-06 15:51:17,417 - Epoch: [51][   37/   37]    Overall Loss 0.442408    Objective Loss 0.442408    Top1 81.171548    LR 0.000036    Time 0.017161    
2023-01-06 15:51:17,490 - --- validate (epoch=51)-----------
2023-01-06 15:51:17,490 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:17,715 - Epoch: [51][    5/    5]    Loss 0.427124    Top1 79.580153    
2023-01-06 15:51:17,790 - ==> Top1: 79.580    Loss: 0.427

2023-01-06 15:51:17,790 - ==> Confusion:
[[333  96   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 15:51:17,791 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:17,791 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:17,796 - 

2023-01-06 15:51:17,796 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:18,124 - Epoch: [52][   10/   37]    Overall Loss 0.445867    Objective Loss 0.445867                                        LR 0.000036    Time 0.032650    
2023-01-06 15:51:18,243 - Epoch: [52][   20/   37]    Overall Loss 0.442591    Objective Loss 0.442591                                        LR 0.000036    Time 0.022290    
2023-01-06 15:51:18,364 - Epoch: [52][   30/   37]    Overall Loss 0.443488    Objective Loss 0.443488                                        LR 0.000036    Time 0.018875    
2023-01-06 15:51:18,436 - Epoch: [52][   37/   37]    Overall Loss 0.439811    Objective Loss 0.439811    Top1 81.799163    LR 0.000036    Time 0.017238    
2023-01-06 15:51:18,507 - --- validate (epoch=52)-----------
2023-01-06 15:51:18,507 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:18,725 - Epoch: [52][    5/    5]    Loss 0.446084    Top1 77.099237    
2023-01-06 15:51:18,794 - ==> Top1: 77.099    Loss: 0.446

2023-01-06 15:51:18,794 - ==> Confusion:
[[254 175   0]
 [ 65 554   0]
 [  0   0   0]]

2023-01-06 15:51:18,795 - ==> Best [Top1: 79.866   Sparsity:0.00   Params: 151104 on epoch: 44]
2023-01-06 15:51:18,795 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:18,800 - 

2023-01-06 15:51:18,800 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:19,136 - Epoch: [53][   10/   37]    Overall Loss 0.444750    Objective Loss 0.444750                                        LR 0.000036    Time 0.033499    
2023-01-06 15:51:19,260 - Epoch: [53][   20/   37]    Overall Loss 0.438773    Objective Loss 0.438773                                        LR 0.000036    Time 0.022921    
2023-01-06 15:51:19,387 - Epoch: [53][   30/   37]    Overall Loss 0.441072    Objective Loss 0.441072                                        LR 0.000036    Time 0.019509    
2023-01-06 15:51:19,465 - Epoch: [53][   37/   37]    Overall Loss 0.439216    Objective Loss 0.439216    Top1 82.635983    LR 0.000036    Time 0.017924    
2023-01-06 15:51:19,543 - --- validate (epoch=53)-----------
2023-01-06 15:51:19,543 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:20,001 - Epoch: [53][    5/    5]    Loss 0.435812    Top1 80.057252    
2023-01-06 15:51:20,068 - ==> Top1: 80.057    Loss: 0.436

2023-01-06 15:51:20,068 - ==> Confusion:
[[333  96   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 15:51:20,069 - ==> Best [Top1: 80.057   Sparsity:0.00   Params: 151104 on epoch: 53]
2023-01-06 15:51:20,069 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:20,076 - 

2023-01-06 15:51:20,076 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:20,418 - Epoch: [54][   10/   37]    Overall Loss 0.437182    Objective Loss 0.437182                                        LR 0.000036    Time 0.034181    
2023-01-06 15:51:20,541 - Epoch: [54][   20/   37]    Overall Loss 0.430279    Objective Loss 0.430279                                        LR 0.000036    Time 0.023204    
2023-01-06 15:51:20,680 - Epoch: [54][   30/   37]    Overall Loss 0.436469    Objective Loss 0.436469                                        LR 0.000036    Time 0.020095    
2023-01-06 15:51:20,754 - Epoch: [54][   37/   37]    Overall Loss 0.435227    Objective Loss 0.435227    Top1 79.288703    LR 0.000036    Time 0.018280    
2023-01-06 15:51:20,831 - --- validate (epoch=54)-----------
2023-01-06 15:51:20,831 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:21,062 - Epoch: [54][    5/    5]    Loss 0.419902    Top1 78.339695    
2023-01-06 15:51:21,127 - ==> Top1: 78.340    Loss: 0.420

2023-01-06 15:51:21,127 - ==> Confusion:
[[271 158   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 15:51:21,128 - ==> Best [Top1: 80.057   Sparsity:0.00   Params: 151104 on epoch: 53]
2023-01-06 15:51:21,128 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:21,133 - 

2023-01-06 15:51:21,134 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:21,459 - Epoch: [55][   10/   37]    Overall Loss 0.426248    Objective Loss 0.426248                                        LR 0.000036    Time 0.032472    
2023-01-06 15:51:21,572 - Epoch: [55][   20/   37]    Overall Loss 0.427297    Objective Loss 0.427297                                        LR 0.000036    Time 0.021896    
2023-01-06 15:51:21,690 - Epoch: [55][   30/   37]    Overall Loss 0.430225    Objective Loss 0.430225                                        LR 0.000036    Time 0.018506    
2023-01-06 15:51:21,761 - Epoch: [55][   37/   37]    Overall Loss 0.431566    Objective Loss 0.431566    Top1 77.824268    LR 0.000036    Time 0.016920    
2023-01-06 15:51:21,841 - --- validate (epoch=55)-----------
2023-01-06 15:51:21,841 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:22,061 - Epoch: [55][    5/    5]    Loss 0.411992    Top1 79.389313    
2023-01-06 15:51:22,124 - ==> Top1: 79.389    Loss: 0.412

2023-01-06 15:51:22,124 - ==> Confusion:
[[294 135   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 15:51:22,125 - ==> Best [Top1: 80.057   Sparsity:0.00   Params: 151104 on epoch: 53]
2023-01-06 15:51:22,125 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:22,130 - 

2023-01-06 15:51:22,131 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:22,459 - Epoch: [56][   10/   37]    Overall Loss 0.429465    Objective Loss 0.429465                                        LR 0.000036    Time 0.032798    
2023-01-06 15:51:22,578 - Epoch: [56][   20/   37]    Overall Loss 0.432146    Objective Loss 0.432146                                        LR 0.000036    Time 0.022299    
2023-01-06 15:51:22,696 - Epoch: [56][   30/   37]    Overall Loss 0.431892    Objective Loss 0.431892                                        LR 0.000036    Time 0.018795    
2023-01-06 15:51:22,767 - Epoch: [56][   37/   37]    Overall Loss 0.428705    Objective Loss 0.428705    Top1 78.661088    LR 0.000036    Time 0.017146    
2023-01-06 15:51:22,849 - --- validate (epoch=56)-----------
2023-01-06 15:51:22,849 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:23,072 - Epoch: [56][    5/    5]    Loss 0.442022    Top1 80.725191    
2023-01-06 15:51:23,133 - ==> Top1: 80.725    Loss: 0.442

2023-01-06 15:51:23,133 - ==> Confusion:
[[317 112   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-06 15:51:23,134 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 15:51:23,134 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:23,141 - 

2023-01-06 15:51:23,141 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:23,470 - Epoch: [57][   10/   37]    Overall Loss 0.438603    Objective Loss 0.438603                                        LR 0.000036    Time 0.032786    
2023-01-06 15:51:23,597 - Epoch: [57][   20/   37]    Overall Loss 0.436795    Objective Loss 0.436795                                        LR 0.000036    Time 0.022731    
2023-01-06 15:51:23,729 - Epoch: [57][   30/   37]    Overall Loss 0.429111    Objective Loss 0.429111                                        LR 0.000036    Time 0.019559    
2023-01-06 15:51:23,803 - Epoch: [57][   37/   37]    Overall Loss 0.427612    Objective Loss 0.427612    Top1 79.916318    LR 0.000036    Time 0.017846    
2023-01-06 15:51:23,881 - --- validate (epoch=57)-----------
2023-01-06 15:51:23,881 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:24,102 - Epoch: [57][    5/    5]    Loss 0.439563    Top1 78.816794    
2023-01-06 15:51:24,180 - ==> Top1: 78.817    Loss: 0.440

2023-01-06 15:51:24,180 - ==> Confusion:
[[288 141   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 15:51:24,181 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 15:51:24,181 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:24,186 - 

2023-01-06 15:51:24,186 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:24,518 - Epoch: [58][   10/   37]    Overall Loss 0.429835    Objective Loss 0.429835                                        LR 0.000036    Time 0.033092    
2023-01-06 15:51:24,642 - Epoch: [58][   20/   37]    Overall Loss 0.429218    Objective Loss 0.429218                                        LR 0.000036    Time 0.022700    
2023-01-06 15:51:24,767 - Epoch: [58][   30/   37]    Overall Loss 0.428143    Objective Loss 0.428143                                        LR 0.000036    Time 0.019292    
2023-01-06 15:51:24,845 - Epoch: [58][   37/   37]    Overall Loss 0.426599    Objective Loss 0.426599    Top1 78.451883    LR 0.000036    Time 0.017749    
2023-01-06 15:51:24,916 - --- validate (epoch=58)-----------
2023-01-06 15:51:24,917 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:25,152 - Epoch: [58][    5/    5]    Loss 0.420833    Top1 79.198473    
2023-01-06 15:51:25,225 - ==> Top1: 79.198    Loss: 0.421

2023-01-06 15:51:25,225 - ==> Confusion:
[[276 153   0]
 [ 65 554   0]
 [  0   0   0]]

2023-01-06 15:51:25,226 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 15:51:25,226 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:25,231 - 

2023-01-06 15:51:25,231 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:25,569 - Epoch: [59][   10/   37]    Overall Loss 0.432350    Objective Loss 0.432350                                        LR 0.000036    Time 0.033716    
2023-01-06 15:51:25,690 - Epoch: [59][   20/   37]    Overall Loss 0.424384    Objective Loss 0.424384                                        LR 0.000036    Time 0.022884    
2023-01-06 15:51:25,818 - Epoch: [59][   30/   37]    Overall Loss 0.424734    Objective Loss 0.424734                                        LR 0.000036    Time 0.019501    
2023-01-06 15:51:25,887 - Epoch: [59][   37/   37]    Overall Loss 0.424005    Objective Loss 0.424005    Top1 82.008368    LR 0.000036    Time 0.017669    
2023-01-06 15:51:25,964 - --- validate (epoch=59)-----------
2023-01-06 15:51:25,964 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:26,188 - Epoch: [59][    5/    5]    Loss 0.450316    Top1 80.057252    
2023-01-06 15:51:26,243 - ==> Top1: 80.057    Loss: 0.450

2023-01-06 15:51:26,244 - ==> Confusion:
[[349  80   0]
 [129 490   0]
 [  0   0   0]]

2023-01-06 15:51:26,245 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 15:51:26,245 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:26,253 - 

2023-01-06 15:51:26,253 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:26,587 - Epoch: [60][   10/   37]    Overall Loss 0.411064    Objective Loss 0.411064                                        LR 0.000036    Time 0.033333    
2023-01-06 15:51:26,728 - Epoch: [60][   20/   37]    Overall Loss 0.418284    Objective Loss 0.418284                                        LR 0.000036    Time 0.023688    
2023-01-06 15:51:26,869 - Epoch: [60][   30/   37]    Overall Loss 0.421858    Objective Loss 0.421858                                        LR 0.000036    Time 0.020458    
2023-01-06 15:51:26,950 - Epoch: [60][   37/   37]    Overall Loss 0.420774    Objective Loss 0.420774    Top1 81.799163    LR 0.000036    Time 0.018790    
2023-01-06 15:51:27,027 - --- validate (epoch=60)-----------
2023-01-06 15:51:27,028 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:27,244 - Epoch: [60][    5/    5]    Loss 0.431428    Top1 80.057252    
2023-01-06 15:51:27,301 - ==> Top1: 80.057    Loss: 0.431

2023-01-06 15:51:27,301 - ==> Confusion:
[[354  75   0]
 [134 485   0]
 [  0   0   0]]

2023-01-06 15:51:27,302 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 15:51:27,302 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:27,307 - 

2023-01-06 15:51:27,307 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:27,627 - Epoch: [61][   10/   37]    Overall Loss 0.411438    Objective Loss 0.411438                                        LR 0.000036    Time 0.031918    
2023-01-06 15:51:27,741 - Epoch: [61][   20/   37]    Overall Loss 0.412678    Objective Loss 0.412678                                        LR 0.000036    Time 0.021623    
2023-01-06 15:51:27,857 - Epoch: [61][   30/   37]    Overall Loss 0.420401    Objective Loss 0.420401                                        LR 0.000036    Time 0.018287    
2023-01-06 15:51:27,927 - Epoch: [61][   37/   37]    Overall Loss 0.419113    Objective Loss 0.419113    Top1 80.543933    LR 0.000036    Time 0.016715    
2023-01-06 15:51:28,002 - --- validate (epoch=61)-----------
2023-01-06 15:51:28,002 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:28,225 - Epoch: [61][    5/    5]    Loss 0.413668    Top1 81.297710    
2023-01-06 15:51:28,293 - ==> Top1: 81.298    Loss: 0.414

2023-01-06 15:51:28,293 - ==> Confusion:
[[318 111   0]
 [ 85 534   0]
 [  0   0   0]]

2023-01-06 15:51:28,294 - ==> Best [Top1: 81.298   Sparsity:0.00   Params: 151104 on epoch: 61]
2023-01-06 15:51:28,294 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:28,301 - 

2023-01-06 15:51:28,301 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:28,637 - Epoch: [62][   10/   37]    Overall Loss 0.422080    Objective Loss 0.422080                                        LR 0.000036    Time 0.033570    
2023-01-06 15:51:28,759 - Epoch: [62][   20/   37]    Overall Loss 0.416251    Objective Loss 0.416251                                        LR 0.000036    Time 0.022883    
2023-01-06 15:51:28,898 - Epoch: [62][   30/   37]    Overall Loss 0.414404    Objective Loss 0.414404                                        LR 0.000036    Time 0.019861    
2023-01-06 15:51:28,975 - Epoch: [62][   37/   37]    Overall Loss 0.414860    Objective Loss 0.414860    Top1 80.753138    LR 0.000036    Time 0.018170    
2023-01-06 15:51:29,051 - --- validate (epoch=62)-----------
2023-01-06 15:51:29,052 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:29,270 - Epoch: [62][    5/    5]    Loss 0.413344    Top1 78.912214    
2023-01-06 15:51:29,352 - ==> Top1: 78.912    Loss: 0.413

2023-01-06 15:51:29,352 - ==> Confusion:
[[258 171   0]
 [ 50 569   0]
 [  0   0   0]]

2023-01-06 15:51:29,353 - ==> Best [Top1: 81.298   Sparsity:0.00   Params: 151104 on epoch: 61]
2023-01-06 15:51:29,353 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:29,358 - 

2023-01-06 15:51:29,358 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:29,685 - Epoch: [63][   10/   37]    Overall Loss 0.423596    Objective Loss 0.423596                                        LR 0.000036    Time 0.032602    
2023-01-06 15:51:29,816 - Epoch: [63][   20/   37]    Overall Loss 0.418524    Objective Loss 0.418524                                        LR 0.000036    Time 0.022834    
2023-01-06 15:51:29,953 - Epoch: [63][   30/   37]    Overall Loss 0.414577    Objective Loss 0.414577                                        LR 0.000036    Time 0.019772    
2023-01-06 15:51:30,031 - Epoch: [63][   37/   37]    Overall Loss 0.415308    Objective Loss 0.415308    Top1 82.217573    LR 0.000036    Time 0.018127    
2023-01-06 15:51:30,107 - --- validate (epoch=63)-----------
2023-01-06 15:51:30,108 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:30,332 - Epoch: [63][    5/    5]    Loss 0.437279    Top1 81.488550    
2023-01-06 15:51:30,402 - ==> Top1: 81.489    Loss: 0.437

2023-01-06 15:51:30,402 - ==> Confusion:
[[326 103   0]
 [ 91 528   0]
 [  0   0   0]]

2023-01-06 15:51:30,403 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:30,403 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:30,409 - 

2023-01-06 15:51:30,409 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:30,751 - Epoch: [64][   10/   37]    Overall Loss 0.413706    Objective Loss 0.413706                                        LR 0.000036    Time 0.034074    
2023-01-06 15:51:30,895 - Epoch: [64][   20/   37]    Overall Loss 0.408855    Objective Loss 0.408855                                        LR 0.000036    Time 0.024211    
2023-01-06 15:51:31,039 - Epoch: [64][   30/   37]    Overall Loss 0.407372    Objective Loss 0.407372                                        LR 0.000036    Time 0.020950    
2023-01-06 15:51:31,117 - Epoch: [64][   37/   37]    Overall Loss 0.411983    Objective Loss 0.411983    Top1 79.497908    LR 0.000036    Time 0.019064    
2023-01-06 15:51:31,203 - --- validate (epoch=64)-----------
2023-01-06 15:51:31,203 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:31,419 - Epoch: [64][    5/    5]    Loss 0.394589    Top1 79.198473    
2023-01-06 15:51:31,485 - ==> Top1: 79.198    Loss: 0.395

2023-01-06 15:51:31,485 - ==> Confusion:
[[284 145   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 15:51:31,486 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:31,486 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:31,491 - 

2023-01-06 15:51:31,491 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:31,826 - Epoch: [65][   10/   37]    Overall Loss 0.406516    Objective Loss 0.406516                                        LR 0.000036    Time 0.033437    
2023-01-06 15:51:31,960 - Epoch: [65][   20/   37]    Overall Loss 0.406037    Objective Loss 0.406037                                        LR 0.000036    Time 0.023375    
2023-01-06 15:51:32,091 - Epoch: [65][   30/   37]    Overall Loss 0.412896    Objective Loss 0.412896                                        LR 0.000036    Time 0.019961    
2023-01-06 15:51:32,166 - Epoch: [65][   37/   37]    Overall Loss 0.409672    Objective Loss 0.409672    Top1 85.774059    LR 0.000036    Time 0.018188    
2023-01-06 15:51:32,241 - --- validate (epoch=65)-----------
2023-01-06 15:51:32,241 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:32,468 - Epoch: [65][    5/    5]    Loss 0.420724    Top1 78.625954    
2023-01-06 15:51:32,537 - ==> Top1: 78.626    Loss: 0.421

2023-01-06 15:51:32,537 - ==> Confusion:
[[276 153   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:51:32,538 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:32,538 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:32,543 - 

2023-01-06 15:51:32,543 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:32,870 - Epoch: [66][   10/   37]    Overall Loss 0.408273    Objective Loss 0.408273                                        LR 0.000036    Time 0.032611    
2023-01-06 15:51:32,989 - Epoch: [66][   20/   37]    Overall Loss 0.413034    Objective Loss 0.413034                                        LR 0.000036    Time 0.022223    
2023-01-06 15:51:33,108 - Epoch: [66][   30/   37]    Overall Loss 0.412319    Objective Loss 0.412319                                        LR 0.000036    Time 0.018774    
2023-01-06 15:51:33,182 - Epoch: [66][   37/   37]    Overall Loss 0.411054    Objective Loss 0.411054    Top1 82.426778    LR 0.000036    Time 0.017224    
2023-01-06 15:51:33,252 - --- validate (epoch=66)-----------
2023-01-06 15:51:33,252 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:33,469 - Epoch: [66][    5/    5]    Loss 0.452507    Top1 78.721374    
2023-01-06 15:51:33,529 - ==> Top1: 78.721    Loss: 0.453

2023-01-06 15:51:33,529 - ==> Confusion:
[[363  66   0]
 [157 462   0]
 [  0   0   0]]

2023-01-06 15:51:33,530 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:33,530 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:33,535 - 

2023-01-06 15:51:33,535 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:33,872 - Epoch: [67][   10/   37]    Overall Loss 0.419336    Objective Loss 0.419336                                        LR 0.000036    Time 0.033596    
2023-01-06 15:51:33,992 - Epoch: [67][   20/   37]    Overall Loss 0.413324    Objective Loss 0.413324                                        LR 0.000036    Time 0.022771    
2023-01-06 15:51:34,112 - Epoch: [67][   30/   37]    Overall Loss 0.410802    Objective Loss 0.410802                                        LR 0.000036    Time 0.019158    
2023-01-06 15:51:34,180 - Epoch: [67][   37/   37]    Overall Loss 0.408395    Objective Loss 0.408395    Top1 79.288703    LR 0.000036    Time 0.017380    
2023-01-06 15:51:34,254 - --- validate (epoch=67)-----------
2023-01-06 15:51:34,254 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:34,477 - Epoch: [67][    5/    5]    Loss 0.392274    Top1 79.293893    
2023-01-06 15:51:34,551 - ==> Top1: 79.294    Loss: 0.392

2023-01-06 15:51:34,551 - ==> Confusion:
[[291 138   0]
 [ 79 540   0]
 [  0   0   0]]

2023-01-06 15:51:34,552 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:34,552 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:34,557 - 

2023-01-06 15:51:34,557 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:35,013 - Epoch: [68][   10/   37]    Overall Loss 0.407746    Objective Loss 0.407746                                        LR 0.000036    Time 0.045550    
2023-01-06 15:51:35,136 - Epoch: [68][   20/   37]    Overall Loss 0.410532    Objective Loss 0.410532                                        LR 0.000036    Time 0.028872    
2023-01-06 15:51:35,257 - Epoch: [68][   30/   37]    Overall Loss 0.405276    Objective Loss 0.405276                                        LR 0.000036    Time 0.023282    
2023-01-06 15:51:35,334 - Epoch: [68][   37/   37]    Overall Loss 0.405806    Objective Loss 0.405806    Top1 84.100418    LR 0.000036    Time 0.020955    
2023-01-06 15:51:35,417 - --- validate (epoch=68)-----------
2023-01-06 15:51:35,417 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:35,637 - Epoch: [68][    5/    5]    Loss 0.404936    Top1 79.675573    
2023-01-06 15:51:35,703 - ==> Top1: 79.676    Loss: 0.405

2023-01-06 15:51:35,703 - ==> Confusion:
[[279 150   0]
 [ 63 556   0]
 [  0   0   0]]

2023-01-06 15:51:35,704 - ==> Best [Top1: 81.489   Sparsity:0.00   Params: 151104 on epoch: 63]
2023-01-06 15:51:35,704 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:35,709 - 

2023-01-06 15:51:35,709 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:36,048 - Epoch: [69][   10/   37]    Overall Loss 0.403220    Objective Loss 0.403220                                        LR 0.000036    Time 0.033773    
2023-01-06 15:51:36,194 - Epoch: [69][   20/   37]    Overall Loss 0.404061    Objective Loss 0.404061                                        LR 0.000036    Time 0.024165    
2023-01-06 15:51:36,344 - Epoch: [69][   30/   37]    Overall Loss 0.405787    Objective Loss 0.405787                                        LR 0.000036    Time 0.021122    
2023-01-06 15:51:36,419 - Epoch: [69][   37/   37]    Overall Loss 0.403676    Objective Loss 0.403676    Top1 82.426778    LR 0.000036    Time 0.019126    
2023-01-06 15:51:36,499 - --- validate (epoch=69)-----------
2023-01-06 15:51:36,499 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:36,721 - Epoch: [69][    5/    5]    Loss 0.429676    Top1 81.679389    
2023-01-06 15:51:36,785 - ==> Top1: 81.679    Loss: 0.430

2023-01-06 15:51:36,785 - ==> Confusion:
[[336  93   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 15:51:36,786 - ==> Best [Top1: 81.679   Sparsity:0.00   Params: 151104 on epoch: 69]
2023-01-06 15:51:36,786 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:36,792 - 

2023-01-06 15:51:36,793 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:37,114 - Epoch: [70][   10/   37]    Overall Loss 0.411432    Objective Loss 0.411432                                        LR 0.000022    Time 0.032078    
2023-01-06 15:51:37,247 - Epoch: [70][   20/   37]    Overall Loss 0.407471    Objective Loss 0.407471                                        LR 0.000022    Time 0.022694    
2023-01-06 15:51:37,385 - Epoch: [70][   30/   37]    Overall Loss 0.404104    Objective Loss 0.404104                                        LR 0.000022    Time 0.019712    
2023-01-06 15:51:37,464 - Epoch: [70][   37/   37]    Overall Loss 0.401147    Objective Loss 0.401147    Top1 80.334728    LR 0.000022    Time 0.018093    
2023-01-06 15:51:37,544 - --- validate (epoch=70)-----------
2023-01-06 15:51:37,545 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:37,766 - Epoch: [70][    5/    5]    Loss 0.409489    Top1 79.675573    
2023-01-06 15:51:37,839 - ==> Top1: 79.676    Loss: 0.409

2023-01-06 15:51:37,839 - ==> Confusion:
[[287 142   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:51:37,840 - ==> Best [Top1: 81.679   Sparsity:0.00   Params: 151104 on epoch: 69]
2023-01-06 15:51:37,840 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:37,845 - 

2023-01-06 15:51:37,845 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:38,181 - Epoch: [71][   10/   37]    Overall Loss 0.402158    Objective Loss 0.402158                                        LR 0.000022    Time 0.033539    
2023-01-06 15:51:38,321 - Epoch: [71][   20/   37]    Overall Loss 0.403423    Objective Loss 0.403423                                        LR 0.000022    Time 0.023713    
2023-01-06 15:51:38,467 - Epoch: [71][   30/   37]    Overall Loss 0.399977    Objective Loss 0.399977                                        LR 0.000022    Time 0.020663    
2023-01-06 15:51:38,544 - Epoch: [71][   37/   37]    Overall Loss 0.400782    Objective Loss 0.400782    Top1 83.891213    LR 0.000022    Time 0.018843    
2023-01-06 15:51:38,634 - --- validate (epoch=71)-----------
2023-01-06 15:51:38,634 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:38,856 - Epoch: [71][    5/    5]    Loss 0.429015    Top1 81.965649    
2023-01-06 15:51:38,932 - ==> Top1: 81.966    Loss: 0.429

2023-01-06 15:51:38,932 - ==> Confusion:
[[324 105   0]
 [ 84 535   0]
 [  0   0   0]]

2023-01-06 15:51:38,933 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:38,933 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:38,940 - 

2023-01-06 15:51:38,940 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:39,269 - Epoch: [72][   10/   37]    Overall Loss 0.379702    Objective Loss 0.379702                                        LR 0.000022    Time 0.032849    
2023-01-06 15:51:39,396 - Epoch: [72][   20/   37]    Overall Loss 0.396722    Objective Loss 0.396722                                        LR 0.000022    Time 0.022777    
2023-01-06 15:51:39,532 - Epoch: [72][   30/   37]    Overall Loss 0.395345    Objective Loss 0.395345                                        LR 0.000022    Time 0.019716    
2023-01-06 15:51:39,610 - Epoch: [72][   37/   37]    Overall Loss 0.398307    Objective Loss 0.398307    Top1 81.171548    LR 0.000022    Time 0.018082    
2023-01-06 15:51:39,687 - --- validate (epoch=72)-----------
2023-01-06 15:51:39,688 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:39,909 - Epoch: [72][    5/    5]    Loss 0.392077    Top1 81.774809    
2023-01-06 15:51:39,984 - ==> Top1: 81.775    Loss: 0.392

2023-01-06 15:51:39,984 - ==> Confusion:
[[345  84   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 15:51:39,985 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:39,985 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:39,990 - 

2023-01-06 15:51:39,990 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:40,321 - Epoch: [73][   10/   37]    Overall Loss 0.394194    Objective Loss 0.394194                                        LR 0.000022    Time 0.033026    
2023-01-06 15:51:40,439 - Epoch: [73][   20/   37]    Overall Loss 0.393255    Objective Loss 0.393255                                        LR 0.000022    Time 0.022399    
2023-01-06 15:51:40,558 - Epoch: [73][   30/   37]    Overall Loss 0.393853    Objective Loss 0.393853                                        LR 0.000022    Time 0.018885    
2023-01-06 15:51:40,629 - Epoch: [73][   37/   37]    Overall Loss 0.397729    Objective Loss 0.397729    Top1 82.635983    LR 0.000022    Time 0.017206    
2023-01-06 15:51:40,699 - --- validate (epoch=73)-----------
2023-01-06 15:51:40,700 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:40,914 - Epoch: [73][    5/    5]    Loss 0.398259    Top1 81.583969    
2023-01-06 15:51:40,981 - ==> Top1: 81.584    Loss: 0.398

2023-01-06 15:51:40,981 - ==> Confusion:
[[349  80   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 15:51:40,982 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:40,982 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:40,987 - 

2023-01-06 15:51:40,987 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:41,326 - Epoch: [74][   10/   37]    Overall Loss 0.391274    Objective Loss 0.391274                                        LR 0.000022    Time 0.033802    
2023-01-06 15:51:41,438 - Epoch: [74][   20/   37]    Overall Loss 0.395190    Objective Loss 0.395190                                        LR 0.000022    Time 0.022519    
2023-01-06 15:51:41,555 - Epoch: [74][   30/   37]    Overall Loss 0.395832    Objective Loss 0.395832                                        LR 0.000022    Time 0.018870    
2023-01-06 15:51:41,620 - Epoch: [74][   37/   37]    Overall Loss 0.396781    Objective Loss 0.396781    Top1 80.125523    LR 0.000022    Time 0.017061    
2023-01-06 15:51:41,694 - --- validate (epoch=74)-----------
2023-01-06 15:51:41,695 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:41,912 - Epoch: [74][    5/    5]    Loss 0.414944    Top1 80.438931    
2023-01-06 15:51:41,997 - ==> Top1: 80.439    Loss: 0.415

2023-01-06 15:51:41,997 - ==> Confusion:
[[292 137   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 15:51:41,998 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:41,998 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:42,004 - 

2023-01-06 15:51:42,004 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:42,355 - Epoch: [75][   10/   37]    Overall Loss 0.392237    Objective Loss 0.392237                                        LR 0.000022    Time 0.035038    
2023-01-06 15:51:42,490 - Epoch: [75][   20/   37]    Overall Loss 0.395703    Objective Loss 0.395703                                        LR 0.000022    Time 0.024251    
2023-01-06 15:51:42,639 - Epoch: [75][   30/   37]    Overall Loss 0.397457    Objective Loss 0.397457                                        LR 0.000022    Time 0.021105    
2023-01-06 15:51:42,716 - Epoch: [75][   37/   37]    Overall Loss 0.395393    Objective Loss 0.395393    Top1 80.543933    LR 0.000022    Time 0.019209    
2023-01-06 15:51:42,790 - --- validate (epoch=75)-----------
2023-01-06 15:51:42,790 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:43,013 - Epoch: [75][    5/    5]    Loss 0.400387    Top1 80.057252    
2023-01-06 15:51:43,072 - ==> Top1: 80.057    Loss: 0.400

2023-01-06 15:51:43,072 - ==> Confusion:
[[285 144   0]
 [ 65 554   0]
 [  0   0   0]]

2023-01-06 15:51:43,073 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:43,073 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:43,078 - 

2023-01-06 15:51:43,078 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:43,421 - Epoch: [76][   10/   37]    Overall Loss 0.393925    Objective Loss 0.393925                                        LR 0.000022    Time 0.034223    
2023-01-06 15:51:43,555 - Epoch: [76][   20/   37]    Overall Loss 0.398268    Objective Loss 0.398268                                        LR 0.000022    Time 0.023759    
2023-01-06 15:51:43,674 - Epoch: [76][   30/   37]    Overall Loss 0.395705    Objective Loss 0.395705                                        LR 0.000022    Time 0.019810    
2023-01-06 15:51:43,752 - Epoch: [76][   37/   37]    Overall Loss 0.394664    Objective Loss 0.394664    Top1 78.661088    LR 0.000022    Time 0.018158    
2023-01-06 15:51:43,827 - --- validate (epoch=76)-----------
2023-01-06 15:51:43,827 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:44,046 - Epoch: [76][    5/    5]    Loss 0.401507    Top1 79.866412    
2023-01-06 15:51:44,121 - ==> Top1: 79.866    Loss: 0.402

2023-01-06 15:51:44,121 - ==> Confusion:
[[361  68   0]
 [143 476   0]
 [  0   0   0]]

2023-01-06 15:51:44,122 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:44,122 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:44,127 - 

2023-01-06 15:51:44,127 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:44,457 - Epoch: [77][   10/   37]    Overall Loss 0.384112    Objective Loss 0.384112                                        LR 0.000022    Time 0.032911    
2023-01-06 15:51:44,579 - Epoch: [77][   20/   37]    Overall Loss 0.392081    Objective Loss 0.392081                                        LR 0.000022    Time 0.022561    
2023-01-06 15:51:44,719 - Epoch: [77][   30/   37]    Overall Loss 0.395002    Objective Loss 0.395002                                        LR 0.000022    Time 0.019685    
2023-01-06 15:51:44,794 - Epoch: [77][   37/   37]    Overall Loss 0.394309    Objective Loss 0.394309    Top1 82.217573    LR 0.000022    Time 0.017980    
2023-01-06 15:51:44,868 - --- validate (epoch=77)-----------
2023-01-06 15:51:44,869 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:45,089 - Epoch: [77][    5/    5]    Loss 0.422890    Top1 81.393130    
2023-01-06 15:51:45,163 - ==> Top1: 81.393    Loss: 0.423

2023-01-06 15:51:45,164 - ==> Confusion:
[[339  90   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 15:51:45,165 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:45,165 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:45,170 - 

2023-01-06 15:51:45,170 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:45,493 - Epoch: [78][   10/   37]    Overall Loss 0.401723    Objective Loss 0.401723                                        LR 0.000022    Time 0.032229    
2023-01-06 15:51:45,609 - Epoch: [78][   20/   37]    Overall Loss 0.398471    Objective Loss 0.398471                                        LR 0.000022    Time 0.021886    
2023-01-06 15:51:45,723 - Epoch: [78][   30/   37]    Overall Loss 0.393618    Objective Loss 0.393618                                        LR 0.000022    Time 0.018374    
2023-01-06 15:51:45,788 - Epoch: [78][   37/   37]    Overall Loss 0.394193    Objective Loss 0.394193    Top1 85.564854    LR 0.000022    Time 0.016667    
2023-01-06 15:51:45,867 - --- validate (epoch=78)-----------
2023-01-06 15:51:45,868 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:46,092 - Epoch: [78][    5/    5]    Loss 0.397898    Top1 80.916031    
2023-01-06 15:51:46,161 - ==> Top1: 80.916    Loss: 0.398

2023-01-06 15:51:46,161 - ==> Confusion:
[[302 127   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 15:51:46,162 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:46,162 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:46,167 - 

2023-01-06 15:51:46,167 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:46,506 - Epoch: [79][   10/   37]    Overall Loss 0.392141    Objective Loss 0.392141                                        LR 0.000022    Time 0.033746    
2023-01-06 15:51:46,630 - Epoch: [79][   20/   37]    Overall Loss 0.394641    Objective Loss 0.394641                                        LR 0.000022    Time 0.023082    
2023-01-06 15:51:46,748 - Epoch: [79][   30/   37]    Overall Loss 0.392479    Objective Loss 0.392479                                        LR 0.000022    Time 0.019309    
2023-01-06 15:51:46,819 - Epoch: [79][   37/   37]    Overall Loss 0.392809    Objective Loss 0.392809    Top1 83.054393    LR 0.000022    Time 0.017551    
2023-01-06 15:51:46,894 - --- validate (epoch=79)-----------
2023-01-06 15:51:46,894 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:47,116 - Epoch: [79][    5/    5]    Loss 0.419766    Top1 81.011450    
2023-01-06 15:51:47,189 - ==> Top1: 81.011    Loss: 0.420

2023-01-06 15:51:47,190 - ==> Confusion:
[[317 112   0]
 [ 87 532   0]
 [  0   0   0]]

2023-01-06 15:51:47,191 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:47,191 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:47,196 - 

2023-01-06 15:51:47,196 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:47,529 - Epoch: [80][   10/   37]    Overall Loss 0.399809    Objective Loss 0.399809                                        LR 0.000022    Time 0.033281    
2023-01-06 15:51:47,663 - Epoch: [80][   20/   37]    Overall Loss 0.394195    Objective Loss 0.394195                                        LR 0.000022    Time 0.023274    
2023-01-06 15:51:47,796 - Epoch: [80][   30/   37]    Overall Loss 0.390638    Objective Loss 0.390638                                        LR 0.000022    Time 0.019948    
2023-01-06 15:51:47,874 - Epoch: [80][   37/   37]    Overall Loss 0.392167    Objective Loss 0.392167    Top1 80.753138    LR 0.000022    Time 0.018275    
2023-01-06 15:51:47,954 - --- validate (epoch=80)-----------
2023-01-06 15:51:47,954 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:48,178 - Epoch: [80][    5/    5]    Loss 0.398779    Top1 79.580153    
2023-01-06 15:51:48,240 - ==> Top1: 79.580    Loss: 0.399

2023-01-06 15:51:48,240 - ==> Confusion:
[[362  67   0]
 [147 472   0]
 [  0   0   0]]

2023-01-06 15:51:48,241 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:48,241 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:48,246 - 

2023-01-06 15:51:48,246 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:48,584 - Epoch: [81][   10/   37]    Overall Loss 0.383940    Objective Loss 0.383940                                        LR 0.000022    Time 0.033700    
2023-01-06 15:51:48,713 - Epoch: [81][   20/   37]    Overall Loss 0.387920    Objective Loss 0.387920                                        LR 0.000022    Time 0.023287    
2023-01-06 15:51:48,851 - Epoch: [81][   30/   37]    Overall Loss 0.390374    Objective Loss 0.390374                                        LR 0.000022    Time 0.020115    
2023-01-06 15:51:48,925 - Epoch: [81][   37/   37]    Overall Loss 0.391902    Objective Loss 0.391902    Top1 79.079498    LR 0.000022    Time 0.018306    
2023-01-06 15:51:48,999 - --- validate (epoch=81)-----------
2023-01-06 15:51:49,000 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:49,224 - Epoch: [81][    5/    5]    Loss 0.413771    Top1 81.393130    
2023-01-06 15:51:49,307 - ==> Top1: 81.393    Loss: 0.414

2023-01-06 15:51:49,308 - ==> Confusion:
[[324 105   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-06 15:51:49,309 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:49,309 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:49,314 - 

2023-01-06 15:51:49,314 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:49,661 - Epoch: [82][   10/   37]    Overall Loss 0.392203    Objective Loss 0.392203                                        LR 0.000022    Time 0.034664    
2023-01-06 15:51:49,790 - Epoch: [82][   20/   37]    Overall Loss 0.389634    Objective Loss 0.389634                                        LR 0.000022    Time 0.023768    
2023-01-06 15:51:49,921 - Epoch: [82][   30/   37]    Overall Loss 0.392335    Objective Loss 0.392335                                        LR 0.000022    Time 0.020186    
2023-01-06 15:51:49,991 - Epoch: [82][   37/   37]    Overall Loss 0.390189    Objective Loss 0.390189    Top1 80.962343    LR 0.000022    Time 0.018261    
2023-01-06 15:51:50,070 - --- validate (epoch=82)-----------
2023-01-06 15:51:50,070 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:50,297 - Epoch: [82][    5/    5]    Loss 0.384363    Top1 79.961832    
2023-01-06 15:51:50,360 - ==> Top1: 79.962    Loss: 0.384

2023-01-06 15:51:50,360 - ==> Confusion:
[[290 139   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:51:50,361 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:50,361 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:50,366 - 

2023-01-06 15:51:50,366 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:50,700 - Epoch: [83][   10/   37]    Overall Loss 0.395667    Objective Loss 0.395667                                        LR 0.000022    Time 0.033374    
2023-01-06 15:51:50,820 - Epoch: [83][   20/   37]    Overall Loss 0.384988    Objective Loss 0.384988                                        LR 0.000022    Time 0.022622    
2023-01-06 15:51:50,943 - Epoch: [83][   30/   37]    Overall Loss 0.386558    Objective Loss 0.386558                                        LR 0.000022    Time 0.019181    
2023-01-06 15:51:51,019 - Epoch: [83][   37/   37]    Overall Loss 0.390111    Objective Loss 0.390111    Top1 84.309623    LR 0.000022    Time 0.017598    
2023-01-06 15:51:51,095 - --- validate (epoch=83)-----------
2023-01-06 15:51:51,095 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:51,316 - Epoch: [83][    5/    5]    Loss 0.395141    Top1 81.870229    
2023-01-06 15:51:51,383 - ==> Top1: 81.870    Loss: 0.395

2023-01-06 15:51:51,384 - ==> Confusion:
[[331  98   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 15:51:51,385 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:51,385 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:51,390 - 

2023-01-06 15:51:51,390 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:51,867 - Epoch: [84][   10/   37]    Overall Loss 0.387186    Objective Loss 0.387186                                        LR 0.000022    Time 0.047598    
2023-01-06 15:51:52,001 - Epoch: [84][   20/   37]    Overall Loss 0.385486    Objective Loss 0.385486                                        LR 0.000022    Time 0.030502    
2023-01-06 15:51:52,141 - Epoch: [84][   30/   37]    Overall Loss 0.388587    Objective Loss 0.388587                                        LR 0.000022    Time 0.024943    
2023-01-06 15:51:52,219 - Epoch: [84][   37/   37]    Overall Loss 0.389275    Objective Loss 0.389275    Top1 79.916318    LR 0.000022    Time 0.022325    
2023-01-06 15:51:52,294 - --- validate (epoch=84)-----------
2023-01-06 15:51:52,294 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:52,513 - Epoch: [84][    5/    5]    Loss 0.418045    Top1 81.774809    
2023-01-06 15:51:52,593 - ==> Top1: 81.775    Loss: 0.418

2023-01-06 15:51:52,593 - ==> Confusion:
[[324 105   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 15:51:52,594 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:52,594 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:52,599 - 

2023-01-06 15:51:52,599 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:52,938 - Epoch: [85][   10/   37]    Overall Loss 0.398788    Objective Loss 0.398788                                        LR 0.000022    Time 0.033858    
2023-01-06 15:51:53,081 - Epoch: [85][   20/   37]    Overall Loss 0.394096    Objective Loss 0.394096                                        LR 0.000022    Time 0.024011    
2023-01-06 15:51:53,224 - Epoch: [85][   30/   37]    Overall Loss 0.390313    Objective Loss 0.390313                                        LR 0.000022    Time 0.020766    
2023-01-06 15:51:53,297 - Epoch: [85][   37/   37]    Overall Loss 0.388100    Objective Loss 0.388100    Top1 83.054393    LR 0.000022    Time 0.018822    
2023-01-06 15:51:53,378 - --- validate (epoch=85)-----------
2023-01-06 15:51:53,378 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:53,606 - Epoch: [85][    5/    5]    Loss 0.389904    Top1 81.870229    
2023-01-06 15:51:53,672 - ==> Top1: 81.870    Loss: 0.390

2023-01-06 15:51:53,672 - ==> Confusion:
[[354  75   0]
 [115 504   0]
 [  0   0   0]]

2023-01-06 15:51:53,673 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:53,674 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:53,679 - 

2023-01-06 15:51:53,679 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:54,019 - Epoch: [86][   10/   37]    Overall Loss 0.389689    Objective Loss 0.389689                                        LR 0.000022    Time 0.033921    
2023-01-06 15:51:54,147 - Epoch: [86][   20/   37]    Overall Loss 0.388246    Objective Loss 0.388246                                        LR 0.000022    Time 0.023342    
2023-01-06 15:51:54,275 - Epoch: [86][   30/   37]    Overall Loss 0.387510    Objective Loss 0.387510                                        LR 0.000022    Time 0.019817    
2023-01-06 15:51:54,350 - Epoch: [86][   37/   37]    Overall Loss 0.388302    Objective Loss 0.388302    Top1 82.635983    LR 0.000022    Time 0.018082    
2023-01-06 15:51:54,424 - --- validate (epoch=86)-----------
2023-01-06 15:51:54,424 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:54,645 - Epoch: [86][    5/    5]    Loss 0.441037    Top1 80.152672    
2023-01-06 15:51:54,713 - ==> Top1: 80.153    Loss: 0.441

2023-01-06 15:51:54,713 - ==> Confusion:
[[362  67   0]
 [141 478   0]
 [  0   0   0]]

2023-01-06 15:51:54,714 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:54,714 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:54,719 - 

2023-01-06 15:51:54,719 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:55,062 - Epoch: [87][   10/   37]    Overall Loss 0.399162    Objective Loss 0.399162                                        LR 0.000022    Time 0.034215    
2023-01-06 15:51:55,194 - Epoch: [87][   20/   37]    Overall Loss 0.393140    Objective Loss 0.393140                                        LR 0.000022    Time 0.023701    
2023-01-06 15:51:55,327 - Epoch: [87][   30/   37]    Overall Loss 0.387478    Objective Loss 0.387478                                        LR 0.000022    Time 0.020206    
2023-01-06 15:51:55,403 - Epoch: [87][   37/   37]    Overall Loss 0.388728    Objective Loss 0.388728    Top1 83.682008    LR 0.000022    Time 0.018427    
2023-01-06 15:51:55,478 - --- validate (epoch=87)-----------
2023-01-06 15:51:55,478 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:55,700 - Epoch: [87][    5/    5]    Loss 0.413913    Top1 80.438931    
2023-01-06 15:51:55,773 - ==> Top1: 80.439    Loss: 0.414

2023-01-06 15:51:55,773 - ==> Confusion:
[[287 142   0]
 [ 63 556   0]
 [  0   0   0]]

2023-01-06 15:51:55,774 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:55,774 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:55,779 - 

2023-01-06 15:51:55,779 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:56,113 - Epoch: [88][   10/   37]    Overall Loss 0.389354    Objective Loss 0.389354                                        LR 0.000022    Time 0.033327    
2023-01-06 15:51:56,250 - Epoch: [88][   20/   37]    Overall Loss 0.382300    Objective Loss 0.382300                                        LR 0.000022    Time 0.023466    
2023-01-06 15:51:56,390 - Epoch: [88][   30/   37]    Overall Loss 0.383117    Objective Loss 0.383117                                        LR 0.000022    Time 0.020314    
2023-01-06 15:51:56,468 - Epoch: [88][   37/   37]    Overall Loss 0.385737    Objective Loss 0.385737    Top1 80.543933    LR 0.000022    Time 0.018576    
2023-01-06 15:51:56,546 - --- validate (epoch=88)-----------
2023-01-06 15:51:56,546 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:56,765 - Epoch: [88][    5/    5]    Loss 0.401592    Top1 81.106870    
2023-01-06 15:51:56,834 - ==> Top1: 81.107    Loss: 0.402

2023-01-06 15:51:56,835 - ==> Confusion:
[[312 117   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 15:51:56,836 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 71]
2023-01-06 15:51:56,836 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:56,840 - 

2023-01-06 15:51:56,841 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:57,167 - Epoch: [89][   10/   37]    Overall Loss 0.377140    Objective Loss 0.377140                                        LR 0.000022    Time 0.032570    
2023-01-06 15:51:57,289 - Epoch: [89][   20/   37]    Overall Loss 0.383876    Objective Loss 0.383876                                        LR 0.000022    Time 0.022354    
2023-01-06 15:51:57,412 - Epoch: [89][   30/   37]    Overall Loss 0.381852    Objective Loss 0.381852                                        LR 0.000022    Time 0.019016    
2023-01-06 15:51:57,482 - Epoch: [89][   37/   37]    Overall Loss 0.383912    Objective Loss 0.383912    Top1 80.753138    LR 0.000022    Time 0.017298    
2023-01-06 15:51:57,562 - --- validate (epoch=89)-----------
2023-01-06 15:51:57,562 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:57,781 - Epoch: [89][    5/    5]    Loss 0.389256    Top1 81.965649    
2023-01-06 15:51:57,851 - ==> Top1: 81.966    Loss: 0.389

2023-01-06 15:51:57,851 - ==> Confusion:
[[353  76   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 15:51:57,852 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 89]
2023-01-06 15:51:57,852 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:57,859 - 

2023-01-06 15:51:57,859 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:58,185 - Epoch: [90][   10/   37]    Overall Loss 0.385578    Objective Loss 0.385578                                        LR 0.000022    Time 0.032576    
2023-01-06 15:51:58,298 - Epoch: [90][   20/   37]    Overall Loss 0.385973    Objective Loss 0.385973                                        LR 0.000022    Time 0.021901    
2023-01-06 15:51:58,412 - Epoch: [90][   30/   37]    Overall Loss 0.386470    Objective Loss 0.386470                                        LR 0.000022    Time 0.018380    
2023-01-06 15:51:58,485 - Epoch: [90][   37/   37]    Overall Loss 0.382464    Objective Loss 0.382464    Top1 83.263598    LR 0.000022    Time 0.016872    
2023-01-06 15:51:58,563 - --- validate (epoch=90)-----------
2023-01-06 15:51:58,563 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:58,784 - Epoch: [90][    5/    5]    Loss 0.383483    Top1 81.393130    
2023-01-06 15:51:58,867 - ==> Top1: 81.393    Loss: 0.383

2023-01-06 15:51:58,867 - ==> Confusion:
[[364  65   0]
 [130 489   0]
 [  0   0   0]]

2023-01-06 15:51:58,868 - ==> Best [Top1: 81.966   Sparsity:0.00   Params: 151104 on epoch: 89]
2023-01-06 15:51:58,868 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:58,873 - 

2023-01-06 15:51:58,873 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:51:59,196 - Epoch: [91][   10/   37]    Overall Loss 0.383281    Objective Loss 0.383281                                        LR 0.000022    Time 0.032277    
2023-01-06 15:51:59,316 - Epoch: [91][   20/   37]    Overall Loss 0.383586    Objective Loss 0.383586                                        LR 0.000022    Time 0.022082    
2023-01-06 15:51:59,442 - Epoch: [91][   30/   37]    Overall Loss 0.383007    Objective Loss 0.383007                                        LR 0.000022    Time 0.018919    
2023-01-06 15:51:59,514 - Epoch: [91][   37/   37]    Overall Loss 0.383199    Objective Loss 0.383199    Top1 84.937238    LR 0.000022    Time 0.017280    
2023-01-06 15:51:59,582 - --- validate (epoch=91)-----------
2023-01-06 15:51:59,582 - 1048 samples (256 per mini-batch)
2023-01-06 15:51:59,800 - Epoch: [91][    5/    5]    Loss 0.409554    Top1 82.347328    
2023-01-06 15:51:59,874 - ==> Top1: 82.347    Loss: 0.410

2023-01-06 15:51:59,874 - ==> Confusion:
[[325 104   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 15:51:59,875 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:51:59,875 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:51:59,882 - 

2023-01-06 15:51:59,882 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:00,212 - Epoch: [92][   10/   37]    Overall Loss 0.382346    Objective Loss 0.382346                                        LR 0.000022    Time 0.032933    
2023-01-06 15:52:00,326 - Epoch: [92][   20/   37]    Overall Loss 0.386441    Objective Loss 0.386441                                        LR 0.000022    Time 0.022137    
2023-01-06 15:52:00,442 - Epoch: [92][   30/   37]    Overall Loss 0.383478    Objective Loss 0.383478                                        LR 0.000022    Time 0.018622    
2023-01-06 15:52:00,507 - Epoch: [92][   37/   37]    Overall Loss 0.381654    Objective Loss 0.381654    Top1 86.610879    LR 0.000022    Time 0.016857    
2023-01-06 15:52:00,587 - --- validate (epoch=92)-----------
2023-01-06 15:52:00,587 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:00,817 - Epoch: [92][    5/    5]    Loss 0.462315    Top1 81.583969    
2023-01-06 15:52:00,891 - ==> Top1: 81.584    Loss: 0.462

2023-01-06 15:52:00,891 - ==> Confusion:
[[354  75   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 15:52:00,892 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:52:00,893 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:00,898 - 

2023-01-06 15:52:00,898 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:01,225 - Epoch: [93][   10/   37]    Overall Loss 0.381513    Objective Loss 0.381513                                        LR 0.000022    Time 0.032686    
2023-01-06 15:52:01,338 - Epoch: [93][   20/   37]    Overall Loss 0.381296    Objective Loss 0.381296                                        LR 0.000022    Time 0.021967    
2023-01-06 15:52:01,463 - Epoch: [93][   30/   37]    Overall Loss 0.383046    Objective Loss 0.383046                                        LR 0.000022    Time 0.018793    
2023-01-06 15:52:01,538 - Epoch: [93][   37/   37]    Overall Loss 0.381010    Objective Loss 0.381010    Top1 84.309623    LR 0.000022    Time 0.017274    
2023-01-06 15:52:01,612 - --- validate (epoch=93)-----------
2023-01-06 15:52:01,612 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:01,829 - Epoch: [93][    5/    5]    Loss 0.418093    Top1 81.488550    
2023-01-06 15:52:01,893 - ==> Top1: 81.489    Loss: 0.418

2023-01-06 15:52:01,893 - ==> Confusion:
[[361  68   0]
 [126 493   0]
 [  0   0   0]]

2023-01-06 15:52:01,894 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:52:01,894 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:01,900 - 

2023-01-06 15:52:01,901 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:02,250 - Epoch: [94][   10/   37]    Overall Loss 0.383975    Objective Loss 0.383975                                        LR 0.000022    Time 0.034864    
2023-01-06 15:52:02,385 - Epoch: [94][   20/   37]    Overall Loss 0.380047    Objective Loss 0.380047                                        LR 0.000022    Time 0.024162    
2023-01-06 15:52:02,522 - Epoch: [94][   30/   37]    Overall Loss 0.380050    Objective Loss 0.380050                                        LR 0.000022    Time 0.020669    
2023-01-06 15:52:02,601 - Epoch: [94][   37/   37]    Overall Loss 0.379398    Objective Loss 0.379398    Top1 80.753138    LR 0.000022    Time 0.018884    
2023-01-06 15:52:02,679 - --- validate (epoch=94)-----------
2023-01-06 15:52:02,680 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:02,900 - Epoch: [94][    5/    5]    Loss 0.392067    Top1 81.679389    
2023-01-06 15:52:02,973 - ==> Top1: 81.679    Loss: 0.392

2023-01-06 15:52:02,973 - ==> Confusion:
[[364  65   0]
 [127 492   0]
 [  0   0   0]]

2023-01-06 15:52:02,974 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:52:02,975 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:02,980 - 

2023-01-06 15:52:02,980 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:03,316 - Epoch: [95][   10/   37]    Overall Loss 0.369883    Objective Loss 0.369883                                        LR 0.000022    Time 0.033601    
2023-01-06 15:52:03,448 - Epoch: [95][   20/   37]    Overall Loss 0.374510    Objective Loss 0.374510                                        LR 0.000022    Time 0.023365    
2023-01-06 15:52:03,575 - Epoch: [95][   30/   37]    Overall Loss 0.380326    Objective Loss 0.380326                                        LR 0.000022    Time 0.019799    
2023-01-06 15:52:03,649 - Epoch: [95][   37/   37]    Overall Loss 0.381805    Objective Loss 0.381805    Top1 81.380753    LR 0.000022    Time 0.018046    
2023-01-06 15:52:03,725 - --- validate (epoch=95)-----------
2023-01-06 15:52:03,725 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:03,946 - Epoch: [95][    5/    5]    Loss 0.417475    Top1 82.251908    
2023-01-06 15:52:04,010 - ==> Top1: 82.252    Loss: 0.417

2023-01-06 15:52:04,010 - ==> Confusion:
[[315 114   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-06 15:52:04,011 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:52:04,011 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:04,016 - 

2023-01-06 15:52:04,016 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:04,347 - Epoch: [96][   10/   37]    Overall Loss 0.377312    Objective Loss 0.377312                                        LR 0.000022    Time 0.033046    
2023-01-06 15:52:04,472 - Epoch: [96][   20/   37]    Overall Loss 0.382344    Objective Loss 0.382344                                        LR 0.000022    Time 0.022733    
2023-01-06 15:52:04,610 - Epoch: [96][   30/   37]    Overall Loss 0.379045    Objective Loss 0.379045                                        LR 0.000022    Time 0.019741    
2023-01-06 15:52:04,687 - Epoch: [96][   37/   37]    Overall Loss 0.378747    Objective Loss 0.378747    Top1 84.309623    LR 0.000022    Time 0.018103    
2023-01-06 15:52:04,763 - --- validate (epoch=96)-----------
2023-01-06 15:52:04,763 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:04,981 - Epoch: [96][    5/    5]    Loss 0.405352    Top1 81.488550    
2023-01-06 15:52:05,044 - ==> Top1: 81.489    Loss: 0.405

2023-01-06 15:52:05,045 - ==> Confusion:
[[364  65   0]
 [129 490   0]
 [  0   0   0]]

2023-01-06 15:52:05,045 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 91]
2023-01-06 15:52:05,046 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:05,050 - 

2023-01-06 15:52:05,051 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:05,400 - Epoch: [97][   10/   37]    Overall Loss 0.380368    Objective Loss 0.380368                                        LR 0.000022    Time 0.034908    
2023-01-06 15:52:05,538 - Epoch: [97][   20/   37]    Overall Loss 0.381457    Objective Loss 0.381457                                        LR 0.000022    Time 0.024301    
2023-01-06 15:52:05,691 - Epoch: [97][   30/   37]    Overall Loss 0.378041    Objective Loss 0.378041                                        LR 0.000022    Time 0.021314    
2023-01-06 15:52:05,768 - Epoch: [97][   37/   37]    Overall Loss 0.378094    Objective Loss 0.378094    Top1 83.472803    LR 0.000022    Time 0.019356    
2023-01-06 15:52:05,853 - --- validate (epoch=97)-----------
2023-01-06 15:52:05,854 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:06,072 - Epoch: [97][    5/    5]    Loss 0.364348    Top1 82.347328    
2023-01-06 15:52:06,141 - ==> Top1: 82.347    Loss: 0.364

2023-01-06 15:52:06,141 - ==> Confusion:
[[335  94   0]
 [ 91 528   0]
 [  0   0   0]]

2023-01-06 15:52:06,142 - ==> Best [Top1: 82.347   Sparsity:0.00   Params: 151104 on epoch: 97]
2023-01-06 15:52:06,143 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:06,149 - 

2023-01-06 15:52:06,149 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:06,486 - Epoch: [98][   10/   37]    Overall Loss 0.359248    Objective Loss 0.359248                                        LR 0.000022    Time 0.033568    
2023-01-06 15:52:06,633 - Epoch: [98][   20/   37]    Overall Loss 0.375512    Objective Loss 0.375512                                        LR 0.000022    Time 0.024127    
2023-01-06 15:52:06,783 - Epoch: [98][   30/   37]    Overall Loss 0.375800    Objective Loss 0.375800                                        LR 0.000022    Time 0.021066    
2023-01-06 15:52:06,862 - Epoch: [98][   37/   37]    Overall Loss 0.376862    Objective Loss 0.376862    Top1 84.937238    LR 0.000022    Time 0.019222    
2023-01-06 15:52:06,940 - --- validate (epoch=98)-----------
2023-01-06 15:52:06,941 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:07,161 - Epoch: [98][    5/    5]    Loss 0.396562    Top1 82.919847    
2023-01-06 15:52:07,247 - ==> Top1: 82.920    Loss: 0.397

2023-01-06 15:52:07,248 - ==> Confusion:
[[361  68   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 15:52:07,249 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:07,249 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:07,255 - 

2023-01-06 15:52:07,255 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:07,608 - Epoch: [99][   10/   37]    Overall Loss 0.386133    Objective Loss 0.386133                                        LR 0.000022    Time 0.035255    
2023-01-06 15:52:07,753 - Epoch: [99][   20/   37]    Overall Loss 0.378709    Objective Loss 0.378709                                        LR 0.000022    Time 0.024847    
2023-01-06 15:52:07,903 - Epoch: [99][   30/   37]    Overall Loss 0.375823    Objective Loss 0.375823                                        LR 0.000022    Time 0.021553    
2023-01-06 15:52:07,982 - Epoch: [99][   37/   37]    Overall Loss 0.376575    Objective Loss 0.376575    Top1 85.355649    LR 0.000022    Time 0.019613    
2023-01-06 15:52:08,069 - --- validate (epoch=99)-----------
2023-01-06 15:52:08,069 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:08,292 - Epoch: [99][    5/    5]    Loss 0.367068    Top1 81.202290    
2023-01-06 15:52:08,361 - ==> Top1: 81.202    Loss: 0.367

2023-01-06 15:52:08,361 - ==> Confusion:
[[299 130   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:52:08,362 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:08,362 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:08,367 - 

2023-01-06 15:52:08,367 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:08,843 - Epoch: [100][   10/   37]    Overall Loss 0.372784    Objective Loss 0.372784                                        LR 0.000013    Time 0.047510    
2023-01-06 15:52:09,008 - Epoch: [100][   20/   37]    Overall Loss 0.374283    Objective Loss 0.374283                                        LR 0.000013    Time 0.031976    
2023-01-06 15:52:09,173 - Epoch: [100][   30/   37]    Overall Loss 0.377317    Objective Loss 0.377317                                        LR 0.000013    Time 0.026787    
2023-01-06 15:52:09,263 - Epoch: [100][   37/   37]    Overall Loss 0.375816    Objective Loss 0.375816    Top1 80.543933    LR 0.000013    Time 0.024150    
2023-01-06 15:52:09,346 - --- validate (epoch=100)-----------
2023-01-06 15:52:09,346 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:09,570 - Epoch: [100][    5/    5]    Loss 0.417346    Top1 81.870229    
2023-01-06 15:52:09,635 - ==> Top1: 81.870    Loss: 0.417

2023-01-06 15:52:09,636 - ==> Confusion:
[[362  67   0]
 [123 496   0]
 [  0   0   0]]

2023-01-06 15:52:09,637 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:09,637 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:09,642 - 

2023-01-06 15:52:09,642 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:09,987 - Epoch: [101][   10/   37]    Overall Loss 0.376688    Objective Loss 0.376688                                        LR 0.000013    Time 0.034447    
2023-01-06 15:52:10,121 - Epoch: [101][   20/   37]    Overall Loss 0.377140    Objective Loss 0.377140                                        LR 0.000013    Time 0.023892    
2023-01-06 15:52:10,250 - Epoch: [101][   30/   37]    Overall Loss 0.376067    Objective Loss 0.376067                                        LR 0.000013    Time 0.020203    
2023-01-06 15:52:10,319 - Epoch: [101][   37/   37]    Overall Loss 0.375050    Objective Loss 0.375050    Top1 82.426778    LR 0.000013    Time 0.018241    
2023-01-06 15:52:10,393 - --- validate (epoch=101)-----------
2023-01-06 15:52:10,393 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:10,613 - Epoch: [101][    5/    5]    Loss 0.418304    Top1 81.870229    
2023-01-06 15:52:10,678 - ==> Top1: 81.870    Loss: 0.418

2023-01-06 15:52:10,679 - ==> Confusion:
[[306 123   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:52:10,680 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:10,680 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:10,685 - 

2023-01-06 15:52:10,685 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:11,011 - Epoch: [102][   10/   37]    Overall Loss 0.374224    Objective Loss 0.374224                                        LR 0.000013    Time 0.032592    
2023-01-06 15:52:11,128 - Epoch: [102][   20/   37]    Overall Loss 0.376827    Objective Loss 0.376827                                        LR 0.000013    Time 0.022114    
2023-01-06 15:52:11,268 - Epoch: [102][   30/   37]    Overall Loss 0.374299    Objective Loss 0.374299                                        LR 0.000013    Time 0.019410    
2023-01-06 15:52:11,345 - Epoch: [102][   37/   37]    Overall Loss 0.375033    Objective Loss 0.375033    Top1 80.962343    LR 0.000013    Time 0.017812    
2023-01-06 15:52:11,420 - --- validate (epoch=102)-----------
2023-01-06 15:52:11,420 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:11,637 - Epoch: [102][    5/    5]    Loss 0.397080    Top1 82.251908    
2023-01-06 15:52:11,706 - ==> Top1: 82.252    Loss: 0.397

2023-01-06 15:52:11,706 - ==> Confusion:
[[349  80   0]
 [106 513   0]
 [  0   0   0]]

2023-01-06 15:52:11,707 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:11,707 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:11,712 - 

2023-01-06 15:52:11,713 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:12,061 - Epoch: [103][   10/   37]    Overall Loss 0.383389    Objective Loss 0.383389                                        LR 0.000013    Time 0.034757    
2023-01-06 15:52:12,211 - Epoch: [103][   20/   37]    Overall Loss 0.377053    Objective Loss 0.377053                                        LR 0.000013    Time 0.024861    
2023-01-06 15:52:12,360 - Epoch: [103][   30/   37]    Overall Loss 0.375504    Objective Loss 0.375504                                        LR 0.000013    Time 0.021543    
2023-01-06 15:52:12,438 - Epoch: [103][   37/   37]    Overall Loss 0.374423    Objective Loss 0.374423    Top1 85.355649    LR 0.000013    Time 0.019563    
2023-01-06 15:52:12,521 - --- validate (epoch=103)-----------
2023-01-06 15:52:12,522 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:12,746 - Epoch: [103][    5/    5]    Loss 0.373288    Top1 82.061069    
2023-01-06 15:52:12,814 - ==> Top1: 82.061    Loss: 0.373

2023-01-06 15:52:12,814 - ==> Confusion:
[[313 116   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-06 15:52:12,816 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:12,816 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:12,820 - 

2023-01-06 15:52:12,821 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:13,160 - Epoch: [104][   10/   37]    Overall Loss 0.384101    Objective Loss 0.384101                                        LR 0.000013    Time 0.033879    
2023-01-06 15:52:13,301 - Epoch: [104][   20/   37]    Overall Loss 0.381923    Objective Loss 0.381923                                        LR 0.000013    Time 0.023982    
2023-01-06 15:52:13,449 - Epoch: [104][   30/   37]    Overall Loss 0.376800    Objective Loss 0.376800                                        LR 0.000013    Time 0.020897    
2023-01-06 15:52:13,526 - Epoch: [104][   37/   37]    Overall Loss 0.373222    Objective Loss 0.373222    Top1 84.728033    LR 0.000013    Time 0.019014    
2023-01-06 15:52:13,600 - --- validate (epoch=104)-----------
2023-01-06 15:52:13,601 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:13,817 - Epoch: [104][    5/    5]    Loss 0.390672    Top1 82.347328    
2023-01-06 15:52:13,889 - ==> Top1: 82.347    Loss: 0.391

2023-01-06 15:52:13,889 - ==> Confusion:
[[336  93   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 15:52:13,890 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:13,891 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:13,896 - 

2023-01-06 15:52:13,896 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:14,231 - Epoch: [105][   10/   37]    Overall Loss 0.385350    Objective Loss 0.385350                                        LR 0.000013    Time 0.033507    
2023-01-06 15:52:14,368 - Epoch: [105][   20/   37]    Overall Loss 0.380250    Objective Loss 0.380250                                        LR 0.000013    Time 0.023556    
2023-01-06 15:52:14,502 - Epoch: [105][   30/   37]    Overall Loss 0.375350    Objective Loss 0.375350                                        LR 0.000013    Time 0.020157    
2023-01-06 15:52:14,579 - Epoch: [105][   37/   37]    Overall Loss 0.373139    Objective Loss 0.373139    Top1 83.682008    LR 0.000013    Time 0.018430    
2023-01-06 15:52:14,649 - --- validate (epoch=105)-----------
2023-01-06 15:52:14,649 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:14,872 - Epoch: [105][    5/    5]    Loss 0.376940    Top1 81.774809    
2023-01-06 15:52:14,945 - ==> Top1: 81.775    Loss: 0.377

2023-01-06 15:52:14,945 - ==> Confusion:
[[355  74   0]
 [117 502   0]
 [  0   0   0]]

2023-01-06 15:52:14,946 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:14,946 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:14,951 - 

2023-01-06 15:52:14,951 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:15,292 - Epoch: [106][   10/   37]    Overall Loss 0.371125    Objective Loss 0.371125                                        LR 0.000013    Time 0.034030    
2023-01-06 15:52:15,440 - Epoch: [106][   20/   37]    Overall Loss 0.403109    Objective Loss 0.403109                                        LR 0.000013    Time 0.024381    
2023-01-06 15:52:15,578 - Epoch: [106][   30/   37]    Overall Loss 0.408504    Objective Loss 0.408504                                        LR 0.000013    Time 0.020868    
2023-01-06 15:52:15,651 - Epoch: [106][   37/   37]    Overall Loss 0.410432    Objective Loss 0.410432    Top1 80.753138    LR 0.000013    Time 0.018873    
2023-01-06 15:52:15,729 - --- validate (epoch=106)-----------
2023-01-06 15:52:15,729 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:15,945 - Epoch: [106][    5/    5]    Loss 0.423983    Top1 81.106870    
2023-01-06 15:52:16,017 - ==> Top1: 81.107    Loss: 0.424

2023-01-06 15:52:16,017 - ==> Confusion:
[[307 122   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:52:16,018 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:16,018 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:16,023 - 

2023-01-06 15:52:16,023 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:16,359 - Epoch: [107][   10/   37]    Overall Loss 0.416753    Objective Loss 0.416753                                        LR 0.000013    Time 0.033488    
2023-01-06 15:52:16,488 - Epoch: [107][   20/   37]    Overall Loss 0.419488    Objective Loss 0.419488                                        LR 0.000013    Time 0.023187    
2023-01-06 15:52:16,634 - Epoch: [107][   30/   37]    Overall Loss 0.416742    Objective Loss 0.416742                                        LR 0.000013    Time 0.020318    
2023-01-06 15:52:16,715 - Epoch: [107][   37/   37]    Overall Loss 0.415839    Objective Loss 0.415839    Top1 84.728033    LR 0.000013    Time 0.018642    
2023-01-06 15:52:16,786 - --- validate (epoch=107)-----------
2023-01-06 15:52:16,786 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:17,017 - Epoch: [107][    5/    5]    Loss 0.438713    Top1 82.824427    
2023-01-06 15:52:17,096 - ==> Top1: 82.824    Loss: 0.439

2023-01-06 15:52:17,096 - ==> Confusion:
[[336  93   0]
 [ 87 532   0]
 [  0   0   0]]

2023-01-06 15:52:17,097 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:17,097 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:17,103 - 

2023-01-06 15:52:17,103 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:17,444 - Epoch: [108][   10/   37]    Overall Loss 0.418365    Objective Loss 0.418365                                        LR 0.000013    Time 0.034107    
2023-01-06 15:52:17,568 - Epoch: [108][   20/   37]    Overall Loss 0.416431    Objective Loss 0.416431                                        LR 0.000013    Time 0.023192    
2023-01-06 15:52:17,696 - Epoch: [108][   30/   37]    Overall Loss 0.414725    Objective Loss 0.414725                                        LR 0.000013    Time 0.019736    
2023-01-06 15:52:17,762 - Epoch: [108][   37/   37]    Overall Loss 0.412986    Objective Loss 0.412986    Top1 79.497908    LR 0.000013    Time 0.017783    
2023-01-06 15:52:17,835 - --- validate (epoch=108)-----------
2023-01-06 15:52:17,835 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:18,060 - Epoch: [108][    5/    5]    Loss 0.421145    Top1 81.297710    
2023-01-06 15:52:18,121 - ==> Top1: 81.298    Loss: 0.421

2023-01-06 15:52:18,121 - ==> Confusion:
[[311 118   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-06 15:52:18,123 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:18,123 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:18,128 - 

2023-01-06 15:52:18,128 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:18,465 - Epoch: [109][   10/   37]    Overall Loss 0.420750    Objective Loss 0.420750                                        LR 0.000013    Time 0.033615    
2023-01-06 15:52:18,591 - Epoch: [109][   20/   37]    Overall Loss 0.408311    Objective Loss 0.408311                                        LR 0.000013    Time 0.023114    
2023-01-06 15:52:18,730 - Epoch: [109][   30/   37]    Overall Loss 0.409161    Objective Loss 0.409161                                        LR 0.000013    Time 0.020013    
2023-01-06 15:52:18,812 - Epoch: [109][   37/   37]    Overall Loss 0.410958    Objective Loss 0.410958    Top1 82.217573    LR 0.000013    Time 0.018434    
2023-01-06 15:52:18,888 - --- validate (epoch=109)-----------
2023-01-06 15:52:18,888 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:19,109 - Epoch: [109][    5/    5]    Loss 0.428888    Top1 80.629771    
2023-01-06 15:52:19,194 - ==> Top1: 80.630    Loss: 0.429

2023-01-06 15:52:19,195 - ==> Confusion:
[[308 121   0]
 [ 82 537   0]
 [  0   0   0]]

2023-01-06 15:52:19,196 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:19,196 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:19,200 - 

2023-01-06 15:52:19,201 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:19,519 - Epoch: [110][   10/   37]    Overall Loss 0.421729    Objective Loss 0.421729                                        LR 0.000013    Time 0.031753    
2023-01-06 15:52:19,634 - Epoch: [110][   20/   37]    Overall Loss 0.416091    Objective Loss 0.416091                                        LR 0.000013    Time 0.021636    
2023-01-06 15:52:19,754 - Epoch: [110][   30/   37]    Overall Loss 0.410228    Objective Loss 0.410228                                        LR 0.000013    Time 0.018408    
2023-01-06 15:52:19,821 - Epoch: [110][   37/   37]    Overall Loss 0.408952    Objective Loss 0.408952    Top1 82.217573    LR 0.000013    Time 0.016717    
2023-01-06 15:52:19,895 - --- validate (epoch=110)-----------
2023-01-06 15:52:19,895 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:20,119 - Epoch: [110][    5/    5]    Loss 0.459067    Top1 80.820611    
2023-01-06 15:52:20,186 - ==> Top1: 80.821    Loss: 0.459

2023-01-06 15:52:20,187 - ==> Confusion:
[[302 127   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-06 15:52:20,188 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:20,188 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:20,193 - 

2023-01-06 15:52:20,193 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:20,539 - Epoch: [111][   10/   37]    Overall Loss 0.397530    Objective Loss 0.397530                                        LR 0.000013    Time 0.034534    
2023-01-06 15:52:20,682 - Epoch: [111][   20/   37]    Overall Loss 0.404125    Objective Loss 0.404125                                        LR 0.000013    Time 0.024391    
2023-01-06 15:52:20,836 - Epoch: [111][   30/   37]    Overall Loss 0.404657    Objective Loss 0.404657                                        LR 0.000013    Time 0.021367    
2023-01-06 15:52:20,927 - Epoch: [111][   37/   37]    Overall Loss 0.407102    Objective Loss 0.407102    Top1 79.707113    LR 0.000013    Time 0.019778    
2023-01-06 15:52:20,999 - --- validate (epoch=111)-----------
2023-01-06 15:52:20,999 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:21,223 - Epoch: [111][    5/    5]    Loss 0.445373    Top1 80.916031    
2023-01-06 15:52:21,297 - ==> Top1: 80.916    Loss: 0.445

2023-01-06 15:52:21,298 - ==> Confusion:
[[329 100   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 15:52:21,299 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:21,299 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:21,304 - 

2023-01-06 15:52:21,304 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:21,640 - Epoch: [112][   10/   37]    Overall Loss 0.414002    Objective Loss 0.414002                                        LR 0.000013    Time 0.033580    
2023-01-06 15:52:21,760 - Epoch: [112][   20/   37]    Overall Loss 0.408197    Objective Loss 0.408197                                        LR 0.000013    Time 0.022740    
2023-01-06 15:52:21,890 - Epoch: [112][   30/   37]    Overall Loss 0.407423    Objective Loss 0.407423                                        LR 0.000013    Time 0.019480    
2023-01-06 15:52:21,965 - Epoch: [112][   37/   37]    Overall Loss 0.406441    Objective Loss 0.406441    Top1 85.564854    LR 0.000013    Time 0.017809    
2023-01-06 15:52:22,041 - --- validate (epoch=112)-----------
2023-01-06 15:52:22,041 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:22,262 - Epoch: [112][    5/    5]    Loss 0.422276    Top1 82.442748    
2023-01-06 15:52:22,330 - ==> Top1: 82.443    Loss: 0.422

2023-01-06 15:52:22,330 - ==> Confusion:
[[327 102   0]
 [ 82 537   0]
 [  0   0   0]]

2023-01-06 15:52:22,332 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:22,332 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:22,337 - 

2023-01-06 15:52:22,337 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:22,676 - Epoch: [113][   10/   37]    Overall Loss 0.407283    Objective Loss 0.407283                                        LR 0.000013    Time 0.033784    
2023-01-06 15:52:22,797 - Epoch: [113][   20/   37]    Overall Loss 0.408876    Objective Loss 0.408876                                        LR 0.000013    Time 0.022964    
2023-01-06 15:52:22,932 - Epoch: [113][   30/   37]    Overall Loss 0.410397    Objective Loss 0.410397                                        LR 0.000013    Time 0.019775    
2023-01-06 15:52:23,011 - Epoch: [113][   37/   37]    Overall Loss 0.404789    Objective Loss 0.404789    Top1 83.263598    LR 0.000013    Time 0.018155    
2023-01-06 15:52:23,080 - --- validate (epoch=113)-----------
2023-01-06 15:52:23,081 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:23,305 - Epoch: [113][    5/    5]    Loss 0.412590    Top1 81.297710    
2023-01-06 15:52:23,386 - ==> Top1: 81.298    Loss: 0.413

2023-01-06 15:52:23,386 - ==> Confusion:
[[311 118   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-06 15:52:23,387 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:23,387 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:23,392 - 

2023-01-06 15:52:23,392 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:23,711 - Epoch: [114][   10/   37]    Overall Loss 0.410060    Objective Loss 0.410060                                        LR 0.000013    Time 0.031796    
2023-01-06 15:52:23,830 - Epoch: [114][   20/   37]    Overall Loss 0.407230    Objective Loss 0.407230                                        LR 0.000013    Time 0.021818    
2023-01-06 15:52:23,959 - Epoch: [114][   30/   37]    Overall Loss 0.405398    Objective Loss 0.405398                                        LR 0.000013    Time 0.018823    
2023-01-06 15:52:24,032 - Epoch: [114][   37/   37]    Overall Loss 0.404149    Objective Loss 0.404149    Top1 85.355649    LR 0.000013    Time 0.017237    
2023-01-06 15:52:24,095 - --- validate (epoch=114)-----------
2023-01-06 15:52:24,096 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:24,313 - Epoch: [114][    5/    5]    Loss 0.397186    Top1 82.347328    
2023-01-06 15:52:24,375 - ==> Top1: 82.347    Loss: 0.397

2023-01-06 15:52:24,375 - ==> Confusion:
[[341  88   0]
 [ 97 522   0]
 [  0   0   0]]

2023-01-06 15:52:24,376 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:24,376 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:24,381 - 

2023-01-06 15:52:24,381 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:24,844 - Epoch: [115][   10/   37]    Overall Loss 0.400671    Objective Loss 0.400671                                        LR 0.000013    Time 0.046265    
2023-01-06 15:52:24,973 - Epoch: [115][   20/   37]    Overall Loss 0.397215    Objective Loss 0.397215                                        LR 0.000013    Time 0.029562    
2023-01-06 15:52:25,114 - Epoch: [115][   30/   37]    Overall Loss 0.401250    Objective Loss 0.401250                                        LR 0.000013    Time 0.024378    
2023-01-06 15:52:25,189 - Epoch: [115][   37/   37]    Overall Loss 0.402513    Objective Loss 0.402513    Top1 84.728033    LR 0.000013    Time 0.021783    
2023-01-06 15:52:25,264 - --- validate (epoch=115)-----------
2023-01-06 15:52:25,264 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:25,483 - Epoch: [115][    5/    5]    Loss 0.443940    Top1 81.583969    
2023-01-06 15:52:25,551 - ==> Top1: 81.584    Loss: 0.444

2023-01-06 15:52:25,551 - ==> Confusion:
[[336  93   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 15:52:25,552 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:25,553 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:25,557 - 

2023-01-06 15:52:25,558 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:25,902 - Epoch: [116][   10/   37]    Overall Loss 0.395681    Objective Loss 0.395681                                        LR 0.000013    Time 0.034425    
2023-01-06 15:52:26,030 - Epoch: [116][   20/   37]    Overall Loss 0.397543    Objective Loss 0.397543                                        LR 0.000013    Time 0.023548    
2023-01-06 15:52:26,158 - Epoch: [116][   30/   37]    Overall Loss 0.398543    Objective Loss 0.398543                                        LR 0.000013    Time 0.019973    
2023-01-06 15:52:26,229 - Epoch: [116][   37/   37]    Overall Loss 0.401729    Objective Loss 0.401729    Top1 84.937238    LR 0.000013    Time 0.018106    
2023-01-06 15:52:26,305 - --- validate (epoch=116)-----------
2023-01-06 15:52:26,305 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:26,522 - Epoch: [116][    5/    5]    Loss 0.434712    Top1 82.061069    
2023-01-06 15:52:26,597 - ==> Top1: 82.061    Loss: 0.435

2023-01-06 15:52:26,598 - ==> Confusion:
[[346  83   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 15:52:26,599 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:26,599 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:26,604 - 

2023-01-06 15:52:26,604 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:26,938 - Epoch: [117][   10/   37]    Overall Loss 0.406510    Objective Loss 0.406510                                        LR 0.000013    Time 0.033414    
2023-01-06 15:52:27,068 - Epoch: [117][   20/   37]    Overall Loss 0.407904    Objective Loss 0.407904                                        LR 0.000013    Time 0.023185    
2023-01-06 15:52:27,200 - Epoch: [117][   30/   37]    Overall Loss 0.402829    Objective Loss 0.402829                                        LR 0.000013    Time 0.019836    
2023-01-06 15:52:27,277 - Epoch: [117][   37/   37]    Overall Loss 0.400875    Objective Loss 0.400875    Top1 81.589958    LR 0.000013    Time 0.018164    
2023-01-06 15:52:27,355 - --- validate (epoch=117)-----------
2023-01-06 15:52:27,355 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:27,576 - Epoch: [117][    5/    5]    Loss 0.401088    Top1 81.393130    
2023-01-06 15:52:27,651 - ==> Top1: 81.393    Loss: 0.401

2023-01-06 15:52:27,651 - ==> Confusion:
[[314 115   0]
 [ 80 539   0]
 [  0   0   0]]

2023-01-06 15:52:27,652 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:27,652 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:27,657 - 

2023-01-06 15:52:27,657 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:27,984 - Epoch: [118][   10/   37]    Overall Loss 0.406660    Objective Loss 0.406660                                        LR 0.000013    Time 0.032654    
2023-01-06 15:52:28,116 - Epoch: [118][   20/   37]    Overall Loss 0.404148    Objective Loss 0.404148                                        LR 0.000013    Time 0.022919    
2023-01-06 15:52:28,252 - Epoch: [118][   30/   37]    Overall Loss 0.398569    Objective Loss 0.398569                                        LR 0.000013    Time 0.019776    
2023-01-06 15:52:28,334 - Epoch: [118][   37/   37]    Overall Loss 0.399879    Objective Loss 0.399879    Top1 81.799163    LR 0.000013    Time 0.018263    
2023-01-06 15:52:28,404 - --- validate (epoch=118)-----------
2023-01-06 15:52:28,404 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:28,621 - Epoch: [118][    5/    5]    Loss 0.415730    Top1 81.202290    
2023-01-06 15:52:28,695 - ==> Top1: 81.202    Loss: 0.416

2023-01-06 15:52:28,695 - ==> Confusion:
[[309 120   0]
 [ 77 542   0]
 [  0   0   0]]

2023-01-06 15:52:28,696 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:28,696 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:28,701 - 

2023-01-06 15:52:28,701 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:29,027 - Epoch: [119][   10/   37]    Overall Loss 0.394419    Objective Loss 0.394419                                        LR 0.000013    Time 0.032552    
2023-01-06 15:52:29,145 - Epoch: [119][   20/   37]    Overall Loss 0.399855    Objective Loss 0.399855                                        LR 0.000013    Time 0.022116    
2023-01-06 15:52:29,265 - Epoch: [119][   30/   37]    Overall Loss 0.401442    Objective Loss 0.401442                                        LR 0.000013    Time 0.018743    
2023-01-06 15:52:29,337 - Epoch: [119][   37/   37]    Overall Loss 0.398261    Objective Loss 0.398261    Top1 84.518828    LR 0.000013    Time 0.017125    
2023-01-06 15:52:29,418 - --- validate (epoch=119)-----------
2023-01-06 15:52:29,419 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:29,638 - Epoch: [119][    5/    5]    Loss 0.421076    Top1 80.248092    
2023-01-06 15:52:29,704 - ==> Top1: 80.248    Loss: 0.421

2023-01-06 15:52:29,704 - ==> Confusion:
[[290 139   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 15:52:29,705 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:29,705 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:29,710 - 

2023-01-06 15:52:29,710 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:30,036 - Epoch: [120][   10/   37]    Overall Loss 0.402085    Objective Loss 0.402085                                        LR 0.000013    Time 0.032558    
2023-01-06 15:52:30,166 - Epoch: [120][   20/   37]    Overall Loss 0.397170    Objective Loss 0.397170                                        LR 0.000013    Time 0.022767    
2023-01-06 15:52:30,309 - Epoch: [120][   30/   37]    Overall Loss 0.392737    Objective Loss 0.392737                                        LR 0.000013    Time 0.019930    
2023-01-06 15:52:30,390 - Epoch: [120][   37/   37]    Overall Loss 0.397258    Objective Loss 0.397258    Top1 81.799163    LR 0.000013    Time 0.018319    
2023-01-06 15:52:30,455 - --- validate (epoch=120)-----------
2023-01-06 15:52:30,455 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:30,671 - Epoch: [120][    5/    5]    Loss 0.411773    Top1 81.583969    
2023-01-06 15:52:30,729 - ==> Top1: 81.584    Loss: 0.412

2023-01-06 15:52:30,730 - ==> Confusion:
[[312 117   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:52:30,731 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:30,731 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:30,736 - 

2023-01-06 15:52:30,736 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:31,083 - Epoch: [121][   10/   37]    Overall Loss 0.392271    Objective Loss 0.392271                                        LR 0.000013    Time 0.034671    
2023-01-06 15:52:31,232 - Epoch: [121][   20/   37]    Overall Loss 0.394430    Objective Loss 0.394430                                        LR 0.000013    Time 0.024729    
2023-01-06 15:52:31,377 - Epoch: [121][   30/   37]    Overall Loss 0.397400    Objective Loss 0.397400                                        LR 0.000013    Time 0.021320    
2023-01-06 15:52:31,459 - Epoch: [121][   37/   37]    Overall Loss 0.395962    Objective Loss 0.395962    Top1 85.774059    LR 0.000013    Time 0.019484    
2023-01-06 15:52:31,525 - --- validate (epoch=121)-----------
2023-01-06 15:52:31,525 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:31,745 - Epoch: [121][    5/    5]    Loss 0.386904    Top1 81.583969    
2023-01-06 15:52:31,821 - ==> Top1: 81.584    Loss: 0.387

2023-01-06 15:52:31,821 - ==> Confusion:
[[346  83   0]
 [110 509   0]
 [  0   0   0]]

2023-01-06 15:52:31,822 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:31,822 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:31,827 - 

2023-01-06 15:52:31,827 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:32,160 - Epoch: [122][   10/   37]    Overall Loss 0.399332    Objective Loss 0.399332                                        LR 0.000013    Time 0.033191    
2023-01-06 15:52:32,284 - Epoch: [122][   20/   37]    Overall Loss 0.392487    Objective Loss 0.392487                                        LR 0.000013    Time 0.022800    
2023-01-06 15:52:32,420 - Epoch: [122][   30/   37]    Overall Loss 0.394901    Objective Loss 0.394901                                        LR 0.000013    Time 0.019702    
2023-01-06 15:52:32,499 - Epoch: [122][   37/   37]    Overall Loss 0.395751    Objective Loss 0.395751    Top1 80.962343    LR 0.000013    Time 0.018116    
2023-01-06 15:52:32,577 - --- validate (epoch=122)-----------
2023-01-06 15:52:32,577 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:32,796 - Epoch: [122][    5/    5]    Loss 0.399204    Top1 80.820611    
2023-01-06 15:52:32,863 - ==> Top1: 80.821    Loss: 0.399

2023-01-06 15:52:32,863 - ==> Confusion:
[[302 127   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-06 15:52:32,865 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:32,865 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:32,870 - 

2023-01-06 15:52:32,870 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:33,203 - Epoch: [123][   10/   37]    Overall Loss 0.391832    Objective Loss 0.391832                                        LR 0.000013    Time 0.033254    
2023-01-06 15:52:33,347 - Epoch: [123][   20/   37]    Overall Loss 0.395775    Objective Loss 0.395775                                        LR 0.000013    Time 0.023794    
2023-01-06 15:52:33,492 - Epoch: [123][   30/   37]    Overall Loss 0.393372    Objective Loss 0.393372                                        LR 0.000013    Time 0.020699    
2023-01-06 15:52:33,572 - Epoch: [123][   37/   37]    Overall Loss 0.394845    Objective Loss 0.394845    Top1 83.263598    LR 0.000013    Time 0.018930    
2023-01-06 15:52:33,649 - --- validate (epoch=123)-----------
2023-01-06 15:52:33,649 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:33,872 - Epoch: [123][    5/    5]    Loss 0.414373    Top1 81.870229    
2023-01-06 15:52:33,943 - ==> Top1: 81.870    Loss: 0.414

2023-01-06 15:52:33,943 - ==> Confusion:
[[325 104   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 15:52:33,944 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:33,944 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:33,949 - 

2023-01-06 15:52:33,949 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:34,289 - Epoch: [124][   10/   37]    Overall Loss 0.397377    Objective Loss 0.397377                                        LR 0.000013    Time 0.033857    
2023-01-06 15:52:34,424 - Epoch: [124][   20/   37]    Overall Loss 0.389043    Objective Loss 0.389043                                        LR 0.000013    Time 0.023666    
2023-01-06 15:52:34,563 - Epoch: [124][   30/   37]    Overall Loss 0.396887    Objective Loss 0.396887                                        LR 0.000013    Time 0.020415    
2023-01-06 15:52:34,642 - Epoch: [124][   37/   37]    Overall Loss 0.394218    Objective Loss 0.394218    Top1 82.426778    LR 0.000013    Time 0.018668    
2023-01-06 15:52:34,721 - --- validate (epoch=124)-----------
2023-01-06 15:52:34,721 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:34,945 - Epoch: [124][    5/    5]    Loss 0.382807    Top1 80.534351    
2023-01-06 15:52:35,019 - ==> Top1: 80.534    Loss: 0.383

2023-01-06 15:52:35,019 - ==> Confusion:
[[305 124   0]
 [ 80 539   0]
 [  0   0   0]]

2023-01-06 15:52:35,020 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:35,020 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:35,025 - 

2023-01-06 15:52:35,025 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:35,354 - Epoch: [125][   10/   37]    Overall Loss 0.406282    Objective Loss 0.406282                                        LR 0.000013    Time 0.032812    
2023-01-06 15:52:35,477 - Epoch: [125][   20/   37]    Overall Loss 0.403319    Objective Loss 0.403319                                        LR 0.000013    Time 0.022538    
2023-01-06 15:52:35,611 - Epoch: [125][   30/   37]    Overall Loss 0.396405    Objective Loss 0.396405                                        LR 0.000013    Time 0.019463    
2023-01-06 15:52:35,687 - Epoch: [125][   37/   37]    Overall Loss 0.393810    Objective Loss 0.393810    Top1 82.635983    LR 0.000013    Time 0.017847    
2023-01-06 15:52:35,766 - --- validate (epoch=125)-----------
2023-01-06 15:52:35,766 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:35,985 - Epoch: [125][    5/    5]    Loss 0.419158    Top1 81.011450    
2023-01-06 15:52:36,061 - ==> Top1: 81.011    Loss: 0.419

2023-01-06 15:52:36,061 - ==> Confusion:
[[305 124   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:52:36,062 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:36,062 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:36,067 - 

2023-01-06 15:52:36,067 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:36,400 - Epoch: [126][   10/   37]    Overall Loss 0.380859    Objective Loss 0.380859                                        LR 0.000013    Time 0.033227    
2023-01-06 15:52:36,530 - Epoch: [126][   20/   37]    Overall Loss 0.388060    Objective Loss 0.388060                                        LR 0.000013    Time 0.023081    
2023-01-06 15:52:36,655 - Epoch: [126][   30/   37]    Overall Loss 0.392222    Objective Loss 0.392222                                        LR 0.000013    Time 0.019536    
2023-01-06 15:52:36,732 - Epoch: [126][   37/   37]    Overall Loss 0.393363    Objective Loss 0.393363    Top1 83.263598    LR 0.000013    Time 0.017906    
2023-01-06 15:52:36,804 - --- validate (epoch=126)-----------
2023-01-06 15:52:36,805 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:37,028 - Epoch: [126][    5/    5]    Loss 0.414084    Top1 80.725191    
2023-01-06 15:52:37,104 - ==> Top1: 80.725    Loss: 0.414

2023-01-06 15:52:37,104 - ==> Confusion:
[[302 127   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:52:37,105 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:37,105 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:37,110 - 

2023-01-06 15:52:37,110 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:37,465 - Epoch: [127][   10/   37]    Overall Loss 0.396144    Objective Loss 0.396144                                        LR 0.000013    Time 0.035387    
2023-01-06 15:52:37,594 - Epoch: [127][   20/   37]    Overall Loss 0.394918    Objective Loss 0.394918                                        LR 0.000013    Time 0.024122    
2023-01-06 15:52:37,720 - Epoch: [127][   30/   37]    Overall Loss 0.391746    Objective Loss 0.391746                                        LR 0.000013    Time 0.020282    
2023-01-06 15:52:37,792 - Epoch: [127][   37/   37]    Overall Loss 0.392545    Objective Loss 0.392545    Top1 82.845188    LR 0.000013    Time 0.018368    
2023-01-06 15:52:37,873 - --- validate (epoch=127)-----------
2023-01-06 15:52:37,873 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:38,093 - Epoch: [127][    5/    5]    Loss 0.408745    Top1 81.297710    
2023-01-06 15:52:38,161 - ==> Top1: 81.298    Loss: 0.409

2023-01-06 15:52:38,161 - ==> Confusion:
[[347  82   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 15:52:38,162 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:38,163 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:38,167 - 

2023-01-06 15:52:38,168 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:38,503 - Epoch: [128][   10/   37]    Overall Loss 0.384446    Objective Loss 0.384446                                        LR 0.000013    Time 0.033485    
2023-01-06 15:52:38,640 - Epoch: [128][   20/   37]    Overall Loss 0.387501    Objective Loss 0.387501                                        LR 0.000013    Time 0.023553    
2023-01-06 15:52:38,785 - Epoch: [128][   30/   37]    Overall Loss 0.392902    Objective Loss 0.392902                                        LR 0.000013    Time 0.020520    
2023-01-06 15:52:38,863 - Epoch: [128][   37/   37]    Overall Loss 0.391416    Objective Loss 0.391416    Top1 86.192469    LR 0.000013    Time 0.018755    
2023-01-06 15:52:38,937 - --- validate (epoch=128)-----------
2023-01-06 15:52:38,938 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:39,162 - Epoch: [128][    5/    5]    Loss 0.405090    Top1 81.297710    
2023-01-06 15:52:39,229 - ==> Top1: 81.298    Loss: 0.405

2023-01-06 15:52:39,229 - ==> Confusion:
[[353  76   0]
 [120 499   0]
 [  0   0   0]]

2023-01-06 15:52:39,230 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:39,230 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:39,235 - 

2023-01-06 15:52:39,235 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:39,700 - Epoch: [129][   10/   37]    Overall Loss 0.385246    Objective Loss 0.385246                                        LR 0.000013    Time 0.046412    
2023-01-06 15:52:39,849 - Epoch: [129][   20/   37]    Overall Loss 0.386709    Objective Loss 0.386709                                        LR 0.000013    Time 0.030621    
2023-01-06 15:52:39,994 - Epoch: [129][   30/   37]    Overall Loss 0.387560    Objective Loss 0.387560                                        LR 0.000013    Time 0.025233    
2023-01-06 15:52:40,072 - Epoch: [129][   37/   37]    Overall Loss 0.391142    Objective Loss 0.391142    Top1 80.753138    LR 0.000013    Time 0.022565    
2023-01-06 15:52:40,149 - --- validate (epoch=129)-----------
2023-01-06 15:52:40,149 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:40,373 - Epoch: [129][    5/    5]    Loss 0.419111    Top1 81.583969    
2023-01-06 15:52:40,432 - ==> Top1: 81.584    Loss: 0.419

2023-01-06 15:52:40,432 - ==> Confusion:
[[345  84   0]
 [109 510   0]
 [  0   0   0]]

2023-01-06 15:52:40,433 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:40,433 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:40,438 - 

2023-01-06 15:52:40,438 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:40,774 - Epoch: [130][   10/   37]    Overall Loss 0.386889    Objective Loss 0.386889                                        LR 0.000013    Time 0.033505    
2023-01-06 15:52:40,908 - Epoch: [130][   20/   37]    Overall Loss 0.383841    Objective Loss 0.383841                                        LR 0.000013    Time 0.023437    
2023-01-06 15:52:41,031 - Epoch: [130][   30/   37]    Overall Loss 0.388456    Objective Loss 0.388456                                        LR 0.000013    Time 0.019714    
2023-01-06 15:52:41,103 - Epoch: [130][   37/   37]    Overall Loss 0.390054    Objective Loss 0.390054    Top1 83.263598    LR 0.000013    Time 0.017912    
2023-01-06 15:52:41,171 - --- validate (epoch=130)-----------
2023-01-06 15:52:41,171 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:41,392 - Epoch: [130][    5/    5]    Loss 0.400907    Top1 80.820611    
2023-01-06 15:52:41,457 - ==> Top1: 80.821    Loss: 0.401

2023-01-06 15:52:41,457 - ==> Confusion:
[[290 139   0]
 [ 62 557   0]
 [  0   0   0]]

2023-01-06 15:52:41,458 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:41,458 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:41,463 - 

2023-01-06 15:52:41,463 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:41,794 - Epoch: [131][   10/   37]    Overall Loss 0.394299    Objective Loss 0.394299                                        LR 0.000013    Time 0.033013    
2023-01-06 15:52:41,923 - Epoch: [131][   20/   37]    Overall Loss 0.392468    Objective Loss 0.392468                                        LR 0.000013    Time 0.022930    
2023-01-06 15:52:42,047 - Epoch: [131][   30/   37]    Overall Loss 0.391172    Objective Loss 0.391172                                        LR 0.000013    Time 0.019387    
2023-01-06 15:52:42,114 - Epoch: [131][   37/   37]    Overall Loss 0.389316    Objective Loss 0.389316    Top1 82.635983    LR 0.000013    Time 0.017535    
2023-01-06 15:52:42,191 - --- validate (epoch=131)-----------
2023-01-06 15:52:42,191 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:42,411 - Epoch: [131][    5/    5]    Loss 0.401839    Top1 80.916031    
2023-01-06 15:52:42,471 - ==> Top1: 80.916    Loss: 0.402

2023-01-06 15:52:42,471 - ==> Confusion:
[[305 124   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:52:42,472 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:42,472 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:42,477 - 

2023-01-06 15:52:42,477 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:42,826 - Epoch: [132][   10/   37]    Overall Loss 0.390256    Objective Loss 0.390256                                        LR 0.000013    Time 0.034832    
2023-01-06 15:52:42,954 - Epoch: [132][   20/   37]    Overall Loss 0.391413    Objective Loss 0.391413                                        LR 0.000013    Time 0.023763    
2023-01-06 15:52:43,089 - Epoch: [132][   30/   37]    Overall Loss 0.390658    Objective Loss 0.390658                                        LR 0.000013    Time 0.020343    
2023-01-06 15:52:43,169 - Epoch: [132][   37/   37]    Overall Loss 0.387932    Objective Loss 0.387932    Top1 86.192469    LR 0.000013    Time 0.018650    
2023-01-06 15:52:43,245 - --- validate (epoch=132)-----------
2023-01-06 15:52:43,245 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:43,467 - Epoch: [132][    5/    5]    Loss 0.412884    Top1 81.202290    
2023-01-06 15:52:43,524 - ==> Top1: 81.202    Loss: 0.413

2023-01-06 15:52:43,525 - ==> Confusion:
[[361  68   0]
 [129 490   0]
 [  0   0   0]]

2023-01-06 15:52:43,526 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:43,526 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:43,531 - 

2023-01-06 15:52:43,531 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:43,860 - Epoch: [133][   10/   37]    Overall Loss 0.382468    Objective Loss 0.382468                                        LR 0.000013    Time 0.032860    
2023-01-06 15:52:43,980 - Epoch: [133][   20/   37]    Overall Loss 0.387273    Objective Loss 0.387273                                        LR 0.000013    Time 0.022393    
2023-01-06 15:52:44,120 - Epoch: [133][   30/   37]    Overall Loss 0.388436    Objective Loss 0.388436                                        LR 0.000013    Time 0.019589    
2023-01-06 15:52:44,196 - Epoch: [133][   37/   37]    Overall Loss 0.387240    Objective Loss 0.387240    Top1 83.472803    LR 0.000013    Time 0.017935    
2023-01-06 15:52:44,267 - --- validate (epoch=133)-----------
2023-01-06 15:52:44,267 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:44,493 - Epoch: [133][    5/    5]    Loss 0.451411    Top1 81.202290    
2023-01-06 15:52:44,561 - ==> Top1: 81.202    Loss: 0.451

2023-01-06 15:52:44,561 - ==> Confusion:
[[351  78   0]
 [119 500   0]
 [  0   0   0]]

2023-01-06 15:52:44,562 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:44,562 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:44,567 - 

2023-01-06 15:52:44,567 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:44,911 - Epoch: [134][   10/   37]    Overall Loss 0.395227    Objective Loss 0.395227                                        LR 0.000013    Time 0.034364    
2023-01-06 15:52:45,046 - Epoch: [134][   20/   37]    Overall Loss 0.386434    Objective Loss 0.386434                                        LR 0.000013    Time 0.023889    
2023-01-06 15:52:45,185 - Epoch: [134][   30/   37]    Overall Loss 0.383286    Objective Loss 0.383286                                        LR 0.000013    Time 0.020527    
2023-01-06 15:52:45,261 - Epoch: [134][   37/   37]    Overall Loss 0.386493    Objective Loss 0.386493    Top1 81.589958    LR 0.000013    Time 0.018702    
2023-01-06 15:52:45,342 - --- validate (epoch=134)-----------
2023-01-06 15:52:45,342 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:45,557 - Epoch: [134][    5/    5]    Loss 0.380328    Top1 82.061069    
2023-01-06 15:52:45,616 - ==> Top1: 82.061    Loss: 0.380

2023-01-06 15:52:45,617 - ==> Confusion:
[[336  93   0]
 [ 95 524   0]
 [  0   0   0]]

2023-01-06 15:52:45,618 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:45,618 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:45,623 - 

2023-01-06 15:52:45,623 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:45,970 - Epoch: [135][   10/   37]    Overall Loss 0.383522    Objective Loss 0.383522                                        LR 0.000013    Time 0.034614    
2023-01-06 15:52:46,118 - Epoch: [135][   20/   37]    Overall Loss 0.385922    Objective Loss 0.385922                                        LR 0.000013    Time 0.024700    
2023-01-06 15:52:46,261 - Epoch: [135][   30/   37]    Overall Loss 0.384303    Objective Loss 0.384303                                        LR 0.000013    Time 0.021230    
2023-01-06 15:52:46,340 - Epoch: [135][   37/   37]    Overall Loss 0.385688    Objective Loss 0.385688    Top1 81.589958    LR 0.000013    Time 0.019343    
2023-01-06 15:52:46,414 - --- validate (epoch=135)-----------
2023-01-06 15:52:46,415 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:46,630 - Epoch: [135][    5/    5]    Loss 0.403245    Top1 81.106870    
2023-01-06 15:52:46,701 - ==> Top1: 81.107    Loss: 0.403

2023-01-06 15:52:46,702 - ==> Confusion:
[[293 136   0]
 [ 62 557   0]
 [  0   0   0]]

2023-01-06 15:52:46,703 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:46,703 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:46,708 - 

2023-01-06 15:52:46,708 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:47,030 - Epoch: [136][   10/   37]    Overall Loss 0.389379    Objective Loss 0.389379                                        LR 0.000013    Time 0.032178    
2023-01-06 15:52:47,155 - Epoch: [136][   20/   37]    Overall Loss 0.383266    Objective Loss 0.383266                                        LR 0.000013    Time 0.022289    
2023-01-06 15:52:47,293 - Epoch: [136][   30/   37]    Overall Loss 0.385304    Objective Loss 0.385304                                        LR 0.000013    Time 0.019468    
2023-01-06 15:52:47,368 - Epoch: [136][   37/   37]    Overall Loss 0.386004    Objective Loss 0.386004    Top1 81.589958    LR 0.000013    Time 0.017793    
2023-01-06 15:52:47,433 - --- validate (epoch=136)-----------
2023-01-06 15:52:47,433 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:47,651 - Epoch: [136][    5/    5]    Loss 0.396051    Top1 81.679389    
2023-01-06 15:52:47,735 - ==> Top1: 81.679    Loss: 0.396

2023-01-06 15:52:47,735 - ==> Confusion:
[[358  71   0]
 [121 498   0]
 [  0   0   0]]

2023-01-06 15:52:47,736 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:47,736 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:47,741 - 

2023-01-06 15:52:47,741 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:48,072 - Epoch: [137][   10/   37]    Overall Loss 0.393046    Objective Loss 0.393046                                        LR 0.000013    Time 0.032962    
2023-01-06 15:52:48,204 - Epoch: [137][   20/   37]    Overall Loss 0.388384    Objective Loss 0.388384                                        LR 0.000013    Time 0.023094    
2023-01-06 15:52:48,343 - Epoch: [137][   30/   37]    Overall Loss 0.387076    Objective Loss 0.387076                                        LR 0.000013    Time 0.020014    
2023-01-06 15:52:48,426 - Epoch: [137][   37/   37]    Overall Loss 0.384049    Objective Loss 0.384049    Top1 84.518828    LR 0.000013    Time 0.018464    
2023-01-06 15:52:48,500 - --- validate (epoch=137)-----------
2023-01-06 15:52:48,500 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:48,725 - Epoch: [137][    5/    5]    Loss 0.395210    Top1 80.629771    
2023-01-06 15:52:48,789 - ==> Top1: 80.630    Loss: 0.395

2023-01-06 15:52:48,789 - ==> Confusion:
[[293 136   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:52:48,790 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:48,790 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:48,795 - 

2023-01-06 15:52:48,795 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:49,123 - Epoch: [138][   10/   37]    Overall Loss 0.387691    Objective Loss 0.387691                                        LR 0.000013    Time 0.032767    
2023-01-06 15:52:49,253 - Epoch: [138][   20/   37]    Overall Loss 0.382861    Objective Loss 0.382861                                        LR 0.000013    Time 0.022832    
2023-01-06 15:52:49,396 - Epoch: [138][   30/   37]    Overall Loss 0.382642    Objective Loss 0.382642                                        LR 0.000013    Time 0.019997    
2023-01-06 15:52:49,476 - Epoch: [138][   37/   37]    Overall Loss 0.383906    Objective Loss 0.383906    Top1 79.497908    LR 0.000013    Time 0.018370    
2023-01-06 15:52:49,556 - --- validate (epoch=138)-----------
2023-01-06 15:52:49,556 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:49,772 - Epoch: [138][    5/    5]    Loss 0.394062    Top1 81.679389    
2023-01-06 15:52:49,836 - ==> Top1: 81.679    Loss: 0.394

2023-01-06 15:52:49,836 - ==> Confusion:
[[310 119   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 15:52:49,837 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:49,837 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:49,842 - 

2023-01-06 15:52:49,842 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:50,169 - Epoch: [139][   10/   37]    Overall Loss 0.391693    Objective Loss 0.391693                                        LR 0.000013    Time 0.032649    
2023-01-06 15:52:50,288 - Epoch: [139][   20/   37]    Overall Loss 0.387141    Objective Loss 0.387141                                        LR 0.000013    Time 0.022217    
2023-01-06 15:52:50,410 - Epoch: [139][   30/   37]    Overall Loss 0.384005    Objective Loss 0.384005                                        LR 0.000013    Time 0.018884    
2023-01-06 15:52:50,481 - Epoch: [139][   37/   37]    Overall Loss 0.382596    Objective Loss 0.382596    Top1 85.146444    LR 0.000013    Time 0.017213    
2023-01-06 15:52:50,549 - --- validate (epoch=139)-----------
2023-01-06 15:52:50,550 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:50,776 - Epoch: [139][    5/    5]    Loss 0.383916    Top1 81.488550    
2023-01-06 15:52:50,842 - ==> Top1: 81.489    Loss: 0.384

2023-01-06 15:52:50,843 - ==> Confusion:
[[353  76   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 15:52:50,844 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:50,844 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:50,851 - 

2023-01-06 15:52:50,851 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:51,175 - Epoch: [140][   10/   37]    Overall Loss 0.397866    Objective Loss 0.397866                                        LR 0.000008    Time 0.032324    
2023-01-06 15:52:51,290 - Epoch: [140][   20/   37]    Overall Loss 0.383675    Objective Loss 0.383675                                        LR 0.000008    Time 0.021862    
2023-01-06 15:52:51,406 - Epoch: [140][   30/   37]    Overall Loss 0.383447    Objective Loss 0.383447                                        LR 0.000008    Time 0.018446    
2023-01-06 15:52:51,476 - Epoch: [140][   37/   37]    Overall Loss 0.382064    Objective Loss 0.382064    Top1 84.100418    LR 0.000008    Time 0.016842    
2023-01-06 15:52:51,544 - --- validate (epoch=140)-----------
2023-01-06 15:52:51,544 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:51,763 - Epoch: [140][    5/    5]    Loss 0.409450    Top1 81.297710    
2023-01-06 15:52:51,831 - ==> Top1: 81.298    Loss: 0.409

2023-01-06 15:52:51,832 - ==> Confusion:
[[360  69   0]
 [127 492   0]
 [  0   0   0]]

2023-01-06 15:52:51,833 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:51,833 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:51,838 - 

2023-01-06 15:52:51,838 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:52,172 - Epoch: [141][   10/   37]    Overall Loss 0.386655    Objective Loss 0.386655                                        LR 0.000008    Time 0.033376    
2023-01-06 15:52:52,305 - Epoch: [141][   20/   37]    Overall Loss 0.388816    Objective Loss 0.388816                                        LR 0.000008    Time 0.023280    
2023-01-06 15:52:52,437 - Epoch: [141][   30/   37]    Overall Loss 0.384266    Objective Loss 0.384266                                        LR 0.000008    Time 0.019901    
2023-01-06 15:52:52,510 - Epoch: [141][   37/   37]    Overall Loss 0.382266    Objective Loss 0.382266    Top1 85.564854    LR 0.000008    Time 0.018098    
2023-01-06 15:52:52,581 - --- validate (epoch=141)-----------
2023-01-06 15:52:52,581 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:52,801 - Epoch: [141][    5/    5]    Loss 0.401049    Top1 81.488550    
2023-01-06 15:52:52,870 - ==> Top1: 81.489    Loss: 0.401

2023-01-06 15:52:52,870 - ==> Confusion:
[[359  70   0]
 [124 495   0]
 [  0   0   0]]

2023-01-06 15:52:52,871 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:52,871 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:52,876 - 

2023-01-06 15:52:52,876 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:53,222 - Epoch: [142][   10/   37]    Overall Loss 0.378995    Objective Loss 0.378995                                        LR 0.000008    Time 0.034562    
2023-01-06 15:52:53,358 - Epoch: [142][   20/   37]    Overall Loss 0.386401    Objective Loss 0.386401                                        LR 0.000008    Time 0.024024    
2023-01-06 15:52:53,483 - Epoch: [142][   30/   37]    Overall Loss 0.380396    Objective Loss 0.380396                                        LR 0.000008    Time 0.020190    
2023-01-06 15:52:53,563 - Epoch: [142][   37/   37]    Overall Loss 0.381014    Objective Loss 0.381014    Top1 80.962343    LR 0.000008    Time 0.018525    
2023-01-06 15:52:53,632 - --- validate (epoch=142)-----------
2023-01-06 15:52:53,633 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:53,856 - Epoch: [142][    5/    5]    Loss 0.386515    Top1 82.061069    
2023-01-06 15:52:53,915 - ==> Top1: 82.061    Loss: 0.387

2023-01-06 15:52:53,915 - ==> Confusion:
[[325 104   0]
 [ 84 535   0]
 [  0   0   0]]

2023-01-06 15:52:53,916 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:53,916 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:53,921 - 

2023-01-06 15:52:53,922 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:54,260 - Epoch: [143][   10/   37]    Overall Loss 0.374285    Objective Loss 0.374285                                        LR 0.000008    Time 0.033767    
2023-01-06 15:52:54,389 - Epoch: [143][   20/   37]    Overall Loss 0.371544    Objective Loss 0.371544                                        LR 0.000008    Time 0.023339    
2023-01-06 15:52:54,525 - Epoch: [143][   30/   37]    Overall Loss 0.375990    Objective Loss 0.375990                                        LR 0.000008    Time 0.020077    
2023-01-06 15:52:54,604 - Epoch: [143][   37/   37]    Overall Loss 0.380792    Objective Loss 0.380792    Top1 82.426778    LR 0.000008    Time 0.018410    
2023-01-06 15:52:54,682 - --- validate (epoch=143)-----------
2023-01-06 15:52:54,682 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:54,903 - Epoch: [143][    5/    5]    Loss 0.393695    Top1 82.156489    
2023-01-06 15:52:54,969 - ==> Top1: 82.156    Loss: 0.394

2023-01-06 15:52:54,969 - ==> Confusion:
[[348  81   0]
 [106 513   0]
 [  0   0   0]]

2023-01-06 15:52:54,970 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:54,970 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:54,975 - 

2023-01-06 15:52:54,975 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:55,313 - Epoch: [144][   10/   37]    Overall Loss 0.371646    Objective Loss 0.371646                                        LR 0.000008    Time 0.033772    
2023-01-06 15:52:55,433 - Epoch: [144][   20/   37]    Overall Loss 0.376477    Objective Loss 0.376477                                        LR 0.000008    Time 0.022866    
2023-01-06 15:52:55,559 - Epoch: [144][   30/   37]    Overall Loss 0.376339    Objective Loss 0.376339                                        LR 0.000008    Time 0.019423    
2023-01-06 15:52:55,630 - Epoch: [144][   37/   37]    Overall Loss 0.380819    Objective Loss 0.380819    Top1 81.589958    LR 0.000008    Time 0.017653    
2023-01-06 15:52:55,715 - --- validate (epoch=144)-----------
2023-01-06 15:52:55,715 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:56,077 - Epoch: [144][    5/    5]    Loss 0.413808    Top1 82.442748    
2023-01-06 15:52:56,148 - ==> Top1: 82.443    Loss: 0.414

2023-01-06 15:52:56,149 - ==> Confusion:
[[320 109   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:52:56,150 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:56,151 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:56,158 - 

2023-01-06 15:52:56,158 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:56,507 - Epoch: [145][   10/   37]    Overall Loss 0.384337    Objective Loss 0.384337                                        LR 0.000008    Time 0.034855    
2023-01-06 15:52:56,657 - Epoch: [145][   20/   37]    Overall Loss 0.381554    Objective Loss 0.381554                                        LR 0.000008    Time 0.024905    
2023-01-06 15:52:56,809 - Epoch: [145][   30/   37]    Overall Loss 0.378593    Objective Loss 0.378593                                        LR 0.000008    Time 0.021655    
2023-01-06 15:52:56,888 - Epoch: [145][   37/   37]    Overall Loss 0.379980    Objective Loss 0.379980    Top1 80.125523    LR 0.000008    Time 0.019680    
2023-01-06 15:52:56,963 - --- validate (epoch=145)-----------
2023-01-06 15:52:56,963 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:57,182 - Epoch: [145][    5/    5]    Loss 0.393060    Top1 82.729008    
2023-01-06 15:52:57,251 - ==> Top1: 82.729    Loss: 0.393

2023-01-06 15:52:57,252 - ==> Confusion:
[[341  88   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-06 15:52:57,253 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:57,253 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:57,258 - 

2023-01-06 15:52:57,258 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:57,584 - Epoch: [146][   10/   37]    Overall Loss 0.382912    Objective Loss 0.382912                                        LR 0.000008    Time 0.032566    
2023-01-06 15:52:57,721 - Epoch: [146][   20/   37]    Overall Loss 0.380366    Objective Loss 0.380366                                        LR 0.000008    Time 0.023083    
2023-01-06 15:52:57,859 - Epoch: [146][   30/   37]    Overall Loss 0.378822    Objective Loss 0.378822                                        LR 0.000008    Time 0.019986    
2023-01-06 15:52:57,936 - Epoch: [146][   37/   37]    Overall Loss 0.379240    Objective Loss 0.379240    Top1 83.263598    LR 0.000008    Time 0.018292    
2023-01-06 15:52:58,013 - --- validate (epoch=146)-----------
2023-01-06 15:52:58,013 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:58,235 - Epoch: [146][    5/    5]    Loss 0.427532    Top1 82.442748    
2023-01-06 15:52:58,299 - ==> Top1: 82.443    Loss: 0.428

2023-01-06 15:52:58,300 - ==> Confusion:
[[321 108   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:52:58,301 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:58,301 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:58,306 - 

2023-01-06 15:52:58,306 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:58,640 - Epoch: [147][   10/   37]    Overall Loss 0.378010    Objective Loss 0.378010                                        LR 0.000008    Time 0.033367    
2023-01-06 15:52:58,769 - Epoch: [147][   20/   37]    Overall Loss 0.375338    Objective Loss 0.375338                                        LR 0.000008    Time 0.023115    
2023-01-06 15:52:58,914 - Epoch: [147][   30/   37]    Overall Loss 0.380501    Objective Loss 0.380501                                        LR 0.000008    Time 0.020218    
2023-01-06 15:52:58,993 - Epoch: [147][   37/   37]    Overall Loss 0.379261    Objective Loss 0.379261    Top1 85.774059    LR 0.000008    Time 0.018534    
2023-01-06 15:52:59,063 - --- validate (epoch=147)-----------
2023-01-06 15:52:59,063 - 1048 samples (256 per mini-batch)
2023-01-06 15:52:59,287 - Epoch: [147][    5/    5]    Loss 0.391825    Top1 81.965649    
2023-01-06 15:52:59,354 - ==> Top1: 81.966    Loss: 0.392

2023-01-06 15:52:59,354 - ==> Confusion:
[[309 120   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 15:52:59,355 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:52:59,355 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:52:59,360 - 

2023-01-06 15:52:59,360 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:52:59,704 - Epoch: [148][   10/   37]    Overall Loss 0.379360    Objective Loss 0.379360                                        LR 0.000008    Time 0.034329    
2023-01-06 15:52:59,848 - Epoch: [148][   20/   37]    Overall Loss 0.378938    Objective Loss 0.378938                                        LR 0.000008    Time 0.024313    
2023-01-06 15:52:59,989 - Epoch: [148][   30/   37]    Overall Loss 0.375126    Objective Loss 0.375126                                        LR 0.000008    Time 0.020898    
2023-01-06 15:53:00,057 - Epoch: [148][   37/   37]    Overall Loss 0.379215    Objective Loss 0.379215    Top1 79.288703    LR 0.000008    Time 0.018784    
2023-01-06 15:53:00,129 - --- validate (epoch=148)-----------
2023-01-06 15:53:00,129 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:00,360 - Epoch: [148][    5/    5]    Loss 0.391337    Top1 81.965649    
2023-01-06 15:53:00,419 - ==> Top1: 81.966    Loss: 0.391

2023-01-06 15:53:00,419 - ==> Confusion:
[[311 118   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:53:00,420 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:00,420 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:00,425 - 

2023-01-06 15:53:00,426 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:00,760 - Epoch: [149][   10/   37]    Overall Loss 0.389383    Objective Loss 0.389383                                        LR 0.000008    Time 0.033369    
2023-01-06 15:53:00,889 - Epoch: [149][   20/   37]    Overall Loss 0.383387    Objective Loss 0.383387                                        LR 0.000008    Time 0.023122    
2023-01-06 15:53:01,018 - Epoch: [149][   30/   37]    Overall Loss 0.382352    Objective Loss 0.382352                                        LR 0.000008    Time 0.019677    
2023-01-06 15:53:01,090 - Epoch: [149][   37/   37]    Overall Loss 0.379733    Objective Loss 0.379733    Top1 82.635983    LR 0.000008    Time 0.017902    
2023-01-06 15:53:01,166 - --- validate (epoch=149)-----------
2023-01-06 15:53:01,166 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:01,386 - Epoch: [149][    5/    5]    Loss 0.376860    Top1 81.679389    
2023-01-06 15:53:01,453 - ==> Top1: 81.679    Loss: 0.377

2023-01-06 15:53:01,454 - ==> Confusion:
[[336  93   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 15:53:01,455 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:01,455 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:01,460 - 

2023-01-06 15:53:01,460 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:01,793 - Epoch: [150][   10/   37]    Overall Loss 0.378506    Objective Loss 0.378506                                        LR 0.000008    Time 0.033277    
2023-01-06 15:53:01,918 - Epoch: [150][   20/   37]    Overall Loss 0.379691    Objective Loss 0.379691                                        LR 0.000008    Time 0.022864    
2023-01-06 15:53:02,052 - Epoch: [150][   30/   37]    Overall Loss 0.376145    Objective Loss 0.376145                                        LR 0.000008    Time 0.019706    
2023-01-06 15:53:02,125 - Epoch: [150][   37/   37]    Overall Loss 0.378829    Objective Loss 0.378829    Top1 83.682008    LR 0.000008    Time 0.017927    
2023-01-06 15:53:02,201 - --- validate (epoch=150)-----------
2023-01-06 15:53:02,202 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:02,419 - Epoch: [150][    5/    5]    Loss 0.381909    Top1 82.061069    
2023-01-06 15:53:02,483 - ==> Top1: 82.061    Loss: 0.382

2023-01-06 15:53:02,483 - ==> Confusion:
[[355  74   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 15:53:02,484 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:02,484 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:02,489 - 

2023-01-06 15:53:02,489 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:02,823 - Epoch: [151][   10/   37]    Overall Loss 0.383251    Objective Loss 0.383251                                        LR 0.000008    Time 0.033346    
2023-01-06 15:53:02,942 - Epoch: [151][   20/   37]    Overall Loss 0.383763    Objective Loss 0.383763                                        LR 0.000008    Time 0.022601    
2023-01-06 15:53:03,076 - Epoch: [151][   30/   37]    Overall Loss 0.379290    Objective Loss 0.379290                                        LR 0.000008    Time 0.019508    
2023-01-06 15:53:03,153 - Epoch: [151][   37/   37]    Overall Loss 0.377471    Objective Loss 0.377471    Top1 81.380753    LR 0.000008    Time 0.017889    
2023-01-06 15:53:03,226 - --- validate (epoch=151)-----------
2023-01-06 15:53:03,226 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:03,443 - Epoch: [151][    5/    5]    Loss 0.398341    Top1 81.583969    
2023-01-06 15:53:03,520 - ==> Top1: 81.584    Loss: 0.398

2023-01-06 15:53:03,520 - ==> Confusion:
[[306 123   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:53:03,522 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:03,522 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:03,527 - 

2023-01-06 15:53:03,527 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:03,873 - Epoch: [152][   10/   37]    Overall Loss 0.382776    Objective Loss 0.382776                                        LR 0.000008    Time 0.034544    
2023-01-06 15:53:04,012 - Epoch: [152][   20/   37]    Overall Loss 0.379648    Objective Loss 0.379648                                        LR 0.000008    Time 0.024223    
2023-01-06 15:53:04,151 - Epoch: [152][   30/   37]    Overall Loss 0.378230    Objective Loss 0.378230                                        LR 0.000008    Time 0.020758    
2023-01-06 15:53:04,231 - Epoch: [152][   37/   37]    Overall Loss 0.377481    Objective Loss 0.377481    Top1 85.774059    LR 0.000008    Time 0.018983    
2023-01-06 15:53:04,300 - --- validate (epoch=152)-----------
2023-01-06 15:53:04,300 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:04,526 - Epoch: [152][    5/    5]    Loss 0.413499    Top1 82.729008    
2023-01-06 15:53:04,592 - ==> Top1: 82.729    Loss: 0.413

2023-01-06 15:53:04,593 - ==> Confusion:
[[352  77   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 15:53:04,594 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:04,594 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:04,599 - 

2023-01-06 15:53:04,599 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:04,941 - Epoch: [153][   10/   37]    Overall Loss 0.386398    Objective Loss 0.386398                                        LR 0.000008    Time 0.034179    
2023-01-06 15:53:05,083 - Epoch: [153][   20/   37]    Overall Loss 0.377193    Objective Loss 0.377193                                        LR 0.000008    Time 0.024138    
2023-01-06 15:53:05,230 - Epoch: [153][   30/   37]    Overall Loss 0.376057    Objective Loss 0.376057                                        LR 0.000008    Time 0.020977    
2023-01-06 15:53:05,311 - Epoch: [153][   37/   37]    Overall Loss 0.377001    Objective Loss 0.377001    Top1 85.355649    LR 0.000008    Time 0.019208    
2023-01-06 15:53:05,379 - --- validate (epoch=153)-----------
2023-01-06 15:53:05,379 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:05,603 - Epoch: [153][    5/    5]    Loss 0.414556    Top1 82.347328    
2023-01-06 15:53:05,673 - ==> Top1: 82.347    Loss: 0.415

2023-01-06 15:53:05,673 - ==> Confusion:
[[351  78   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 15:53:05,675 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:05,675 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:05,680 - 

2023-01-06 15:53:05,680 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:06,018 - Epoch: [154][   10/   37]    Overall Loss 0.381597    Objective Loss 0.381597                                        LR 0.000008    Time 0.033753    
2023-01-06 15:53:06,152 - Epoch: [154][   20/   37]    Overall Loss 0.384055    Objective Loss 0.384055                                        LR 0.000008    Time 0.023562    
2023-01-06 15:53:06,287 - Epoch: [154][   30/   37]    Overall Loss 0.379994    Objective Loss 0.379994                                        LR 0.000008    Time 0.020181    
2023-01-06 15:53:06,359 - Epoch: [154][   37/   37]    Overall Loss 0.377525    Objective Loss 0.377525    Top1 85.146444    LR 0.000008    Time 0.018308    
2023-01-06 15:53:06,431 - --- validate (epoch=154)-----------
2023-01-06 15:53:06,431 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:06,648 - Epoch: [154][    5/    5]    Loss 0.397751    Top1 81.202290    
2023-01-06 15:53:06,720 - ==> Top1: 81.202    Loss: 0.398

2023-01-06 15:53:06,721 - ==> Confusion:
[[302 127   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:53:06,722 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:06,722 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:06,729 - 

2023-01-06 15:53:06,729 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:07,085 - Epoch: [155][   10/   37]    Overall Loss 0.389930    Objective Loss 0.389930                                        LR 0.000008    Time 0.035517    
2023-01-06 15:53:07,234 - Epoch: [155][   20/   37]    Overall Loss 0.377534    Objective Loss 0.377534                                        LR 0.000008    Time 0.025172    
2023-01-06 15:53:07,380 - Epoch: [155][   30/   37]    Overall Loss 0.376902    Objective Loss 0.376902                                        LR 0.000008    Time 0.021621    
2023-01-06 15:53:07,471 - Epoch: [155][   37/   37]    Overall Loss 0.376779    Objective Loss 0.376779    Top1 84.728033    LR 0.000008    Time 0.019965    
2023-01-06 15:53:07,548 - --- validate (epoch=155)-----------
2023-01-06 15:53:07,548 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:07,769 - Epoch: [155][    5/    5]    Loss 0.403783    Top1 81.870229    
2023-01-06 15:53:07,842 - ==> Top1: 81.870    Loss: 0.404

2023-01-06 15:53:07,842 - ==> Confusion:
[[306 123   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:53:07,843 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:07,843 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:07,849 - 

2023-01-06 15:53:07,849 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:08,181 - Epoch: [156][   10/   37]    Overall Loss 0.376009    Objective Loss 0.376009                                        LR 0.000008    Time 0.033128    
2023-01-06 15:53:08,310 - Epoch: [156][   20/   37]    Overall Loss 0.380320    Objective Loss 0.380320                                        LR 0.000008    Time 0.023003    
2023-01-06 15:53:08,440 - Epoch: [156][   30/   37]    Overall Loss 0.376176    Objective Loss 0.376176                                        LR 0.000008    Time 0.019654    
2023-01-06 15:53:08,511 - Epoch: [156][   37/   37]    Overall Loss 0.375419    Objective Loss 0.375419    Top1 83.054393    LR 0.000008    Time 0.017854    
2023-01-06 15:53:08,588 - --- validate (epoch=156)-----------
2023-01-06 15:53:08,589 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:08,807 - Epoch: [156][    5/    5]    Loss 0.388429    Top1 82.061069    
2023-01-06 15:53:08,879 - ==> Top1: 82.061    Loss: 0.388

2023-01-06 15:53:08,880 - ==> Confusion:
[[316 113   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 15:53:08,881 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:08,881 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:08,886 - 

2023-01-06 15:53:08,886 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:09,224 - Epoch: [157][   10/   37]    Overall Loss 0.359996    Objective Loss 0.359996                                        LR 0.000008    Time 0.033752    
2023-01-06 15:53:09,352 - Epoch: [157][   20/   37]    Overall Loss 0.370386    Objective Loss 0.370386                                        LR 0.000008    Time 0.023269    
2023-01-06 15:53:09,480 - Epoch: [157][   30/   37]    Overall Loss 0.372784    Objective Loss 0.372784                                        LR 0.000008    Time 0.019772    
2023-01-06 15:53:09,555 - Epoch: [157][   37/   37]    Overall Loss 0.375534    Objective Loss 0.375534    Top1 83.263598    LR 0.000008    Time 0.018051    
2023-01-06 15:53:09,633 - --- validate (epoch=157)-----------
2023-01-06 15:53:09,633 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:09,853 - Epoch: [157][    5/    5]    Loss 0.395484    Top1 81.679389    
2023-01-06 15:53:09,921 - ==> Top1: 81.679    Loss: 0.395

2023-01-06 15:53:09,922 - ==> Confusion:
[[355  74   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 15:53:09,923 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:09,923 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:09,928 - 

2023-01-06 15:53:09,928 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:10,267 - Epoch: [158][   10/   37]    Overall Loss 0.388111    Objective Loss 0.388111                                        LR 0.000008    Time 0.033884    
2023-01-06 15:53:10,380 - Epoch: [158][   20/   37]    Overall Loss 0.382062    Objective Loss 0.382062                                        LR 0.000008    Time 0.022564    
2023-01-06 15:53:10,500 - Epoch: [158][   30/   37]    Overall Loss 0.375482    Objective Loss 0.375482                                        LR 0.000008    Time 0.019013    
2023-01-06 15:53:10,570 - Epoch: [158][   37/   37]    Overall Loss 0.375369    Objective Loss 0.375369    Top1 83.891213    LR 0.000008    Time 0.017306    
2023-01-06 15:53:10,646 - --- validate (epoch=158)-----------
2023-01-06 15:53:10,646 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:10,865 - Epoch: [158][    5/    5]    Loss 0.459033    Top1 82.442748    
2023-01-06 15:53:10,928 - ==> Top1: 82.443    Loss: 0.459

2023-01-06 15:53:10,928 - ==> Confusion:
[[353  76   0]
 [108 511   0]
 [  0   0   0]]

2023-01-06 15:53:10,929 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:10,930 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:10,934 - 

2023-01-06 15:53:10,935 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:11,264 - Epoch: [159][   10/   37]    Overall Loss 0.377652    Objective Loss 0.377652                                        LR 0.000008    Time 0.032914    
2023-01-06 15:53:11,390 - Epoch: [159][   20/   37]    Overall Loss 0.374264    Objective Loss 0.374264                                        LR 0.000008    Time 0.022689    
2023-01-06 15:53:11,532 - Epoch: [159][   30/   37]    Overall Loss 0.376445    Objective Loss 0.376445                                        LR 0.000008    Time 0.019871    
2023-01-06 15:53:11,608 - Epoch: [159][   37/   37]    Overall Loss 0.376113    Objective Loss 0.376113    Top1 83.054393    LR 0.000008    Time 0.018162    
2023-01-06 15:53:11,681 - --- validate (epoch=159)-----------
2023-01-06 15:53:11,681 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:11,900 - Epoch: [159][    5/    5]    Loss 0.388120    Top1 81.583969    
2023-01-06 15:53:11,963 - ==> Top1: 81.584    Loss: 0.388

2023-01-06 15:53:11,964 - ==> Confusion:
[[304 125   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 15:53:11,965 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:11,965 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:11,970 - 

2023-01-06 15:53:11,970 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:12,440 - Epoch: [160][   10/   37]    Overall Loss 0.379346    Objective Loss 0.379346                                        LR 0.000008    Time 0.046949    
2023-01-06 15:53:12,588 - Epoch: [160][   20/   37]    Overall Loss 0.381341    Objective Loss 0.381341                                        LR 0.000008    Time 0.030826    
2023-01-06 15:53:12,730 - Epoch: [160][   30/   37]    Overall Loss 0.379143    Objective Loss 0.379143                                        LR 0.000008    Time 0.025296    
2023-01-06 15:53:12,809 - Epoch: [160][   37/   37]    Overall Loss 0.375099    Objective Loss 0.375099    Top1 83.891213    LR 0.000008    Time 0.022639    
2023-01-06 15:53:12,887 - --- validate (epoch=160)-----------
2023-01-06 15:53:12,888 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:13,110 - Epoch: [160][    5/    5]    Loss 0.387183    Top1 82.347328    
2023-01-06 15:53:13,172 - ==> Top1: 82.347    Loss: 0.387

2023-01-06 15:53:13,172 - ==> Confusion:
[[316 113   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-06 15:53:13,174 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:13,174 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:13,179 - 

2023-01-06 15:53:13,179 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:13,518 - Epoch: [161][   10/   37]    Overall Loss 0.365473    Objective Loss 0.365473                                        LR 0.000008    Time 0.033818    
2023-01-06 15:53:13,645 - Epoch: [161][   20/   37]    Overall Loss 0.371698    Objective Loss 0.371698                                        LR 0.000008    Time 0.023269    
2023-01-06 15:53:13,773 - Epoch: [161][   30/   37]    Overall Loss 0.376390    Objective Loss 0.376390                                        LR 0.000008    Time 0.019761    
2023-01-06 15:53:13,845 - Epoch: [161][   37/   37]    Overall Loss 0.373632    Objective Loss 0.373632    Top1 83.472803    LR 0.000008    Time 0.017964    
2023-01-06 15:53:13,918 - --- validate (epoch=161)-----------
2023-01-06 15:53:13,918 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:14,134 - Epoch: [161][    5/    5]    Loss 0.408454    Top1 81.679389    
2023-01-06 15:53:14,213 - ==> Top1: 81.679    Loss: 0.408

2023-01-06 15:53:14,214 - ==> Confusion:
[[305 124   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 15:53:14,215 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:14,215 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:14,220 - 

2023-01-06 15:53:14,220 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:14,553 - Epoch: [162][   10/   37]    Overall Loss 0.372097    Objective Loss 0.372097                                        LR 0.000008    Time 0.033229    
2023-01-06 15:53:14,689 - Epoch: [162][   20/   37]    Overall Loss 0.375100    Objective Loss 0.375100                                        LR 0.000008    Time 0.023409    
2023-01-06 15:53:14,823 - Epoch: [162][   30/   37]    Overall Loss 0.374758    Objective Loss 0.374758                                        LR 0.000008    Time 0.020044    
2023-01-06 15:53:14,895 - Epoch: [162][   37/   37]    Overall Loss 0.374122    Objective Loss 0.374122    Top1 83.472803    LR 0.000008    Time 0.018207    
2023-01-06 15:53:14,976 - --- validate (epoch=162)-----------
2023-01-06 15:53:14,977 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:15,205 - Epoch: [162][    5/    5]    Loss 0.378498    Top1 81.965649    
2023-01-06 15:53:15,278 - ==> Top1: 81.966    Loss: 0.378

2023-01-06 15:53:15,278 - ==> Confusion:
[[306 123   0]
 [ 66 553   0]
 [  0   0   0]]

2023-01-06 15:53:15,279 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:15,280 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:15,284 - 

2023-01-06 15:53:15,285 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:15,612 - Epoch: [163][   10/   37]    Overall Loss 0.381178    Objective Loss 0.381178                                        LR 0.000008    Time 0.032663    
2023-01-06 15:53:15,731 - Epoch: [163][   20/   37]    Overall Loss 0.375044    Objective Loss 0.375044                                        LR 0.000008    Time 0.022270    
2023-01-06 15:53:15,856 - Epoch: [163][   30/   37]    Overall Loss 0.372035    Objective Loss 0.372035                                        LR 0.000008    Time 0.019005    
2023-01-06 15:53:15,924 - Epoch: [163][   37/   37]    Overall Loss 0.373656    Objective Loss 0.373656    Top1 79.916318    LR 0.000008    Time 0.017241    
2023-01-06 15:53:15,992 - --- validate (epoch=163)-----------
2023-01-06 15:53:15,992 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:16,221 - Epoch: [163][    5/    5]    Loss 0.396253    Top1 82.347328    
2023-01-06 15:53:16,282 - ==> Top1: 82.347    Loss: 0.396

2023-01-06 15:53:16,282 - ==> Confusion:
[[334  95   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-06 15:53:16,283 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:16,283 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:16,288 - 

2023-01-06 15:53:16,288 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:16,626 - Epoch: [164][   10/   37]    Overall Loss 0.368446    Objective Loss 0.368446                                        LR 0.000008    Time 0.033680    
2023-01-06 15:53:16,751 - Epoch: [164][   20/   37]    Overall Loss 0.373552    Objective Loss 0.373552                                        LR 0.000008    Time 0.023075    
2023-01-06 15:53:16,881 - Epoch: [164][   30/   37]    Overall Loss 0.373339    Objective Loss 0.373339                                        LR 0.000008    Time 0.019717    
2023-01-06 15:53:16,960 - Epoch: [164][   37/   37]    Overall Loss 0.373088    Objective Loss 0.373088    Top1 83.054393    LR 0.000008    Time 0.018104    
2023-01-06 15:53:17,059 - --- validate (epoch=164)-----------
2023-01-06 15:53:17,060 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:17,290 - Epoch: [164][    5/    5]    Loss 0.396329    Top1 82.538168    
2023-01-06 15:53:17,356 - ==> Top1: 82.538    Loss: 0.396

2023-01-06 15:53:17,356 - ==> Confusion:
[[319 110   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 15:53:17,357 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:17,358 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:17,363 - 

2023-01-06 15:53:17,363 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:17,718 - Epoch: [165][   10/   37]    Overall Loss 0.374731    Objective Loss 0.374731                                        LR 0.000008    Time 0.035488    
2023-01-06 15:53:17,853 - Epoch: [165][   20/   37]    Overall Loss 0.369285    Objective Loss 0.369285                                        LR 0.000008    Time 0.024473    
2023-01-06 15:53:17,986 - Epoch: [165][   30/   37]    Overall Loss 0.372810    Objective Loss 0.372810                                        LR 0.000008    Time 0.020735    
2023-01-06 15:53:18,066 - Epoch: [165][   37/   37]    Overall Loss 0.372611    Objective Loss 0.372611    Top1 86.401674    LR 0.000008    Time 0.018961    
2023-01-06 15:53:18,139 - --- validate (epoch=165)-----------
2023-01-06 15:53:18,140 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:18,365 - Epoch: [165][    5/    5]    Loss 0.391322    Top1 81.965649    
2023-01-06 15:53:18,427 - ==> Top1: 81.966    Loss: 0.391

2023-01-06 15:53:18,427 - ==> Confusion:
[[359  70   0]
 [119 500   0]
 [  0   0   0]]

2023-01-06 15:53:18,429 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:18,429 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:18,434 - 

2023-01-06 15:53:18,434 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:18,793 - Epoch: [166][   10/   37]    Overall Loss 0.379299    Objective Loss 0.379299                                        LR 0.000008    Time 0.035912    
2023-01-06 15:53:18,935 - Epoch: [166][   20/   37]    Overall Loss 0.375306    Objective Loss 0.375306                                        LR 0.000008    Time 0.024999    
2023-01-06 15:53:19,077 - Epoch: [166][   30/   37]    Overall Loss 0.374454    Objective Loss 0.374454                                        LR 0.000008    Time 0.021376    
2023-01-06 15:53:19,156 - Epoch: [166][   37/   37]    Overall Loss 0.372392    Objective Loss 0.372392    Top1 85.983264    LR 0.000008    Time 0.019477    
2023-01-06 15:53:19,234 - --- validate (epoch=166)-----------
2023-01-06 15:53:19,235 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:19,455 - Epoch: [166][    5/    5]    Loss 0.425448    Top1 82.633588    
2023-01-06 15:53:19,527 - ==> Top1: 82.634    Loss: 0.425

2023-01-06 15:53:19,527 - ==> Confusion:
[[354  75   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 15:53:19,528 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:19,528 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:19,533 - 

2023-01-06 15:53:19,534 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:19,876 - Epoch: [167][   10/   37]    Overall Loss 0.371018    Objective Loss 0.371018                                        LR 0.000008    Time 0.034183    
2023-01-06 15:53:20,020 - Epoch: [167][   20/   37]    Overall Loss 0.362755    Objective Loss 0.362755                                        LR 0.000008    Time 0.024260    
2023-01-06 15:53:20,161 - Epoch: [167][   30/   37]    Overall Loss 0.369149    Objective Loss 0.369149                                        LR 0.000008    Time 0.020872    
2023-01-06 15:53:20,239 - Epoch: [167][   37/   37]    Overall Loss 0.373119    Objective Loss 0.373119    Top1 82.008368    LR 0.000008    Time 0.019031    
2023-01-06 15:53:20,320 - --- validate (epoch=167)-----------
2023-01-06 15:53:20,320 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:20,539 - Epoch: [167][    5/    5]    Loss 0.378150    Top1 82.156489    
2023-01-06 15:53:20,600 - ==> Top1: 82.156    Loss: 0.378

2023-01-06 15:53:20,601 - ==> Confusion:
[[308 121   0]
 [ 66 553   0]
 [  0   0   0]]

2023-01-06 15:53:20,602 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:20,602 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:20,607 - 

2023-01-06 15:53:20,607 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:20,939 - Epoch: [168][   10/   37]    Overall Loss 0.378482    Objective Loss 0.378482                                        LR 0.000008    Time 0.033151    
2023-01-06 15:53:21,064 - Epoch: [168][   20/   37]    Overall Loss 0.371425    Objective Loss 0.371425                                        LR 0.000008    Time 0.022798    
2023-01-06 15:53:21,205 - Epoch: [168][   30/   37]    Overall Loss 0.371766    Objective Loss 0.371766                                        LR 0.000008    Time 0.019873    
2023-01-06 15:53:21,282 - Epoch: [168][   37/   37]    Overall Loss 0.372183    Objective Loss 0.372183    Top1 83.682008    LR 0.000008    Time 0.018204    
2023-01-06 15:53:21,360 - --- validate (epoch=168)-----------
2023-01-06 15:53:21,361 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:21,579 - Epoch: [168][    5/    5]    Loss 0.381311    Top1 81.774809    
2023-01-06 15:53:21,638 - ==> Top1: 81.775    Loss: 0.381

2023-01-06 15:53:21,638 - ==> Confusion:
[[356  73   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 15:53:21,639 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:21,639 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:21,644 - 

2023-01-06 15:53:21,644 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:21,986 - Epoch: [169][   10/   37]    Overall Loss 0.371242    Objective Loss 0.371242                                        LR 0.000008    Time 0.034080    
2023-01-06 15:53:22,136 - Epoch: [169][   20/   37]    Overall Loss 0.368165    Objective Loss 0.368165                                        LR 0.000008    Time 0.024545    
2023-01-06 15:53:22,286 - Epoch: [169][   30/   37]    Overall Loss 0.371397    Objective Loss 0.371397                                        LR 0.000008    Time 0.021351    
2023-01-06 15:53:22,366 - Epoch: [169][   37/   37]    Overall Loss 0.371491    Objective Loss 0.371491    Top1 85.983264    LR 0.000008    Time 0.019464    
2023-01-06 15:53:22,451 - --- validate (epoch=169)-----------
2023-01-06 15:53:22,451 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:22,674 - Epoch: [169][    5/    5]    Loss 0.410067    Top1 82.442748    
2023-01-06 15:53:22,742 - ==> Top1: 82.443    Loss: 0.410

2023-01-06 15:53:22,742 - ==> Confusion:
[[359  70   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 15:53:22,743 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:22,743 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:22,748 - 

2023-01-06 15:53:22,748 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:23,094 - Epoch: [170][   10/   37]    Overall Loss 0.382101    Objective Loss 0.382101                                        LR 0.000008    Time 0.034466    
2023-01-06 15:53:23,239 - Epoch: [170][   20/   37]    Overall Loss 0.384027    Objective Loss 0.384027                                        LR 0.000008    Time 0.024480    
2023-01-06 15:53:23,392 - Epoch: [170][   30/   37]    Overall Loss 0.376344    Objective Loss 0.376344                                        LR 0.000008    Time 0.021387    
2023-01-06 15:53:23,473 - Epoch: [170][   37/   37]    Overall Loss 0.371897    Objective Loss 0.371897    Top1 82.008368    LR 0.000008    Time 0.019538    
2023-01-06 15:53:23,551 - --- validate (epoch=170)-----------
2023-01-06 15:53:23,552 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:23,774 - Epoch: [170][    5/    5]    Loss 0.370798    Top1 80.916031    
2023-01-06 15:53:23,839 - ==> Top1: 80.916    Loss: 0.371

2023-01-06 15:53:23,840 - ==> Confusion:
[[308 121   0]
 [ 79 540   0]
 [  0   0   0]]

2023-01-06 15:53:23,841 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:23,841 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:23,846 - 

2023-01-06 15:53:23,846 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:24,184 - Epoch: [171][   10/   37]    Overall Loss 0.355341    Objective Loss 0.355341                                        LR 0.000008    Time 0.033727    
2023-01-06 15:53:24,333 - Epoch: [171][   20/   37]    Overall Loss 0.368653    Objective Loss 0.368653                                        LR 0.000008    Time 0.024274    
2023-01-06 15:53:24,482 - Epoch: [171][   30/   37]    Overall Loss 0.373362    Objective Loss 0.373362                                        LR 0.000008    Time 0.021134    
2023-01-06 15:53:24,561 - Epoch: [171][   37/   37]    Overall Loss 0.371133    Objective Loss 0.371133    Top1 86.401674    LR 0.000008    Time 0.019289    
2023-01-06 15:53:24,628 - --- validate (epoch=171)-----------
2023-01-06 15:53:24,628 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:24,858 - Epoch: [171][    5/    5]    Loss 0.372244    Top1 80.916031    
2023-01-06 15:53:24,916 - ==> Top1: 80.916    Loss: 0.372

2023-01-06 15:53:24,916 - ==> Confusion:
[[367  62   0]
 [138 481   0]
 [  0   0   0]]

2023-01-06 15:53:24,917 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:24,917 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:24,922 - 

2023-01-06 15:53:24,922 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:25,256 - Epoch: [172][   10/   37]    Overall Loss 0.378061    Objective Loss 0.378061                                        LR 0.000008    Time 0.033275    
2023-01-06 15:53:25,395 - Epoch: [172][   20/   37]    Overall Loss 0.373338    Objective Loss 0.373338                                        LR 0.000008    Time 0.023582    
2023-01-06 15:53:25,533 - Epoch: [172][   30/   37]    Overall Loss 0.372169    Objective Loss 0.372169                                        LR 0.000008    Time 0.020299    
2023-01-06 15:53:25,613 - Epoch: [172][   37/   37]    Overall Loss 0.371138    Objective Loss 0.371138    Top1 83.682008    LR 0.000008    Time 0.018617    
2023-01-06 15:53:25,682 - --- validate (epoch=172)-----------
2023-01-06 15:53:25,682 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:25,898 - Epoch: [172][    5/    5]    Loss 0.401844    Top1 81.679389    
2023-01-06 15:53:25,959 - ==> Top1: 81.679    Loss: 0.402

2023-01-06 15:53:25,960 - ==> Confusion:
[[301 128   0]
 [ 64 555   0]
 [  0   0   0]]

2023-01-06 15:53:25,961 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:25,961 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:25,966 - 

2023-01-06 15:53:25,966 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:26,299 - Epoch: [173][   10/   37]    Overall Loss 0.368573    Objective Loss 0.368573                                        LR 0.000008    Time 0.033202    
2023-01-06 15:53:26,442 - Epoch: [173][   20/   37]    Overall Loss 0.373622    Objective Loss 0.373622                                        LR 0.000008    Time 0.023723    
2023-01-06 15:53:26,590 - Epoch: [173][   30/   37]    Overall Loss 0.371645    Objective Loss 0.371645                                        LR 0.000008    Time 0.020734    
2023-01-06 15:53:26,668 - Epoch: [173][   37/   37]    Overall Loss 0.370706    Objective Loss 0.370706    Top1 82.845188    LR 0.000008    Time 0.018921    
2023-01-06 15:53:26,743 - --- validate (epoch=173)-----------
2023-01-06 15:53:26,743 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:26,965 - Epoch: [173][    5/    5]    Loss 0.397891    Top1 82.156489    
2023-01-06 15:53:27,026 - ==> Top1: 82.156    Loss: 0.398

2023-01-06 15:53:27,026 - ==> Confusion:
[[353  76   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 15:53:27,027 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:27,028 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:27,033 - 

2023-01-06 15:53:27,034 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:27,370 - Epoch: [174][   10/   37]    Overall Loss 0.374881    Objective Loss 0.374881                                        LR 0.000008    Time 0.033586    
2023-01-06 15:53:27,485 - Epoch: [174][   20/   37]    Overall Loss 0.372038    Objective Loss 0.372038                                        LR 0.000008    Time 0.022530    
2023-01-06 15:53:27,604 - Epoch: [174][   30/   37]    Overall Loss 0.369290    Objective Loss 0.369290                                        LR 0.000008    Time 0.018955    
2023-01-06 15:53:27,682 - Epoch: [174][   37/   37]    Overall Loss 0.370002    Objective Loss 0.370002    Top1 83.682008    LR 0.000008    Time 0.017484    
2023-01-06 15:53:27,762 - --- validate (epoch=174)-----------
2023-01-06 15:53:27,762 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:28,251 - Epoch: [174][    5/    5]    Loss 0.427738    Top1 82.538168    
2023-01-06 15:53:28,315 - ==> Top1: 82.538    Loss: 0.428

2023-01-06 15:53:28,316 - ==> Confusion:
[[358  71   0]
 [112 507   0]
 [  0   0   0]]

2023-01-06 15:53:28,317 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:28,317 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:28,322 - 

2023-01-06 15:53:28,322 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:28,665 - Epoch: [175][   10/   37]    Overall Loss 0.373150    Objective Loss 0.373150                                        LR 0.000008    Time 0.034267    
2023-01-06 15:53:28,797 - Epoch: [175][   20/   37]    Overall Loss 0.371844    Objective Loss 0.371844                                        LR 0.000008    Time 0.023711    
2023-01-06 15:53:28,936 - Epoch: [175][   30/   37]    Overall Loss 0.368993    Objective Loss 0.368993                                        LR 0.000008    Time 0.020422    
2023-01-06 15:53:29,015 - Epoch: [175][   37/   37]    Overall Loss 0.369985    Objective Loss 0.369985    Top1 83.472803    LR 0.000008    Time 0.018689    
2023-01-06 15:53:29,096 - --- validate (epoch=175)-----------
2023-01-06 15:53:29,096 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:29,313 - Epoch: [175][    5/    5]    Loss 0.370726    Top1 82.442748    
2023-01-06 15:53:29,386 - ==> Top1: 82.443    Loss: 0.371

2023-01-06 15:53:29,386 - ==> Confusion:
[[325 104   0]
 [ 80 539   0]
 [  0   0   0]]

2023-01-06 15:53:29,387 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:29,387 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:29,392 - 

2023-01-06 15:53:29,392 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:29,741 - Epoch: [176][   10/   37]    Overall Loss 0.373152    Objective Loss 0.373152                                        LR 0.000008    Time 0.034811    
2023-01-06 15:53:29,878 - Epoch: [176][   20/   37]    Overall Loss 0.372592    Objective Loss 0.372592                                        LR 0.000008    Time 0.024235    
2023-01-06 15:53:30,017 - Epoch: [176][   30/   37]    Overall Loss 0.369280    Objective Loss 0.369280                                        LR 0.000008    Time 0.020782    
2023-01-06 15:53:30,096 - Epoch: [176][   37/   37]    Overall Loss 0.370081    Objective Loss 0.370081    Top1 83.682008    LR 0.000008    Time 0.018957    
2023-01-06 15:53:30,173 - --- validate (epoch=176)-----------
2023-01-06 15:53:30,173 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:30,390 - Epoch: [176][    5/    5]    Loss 0.375717    Top1 82.251908    
2023-01-06 15:53:30,450 - ==> Top1: 82.252    Loss: 0.376

2023-01-06 15:53:30,450 - ==> Confusion:
[[313 116   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:53:30,451 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:30,451 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:30,456 - 

2023-01-06 15:53:30,456 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:30,786 - Epoch: [177][   10/   37]    Overall Loss 0.369708    Objective Loss 0.369708                                        LR 0.000008    Time 0.032925    
2023-01-06 15:53:30,919 - Epoch: [177][   20/   37]    Overall Loss 0.371044    Objective Loss 0.371044                                        LR 0.000008    Time 0.023066    
2023-01-06 15:53:31,063 - Epoch: [177][   30/   37]    Overall Loss 0.370476    Objective Loss 0.370476                                        LR 0.000008    Time 0.020168    
2023-01-06 15:53:31,141 - Epoch: [177][   37/   37]    Overall Loss 0.369052    Objective Loss 0.369052    Top1 86.610879    LR 0.000008    Time 0.018469    
2023-01-06 15:53:31,220 - --- validate (epoch=177)-----------
2023-01-06 15:53:31,220 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:31,451 - Epoch: [177][    5/    5]    Loss 0.402921    Top1 82.061069    
2023-01-06 15:53:31,516 - ==> Top1: 82.061    Loss: 0.403

2023-01-06 15:53:31,516 - ==> Confusion:
[[324 105   0]
 [ 83 536   0]
 [  0   0   0]]

2023-01-06 15:53:31,517 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:31,517 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:31,524 - 

2023-01-06 15:53:31,524 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:31,851 - Epoch: [178][   10/   37]    Overall Loss 0.373639    Objective Loss 0.373639                                        LR 0.000008    Time 0.032654    
2023-01-06 15:53:31,998 - Epoch: [178][   20/   37]    Overall Loss 0.368321    Objective Loss 0.368321                                        LR 0.000008    Time 0.023658    
2023-01-06 15:53:32,149 - Epoch: [178][   30/   37]    Overall Loss 0.369524    Objective Loss 0.369524                                        LR 0.000008    Time 0.020807    
2023-01-06 15:53:32,228 - Epoch: [178][   37/   37]    Overall Loss 0.368954    Objective Loss 0.368954    Top1 83.054393    LR 0.000008    Time 0.018978    
2023-01-06 15:53:32,312 - --- validate (epoch=178)-----------
2023-01-06 15:53:32,312 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:32,536 - Epoch: [178][    5/    5]    Loss 0.381633    Top1 81.393130    
2023-01-06 15:53:32,608 - ==> Top1: 81.393    Loss: 0.382

2023-01-06 15:53:32,609 - ==> Confusion:
[[301 128   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:53:32,610 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:32,610 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:32,615 - 

2023-01-06 15:53:32,615 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:32,961 - Epoch: [179][   10/   37]    Overall Loss 0.369757    Objective Loss 0.369757                                        LR 0.000008    Time 0.034582    
2023-01-06 15:53:33,108 - Epoch: [179][   20/   37]    Overall Loss 0.365742    Objective Loss 0.365742                                        LR 0.000008    Time 0.024596    
2023-01-06 15:53:33,257 - Epoch: [179][   30/   37]    Overall Loss 0.364116    Objective Loss 0.364116                                        LR 0.000008    Time 0.021352    
2023-01-06 15:53:33,333 - Epoch: [179][   37/   37]    Overall Loss 0.368756    Objective Loss 0.368756    Top1 83.263598    LR 0.000008    Time 0.019362    
2023-01-06 15:53:33,414 - --- validate (epoch=179)-----------
2023-01-06 15:53:33,414 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:33,634 - Epoch: [179][    5/    5]    Loss 0.379433    Top1 81.106870    
2023-01-06 15:53:33,695 - ==> Top1: 81.107    Loss: 0.379

2023-01-06 15:53:33,695 - ==> Confusion:
[[302 127   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:53:33,696 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:33,696 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:33,701 - 

2023-01-06 15:53:33,701 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:34,024 - Epoch: [180][   10/   37]    Overall Loss 0.378253    Objective Loss 0.378253                                        LR 0.000005    Time 0.032209    
2023-01-06 15:53:34,149 - Epoch: [180][   20/   37]    Overall Loss 0.374031    Objective Loss 0.374031                                        LR 0.000005    Time 0.022344    
2023-01-06 15:53:34,273 - Epoch: [180][   30/   37]    Overall Loss 0.366502    Objective Loss 0.366502                                        LR 0.000005    Time 0.019019    
2023-01-06 15:53:34,341 - Epoch: [180][   37/   37]    Overall Loss 0.368039    Objective Loss 0.368039    Top1 85.983264    LR 0.000005    Time 0.017240    
2023-01-06 15:53:34,414 - --- validate (epoch=180)-----------
2023-01-06 15:53:34,415 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:34,645 - Epoch: [180][    5/    5]    Loss 0.405054    Top1 81.393130    
2023-01-06 15:53:34,717 - ==> Top1: 81.393    Loss: 0.405

2023-01-06 15:53:34,718 - ==> Confusion:
[[362  67   0]
 [128 491   0]
 [  0   0   0]]

2023-01-06 15:53:34,719 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:34,719 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:34,724 - 

2023-01-06 15:53:34,724 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:35,052 - Epoch: [181][   10/   37]    Overall Loss 0.372690    Objective Loss 0.372690                                        LR 0.000005    Time 0.032758    
2023-01-06 15:53:35,166 - Epoch: [181][   20/   37]    Overall Loss 0.373372    Objective Loss 0.373372                                        LR 0.000005    Time 0.022030    
2023-01-06 15:53:35,283 - Epoch: [181][   30/   37]    Overall Loss 0.368902    Objective Loss 0.368902                                        LR 0.000005    Time 0.018584    
2023-01-06 15:53:35,354 - Epoch: [181][   37/   37]    Overall Loss 0.368965    Objective Loss 0.368965    Top1 82.426778    LR 0.000005    Time 0.016972    
2023-01-06 15:53:35,427 - --- validate (epoch=181)-----------
2023-01-06 15:53:35,428 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:35,649 - Epoch: [181][    5/    5]    Loss 0.395194    Top1 81.870229    
2023-01-06 15:53:35,715 - ==> Top1: 81.870    Loss: 0.395

2023-01-06 15:53:35,715 - ==> Confusion:
[[310 119   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:53:35,716 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:35,716 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:35,721 - 

2023-01-06 15:53:35,721 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:36,063 - Epoch: [182][   10/   37]    Overall Loss 0.359922    Objective Loss 0.359922                                        LR 0.000005    Time 0.034093    
2023-01-06 15:53:36,205 - Epoch: [182][   20/   37]    Overall Loss 0.363122    Objective Loss 0.363122                                        LR 0.000005    Time 0.024107    
2023-01-06 15:53:36,328 - Epoch: [182][   30/   37]    Overall Loss 0.366787    Objective Loss 0.366787                                        LR 0.000005    Time 0.020165    
2023-01-06 15:53:36,399 - Epoch: [182][   37/   37]    Overall Loss 0.367567    Objective Loss 0.367567    Top1 84.100418    LR 0.000005    Time 0.018264    
2023-01-06 15:53:36,477 - --- validate (epoch=182)-----------
2023-01-06 15:53:36,478 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:36,699 - Epoch: [182][    5/    5]    Loss 0.378115    Top1 82.633588    
2023-01-06 15:53:36,767 - ==> Top1: 82.634    Loss: 0.378

2023-01-06 15:53:36,767 - ==> Confusion:
[[351  78   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 15:53:36,769 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:36,769 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:36,774 - 

2023-01-06 15:53:36,774 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:37,110 - Epoch: [183][   10/   37]    Overall Loss 0.370548    Objective Loss 0.370548                                        LR 0.000005    Time 0.033586    
2023-01-06 15:53:37,243 - Epoch: [183][   20/   37]    Overall Loss 0.364349    Objective Loss 0.364349                                        LR 0.000005    Time 0.023406    
2023-01-06 15:53:37,379 - Epoch: [183][   30/   37]    Overall Loss 0.368372    Objective Loss 0.368372                                        LR 0.000005    Time 0.020130    
2023-01-06 15:53:37,462 - Epoch: [183][   37/   37]    Overall Loss 0.367353    Objective Loss 0.367353    Top1 85.983264    LR 0.000005    Time 0.018559    
2023-01-06 15:53:37,538 - --- validate (epoch=183)-----------
2023-01-06 15:53:37,538 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:37,765 - Epoch: [183][    5/    5]    Loss 0.391434    Top1 82.824427    
2023-01-06 15:53:37,832 - ==> Top1: 82.824    Loss: 0.391

2023-01-06 15:53:37,833 - ==> Confusion:
[[325 104   0]
 [ 76 543   0]
 [  0   0   0]]

2023-01-06 15:53:37,834 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:37,834 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:37,839 - 

2023-01-06 15:53:37,839 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:38,189 - Epoch: [184][   10/   37]    Overall Loss 0.382214    Objective Loss 0.382214                                        LR 0.000005    Time 0.034906    
2023-01-06 15:53:38,345 - Epoch: [184][   20/   37]    Overall Loss 0.370808    Objective Loss 0.370808                                        LR 0.000005    Time 0.025256    
2023-01-06 15:53:38,511 - Epoch: [184][   30/   37]    Overall Loss 0.369599    Objective Loss 0.369599                                        LR 0.000005    Time 0.022363    
2023-01-06 15:53:38,600 - Epoch: [184][   37/   37]    Overall Loss 0.367650    Objective Loss 0.367650    Top1 83.682008    LR 0.000005    Time 0.020511    
2023-01-06 15:53:38,677 - --- validate (epoch=184)-----------
2023-01-06 15:53:38,678 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:38,897 - Epoch: [184][    5/    5]    Loss 0.382083    Top1 81.870229    
2023-01-06 15:53:38,969 - ==> Top1: 81.870    Loss: 0.382

2023-01-06 15:53:38,969 - ==> Confusion:
[[313 116   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-06 15:53:38,970 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 98]
2023-01-06 15:53:38,970 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:38,975 - 

2023-01-06 15:53:38,976 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:39,343 - Epoch: [185][   10/   37]    Overall Loss 0.369486    Objective Loss 0.369486                                        LR 0.000005    Time 0.036730    
2023-01-06 15:53:39,493 - Epoch: [185][   20/   37]    Overall Loss 0.368957    Objective Loss 0.368957                                        LR 0.000005    Time 0.025835    
2023-01-06 15:53:39,639 - Epoch: [185][   30/   37]    Overall Loss 0.367791    Objective Loss 0.367791                                        LR 0.000005    Time 0.022054    
2023-01-06 15:53:39,719 - Epoch: [185][   37/   37]    Overall Loss 0.367288    Objective Loss 0.367288    Top1 82.845188    LR 0.000005    Time 0.020049    
2023-01-06 15:53:39,796 - --- validate (epoch=185)-----------
2023-01-06 15:53:39,797 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:40,014 - Epoch: [185][    5/    5]    Loss 0.412839    Top1 82.919847    
2023-01-06 15:53:40,079 - ==> Top1: 82.920    Loss: 0.413

2023-01-06 15:53:40,079 - ==> Confusion:
[[343  86   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-06 15:53:40,080 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:40,080 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:40,087 - 

2023-01-06 15:53:40,087 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:40,408 - Epoch: [186][   10/   37]    Overall Loss 0.373833    Objective Loss 0.373833                                        LR 0.000005    Time 0.032037    
2023-01-06 15:53:40,533 - Epoch: [186][   20/   37]    Overall Loss 0.374748    Objective Loss 0.374748                                        LR 0.000005    Time 0.022254    
2023-01-06 15:53:40,680 - Epoch: [186][   30/   37]    Overall Loss 0.369656    Objective Loss 0.369656                                        LR 0.000005    Time 0.019739    
2023-01-06 15:53:40,759 - Epoch: [186][   37/   37]    Overall Loss 0.366847    Objective Loss 0.366847    Top1 84.309623    LR 0.000005    Time 0.018120    
2023-01-06 15:53:40,823 - --- validate (epoch=186)-----------
2023-01-06 15:53:40,823 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:41,049 - Epoch: [186][    5/    5]    Loss 0.381695    Top1 82.633588    
2023-01-06 15:53:41,110 - ==> Top1: 82.634    Loss: 0.382

2023-01-06 15:53:41,110 - ==> Confusion:
[[318 111   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 15:53:41,112 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:41,112 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:41,117 - 

2023-01-06 15:53:41,117 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:41,448 - Epoch: [187][   10/   37]    Overall Loss 0.369705    Objective Loss 0.369705                                        LR 0.000005    Time 0.033014    
2023-01-06 15:53:41,580 - Epoch: [187][   20/   37]    Overall Loss 0.363884    Objective Loss 0.363884                                        LR 0.000005    Time 0.023099    
2023-01-06 15:53:41,726 - Epoch: [187][   30/   37]    Overall Loss 0.363974    Objective Loss 0.363974                                        LR 0.000005    Time 0.020261    
2023-01-06 15:53:41,804 - Epoch: [187][   37/   37]    Overall Loss 0.367299    Objective Loss 0.367299    Top1 82.635983    LR 0.000005    Time 0.018530    
2023-01-06 15:53:41,885 - --- validate (epoch=187)-----------
2023-01-06 15:53:41,886 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:42,110 - Epoch: [187][    5/    5]    Loss 0.358649    Top1 82.538168    
2023-01-06 15:53:42,195 - ==> Top1: 82.538    Loss: 0.359

2023-01-06 15:53:42,195 - ==> Confusion:
[[320 109   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-06 15:53:42,196 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:42,196 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:42,201 - 

2023-01-06 15:53:42,201 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:42,539 - Epoch: [188][   10/   37]    Overall Loss 0.368717    Objective Loss 0.368717                                        LR 0.000005    Time 0.033703    
2023-01-06 15:53:42,672 - Epoch: [188][   20/   37]    Overall Loss 0.372017    Objective Loss 0.372017                                        LR 0.000005    Time 0.023468    
2023-01-06 15:53:42,813 - Epoch: [188][   30/   37]    Overall Loss 0.368302    Objective Loss 0.368302                                        LR 0.000005    Time 0.020340    
2023-01-06 15:53:42,893 - Epoch: [188][   37/   37]    Overall Loss 0.367072    Objective Loss 0.367072    Top1 84.309623    LR 0.000005    Time 0.018658    
2023-01-06 15:53:42,966 - --- validate (epoch=188)-----------
2023-01-06 15:53:42,966 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:43,184 - Epoch: [188][    5/    5]    Loss 0.424735    Top1 82.061069    
2023-01-06 15:53:43,245 - ==> Top1: 82.061    Loss: 0.425

2023-01-06 15:53:43,245 - ==> Confusion:
[[354  75   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 15:53:43,246 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:43,247 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:43,251 - 

2023-01-06 15:53:43,252 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:43,578 - Epoch: [189][   10/   37]    Overall Loss 0.373831    Objective Loss 0.373831                                        LR 0.000005    Time 0.032593    
2023-01-06 15:53:43,711 - Epoch: [189][   20/   37]    Overall Loss 0.368396    Objective Loss 0.368396                                        LR 0.000005    Time 0.022940    
2023-01-06 15:53:43,852 - Epoch: [189][   30/   37]    Overall Loss 0.367832    Objective Loss 0.367832                                        LR 0.000005    Time 0.019971    
2023-01-06 15:53:43,933 - Epoch: [189][   37/   37]    Overall Loss 0.368054    Objective Loss 0.368054    Top1 80.543933    LR 0.000005    Time 0.018374    
2023-01-06 15:53:43,997 - --- validate (epoch=189)-----------
2023-01-06 15:53:43,998 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:44,221 - Epoch: [189][    5/    5]    Loss 0.396599    Top1 81.011450    
2023-01-06 15:53:44,290 - ==> Top1: 81.011    Loss: 0.397

2023-01-06 15:53:44,290 - ==> Confusion:
[[300 129   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 15:53:44,291 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:44,291 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:44,296 - 

2023-01-06 15:53:44,296 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:44,625 - Epoch: [190][   10/   37]    Overall Loss 0.367382    Objective Loss 0.367382                                        LR 0.000003    Time 0.032807    
2023-01-06 15:53:44,752 - Epoch: [190][   20/   37]    Overall Loss 0.369364    Objective Loss 0.369364                                        LR 0.000003    Time 0.022734    
2023-01-06 15:53:44,880 - Epoch: [190][   30/   37]    Overall Loss 0.369476    Objective Loss 0.369476                                        LR 0.000003    Time 0.019397    
2023-01-06 15:53:44,956 - Epoch: [190][   37/   37]    Overall Loss 0.366746    Objective Loss 0.366746    Top1 86.610879    LR 0.000003    Time 0.017776    
2023-01-06 15:53:45,028 - --- validate (epoch=190)-----------
2023-01-06 15:53:45,029 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:45,249 - Epoch: [190][    5/    5]    Loss 0.368613    Top1 82.729008    
2023-01-06 15:53:45,318 - ==> Top1: 82.729    Loss: 0.369

2023-01-06 15:53:45,319 - ==> Confusion:
[[336  93   0]
 [ 88 531   0]
 [  0   0   0]]

2023-01-06 15:53:45,320 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:45,320 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:45,325 - 

2023-01-06 15:53:45,325 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:45,934 - Epoch: [191][   10/   37]    Overall Loss 0.367064    Objective Loss 0.367064                                        LR 0.000003    Time 0.060870    
2023-01-06 15:53:46,087 - Epoch: [191][   20/   37]    Overall Loss 0.364162    Objective Loss 0.364162                                        LR 0.000003    Time 0.038035    
2023-01-06 15:53:46,225 - Epoch: [191][   30/   37]    Overall Loss 0.363103    Objective Loss 0.363103                                        LR 0.000003    Time 0.029951    
2023-01-06 15:53:46,300 - Epoch: [191][   37/   37]    Overall Loss 0.365846    Objective Loss 0.365846    Top1 80.753138    LR 0.000003    Time 0.026297    
2023-01-06 15:53:46,389 - --- validate (epoch=191)-----------
2023-01-06 15:53:46,389 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:46,611 - Epoch: [191][    5/    5]    Loss 0.370360    Top1 82.729008    
2023-01-06 15:53:46,689 - ==> Top1: 82.729    Loss: 0.370

2023-01-06 15:53:46,689 - ==> Confusion:
[[348  81   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 15:53:46,690 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 15:53:46,690 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:46,695 - 

2023-01-06 15:53:46,695 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:47,017 - Epoch: [192][   10/   37]    Overall Loss 0.364138    Objective Loss 0.364138                                        LR 0.000003    Time 0.032133    
2023-01-06 15:53:47,136 - Epoch: [192][   20/   37]    Overall Loss 0.360871    Objective Loss 0.360871                                        LR 0.000003    Time 0.021978    
2023-01-06 15:53:47,259 - Epoch: [192][   30/   37]    Overall Loss 0.365459    Objective Loss 0.365459                                        LR 0.000003    Time 0.018750    
2023-01-06 15:53:47,327 - Epoch: [192][   37/   37]    Overall Loss 0.365834    Objective Loss 0.365834    Top1 86.820084    LR 0.000003    Time 0.017024    
2023-01-06 15:53:47,391 - --- validate (epoch=192)-----------
2023-01-06 15:53:47,391 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:47,614 - Epoch: [192][    5/    5]    Loss 0.371950    Top1 83.015267    
2023-01-06 15:53:47,673 - ==> Top1: 83.015    Loss: 0.372

2023-01-06 15:53:47,674 - ==> Confusion:
[[343  86   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 15:53:47,675 - ==> Best [Top1: 83.015   Sparsity:0.00   Params: 151104 on epoch: 192]
2023-01-06 15:53:47,675 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:47,681 - 

2023-01-06 15:53:47,681 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:48,033 - Epoch: [193][   10/   37]    Overall Loss 0.365890    Objective Loss 0.365890                                        LR 0.000003    Time 0.035084    
2023-01-06 15:53:48,176 - Epoch: [193][   20/   37]    Overall Loss 0.364872    Objective Loss 0.364872                                        LR 0.000003    Time 0.024664    
2023-01-06 15:53:48,314 - Epoch: [193][   30/   37]    Overall Loss 0.364991    Objective Loss 0.364991                                        LR 0.000003    Time 0.021043    
2023-01-06 15:53:48,390 - Epoch: [193][   37/   37]    Overall Loss 0.365443    Objective Loss 0.365443    Top1 81.799163    LR 0.000003    Time 0.019114    
2023-01-06 15:53:48,466 - --- validate (epoch=193)-----------
2023-01-06 15:53:48,466 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:48,682 - Epoch: [193][    5/    5]    Loss 0.421754    Top1 83.301527    
2023-01-06 15:53:48,740 - ==> Top1: 83.302    Loss: 0.422

2023-01-06 15:53:48,740 - ==> Confusion:
[[339  90   0]
 [ 85 534   0]
 [  0   0   0]]

2023-01-06 15:53:48,741 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 193]
2023-01-06 15:53:48,741 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:48,747 - 

2023-01-06 15:53:48,747 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:49,082 - Epoch: [194][   10/   37]    Overall Loss 0.356714    Objective Loss 0.356714                                        LR 0.000003    Time 0.033356    
2023-01-06 15:53:49,203 - Epoch: [194][   20/   37]    Overall Loss 0.361917    Objective Loss 0.361917                                        LR 0.000003    Time 0.022739    
2023-01-06 15:53:49,334 - Epoch: [194][   30/   37]    Overall Loss 0.361908    Objective Loss 0.361908                                        LR 0.000003    Time 0.019518    
2023-01-06 15:53:49,402 - Epoch: [194][   37/   37]    Overall Loss 0.364952    Objective Loss 0.364952    Top1 84.100418    LR 0.000003    Time 0.017658    
2023-01-06 15:53:49,476 - --- validate (epoch=194)-----------
2023-01-06 15:53:49,476 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:49,694 - Epoch: [194][    5/    5]    Loss 0.419215    Top1 82.347328    
2023-01-06 15:53:49,750 - ==> Top1: 82.347    Loss: 0.419

2023-01-06 15:53:49,750 - ==> Confusion:
[[340  89   0]
 [ 96 523   0]
 [  0   0   0]]

2023-01-06 15:53:49,752 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 193]
2023-01-06 15:53:49,752 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:49,756 - 

2023-01-06 15:53:49,757 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:50,081 - Epoch: [195][   10/   37]    Overall Loss 0.359673    Objective Loss 0.359673                                        LR 0.000003    Time 0.032407    
2023-01-06 15:53:50,205 - Epoch: [195][   20/   37]    Overall Loss 0.363165    Objective Loss 0.363165                                        LR 0.000003    Time 0.022360    
2023-01-06 15:53:50,332 - Epoch: [195][   30/   37]    Overall Loss 0.367089    Objective Loss 0.367089                                        LR 0.000003    Time 0.019125    
2023-01-06 15:53:50,399 - Epoch: [195][   37/   37]    Overall Loss 0.365511    Objective Loss 0.365511    Top1 81.380753    LR 0.000003    Time 0.017325    
2023-01-06 15:53:50,493 - --- validate (epoch=195)-----------
2023-01-06 15:53:50,494 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:50,713 - Epoch: [195][    5/    5]    Loss 0.366436    Top1 83.110687    
2023-01-06 15:53:50,776 - ==> Top1: 83.111    Loss: 0.366

2023-01-06 15:53:50,776 - ==> Confusion:
[[349  80   0]
 [ 97 522   0]
 [  0   0   0]]

2023-01-06 15:53:50,777 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 193]
2023-01-06 15:53:50,778 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:50,783 - 

2023-01-06 15:53:50,783 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:51,117 - Epoch: [196][   10/   37]    Overall Loss 0.367160    Objective Loss 0.367160                                        LR 0.000003    Time 0.033372    
2023-01-06 15:53:51,251 - Epoch: [196][   20/   37]    Overall Loss 0.365747    Objective Loss 0.365747                                        LR 0.000003    Time 0.023365    
2023-01-06 15:53:51,387 - Epoch: [196][   30/   37]    Overall Loss 0.363496    Objective Loss 0.363496                                        LR 0.000003    Time 0.020088    
2023-01-06 15:53:51,463 - Epoch: [196][   37/   37]    Overall Loss 0.365526    Objective Loss 0.365526    Top1 82.845188    LR 0.000003    Time 0.018321    
2023-01-06 15:53:51,547 - --- validate (epoch=196)-----------
2023-01-06 15:53:51,547 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:51,766 - Epoch: [196][    5/    5]    Loss 0.383809    Top1 82.156489    
2023-01-06 15:53:51,832 - ==> Top1: 82.156    Loss: 0.384

2023-01-06 15:53:51,833 - ==> Confusion:
[[309 120   0]
 [ 67 552   0]
 [  0   0   0]]

2023-01-06 15:53:51,834 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 193]
2023-01-06 15:53:51,834 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:51,839 - 

2023-01-06 15:53:51,839 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:52,171 - Epoch: [197][   10/   37]    Overall Loss 0.370638    Objective Loss 0.370638                                        LR 0.000003    Time 0.033163    
2023-01-06 15:53:52,296 - Epoch: [197][   20/   37]    Overall Loss 0.373011    Objective Loss 0.373011                                        LR 0.000003    Time 0.022794    
2023-01-06 15:53:52,422 - Epoch: [197][   30/   37]    Overall Loss 0.364246    Objective Loss 0.364246                                        LR 0.000003    Time 0.019369    
2023-01-06 15:53:52,496 - Epoch: [197][   37/   37]    Overall Loss 0.364877    Objective Loss 0.364877    Top1 82.635983    LR 0.000003    Time 0.017719    
2023-01-06 15:53:52,577 - --- validate (epoch=197)-----------
2023-01-06 15:53:52,577 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:52,794 - Epoch: [197][    5/    5]    Loss 0.376589    Top1 83.206107    
2023-01-06 15:53:52,860 - ==> Top1: 83.206    Loss: 0.377

2023-01-06 15:53:52,860 - ==> Confusion:
[[338  91   0]
 [ 85 534   0]
 [  0   0   0]]

2023-01-06 15:53:52,861 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 193]
2023-01-06 15:53:52,861 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:52,866 - 

2023-01-06 15:53:52,866 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:53,200 - Epoch: [198][   10/   37]    Overall Loss 0.362765    Objective Loss 0.362765                                        LR 0.000003    Time 0.033338    
2023-01-06 15:53:53,328 - Epoch: [198][   20/   37]    Overall Loss 0.359922    Objective Loss 0.359922                                        LR 0.000003    Time 0.023054    
2023-01-06 15:53:53,469 - Epoch: [198][   30/   37]    Overall Loss 0.362335    Objective Loss 0.362335                                        LR 0.000003    Time 0.020059    
2023-01-06 15:53:53,547 - Epoch: [198][   37/   37]    Overall Loss 0.365018    Objective Loss 0.365018    Top1 84.100418    LR 0.000003    Time 0.018341    
2023-01-06 15:53:53,618 - --- validate (epoch=198)-----------
2023-01-06 15:53:53,618 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:53,839 - Epoch: [198][    5/    5]    Loss 0.365080    Top1 83.301527    
2023-01-06 15:53:53,910 - ==> Top1: 83.302    Loss: 0.365

2023-01-06 15:53:53,910 - ==> Confusion:
[[351  78   0]
 [ 97 522   0]
 [  0   0   0]]

2023-01-06 15:53:53,911 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 198]
2023-01-06 15:53:53,911 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:53,917 - 

2023-01-06 15:53:53,918 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 15:53:54,240 - Epoch: [199][   10/   37]    Overall Loss 0.369799    Objective Loss 0.369799                                        LR 0.000003    Time 0.032215    
2023-01-06 15:53:54,355 - Epoch: [199][   20/   37]    Overall Loss 0.361999    Objective Loss 0.361999                                        LR 0.000003    Time 0.021811    
2023-01-06 15:53:54,477 - Epoch: [199][   30/   37]    Overall Loss 0.365860    Objective Loss 0.365860                                        LR 0.000003    Time 0.018609    
2023-01-06 15:53:54,553 - Epoch: [199][   37/   37]    Overall Loss 0.364885    Objective Loss 0.364885    Top1 83.682008    LR 0.000003    Time 0.017131    
2023-01-06 15:53:54,631 - --- validate (epoch=199)-----------
2023-01-06 15:53:54,631 - 1048 samples (256 per mini-batch)
2023-01-06 15:53:54,861 - Epoch: [199][    5/    5]    Loss 0.375867    Top1 82.156489    
2023-01-06 15:53:54,924 - ==> Top1: 82.156    Loss: 0.376

2023-01-06 15:53:54,924 - ==> Confusion:
[[323 106   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 15:53:54,925 - ==> Best [Top1: 83.302   Sparsity:0.00   Params: 151104 on epoch: 198]
2023-01-06 15:53:54,925 - Saving checkpoint to: logs/2023.01.06-155014/qat_checkpoint.pth.tar
2023-01-06 15:53:54,930 - --- test ---------------------
2023-01-06 15:53:54,930 - 1317 samples (256 per mini-batch)
2023-01-06 15:53:55,158 - Test: [    6/    6]    Loss 0.429193    Top1 80.941534    
2023-01-06 15:53:55,235 - ==> Top1: 80.942    Loss: 0.429

2023-01-06 15:53:55,236 - ==> Confusion:
[[428 133   0]
 [118 638   0]
 [  0   0   0]]

2023-01-06 15:53:55,246 - 
2023-01-06 15:53:55,247 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-155014/2023.01.06-155014.log
