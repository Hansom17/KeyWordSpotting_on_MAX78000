2023-01-06 16:51:56,429 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-165156/2023.01.06-165156.log
2023-01-06 16:51:58,479 - => loading checkpoint qat_best.pth.tar
2023-01-06 16:51:58,481 - => Checkpoint contents:
+----------------------+-------------+----------------+
| Key                  | Type        | Value          |
|----------------------+-------------+----------------|
| arch                 | str         | ai85kws20netv2 |
| compression_sched    | dict        |                |
| epoch                | int         | 146            |
| extras               | dict        |                |
| optimizer_state_dict | dict        |                |
| optimizer_type       | type        | Adam           |
| state_dict           | OrderedDict |                |
+----------------------+-------------+----------------+

2023-01-06 16:51:58,482 - => Checkpoint['extras'] contents:
+--------------+--------+----------+
| Key          | Type   |    Value |
|--------------+--------+----------|
| best_epoch   | int    | 146      |
| best_mAP     | int    |   0      |
| best_top1    | float  |  85.4962 |
| current_mAP  | int    |   0      |
| current_top1 | float  |  85.4962 |
+--------------+--------+----------+

2023-01-06 16:51:58,482 - Loaded compression schedule from checkpoint (epoch 146)
2023-01-06 16:51:58,484 - => loaded 'state_dict' from checkpoint 'qat_best.pth.tar'
2023-01-06 16:51:58,491 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 16:51:58,491 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 16:52:51,254 - Dataset sizes:
	training=62882
	validation=6986
	test=13117
2023-01-06 16:52:51,254 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 16:52:51,257 - 

2023-01-06 16:52:51,257 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:52:52,302 - Epoch: [0][   10/  246]    Overall Loss 16.439985    Objective Loss 16.439985                                        LR 0.000060    Time 0.104435    
2023-01-06 16:52:52,486 - Epoch: [0][   20/  246]    Overall Loss 11.905280    Objective Loss 11.905280                                        LR 0.000060    Time 0.061357    
2023-01-06 16:52:52,663 - Epoch: [0][   30/  246]    Overall Loss 9.177073    Objective Loss 9.177073                                        LR 0.000060    Time 0.046816    
2023-01-06 16:52:52,852 - Epoch: [0][   40/  246]    Overall Loss 7.459474    Objective Loss 7.459474                                        LR 0.000060    Time 0.039822    
2023-01-06 16:52:53,038 - Epoch: [0][   50/  246]    Overall Loss 6.281006    Objective Loss 6.281006                                        LR 0.000060    Time 0.035577    
2023-01-06 16:52:53,227 - Epoch: [0][   60/  246]    Overall Loss 5.436760    Objective Loss 5.436760                                        LR 0.000060    Time 0.032793    
2023-01-06 16:52:53,434 - Epoch: [0][   70/  246]    Overall Loss 4.806436    Objective Loss 4.806436                                        LR 0.000060    Time 0.031061    
2023-01-06 16:52:53,650 - Epoch: [0][   80/  246]    Overall Loss 4.316269    Objective Loss 4.316269                                        LR 0.000060    Time 0.029870    
2023-01-06 16:52:53,865 - Epoch: [0][   90/  246]    Overall Loss 3.920920    Objective Loss 3.920920                                        LR 0.000060    Time 0.028931    
2023-01-06 16:52:54,045 - Epoch: [0][  100/  246]    Overall Loss 3.597920    Objective Loss 3.597920                                        LR 0.000060    Time 0.027841    
2023-01-06 16:52:54,230 - Epoch: [0][  110/  246]    Overall Loss 3.338558    Objective Loss 3.338558                                        LR 0.000060    Time 0.026987    
2023-01-06 16:52:54,406 - Epoch: [0][  120/  246]    Overall Loss 3.115601    Objective Loss 3.115601                                        LR 0.000060    Time 0.026201    
2023-01-06 16:52:54,586 - Epoch: [0][  130/  246]    Overall Loss 2.922833    Objective Loss 2.922833                                        LR 0.000060    Time 0.025564    
2023-01-06 16:52:54,760 - Epoch: [0][  140/  246]    Overall Loss 2.752100    Objective Loss 2.752100                                        LR 0.000060    Time 0.024981    
2023-01-06 16:52:54,942 - Epoch: [0][  150/  246]    Overall Loss 2.604313    Objective Loss 2.604313                                        LR 0.000060    Time 0.024524    
2023-01-06 16:52:55,130 - Epoch: [0][  160/  246]    Overall Loss 2.474697    Objective Loss 2.474697                                        LR 0.000060    Time 0.024167    
2023-01-06 16:52:55,315 - Epoch: [0][  170/  246]    Overall Loss 2.361377    Objective Loss 2.361377                                        LR 0.000060    Time 0.023831    
2023-01-06 16:52:55,501 - Epoch: [0][  180/  246]    Overall Loss 2.260877    Objective Loss 2.260877                                        LR 0.000060    Time 0.023529    
2023-01-06 16:52:55,675 - Epoch: [0][  190/  246]    Overall Loss 2.169181    Objective Loss 2.169181                                        LR 0.000060    Time 0.023205    
2023-01-06 16:52:55,858 - Epoch: [0][  200/  246]    Overall Loss 2.085667    Objective Loss 2.085667                                        LR 0.000060    Time 0.022960    
2023-01-06 16:52:56,064 - Epoch: [0][  210/  246]    Overall Loss 2.012235    Objective Loss 2.012235                                        LR 0.000060    Time 0.022847    
2023-01-06 16:52:56,268 - Epoch: [0][  220/  246]    Overall Loss 1.944348    Objective Loss 1.944348                                        LR 0.000060    Time 0.022731    
2023-01-06 16:52:56,476 - Epoch: [0][  230/  246]    Overall Loss 1.881642    Objective Loss 1.881642                                        LR 0.000060    Time 0.022649    
2023-01-06 16:52:56,689 - Epoch: [0][  240/  246]    Overall Loss 1.824223    Objective Loss 1.824223                                        LR 0.000060    Time 0.022589    
2023-01-06 16:52:56,779 - Epoch: [0][  246/  246]    Overall Loss 1.791694    Objective Loss 1.791694    Top1 84.449761    LR 0.000060    Time 0.022403    
2023-01-06 16:52:56,945 - --- validate (epoch=0)-----------
2023-01-06 16:52:56,946 - 6986 samples (256 per mini-batch)
2023-01-06 16:52:57,369 - Epoch: [0][   10/   28]    Loss 0.485040    Top1 86.367188    
2023-01-06 16:52:57,482 - Epoch: [0][   20/   28]    Loss 0.513953    Top1 85.097656    
2023-01-06 16:52:57,549 - Epoch: [0][   28/   28]    Loss 0.509026    Top1 85.098769    
2023-01-06 16:52:57,699 - ==> Top1: 85.099    Loss: 0.509

2023-01-06 16:52:57,699 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:52:57,700 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 155168 on epoch: 0]
2023-01-06 16:52:57,700 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:52:57,707 - 

2023-01-06 16:52:57,707 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:52:58,256 - Epoch: [1][   10/  246]    Overall Loss 0.517177    Objective Loss 0.517177                                        LR 0.000060    Time 0.054855    
2023-01-06 16:52:58,440 - Epoch: [1][   20/  246]    Overall Loss 0.509006    Objective Loss 0.509006                                        LR 0.000060    Time 0.036558    
2023-01-06 16:52:58,620 - Epoch: [1][   30/  246]    Overall Loss 0.509455    Objective Loss 0.509455                                        LR 0.000060    Time 0.030363    
2023-01-06 16:52:58,787 - Epoch: [1][   40/  246]    Overall Loss 0.511778    Objective Loss 0.511778                                        LR 0.000060    Time 0.026944    
2023-01-06 16:52:58,956 - Epoch: [1][   50/  246]    Overall Loss 0.513901    Objective Loss 0.513901                                        LR 0.000060    Time 0.024925    
2023-01-06 16:52:59,147 - Epoch: [1][   60/  246]    Overall Loss 0.516593    Objective Loss 0.516593                                        LR 0.000060    Time 0.023959    
2023-01-06 16:52:59,345 - Epoch: [1][   70/  246]    Overall Loss 0.514977    Objective Loss 0.514977                                        LR 0.000060    Time 0.023349    
2023-01-06 16:52:59,528 - Epoch: [1][   80/  246]    Overall Loss 0.512604    Objective Loss 0.512604                                        LR 0.000060    Time 0.022718    
2023-01-06 16:52:59,719 - Epoch: [1][   90/  246]    Overall Loss 0.509751    Objective Loss 0.509751                                        LR 0.000060    Time 0.022308    
2023-01-06 16:52:59,904 - Epoch: [1][  100/  246]    Overall Loss 0.509974    Objective Loss 0.509974                                        LR 0.000060    Time 0.021930    
2023-01-06 16:53:00,106 - Epoch: [1][  110/  246]    Overall Loss 0.511016    Objective Loss 0.511016                                        LR 0.000060    Time 0.021767    
2023-01-06 16:53:00,298 - Epoch: [1][  120/  246]    Overall Loss 0.510952    Objective Loss 0.510952                                        LR 0.000060    Time 0.021551    
2023-01-06 16:53:00,500 - Epoch: [1][  130/  246]    Overall Loss 0.512571    Objective Loss 0.512571                                        LR 0.000060    Time 0.021441    
2023-01-06 16:53:00,697 - Epoch: [1][  140/  246]    Overall Loss 0.511957    Objective Loss 0.511957                                        LR 0.000060    Time 0.021321    
2023-01-06 16:53:00,898 - Epoch: [1][  150/  246]    Overall Loss 0.510427    Objective Loss 0.510427                                        LR 0.000060    Time 0.021237    
2023-01-06 16:53:01,090 - Epoch: [1][  160/  246]    Overall Loss 0.510673    Objective Loss 0.510673                                        LR 0.000060    Time 0.021107    
2023-01-06 16:53:01,292 - Epoch: [1][  170/  246]    Overall Loss 0.509914    Objective Loss 0.509914                                        LR 0.000060    Time 0.021052    
2023-01-06 16:53:01,477 - Epoch: [1][  180/  246]    Overall Loss 0.510459    Objective Loss 0.510459                                        LR 0.000060    Time 0.020907    
2023-01-06 16:53:01,646 - Epoch: [1][  190/  246]    Overall Loss 0.508623    Objective Loss 0.508623                                        LR 0.000060    Time 0.020692    
2023-01-06 16:53:01,817 - Epoch: [1][  200/  246]    Overall Loss 0.506559    Objective Loss 0.506559                                        LR 0.000060    Time 0.020513    
2023-01-06 16:53:01,991 - Epoch: [1][  210/  246]    Overall Loss 0.507138    Objective Loss 0.507138                                        LR 0.000060    Time 0.020363    
2023-01-06 16:53:02,157 - Epoch: [1][  220/  246]    Overall Loss 0.506773    Objective Loss 0.506773                                        LR 0.000060    Time 0.020192    
2023-01-06 16:53:02,330 - Epoch: [1][  230/  246]    Overall Loss 0.505900    Objective Loss 0.505900                                        LR 0.000060    Time 0.020064    
2023-01-06 16:53:02,515 - Epoch: [1][  240/  246]    Overall Loss 0.505159    Objective Loss 0.505159                                        LR 0.000060    Time 0.019996    
2023-01-06 16:53:02,610 - Epoch: [1][  246/  246]    Overall Loss 0.504550    Objective Loss 0.504550    Top1 85.645933    LR 0.000060    Time 0.019893    
2023-01-06 16:53:02,747 - --- validate (epoch=1)-----------
2023-01-06 16:53:02,747 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:03,182 - Epoch: [1][   10/   28]    Loss 0.477504    Top1 85.468750    
2023-01-06 16:53:03,299 - Epoch: [1][   20/   28]    Loss 0.486223    Top1 85.019531    
2023-01-06 16:53:03,366 - Epoch: [1][   28/   28]    Loss 0.483632    Top1 85.098769    
2023-01-06 16:53:03,499 - ==> Top1: 85.099    Loss: 0.484

2023-01-06 16:53:03,499 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:53:03,500 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 155168 on epoch: 1]
2023-01-06 16:53:03,500 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:03,507 - 

2023-01-06 16:53:03,507 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:04,186 - Epoch: [2][   10/  246]    Overall Loss 0.461609    Objective Loss 0.461609                                        LR 0.000060    Time 0.067782    
2023-01-06 16:53:04,368 - Epoch: [2][   20/  246]    Overall Loss 0.468277    Objective Loss 0.468277                                        LR 0.000060    Time 0.042961    
2023-01-06 16:53:04,551 - Epoch: [2][   30/  246]    Overall Loss 0.476659    Objective Loss 0.476659                                        LR 0.000060    Time 0.034745    
2023-01-06 16:53:04,735 - Epoch: [2][   40/  246]    Overall Loss 0.482589    Objective Loss 0.482589                                        LR 0.000060    Time 0.030636    
2023-01-06 16:53:04,922 - Epoch: [2][   50/  246]    Overall Loss 0.484443    Objective Loss 0.484443                                        LR 0.000060    Time 0.028254    
2023-01-06 16:53:05,105 - Epoch: [2][   60/  246]    Overall Loss 0.481507    Objective Loss 0.481507                                        LR 0.000060    Time 0.026587    
2023-01-06 16:53:05,287 - Epoch: [2][   70/  246]    Overall Loss 0.481848    Objective Loss 0.481848                                        LR 0.000060    Time 0.025391    
2023-01-06 16:53:05,452 - Epoch: [2][   80/  246]    Overall Loss 0.479822    Objective Loss 0.479822                                        LR 0.000060    Time 0.024270    
2023-01-06 16:53:05,616 - Epoch: [2][   90/  246]    Overall Loss 0.480481    Objective Loss 0.480481                                        LR 0.000060    Time 0.023395    
2023-01-06 16:53:05,780 - Epoch: [2][  100/  246]    Overall Loss 0.477875    Objective Loss 0.477875                                        LR 0.000060    Time 0.022675    
2023-01-06 16:53:05,944 - Epoch: [2][  110/  246]    Overall Loss 0.478950    Objective Loss 0.478950                                        LR 0.000060    Time 0.022108    
2023-01-06 16:53:06,107 - Epoch: [2][  120/  246]    Overall Loss 0.478174    Objective Loss 0.478174                                        LR 0.000060    Time 0.021612    
2023-01-06 16:53:06,277 - Epoch: [2][  130/  246]    Overall Loss 0.476864    Objective Loss 0.476864                                        LR 0.000060    Time 0.021255    
2023-01-06 16:53:06,466 - Epoch: [2][  140/  246]    Overall Loss 0.475767    Objective Loss 0.475767                                        LR 0.000060    Time 0.021083    
2023-01-06 16:53:06,654 - Epoch: [2][  150/  246]    Overall Loss 0.474283    Objective Loss 0.474283                                        LR 0.000060    Time 0.020926    
2023-01-06 16:53:06,837 - Epoch: [2][  160/  246]    Overall Loss 0.473262    Objective Loss 0.473262                                        LR 0.000060    Time 0.020761    
2023-01-06 16:53:07,001 - Epoch: [2][  170/  246]    Overall Loss 0.472334    Objective Loss 0.472334                                        LR 0.000060    Time 0.020506    
2023-01-06 16:53:07,167 - Epoch: [2][  180/  246]    Overall Loss 0.471754    Objective Loss 0.471754                                        LR 0.000060    Time 0.020284    
2023-01-06 16:53:07,333 - Epoch: [2][  190/  246]    Overall Loss 0.471626    Objective Loss 0.471626                                        LR 0.000060    Time 0.020088    
2023-01-06 16:53:07,507 - Epoch: [2][  200/  246]    Overall Loss 0.471254    Objective Loss 0.471254                                        LR 0.000060    Time 0.019955    
2023-01-06 16:53:07,670 - Epoch: [2][  210/  246]    Overall Loss 0.472440    Objective Loss 0.472440                                        LR 0.000060    Time 0.019778    
2023-01-06 16:53:07,870 - Epoch: [2][  220/  246]    Overall Loss 0.471600    Objective Loss 0.471600                                        LR 0.000060    Time 0.019785    
2023-01-06 16:53:08,087 - Epoch: [2][  230/  246]    Overall Loss 0.470535    Objective Loss 0.470535                                        LR 0.000060    Time 0.019869    
2023-01-06 16:53:08,315 - Epoch: [2][  240/  246]    Overall Loss 0.470277    Objective Loss 0.470277                                        LR 0.000060    Time 0.019989    
2023-01-06 16:53:08,426 - Epoch: [2][  246/  246]    Overall Loss 0.469474    Objective Loss 0.469474    Top1 86.124402    LR 0.000060    Time 0.019951    
2023-01-06 16:53:08,565 - --- validate (epoch=2)-----------
2023-01-06 16:53:08,566 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:09,018 - Epoch: [2][   10/   28]    Loss 0.469254    Top1 84.726562    
2023-01-06 16:53:09,134 - Epoch: [2][   20/   28]    Loss 0.459647    Top1 84.941406    
2023-01-06 16:53:09,203 - Epoch: [2][   28/   28]    Loss 0.458043    Top1 85.084455    
2023-01-06 16:53:09,357 - ==> Top1: 85.084    Loss: 0.458

2023-01-06 16:53:09,357 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    1 5944]]

2023-01-06 16:53:09,358 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 155168 on epoch: 1]
2023-01-06 16:53:09,358 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:09,364 - 

2023-01-06 16:53:09,365 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:09,933 - Epoch: [3][   10/  246]    Overall Loss 0.452357    Objective Loss 0.452357                                        LR 0.000060    Time 0.056743    
2023-01-06 16:53:10,113 - Epoch: [3][   20/  246]    Overall Loss 0.463599    Objective Loss 0.463599                                        LR 0.000060    Time 0.037355    
2023-01-06 16:53:10,293 - Epoch: [3][   30/  246]    Overall Loss 0.463246    Objective Loss 0.463246                                        LR 0.000060    Time 0.030904    
2023-01-06 16:53:10,474 - Epoch: [3][   40/  246]    Overall Loss 0.459343    Objective Loss 0.459343                                        LR 0.000060    Time 0.027690    
2023-01-06 16:53:10,655 - Epoch: [3][   50/  246]    Overall Loss 0.458241    Objective Loss 0.458241                                        LR 0.000060    Time 0.025772    
2023-01-06 16:53:10,835 - Epoch: [3][   60/  246]    Overall Loss 0.458013    Objective Loss 0.458013                                        LR 0.000060    Time 0.024467    
2023-01-06 16:53:11,017 - Epoch: [3][   70/  246]    Overall Loss 0.457415    Objective Loss 0.457415                                        LR 0.000060    Time 0.023560    
2023-01-06 16:53:11,205 - Epoch: [3][   80/  246]    Overall Loss 0.455768    Objective Loss 0.455768                                        LR 0.000060    Time 0.022969    
2023-01-06 16:53:11,404 - Epoch: [3][   90/  246]    Overall Loss 0.455245    Objective Loss 0.455245                                        LR 0.000060    Time 0.022616    
2023-01-06 16:53:11,601 - Epoch: [3][  100/  246]    Overall Loss 0.454084    Objective Loss 0.454084                                        LR 0.000060    Time 0.022323    
2023-01-06 16:53:11,799 - Epoch: [3][  110/  246]    Overall Loss 0.455540    Objective Loss 0.455540                                        LR 0.000060    Time 0.022089    
2023-01-06 16:53:11,986 - Epoch: [3][  120/  246]    Overall Loss 0.456115    Objective Loss 0.456115                                        LR 0.000060    Time 0.021804    
2023-01-06 16:53:12,169 - Epoch: [3][  130/  246]    Overall Loss 0.455381    Objective Loss 0.455381                                        LR 0.000060    Time 0.021531    
2023-01-06 16:53:12,352 - Epoch: [3][  140/  246]    Overall Loss 0.455485    Objective Loss 0.455485                                        LR 0.000060    Time 0.021289    
2023-01-06 16:53:12,538 - Epoch: [3][  150/  246]    Overall Loss 0.452936    Objective Loss 0.452936                                        LR 0.000060    Time 0.021109    
2023-01-06 16:53:12,733 - Epoch: [3][  160/  246]    Overall Loss 0.453033    Objective Loss 0.453033                                        LR 0.000060    Time 0.021006    
2023-01-06 16:53:12,921 - Epoch: [3][  170/  246]    Overall Loss 0.452739    Objective Loss 0.452739                                        LR 0.000060    Time 0.020872    
2023-01-06 16:53:13,113 - Epoch: [3][  180/  246]    Overall Loss 0.451980    Objective Loss 0.451980                                        LR 0.000060    Time 0.020780    
2023-01-06 16:53:13,305 - Epoch: [3][  190/  246]    Overall Loss 0.453815    Objective Loss 0.453815                                        LR 0.000060    Time 0.020693    
2023-01-06 16:53:13,501 - Epoch: [3][  200/  246]    Overall Loss 0.453817    Objective Loss 0.453817                                        LR 0.000060    Time 0.020639    
2023-01-06 16:53:13,693 - Epoch: [3][  210/  246]    Overall Loss 0.454127    Objective Loss 0.454127                                        LR 0.000060    Time 0.020567    
2023-01-06 16:53:13,882 - Epoch: [3][  220/  246]    Overall Loss 0.453753    Objective Loss 0.453753                                        LR 0.000060    Time 0.020491    
2023-01-06 16:53:14,068 - Epoch: [3][  230/  246]    Overall Loss 0.452895    Objective Loss 0.452895                                        LR 0.000060    Time 0.020405    
2023-01-06 16:53:14,266 - Epoch: [3][  240/  246]    Overall Loss 0.452898    Objective Loss 0.452898                                        LR 0.000060    Time 0.020378    
2023-01-06 16:53:14,360 - Epoch: [3][  246/  246]    Overall Loss 0.452475    Objective Loss 0.452475    Top1 84.449761    LR 0.000060    Time 0.020265    
2023-01-06 16:53:14,493 - --- validate (epoch=3)-----------
2023-01-06 16:53:14,493 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:15,253 - Epoch: [3][   10/   28]    Loss 0.429162    Top1 85.820312    
2023-01-06 16:53:15,363 - Epoch: [3][   20/   28]    Loss 0.441022    Top1 85.312500    
2023-01-06 16:53:15,432 - Epoch: [3][   28/   28]    Loss 0.442478    Top1 85.098769    
2023-01-06 16:53:15,603 - ==> Top1: 85.099    Loss: 0.442

2023-01-06 16:53:15,603 - ==> Confusion:
[[   1    0  438]
 [   0    0  602]
 [   1    0 5944]]

2023-01-06 16:53:15,604 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 155168 on epoch: 3]
2023-01-06 16:53:15,605 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:15,615 - 

2023-01-06 16:53:15,615 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:16,234 - Epoch: [4][   10/  246]    Overall Loss 0.452791    Objective Loss 0.452791                                        LR 0.000060    Time 0.061816    
2023-01-06 16:53:16,447 - Epoch: [4][   20/  246]    Overall Loss 0.441433    Objective Loss 0.441433                                        LR 0.000060    Time 0.041534    
2023-01-06 16:53:16,652 - Epoch: [4][   30/  246]    Overall Loss 0.445488    Objective Loss 0.445488                                        LR 0.000060    Time 0.034489    
2023-01-06 16:53:16,858 - Epoch: [4][   40/  246]    Overall Loss 0.444922    Objective Loss 0.444922                                        LR 0.000060    Time 0.031018    
2023-01-06 16:53:17,065 - Epoch: [4][   50/  246]    Overall Loss 0.444924    Objective Loss 0.444924                                        LR 0.000060    Time 0.028932    
2023-01-06 16:53:17,253 - Epoch: [4][   60/  246]    Overall Loss 0.443655    Objective Loss 0.443655                                        LR 0.000060    Time 0.027245    
2023-01-06 16:53:17,418 - Epoch: [4][   70/  246]    Overall Loss 0.442575    Objective Loss 0.442575                                        LR 0.000060    Time 0.025704    
2023-01-06 16:53:17,583 - Epoch: [4][   80/  246]    Overall Loss 0.445095    Objective Loss 0.445095                                        LR 0.000060    Time 0.024544    
2023-01-06 16:53:17,746 - Epoch: [4][   90/  246]    Overall Loss 0.443573    Objective Loss 0.443573                                        LR 0.000060    Time 0.023633    
2023-01-06 16:53:17,910 - Epoch: [4][  100/  246]    Overall Loss 0.442951    Objective Loss 0.442951                                        LR 0.000060    Time 0.022908    
2023-01-06 16:53:18,075 - Epoch: [4][  110/  246]    Overall Loss 0.442938    Objective Loss 0.442938                                        LR 0.000060    Time 0.022318    
2023-01-06 16:53:18,239 - Epoch: [4][  120/  246]    Overall Loss 0.439772    Objective Loss 0.439772                                        LR 0.000060    Time 0.021825    
2023-01-06 16:53:18,403 - Epoch: [4][  130/  246]    Overall Loss 0.441424    Objective Loss 0.441424                                        LR 0.000060    Time 0.021405    
2023-01-06 16:53:18,568 - Epoch: [4][  140/  246]    Overall Loss 0.441593    Objective Loss 0.441593                                        LR 0.000060    Time 0.021051    
2023-01-06 16:53:18,732 - Epoch: [4][  150/  246]    Overall Loss 0.442113    Objective Loss 0.442113                                        LR 0.000060    Time 0.020737    
2023-01-06 16:53:18,895 - Epoch: [4][  160/  246]    Overall Loss 0.443491    Objective Loss 0.443491                                        LR 0.000060    Time 0.020460    
2023-01-06 16:53:19,069 - Epoch: [4][  170/  246]    Overall Loss 0.442341    Objective Loss 0.442341                                        LR 0.000060    Time 0.020278    
2023-01-06 16:53:19,250 - Epoch: [4][  180/  246]    Overall Loss 0.442867    Objective Loss 0.442867                                        LR 0.000060    Time 0.020155    
2023-01-06 16:53:19,438 - Epoch: [4][  190/  246]    Overall Loss 0.442164    Objective Loss 0.442164                                        LR 0.000060    Time 0.020086    
2023-01-06 16:53:19,623 - Epoch: [4][  200/  246]    Overall Loss 0.440619    Objective Loss 0.440619                                        LR 0.000060    Time 0.020002    
2023-01-06 16:53:19,807 - Epoch: [4][  210/  246]    Overall Loss 0.440694    Objective Loss 0.440694                                        LR 0.000060    Time 0.019927    
2023-01-06 16:53:19,989 - Epoch: [4][  220/  246]    Overall Loss 0.441098    Objective Loss 0.441098                                        LR 0.000060    Time 0.019846    
2023-01-06 16:53:20,175 - Epoch: [4][  230/  246]    Overall Loss 0.441552    Objective Loss 0.441552                                        LR 0.000060    Time 0.019788    
2023-01-06 16:53:20,367 - Epoch: [4][  240/  246]    Overall Loss 0.441981    Objective Loss 0.441981                                        LR 0.000060    Time 0.019763    
2023-01-06 16:53:20,461 - Epoch: [4][  246/  246]    Overall Loss 0.442106    Objective Loss 0.442106    Top1 83.732057    LR 0.000060    Time 0.019663    
2023-01-06 16:53:20,614 - --- validate (epoch=4)-----------
2023-01-06 16:53:20,614 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:21,071 - Epoch: [4][   10/   28]    Loss 0.443372    Top1 84.726562    
2023-01-06 16:53:21,184 - Epoch: [4][   20/   28]    Loss 0.442245    Top1 84.804688    
2023-01-06 16:53:21,254 - Epoch: [4][   28/   28]    Loss 0.432889    Top1 85.170341    
2023-01-06 16:53:21,415 - ==> Top1: 85.170    Loss: 0.433

2023-01-06 16:53:21,415 - ==> Confusion:
[[  16    0  423]
 [   0    0  602]
 [  11    0 5934]]

2023-01-06 16:53:21,416 - ==> Best [Top1: 85.170   Sparsity:0.00   Params: 155168 on epoch: 4]
2023-01-06 16:53:21,416 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:21,424 - 

2023-01-06 16:53:21,424 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:22,118 - Epoch: [5][   10/  246]    Overall Loss 0.445443    Objective Loss 0.445443                                        LR 0.000060    Time 0.069344    
2023-01-06 16:53:22,298 - Epoch: [5][   20/  246]    Overall Loss 0.432985    Objective Loss 0.432985                                        LR 0.000060    Time 0.043630    
2023-01-06 16:53:22,478 - Epoch: [5][   30/  246]    Overall Loss 0.433459    Objective Loss 0.433459                                        LR 0.000060    Time 0.035094    
2023-01-06 16:53:22,660 - Epoch: [5][   40/  246]    Overall Loss 0.433250    Objective Loss 0.433250                                        LR 0.000060    Time 0.030862    
2023-01-06 16:53:22,842 - Epoch: [5][   50/  246]    Overall Loss 0.433893    Objective Loss 0.433893                                        LR 0.000060    Time 0.028315    
2023-01-06 16:53:23,027 - Epoch: [5][   60/  246]    Overall Loss 0.434260    Objective Loss 0.434260                                        LR 0.000060    Time 0.026671    
2023-01-06 16:53:23,213 - Epoch: [5][   70/  246]    Overall Loss 0.437003    Objective Loss 0.437003                                        LR 0.000060    Time 0.025515    
2023-01-06 16:53:23,385 - Epoch: [5][   80/  246]    Overall Loss 0.439840    Objective Loss 0.439840                                        LR 0.000060    Time 0.024470    
2023-01-06 16:53:23,559 - Epoch: [5][   90/  246]    Overall Loss 0.439338    Objective Loss 0.439338                                        LR 0.000060    Time 0.023684    
2023-01-06 16:53:23,736 - Epoch: [5][  100/  246]    Overall Loss 0.437247    Objective Loss 0.437247                                        LR 0.000060    Time 0.023078    
2023-01-06 16:53:23,918 - Epoch: [5][  110/  246]    Overall Loss 0.437761    Objective Loss 0.437761                                        LR 0.000060    Time 0.022635    
2023-01-06 16:53:24,088 - Epoch: [5][  120/  246]    Overall Loss 0.436790    Objective Loss 0.436790                                        LR 0.000060    Time 0.022158    
2023-01-06 16:53:24,269 - Epoch: [5][  130/  246]    Overall Loss 0.436445    Objective Loss 0.436445                                        LR 0.000060    Time 0.021846    
2023-01-06 16:53:24,448 - Epoch: [5][  140/  246]    Overall Loss 0.435877    Objective Loss 0.435877                                        LR 0.000060    Time 0.021559    
2023-01-06 16:53:24,623 - Epoch: [5][  150/  246]    Overall Loss 0.434303    Objective Loss 0.434303                                        LR 0.000060    Time 0.021288    
2023-01-06 16:53:24,802 - Epoch: [5][  160/  246]    Overall Loss 0.435407    Objective Loss 0.435407                                        LR 0.000060    Time 0.021076    
2023-01-06 16:53:24,979 - Epoch: [5][  170/  246]    Overall Loss 0.436333    Objective Loss 0.436333                                        LR 0.000060    Time 0.020872    
2023-01-06 16:53:25,153 - Epoch: [5][  180/  246]    Overall Loss 0.435094    Objective Loss 0.435094                                        LR 0.000060    Time 0.020681    
2023-01-06 16:53:25,335 - Epoch: [5][  190/  246]    Overall Loss 0.434507    Objective Loss 0.434507                                        LR 0.000060    Time 0.020548    
2023-01-06 16:53:25,517 - Epoch: [5][  200/  246]    Overall Loss 0.433868    Objective Loss 0.433868                                        LR 0.000060    Time 0.020431    
2023-01-06 16:53:25,697 - Epoch: [5][  210/  246]    Overall Loss 0.434829    Objective Loss 0.434829                                        LR 0.000060    Time 0.020312    
2023-01-06 16:53:25,872 - Epoch: [5][  220/  246]    Overall Loss 0.434654    Objective Loss 0.434654                                        LR 0.000060    Time 0.020180    
2023-01-06 16:53:26,047 - Epoch: [5][  230/  246]    Overall Loss 0.435116    Objective Loss 0.435116                                        LR 0.000060    Time 0.020066    
2023-01-06 16:53:26,240 - Epoch: [5][  240/  246]    Overall Loss 0.434899    Objective Loss 0.434899                                        LR 0.000060    Time 0.020029    
2023-01-06 16:53:26,336 - Epoch: [5][  246/  246]    Overall Loss 0.434830    Objective Loss 0.434830    Top1 82.775120    LR 0.000060    Time 0.019934    
2023-01-06 16:53:26,513 - --- validate (epoch=5)-----------
2023-01-06 16:53:26,513 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:26,958 - Epoch: [5][   10/   28]    Loss 0.438376    Top1 84.765625    
2023-01-06 16:53:27,071 - Epoch: [5][   20/   28]    Loss 0.430497    Top1 85.058594    
2023-01-06 16:53:27,140 - Epoch: [5][   28/   28]    Loss 0.426285    Top1 85.198969    
2023-01-06 16:53:27,298 - ==> Top1: 85.199    Loss: 0.426

2023-01-06 16:53:27,298 - ==> Confusion:
[[  23    0  416]
 [   0    0  602]
 [  16    0 5929]]

2023-01-06 16:53:27,299 - ==> Best [Top1: 85.199   Sparsity:0.00   Params: 155168 on epoch: 5]
2023-01-06 16:53:27,299 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:27,306 - 

2023-01-06 16:53:27,306 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:27,877 - Epoch: [6][   10/  246]    Overall Loss 0.437491    Objective Loss 0.437491                                        LR 0.000060    Time 0.056985    
2023-01-06 16:53:28,071 - Epoch: [6][   20/  246]    Overall Loss 0.449588    Objective Loss 0.449588                                        LR 0.000060    Time 0.038169    
2023-01-06 16:53:28,260 - Epoch: [6][   30/  246]    Overall Loss 0.444688    Objective Loss 0.444688                                        LR 0.000060    Time 0.031745    
2023-01-06 16:53:28,450 - Epoch: [6][   40/  246]    Overall Loss 0.438364    Objective Loss 0.438364                                        LR 0.000060    Time 0.028536    
2023-01-06 16:53:28,647 - Epoch: [6][   50/  246]    Overall Loss 0.431330    Objective Loss 0.431330                                        LR 0.000060    Time 0.026756    
2023-01-06 16:53:28,842 - Epoch: [6][   60/  246]    Overall Loss 0.428045    Objective Loss 0.428045                                        LR 0.000060    Time 0.025546    
2023-01-06 16:53:29,020 - Epoch: [6][   70/  246]    Overall Loss 0.428856    Objective Loss 0.428856                                        LR 0.000060    Time 0.024436    
2023-01-06 16:53:29,210 - Epoch: [6][   80/  246]    Overall Loss 0.430363    Objective Loss 0.430363                                        LR 0.000060    Time 0.023748    
2023-01-06 16:53:29,398 - Epoch: [6][   90/  246]    Overall Loss 0.427115    Objective Loss 0.427115                                        LR 0.000060    Time 0.023194    
2023-01-06 16:53:29,573 - Epoch: [6][  100/  246]    Overall Loss 0.426513    Objective Loss 0.426513                                        LR 0.000060    Time 0.022629    
2023-01-06 16:53:29,746 - Epoch: [6][  110/  246]    Overall Loss 0.425842    Objective Loss 0.425842                                        LR 0.000060    Time 0.022138    
2023-01-06 16:53:29,918 - Epoch: [6][  120/  246]    Overall Loss 0.426398    Objective Loss 0.426398                                        LR 0.000060    Time 0.021726    
2023-01-06 16:53:30,088 - Epoch: [6][  130/  246]    Overall Loss 0.425112    Objective Loss 0.425112                                        LR 0.000060    Time 0.021359    
2023-01-06 16:53:30,261 - Epoch: [6][  140/  246]    Overall Loss 0.426890    Objective Loss 0.426890                                        LR 0.000060    Time 0.021069    
2023-01-06 16:53:30,437 - Epoch: [6][  150/  246]    Overall Loss 0.427797    Objective Loss 0.427797                                        LR 0.000060    Time 0.020836    
2023-01-06 16:53:30,611 - Epoch: [6][  160/  246]    Overall Loss 0.428926    Objective Loss 0.428926                                        LR 0.000060    Time 0.020617    
2023-01-06 16:53:30,787 - Epoch: [6][  170/  246]    Overall Loss 0.427964    Objective Loss 0.427964                                        LR 0.000060    Time 0.020437    
2023-01-06 16:53:30,960 - Epoch: [6][  180/  246]    Overall Loss 0.428032    Objective Loss 0.428032                                        LR 0.000060    Time 0.020261    
2023-01-06 16:53:31,146 - Epoch: [6][  190/  246]    Overall Loss 0.428795    Objective Loss 0.428795                                        LR 0.000060    Time 0.020172    
2023-01-06 16:53:31,337 - Epoch: [6][  200/  246]    Overall Loss 0.428934    Objective Loss 0.428934                                        LR 0.000060    Time 0.020117    
2023-01-06 16:53:31,532 - Epoch: [6][  210/  246]    Overall Loss 0.429055    Objective Loss 0.429055                                        LR 0.000060    Time 0.020084    
2023-01-06 16:53:31,727 - Epoch: [6][  220/  246]    Overall Loss 0.428129    Objective Loss 0.428129                                        LR 0.000060    Time 0.020057    
2023-01-06 16:53:31,917 - Epoch: [6][  230/  246]    Overall Loss 0.428386    Objective Loss 0.428386                                        LR 0.000060    Time 0.020009    
2023-01-06 16:53:32,110 - Epoch: [6][  240/  246]    Overall Loss 0.427821    Objective Loss 0.427821                                        LR 0.000060    Time 0.019980    
2023-01-06 16:53:32,203 - Epoch: [6][  246/  246]    Overall Loss 0.426893    Objective Loss 0.426893    Top1 87.559809    LR 0.000060    Time 0.019867    
2023-01-06 16:53:32,342 - --- validate (epoch=6)-----------
2023-01-06 16:53:32,343 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:32,790 - Epoch: [6][   10/   28]    Loss 0.423747    Top1 84.960938    
2023-01-06 16:53:32,903 - Epoch: [6][   20/   28]    Loss 0.411292    Top1 85.625000    
2023-01-06 16:53:32,972 - Epoch: [6][   28/   28]    Loss 0.419628    Top1 85.299170    
2023-01-06 16:53:33,107 - ==> Top1: 85.299    Loss: 0.420

2023-01-06 16:53:33,107 - ==> Confusion:
[[  29    0  410]
 [   0    0  602]
 [  15    0 5930]]

2023-01-06 16:53:33,108 - ==> Best [Top1: 85.299   Sparsity:0.00   Params: 155168 on epoch: 6]
2023-01-06 16:53:33,109 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:33,122 - 

2023-01-06 16:53:33,123 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:33,830 - Epoch: [7][   10/  246]    Overall Loss 0.456764    Objective Loss 0.456764                                        LR 0.000060    Time 0.070635    
2023-01-06 16:53:34,013 - Epoch: [7][   20/  246]    Overall Loss 0.428617    Objective Loss 0.428617                                        LR 0.000060    Time 0.044437    
2023-01-06 16:53:34,209 - Epoch: [7][   30/  246]    Overall Loss 0.424787    Objective Loss 0.424787                                        LR 0.000060    Time 0.036148    
2023-01-06 16:53:34,400 - Epoch: [7][   40/  246]    Overall Loss 0.425433    Objective Loss 0.425433                                        LR 0.000060    Time 0.031873    
2023-01-06 16:53:34,589 - Epoch: [7][   50/  246]    Overall Loss 0.423615    Objective Loss 0.423615                                        LR 0.000060    Time 0.029288    
2023-01-06 16:53:34,777 - Epoch: [7][   60/  246]    Overall Loss 0.425072    Objective Loss 0.425072                                        LR 0.000060    Time 0.027532    
2023-01-06 16:53:34,971 - Epoch: [7][   70/  246]    Overall Loss 0.423249    Objective Loss 0.423249                                        LR 0.000060    Time 0.026354    
2023-01-06 16:53:35,167 - Epoch: [7][   80/  246]    Overall Loss 0.421119    Objective Loss 0.421119                                        LR 0.000060    Time 0.025513    
2023-01-06 16:53:35,370 - Epoch: [7][   90/  246]    Overall Loss 0.421366    Objective Loss 0.421366                                        LR 0.000060    Time 0.024929    
2023-01-06 16:53:35,568 - Epoch: [7][  100/  246]    Overall Loss 0.420575    Objective Loss 0.420575                                        LR 0.000060    Time 0.024410    
2023-01-06 16:53:35,753 - Epoch: [7][  110/  246]    Overall Loss 0.420271    Objective Loss 0.420271                                        LR 0.000060    Time 0.023872    
2023-01-06 16:53:35,950 - Epoch: [7][  120/  246]    Overall Loss 0.418898    Objective Loss 0.418898                                        LR 0.000060    Time 0.023521    
2023-01-06 16:53:36,147 - Epoch: [7][  130/  246]    Overall Loss 0.419891    Objective Loss 0.419891                                        LR 0.000060    Time 0.023219    
2023-01-06 16:53:36,340 - Epoch: [7][  140/  246]    Overall Loss 0.419685    Objective Loss 0.419685                                        LR 0.000060    Time 0.022942    
2023-01-06 16:53:36,509 - Epoch: [7][  150/  246]    Overall Loss 0.420490    Objective Loss 0.420490                                        LR 0.000060    Time 0.022527    
2023-01-06 16:53:36,687 - Epoch: [7][  160/  246]    Overall Loss 0.421727    Objective Loss 0.421727                                        LR 0.000060    Time 0.022228    
2023-01-06 16:53:36,858 - Epoch: [7][  170/  246]    Overall Loss 0.422097    Objective Loss 0.422097                                        LR 0.000060    Time 0.021928    
2023-01-06 16:53:37,023 - Epoch: [7][  180/  246]    Overall Loss 0.421941    Objective Loss 0.421941                                        LR 0.000060    Time 0.021625    
2023-01-06 16:53:37,221 - Epoch: [7][  190/  246]    Overall Loss 0.421713    Objective Loss 0.421713                                        LR 0.000060    Time 0.021525    
2023-01-06 16:53:37,417 - Epoch: [7][  200/  246]    Overall Loss 0.420187    Objective Loss 0.420187                                        LR 0.000060    Time 0.021425    
2023-01-06 16:53:37,614 - Epoch: [7][  210/  246]    Overall Loss 0.419470    Objective Loss 0.419470                                        LR 0.000060    Time 0.021342    
2023-01-06 16:53:37,810 - Epoch: [7][  220/  246]    Overall Loss 0.418868    Objective Loss 0.418868                                        LR 0.000060    Time 0.021262    
2023-01-06 16:53:38,000 - Epoch: [7][  230/  246]    Overall Loss 0.419788    Objective Loss 0.419788                                        LR 0.000060    Time 0.021163    
2023-01-06 16:53:38,202 - Epoch: [7][  240/  246]    Overall Loss 0.419211    Objective Loss 0.419211                                        LR 0.000060    Time 0.021119    
2023-01-06 16:53:38,299 - Epoch: [7][  246/  246]    Overall Loss 0.419667    Objective Loss 0.419667    Top1 83.492823    LR 0.000060    Time 0.021000    
2023-01-06 16:53:38,433 - --- validate (epoch=7)-----------
2023-01-06 16:53:38,434 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:38,880 - Epoch: [7][   10/   28]    Loss 0.412229    Top1 85.781250    
2023-01-06 16:53:38,992 - Epoch: [7][   20/   28]    Loss 0.405089    Top1 85.878906    
2023-01-06 16:53:39,060 - Epoch: [7][   28/   28]    Loss 0.412746    Top1 85.499571    
2023-01-06 16:53:39,204 - ==> Top1: 85.500    Loss: 0.413

2023-01-06 16:53:39,204 - ==> Confusion:
[[  90    1  348]
 [   4   18  580]
 [  74    6 5865]]

2023-01-06 16:53:39,205 - ==> Best [Top1: 85.500   Sparsity:0.00   Params: 155168 on epoch: 7]
2023-01-06 16:53:39,205 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:39,216 - 

2023-01-06 16:53:39,216 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:39,919 - Epoch: [8][   10/  246]    Overall Loss 0.408680    Objective Loss 0.408680                                        LR 0.000060    Time 0.070210    
2023-01-06 16:53:40,102 - Epoch: [8][   20/  246]    Overall Loss 0.419285    Objective Loss 0.419285                                        LR 0.000060    Time 0.044212    
2023-01-06 16:53:40,277 - Epoch: [8][   30/  246]    Overall Loss 0.415341    Objective Loss 0.415341                                        LR 0.000060    Time 0.035285    
2023-01-06 16:53:40,454 - Epoch: [8][   40/  246]    Overall Loss 0.416337    Objective Loss 0.416337                                        LR 0.000060    Time 0.030896    
2023-01-06 16:53:40,635 - Epoch: [8][   50/  246]    Overall Loss 0.416687    Objective Loss 0.416687                                        LR 0.000060    Time 0.028331    
2023-01-06 16:53:40,814 - Epoch: [8][   60/  246]    Overall Loss 0.415687    Objective Loss 0.415687                                        LR 0.000060    Time 0.026583    
2023-01-06 16:53:40,991 - Epoch: [8][   70/  246]    Overall Loss 0.417823    Objective Loss 0.417823                                        LR 0.000060    Time 0.025314    
2023-01-06 16:53:41,169 - Epoch: [8][   80/  246]    Overall Loss 0.415192    Objective Loss 0.415192                                        LR 0.000060    Time 0.024371    
2023-01-06 16:53:41,350 - Epoch: [8][   90/  246]    Overall Loss 0.415587    Objective Loss 0.415587                                        LR 0.000060    Time 0.023664    
2023-01-06 16:53:41,533 - Epoch: [8][  100/  246]    Overall Loss 0.416584    Objective Loss 0.416584                                        LR 0.000060    Time 0.023131    
2023-01-06 16:53:41,712 - Epoch: [8][  110/  246]    Overall Loss 0.416519    Objective Loss 0.416519                                        LR 0.000060    Time 0.022644    
2023-01-06 16:53:41,890 - Epoch: [8][  120/  246]    Overall Loss 0.417221    Objective Loss 0.417221                                        LR 0.000060    Time 0.022242    
2023-01-06 16:53:42,067 - Epoch: [8][  130/  246]    Overall Loss 0.417123    Objective Loss 0.417123                                        LR 0.000060    Time 0.021891    
2023-01-06 16:53:42,234 - Epoch: [8][  140/  246]    Overall Loss 0.417232    Objective Loss 0.417232                                        LR 0.000060    Time 0.021519    
2023-01-06 16:53:42,401 - Epoch: [8][  150/  246]    Overall Loss 0.416177    Objective Loss 0.416177                                        LR 0.000060    Time 0.021196    
2023-01-06 16:53:42,567 - Epoch: [8][  160/  246]    Overall Loss 0.416630    Objective Loss 0.416630                                        LR 0.000060    Time 0.020907    
2023-01-06 16:53:42,735 - Epoch: [8][  170/  246]    Overall Loss 0.415025    Objective Loss 0.415025                                        LR 0.000060    Time 0.020661    
2023-01-06 16:53:42,898 - Epoch: [8][  180/  246]    Overall Loss 0.414060    Objective Loss 0.414060                                        LR 0.000060    Time 0.020417    
2023-01-06 16:53:43,061 - Epoch: [8][  190/  246]    Overall Loss 0.414346    Objective Loss 0.414346                                        LR 0.000060    Time 0.020197    
2023-01-06 16:53:43,224 - Epoch: [8][  200/  246]    Overall Loss 0.414023    Objective Loss 0.414023                                        LR 0.000060    Time 0.020001    
2023-01-06 16:53:43,388 - Epoch: [8][  210/  246]    Overall Loss 0.414887    Objective Loss 0.414887                                        LR 0.000060    Time 0.019829    
2023-01-06 16:53:43,551 - Epoch: [8][  220/  246]    Overall Loss 0.414168    Objective Loss 0.414168                                        LR 0.000060    Time 0.019668    
2023-01-06 16:53:43,730 - Epoch: [8][  230/  246]    Overall Loss 0.414417    Objective Loss 0.414417                                        LR 0.000060    Time 0.019590    
2023-01-06 16:53:43,916 - Epoch: [8][  240/  246]    Overall Loss 0.414543    Objective Loss 0.414543                                        LR 0.000060    Time 0.019547    
2023-01-06 16:53:44,008 - Epoch: [8][  246/  246]    Overall Loss 0.414670    Objective Loss 0.414670    Top1 85.406699    LR 0.000060    Time 0.019443    
2023-01-06 16:53:44,140 - --- validate (epoch=8)-----------
2023-01-06 16:53:44,140 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:44,593 - Epoch: [8][   10/   28]    Loss 0.414739    Top1 85.273438    
2023-01-06 16:53:44,707 - Epoch: [8][   20/   28]    Loss 0.405018    Top1 85.644531    
2023-01-06 16:53:44,773 - Epoch: [8][   28/   28]    Loss 0.405080    Top1 85.528199    
2023-01-06 16:53:44,929 - ==> Top1: 85.528    Loss: 0.405

2023-01-06 16:53:44,929 - ==> Confusion:
[[  88    1  350]
 [   5   11  586]
 [  68    1 5876]]

2023-01-06 16:53:44,930 - ==> Best [Top1: 85.528   Sparsity:0.00   Params: 155168 on epoch: 8]
2023-01-06 16:53:44,930 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:44,938 - 

2023-01-06 16:53:44,938 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:45,513 - Epoch: [9][   10/  246]    Overall Loss 0.410643    Objective Loss 0.410643                                        LR 0.000060    Time 0.057463    
2023-01-06 16:53:45,710 - Epoch: [9][   20/  246]    Overall Loss 0.401994    Objective Loss 0.401994                                        LR 0.000060    Time 0.038537    
2023-01-06 16:53:45,919 - Epoch: [9][   30/  246]    Overall Loss 0.400246    Objective Loss 0.400246                                        LR 0.000060    Time 0.032666    
2023-01-06 16:53:46,135 - Epoch: [9][   40/  246]    Overall Loss 0.401145    Objective Loss 0.401145                                        LR 0.000060    Time 0.029881    
2023-01-06 16:53:46,348 - Epoch: [9][   50/  246]    Overall Loss 0.400149    Objective Loss 0.400149                                        LR 0.000060    Time 0.028153    
2023-01-06 16:53:46,558 - Epoch: [9][   60/  246]    Overall Loss 0.403307    Objective Loss 0.403307                                        LR 0.000060    Time 0.026948    
2023-01-06 16:53:46,756 - Epoch: [9][   70/  246]    Overall Loss 0.406835    Objective Loss 0.406835                                        LR 0.000060    Time 0.025926    
2023-01-06 16:53:46,963 - Epoch: [9][   80/  246]    Overall Loss 0.404491    Objective Loss 0.404491                                        LR 0.000060    Time 0.025265    
2023-01-06 16:53:47,160 - Epoch: [9][   90/  246]    Overall Loss 0.404830    Objective Loss 0.404830                                        LR 0.000060    Time 0.024647    
2023-01-06 16:53:47,358 - Epoch: [9][  100/  246]    Overall Loss 0.404706    Objective Loss 0.404706                                        LR 0.000060    Time 0.024160    
2023-01-06 16:53:47,555 - Epoch: [9][  110/  246]    Overall Loss 0.404848    Objective Loss 0.404848                                        LR 0.000060    Time 0.023751    
2023-01-06 16:53:47,744 - Epoch: [9][  120/  246]    Overall Loss 0.404190    Objective Loss 0.404190                                        LR 0.000060    Time 0.023339    
2023-01-06 16:53:47,933 - Epoch: [9][  130/  246]    Overall Loss 0.403183    Objective Loss 0.403183                                        LR 0.000060    Time 0.023000    
2023-01-06 16:53:48,125 - Epoch: [9][  140/  246]    Overall Loss 0.404118    Objective Loss 0.404118                                        LR 0.000060    Time 0.022721    
2023-01-06 16:53:48,315 - Epoch: [9][  150/  246]    Overall Loss 0.405052    Objective Loss 0.405052                                        LR 0.000060    Time 0.022474    
2023-01-06 16:53:48,506 - Epoch: [9][  160/  246]    Overall Loss 0.405056    Objective Loss 0.405056                                        LR 0.000060    Time 0.022261    
2023-01-06 16:53:48,698 - Epoch: [9][  170/  246]    Overall Loss 0.405854    Objective Loss 0.405854                                        LR 0.000060    Time 0.022076    
2023-01-06 16:53:48,889 - Epoch: [9][  180/  246]    Overall Loss 0.405349    Objective Loss 0.405349                                        LR 0.000060    Time 0.021912    
2023-01-06 16:53:49,079 - Epoch: [9][  190/  246]    Overall Loss 0.405862    Objective Loss 0.405862                                        LR 0.000060    Time 0.021755    
2023-01-06 16:53:49,267 - Epoch: [9][  200/  246]    Overall Loss 0.405860    Objective Loss 0.405860                                        LR 0.000060    Time 0.021607    
2023-01-06 16:53:49,455 - Epoch: [9][  210/  246]    Overall Loss 0.406007    Objective Loss 0.406007                                        LR 0.000060    Time 0.021470    
2023-01-06 16:53:49,647 - Epoch: [9][  220/  246]    Overall Loss 0.405982    Objective Loss 0.405982                                        LR 0.000060    Time 0.021368    
2023-01-06 16:53:49,836 - Epoch: [9][  230/  246]    Overall Loss 0.406327    Objective Loss 0.406327                                        LR 0.000060    Time 0.021257    
2023-01-06 16:53:50,037 - Epoch: [9][  240/  246]    Overall Loss 0.406911    Objective Loss 0.406911                                        LR 0.000060    Time 0.021208    
2023-01-06 16:53:50,133 - Epoch: [9][  246/  246]    Overall Loss 0.406884    Objective Loss 0.406884    Top1 84.928230    LR 0.000060    Time 0.021079    
2023-01-06 16:53:50,274 - --- validate (epoch=9)-----------
2023-01-06 16:53:50,274 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:50,729 - Epoch: [9][   10/   28]    Loss 0.406306    Top1 85.117188    
2023-01-06 16:53:50,852 - Epoch: [9][   20/   28]    Loss 0.391381    Top1 86.054688    
2023-01-06 16:53:50,918 - Epoch: [9][   28/   28]    Loss 0.397956    Top1 85.757229    
2023-01-06 16:53:51,072 - ==> Top1: 85.757    Loss: 0.398

2023-01-06 16:53:51,072 - ==> Confusion:
[[  53    1  385]
 [   1   26  575]
 [  25    8 5912]]

2023-01-06 16:53:51,073 - ==> Best [Top1: 85.757   Sparsity:0.00   Params: 155168 on epoch: 9]
2023-01-06 16:53:51,073 - Saving checkpoint to: logs/2023.01.06-165156/checkpoint.pth.tar
2023-01-06 16:53:51,095 - 

2023-01-06 16:53:51,095 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:51,797 - Epoch: [10][   10/  246]    Overall Loss 0.433927    Objective Loss 0.433927                                        LR 0.000060    Time 0.070168    
2023-01-06 16:53:51,990 - Epoch: [10][   20/  246]    Overall Loss 0.426100    Objective Loss 0.426100                                        LR 0.000060    Time 0.044690    
2023-01-06 16:53:52,167 - Epoch: [10][   30/  246]    Overall Loss 0.415496    Objective Loss 0.415496                                        LR 0.000060    Time 0.035635    
2023-01-06 16:53:52,342 - Epoch: [10][   40/  246]    Overall Loss 0.415221    Objective Loss 0.415221                                        LR 0.000060    Time 0.031102    
2023-01-06 16:53:52,521 - Epoch: [10][   50/  246]    Overall Loss 0.409922    Objective Loss 0.409922                                        LR 0.000060    Time 0.028444    
2023-01-06 16:53:52,704 - Epoch: [10][   60/  246]    Overall Loss 0.405082    Objective Loss 0.405082                                        LR 0.000060    Time 0.026757    
2023-01-06 16:53:52,900 - Epoch: [10][   70/  246]    Overall Loss 0.407754    Objective Loss 0.407754                                        LR 0.000060    Time 0.025730    
2023-01-06 16:53:53,090 - Epoch: [10][   80/  246]    Overall Loss 0.411092    Objective Loss 0.411092                                        LR 0.000060    Time 0.024880    
2023-01-06 16:53:53,279 - Epoch: [10][   90/  246]    Overall Loss 0.408813    Objective Loss 0.408813                                        LR 0.000060    Time 0.024214    
2023-01-06 16:53:53,469 - Epoch: [10][  100/  246]    Overall Loss 0.408569    Objective Loss 0.408569                                        LR 0.000060    Time 0.023690    
2023-01-06 16:53:53,653 - Epoch: [10][  110/  246]    Overall Loss 0.405845    Objective Loss 0.405845                                        LR 0.000060    Time 0.023208    
2023-01-06 16:53:53,842 - Epoch: [10][  120/  246]    Overall Loss 0.404020    Objective Loss 0.404020                                        LR 0.000060    Time 0.022846    
2023-01-06 16:53:54,034 - Epoch: [10][  130/  246]    Overall Loss 0.403211    Objective Loss 0.403211                                        LR 0.000060    Time 0.022563    
2023-01-06 16:53:54,227 - Epoch: [10][  140/  246]    Overall Loss 0.402960    Objective Loss 0.402960                                        LR 0.000060    Time 0.022328    
2023-01-06 16:53:54,420 - Epoch: [10][  150/  246]    Overall Loss 0.402357    Objective Loss 0.402357                                        LR 0.000060    Time 0.022121    
2023-01-06 16:53:54,612 - Epoch: [10][  160/  246]    Overall Loss 0.401620    Objective Loss 0.401620                                        LR 0.000060    Time 0.021937    
2023-01-06 16:53:54,800 - Epoch: [10][  170/  246]    Overall Loss 0.401482    Objective Loss 0.401482                                        LR 0.000060    Time 0.021748    
2023-01-06 16:53:54,992 - Epoch: [10][  180/  246]    Overall Loss 0.400424    Objective Loss 0.400424                                        LR 0.000060    Time 0.021605    
2023-01-06 16:53:55,184 - Epoch: [10][  190/  246]    Overall Loss 0.402010    Objective Loss 0.402010                                        LR 0.000060    Time 0.021479    
2023-01-06 16:53:55,380 - Epoch: [10][  200/  246]    Overall Loss 0.402643    Objective Loss 0.402643                                        LR 0.000060    Time 0.021385    
2023-01-06 16:53:55,596 - Epoch: [10][  210/  246]    Overall Loss 0.401072    Objective Loss 0.401072                                        LR 0.000060    Time 0.021389    
2023-01-06 16:53:55,833 - Epoch: [10][  220/  246]    Overall Loss 0.401214    Objective Loss 0.401214                                        LR 0.000060    Time 0.021492    
2023-01-06 16:53:56,055 - Epoch: [10][  230/  246]    Overall Loss 0.401504    Objective Loss 0.401504                                        LR 0.000060    Time 0.021522    
2023-01-06 16:53:56,284 - Epoch: [10][  240/  246]    Overall Loss 0.401864    Objective Loss 0.401864                                        LR 0.000060    Time 0.021580    
2023-01-06 16:53:56,389 - Epoch: [10][  246/  246]    Overall Loss 0.401658    Objective Loss 0.401658    Top1 86.124402    LR 0.000060    Time 0.021480    
2023-01-06 16:53:56,556 - --- validate (epoch=10)-----------
2023-01-06 16:53:56,556 - 6986 samples (256 per mini-batch)
2023-01-06 16:53:57,001 - Epoch: [10][   10/   28]    Loss 0.394469    Top1 85.468750    
2023-01-06 16:53:57,114 - Epoch: [10][   20/   28]    Loss 0.400002    Top1 85.546875    
2023-01-06 16:53:57,183 - Epoch: [10][   28/   28]    Loss 0.393352    Top1 85.628400    
2023-01-06 16:53:57,342 - ==> Top1: 85.628    Loss: 0.393

2023-01-06 16:53:57,342 - ==> Confusion:
[[  35    1  403]
 [   0   19  583]
 [  13    4 5928]]

2023-01-06 16:53:57,343 - ==> Best [Top1: 85.628   Sparsity:0.00   Params: 155168 on epoch: 10]
2023-01-06 16:53:57,343 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:53:57,350 - 

2023-01-06 16:53:57,350 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:53:57,893 - Epoch: [11][   10/  246]    Overall Loss 0.399302    Objective Loss 0.399302                                        LR 0.000060    Time 0.054275    
2023-01-06 16:53:58,080 - Epoch: [11][   20/  246]    Overall Loss 0.403219    Objective Loss 0.403219                                        LR 0.000060    Time 0.036463    
2023-01-06 16:53:58,293 - Epoch: [11][   30/  246]    Overall Loss 0.395585    Objective Loss 0.395585                                        LR 0.000060    Time 0.031380    
2023-01-06 16:53:58,523 - Epoch: [11][   40/  246]    Overall Loss 0.400786    Objective Loss 0.400786                                        LR 0.000060    Time 0.029278    
2023-01-06 16:53:58,747 - Epoch: [11][   50/  246]    Overall Loss 0.398946    Objective Loss 0.398946                                        LR 0.000060    Time 0.027888    
2023-01-06 16:53:58,977 - Epoch: [11][   60/  246]    Overall Loss 0.399757    Objective Loss 0.399757                                        LR 0.000060    Time 0.027075    
2023-01-06 16:53:59,189 - Epoch: [11][   70/  246]    Overall Loss 0.397399    Objective Loss 0.397399                                        LR 0.000060    Time 0.026218    
2023-01-06 16:53:59,414 - Epoch: [11][   80/  246]    Overall Loss 0.397641    Objective Loss 0.397641                                        LR 0.000060    Time 0.025759    
2023-01-06 16:53:59,646 - Epoch: [11][   90/  246]    Overall Loss 0.396474    Objective Loss 0.396474                                        LR 0.000060    Time 0.025467    
2023-01-06 16:53:59,828 - Epoch: [11][  100/  246]    Overall Loss 0.397565    Objective Loss 0.397565                                        LR 0.000060    Time 0.024738    
2023-01-06 16:54:00,001 - Epoch: [11][  110/  246]    Overall Loss 0.396680    Objective Loss 0.396680                                        LR 0.000060    Time 0.024057    
2023-01-06 16:54:00,176 - Epoch: [11][  120/  246]    Overall Loss 0.396068    Objective Loss 0.396068                                        LR 0.000060    Time 0.023508    
2023-01-06 16:54:00,356 - Epoch: [11][  130/  246]    Overall Loss 0.397061    Objective Loss 0.397061                                        LR 0.000060    Time 0.023076    
2023-01-06 16:54:00,530 - Epoch: [11][  140/  246]    Overall Loss 0.396975    Objective Loss 0.396975                                        LR 0.000060    Time 0.022671    
2023-01-06 16:54:00,729 - Epoch: [11][  150/  246]    Overall Loss 0.396064    Objective Loss 0.396064                                        LR 0.000060    Time 0.022485    
2023-01-06 16:54:00,947 - Epoch: [11][  160/  246]    Overall Loss 0.397566    Objective Loss 0.397566                                        LR 0.000060    Time 0.022438    
2023-01-06 16:54:01,178 - Epoch: [11][  170/  246]    Overall Loss 0.398538    Objective Loss 0.398538                                        LR 0.000060    Time 0.022476    
2023-01-06 16:54:01,427 - Epoch: [11][  180/  246]    Overall Loss 0.400097    Objective Loss 0.400097                                        LR 0.000060    Time 0.022606    
2023-01-06 16:54:01,671 - Epoch: [11][  190/  246]    Overall Loss 0.398211    Objective Loss 0.398211                                        LR 0.000060    Time 0.022699    
2023-01-06 16:54:01,917 - Epoch: [11][  200/  246]    Overall Loss 0.398725    Objective Loss 0.398725                                        LR 0.000060    Time 0.022792    
2023-01-06 16:54:02,142 - Epoch: [11][  210/  246]    Overall Loss 0.398643    Objective Loss 0.398643                                        LR 0.000060    Time 0.022774    
2023-01-06 16:54:02,359 - Epoch: [11][  220/  246]    Overall Loss 0.397916    Objective Loss 0.397916                                        LR 0.000060    Time 0.022725    
2023-01-06 16:54:02,590 - Epoch: [11][  230/  246]    Overall Loss 0.397848    Objective Loss 0.397848                                        LR 0.000060    Time 0.022736    
2023-01-06 16:54:02,832 - Epoch: [11][  240/  246]    Overall Loss 0.396909    Objective Loss 0.396909                                        LR 0.000060    Time 0.022798    
2023-01-06 16:54:02,945 - Epoch: [11][  246/  246]    Overall Loss 0.395916    Objective Loss 0.395916    Top1 85.645933    LR 0.000060    Time 0.022699    
2023-01-06 16:54:03,070 - --- validate (epoch=11)-----------
2023-01-06 16:54:03,070 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:03,648 - Epoch: [11][   10/   28]    Loss 0.390349    Top1 86.328125    
2023-01-06 16:54:03,759 - Epoch: [11][   20/   28]    Loss 0.387965    Top1 86.250000    
2023-01-06 16:54:03,827 - Epoch: [11][   28/   28]    Loss 0.388331    Top1 86.057830    
2023-01-06 16:54:03,960 - ==> Top1: 86.058    Loss: 0.388

2023-01-06 16:54:03,961 - ==> Confusion:
[[  71    2  366]
 [   3   41  558]
 [  33   12 5900]]

2023-01-06 16:54:03,962 - ==> Best [Top1: 86.058   Sparsity:0.00   Params: 155168 on epoch: 11]
2023-01-06 16:54:03,962 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:03,969 - 

2023-01-06 16:54:03,969 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:04,553 - Epoch: [12][   10/  246]    Overall Loss 0.385880    Objective Loss 0.385880                                        LR 0.000060    Time 0.058290    
2023-01-06 16:54:04,740 - Epoch: [12][   20/  246]    Overall Loss 0.386383    Objective Loss 0.386383                                        LR 0.000060    Time 0.038467    
2023-01-06 16:54:04,929 - Epoch: [12][   30/  246]    Overall Loss 0.389212    Objective Loss 0.389212                                        LR 0.000060    Time 0.031911    
2023-01-06 16:54:05,116 - Epoch: [12][   40/  246]    Overall Loss 0.385831    Objective Loss 0.385831                                        LR 0.000060    Time 0.028581    
2023-01-06 16:54:05,309 - Epoch: [12][   50/  246]    Overall Loss 0.388469    Objective Loss 0.388469                                        LR 0.000060    Time 0.026723    
2023-01-06 16:54:05,505 - Epoch: [12][   60/  246]    Overall Loss 0.389775    Objective Loss 0.389775                                        LR 0.000060    Time 0.025534    
2023-01-06 16:54:05,700 - Epoch: [12][   70/  246]    Overall Loss 0.391878    Objective Loss 0.391878                                        LR 0.000060    Time 0.024661    
2023-01-06 16:54:05,896 - Epoch: [12][   80/  246]    Overall Loss 0.395704    Objective Loss 0.395704                                        LR 0.000060    Time 0.024023    
2023-01-06 16:54:06,095 - Epoch: [12][   90/  246]    Overall Loss 0.397082    Objective Loss 0.397082                                        LR 0.000060    Time 0.023567    
2023-01-06 16:54:06,297 - Epoch: [12][  100/  246]    Overall Loss 0.394688    Objective Loss 0.394688                                        LR 0.000060    Time 0.023219    
2023-01-06 16:54:06,510 - Epoch: [12][  110/  246]    Overall Loss 0.397092    Objective Loss 0.397092                                        LR 0.000060    Time 0.023043    
2023-01-06 16:54:06,719 - Epoch: [12][  120/  246]    Overall Loss 0.396056    Objective Loss 0.396056                                        LR 0.000060    Time 0.022866    
2023-01-06 16:54:06,930 - Epoch: [12][  130/  246]    Overall Loss 0.395983    Objective Loss 0.395983                                        LR 0.000060    Time 0.022722    
2023-01-06 16:54:07,128 - Epoch: [12][  140/  246]    Overall Loss 0.395640    Objective Loss 0.395640                                        LR 0.000060    Time 0.022515    
2023-01-06 16:54:07,294 - Epoch: [12][  150/  246]    Overall Loss 0.394843    Objective Loss 0.394843                                        LR 0.000060    Time 0.022118    
2023-01-06 16:54:07,465 - Epoch: [12][  160/  246]    Overall Loss 0.393037    Objective Loss 0.393037                                        LR 0.000060    Time 0.021800    
2023-01-06 16:54:07,632 - Epoch: [12][  170/  246]    Overall Loss 0.392672    Objective Loss 0.392672                                        LR 0.000060    Time 0.021500    
2023-01-06 16:54:07,818 - Epoch: [12][  180/  246]    Overall Loss 0.391658    Objective Loss 0.391658                                        LR 0.000060    Time 0.021336    
2023-01-06 16:54:08,009 - Epoch: [12][  190/  246]    Overall Loss 0.392627    Objective Loss 0.392627                                        LR 0.000060    Time 0.021217    
2023-01-06 16:54:08,199 - Epoch: [12][  200/  246]    Overall Loss 0.393519    Objective Loss 0.393519                                        LR 0.000060    Time 0.021104    
2023-01-06 16:54:08,389 - Epoch: [12][  210/  246]    Overall Loss 0.393635    Objective Loss 0.393635                                        LR 0.000060    Time 0.021002    
2023-01-06 16:54:08,579 - Epoch: [12][  220/  246]    Overall Loss 0.393718    Objective Loss 0.393718                                        LR 0.000060    Time 0.020909    
2023-01-06 16:54:08,769 - Epoch: [12][  230/  246]    Overall Loss 0.393471    Objective Loss 0.393471                                        LR 0.000060    Time 0.020824    
2023-01-06 16:54:08,970 - Epoch: [12][  240/  246]    Overall Loss 0.392908    Objective Loss 0.392908                                        LR 0.000060    Time 0.020794    
2023-01-06 16:54:09,070 - Epoch: [12][  246/  246]    Overall Loss 0.392653    Objective Loss 0.392653    Top1 87.320574    LR 0.000060    Time 0.020689    
2023-01-06 16:54:09,208 - --- validate (epoch=12)-----------
2023-01-06 16:54:09,209 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:09,659 - Epoch: [12][   10/   28]    Loss 0.381353    Top1 85.625000    
2023-01-06 16:54:09,770 - Epoch: [12][   20/   28]    Loss 0.383618    Top1 86.289062    
2023-01-06 16:54:09,839 - Epoch: [12][   28/   28]    Loss 0.383833    Top1 86.430003    
2023-01-06 16:54:09,989 - ==> Top1: 86.430    Loss: 0.384

2023-01-06 16:54:09,990 - ==> Confusion:
[[  96    2  341]
 [   3   61  538]
 [  45   19 5881]]

2023-01-06 16:54:09,991 - ==> Best [Top1: 86.430   Sparsity:0.00   Params: 155168 on epoch: 12]
2023-01-06 16:54:09,991 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:09,998 - 

2023-01-06 16:54:09,998 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:10,702 - Epoch: [13][   10/  246]    Overall Loss 0.404882    Objective Loss 0.404882                                        LR 0.000060    Time 0.070311    
2023-01-06 16:54:10,903 - Epoch: [13][   20/  246]    Overall Loss 0.397420    Objective Loss 0.397420                                        LR 0.000060    Time 0.045189    
2023-01-06 16:54:11,085 - Epoch: [13][   30/  246]    Overall Loss 0.402180    Objective Loss 0.402180                                        LR 0.000060    Time 0.036170    
2023-01-06 16:54:11,276 - Epoch: [13][   40/  246]    Overall Loss 0.401950    Objective Loss 0.401950                                        LR 0.000060    Time 0.031893    
2023-01-06 16:54:11,472 - Epoch: [13][   50/  246]    Overall Loss 0.400160    Objective Loss 0.400160                                        LR 0.000060    Time 0.029431    
2023-01-06 16:54:11,682 - Epoch: [13][   60/  246]    Overall Loss 0.396627    Objective Loss 0.396627                                        LR 0.000060    Time 0.028021    
2023-01-06 16:54:11,873 - Epoch: [13][   70/  246]    Overall Loss 0.392361    Objective Loss 0.392361                                        LR 0.000060    Time 0.026738    
2023-01-06 16:54:12,064 - Epoch: [13][   80/  246]    Overall Loss 0.394174    Objective Loss 0.394174                                        LR 0.000060    Time 0.025774    
2023-01-06 16:54:12,246 - Epoch: [13][   90/  246]    Overall Loss 0.391887    Objective Loss 0.391887                                        LR 0.000060    Time 0.024932    
2023-01-06 16:54:12,436 - Epoch: [13][  100/  246]    Overall Loss 0.390227    Objective Loss 0.390227                                        LR 0.000060    Time 0.024339    
2023-01-06 16:54:12,617 - Epoch: [13][  110/  246]    Overall Loss 0.387051    Objective Loss 0.387051                                        LR 0.000060    Time 0.023768    
2023-01-06 16:54:12,807 - Epoch: [13][  120/  246]    Overall Loss 0.386480    Objective Loss 0.386480                                        LR 0.000060    Time 0.023368    
2023-01-06 16:54:13,002 - Epoch: [13][  130/  246]    Overall Loss 0.386763    Objective Loss 0.386763                                        LR 0.000060    Time 0.023069    
2023-01-06 16:54:13,210 - Epoch: [13][  140/  246]    Overall Loss 0.386560    Objective Loss 0.386560                                        LR 0.000060    Time 0.022903    
2023-01-06 16:54:13,405 - Epoch: [13][  150/  246]    Overall Loss 0.386426    Objective Loss 0.386426                                        LR 0.000060    Time 0.022676    
2023-01-06 16:54:13,612 - Epoch: [13][  160/  246]    Overall Loss 0.385368    Objective Loss 0.385368                                        LR 0.000060    Time 0.022551    
2023-01-06 16:54:13,811 - Epoch: [13][  170/  246]    Overall Loss 0.386774    Objective Loss 0.386774                                        LR 0.000060    Time 0.022392    
2023-01-06 16:54:14,017 - Epoch: [13][  180/  246]    Overall Loss 0.387371    Objective Loss 0.387371                                        LR 0.000060    Time 0.022288    
2023-01-06 16:54:14,215 - Epoch: [13][  190/  246]    Overall Loss 0.387485    Objective Loss 0.387485                                        LR 0.000060    Time 0.022158    
2023-01-06 16:54:14,428 - Epoch: [13][  200/  246]    Overall Loss 0.388858    Objective Loss 0.388858                                        LR 0.000060    Time 0.022111    
2023-01-06 16:54:14,634 - Epoch: [13][  210/  246]    Overall Loss 0.388656    Objective Loss 0.388656                                        LR 0.000060    Time 0.022040    
2023-01-06 16:54:14,857 - Epoch: [13][  220/  246]    Overall Loss 0.388601    Objective Loss 0.388601                                        LR 0.000060    Time 0.022049    
2023-01-06 16:54:15,041 - Epoch: [13][  230/  246]    Overall Loss 0.388240    Objective Loss 0.388240                                        LR 0.000060    Time 0.021888    
2023-01-06 16:54:15,245 - Epoch: [13][  240/  246]    Overall Loss 0.387707    Objective Loss 0.387707                                        LR 0.000060    Time 0.021823    
2023-01-06 16:54:15,337 - Epoch: [13][  246/  246]    Overall Loss 0.387595    Objective Loss 0.387595    Top1 87.559809    LR 0.000060    Time 0.021666    
2023-01-06 16:54:15,479 - --- validate (epoch=13)-----------
2023-01-06 16:54:15,479 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:15,940 - Epoch: [13][   10/   28]    Loss 0.378022    Top1 86.562500    
2023-01-06 16:54:16,057 - Epoch: [13][   20/   28]    Loss 0.375143    Top1 86.660156    
2023-01-06 16:54:16,126 - Epoch: [13][   28/   28]    Loss 0.380108    Top1 86.558832    
2023-01-06 16:54:16,290 - ==> Top1: 86.559    Loss: 0.380

2023-01-06 16:54:16,291 - ==> Confusion:
[[  93    2  344]
 [   4   64  534]
 [  33   22 5890]]

2023-01-06 16:54:16,292 - ==> Best [Top1: 86.559   Sparsity:0.00   Params: 155168 on epoch: 13]
2023-01-06 16:54:16,292 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:16,299 - 

2023-01-06 16:54:16,299 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:16,855 - Epoch: [14][   10/  246]    Overall Loss 0.374612    Objective Loss 0.374612                                        LR 0.000060    Time 0.055535    
2023-01-06 16:54:17,019 - Epoch: [14][   20/  246]    Overall Loss 0.388631    Objective Loss 0.388631                                        LR 0.000060    Time 0.035881    
2023-01-06 16:54:17,194 - Epoch: [14][   30/  246]    Overall Loss 0.393897    Objective Loss 0.393897                                        LR 0.000060    Time 0.029745    
2023-01-06 16:54:17,376 - Epoch: [14][   40/  246]    Overall Loss 0.395592    Objective Loss 0.395592                                        LR 0.000060    Time 0.026824    
2023-01-06 16:54:17,576 - Epoch: [14][   50/  246]    Overall Loss 0.393211    Objective Loss 0.393211                                        LR 0.000060    Time 0.025440    
2023-01-06 16:54:17,771 - Epoch: [14][   60/  246]    Overall Loss 0.391732    Objective Loss 0.391732                                        LR 0.000060    Time 0.024443    
2023-01-06 16:54:17,968 - Epoch: [14][   70/  246]    Overall Loss 0.391269    Objective Loss 0.391269                                        LR 0.000060    Time 0.023758    
2023-01-06 16:54:18,142 - Epoch: [14][   80/  246]    Overall Loss 0.391213    Objective Loss 0.391213                                        LR 0.000060    Time 0.022939    
2023-01-06 16:54:18,316 - Epoch: [14][   90/  246]    Overall Loss 0.387288    Objective Loss 0.387288                                        LR 0.000060    Time 0.022320    
2023-01-06 16:54:18,487 - Epoch: [14][  100/  246]    Overall Loss 0.388888    Objective Loss 0.388888                                        LR 0.000060    Time 0.021777    
2023-01-06 16:54:18,661 - Epoch: [14][  110/  246]    Overall Loss 0.387878    Objective Loss 0.387878                                        LR 0.000060    Time 0.021381    
2023-01-06 16:54:18,837 - Epoch: [14][  120/  246]    Overall Loss 0.385961    Objective Loss 0.385961                                        LR 0.000060    Time 0.021061    
2023-01-06 16:54:19,026 - Epoch: [14][  130/  246]    Overall Loss 0.384488    Objective Loss 0.384488                                        LR 0.000060    Time 0.020889    
2023-01-06 16:54:19,191 - Epoch: [14][  140/  246]    Overall Loss 0.383977    Objective Loss 0.383977                                        LR 0.000060    Time 0.020576    
2023-01-06 16:54:19,353 - Epoch: [14][  150/  246]    Overall Loss 0.384196    Objective Loss 0.384196                                        LR 0.000060    Time 0.020277    
2023-01-06 16:54:19,513 - Epoch: [14][  160/  246]    Overall Loss 0.381924    Objective Loss 0.381924                                        LR 0.000060    Time 0.020009    
2023-01-06 16:54:19,672 - Epoch: [14][  170/  246]    Overall Loss 0.381430    Objective Loss 0.381430                                        LR 0.000060    Time 0.019768    
2023-01-06 16:54:19,833 - Epoch: [14][  180/  246]    Overall Loss 0.382465    Objective Loss 0.382465                                        LR 0.000060    Time 0.019564    
2023-01-06 16:54:20,001 - Epoch: [14][  190/  246]    Overall Loss 0.383420    Objective Loss 0.383420                                        LR 0.000060    Time 0.019412    
2023-01-06 16:54:20,196 - Epoch: [14][  200/  246]    Overall Loss 0.384155    Objective Loss 0.384155                                        LR 0.000060    Time 0.019419    
2023-01-06 16:54:20,397 - Epoch: [14][  210/  246]    Overall Loss 0.383714    Objective Loss 0.383714                                        LR 0.000060    Time 0.019448    
2023-01-06 16:54:20,598 - Epoch: [14][  220/  246]    Overall Loss 0.383721    Objective Loss 0.383721                                        LR 0.000060    Time 0.019474    
2023-01-06 16:54:20,798 - Epoch: [14][  230/  246]    Overall Loss 0.384217    Objective Loss 0.384217                                        LR 0.000060    Time 0.019494    
2023-01-06 16:54:21,005 - Epoch: [14][  240/  246]    Overall Loss 0.385046    Objective Loss 0.385046                                        LR 0.000060    Time 0.019543    
2023-01-06 16:54:21,100 - Epoch: [14][  246/  246]    Overall Loss 0.384607    Objective Loss 0.384607    Top1 85.885167    LR 0.000060    Time 0.019455    
2023-01-06 16:54:21,245 - --- validate (epoch=14)-----------
2023-01-06 16:54:21,246 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:21,698 - Epoch: [14][   10/   28]    Loss 0.362714    Top1 86.640625    
2023-01-06 16:54:21,817 - Epoch: [14][   20/   28]    Loss 0.379029    Top1 85.976562    
2023-01-06 16:54:21,884 - Epoch: [14][   28/   28]    Loss 0.373392    Top1 86.315488    
2023-01-06 16:54:22,034 - ==> Top1: 86.315    Loss: 0.373

2023-01-06 16:54:22,034 - ==> Confusion:
[[  76    1  362]
 [   2   48  552]
 [  26   13 5906]]

2023-01-06 16:54:22,035 - ==> Best [Top1: 86.559   Sparsity:0.00   Params: 155168 on epoch: 13]
2023-01-06 16:54:22,035 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:22,041 - 

2023-01-06 16:54:22,041 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:22,719 - Epoch: [15][   10/  246]    Overall Loss 0.374984    Objective Loss 0.374984                                        LR 0.000060    Time 0.067699    
2023-01-06 16:54:22,878 - Epoch: [15][   20/  246]    Overall Loss 0.382528    Objective Loss 0.382528                                        LR 0.000060    Time 0.041758    
2023-01-06 16:54:23,059 - Epoch: [15][   30/  246]    Overall Loss 0.391042    Objective Loss 0.391042                                        LR 0.000060    Time 0.033826    
2023-01-06 16:54:23,257 - Epoch: [15][   40/  246]    Overall Loss 0.389727    Objective Loss 0.389727                                        LR 0.000060    Time 0.030325    
2023-01-06 16:54:23,443 - Epoch: [15][   50/  246]    Overall Loss 0.390702    Objective Loss 0.390702                                        LR 0.000060    Time 0.027964    
2023-01-06 16:54:23,614 - Epoch: [15][   60/  246]    Overall Loss 0.389045    Objective Loss 0.389045                                        LR 0.000060    Time 0.026153    
2023-01-06 16:54:23,802 - Epoch: [15][   70/  246]    Overall Loss 0.388739    Objective Loss 0.388739                                        LR 0.000060    Time 0.025098    
2023-01-06 16:54:23,997 - Epoch: [15][   80/  246]    Overall Loss 0.385535    Objective Loss 0.385535                                        LR 0.000060    Time 0.024386    
2023-01-06 16:54:24,187 - Epoch: [15][   90/  246]    Overall Loss 0.386180    Objective Loss 0.386180                                        LR 0.000060    Time 0.023783    
2023-01-06 16:54:24,373 - Epoch: [15][  100/  246]    Overall Loss 0.387414    Objective Loss 0.387414                                        LR 0.000060    Time 0.023268    
2023-01-06 16:54:24,555 - Epoch: [15][  110/  246]    Overall Loss 0.386522    Objective Loss 0.386522                                        LR 0.000060    Time 0.022803    
2023-01-06 16:54:24,737 - Epoch: [15][  120/  246]    Overall Loss 0.386119    Objective Loss 0.386119                                        LR 0.000060    Time 0.022412    
2023-01-06 16:54:24,918 - Epoch: [15][  130/  246]    Overall Loss 0.384606    Objective Loss 0.384606                                        LR 0.000060    Time 0.022083    
2023-01-06 16:54:25,101 - Epoch: [15][  140/  246]    Overall Loss 0.384510    Objective Loss 0.384510                                        LR 0.000060    Time 0.021804    
2023-01-06 16:54:25,283 - Epoch: [15][  150/  246]    Overall Loss 0.384192    Objective Loss 0.384192                                        LR 0.000060    Time 0.021566    
2023-01-06 16:54:25,467 - Epoch: [15][  160/  246]    Overall Loss 0.381676    Objective Loss 0.381676                                        LR 0.000060    Time 0.021366    
2023-01-06 16:54:25,650 - Epoch: [15][  170/  246]    Overall Loss 0.381897    Objective Loss 0.381897                                        LR 0.000060    Time 0.021183    
2023-01-06 16:54:25,831 - Epoch: [15][  180/  246]    Overall Loss 0.380623    Objective Loss 0.380623                                        LR 0.000060    Time 0.021009    
2023-01-06 16:54:26,011 - Epoch: [15][  190/  246]    Overall Loss 0.381133    Objective Loss 0.381133                                        LR 0.000060    Time 0.020849    
2023-01-06 16:54:26,195 - Epoch: [15][  200/  246]    Overall Loss 0.381534    Objective Loss 0.381534                                        LR 0.000060    Time 0.020723    
2023-01-06 16:54:26,378 - Epoch: [15][  210/  246]    Overall Loss 0.381442    Objective Loss 0.381442                                        LR 0.000060    Time 0.020605    
2023-01-06 16:54:26,558 - Epoch: [15][  220/  246]    Overall Loss 0.382226    Objective Loss 0.382226                                        LR 0.000060    Time 0.020486    
2023-01-06 16:54:26,743 - Epoch: [15][  230/  246]    Overall Loss 0.382615    Objective Loss 0.382615                                        LR 0.000060    Time 0.020397    
2023-01-06 16:54:26,934 - Epoch: [15][  240/  246]    Overall Loss 0.382436    Objective Loss 0.382436                                        LR 0.000060    Time 0.020341    
2023-01-06 16:54:27,030 - Epoch: [15][  246/  246]    Overall Loss 0.382428    Objective Loss 0.382428    Top1 89.234450    LR 0.000060    Time 0.020237    
2023-01-06 16:54:27,205 - --- validate (epoch=15)-----------
2023-01-06 16:54:27,205 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:27,678 - Epoch: [15][   10/   28]    Loss 0.381501    Top1 86.328125    
2023-01-06 16:54:27,813 - Epoch: [15][   20/   28]    Loss 0.383309    Top1 85.976562    
2023-01-06 16:54:27,881 - Epoch: [15][   28/   28]    Loss 0.377982    Top1 86.458632    
2023-01-06 16:54:28,000 - ==> Top1: 86.459    Loss: 0.378

2023-01-06 16:54:28,001 - ==> Confusion:
[[ 128    1  310]
 [   8   44  550]
 [  67   10 5868]]

2023-01-06 16:54:28,002 - ==> Best [Top1: 86.559   Sparsity:0.00   Params: 155168 on epoch: 13]
2023-01-06 16:54:28,002 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:28,008 - 

2023-01-06 16:54:28,008 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:28,740 - Epoch: [16][   10/  246]    Overall Loss 0.392353    Objective Loss 0.392353                                        LR 0.000060    Time 0.073151    
2023-01-06 16:54:28,934 - Epoch: [16][   20/  246]    Overall Loss 0.384661    Objective Loss 0.384661                                        LR 0.000060    Time 0.046264    
2023-01-06 16:54:29,115 - Epoch: [16][   30/  246]    Overall Loss 0.378520    Objective Loss 0.378520                                        LR 0.000060    Time 0.036861    
2023-01-06 16:54:29,282 - Epoch: [16][   40/  246]    Overall Loss 0.375525    Objective Loss 0.375525                                        LR 0.000060    Time 0.031812    
2023-01-06 16:54:29,458 - Epoch: [16][   50/  246]    Overall Loss 0.378152    Objective Loss 0.378152                                        LR 0.000060    Time 0.028958    
2023-01-06 16:54:29,644 - Epoch: [16][   60/  246]    Overall Loss 0.375361    Objective Loss 0.375361                                        LR 0.000060    Time 0.027222    
2023-01-06 16:54:29,822 - Epoch: [16][   70/  246]    Overall Loss 0.373898    Objective Loss 0.373898                                        LR 0.000060    Time 0.025870    
2023-01-06 16:54:30,020 - Epoch: [16][   80/  246]    Overall Loss 0.372406    Objective Loss 0.372406                                        LR 0.000060    Time 0.025112    
2023-01-06 16:54:30,228 - Epoch: [16][   90/  246]    Overall Loss 0.374190    Objective Loss 0.374190                                        LR 0.000060    Time 0.024622    
2023-01-06 16:54:30,437 - Epoch: [16][  100/  246]    Overall Loss 0.374895    Objective Loss 0.374895                                        LR 0.000060    Time 0.024252    
2023-01-06 16:54:30,644 - Epoch: [16][  110/  246]    Overall Loss 0.376030    Objective Loss 0.376030                                        LR 0.000060    Time 0.023919    
2023-01-06 16:54:30,850 - Epoch: [16][  120/  246]    Overall Loss 0.377281    Objective Loss 0.377281                                        LR 0.000060    Time 0.023639    
2023-01-06 16:54:31,055 - Epoch: [16][  130/  246]    Overall Loss 0.377316    Objective Loss 0.377316                                        LR 0.000060    Time 0.023396    
2023-01-06 16:54:31,251 - Epoch: [16][  140/  246]    Overall Loss 0.377724    Objective Loss 0.377724                                        LR 0.000060    Time 0.023126    
2023-01-06 16:54:31,424 - Epoch: [16][  150/  246]    Overall Loss 0.377449    Objective Loss 0.377449                                        LR 0.000060    Time 0.022732    
2023-01-06 16:54:31,597 - Epoch: [16][  160/  246]    Overall Loss 0.378284    Objective Loss 0.378284                                        LR 0.000060    Time 0.022390    
2023-01-06 16:54:31,784 - Epoch: [16][  170/  246]    Overall Loss 0.378463    Objective Loss 0.378463                                        LR 0.000060    Time 0.022171    
2023-01-06 16:54:31,980 - Epoch: [16][  180/  246]    Overall Loss 0.378811    Objective Loss 0.378811                                        LR 0.000060    Time 0.022030    
2023-01-06 16:54:32,167 - Epoch: [16][  190/  246]    Overall Loss 0.376988    Objective Loss 0.376988                                        LR 0.000060    Time 0.021850    
2023-01-06 16:54:32,338 - Epoch: [16][  200/  246]    Overall Loss 0.377047    Objective Loss 0.377047                                        LR 0.000060    Time 0.021612    
2023-01-06 16:54:32,515 - Epoch: [16][  210/  246]    Overall Loss 0.376363    Objective Loss 0.376363                                        LR 0.000060    Time 0.021423    
2023-01-06 16:54:32,687 - Epoch: [16][  220/  246]    Overall Loss 0.376111    Objective Loss 0.376111                                        LR 0.000060    Time 0.021230    
2023-01-06 16:54:32,865 - Epoch: [16][  230/  246]    Overall Loss 0.376287    Objective Loss 0.376287                                        LR 0.000060    Time 0.021081    
2023-01-06 16:54:33,071 - Epoch: [16][  240/  246]    Overall Loss 0.377987    Objective Loss 0.377987                                        LR 0.000060    Time 0.021059    
2023-01-06 16:54:33,171 - Epoch: [16][  246/  246]    Overall Loss 0.377933    Objective Loss 0.377933    Top1 84.928230    LR 0.000060    Time 0.020948    
2023-01-06 16:54:33,297 - --- validate (epoch=16)-----------
2023-01-06 16:54:33,297 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:33,757 - Epoch: [16][   10/   28]    Loss 0.366698    Top1 86.679688    
2023-01-06 16:54:33,883 - Epoch: [16][   20/   28]    Loss 0.367502    Top1 86.621094    
2023-01-06 16:54:33,950 - Epoch: [16][   28/   28]    Loss 0.368519    Top1 86.558832    
2023-01-06 16:54:34,089 - ==> Top1: 86.559    Loss: 0.369

2023-01-06 16:54:34,089 - ==> Confusion:
[[  62    2  375]
 [   1   84  517]
 [  21   23 5901]]

2023-01-06 16:54:34,090 - ==> Best [Top1: 86.559   Sparsity:0.00   Params: 155168 on epoch: 16]
2023-01-06 16:54:34,091 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:34,098 - 

2023-01-06 16:54:34,098 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:34,669 - Epoch: [17][   10/  246]    Overall Loss 0.389888    Objective Loss 0.389888                                        LR 0.000060    Time 0.056992    
2023-01-06 16:54:34,874 - Epoch: [17][   20/  246]    Overall Loss 0.380830    Objective Loss 0.380830                                        LR 0.000060    Time 0.038735    
2023-01-06 16:54:35,089 - Epoch: [17][   30/  246]    Overall Loss 0.382613    Objective Loss 0.382613                                        LR 0.000060    Time 0.032973    
2023-01-06 16:54:35,286 - Epoch: [17][   40/  246]    Overall Loss 0.380614    Objective Loss 0.380614                                        LR 0.000060    Time 0.029660    
2023-01-06 16:54:35,481 - Epoch: [17][   50/  246]    Overall Loss 0.380602    Objective Loss 0.380602                                        LR 0.000060    Time 0.027613    
2023-01-06 16:54:35,673 - Epoch: [17][   60/  246]    Overall Loss 0.380892    Objective Loss 0.380892                                        LR 0.000060    Time 0.026202    
2023-01-06 16:54:35,841 - Epoch: [17][   70/  246]    Overall Loss 0.378058    Objective Loss 0.378058                                        LR 0.000060    Time 0.024857    
2023-01-06 16:54:36,006 - Epoch: [17][   80/  246]    Overall Loss 0.378675    Objective Loss 0.378675                                        LR 0.000060    Time 0.023805    
2023-01-06 16:54:36,189 - Epoch: [17][   90/  246]    Overall Loss 0.377094    Objective Loss 0.377094                                        LR 0.000060    Time 0.023188    
2023-01-06 16:54:36,376 - Epoch: [17][  100/  246]    Overall Loss 0.377821    Objective Loss 0.377821                                        LR 0.000060    Time 0.022741    
2023-01-06 16:54:36,558 - Epoch: [17][  110/  246]    Overall Loss 0.376413    Objective Loss 0.376413                                        LR 0.000060    Time 0.022327    
2023-01-06 16:54:36,737 - Epoch: [17][  120/  246]    Overall Loss 0.376077    Objective Loss 0.376077                                        LR 0.000060    Time 0.021950    
2023-01-06 16:54:36,922 - Epoch: [17][  130/  246]    Overall Loss 0.377361    Objective Loss 0.377361                                        LR 0.000060    Time 0.021686    
2023-01-06 16:54:37,112 - Epoch: [17][  140/  246]    Overall Loss 0.377693    Objective Loss 0.377693                                        LR 0.000060    Time 0.021487    
2023-01-06 16:54:37,301 - Epoch: [17][  150/  246]    Overall Loss 0.377908    Objective Loss 0.377908                                        LR 0.000060    Time 0.021316    
2023-01-06 16:54:37,492 - Epoch: [17][  160/  246]    Overall Loss 0.377817    Objective Loss 0.377817                                        LR 0.000060    Time 0.021173    
2023-01-06 16:54:37,679 - Epoch: [17][  170/  246]    Overall Loss 0.377338    Objective Loss 0.377338                                        LR 0.000060    Time 0.021028    
2023-01-06 16:54:37,849 - Epoch: [17][  180/  246]    Overall Loss 0.376449    Objective Loss 0.376449                                        LR 0.000060    Time 0.020800    
2023-01-06 16:54:38,050 - Epoch: [17][  190/  246]    Overall Loss 0.375068    Objective Loss 0.375068                                        LR 0.000060    Time 0.020761    
2023-01-06 16:54:38,252 - Epoch: [17][  200/  246]    Overall Loss 0.375443    Objective Loss 0.375443                                        LR 0.000060    Time 0.020731    
2023-01-06 16:54:38,452 - Epoch: [17][  210/  246]    Overall Loss 0.375232    Objective Loss 0.375232                                        LR 0.000060    Time 0.020696    
2023-01-06 16:54:38,655 - Epoch: [17][  220/  246]    Overall Loss 0.375536    Objective Loss 0.375536                                        LR 0.000060    Time 0.020677    
2023-01-06 16:54:38,855 - Epoch: [17][  230/  246]    Overall Loss 0.376111    Objective Loss 0.376111                                        LR 0.000060    Time 0.020646    
2023-01-06 16:54:39,067 - Epoch: [17][  240/  246]    Overall Loss 0.375616    Objective Loss 0.375616                                        LR 0.000060    Time 0.020663    
2023-01-06 16:54:39,166 - Epoch: [17][  246/  246]    Overall Loss 0.375391    Objective Loss 0.375391    Top1 85.885167    LR 0.000060    Time 0.020558    
2023-01-06 16:54:39,299 - --- validate (epoch=17)-----------
2023-01-06 16:54:39,300 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:39,752 - Epoch: [17][   10/   28]    Loss 0.368023    Top1 87.734375    
2023-01-06 16:54:39,863 - Epoch: [17][   20/   28]    Loss 0.376875    Top1 87.011719    
2023-01-06 16:54:39,931 - Epoch: [17][   28/   28]    Loss 0.373995    Top1 87.045520    
2023-01-06 16:54:40,096 - ==> Top1: 87.046    Loss: 0.374

2023-01-06 16:54:40,096 - ==> Confusion:
[[ 142    1  296]
 [   8   93  501]
 [  75   24 5846]]

2023-01-06 16:54:40,097 - ==> Best [Top1: 87.046   Sparsity:0.00   Params: 155168 on epoch: 17]
2023-01-06 16:54:40,097 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:40,104 - 

2023-01-06 16:54:40,104 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:40,827 - Epoch: [18][   10/  246]    Overall Loss 0.382922    Objective Loss 0.382922                                        LR 0.000060    Time 0.072171    
2023-01-06 16:54:41,028 - Epoch: [18][   20/  246]    Overall Loss 0.374447    Objective Loss 0.374447                                        LR 0.000060    Time 0.046123    
2023-01-06 16:54:41,227 - Epoch: [18][   30/  246]    Overall Loss 0.374581    Objective Loss 0.374581                                        LR 0.000060    Time 0.037324    
2023-01-06 16:54:41,431 - Epoch: [18][   40/  246]    Overall Loss 0.374499    Objective Loss 0.374499                                        LR 0.000060    Time 0.033069    
2023-01-06 16:54:41,629 - Epoch: [18][   50/  246]    Overall Loss 0.375006    Objective Loss 0.375006                                        LR 0.000060    Time 0.030387    
2023-01-06 16:54:41,835 - Epoch: [18][   60/  246]    Overall Loss 0.374443    Objective Loss 0.374443                                        LR 0.000060    Time 0.028749    
2023-01-06 16:54:42,033 - Epoch: [18][   70/  246]    Overall Loss 0.374651    Objective Loss 0.374651                                        LR 0.000060    Time 0.027471    
2023-01-06 16:54:42,241 - Epoch: [18][   80/  246]    Overall Loss 0.374660    Objective Loss 0.374660                                        LR 0.000060    Time 0.026629    
2023-01-06 16:54:42,444 - Epoch: [18][   90/  246]    Overall Loss 0.375033    Objective Loss 0.375033                                        LR 0.000060    Time 0.025904    
2023-01-06 16:54:42,660 - Epoch: [18][  100/  246]    Overall Loss 0.374960    Objective Loss 0.374960                                        LR 0.000060    Time 0.025465    
2023-01-06 16:54:42,867 - Epoch: [18][  110/  246]    Overall Loss 0.376353    Objective Loss 0.376353                                        LR 0.000060    Time 0.025031    
2023-01-06 16:54:43,084 - Epoch: [18][  120/  246]    Overall Loss 0.375540    Objective Loss 0.375540                                        LR 0.000060    Time 0.024744    
2023-01-06 16:54:43,292 - Epoch: [18][  130/  246]    Overall Loss 0.375674    Objective Loss 0.375674                                        LR 0.000060    Time 0.024440    
2023-01-06 16:54:43,507 - Epoch: [18][  140/  246]    Overall Loss 0.374441    Objective Loss 0.374441                                        LR 0.000060    Time 0.024225    
2023-01-06 16:54:43,715 - Epoch: [18][  150/  246]    Overall Loss 0.374245    Objective Loss 0.374245                                        LR 0.000060    Time 0.023999    
2023-01-06 16:54:43,933 - Epoch: [18][  160/  246]    Overall Loss 0.374837    Objective Loss 0.374837                                        LR 0.000060    Time 0.023856    
2023-01-06 16:54:44,131 - Epoch: [18][  170/  246]    Overall Loss 0.374328    Objective Loss 0.374328                                        LR 0.000060    Time 0.023618    
2023-01-06 16:54:44,337 - Epoch: [18][  180/  246]    Overall Loss 0.372982    Objective Loss 0.372982                                        LR 0.000060    Time 0.023446    
2023-01-06 16:54:44,541 - Epoch: [18][  190/  246]    Overall Loss 0.373118    Objective Loss 0.373118                                        LR 0.000060    Time 0.023283    
2023-01-06 16:54:44,768 - Epoch: [18][  200/  246]    Overall Loss 0.373285    Objective Loss 0.373285                                        LR 0.000060    Time 0.023256    
2023-01-06 16:54:44,932 - Epoch: [18][  210/  246]    Overall Loss 0.373293    Objective Loss 0.373293                                        LR 0.000060    Time 0.022927    
2023-01-06 16:54:45,099 - Epoch: [18][  220/  246]    Overall Loss 0.373831    Objective Loss 0.373831                                        LR 0.000060    Time 0.022643    
2023-01-06 16:54:45,262 - Epoch: [18][  230/  246]    Overall Loss 0.373920    Objective Loss 0.373920                                        LR 0.000060    Time 0.022366    
2023-01-06 16:54:45,434 - Epoch: [18][  240/  246]    Overall Loss 0.373360    Objective Loss 0.373360                                        LR 0.000060    Time 0.022146    
2023-01-06 16:54:45,519 - Epoch: [18][  246/  246]    Overall Loss 0.373132    Objective Loss 0.373132    Top1 89.234450    LR 0.000060    Time 0.021952    
2023-01-06 16:54:45,697 - --- validate (epoch=18)-----------
2023-01-06 16:54:45,697 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:46,137 - Epoch: [18][   10/   28]    Loss 0.366186    Top1 87.187500    
2023-01-06 16:54:46,249 - Epoch: [18][   20/   28]    Loss 0.359155    Top1 87.519531    
2023-01-06 16:54:46,318 - Epoch: [18][   28/   28]    Loss 0.360465    Top1 87.288863    
2023-01-06 16:54:46,464 - ==> Top1: 87.289    Loss: 0.360

2023-01-06 16:54:46,464 - ==> Confusion:
[[ 136    2  301]
 [   5  106  491]
 [  58   31 5856]]

2023-01-06 16:54:46,466 - ==> Best [Top1: 87.289   Sparsity:0.00   Params: 155168 on epoch: 18]
2023-01-06 16:54:46,466 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:46,473 - 

2023-01-06 16:54:46,473 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:47,036 - Epoch: [19][   10/  246]    Overall Loss 0.372104    Objective Loss 0.372104                                        LR 0.000060    Time 0.056291    
2023-01-06 16:54:47,224 - Epoch: [19][   20/  246]    Overall Loss 0.380483    Objective Loss 0.380483                                        LR 0.000060    Time 0.037502    
2023-01-06 16:54:47,415 - Epoch: [19][   30/  246]    Overall Loss 0.367284    Objective Loss 0.367284                                        LR 0.000060    Time 0.031338    
2023-01-06 16:54:47,607 - Epoch: [19][   40/  246]    Overall Loss 0.370407    Objective Loss 0.370407                                        LR 0.000060    Time 0.028291    
2023-01-06 16:54:47,796 - Epoch: [19][   50/  246]    Overall Loss 0.370317    Objective Loss 0.370317                                        LR 0.000060    Time 0.026410    
2023-01-06 16:54:47,987 - Epoch: [19][   60/  246]    Overall Loss 0.372944    Objective Loss 0.372944                                        LR 0.000060    Time 0.025185    
2023-01-06 16:54:48,178 - Epoch: [19][   70/  246]    Overall Loss 0.369690    Objective Loss 0.369690                                        LR 0.000060    Time 0.024307    
2023-01-06 16:54:48,372 - Epoch: [19][   80/  246]    Overall Loss 0.370178    Objective Loss 0.370178                                        LR 0.000060    Time 0.023690    
2023-01-06 16:54:48,569 - Epoch: [19][   90/  246]    Overall Loss 0.369839    Objective Loss 0.369839                                        LR 0.000060    Time 0.023252    
2023-01-06 16:54:48,746 - Epoch: [19][  100/  246]    Overall Loss 0.371195    Objective Loss 0.371195                                        LR 0.000060    Time 0.022693    
2023-01-06 16:54:48,922 - Epoch: [19][  110/  246]    Overall Loss 0.372110    Objective Loss 0.372110                                        LR 0.000060    Time 0.022226    
2023-01-06 16:54:49,113 - Epoch: [19][  120/  246]    Overall Loss 0.371412    Objective Loss 0.371412                                        LR 0.000060    Time 0.021959    
2023-01-06 16:54:49,292 - Epoch: [19][  130/  246]    Overall Loss 0.372589    Objective Loss 0.372589                                        LR 0.000060    Time 0.021648    
2023-01-06 16:54:49,481 - Epoch: [19][  140/  246]    Overall Loss 0.373216    Objective Loss 0.373216                                        LR 0.000060    Time 0.021447    
2023-01-06 16:54:49,660 - Epoch: [19][  150/  246]    Overall Loss 0.373587    Objective Loss 0.373587                                        LR 0.000060    Time 0.021210    
2023-01-06 16:54:49,851 - Epoch: [19][  160/  246]    Overall Loss 0.372685    Objective Loss 0.372685                                        LR 0.000060    Time 0.021075    
2023-01-06 16:54:50,032 - Epoch: [19][  170/  246]    Overall Loss 0.371707    Objective Loss 0.371707                                        LR 0.000060    Time 0.020895    
2023-01-06 16:54:50,220 - Epoch: [19][  180/  246]    Overall Loss 0.370405    Objective Loss 0.370405                                        LR 0.000060    Time 0.020780    
2023-01-06 16:54:50,401 - Epoch: [19][  190/  246]    Overall Loss 0.370755    Objective Loss 0.370755                                        LR 0.000060    Time 0.020637    
2023-01-06 16:54:50,591 - Epoch: [19][  200/  246]    Overall Loss 0.370517    Objective Loss 0.370517                                        LR 0.000060    Time 0.020554    
2023-01-06 16:54:50,772 - Epoch: [19][  210/  246]    Overall Loss 0.371098    Objective Loss 0.371098                                        LR 0.000060    Time 0.020434    
2023-01-06 16:54:50,961 - Epoch: [19][  220/  246]    Overall Loss 0.369629    Objective Loss 0.369629                                        LR 0.000060    Time 0.020364    
2023-01-06 16:54:51,143 - Epoch: [19][  230/  246]    Overall Loss 0.369392    Objective Loss 0.369392                                        LR 0.000060    Time 0.020268    
2023-01-06 16:54:51,334 - Epoch: [19][  240/  246]    Overall Loss 0.370355    Objective Loss 0.370355                                        LR 0.000060    Time 0.020220    
2023-01-06 16:54:51,424 - Epoch: [19][  246/  246]    Overall Loss 0.370302    Objective Loss 0.370302    Top1 88.038278    LR 0.000060    Time 0.020089    
2023-01-06 16:54:51,580 - --- validate (epoch=19)-----------
2023-01-06 16:54:51,580 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:52,029 - Epoch: [19][   10/   28]    Loss 0.354891    Top1 86.953125    
2023-01-06 16:54:52,141 - Epoch: [19][   20/   28]    Loss 0.364040    Top1 86.621094    
2023-01-06 16:54:52,210 - Epoch: [19][   28/   28]    Loss 0.363930    Top1 86.444317    
2023-01-06 16:54:52,363 - ==> Top1: 86.444    Loss: 0.364

2023-01-06 16:54:52,363 - ==> Confusion:
[[  71    1  367]
 [   2   59  541]
 [  26   10 5909]]

2023-01-06 16:54:52,364 - ==> Best [Top1: 87.289   Sparsity:0.00   Params: 155168 on epoch: 18]
2023-01-06 16:54:52,364 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:52,370 - 

2023-01-06 16:54:52,370 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:53,052 - Epoch: [20][   10/  246]    Overall Loss 0.379830    Objective Loss 0.379830                                        LR 0.000060    Time 0.068098    
2023-01-06 16:54:53,227 - Epoch: [20][   20/  246]    Overall Loss 0.368726    Objective Loss 0.368726                                        LR 0.000060    Time 0.042784    
2023-01-06 16:54:53,417 - Epoch: [20][   30/  246]    Overall Loss 0.380716    Objective Loss 0.380716                                        LR 0.000060    Time 0.034834    
2023-01-06 16:54:53,611 - Epoch: [20][   40/  246]    Overall Loss 0.380683    Objective Loss 0.380683                                        LR 0.000060    Time 0.030964    
2023-01-06 16:54:53,807 - Epoch: [20][   50/  246]    Overall Loss 0.381646    Objective Loss 0.381646                                        LR 0.000060    Time 0.028690    
2023-01-06 16:54:54,003 - Epoch: [20][   60/  246]    Overall Loss 0.379877    Objective Loss 0.379877                                        LR 0.000060    Time 0.027176    
2023-01-06 16:54:54,200 - Epoch: [20][   70/  246]    Overall Loss 0.378974    Objective Loss 0.378974                                        LR 0.000060    Time 0.026101    
2023-01-06 16:54:54,396 - Epoch: [20][   80/  246]    Overall Loss 0.377372    Objective Loss 0.377372                                        LR 0.000060    Time 0.025277    
2023-01-06 16:54:54,593 - Epoch: [20][   90/  246]    Overall Loss 0.374926    Objective Loss 0.374926                                        LR 0.000060    Time 0.024653    
2023-01-06 16:54:54,788 - Epoch: [20][  100/  246]    Overall Loss 0.374861    Objective Loss 0.374861                                        LR 0.000060    Time 0.024142    
2023-01-06 16:54:54,986 - Epoch: [20][  110/  246]    Overall Loss 0.374686    Objective Loss 0.374686                                        LR 0.000060    Time 0.023739    
2023-01-06 16:54:55,181 - Epoch: [20][  120/  246]    Overall Loss 0.373681    Objective Loss 0.373681                                        LR 0.000060    Time 0.023385    
2023-01-06 16:54:55,377 - Epoch: [20][  130/  246]    Overall Loss 0.372698    Objective Loss 0.372698                                        LR 0.000060    Time 0.023092    
2023-01-06 16:54:55,572 - Epoch: [20][  140/  246]    Overall Loss 0.372378    Objective Loss 0.372378                                        LR 0.000060    Time 0.022831    
2023-01-06 16:54:55,770 - Epoch: [20][  150/  246]    Overall Loss 0.371300    Objective Loss 0.371300                                        LR 0.000060    Time 0.022624    
2023-01-06 16:54:55,963 - Epoch: [20][  160/  246]    Overall Loss 0.371407    Objective Loss 0.371407                                        LR 0.000060    Time 0.022419    
2023-01-06 16:54:56,166 - Epoch: [20][  170/  246]    Overall Loss 0.370699    Objective Loss 0.370699                                        LR 0.000060    Time 0.022289    
2023-01-06 16:54:56,366 - Epoch: [20][  180/  246]    Overall Loss 0.370032    Objective Loss 0.370032                                        LR 0.000060    Time 0.022158    
2023-01-06 16:54:56,572 - Epoch: [20][  190/  246]    Overall Loss 0.369450    Objective Loss 0.369450                                        LR 0.000060    Time 0.022079    
2023-01-06 16:54:56,767 - Epoch: [20][  200/  246]    Overall Loss 0.369315    Objective Loss 0.369315                                        LR 0.000060    Time 0.021946    
2023-01-06 16:54:56,965 - Epoch: [20][  210/  246]    Overall Loss 0.369959    Objective Loss 0.369959                                        LR 0.000060    Time 0.021841    
2023-01-06 16:54:57,161 - Epoch: [20][  220/  246]    Overall Loss 0.369587    Objective Loss 0.369587                                        LR 0.000060    Time 0.021738    
2023-01-06 16:54:57,357 - Epoch: [20][  230/  246]    Overall Loss 0.369912    Objective Loss 0.369912                                        LR 0.000060    Time 0.021644    
2023-01-06 16:54:57,564 - Epoch: [20][  240/  246]    Overall Loss 0.369288    Objective Loss 0.369288                                        LR 0.000060    Time 0.021605    
2023-01-06 16:54:57,661 - Epoch: [20][  246/  246]    Overall Loss 0.369124    Objective Loss 0.369124    Top1 86.842105    LR 0.000060    Time 0.021468    
2023-01-06 16:54:57,819 - --- validate (epoch=20)-----------
2023-01-06 16:54:57,819 - 6986 samples (256 per mini-batch)
2023-01-06 16:54:58,254 - Epoch: [20][   10/   28]    Loss 0.373228    Top1 86.054688    
2023-01-06 16:54:58,367 - Epoch: [20][   20/   28]    Loss 0.368497    Top1 86.210938    
2023-01-06 16:54:58,434 - Epoch: [20][   28/   28]    Loss 0.367039    Top1 86.172345    
2023-01-06 16:54:58,618 - ==> Top1: 86.172    Loss: 0.367

2023-01-06 16:54:58,619 - ==> Confusion:
[[  40    1  398]
 [   1   54  547]
 [  11    8 5926]]

2023-01-06 16:54:58,620 - ==> Best [Top1: 87.289   Sparsity:0.00   Params: 155168 on epoch: 18]
2023-01-06 16:54:58,620 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:54:58,626 - 

2023-01-06 16:54:58,626 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:54:59,323 - Epoch: [21][   10/  246]    Overall Loss 0.388831    Objective Loss 0.388831                                        LR 0.000060    Time 0.069586    
2023-01-06 16:54:59,486 - Epoch: [21][   20/  246]    Overall Loss 0.376191    Objective Loss 0.376191                                        LR 0.000060    Time 0.042914    
2023-01-06 16:54:59,646 - Epoch: [21][   30/  246]    Overall Loss 0.375066    Objective Loss 0.375066                                        LR 0.000060    Time 0.033914    
2023-01-06 16:54:59,819 - Epoch: [21][   40/  246]    Overall Loss 0.372353    Objective Loss 0.372353                                        LR 0.000060    Time 0.029757    
2023-01-06 16:55:00,001 - Epoch: [21][   50/  246]    Overall Loss 0.371413    Objective Loss 0.371413                                        LR 0.000060    Time 0.027442    
2023-01-06 16:55:00,174 - Epoch: [21][   60/  246]    Overall Loss 0.368837    Objective Loss 0.368837                                        LR 0.000060    Time 0.025741    
2023-01-06 16:55:00,353 - Epoch: [21][   70/  246]    Overall Loss 0.368559    Objective Loss 0.368559                                        LR 0.000060    Time 0.024617    
2023-01-06 16:55:00,546 - Epoch: [21][   80/  246]    Overall Loss 0.365543    Objective Loss 0.365543                                        LR 0.000060    Time 0.023939    
2023-01-06 16:55:00,729 - Epoch: [21][   90/  246]    Overall Loss 0.365681    Objective Loss 0.365681                                        LR 0.000060    Time 0.023317    
2023-01-06 16:55:00,904 - Epoch: [21][  100/  246]    Overall Loss 0.363473    Objective Loss 0.363473                                        LR 0.000060    Time 0.022731    
2023-01-06 16:55:01,071 - Epoch: [21][  110/  246]    Overall Loss 0.362175    Objective Loss 0.362175                                        LR 0.000060    Time 0.022182    
2023-01-06 16:55:01,253 - Epoch: [21][  120/  246]    Overall Loss 0.362866    Objective Loss 0.362866                                        LR 0.000060    Time 0.021840    
2023-01-06 16:55:01,420 - Epoch: [21][  130/  246]    Overall Loss 0.364608    Objective Loss 0.364608                                        LR 0.000060    Time 0.021446    
2023-01-06 16:55:01,586 - Epoch: [21][  140/  246]    Overall Loss 0.363347    Objective Loss 0.363347                                        LR 0.000060    Time 0.021095    
2023-01-06 16:55:01,752 - Epoch: [21][  150/  246]    Overall Loss 0.364593    Objective Loss 0.364593                                        LR 0.000060    Time 0.020791    
2023-01-06 16:55:01,925 - Epoch: [21][  160/  246]    Overall Loss 0.364969    Objective Loss 0.364969                                        LR 0.000060    Time 0.020574    
2023-01-06 16:55:02,094 - Epoch: [21][  170/  246]    Overall Loss 0.366070    Objective Loss 0.366070                                        LR 0.000060    Time 0.020357    
2023-01-06 16:55:02,290 - Epoch: [21][  180/  246]    Overall Loss 0.365978    Objective Loss 0.365978                                        LR 0.000060    Time 0.020311    
2023-01-06 16:55:02,513 - Epoch: [21][  190/  246]    Overall Loss 0.366315    Objective Loss 0.366315                                        LR 0.000060    Time 0.020413    
2023-01-06 16:55:02,738 - Epoch: [21][  200/  246]    Overall Loss 0.367146    Objective Loss 0.367146                                        LR 0.000060    Time 0.020516    
2023-01-06 16:55:02,968 - Epoch: [21][  210/  246]    Overall Loss 0.367106    Objective Loss 0.367106                                        LR 0.000060    Time 0.020633    
2023-01-06 16:55:03,200 - Epoch: [21][  220/  246]    Overall Loss 0.366999    Objective Loss 0.366999                                        LR 0.000060    Time 0.020748    
2023-01-06 16:55:03,429 - Epoch: [21][  230/  246]    Overall Loss 0.366532    Objective Loss 0.366532                                        LR 0.000060    Time 0.020837    
2023-01-06 16:55:03,661 - Epoch: [21][  240/  246]    Overall Loss 0.365646    Objective Loss 0.365646                                        LR 0.000060    Time 0.020934    
2023-01-06 16:55:03,757 - Epoch: [21][  246/  246]    Overall Loss 0.365619    Objective Loss 0.365619    Top1 86.124402    LR 0.000060    Time 0.020813    
2023-01-06 16:55:03,898 - --- validate (epoch=21)-----------
2023-01-06 16:55:03,899 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:04,340 - Epoch: [21][   10/   28]    Loss 0.363896    Top1 86.875000    
2023-01-06 16:55:04,459 - Epoch: [21][   20/   28]    Loss 0.354408    Top1 87.070312    
2023-01-06 16:55:04,530 - Epoch: [21][   28/   28]    Loss 0.354792    Top1 87.403378    
2023-01-06 16:55:04,689 - ==> Top1: 87.403    Loss: 0.355

2023-01-06 16:55:04,690 - ==> Confusion:
[[ 136    6  297]
 [   3  116  483]
 [  54   37 5854]]

2023-01-06 16:55:04,691 - ==> Best [Top1: 87.403   Sparsity:0.00   Params: 155168 on epoch: 21]
2023-01-06 16:55:04,691 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:04,698 - 

2023-01-06 16:55:04,698 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:05,286 - Epoch: [22][   10/  246]    Overall Loss 0.375356    Objective Loss 0.375356                                        LR 0.000060    Time 0.058657    
2023-01-06 16:55:05,489 - Epoch: [22][   20/  246]    Overall Loss 0.363608    Objective Loss 0.363608                                        LR 0.000060    Time 0.039452    
2023-01-06 16:55:05,693 - Epoch: [22][   30/  246]    Overall Loss 0.368613    Objective Loss 0.368613                                        LR 0.000060    Time 0.033098    
2023-01-06 16:55:05,891 - Epoch: [22][   40/  246]    Overall Loss 0.366317    Objective Loss 0.366317                                        LR 0.000060    Time 0.029760    
2023-01-06 16:55:06,087 - Epoch: [22][   50/  246]    Overall Loss 0.364883    Objective Loss 0.364883                                        LR 0.000060    Time 0.027717    
2023-01-06 16:55:06,284 - Epoch: [22][   60/  246]    Overall Loss 0.363771    Objective Loss 0.363771                                        LR 0.000060    Time 0.026385    
2023-01-06 16:55:06,478 - Epoch: [22][   70/  246]    Overall Loss 0.365319    Objective Loss 0.365319                                        LR 0.000060    Time 0.025381    
2023-01-06 16:55:06,671 - Epoch: [22][   80/  246]    Overall Loss 0.362494    Objective Loss 0.362494                                        LR 0.000060    Time 0.024613    
2023-01-06 16:55:06,865 - Epoch: [22][   90/  246]    Overall Loss 0.363565    Objective Loss 0.363565                                        LR 0.000060    Time 0.024027    
2023-01-06 16:55:07,058 - Epoch: [22][  100/  246]    Overall Loss 0.365386    Objective Loss 0.365386                                        LR 0.000060    Time 0.023553    
2023-01-06 16:55:07,250 - Epoch: [22][  110/  246]    Overall Loss 0.365153    Objective Loss 0.365153                                        LR 0.000060    Time 0.023158    
2023-01-06 16:55:07,443 - Epoch: [22][  120/  246]    Overall Loss 0.362319    Objective Loss 0.362319                                        LR 0.000060    Time 0.022830    
2023-01-06 16:55:07,636 - Epoch: [22][  130/  246]    Overall Loss 0.361913    Objective Loss 0.361913                                        LR 0.000060    Time 0.022555    
2023-01-06 16:55:07,828 - Epoch: [22][  140/  246]    Overall Loss 0.363143    Objective Loss 0.363143                                        LR 0.000060    Time 0.022317    
2023-01-06 16:55:08,021 - Epoch: [22][  150/  246]    Overall Loss 0.363701    Objective Loss 0.363701                                        LR 0.000060    Time 0.022110    
2023-01-06 16:55:08,214 - Epoch: [22][  160/  246]    Overall Loss 0.362746    Objective Loss 0.362746                                        LR 0.000060    Time 0.021934    
2023-01-06 16:55:08,408 - Epoch: [22][  170/  246]    Overall Loss 0.363026    Objective Loss 0.363026                                        LR 0.000060    Time 0.021779    
2023-01-06 16:55:08,599 - Epoch: [22][  180/  246]    Overall Loss 0.362489    Objective Loss 0.362489                                        LR 0.000060    Time 0.021633    
2023-01-06 16:55:08,792 - Epoch: [22][  190/  246]    Overall Loss 0.362654    Objective Loss 0.362654                                        LR 0.000060    Time 0.021506    
2023-01-06 16:55:08,984 - Epoch: [22][  200/  246]    Overall Loss 0.362917    Objective Loss 0.362917                                        LR 0.000060    Time 0.021391    
2023-01-06 16:55:09,177 - Epoch: [22][  210/  246]    Overall Loss 0.364316    Objective Loss 0.364316                                        LR 0.000060    Time 0.021288    
2023-01-06 16:55:09,370 - Epoch: [22][  220/  246]    Overall Loss 0.363917    Objective Loss 0.363917                                        LR 0.000060    Time 0.021195    
2023-01-06 16:55:09,562 - Epoch: [22][  230/  246]    Overall Loss 0.362982    Objective Loss 0.362982                                        LR 0.000060    Time 0.021108    
2023-01-06 16:55:09,768 - Epoch: [22][  240/  246]    Overall Loss 0.363253    Objective Loss 0.363253                                        LR 0.000060    Time 0.021086    
2023-01-06 16:55:09,864 - Epoch: [22][  246/  246]    Overall Loss 0.362832    Objective Loss 0.362832    Top1 85.885167    LR 0.000060    Time 0.020959    
2023-01-06 16:55:10,013 - --- validate (epoch=22)-----------
2023-01-06 16:55:10,013 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:10,472 - Epoch: [22][   10/   28]    Loss 0.354545    Top1 87.343750    
2023-01-06 16:55:10,585 - Epoch: [22][   20/   28]    Loss 0.355054    Top1 87.148438    
2023-01-06 16:55:10,654 - Epoch: [22][   28/   28]    Loss 0.352556    Top1 87.059834    
2023-01-06 16:55:10,792 - ==> Top1: 87.060    Loss: 0.353

2023-01-06 16:55:10,793 - ==> Confusion:
[[ 118    4  317]
 [   4   87  511]
 [  43   25 5877]]

2023-01-06 16:55:10,794 - ==> Best [Top1: 87.403   Sparsity:0.00   Params: 155168 on epoch: 21]
2023-01-06 16:55:10,794 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:10,800 - 

2023-01-06 16:55:10,800 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:11,537 - Epoch: [23][   10/  246]    Overall Loss 0.344663    Objective Loss 0.344663                                        LR 0.000060    Time 0.073680    
2023-01-06 16:55:11,738 - Epoch: [23][   20/  246]    Overall Loss 0.359536    Objective Loss 0.359536                                        LR 0.000060    Time 0.046840    
2023-01-06 16:55:11,942 - Epoch: [23][   30/  246]    Overall Loss 0.361020    Objective Loss 0.361020                                        LR 0.000060    Time 0.038020    
2023-01-06 16:55:12,136 - Epoch: [23][   40/  246]    Overall Loss 0.356787    Objective Loss 0.356787                                        LR 0.000060    Time 0.033366    
2023-01-06 16:55:12,341 - Epoch: [23][   50/  246]    Overall Loss 0.354771    Objective Loss 0.354771                                        LR 0.000060    Time 0.030776    
2023-01-06 16:55:12,535 - Epoch: [23][   60/  246]    Overall Loss 0.356247    Objective Loss 0.356247                                        LR 0.000060    Time 0.028874    
2023-01-06 16:55:12,736 - Epoch: [23][   70/  246]    Overall Loss 0.357450    Objective Loss 0.357450                                        LR 0.000060    Time 0.027613    
2023-01-06 16:55:12,930 - Epoch: [23][   80/  246]    Overall Loss 0.359063    Objective Loss 0.359063                                        LR 0.000060    Time 0.026587    
2023-01-06 16:55:13,128 - Epoch: [23][   90/  246]    Overall Loss 0.358903    Objective Loss 0.358903                                        LR 0.000060    Time 0.025824    
2023-01-06 16:55:13,322 - Epoch: [23][  100/  246]    Overall Loss 0.362166    Objective Loss 0.362166                                        LR 0.000060    Time 0.025183    
2023-01-06 16:55:13,523 - Epoch: [23][  110/  246]    Overall Loss 0.359967    Objective Loss 0.359967                                        LR 0.000060    Time 0.024715    
2023-01-06 16:55:13,714 - Epoch: [23][  120/  246]    Overall Loss 0.359923    Objective Loss 0.359923                                        LR 0.000060    Time 0.024245    
2023-01-06 16:55:13,907 - Epoch: [23][  130/  246]    Overall Loss 0.362038    Objective Loss 0.362038                                        LR 0.000060    Time 0.023859    
2023-01-06 16:55:14,099 - Epoch: [23][  140/  246]    Overall Loss 0.360110    Objective Loss 0.360110                                        LR 0.000060    Time 0.023518    
2023-01-06 16:55:14,300 - Epoch: [23][  150/  246]    Overall Loss 0.358190    Objective Loss 0.358190                                        LR 0.000060    Time 0.023284    
2023-01-06 16:55:14,494 - Epoch: [23][  160/  246]    Overall Loss 0.357400    Objective Loss 0.357400                                        LR 0.000060    Time 0.023041    
2023-01-06 16:55:14,694 - Epoch: [23][  170/  246]    Overall Loss 0.358271    Objective Loss 0.358271                                        LR 0.000060    Time 0.022860    
2023-01-06 16:55:14,892 - Epoch: [23][  180/  246]    Overall Loss 0.358455    Objective Loss 0.358455                                        LR 0.000060    Time 0.022689    
2023-01-06 16:55:15,092 - Epoch: [23][  190/  246]    Overall Loss 0.359344    Objective Loss 0.359344                                        LR 0.000060    Time 0.022544    
2023-01-06 16:55:15,286 - Epoch: [23][  200/  246]    Overall Loss 0.359580    Objective Loss 0.359580                                        LR 0.000060    Time 0.022386    
2023-01-06 16:55:15,486 - Epoch: [23][  210/  246]    Overall Loss 0.359799    Objective Loss 0.359799                                        LR 0.000060    Time 0.022269    
2023-01-06 16:55:15,680 - Epoch: [23][  220/  246]    Overall Loss 0.358694    Objective Loss 0.358694                                        LR 0.000060    Time 0.022137    
2023-01-06 16:55:15,880 - Epoch: [23][  230/  246]    Overall Loss 0.358322    Objective Loss 0.358322                                        LR 0.000060    Time 0.022042    
2023-01-06 16:55:16,085 - Epoch: [23][  240/  246]    Overall Loss 0.357737    Objective Loss 0.357737                                        LR 0.000060    Time 0.021978    
2023-01-06 16:55:16,181 - Epoch: [23][  246/  246]    Overall Loss 0.357665    Objective Loss 0.357665    Top1 86.363636    LR 0.000060    Time 0.021829    
2023-01-06 16:55:16,338 - --- validate (epoch=23)-----------
2023-01-06 16:55:16,338 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:16,801 - Epoch: [23][   10/   28]    Loss 0.356358    Top1 87.382812    
2023-01-06 16:55:16,927 - Epoch: [23][   20/   28]    Loss 0.351518    Top1 87.460938    
2023-01-06 16:55:16,994 - Epoch: [23][   28/   28]    Loss 0.355858    Top1 87.188663    
2023-01-06 16:55:17,124 - ==> Top1: 87.189    Loss: 0.356

2023-01-06 16:55:17,124 - ==> Confusion:
[[ 125    5  309]
 [   9  109  484]
 [  58   30 5857]]

2023-01-06 16:55:17,125 - ==> Best [Top1: 87.403   Sparsity:0.00   Params: 155168 on epoch: 21]
2023-01-06 16:55:17,125 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:17,131 - 

2023-01-06 16:55:17,131 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:17,715 - Epoch: [24][   10/  246]    Overall Loss 0.361643    Objective Loss 0.361643                                        LR 0.000060    Time 0.058373    
2023-01-06 16:55:17,909 - Epoch: [24][   20/  246]    Overall Loss 0.355658    Objective Loss 0.355658                                        LR 0.000060    Time 0.038855    
2023-01-06 16:55:18,103 - Epoch: [24][   30/  246]    Overall Loss 0.354811    Objective Loss 0.354811                                        LR 0.000060    Time 0.032342    
2023-01-06 16:55:18,295 - Epoch: [24][   40/  246]    Overall Loss 0.354994    Objective Loss 0.354994                                        LR 0.000060    Time 0.029045    
2023-01-06 16:55:18,487 - Epoch: [24][   50/  246]    Overall Loss 0.356843    Objective Loss 0.356843                                        LR 0.000060    Time 0.027074    
2023-01-06 16:55:18,678 - Epoch: [24][   60/  246]    Overall Loss 0.356372    Objective Loss 0.356372                                        LR 0.000060    Time 0.025745    
2023-01-06 16:55:18,875 - Epoch: [24][   70/  246]    Overall Loss 0.357148    Objective Loss 0.357148                                        LR 0.000060    Time 0.024876    
2023-01-06 16:55:19,071 - Epoch: [24][   80/  246]    Overall Loss 0.358035    Objective Loss 0.358035                                        LR 0.000060    Time 0.024203    
2023-01-06 16:55:19,267 - Epoch: [24][   90/  246]    Overall Loss 0.361376    Objective Loss 0.361376                                        LR 0.000060    Time 0.023698    
2023-01-06 16:55:19,462 - Epoch: [24][  100/  246]    Overall Loss 0.360632    Objective Loss 0.360632                                        LR 0.000060    Time 0.023270    
2023-01-06 16:55:19,658 - Epoch: [24][  110/  246]    Overall Loss 0.358127    Objective Loss 0.358127                                        LR 0.000060    Time 0.022937    
2023-01-06 16:55:19,854 - Epoch: [24][  120/  246]    Overall Loss 0.359033    Objective Loss 0.359033                                        LR 0.000060    Time 0.022649    
2023-01-06 16:55:20,055 - Epoch: [24][  130/  246]    Overall Loss 0.358716    Objective Loss 0.358716                                        LR 0.000060    Time 0.022453    
2023-01-06 16:55:20,252 - Epoch: [24][  140/  246]    Overall Loss 0.357624    Objective Loss 0.357624                                        LR 0.000060    Time 0.022252    
2023-01-06 16:55:20,456 - Epoch: [24][  150/  246]    Overall Loss 0.356942    Objective Loss 0.356942                                        LR 0.000060    Time 0.022129    
2023-01-06 16:55:20,658 - Epoch: [24][  160/  246]    Overall Loss 0.356941    Objective Loss 0.356941                                        LR 0.000060    Time 0.022005    
2023-01-06 16:55:20,864 - Epoch: [24][  170/  246]    Overall Loss 0.356524    Objective Loss 0.356524                                        LR 0.000060    Time 0.021922    
2023-01-06 16:55:21,066 - Epoch: [24][  180/  246]    Overall Loss 0.357029    Objective Loss 0.357029                                        LR 0.000060    Time 0.021824    
2023-01-06 16:55:21,263 - Epoch: [24][  190/  246]    Overall Loss 0.357333    Objective Loss 0.357333                                        LR 0.000060    Time 0.021709    
2023-01-06 16:55:21,462 - Epoch: [24][  200/  246]    Overall Loss 0.356257    Objective Loss 0.356257                                        LR 0.000060    Time 0.021618    
2023-01-06 16:55:21,659 - Epoch: [24][  210/  246]    Overall Loss 0.355363    Objective Loss 0.355363                                        LR 0.000060    Time 0.021521    
2023-01-06 16:55:21,847 - Epoch: [24][  220/  246]    Overall Loss 0.355813    Objective Loss 0.355813                                        LR 0.000060    Time 0.021400    
2023-01-06 16:55:22,043 - Epoch: [24][  230/  246]    Overall Loss 0.356362    Objective Loss 0.356362                                        LR 0.000060    Time 0.021318    
2023-01-06 16:55:22,250 - Epoch: [24][  240/  246]    Overall Loss 0.356407    Objective Loss 0.356407                                        LR 0.000060    Time 0.021292    
2023-01-06 16:55:22,345 - Epoch: [24][  246/  246]    Overall Loss 0.355984    Objective Loss 0.355984    Top1 88.755981    LR 0.000060    Time 0.021157    
2023-01-06 16:55:22,486 - --- validate (epoch=24)-----------
2023-01-06 16:55:22,486 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:22,944 - Epoch: [24][   10/   28]    Loss 0.369654    Top1 86.171875    
2023-01-06 16:55:23,061 - Epoch: [24][   20/   28]    Loss 0.352553    Top1 86.679688    
2023-01-06 16:55:23,128 - Epoch: [24][   28/   28]    Loss 0.347521    Top1 86.959634    
2023-01-06 16:55:23,263 - ==> Top1: 86.960    Loss: 0.348

2023-01-06 16:55:23,264 - ==> Confusion:
[[  92    2  345]
 [   3   80  519]
 [  22   20 5903]]

2023-01-06 16:55:23,265 - ==> Best [Top1: 87.403   Sparsity:0.00   Params: 155168 on epoch: 21]
2023-01-06 16:55:23,265 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:23,271 - 

2023-01-06 16:55:23,271 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:23,969 - Epoch: [25][   10/  246]    Overall Loss 0.349694    Objective Loss 0.349694                                        LR 0.000060    Time 0.069751    
2023-01-06 16:55:24,163 - Epoch: [25][   20/  246]    Overall Loss 0.360861    Objective Loss 0.360861                                        LR 0.000060    Time 0.044564    
2023-01-06 16:55:24,357 - Epoch: [25][   30/  246]    Overall Loss 0.360582    Objective Loss 0.360582                                        LR 0.000060    Time 0.036142    
2023-01-06 16:55:24,549 - Epoch: [25][   40/  246]    Overall Loss 0.356350    Objective Loss 0.356350                                        LR 0.000060    Time 0.031917    
2023-01-06 16:55:24,741 - Epoch: [25][   50/  246]    Overall Loss 0.355449    Objective Loss 0.355449                                        LR 0.000060    Time 0.029367    
2023-01-06 16:55:24,933 - Epoch: [25][   60/  246]    Overall Loss 0.353301    Objective Loss 0.353301                                        LR 0.000060    Time 0.027659    
2023-01-06 16:55:25,124 - Epoch: [25][   70/  246]    Overall Loss 0.352211    Objective Loss 0.352211                                        LR 0.000060    Time 0.026430    
2023-01-06 16:55:25,315 - Epoch: [25][   80/  246]    Overall Loss 0.355037    Objective Loss 0.355037                                        LR 0.000060    Time 0.025511    
2023-01-06 16:55:25,505 - Epoch: [25][   90/  246]    Overall Loss 0.356295    Objective Loss 0.356295                                        LR 0.000060    Time 0.024785    
2023-01-06 16:55:25,695 - Epoch: [25][  100/  246]    Overall Loss 0.355744    Objective Loss 0.355744                                        LR 0.000060    Time 0.024206    
2023-01-06 16:55:25,886 - Epoch: [25][  110/  246]    Overall Loss 0.355337    Objective Loss 0.355337                                        LR 0.000060    Time 0.023733    
2023-01-06 16:55:26,078 - Epoch: [25][  120/  246]    Overall Loss 0.354987    Objective Loss 0.354987                                        LR 0.000060    Time 0.023353    
2023-01-06 16:55:26,269 - Epoch: [25][  130/  246]    Overall Loss 0.354595    Objective Loss 0.354595                                        LR 0.000060    Time 0.023023    
2023-01-06 16:55:26,459 - Epoch: [25][  140/  246]    Overall Loss 0.354227    Objective Loss 0.354227                                        LR 0.000060    Time 0.022735    
2023-01-06 16:55:26,650 - Epoch: [25][  150/  246]    Overall Loss 0.354728    Objective Loss 0.354728                                        LR 0.000060    Time 0.022489    
2023-01-06 16:55:26,841 - Epoch: [25][  160/  246]    Overall Loss 0.354869    Objective Loss 0.354869                                        LR 0.000060    Time 0.022273    
2023-01-06 16:55:27,032 - Epoch: [25][  170/  246]    Overall Loss 0.356232    Objective Loss 0.356232                                        LR 0.000060    Time 0.022085    
2023-01-06 16:55:27,223 - Epoch: [25][  180/  246]    Overall Loss 0.357076    Objective Loss 0.357076                                        LR 0.000060    Time 0.021920    
2023-01-06 16:55:27,414 - Epoch: [25][  190/  246]    Overall Loss 0.357296    Objective Loss 0.357296                                        LR 0.000060    Time 0.021768    
2023-01-06 16:55:27,605 - Epoch: [25][  200/  246]    Overall Loss 0.356073    Objective Loss 0.356073                                        LR 0.000060    Time 0.021633    
2023-01-06 16:55:27,796 - Epoch: [25][  210/  246]    Overall Loss 0.356126    Objective Loss 0.356126                                        LR 0.000060    Time 0.021511    
2023-01-06 16:55:27,987 - Epoch: [25][  220/  246]    Overall Loss 0.356148    Objective Loss 0.356148                                        LR 0.000060    Time 0.021401    
2023-01-06 16:55:28,179 - Epoch: [25][  230/  246]    Overall Loss 0.354155    Objective Loss 0.354155                                        LR 0.000060    Time 0.021303    
2023-01-06 16:55:28,385 - Epoch: [25][  240/  246]    Overall Loss 0.354222    Objective Loss 0.354222                                        LR 0.000060    Time 0.021272    
2023-01-06 16:55:28,479 - Epoch: [25][  246/  246]    Overall Loss 0.353965    Objective Loss 0.353965    Top1 86.602871    LR 0.000060    Time 0.021135    
2023-01-06 16:55:28,608 - --- validate (epoch=25)-----------
2023-01-06 16:55:28,608 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:29,060 - Epoch: [25][   10/   28]    Loss 0.354802    Top1 87.617188    
2023-01-06 16:55:29,171 - Epoch: [25][   20/   28]    Loss 0.351498    Top1 87.519531    
2023-01-06 16:55:29,238 - Epoch: [25][   28/   28]    Loss 0.339830    Top1 87.804180    
2023-01-06 16:55:29,387 - ==> Top1: 87.804    Loss: 0.340

2023-01-06 16:55:29,387 - ==> Confusion:
[[ 174    5  260]
 [  10  128  464]
 [  77   36 5832]]

2023-01-06 16:55:29,388 - ==> Best [Top1: 87.804   Sparsity:0.00   Params: 155168 on epoch: 25]
2023-01-06 16:55:29,388 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:29,396 - 

2023-01-06 16:55:29,396 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:29,975 - Epoch: [26][   10/  246]    Overall Loss 0.349858    Objective Loss 0.349858                                        LR 0.000060    Time 0.057876    
2023-01-06 16:55:30,174 - Epoch: [26][   20/  246]    Overall Loss 0.347283    Objective Loss 0.347283                                        LR 0.000060    Time 0.038854    
2023-01-06 16:55:30,382 - Epoch: [26][   30/  246]    Overall Loss 0.349278    Objective Loss 0.349278                                        LR 0.000060    Time 0.032768    
2023-01-06 16:55:30,592 - Epoch: [26][   40/  246]    Overall Loss 0.348481    Objective Loss 0.348481                                        LR 0.000060    Time 0.029809    
2023-01-06 16:55:30,797 - Epoch: [26][   50/  246]    Overall Loss 0.350314    Objective Loss 0.350314                                        LR 0.000060    Time 0.027941    
2023-01-06 16:55:31,006 - Epoch: [26][   60/  246]    Overall Loss 0.349494    Objective Loss 0.349494                                        LR 0.000060    Time 0.026762    
2023-01-06 16:55:31,214 - Epoch: [26][   70/  246]    Overall Loss 0.351765    Objective Loss 0.351765                                        LR 0.000060    Time 0.025904    
2023-01-06 16:55:31,426 - Epoch: [26][   80/  246]    Overall Loss 0.352373    Objective Loss 0.352373                                        LR 0.000060    Time 0.025318    
2023-01-06 16:55:31,637 - Epoch: [26][   90/  246]    Overall Loss 0.352538    Objective Loss 0.352538                                        LR 0.000060    Time 0.024845    
2023-01-06 16:55:31,848 - Epoch: [26][  100/  246]    Overall Loss 0.354381    Objective Loss 0.354381                                        LR 0.000060    Time 0.024461    
2023-01-06 16:55:32,058 - Epoch: [26][  110/  246]    Overall Loss 0.354501    Objective Loss 0.354501                                        LR 0.000060    Time 0.024144    
2023-01-06 16:55:32,269 - Epoch: [26][  120/  246]    Overall Loss 0.354952    Objective Loss 0.354952                                        LR 0.000060    Time 0.023888    
2023-01-06 16:55:32,479 - Epoch: [26][  130/  246]    Overall Loss 0.352650    Objective Loss 0.352650                                        LR 0.000060    Time 0.023661    
2023-01-06 16:55:32,690 - Epoch: [26][  140/  246]    Overall Loss 0.353112    Objective Loss 0.353112                                        LR 0.000060    Time 0.023478    
2023-01-06 16:55:32,899 - Epoch: [26][  150/  246]    Overall Loss 0.352777    Objective Loss 0.352777                                        LR 0.000060    Time 0.023302    
2023-01-06 16:55:33,111 - Epoch: [26][  160/  246]    Overall Loss 0.353587    Objective Loss 0.353587                                        LR 0.000060    Time 0.023172    
2023-01-06 16:55:33,318 - Epoch: [26][  170/  246]    Overall Loss 0.352535    Objective Loss 0.352535                                        LR 0.000060    Time 0.023025    
2023-01-06 16:55:33,530 - Epoch: [26][  180/  246]    Overall Loss 0.352634    Objective Loss 0.352634                                        LR 0.000060    Time 0.022919    
2023-01-06 16:55:33,735 - Epoch: [26][  190/  246]    Overall Loss 0.351726    Objective Loss 0.351726                                        LR 0.000060    Time 0.022790    
2023-01-06 16:55:33,947 - Epoch: [26][  200/  246]    Overall Loss 0.352921    Objective Loss 0.352921                                        LR 0.000060    Time 0.022706    
2023-01-06 16:55:34,154 - Epoch: [26][  210/  246]    Overall Loss 0.352899    Objective Loss 0.352899                                        LR 0.000060    Time 0.022610    
2023-01-06 16:55:34,359 - Epoch: [26][  220/  246]    Overall Loss 0.352731    Objective Loss 0.352731                                        LR 0.000060    Time 0.022512    
2023-01-06 16:55:34,567 - Epoch: [26][  230/  246]    Overall Loss 0.352600    Objective Loss 0.352600                                        LR 0.000060    Time 0.022436    
2023-01-06 16:55:34,778 - Epoch: [26][  240/  246]    Overall Loss 0.352161    Objective Loss 0.352161                                        LR 0.000060    Time 0.022381    
2023-01-06 16:55:34,874 - Epoch: [26][  246/  246]    Overall Loss 0.351876    Objective Loss 0.351876    Top1 86.363636    LR 0.000060    Time 0.022222    
2023-01-06 16:55:35,031 - --- validate (epoch=26)-----------
2023-01-06 16:55:35,032 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:35,759 - Epoch: [26][   10/   28]    Loss 0.332937    Top1 88.281250    
2023-01-06 16:55:35,869 - Epoch: [26][   20/   28]    Loss 0.341701    Top1 87.675781    
2023-01-06 16:55:35,939 - Epoch: [26][   28/   28]    Loss 0.342035    Top1 87.832808    
2023-01-06 16:55:36,100 - ==> Top1: 87.833    Loss: 0.342

2023-01-06 16:55:36,100 - ==> Confusion:
[[ 136   11  292]
 [   4  165  433]
 [  54   56 5835]]

2023-01-06 16:55:36,101 - ==> Best [Top1: 87.833   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-06 16:55:36,101 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:36,109 - 

2023-01-06 16:55:36,109 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:36,678 - Epoch: [27][   10/  246]    Overall Loss 0.347547    Objective Loss 0.347547                                        LR 0.000060    Time 0.056886    
2023-01-06 16:55:36,872 - Epoch: [27][   20/  246]    Overall Loss 0.342045    Objective Loss 0.342045                                        LR 0.000060    Time 0.038109    
2023-01-06 16:55:37,072 - Epoch: [27][   30/  246]    Overall Loss 0.348244    Objective Loss 0.348244                                        LR 0.000060    Time 0.032055    
2023-01-06 16:55:37,295 - Epoch: [27][   40/  246]    Overall Loss 0.345445    Objective Loss 0.345445                                        LR 0.000060    Time 0.029615    
2023-01-06 16:55:37,520 - Epoch: [27][   50/  246]    Overall Loss 0.350078    Objective Loss 0.350078                                        LR 0.000060    Time 0.028185    
2023-01-06 16:55:37,755 - Epoch: [27][   60/  246]    Overall Loss 0.348737    Objective Loss 0.348737                                        LR 0.000060    Time 0.027390    
2023-01-06 16:55:37,986 - Epoch: [27][   70/  246]    Overall Loss 0.350892    Objective Loss 0.350892                                        LR 0.000060    Time 0.026765    
2023-01-06 16:55:38,219 - Epoch: [27][   80/  246]    Overall Loss 0.352138    Objective Loss 0.352138                                        LR 0.000060    Time 0.026327    
2023-01-06 16:55:38,460 - Epoch: [27][   90/  246]    Overall Loss 0.351529    Objective Loss 0.351529                                        LR 0.000060    Time 0.026083    
2023-01-06 16:55:38,709 - Epoch: [27][  100/  246]    Overall Loss 0.349339    Objective Loss 0.349339                                        LR 0.000060    Time 0.025952    
2023-01-06 16:55:38,954 - Epoch: [27][  110/  246]    Overall Loss 0.350794    Objective Loss 0.350794                                        LR 0.000060    Time 0.025811    
2023-01-06 16:55:39,175 - Epoch: [27][  120/  246]    Overall Loss 0.351383    Objective Loss 0.351383                                        LR 0.000060    Time 0.025499    
2023-01-06 16:55:39,376 - Epoch: [27][  130/  246]    Overall Loss 0.351150    Objective Loss 0.351150                                        LR 0.000060    Time 0.025077    
2023-01-06 16:55:39,593 - Epoch: [27][  140/  246]    Overall Loss 0.350033    Objective Loss 0.350033                                        LR 0.000060    Time 0.024833    
2023-01-06 16:55:39,808 - Epoch: [27][  150/  246]    Overall Loss 0.349110    Objective Loss 0.349110                                        LR 0.000060    Time 0.024608    
2023-01-06 16:55:40,023 - Epoch: [27][  160/  246]    Overall Loss 0.349165    Objective Loss 0.349165                                        LR 0.000060    Time 0.024410    
2023-01-06 16:55:40,223 - Epoch: [27][  170/  246]    Overall Loss 0.349601    Objective Loss 0.349601                                        LR 0.000060    Time 0.024151    
2023-01-06 16:55:40,425 - Epoch: [27][  180/  246]    Overall Loss 0.348790    Objective Loss 0.348790                                        LR 0.000060    Time 0.023927    
2023-01-06 16:55:40,635 - Epoch: [27][  190/  246]    Overall Loss 0.349587    Objective Loss 0.349587                                        LR 0.000060    Time 0.023772    
2023-01-06 16:55:40,845 - Epoch: [27][  200/  246]    Overall Loss 0.348280    Objective Loss 0.348280                                        LR 0.000060    Time 0.023631    
2023-01-06 16:55:41,053 - Epoch: [27][  210/  246]    Overall Loss 0.349598    Objective Loss 0.349598                                        LR 0.000060    Time 0.023495    
2023-01-06 16:55:41,256 - Epoch: [27][  220/  246]    Overall Loss 0.349401    Objective Loss 0.349401                                        LR 0.000060    Time 0.023346    
2023-01-06 16:55:41,461 - Epoch: [27][  230/  246]    Overall Loss 0.349820    Objective Loss 0.349820                                        LR 0.000060    Time 0.023222    
2023-01-06 16:55:41,680 - Epoch: [27][  240/  246]    Overall Loss 0.350670    Objective Loss 0.350670                                        LR 0.000060    Time 0.023164    
2023-01-06 16:55:41,777 - Epoch: [27][  246/  246]    Overall Loss 0.350807    Objective Loss 0.350807    Top1 86.602871    LR 0.000060    Time 0.022993    
2023-01-06 16:55:41,912 - --- validate (epoch=27)-----------
2023-01-06 16:55:41,913 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:42,362 - Epoch: [27][   10/   28]    Loss 0.328050    Top1 88.242188    
2023-01-06 16:55:42,476 - Epoch: [27][   20/   28]    Loss 0.341378    Top1 87.480469    
2023-01-06 16:55:42,543 - Epoch: [27][   28/   28]    Loss 0.345129    Top1 87.432007    
2023-01-06 16:55:42,672 - ==> Top1: 87.432    Loss: 0.345

2023-01-06 16:55:42,672 - ==> Confusion:
[[ 144    3  292]
 [   5   93  504]
 [  57   17 5871]]

2023-01-06 16:55:42,673 - ==> Best [Top1: 87.833   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-06 16:55:42,674 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:42,679 - 

2023-01-06 16:55:42,680 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:43,387 - Epoch: [28][   10/  246]    Overall Loss 0.347408    Objective Loss 0.347408                                        LR 0.000060    Time 0.070712    
2023-01-06 16:55:43,596 - Epoch: [28][   20/  246]    Overall Loss 0.332147    Objective Loss 0.332147                                        LR 0.000060    Time 0.045764    
2023-01-06 16:55:43,804 - Epoch: [28][   30/  246]    Overall Loss 0.333130    Objective Loss 0.333130                                        LR 0.000060    Time 0.037419    
2023-01-06 16:55:44,009 - Epoch: [28][   40/  246]    Overall Loss 0.333998    Objective Loss 0.333998                                        LR 0.000060    Time 0.033182    
2023-01-06 16:55:44,220 - Epoch: [28][   50/  246]    Overall Loss 0.334949    Objective Loss 0.334949                                        LR 0.000060    Time 0.030766    
2023-01-06 16:55:44,427 - Epoch: [28][   60/  246]    Overall Loss 0.338723    Objective Loss 0.338723                                        LR 0.000060    Time 0.029084    
2023-01-06 16:55:44,640 - Epoch: [28][   70/  246]    Overall Loss 0.340710    Objective Loss 0.340710                                        LR 0.000060    Time 0.027960    
2023-01-06 16:55:44,846 - Epoch: [28][   80/  246]    Overall Loss 0.338081    Objective Loss 0.338081                                        LR 0.000060    Time 0.027034    
2023-01-06 16:55:45,057 - Epoch: [28][   90/  246]    Overall Loss 0.339716    Objective Loss 0.339716                                        LR 0.000060    Time 0.026371    
2023-01-06 16:55:45,258 - Epoch: [28][  100/  246]    Overall Loss 0.341815    Objective Loss 0.341815                                        LR 0.000060    Time 0.025744    
2023-01-06 16:55:45,464 - Epoch: [28][  110/  246]    Overall Loss 0.341874    Objective Loss 0.341874                                        LR 0.000060    Time 0.025272    
2023-01-06 16:55:45,664 - Epoch: [28][  120/  246]    Overall Loss 0.340077    Objective Loss 0.340077                                        LR 0.000060    Time 0.024834    
2023-01-06 16:55:45,871 - Epoch: [28][  130/  246]    Overall Loss 0.339404    Objective Loss 0.339404                                        LR 0.000060    Time 0.024508    
2023-01-06 16:55:46,072 - Epoch: [28][  140/  246]    Overall Loss 0.341200    Objective Loss 0.341200                                        LR 0.000060    Time 0.024194    
2023-01-06 16:55:46,276 - Epoch: [28][  150/  246]    Overall Loss 0.341560    Objective Loss 0.341560                                        LR 0.000060    Time 0.023936    
2023-01-06 16:55:46,478 - Epoch: [28][  160/  246]    Overall Loss 0.340493    Objective Loss 0.340493                                        LR 0.000060    Time 0.023703    
2023-01-06 16:55:46,699 - Epoch: [28][  170/  246]    Overall Loss 0.341711    Objective Loss 0.341711                                        LR 0.000060    Time 0.023602    
2023-01-06 16:55:46,906 - Epoch: [28][  180/  246]    Overall Loss 0.342208    Objective Loss 0.342208                                        LR 0.000060    Time 0.023441    
2023-01-06 16:55:47,105 - Epoch: [28][  190/  246]    Overall Loss 0.342473    Objective Loss 0.342473                                        LR 0.000060    Time 0.023256    
2023-01-06 16:55:47,298 - Epoch: [28][  200/  246]    Overall Loss 0.343911    Objective Loss 0.343911                                        LR 0.000060    Time 0.023055    
2023-01-06 16:55:47,497 - Epoch: [28][  210/  246]    Overall Loss 0.344693    Objective Loss 0.344693                                        LR 0.000060    Time 0.022900    
2023-01-06 16:55:47,695 - Epoch: [28][  220/  246]    Overall Loss 0.344393    Objective Loss 0.344393                                        LR 0.000060    Time 0.022761    
2023-01-06 16:55:47,896 - Epoch: [28][  230/  246]    Overall Loss 0.345266    Objective Loss 0.345266                                        LR 0.000060    Time 0.022643    
2023-01-06 16:55:48,104 - Epoch: [28][  240/  246]    Overall Loss 0.345839    Objective Loss 0.345839                                        LR 0.000060    Time 0.022565    
2023-01-06 16:55:48,199 - Epoch: [28][  246/  246]    Overall Loss 0.345742    Objective Loss 0.345742    Top1 88.755981    LR 0.000060    Time 0.022398    
2023-01-06 16:55:48,342 - --- validate (epoch=28)-----------
2023-01-06 16:55:48,342 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:48,781 - Epoch: [28][   10/   28]    Loss 0.383060    Top1 86.328125    
2023-01-06 16:55:48,900 - Epoch: [28][   20/   28]    Loss 0.363138    Top1 86.953125    
2023-01-06 16:55:48,968 - Epoch: [28][   28/   28]    Loss 0.361895    Top1 87.117091    
2023-01-06 16:55:49,128 - ==> Top1: 87.117    Loss: 0.362

2023-01-06 16:55:49,129 - ==> Confusion:
[[ 162    8  269]
 [  17  128  457]
 [ 102   47 5796]]

2023-01-06 16:55:49,130 - ==> Best [Top1: 87.833   Sparsity:0.00   Params: 155168 on epoch: 26]
2023-01-06 16:55:49,130 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:49,136 - 

2023-01-06 16:55:49,136 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:49,716 - Epoch: [29][   10/  246]    Overall Loss 0.363488    Objective Loss 0.363488                                        LR 0.000060    Time 0.058005    
2023-01-06 16:55:49,927 - Epoch: [29][   20/  246]    Overall Loss 0.356184    Objective Loss 0.356184                                        LR 0.000060    Time 0.039530    
2023-01-06 16:55:50,121 - Epoch: [29][   30/  246]    Overall Loss 0.351042    Objective Loss 0.351042                                        LR 0.000060    Time 0.032802    
2023-01-06 16:55:50,319 - Epoch: [29][   40/  246]    Overall Loss 0.348050    Objective Loss 0.348050                                        LR 0.000060    Time 0.029523    
2023-01-06 16:55:50,507 - Epoch: [29][   50/  246]    Overall Loss 0.349857    Objective Loss 0.349857                                        LR 0.000060    Time 0.027378    
2023-01-06 16:55:50,695 - Epoch: [29][   60/  246]    Overall Loss 0.344548    Objective Loss 0.344548                                        LR 0.000060    Time 0.025950    
2023-01-06 16:55:50,884 - Epoch: [29][   70/  246]    Overall Loss 0.344065    Objective Loss 0.344065                                        LR 0.000060    Time 0.024925    
2023-01-06 16:55:51,074 - Epoch: [29][   80/  246]    Overall Loss 0.343096    Objective Loss 0.343096                                        LR 0.000060    Time 0.024180    
2023-01-06 16:55:51,258 - Epoch: [29][   90/  246]    Overall Loss 0.341989    Objective Loss 0.341989                                        LR 0.000060    Time 0.023541    
2023-01-06 16:55:51,453 - Epoch: [29][  100/  246]    Overall Loss 0.340438    Objective Loss 0.340438                                        LR 0.000060    Time 0.023127    
2023-01-06 16:55:51,646 - Epoch: [29][  110/  246]    Overall Loss 0.340726    Objective Loss 0.340726                                        LR 0.000060    Time 0.022778    
2023-01-06 16:55:51,841 - Epoch: [29][  120/  246]    Overall Loss 0.338509    Objective Loss 0.338509                                        LR 0.000060    Time 0.022501    
2023-01-06 16:55:52,031 - Epoch: [29][  130/  246]    Overall Loss 0.337888    Objective Loss 0.337888                                        LR 0.000060    Time 0.022231    
2023-01-06 16:55:52,223 - Epoch: [29][  140/  246]    Overall Loss 0.338000    Objective Loss 0.338000                                        LR 0.000060    Time 0.022010    
2023-01-06 16:55:52,414 - Epoch: [29][  150/  246]    Overall Loss 0.338815    Objective Loss 0.338815                                        LR 0.000060    Time 0.021817    
2023-01-06 16:55:52,606 - Epoch: [29][  160/  246]    Overall Loss 0.340550    Objective Loss 0.340550                                        LR 0.000060    Time 0.021648    
2023-01-06 16:55:52,797 - Epoch: [29][  170/  246]    Overall Loss 0.340942    Objective Loss 0.340942                                        LR 0.000060    Time 0.021500    
2023-01-06 16:55:52,994 - Epoch: [29][  180/  246]    Overall Loss 0.341981    Objective Loss 0.341981                                        LR 0.000060    Time 0.021394    
2023-01-06 16:55:53,184 - Epoch: [29][  190/  246]    Overall Loss 0.341981    Objective Loss 0.341981                                        LR 0.000060    Time 0.021266    
2023-01-06 16:55:53,378 - Epoch: [29][  200/  246]    Overall Loss 0.342768    Objective Loss 0.342768                                        LR 0.000060    Time 0.021170    
2023-01-06 16:55:53,583 - Epoch: [29][  210/  246]    Overall Loss 0.343706    Objective Loss 0.343706                                        LR 0.000060    Time 0.021139    
2023-01-06 16:55:53,810 - Epoch: [29][  220/  246]    Overall Loss 0.344480    Objective Loss 0.344480                                        LR 0.000060    Time 0.021208    
2023-01-06 16:55:54,038 - Epoch: [29][  230/  246]    Overall Loss 0.345190    Objective Loss 0.345190                                        LR 0.000060    Time 0.021274    
2023-01-06 16:55:54,256 - Epoch: [29][  240/  246]    Overall Loss 0.345657    Objective Loss 0.345657                                        LR 0.000060    Time 0.021296    
2023-01-06 16:55:54,351 - Epoch: [29][  246/  246]    Overall Loss 0.345338    Objective Loss 0.345338    Top1 88.516746    LR 0.000060    Time 0.021163    
2023-01-06 16:55:54,482 - --- validate (epoch=29)-----------
2023-01-06 16:55:54,482 - 6986 samples (256 per mini-batch)
2023-01-06 16:55:54,928 - Epoch: [29][   10/   28]    Loss 0.325290    Top1 88.320312    
2023-01-06 16:55:55,048 - Epoch: [29][   20/   28]    Loss 0.329847    Top1 88.066406    
2023-01-06 16:55:55,118 - Epoch: [29][   28/   28]    Loss 0.337904    Top1 87.890066    
2023-01-06 16:55:55,271 - ==> Top1: 87.890    Loss: 0.338

2023-01-06 16:55:55,272 - ==> Confusion:
[[ 146    6  287]
 [   9  130  463]
 [  49   32 5864]]

2023-01-06 16:55:55,273 - ==> Best [Top1: 87.890   Sparsity:0.00   Params: 155168 on epoch: 29]
2023-01-06 16:55:55,273 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:55:55,280 - 

2023-01-06 16:55:55,280 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:55:55,995 - Epoch: [30][   10/  246]    Overall Loss 0.338281    Objective Loss 0.338281                                        LR 0.000060    Time 0.071417    
2023-01-06 16:55:56,192 - Epoch: [30][   20/  246]    Overall Loss 0.343462    Objective Loss 0.343462                                        LR 0.000060    Time 0.045509    
2023-01-06 16:55:56,384 - Epoch: [30][   30/  246]    Overall Loss 0.335109    Objective Loss 0.335109                                        LR 0.000060    Time 0.036728    
2023-01-06 16:55:56,577 - Epoch: [30][   40/  246]    Overall Loss 0.344068    Objective Loss 0.344068                                        LR 0.000060    Time 0.032370    
2023-01-06 16:55:56,772 - Epoch: [30][   50/  246]    Overall Loss 0.347345    Objective Loss 0.347345                                        LR 0.000060    Time 0.029787    
2023-01-06 16:55:56,965 - Epoch: [30][   60/  246]    Overall Loss 0.348682    Objective Loss 0.348682                                        LR 0.000060    Time 0.028034    
2023-01-06 16:55:57,151 - Epoch: [30][   70/  246]    Overall Loss 0.349193    Objective Loss 0.349193                                        LR 0.000060    Time 0.026686    
2023-01-06 16:55:57,337 - Epoch: [30][   80/  246]    Overall Loss 0.349287    Objective Loss 0.349287                                        LR 0.000060    Time 0.025665    
2023-01-06 16:55:57,524 - Epoch: [30][   90/  246]    Overall Loss 0.348697    Objective Loss 0.348697                                        LR 0.000060    Time 0.024887    
2023-01-06 16:55:57,711 - Epoch: [30][  100/  246]    Overall Loss 0.346688    Objective Loss 0.346688                                        LR 0.000060    Time 0.024263    
2023-01-06 16:55:57,898 - Epoch: [30][  110/  246]    Overall Loss 0.347630    Objective Loss 0.347630                                        LR 0.000060    Time 0.023757    
2023-01-06 16:55:58,086 - Epoch: [30][  120/  246]    Overall Loss 0.345070    Objective Loss 0.345070                                        LR 0.000060    Time 0.023338    
2023-01-06 16:55:58,274 - Epoch: [30][  130/  246]    Overall Loss 0.344010    Objective Loss 0.344010                                        LR 0.000060    Time 0.022990    
2023-01-06 16:55:58,458 - Epoch: [30][  140/  246]    Overall Loss 0.343603    Objective Loss 0.343603                                        LR 0.000060    Time 0.022658    
2023-01-06 16:55:58,647 - Epoch: [30][  150/  246]    Overall Loss 0.343690    Objective Loss 0.343690                                        LR 0.000060    Time 0.022402    
2023-01-06 16:55:58,834 - Epoch: [30][  160/  246]    Overall Loss 0.342950    Objective Loss 0.342950                                        LR 0.000060    Time 0.022169    
2023-01-06 16:55:59,023 - Epoch: [30][  170/  246]    Overall Loss 0.342678    Objective Loss 0.342678                                        LR 0.000060    Time 0.021973    
2023-01-06 16:55:59,220 - Epoch: [30][  180/  246]    Overall Loss 0.341786    Objective Loss 0.341786                                        LR 0.000060    Time 0.021845    
2023-01-06 16:55:59,416 - Epoch: [30][  190/  246]    Overall Loss 0.342228    Objective Loss 0.342228                                        LR 0.000060    Time 0.021729    
2023-01-06 16:55:59,615 - Epoch: [30][  200/  246]    Overall Loss 0.343027    Objective Loss 0.343027                                        LR 0.000060    Time 0.021631    
2023-01-06 16:55:59,804 - Epoch: [30][  210/  246]    Overall Loss 0.343089    Objective Loss 0.343089                                        LR 0.000060    Time 0.021500    
2023-01-06 16:55:59,988 - Epoch: [30][  220/  246]    Overall Loss 0.342497    Objective Loss 0.342497                                        LR 0.000060    Time 0.021359    
2023-01-06 16:56:00,183 - Epoch: [30][  230/  246]    Overall Loss 0.342187    Objective Loss 0.342187                                        LR 0.000060    Time 0.021277    
2023-01-06 16:56:00,393 - Epoch: [30][  240/  246]    Overall Loss 0.340962    Objective Loss 0.340962                                        LR 0.000060    Time 0.021262    
2023-01-06 16:56:00,487 - Epoch: [30][  246/  246]    Overall Loss 0.341485    Objective Loss 0.341485    Top1 84.210526    LR 0.000060    Time 0.021127    
2023-01-06 16:56:00,630 - --- validate (epoch=30)-----------
2023-01-06 16:56:00,630 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:01,070 - Epoch: [30][   10/   28]    Loss 0.341042    Top1 87.851562    
2023-01-06 16:56:01,182 - Epoch: [30][   20/   28]    Loss 0.331784    Top1 87.949219    
2023-01-06 16:56:01,251 - Epoch: [30][   28/   28]    Loss 0.332472    Top1 88.162038    
2023-01-06 16:56:01,378 - ==> Top1: 88.162    Loss: 0.332

2023-01-06 16:56:01,378 - ==> Confusion:
[[ 201    5  233]
 [  16  151  435]
 [  94   44 5807]]

2023-01-06 16:56:01,379 - ==> Best [Top1: 88.162   Sparsity:0.00   Params: 155168 on epoch: 30]
2023-01-06 16:56:01,379 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:01,387 - 

2023-01-06 16:56:01,387 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:02,101 - Epoch: [31][   10/  246]    Overall Loss 0.351499    Objective Loss 0.351499                                        LR 0.000060    Time 0.071344    
2023-01-06 16:56:02,302 - Epoch: [31][   20/  246]    Overall Loss 0.349345    Objective Loss 0.349345                                        LR 0.000060    Time 0.045683    
2023-01-06 16:56:02,492 - Epoch: [31][   30/  246]    Overall Loss 0.346623    Objective Loss 0.346623                                        LR 0.000060    Time 0.036802    
2023-01-06 16:56:02,684 - Epoch: [31][   40/  246]    Overall Loss 0.342957    Objective Loss 0.342957                                        LR 0.000060    Time 0.032389    
2023-01-06 16:56:02,876 - Epoch: [31][   50/  246]    Overall Loss 0.345497    Objective Loss 0.345497                                        LR 0.000060    Time 0.029744    
2023-01-06 16:56:03,070 - Epoch: [31][   60/  246]    Overall Loss 0.346197    Objective Loss 0.346197                                        LR 0.000060    Time 0.028001    
2023-01-06 16:56:03,263 - Epoch: [31][   70/  246]    Overall Loss 0.343514    Objective Loss 0.343514                                        LR 0.000060    Time 0.026755    
2023-01-06 16:56:03,454 - Epoch: [31][   80/  246]    Overall Loss 0.342559    Objective Loss 0.342559                                        LR 0.000060    Time 0.025804    
2023-01-06 16:56:03,644 - Epoch: [31][   90/  246]    Overall Loss 0.342113    Objective Loss 0.342113                                        LR 0.000060    Time 0.025041    
2023-01-06 16:56:03,836 - Epoch: [31][  100/  246]    Overall Loss 0.341466    Objective Loss 0.341466                                        LR 0.000060    Time 0.024453    
2023-01-06 16:56:04,029 - Epoch: [31][  110/  246]    Overall Loss 0.340654    Objective Loss 0.340654                                        LR 0.000060    Time 0.023982    
2023-01-06 16:56:04,224 - Epoch: [31][  120/  246]    Overall Loss 0.340212    Objective Loss 0.340212                                        LR 0.000060    Time 0.023600    
2023-01-06 16:56:04,415 - Epoch: [31][  130/  246]    Overall Loss 0.339586    Objective Loss 0.339586                                        LR 0.000060    Time 0.023256    
2023-01-06 16:56:04,607 - Epoch: [31][  140/  246]    Overall Loss 0.339172    Objective Loss 0.339172                                        LR 0.000060    Time 0.022962    
2023-01-06 16:56:04,800 - Epoch: [31][  150/  246]    Overall Loss 0.339046    Objective Loss 0.339046                                        LR 0.000060    Time 0.022717    
2023-01-06 16:56:04,993 - Epoch: [31][  160/  246]    Overall Loss 0.339336    Objective Loss 0.339336                                        LR 0.000060    Time 0.022498    
2023-01-06 16:56:05,186 - Epoch: [31][  170/  246]    Overall Loss 0.340585    Objective Loss 0.340585                                        LR 0.000060    Time 0.022312    
2023-01-06 16:56:05,379 - Epoch: [31][  180/  246]    Overall Loss 0.341524    Objective Loss 0.341524                                        LR 0.000060    Time 0.022139    
2023-01-06 16:56:05,572 - Epoch: [31][  190/  246]    Overall Loss 0.340987    Objective Loss 0.340987                                        LR 0.000060    Time 0.021986    
2023-01-06 16:56:05,764 - Epoch: [31][  200/  246]    Overall Loss 0.340559    Objective Loss 0.340559                                        LR 0.000060    Time 0.021846    
2023-01-06 16:56:05,957 - Epoch: [31][  210/  246]    Overall Loss 0.340242    Objective Loss 0.340242                                        LR 0.000060    Time 0.021723    
2023-01-06 16:56:06,151 - Epoch: [31][  220/  246]    Overall Loss 0.339577    Objective Loss 0.339577                                        LR 0.000060    Time 0.021616    
2023-01-06 16:56:06,344 - Epoch: [31][  230/  246]    Overall Loss 0.339515    Objective Loss 0.339515                                        LR 0.000060    Time 0.021515    
2023-01-06 16:56:06,550 - Epoch: [31][  240/  246]    Overall Loss 0.339454    Objective Loss 0.339454                                        LR 0.000060    Time 0.021476    
2023-01-06 16:56:06,647 - Epoch: [31][  246/  246]    Overall Loss 0.338940    Objective Loss 0.338940    Top1 87.799043    LR 0.000060    Time 0.021343    
2023-01-06 16:56:06,776 - --- validate (epoch=31)-----------
2023-01-06 16:56:06,776 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:07,242 - Epoch: [31][   10/   28]    Loss 0.341486    Top1 88.007812    
2023-01-06 16:56:07,359 - Epoch: [31][   20/   28]    Loss 0.341851    Top1 87.792969    
2023-01-06 16:56:07,427 - Epoch: [31][   28/   28]    Loss 0.333552    Top1 88.004581    
2023-01-06 16:56:07,549 - ==> Top1: 88.005    Loss: 0.334

2023-01-06 16:56:07,549 - ==> Confusion:
[[ 158   13  268]
 [   6  179  417]
 [  69   65 5811]]

2023-01-06 16:56:07,551 - ==> Best [Top1: 88.162   Sparsity:0.00   Params: 155168 on epoch: 30]
2023-01-06 16:56:07,551 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:07,557 - 

2023-01-06 16:56:07,557 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:08,126 - Epoch: [32][   10/  246]    Overall Loss 0.358899    Objective Loss 0.358899                                        LR 0.000060    Time 0.056843    
2023-01-06 16:56:08,318 - Epoch: [32][   20/  246]    Overall Loss 0.359920    Objective Loss 0.359920                                        LR 0.000060    Time 0.038009    
2023-01-06 16:56:08,508 - Epoch: [32][   30/  246]    Overall Loss 0.344974    Objective Loss 0.344974                                        LR 0.000060    Time 0.031670    
2023-01-06 16:56:08,698 - Epoch: [32][   40/  246]    Overall Loss 0.340403    Objective Loss 0.340403                                        LR 0.000060    Time 0.028493    
2023-01-06 16:56:08,885 - Epoch: [32][   50/  246]    Overall Loss 0.338816    Objective Loss 0.338816                                        LR 0.000060    Time 0.026519    
2023-01-06 16:56:09,075 - Epoch: [32][   60/  246]    Overall Loss 0.336653    Objective Loss 0.336653                                        LR 0.000060    Time 0.025269    
2023-01-06 16:56:09,263 - Epoch: [32][   70/  246]    Overall Loss 0.336554    Objective Loss 0.336554                                        LR 0.000060    Time 0.024328    
2023-01-06 16:56:09,451 - Epoch: [32][   80/  246]    Overall Loss 0.338063    Objective Loss 0.338063                                        LR 0.000060    Time 0.023639    
2023-01-06 16:56:09,639 - Epoch: [32][   90/  246]    Overall Loss 0.339092    Objective Loss 0.339092                                        LR 0.000060    Time 0.023095    
2023-01-06 16:56:09,832 - Epoch: [32][  100/  246]    Overall Loss 0.339337    Objective Loss 0.339337                                        LR 0.000060    Time 0.022717    
2023-01-06 16:56:10,023 - Epoch: [32][  110/  246]    Overall Loss 0.340103    Objective Loss 0.340103                                        LR 0.000060    Time 0.022378    
2023-01-06 16:56:10,217 - Epoch: [32][  120/  246]    Overall Loss 0.339824    Objective Loss 0.339824                                        LR 0.000060    Time 0.022127    
2023-01-06 16:56:10,406 - Epoch: [32][  130/  246]    Overall Loss 0.338783    Objective Loss 0.338783                                        LR 0.000060    Time 0.021874    
2023-01-06 16:56:10,601 - Epoch: [32][  140/  246]    Overall Loss 0.339899    Objective Loss 0.339899                                        LR 0.000060    Time 0.021703    
2023-01-06 16:56:10,790 - Epoch: [32][  150/  246]    Overall Loss 0.341101    Objective Loss 0.341101                                        LR 0.000060    Time 0.021515    
2023-01-06 16:56:10,984 - Epoch: [32][  160/  246]    Overall Loss 0.340522    Objective Loss 0.340522                                        LR 0.000060    Time 0.021380    
2023-01-06 16:56:11,175 - Epoch: [32][  170/  246]    Overall Loss 0.339092    Objective Loss 0.339092                                        LR 0.000060    Time 0.021245    
2023-01-06 16:56:11,364 - Epoch: [32][  180/  246]    Overall Loss 0.339553    Objective Loss 0.339553                                        LR 0.000060    Time 0.021112    
2023-01-06 16:56:11,556 - Epoch: [32][  190/  246]    Overall Loss 0.338477    Objective Loss 0.338477                                        LR 0.000060    Time 0.021002    
2023-01-06 16:56:11,743 - Epoch: [32][  200/  246]    Overall Loss 0.338188    Objective Loss 0.338188                                        LR 0.000060    Time 0.020881    
2023-01-06 16:56:11,934 - Epoch: [32][  210/  246]    Overall Loss 0.338593    Objective Loss 0.338593                                        LR 0.000060    Time 0.020799    
2023-01-06 16:56:12,120 - Epoch: [32][  220/  246]    Overall Loss 0.338547    Objective Loss 0.338547                                        LR 0.000060    Time 0.020698    
2023-01-06 16:56:12,310 - Epoch: [32][  230/  246]    Overall Loss 0.338420    Objective Loss 0.338420                                        LR 0.000060    Time 0.020622    
2023-01-06 16:56:12,514 - Epoch: [32][  240/  246]    Overall Loss 0.338302    Objective Loss 0.338302                                        LR 0.000060    Time 0.020611    
2023-01-06 16:56:12,612 - Epoch: [32][  246/  246]    Overall Loss 0.337496    Objective Loss 0.337496    Top1 88.277512    LR 0.000060    Time 0.020503    
2023-01-06 16:56:12,757 - --- validate (epoch=32)-----------
2023-01-06 16:56:12,757 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:13,209 - Epoch: [32][   10/   28]    Loss 0.326440    Top1 88.164062    
2023-01-06 16:56:13,322 - Epoch: [32][   20/   28]    Loss 0.326425    Top1 88.398438    
2023-01-06 16:56:13,391 - Epoch: [32][   28/   28]    Loss 0.332284    Top1 88.276553    
2023-01-06 16:56:13,552 - ==> Top1: 88.277    Loss: 0.332

2023-01-06 16:56:13,552 - ==> Confusion:
[[ 161    5  273]
 [  10  157  435]
 [  54   42 5849]]

2023-01-06 16:56:13,553 - ==> Best [Top1: 88.277   Sparsity:0.00   Params: 155168 on epoch: 32]
2023-01-06 16:56:13,553 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:13,561 - 

2023-01-06 16:56:13,561 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:14,282 - Epoch: [33][   10/  246]    Overall Loss 0.342134    Objective Loss 0.342134                                        LR 0.000060    Time 0.072055    
2023-01-06 16:56:14,487 - Epoch: [33][   20/  246]    Overall Loss 0.338898    Objective Loss 0.338898                                        LR 0.000060    Time 0.046150    
2023-01-06 16:56:14,691 - Epoch: [33][   30/  246]    Overall Loss 0.337659    Objective Loss 0.337659                                        LR 0.000060    Time 0.037579    
2023-01-06 16:56:14,888 - Epoch: [33][   40/  246]    Overall Loss 0.335787    Objective Loss 0.335787                                        LR 0.000060    Time 0.033090    
2023-01-06 16:56:15,091 - Epoch: [33][   50/  246]    Overall Loss 0.337795    Objective Loss 0.337795                                        LR 0.000060    Time 0.030537    
2023-01-06 16:56:15,287 - Epoch: [33][   60/  246]    Overall Loss 0.337543    Objective Loss 0.337543                                        LR 0.000060    Time 0.028699    
2023-01-06 16:56:15,494 - Epoch: [33][   70/  246]    Overall Loss 0.340622    Objective Loss 0.340622                                        LR 0.000060    Time 0.027548    
2023-01-06 16:56:15,694 - Epoch: [33][   80/  246]    Overall Loss 0.340725    Objective Loss 0.340725                                        LR 0.000060    Time 0.026603    
2023-01-06 16:56:15,900 - Epoch: [33][   90/  246]    Overall Loss 0.339330    Objective Loss 0.339330                                        LR 0.000060    Time 0.025938    
2023-01-06 16:56:16,095 - Epoch: [33][  100/  246]    Overall Loss 0.338333    Objective Loss 0.338333                                        LR 0.000060    Time 0.025281    
2023-01-06 16:56:16,291 - Epoch: [33][  110/  246]    Overall Loss 0.338426    Objective Loss 0.338426                                        LR 0.000060    Time 0.024766    
2023-01-06 16:56:16,487 - Epoch: [33][  120/  246]    Overall Loss 0.338586    Objective Loss 0.338586                                        LR 0.000060    Time 0.024334    
2023-01-06 16:56:16,682 - Epoch: [33][  130/  246]    Overall Loss 0.339161    Objective Loss 0.339161                                        LR 0.000060    Time 0.023958    
2023-01-06 16:56:16,878 - Epoch: [33][  140/  246]    Overall Loss 0.339939    Objective Loss 0.339939                                        LR 0.000060    Time 0.023645    
2023-01-06 16:56:17,080 - Epoch: [33][  150/  246]    Overall Loss 0.339889    Objective Loss 0.339889                                        LR 0.000060    Time 0.023411    
2023-01-06 16:56:17,282 - Epoch: [33][  160/  246]    Overall Loss 0.339480    Objective Loss 0.339480                                        LR 0.000060    Time 0.023206    
2023-01-06 16:56:17,486 - Epoch: [33][  170/  246]    Overall Loss 0.338573    Objective Loss 0.338573                                        LR 0.000060    Time 0.023042    
2023-01-06 16:56:17,689 - Epoch: [33][  180/  246]    Overall Loss 0.338797    Objective Loss 0.338797                                        LR 0.000060    Time 0.022888    
2023-01-06 16:56:17,894 - Epoch: [33][  190/  246]    Overall Loss 0.338117    Objective Loss 0.338117                                        LR 0.000060    Time 0.022758    
2023-01-06 16:56:18,089 - Epoch: [33][  200/  246]    Overall Loss 0.336844    Objective Loss 0.336844                                        LR 0.000060    Time 0.022596    
2023-01-06 16:56:18,298 - Epoch: [33][  210/  246]    Overall Loss 0.336248    Objective Loss 0.336248                                        LR 0.000060    Time 0.022511    
2023-01-06 16:56:18,549 - Epoch: [33][  220/  246]    Overall Loss 0.335974    Objective Loss 0.335974                                        LR 0.000060    Time 0.022627    
2023-01-06 16:56:18,790 - Epoch: [33][  230/  246]    Overall Loss 0.336375    Objective Loss 0.336375                                        LR 0.000060    Time 0.022683    
2023-01-06 16:56:19,039 - Epoch: [33][  240/  246]    Overall Loss 0.334081    Objective Loss 0.334081                                        LR 0.000060    Time 0.022772    
2023-01-06 16:56:19,151 - Epoch: [33][  246/  246]    Overall Loss 0.333737    Objective Loss 0.333737    Top1 89.952153    LR 0.000060    Time 0.022675    
2023-01-06 16:56:19,295 - --- validate (epoch=33)-----------
2023-01-06 16:56:19,295 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:19,746 - Epoch: [33][   10/   28]    Loss 0.347085    Top1 87.539062    
2023-01-06 16:56:19,857 - Epoch: [33][   20/   28]    Loss 0.337304    Top1 87.968750    
2023-01-06 16:56:19,924 - Epoch: [33][   28/   28]    Loss 0.330872    Top1 88.076152    
2023-01-06 16:56:20,069 - ==> Top1: 88.076    Loss: 0.331

2023-01-06 16:56:20,069 - ==> Confusion:
[[ 148    5  286]
 [   9  130  463]
 [  41   29 5875]]

2023-01-06 16:56:20,070 - ==> Best [Top1: 88.277   Sparsity:0.00   Params: 155168 on epoch: 32]
2023-01-06 16:56:20,070 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:20,076 - 

2023-01-06 16:56:20,076 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:20,805 - Epoch: [34][   10/  246]    Overall Loss 0.340929    Objective Loss 0.340929                                        LR 0.000060    Time 0.072771    
2023-01-06 16:56:21,004 - Epoch: [34][   20/  246]    Overall Loss 0.337358    Objective Loss 0.337358                                        LR 0.000060    Time 0.046326    
2023-01-06 16:56:21,201 - Epoch: [34][   30/  246]    Overall Loss 0.338259    Objective Loss 0.338259                                        LR 0.000060    Time 0.037441    
2023-01-06 16:56:21,397 - Epoch: [34][   40/  246]    Overall Loss 0.333879    Objective Loss 0.333879                                        LR 0.000060    Time 0.032980    
2023-01-06 16:56:21,587 - Epoch: [34][   50/  246]    Overall Loss 0.337541    Objective Loss 0.337541                                        LR 0.000060    Time 0.030170    
2023-01-06 16:56:21,780 - Epoch: [34][   60/  246]    Overall Loss 0.337659    Objective Loss 0.337659                                        LR 0.000060    Time 0.028355    
2023-01-06 16:56:21,979 - Epoch: [34][   70/  246]    Overall Loss 0.336326    Objective Loss 0.336326                                        LR 0.000060    Time 0.027146    
2023-01-06 16:56:22,196 - Epoch: [34][   80/  246]    Overall Loss 0.335597    Objective Loss 0.335597                                        LR 0.000060    Time 0.026452    
2023-01-06 16:56:22,423 - Epoch: [34][   90/  246]    Overall Loss 0.335723    Objective Loss 0.335723                                        LR 0.000060    Time 0.026034    
2023-01-06 16:56:22,653 - Epoch: [34][  100/  246]    Overall Loss 0.334122    Objective Loss 0.334122                                        LR 0.000060    Time 0.025722    
2023-01-06 16:56:22,883 - Epoch: [34][  110/  246]    Overall Loss 0.336532    Objective Loss 0.336532                                        LR 0.000060    Time 0.025473    
2023-01-06 16:56:23,131 - Epoch: [34][  120/  246]    Overall Loss 0.334293    Objective Loss 0.334293                                        LR 0.000060    Time 0.025409    
2023-01-06 16:56:23,362 - Epoch: [34][  130/  246]    Overall Loss 0.332496    Objective Loss 0.332496                                        LR 0.000060    Time 0.025219    
2023-01-06 16:56:23,595 - Epoch: [34][  140/  246]    Overall Loss 0.333182    Objective Loss 0.333182                                        LR 0.000060    Time 0.025077    
2023-01-06 16:56:23,814 - Epoch: [34][  150/  246]    Overall Loss 0.333545    Objective Loss 0.333545                                        LR 0.000060    Time 0.024864    
2023-01-06 16:56:24,035 - Epoch: [34][  160/  246]    Overall Loss 0.332399    Objective Loss 0.332399                                        LR 0.000060    Time 0.024690    
2023-01-06 16:56:24,253 - Epoch: [34][  170/  246]    Overall Loss 0.333458    Objective Loss 0.333458                                        LR 0.000060    Time 0.024516    
2023-01-06 16:56:24,483 - Epoch: [34][  180/  246]    Overall Loss 0.332779    Objective Loss 0.332779                                        LR 0.000060    Time 0.024433    
2023-01-06 16:56:24,728 - Epoch: [34][  190/  246]    Overall Loss 0.332557    Objective Loss 0.332557                                        LR 0.000060    Time 0.024431    
2023-01-06 16:56:24,974 - Epoch: [34][  200/  246]    Overall Loss 0.331152    Objective Loss 0.331152                                        LR 0.000060    Time 0.024440    
2023-01-06 16:56:25,219 - Epoch: [34][  210/  246]    Overall Loss 0.331750    Objective Loss 0.331750                                        LR 0.000060    Time 0.024437    
2023-01-06 16:56:25,463 - Epoch: [34][  220/  246]    Overall Loss 0.331943    Objective Loss 0.331943                                        LR 0.000060    Time 0.024437    
2023-01-06 16:56:25,706 - Epoch: [34][  230/  246]    Overall Loss 0.330928    Objective Loss 0.330928                                        LR 0.000060    Time 0.024428    
2023-01-06 16:56:25,952 - Epoch: [34][  240/  246]    Overall Loss 0.330438    Objective Loss 0.330438                                        LR 0.000060    Time 0.024426    
2023-01-06 16:56:26,049 - Epoch: [34][  246/  246]    Overall Loss 0.330814    Objective Loss 0.330814    Top1 86.842105    LR 0.000060    Time 0.024225    
2023-01-06 16:56:26,191 - --- validate (epoch=34)-----------
2023-01-06 16:56:26,192 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:26,635 - Epoch: [34][   10/   28]    Loss 0.329361    Top1 88.437500    
2023-01-06 16:56:26,750 - Epoch: [34][   20/   28]    Loss 0.332706    Top1 88.183594    
2023-01-06 16:56:26,817 - Epoch: [34][   28/   28]    Loss 0.325015    Top1 88.262239    
2023-01-06 16:56:26,972 - ==> Top1: 88.262    Loss: 0.325

2023-01-06 16:56:26,972 - ==> Confusion:
[[ 154    7  278]
 [   6  148  448]
 [  51   30 5864]]

2023-01-06 16:56:26,973 - ==> Best [Top1: 88.277   Sparsity:0.00   Params: 155168 on epoch: 32]
2023-01-06 16:56:26,973 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:26,979 - 

2023-01-06 16:56:26,979 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:27,563 - Epoch: [35][   10/  246]    Overall Loss 0.318281    Objective Loss 0.318281                                        LR 0.000060    Time 0.058318    
2023-01-06 16:56:27,762 - Epoch: [35][   20/  246]    Overall Loss 0.324568    Objective Loss 0.324568                                        LR 0.000060    Time 0.039051    
2023-01-06 16:56:27,957 - Epoch: [35][   30/  246]    Overall Loss 0.325128    Objective Loss 0.325128                                        LR 0.000060    Time 0.032523    
2023-01-06 16:56:28,150 - Epoch: [35][   40/  246]    Overall Loss 0.326353    Objective Loss 0.326353                                        LR 0.000060    Time 0.029211    
2023-01-06 16:56:28,342 - Epoch: [35][   50/  246]    Overall Loss 0.332224    Objective Loss 0.332224                                        LR 0.000060    Time 0.027209    
2023-01-06 16:56:28,537 - Epoch: [35][   60/  246]    Overall Loss 0.337048    Objective Loss 0.337048                                        LR 0.000060    Time 0.025906    
2023-01-06 16:56:28,733 - Epoch: [35][   70/  246]    Overall Loss 0.338837    Objective Loss 0.338837                                        LR 0.000060    Time 0.025004    
2023-01-06 16:56:28,927 - Epoch: [35][   80/  246]    Overall Loss 0.336691    Objective Loss 0.336691                                        LR 0.000060    Time 0.024297    
2023-01-06 16:56:29,121 - Epoch: [35][   90/  246]    Overall Loss 0.336878    Objective Loss 0.336878                                        LR 0.000060    Time 0.023750    
2023-01-06 16:56:29,315 - Epoch: [35][  100/  246]    Overall Loss 0.334864    Objective Loss 0.334864                                        LR 0.000060    Time 0.023314    
2023-01-06 16:56:29,509 - Epoch: [35][  110/  246]    Overall Loss 0.334507    Objective Loss 0.334507                                        LR 0.000060    Time 0.022958    
2023-01-06 16:56:29,703 - Epoch: [35][  120/  246]    Overall Loss 0.335274    Objective Loss 0.335274                                        LR 0.000060    Time 0.022652    
2023-01-06 16:56:29,898 - Epoch: [35][  130/  246]    Overall Loss 0.334188    Objective Loss 0.334188                                        LR 0.000060    Time 0.022413    
2023-01-06 16:56:30,093 - Epoch: [35][  140/  246]    Overall Loss 0.332884    Objective Loss 0.332884                                        LR 0.000060    Time 0.022196    
2023-01-06 16:56:30,286 - Epoch: [35][  150/  246]    Overall Loss 0.331377    Objective Loss 0.331377                                        LR 0.000060    Time 0.022004    
2023-01-06 16:56:30,480 - Epoch: [35][  160/  246]    Overall Loss 0.330918    Objective Loss 0.330918                                        LR 0.000060    Time 0.021836    
2023-01-06 16:56:30,674 - Epoch: [35][  170/  246]    Overall Loss 0.330339    Objective Loss 0.330339                                        LR 0.000060    Time 0.021690    
2023-01-06 16:56:30,866 - Epoch: [35][  180/  246]    Overall Loss 0.329543    Objective Loss 0.329543                                        LR 0.000060    Time 0.021553    
2023-01-06 16:56:31,061 - Epoch: [35][  190/  246]    Overall Loss 0.328208    Objective Loss 0.328208                                        LR 0.000060    Time 0.021443    
2023-01-06 16:56:31,261 - Epoch: [35][  200/  246]    Overall Loss 0.327804    Objective Loss 0.327804                                        LR 0.000060    Time 0.021370    
2023-01-06 16:56:31,466 - Epoch: [35][  210/  246]    Overall Loss 0.329077    Objective Loss 0.329077                                        LR 0.000060    Time 0.021327    
2023-01-06 16:56:31,671 - Epoch: [35][  220/  246]    Overall Loss 0.329047    Objective Loss 0.329047                                        LR 0.000060    Time 0.021287    
2023-01-06 16:56:31,850 - Epoch: [35][  230/  246]    Overall Loss 0.328492    Objective Loss 0.328492                                        LR 0.000060    Time 0.021139    
2023-01-06 16:56:32,062 - Epoch: [35][  240/  246]    Overall Loss 0.329250    Objective Loss 0.329250                                        LR 0.000060    Time 0.021139    
2023-01-06 16:56:32,157 - Epoch: [35][  246/  246]    Overall Loss 0.328949    Objective Loss 0.328949    Top1 87.081340    LR 0.000060    Time 0.021010    
2023-01-06 16:56:32,289 - --- validate (epoch=35)-----------
2023-01-06 16:56:32,290 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:32,738 - Epoch: [35][   10/   28]    Loss 0.318656    Top1 88.554688    
2023-01-06 16:56:32,850 - Epoch: [35][   20/   28]    Loss 0.323139    Top1 88.359375    
2023-01-06 16:56:32,918 - Epoch: [35][   28/   28]    Loss 0.324234    Top1 88.147724    
2023-01-06 16:56:33,069 - ==> Top1: 88.148    Loss: 0.324

2023-01-06 16:56:33,070 - ==> Confusion:
[[ 161    7  271]
 [  11  135  456]
 [  50   33 5862]]

2023-01-06 16:56:33,071 - ==> Best [Top1: 88.277   Sparsity:0.00   Params: 155168 on epoch: 32]
2023-01-06 16:56:33,071 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:33,079 - 

2023-01-06 16:56:33,079 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:33,794 - Epoch: [36][   10/  246]    Overall Loss 0.356104    Objective Loss 0.356104                                        LR 0.000060    Time 0.071376    
2023-01-06 16:56:33,983 - Epoch: [36][   20/  246]    Overall Loss 0.341239    Objective Loss 0.341239                                        LR 0.000060    Time 0.045150    
2023-01-06 16:56:34,188 - Epoch: [36][   30/  246]    Overall Loss 0.337017    Objective Loss 0.337017                                        LR 0.000060    Time 0.036923    
2023-01-06 16:56:34,420 - Epoch: [36][   40/  246]    Overall Loss 0.335903    Objective Loss 0.335903                                        LR 0.000060    Time 0.033460    
2023-01-06 16:56:34,646 - Epoch: [36][   50/  246]    Overall Loss 0.334544    Objective Loss 0.334544                                        LR 0.000060    Time 0.031282    
2023-01-06 16:56:34,872 - Epoch: [36][   60/  246]    Overall Loss 0.333719    Objective Loss 0.333719                                        LR 0.000060    Time 0.029838    
2023-01-06 16:56:35,117 - Epoch: [36][   70/  246]    Overall Loss 0.332475    Objective Loss 0.332475                                        LR 0.000060    Time 0.029057    
2023-01-06 16:56:35,354 - Epoch: [36][   80/  246]    Overall Loss 0.330672    Objective Loss 0.330672                                        LR 0.000060    Time 0.028380    
2023-01-06 16:56:35,595 - Epoch: [36][   90/  246]    Overall Loss 0.332958    Objective Loss 0.332958                                        LR 0.000060    Time 0.027899    
2023-01-06 16:56:35,841 - Epoch: [36][  100/  246]    Overall Loss 0.331980    Objective Loss 0.331980                                        LR 0.000060    Time 0.027549    
2023-01-06 16:56:36,083 - Epoch: [36][  110/  246]    Overall Loss 0.330790    Objective Loss 0.330790                                        LR 0.000060    Time 0.027238    
2023-01-06 16:56:36,263 - Epoch: [36][  120/  246]    Overall Loss 0.329235    Objective Loss 0.329235                                        LR 0.000060    Time 0.026465    
2023-01-06 16:56:36,440 - Epoch: [36][  130/  246]    Overall Loss 0.328014    Objective Loss 0.328014                                        LR 0.000060    Time 0.025789    
2023-01-06 16:56:36,633 - Epoch: [36][  140/  246]    Overall Loss 0.326915    Objective Loss 0.326915                                        LR 0.000060    Time 0.025327    
2023-01-06 16:56:36,835 - Epoch: [36][  150/  246]    Overall Loss 0.327852    Objective Loss 0.327852                                        LR 0.000060    Time 0.024982    
2023-01-06 16:56:37,032 - Epoch: [36][  160/  246]    Overall Loss 0.326601    Objective Loss 0.326601                                        LR 0.000060    Time 0.024649    
2023-01-06 16:56:37,239 - Epoch: [36][  170/  246]    Overall Loss 0.326757    Objective Loss 0.326757                                        LR 0.000060    Time 0.024411    
2023-01-06 16:56:37,444 - Epoch: [36][  180/  246]    Overall Loss 0.328384    Objective Loss 0.328384                                        LR 0.000060    Time 0.024183    
2023-01-06 16:56:37,653 - Epoch: [36][  190/  246]    Overall Loss 0.328253    Objective Loss 0.328253                                        LR 0.000060    Time 0.024010    
2023-01-06 16:56:37,857 - Epoch: [36][  200/  246]    Overall Loss 0.328309    Objective Loss 0.328309                                        LR 0.000060    Time 0.023818    
2023-01-06 16:56:38,057 - Epoch: [36][  210/  246]    Overall Loss 0.328454    Objective Loss 0.328454                                        LR 0.000060    Time 0.023635    
2023-01-06 16:56:38,250 - Epoch: [36][  220/  246]    Overall Loss 0.329090    Objective Loss 0.329090                                        LR 0.000060    Time 0.023433    
2023-01-06 16:56:38,428 - Epoch: [36][  230/  246]    Overall Loss 0.328885    Objective Loss 0.328885                                        LR 0.000060    Time 0.023189    
2023-01-06 16:56:38,630 - Epoch: [36][  240/  246]    Overall Loss 0.329130    Objective Loss 0.329130                                        LR 0.000060    Time 0.023063    
2023-01-06 16:56:38,724 - Epoch: [36][  246/  246]    Overall Loss 0.328595    Objective Loss 0.328595    Top1 86.842105    LR 0.000060    Time 0.022881    
2023-01-06 16:56:38,857 - --- validate (epoch=36)-----------
2023-01-06 16:56:38,857 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:39,310 - Epoch: [36][   10/   28]    Loss 0.337529    Top1 87.929688    
2023-01-06 16:56:39,422 - Epoch: [36][   20/   28]    Loss 0.321812    Top1 88.222656    
2023-01-06 16:56:39,491 - Epoch: [36][   28/   28]    Loss 0.325003    Top1 88.462640    
2023-01-06 16:56:39,633 - ==> Top1: 88.463    Loss: 0.325

2023-01-06 16:56:39,633 - ==> Confusion:
[[ 161    7  271]
 [  11  168  423]
 [  50   44 5851]]

2023-01-06 16:56:39,634 - ==> Best [Top1: 88.463   Sparsity:0.00   Params: 155168 on epoch: 36]
2023-01-06 16:56:39,634 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:39,641 - 

2023-01-06 16:56:39,641 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:40,196 - Epoch: [37][   10/  246]    Overall Loss 0.339824    Objective Loss 0.339824                                        LR 0.000060    Time 0.055360    
2023-01-06 16:56:40,363 - Epoch: [37][   20/  246]    Overall Loss 0.330389    Objective Loss 0.330389                                        LR 0.000060    Time 0.036004    
2023-01-06 16:56:40,554 - Epoch: [37][   30/  246]    Overall Loss 0.328141    Objective Loss 0.328141                                        LR 0.000060    Time 0.030364    
2023-01-06 16:56:40,747 - Epoch: [37][   40/  246]    Overall Loss 0.326567    Objective Loss 0.326567                                        LR 0.000060    Time 0.027581    
2023-01-06 16:56:40,941 - Epoch: [37][   50/  246]    Overall Loss 0.330380    Objective Loss 0.330380                                        LR 0.000060    Time 0.025934    
2023-01-06 16:56:41,136 - Epoch: [37][   60/  246]    Overall Loss 0.330041    Objective Loss 0.330041                                        LR 0.000060    Time 0.024865    
2023-01-06 16:56:41,330 - Epoch: [37][   70/  246]    Overall Loss 0.329917    Objective Loss 0.329917                                        LR 0.000060    Time 0.024079    
2023-01-06 16:56:41,521 - Epoch: [37][   80/  246]    Overall Loss 0.325091    Objective Loss 0.325091                                        LR 0.000060    Time 0.023446    
2023-01-06 16:56:41,727 - Epoch: [37][   90/  246]    Overall Loss 0.326701    Objective Loss 0.326701                                        LR 0.000060    Time 0.023131    
2023-01-06 16:56:41,951 - Epoch: [37][  100/  246]    Overall Loss 0.326332    Objective Loss 0.326332                                        LR 0.000060    Time 0.023052    
2023-01-06 16:56:42,173 - Epoch: [37][  110/  246]    Overall Loss 0.325913    Objective Loss 0.325913                                        LR 0.000060    Time 0.022972    
2023-01-06 16:56:42,405 - Epoch: [37][  120/  246]    Overall Loss 0.325808    Objective Loss 0.325808                                        LR 0.000060    Time 0.022988    
2023-01-06 16:56:42,634 - Epoch: [37][  130/  246]    Overall Loss 0.325332    Objective Loss 0.325332                                        LR 0.000060    Time 0.022980    
2023-01-06 16:56:42,858 - Epoch: [37][  140/  246]    Overall Loss 0.324781    Objective Loss 0.324781                                        LR 0.000060    Time 0.022929    
2023-01-06 16:56:43,079 - Epoch: [37][  150/  246]    Overall Loss 0.326178    Objective Loss 0.326178                                        LR 0.000060    Time 0.022877    
2023-01-06 16:56:43,302 - Epoch: [37][  160/  246]    Overall Loss 0.327121    Objective Loss 0.327121                                        LR 0.000060    Time 0.022835    
2023-01-06 16:56:43,521 - Epoch: [37][  170/  246]    Overall Loss 0.326578    Objective Loss 0.326578                                        LR 0.000060    Time 0.022779    
2023-01-06 16:56:43,744 - Epoch: [37][  180/  246]    Overall Loss 0.327024    Objective Loss 0.327024                                        LR 0.000060    Time 0.022748    
2023-01-06 16:56:43,973 - Epoch: [37][  190/  246]    Overall Loss 0.327805    Objective Loss 0.327805                                        LR 0.000060    Time 0.022753    
2023-01-06 16:56:44,194 - Epoch: [37][  200/  246]    Overall Loss 0.326541    Objective Loss 0.326541                                        LR 0.000060    Time 0.022721    
2023-01-06 16:56:44,415 - Epoch: [37][  210/  246]    Overall Loss 0.326032    Objective Loss 0.326032                                        LR 0.000060    Time 0.022688    
2023-01-06 16:56:44,636 - Epoch: [37][  220/  246]    Overall Loss 0.325909    Objective Loss 0.325909                                        LR 0.000060    Time 0.022660    
2023-01-06 16:56:44,857 - Epoch: [37][  230/  246]    Overall Loss 0.325602    Objective Loss 0.325602                                        LR 0.000060    Time 0.022633    
2023-01-06 16:56:45,088 - Epoch: [37][  240/  246]    Overall Loss 0.325866    Objective Loss 0.325866                                        LR 0.000060    Time 0.022649    
2023-01-06 16:56:45,180 - Epoch: [37][  246/  246]    Overall Loss 0.325983    Objective Loss 0.325983    Top1 88.277512    LR 0.000060    Time 0.022471    
2023-01-06 16:56:45,330 - --- validate (epoch=37)-----------
2023-01-06 16:56:45,330 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:45,783 - Epoch: [37][   10/   28]    Loss 0.335204    Top1 87.851562    
2023-01-06 16:56:45,900 - Epoch: [37][   20/   28]    Loss 0.324738    Top1 88.144531    
2023-01-06 16:56:45,970 - Epoch: [37][   28/   28]    Loss 0.325935    Top1 88.119095    
2023-01-06 16:56:46,111 - ==> Top1: 88.119    Loss: 0.326

2023-01-06 16:56:46,111 - ==> Confusion:
[[ 176   13  250]
 [  14  197  391]
 [  78   84 5783]]

2023-01-06 16:56:46,112 - ==> Best [Top1: 88.463   Sparsity:0.00   Params: 155168 on epoch: 36]
2023-01-06 16:56:46,113 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:46,121 - 

2023-01-06 16:56:46,121 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:46,842 - Epoch: [38][   10/  246]    Overall Loss 0.312044    Objective Loss 0.312044                                        LR 0.000060    Time 0.072022    
2023-01-06 16:56:47,039 - Epoch: [38][   20/  246]    Overall Loss 0.326821    Objective Loss 0.326821                                        LR 0.000060    Time 0.045850    
2023-01-06 16:56:47,233 - Epoch: [38][   30/  246]    Overall Loss 0.322606    Objective Loss 0.322606                                        LR 0.000060    Time 0.037018    
2023-01-06 16:56:47,425 - Epoch: [38][   40/  246]    Overall Loss 0.320941    Objective Loss 0.320941                                        LR 0.000060    Time 0.032549    
2023-01-06 16:56:47,617 - Epoch: [38][   50/  246]    Overall Loss 0.322191    Objective Loss 0.322191                                        LR 0.000060    Time 0.029872    
2023-01-06 16:56:47,809 - Epoch: [38][   60/  246]    Overall Loss 0.322388    Objective Loss 0.322388                                        LR 0.000060    Time 0.028094    
2023-01-06 16:56:47,999 - Epoch: [38][   70/  246]    Overall Loss 0.322707    Objective Loss 0.322707                                        LR 0.000060    Time 0.026789    
2023-01-06 16:56:48,190 - Epoch: [38][   80/  246]    Overall Loss 0.322841    Objective Loss 0.322841                                        LR 0.000060    Time 0.025827    
2023-01-06 16:56:48,381 - Epoch: [38][   90/  246]    Overall Loss 0.323342    Objective Loss 0.323342                                        LR 0.000060    Time 0.025073    
2023-01-06 16:56:48,570 - Epoch: [38][  100/  246]    Overall Loss 0.323021    Objective Loss 0.323021                                        LR 0.000060    Time 0.024456    
2023-01-06 16:56:48,761 - Epoch: [38][  110/  246]    Overall Loss 0.321668    Objective Loss 0.321668                                        LR 0.000060    Time 0.023959    
2023-01-06 16:56:48,952 - Epoch: [38][  120/  246]    Overall Loss 0.321278    Objective Loss 0.321278                                        LR 0.000060    Time 0.023550    
2023-01-06 16:56:49,143 - Epoch: [38][  130/  246]    Overall Loss 0.319939    Objective Loss 0.319939                                        LR 0.000060    Time 0.023207    
2023-01-06 16:56:49,333 - Epoch: [38][  140/  246]    Overall Loss 0.320332    Objective Loss 0.320332                                        LR 0.000060    Time 0.022908    
2023-01-06 16:56:49,522 - Epoch: [38][  150/  246]    Overall Loss 0.319776    Objective Loss 0.319776                                        LR 0.000060    Time 0.022637    
2023-01-06 16:56:49,712 - Epoch: [38][  160/  246]    Overall Loss 0.319330    Objective Loss 0.319330                                        LR 0.000060    Time 0.022407    
2023-01-06 16:56:49,902 - Epoch: [38][  170/  246]    Overall Loss 0.318835    Objective Loss 0.318835                                        LR 0.000060    Time 0.022205    
2023-01-06 16:56:50,092 - Epoch: [38][  180/  246]    Overall Loss 0.320762    Objective Loss 0.320762                                        LR 0.000060    Time 0.022026    
2023-01-06 16:56:50,283 - Epoch: [38][  190/  246]    Overall Loss 0.319952    Objective Loss 0.319952                                        LR 0.000060    Time 0.021868    
2023-01-06 16:56:50,474 - Epoch: [38][  200/  246]    Overall Loss 0.320624    Objective Loss 0.320624                                        LR 0.000060    Time 0.021727    
2023-01-06 16:56:50,665 - Epoch: [38][  210/  246]    Overall Loss 0.320585    Objective Loss 0.320585                                        LR 0.000060    Time 0.021600    
2023-01-06 16:56:50,855 - Epoch: [38][  220/  246]    Overall Loss 0.321563    Objective Loss 0.321563                                        LR 0.000060    Time 0.021482    
2023-01-06 16:56:51,045 - Epoch: [38][  230/  246]    Overall Loss 0.321887    Objective Loss 0.321887                                        LR 0.000060    Time 0.021371    
2023-01-06 16:56:51,248 - Epoch: [38][  240/  246]    Overall Loss 0.322669    Objective Loss 0.322669                                        LR 0.000060    Time 0.021327    
2023-01-06 16:56:51,345 - Epoch: [38][  246/  246]    Overall Loss 0.322804    Objective Loss 0.322804    Top1 87.799043    LR 0.000060    Time 0.021198    
2023-01-06 16:56:51,488 - --- validate (epoch=38)-----------
2023-01-06 16:56:51,489 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:51,938 - Epoch: [38][   10/   28]    Loss 0.313693    Top1 88.750000    
2023-01-06 16:56:52,051 - Epoch: [38][   20/   28]    Loss 0.324484    Top1 88.515625    
2023-01-06 16:56:52,120 - Epoch: [38][   28/   28]    Loss 0.322899    Top1 88.548526    
2023-01-06 16:56:52,278 - ==> Top1: 88.549    Loss: 0.323

2023-01-06 16:56:52,279 - ==> Confusion:
[[ 193    8  238]
 [  14  165  423]
 [  65   52 5828]]

2023-01-06 16:56:52,280 - ==> Best [Top1: 88.549   Sparsity:0.00   Params: 155168 on epoch: 38]
2023-01-06 16:56:52,280 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:52,287 - 

2023-01-06 16:56:52,287 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:53,007 - Epoch: [39][   10/  246]    Overall Loss 0.319413    Objective Loss 0.319413                                        LR 0.000060    Time 0.071905    
2023-01-06 16:56:53,199 - Epoch: [39][   20/  246]    Overall Loss 0.317777    Objective Loss 0.317777                                        LR 0.000060    Time 0.045531    
2023-01-06 16:56:53,391 - Epoch: [39][   30/  246]    Overall Loss 0.322933    Objective Loss 0.322933                                        LR 0.000060    Time 0.036712    
2023-01-06 16:56:53,584 - Epoch: [39][   40/  246]    Overall Loss 0.322465    Objective Loss 0.322465                                        LR 0.000060    Time 0.032338    
2023-01-06 16:56:53,768 - Epoch: [39][   50/  246]    Overall Loss 0.322153    Objective Loss 0.322153                                        LR 0.000060    Time 0.029547    
2023-01-06 16:56:53,955 - Epoch: [39][   60/  246]    Overall Loss 0.324471    Objective Loss 0.324471                                        LR 0.000060    Time 0.027736    
2023-01-06 16:56:54,147 - Epoch: [39][   70/  246]    Overall Loss 0.323921    Objective Loss 0.323921                                        LR 0.000060    Time 0.026504    
2023-01-06 16:56:54,334 - Epoch: [39][   80/  246]    Overall Loss 0.324987    Objective Loss 0.324987                                        LR 0.000060    Time 0.025526    
2023-01-06 16:56:54,531 - Epoch: [39][   90/  246]    Overall Loss 0.324887    Objective Loss 0.324887                                        LR 0.000060    Time 0.024875    
2023-01-06 16:56:54,720 - Epoch: [39][  100/  246]    Overall Loss 0.322542    Objective Loss 0.322542                                        LR 0.000060    Time 0.024274    
2023-01-06 16:56:54,916 - Epoch: [39][  110/  246]    Overall Loss 0.322500    Objective Loss 0.322500                                        LR 0.000060    Time 0.023845    
2023-01-06 16:56:55,108 - Epoch: [39][  120/  246]    Overall Loss 0.322169    Objective Loss 0.322169                                        LR 0.000060    Time 0.023450    
2023-01-06 16:56:55,292 - Epoch: [39][  130/  246]    Overall Loss 0.322715    Objective Loss 0.322715                                        LR 0.000060    Time 0.023060    
2023-01-06 16:56:55,469 - Epoch: [39][  140/  246]    Overall Loss 0.322435    Objective Loss 0.322435                                        LR 0.000060    Time 0.022671    
2023-01-06 16:56:55,669 - Epoch: [39][  150/  246]    Overall Loss 0.322594    Objective Loss 0.322594                                        LR 0.000060    Time 0.022487    
2023-01-06 16:56:55,846 - Epoch: [39][  160/  246]    Overall Loss 0.321694    Objective Loss 0.321694                                        LR 0.000060    Time 0.022190    
2023-01-06 16:56:56,035 - Epoch: [39][  170/  246]    Overall Loss 0.321651    Objective Loss 0.321651                                        LR 0.000060    Time 0.021994    
2023-01-06 16:56:56,227 - Epoch: [39][  180/  246]    Overall Loss 0.322785    Objective Loss 0.322785                                        LR 0.000060    Time 0.021833    
2023-01-06 16:56:56,420 - Epoch: [39][  190/  246]    Overall Loss 0.320900    Objective Loss 0.320900                                        LR 0.000060    Time 0.021700    
2023-01-06 16:56:56,609 - Epoch: [39][  200/  246]    Overall Loss 0.320120    Objective Loss 0.320120                                        LR 0.000060    Time 0.021555    
2023-01-06 16:56:56,802 - Epoch: [39][  210/  246]    Overall Loss 0.321065    Objective Loss 0.321065                                        LR 0.000060    Time 0.021445    
2023-01-06 16:56:56,991 - Epoch: [39][  220/  246]    Overall Loss 0.320882    Objective Loss 0.320882                                        LR 0.000060    Time 0.021328    
2023-01-06 16:56:57,190 - Epoch: [39][  230/  246]    Overall Loss 0.320495    Objective Loss 0.320495                                        LR 0.000060    Time 0.021267    
2023-01-06 16:56:57,402 - Epoch: [39][  240/  246]    Overall Loss 0.320966    Objective Loss 0.320966                                        LR 0.000060    Time 0.021259    
2023-01-06 16:56:57,498 - Epoch: [39][  246/  246]    Overall Loss 0.320744    Objective Loss 0.320744    Top1 89.712919    LR 0.000060    Time 0.021133    
2023-01-06 16:56:57,641 - --- validate (epoch=39)-----------
2023-01-06 16:56:57,641 - 6986 samples (256 per mini-batch)
2023-01-06 16:56:58,081 - Epoch: [39][   10/   28]    Loss 0.318269    Top1 89.023438    
2023-01-06 16:56:58,197 - Epoch: [39][   20/   28]    Loss 0.318483    Top1 88.593750    
2023-01-06 16:56:58,264 - Epoch: [39][   28/   28]    Loss 0.316267    Top1 88.577154    
2023-01-06 16:56:58,408 - ==> Top1: 88.577    Loss: 0.316

2023-01-06 16:56:58,409 - ==> Confusion:
[[ 183   12  244]
 [  12  196  394]
 [  63   73 5809]]

2023-01-06 16:56:58,410 - ==> Best [Top1: 88.577   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-06 16:56:58,410 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:56:58,418 - 

2023-01-06 16:56:58,418 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:56:58,993 - Epoch: [40][   10/  246]    Overall Loss 0.317277    Objective Loss 0.317277                                        LR 0.000036    Time 0.057491    
2023-01-06 16:56:59,194 - Epoch: [40][   20/  246]    Overall Loss 0.315677    Objective Loss 0.315677                                        LR 0.000036    Time 0.038763    
2023-01-06 16:56:59,395 - Epoch: [40][   30/  246]    Overall Loss 0.305327    Objective Loss 0.305327                                        LR 0.000036    Time 0.032516    
2023-01-06 16:56:59,597 - Epoch: [40][   40/  246]    Overall Loss 0.308375    Objective Loss 0.308375                                        LR 0.000036    Time 0.029431    
2023-01-06 16:56:59,797 - Epoch: [40][   50/  246]    Overall Loss 0.310010    Objective Loss 0.310010                                        LR 0.000036    Time 0.027541    
2023-01-06 16:56:59,999 - Epoch: [40][   60/  246]    Overall Loss 0.308040    Objective Loss 0.308040                                        LR 0.000036    Time 0.026314    
2023-01-06 16:57:00,199 - Epoch: [40][   70/  246]    Overall Loss 0.312181    Objective Loss 0.312181                                        LR 0.000036    Time 0.025405    
2023-01-06 16:57:00,403 - Epoch: [40][   80/  246]    Overall Loss 0.310812    Objective Loss 0.310812                                        LR 0.000036    Time 0.024777    
2023-01-06 16:57:00,604 - Epoch: [40][   90/  246]    Overall Loss 0.312714    Objective Loss 0.312714                                        LR 0.000036    Time 0.024251    
2023-01-06 16:57:00,809 - Epoch: [40][  100/  246]    Overall Loss 0.313613    Objective Loss 0.313613                                        LR 0.000036    Time 0.023874    
2023-01-06 16:57:01,009 - Epoch: [40][  110/  246]    Overall Loss 0.312683    Objective Loss 0.312683                                        LR 0.000036    Time 0.023522    
2023-01-06 16:57:01,209 - Epoch: [40][  120/  246]    Overall Loss 0.313928    Objective Loss 0.313928                                        LR 0.000036    Time 0.023222    
2023-01-06 16:57:01,409 - Epoch: [40][  130/  246]    Overall Loss 0.314402    Objective Loss 0.314402                                        LR 0.000036    Time 0.022973    
2023-01-06 16:57:01,611 - Epoch: [40][  140/  246]    Overall Loss 0.314120    Objective Loss 0.314120                                        LR 0.000036    Time 0.022772    
2023-01-06 16:57:01,808 - Epoch: [40][  150/  246]    Overall Loss 0.314971    Objective Loss 0.314971                                        LR 0.000036    Time 0.022563    
2023-01-06 16:57:02,009 - Epoch: [40][  160/  246]    Overall Loss 0.315487    Objective Loss 0.315487                                        LR 0.000036    Time 0.022405    
2023-01-06 16:57:02,207 - Epoch: [40][  170/  246]    Overall Loss 0.315190    Objective Loss 0.315190                                        LR 0.000036    Time 0.022250    
2023-01-06 16:57:02,408 - Epoch: [40][  180/  246]    Overall Loss 0.315948    Objective Loss 0.315948                                        LR 0.000036    Time 0.022130    
2023-01-06 16:57:02,608 - Epoch: [40][  190/  246]    Overall Loss 0.316650    Objective Loss 0.316650                                        LR 0.000036    Time 0.022018    
2023-01-06 16:57:02,810 - Epoch: [40][  200/  246]    Overall Loss 0.317396    Objective Loss 0.317396                                        LR 0.000036    Time 0.021922    
2023-01-06 16:57:03,007 - Epoch: [40][  210/  246]    Overall Loss 0.316447    Objective Loss 0.316447                                        LR 0.000036    Time 0.021815    
2023-01-06 16:57:03,199 - Epoch: [40][  220/  246]    Overall Loss 0.316733    Objective Loss 0.316733                                        LR 0.000036    Time 0.021697    
2023-01-06 16:57:03,387 - Epoch: [40][  230/  246]    Overall Loss 0.317679    Objective Loss 0.317679                                        LR 0.000036    Time 0.021567    
2023-01-06 16:57:03,590 - Epoch: [40][  240/  246]    Overall Loss 0.318094    Objective Loss 0.318094                                        LR 0.000036    Time 0.021514    
2023-01-06 16:57:03,685 - Epoch: [40][  246/  246]    Overall Loss 0.317784    Objective Loss 0.317784    Top1 87.799043    LR 0.000036    Time 0.021376    
2023-01-06 16:57:03,831 - --- validate (epoch=40)-----------
2023-01-06 16:57:03,831 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:04,287 - Epoch: [40][   10/   28]    Loss 0.306156    Top1 88.632812    
2023-01-06 16:57:04,404 - Epoch: [40][   20/   28]    Loss 0.313752    Top1 88.339844    
2023-01-06 16:57:04,472 - Epoch: [40][   28/   28]    Loss 0.313155    Top1 88.562840    
2023-01-06 16:57:04,602 - ==> Top1: 88.563    Loss: 0.313

2023-01-06 16:57:04,602 - ==> Confusion:
[[ 162    6  271]
 [  11  154  437]
 [  42   32 5871]]

2023-01-06 16:57:04,603 - ==> Best [Top1: 88.577   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-06 16:57:04,604 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:04,609 - 

2023-01-06 16:57:04,610 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:05,298 - Epoch: [41][   10/  246]    Overall Loss 0.305141    Objective Loss 0.305141                                        LR 0.000036    Time 0.068752    
2023-01-06 16:57:05,465 - Epoch: [41][   20/  246]    Overall Loss 0.321327    Objective Loss 0.321327                                        LR 0.000036    Time 0.042719    
2023-01-06 16:57:05,658 - Epoch: [41][   30/  246]    Overall Loss 0.316987    Objective Loss 0.316987                                        LR 0.000036    Time 0.034907    
2023-01-06 16:57:05,859 - Epoch: [41][   40/  246]    Overall Loss 0.315502    Objective Loss 0.315502                                        LR 0.000036    Time 0.031196    
2023-01-06 16:57:06,058 - Epoch: [41][   50/  246]    Overall Loss 0.317861    Objective Loss 0.317861                                        LR 0.000036    Time 0.028919    
2023-01-06 16:57:06,255 - Epoch: [41][   60/  246]    Overall Loss 0.318560    Objective Loss 0.318560                                        LR 0.000036    Time 0.027388    
2023-01-06 16:57:06,449 - Epoch: [41][   70/  246]    Overall Loss 0.319403    Objective Loss 0.319403                                        LR 0.000036    Time 0.026235    
2023-01-06 16:57:06,645 - Epoch: [41][   80/  246]    Overall Loss 0.321139    Objective Loss 0.321139                                        LR 0.000036    Time 0.025406    
2023-01-06 16:57:06,837 - Epoch: [41][   90/  246]    Overall Loss 0.323919    Objective Loss 0.323919                                        LR 0.000036    Time 0.024704    
2023-01-06 16:57:07,032 - Epoch: [41][  100/  246]    Overall Loss 0.322094    Objective Loss 0.322094                                        LR 0.000036    Time 0.024188    
2023-01-06 16:57:07,227 - Epoch: [41][  110/  246]    Overall Loss 0.317908    Objective Loss 0.317908                                        LR 0.000036    Time 0.023752    
2023-01-06 16:57:07,422 - Epoch: [41][  120/  246]    Overall Loss 0.316840    Objective Loss 0.316840                                        LR 0.000036    Time 0.023399    
2023-01-06 16:57:07,616 - Epoch: [41][  130/  246]    Overall Loss 0.316334    Objective Loss 0.316334                                        LR 0.000036    Time 0.023084    
2023-01-06 16:57:07,811 - Epoch: [41][  140/  246]    Overall Loss 0.315237    Objective Loss 0.315237                                        LR 0.000036    Time 0.022830    
2023-01-06 16:57:08,003 - Epoch: [41][  150/  246]    Overall Loss 0.314565    Objective Loss 0.314565                                        LR 0.000036    Time 0.022582    
2023-01-06 16:57:08,199 - Epoch: [41][  160/  246]    Overall Loss 0.315128    Objective Loss 0.315128                                        LR 0.000036    Time 0.022394    
2023-01-06 16:57:08,392 - Epoch: [41][  170/  246]    Overall Loss 0.314488    Objective Loss 0.314488                                        LR 0.000036    Time 0.022211    
2023-01-06 16:57:08,587 - Epoch: [41][  180/  246]    Overall Loss 0.316030    Objective Loss 0.316030                                        LR 0.000036    Time 0.022058    
2023-01-06 16:57:08,779 - Epoch: [41][  190/  246]    Overall Loss 0.315811    Objective Loss 0.315811                                        LR 0.000036    Time 0.021908    
2023-01-06 16:57:08,974 - Epoch: [41][  200/  246]    Overall Loss 0.315601    Objective Loss 0.315601                                        LR 0.000036    Time 0.021783    
2023-01-06 16:57:09,166 - Epoch: [41][  210/  246]    Overall Loss 0.315413    Objective Loss 0.315413                                        LR 0.000036    Time 0.021660    
2023-01-06 16:57:09,361 - Epoch: [41][  220/  246]    Overall Loss 0.316271    Objective Loss 0.316271                                        LR 0.000036    Time 0.021559    
2023-01-06 16:57:09,545 - Epoch: [41][  230/  246]    Overall Loss 0.316109    Objective Loss 0.316109                                        LR 0.000036    Time 0.021420    
2023-01-06 16:57:09,746 - Epoch: [41][  240/  246]    Overall Loss 0.317029    Objective Loss 0.317029                                        LR 0.000036    Time 0.021365    
2023-01-06 16:57:09,843 - Epoch: [41][  246/  246]    Overall Loss 0.316437    Objective Loss 0.316437    Top1 90.430622    LR 0.000036    Time 0.021235    
2023-01-06 16:57:09,988 - --- validate (epoch=41)-----------
2023-01-06 16:57:09,988 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:10,452 - Epoch: [41][   10/   28]    Loss 0.309988    Top1 88.476562    
2023-01-06 16:57:10,578 - Epoch: [41][   20/   28]    Loss 0.322098    Top1 88.085938    
2023-01-06 16:57:10,646 - Epoch: [41][   28/   28]    Loss 0.324137    Top1 87.832808    
2023-01-06 16:57:10,791 - ==> Top1: 87.833    Loss: 0.324

2023-01-06 16:57:10,792 - ==> Confusion:
[[ 128    8  303]
 [   8  118  476]
 [  29   26 5890]]

2023-01-06 16:57:10,793 - ==> Best [Top1: 88.577   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-06 16:57:10,793 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:10,799 - 

2023-01-06 16:57:10,799 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:11,356 - Epoch: [42][   10/  246]    Overall Loss 0.310509    Objective Loss 0.310509                                        LR 0.000036    Time 0.055686    
2023-01-06 16:57:11,529 - Epoch: [42][   20/  246]    Overall Loss 0.307690    Objective Loss 0.307690                                        LR 0.000036    Time 0.036439    
2023-01-06 16:57:11,714 - Epoch: [42][   30/  246]    Overall Loss 0.308637    Objective Loss 0.308637                                        LR 0.000036    Time 0.030456    
2023-01-06 16:57:11,913 - Epoch: [42][   40/  246]    Overall Loss 0.310134    Objective Loss 0.310134                                        LR 0.000036    Time 0.027810    
2023-01-06 16:57:12,106 - Epoch: [42][   50/  246]    Overall Loss 0.311557    Objective Loss 0.311557                                        LR 0.000036    Time 0.026086    
2023-01-06 16:57:12,306 - Epoch: [42][   60/  246]    Overall Loss 0.314866    Objective Loss 0.314866                                        LR 0.000036    Time 0.025070    
2023-01-06 16:57:12,498 - Epoch: [42][   70/  246]    Overall Loss 0.314685    Objective Loss 0.314685                                        LR 0.000036    Time 0.024223    
2023-01-06 16:57:12,689 - Epoch: [42][   80/  246]    Overall Loss 0.313550    Objective Loss 0.313550                                        LR 0.000036    Time 0.023570    
2023-01-06 16:57:12,887 - Epoch: [42][   90/  246]    Overall Loss 0.316406    Objective Loss 0.316406                                        LR 0.000036    Time 0.023145    
2023-01-06 16:57:13,078 - Epoch: [42][  100/  246]    Overall Loss 0.316752    Objective Loss 0.316752                                        LR 0.000036    Time 0.022740    
2023-01-06 16:57:13,266 - Epoch: [42][  110/  246]    Overall Loss 0.314516    Objective Loss 0.314516                                        LR 0.000036    Time 0.022375    
2023-01-06 16:57:13,456 - Epoch: [42][  120/  246]    Overall Loss 0.313770    Objective Loss 0.313770                                        LR 0.000036    Time 0.022092    
2023-01-06 16:57:13,648 - Epoch: [42][  130/  246]    Overall Loss 0.314772    Objective Loss 0.314772                                        LR 0.000036    Time 0.021868    
2023-01-06 16:57:13,838 - Epoch: [42][  140/  246]    Overall Loss 0.315341    Objective Loss 0.315341                                        LR 0.000036    Time 0.021663    
2023-01-06 16:57:14,031 - Epoch: [42][  150/  246]    Overall Loss 0.315955    Objective Loss 0.315955                                        LR 0.000036    Time 0.021500    
2023-01-06 16:57:14,219 - Epoch: [42][  160/  246]    Overall Loss 0.315144    Objective Loss 0.315144                                        LR 0.000036    Time 0.021329    
2023-01-06 16:57:14,404 - Epoch: [42][  170/  246]    Overall Loss 0.313842    Objective Loss 0.313842                                        LR 0.000036    Time 0.021162    
2023-01-06 16:57:14,590 - Epoch: [42][  180/  246]    Overall Loss 0.313401    Objective Loss 0.313401                                        LR 0.000036    Time 0.021018    
2023-01-06 16:57:14,778 - Epoch: [42][  190/  246]    Overall Loss 0.312895    Objective Loss 0.312895                                        LR 0.000036    Time 0.020899    
2023-01-06 16:57:14,965 - Epoch: [42][  200/  246]    Overall Loss 0.313442    Objective Loss 0.313442                                        LR 0.000036    Time 0.020789    
2023-01-06 16:57:15,153 - Epoch: [42][  210/  246]    Overall Loss 0.314500    Objective Loss 0.314500                                        LR 0.000036    Time 0.020691    
2023-01-06 16:57:15,341 - Epoch: [42][  220/  246]    Overall Loss 0.314989    Objective Loss 0.314989                                        LR 0.000036    Time 0.020603    
2023-01-06 16:57:15,528 - Epoch: [42][  230/  246]    Overall Loss 0.314806    Objective Loss 0.314806                                        LR 0.000036    Time 0.020520    
2023-01-06 16:57:15,726 - Epoch: [42][  240/  246]    Overall Loss 0.313925    Objective Loss 0.313925                                        LR 0.000036    Time 0.020492    
2023-01-06 16:57:15,822 - Epoch: [42][  246/  246]    Overall Loss 0.314379    Objective Loss 0.314379    Top1 86.602871    LR 0.000036    Time 0.020379    
2023-01-06 16:57:15,966 - --- validate (epoch=42)-----------
2023-01-06 16:57:15,966 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:16,410 - Epoch: [42][   10/   28]    Loss 0.320989    Top1 88.320312    
2023-01-06 16:57:16,522 - Epoch: [42][   20/   28]    Loss 0.315898    Top1 88.183594    
2023-01-06 16:57:16,591 - Epoch: [42][   28/   28]    Loss 0.316330    Top1 88.448325    
2023-01-06 16:57:16,739 - ==> Top1: 88.448    Loss: 0.316

2023-01-06 16:57:16,739 - ==> Confusion:
[[ 194   16  229]
 [  16  221  365]
 [  85   96 5764]]

2023-01-06 16:57:16,740 - ==> Best [Top1: 88.577   Sparsity:0.00   Params: 155168 on epoch: 39]
2023-01-06 16:57:16,740 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:16,746 - 

2023-01-06 16:57:16,746 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:17,462 - Epoch: [43][   10/  246]    Overall Loss 0.316224    Objective Loss 0.316224                                        LR 0.000036    Time 0.071552    
2023-01-06 16:57:17,668 - Epoch: [43][   20/  246]    Overall Loss 0.299987    Objective Loss 0.299987                                        LR 0.000036    Time 0.045930    
2023-01-06 16:57:17,863 - Epoch: [43][   30/  246]    Overall Loss 0.303983    Objective Loss 0.303983                                        LR 0.000036    Time 0.037119    
2023-01-06 16:57:18,065 - Epoch: [43][   40/  246]    Overall Loss 0.304463    Objective Loss 0.304463                                        LR 0.000036    Time 0.032868    
2023-01-06 16:57:18,257 - Epoch: [43][   50/  246]    Overall Loss 0.306216    Objective Loss 0.306216                                        LR 0.000036    Time 0.030144    
2023-01-06 16:57:18,457 - Epoch: [43][   60/  246]    Overall Loss 0.305420    Objective Loss 0.305420                                        LR 0.000036    Time 0.028449    
2023-01-06 16:57:18,651 - Epoch: [43][   70/  246]    Overall Loss 0.306590    Objective Loss 0.306590                                        LR 0.000036    Time 0.027144    
2023-01-06 16:57:18,853 - Epoch: [43][   80/  246]    Overall Loss 0.307430    Objective Loss 0.307430                                        LR 0.000036    Time 0.026274    
2023-01-06 16:57:19,046 - Epoch: [43][   90/  246]    Overall Loss 0.308228    Objective Loss 0.308228                                        LR 0.000036    Time 0.025497    
2023-01-06 16:57:19,249 - Epoch: [43][  100/  246]    Overall Loss 0.310259    Objective Loss 0.310259                                        LR 0.000036    Time 0.024975    
2023-01-06 16:57:19,444 - Epoch: [43][  110/  246]    Overall Loss 0.310858    Objective Loss 0.310858                                        LR 0.000036    Time 0.024471    
2023-01-06 16:57:19,651 - Epoch: [43][  120/  246]    Overall Loss 0.310852    Objective Loss 0.310852                                        LR 0.000036    Time 0.024155    
2023-01-06 16:57:19,850 - Epoch: [43][  130/  246]    Overall Loss 0.310773    Objective Loss 0.310773                                        LR 0.000036    Time 0.023822    
2023-01-06 16:57:20,059 - Epoch: [43][  140/  246]    Overall Loss 0.311756    Objective Loss 0.311756                                        LR 0.000036    Time 0.023615    
2023-01-06 16:57:20,258 - Epoch: [43][  150/  246]    Overall Loss 0.311705    Objective Loss 0.311705                                        LR 0.000036    Time 0.023362    
2023-01-06 16:57:20,467 - Epoch: [43][  160/  246]    Overall Loss 0.311678    Objective Loss 0.311678                                        LR 0.000036    Time 0.023204    
2023-01-06 16:57:20,666 - Epoch: [43][  170/  246]    Overall Loss 0.313771    Objective Loss 0.313771                                        LR 0.000036    Time 0.023011    
2023-01-06 16:57:20,875 - Epoch: [43][  180/  246]    Overall Loss 0.314622    Objective Loss 0.314622                                        LR 0.000036    Time 0.022893    
2023-01-06 16:57:21,074 - Epoch: [43][  190/  246]    Overall Loss 0.314486    Objective Loss 0.314486                                        LR 0.000036    Time 0.022730    
2023-01-06 16:57:21,282 - Epoch: [43][  200/  246]    Overall Loss 0.314086    Objective Loss 0.314086                                        LR 0.000036    Time 0.022634    
2023-01-06 16:57:21,480 - Epoch: [43][  210/  246]    Overall Loss 0.314419    Objective Loss 0.314419                                        LR 0.000036    Time 0.022498    
2023-01-06 16:57:21,680 - Epoch: [43][  220/  246]    Overall Loss 0.314732    Objective Loss 0.314732                                        LR 0.000036    Time 0.022383    
2023-01-06 16:57:21,874 - Epoch: [43][  230/  246]    Overall Loss 0.315213    Objective Loss 0.315213                                        LR 0.000036    Time 0.022249    
2023-01-06 16:57:22,083 - Epoch: [43][  240/  246]    Overall Loss 0.313567    Objective Loss 0.313567                                        LR 0.000036    Time 0.022194    
2023-01-06 16:57:22,178 - Epoch: [43][  246/  246]    Overall Loss 0.312914    Objective Loss 0.312914    Top1 88.277512    LR 0.000036    Time 0.022039    
2023-01-06 16:57:22,317 - --- validate (epoch=43)-----------
2023-01-06 16:57:22,317 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:22,757 - Epoch: [43][   10/   28]    Loss 0.324493    Top1 87.734375    
2023-01-06 16:57:22,868 - Epoch: [43][   20/   28]    Loss 0.306887    Top1 88.808594    
2023-01-06 16:57:22,939 - Epoch: [43][   28/   28]    Loss 0.312524    Top1 88.663040    
2023-01-06 16:57:23,106 - ==> Top1: 88.663    Loss: 0.313

2023-01-06 16:57:23,106 - ==> Confusion:
[[ 178    7  254]
 [  10  158  434]
 [  49   38 5858]]

2023-01-06 16:57:23,108 - ==> Best [Top1: 88.663   Sparsity:0.00   Params: 155168 on epoch: 43]
2023-01-06 16:57:23,108 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:23,115 - 

2023-01-06 16:57:23,115 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:23,829 - Epoch: [44][   10/  246]    Overall Loss 0.332969    Objective Loss 0.332969                                        LR 0.000036    Time 0.071326    
2023-01-06 16:57:24,022 - Epoch: [44][   20/  246]    Overall Loss 0.327393    Objective Loss 0.327393                                        LR 0.000036    Time 0.045267    
2023-01-06 16:57:24,212 - Epoch: [44][   30/  246]    Overall Loss 0.329178    Objective Loss 0.329178                                        LR 0.000036    Time 0.036502    
2023-01-06 16:57:24,408 - Epoch: [44][   40/  246]    Overall Loss 0.326312    Objective Loss 0.326312                                        LR 0.000036    Time 0.032283    
2023-01-06 16:57:24,601 - Epoch: [44][   50/  246]    Overall Loss 0.324105    Objective Loss 0.324105                                        LR 0.000036    Time 0.029673    
2023-01-06 16:57:24,795 - Epoch: [44][   60/  246]    Overall Loss 0.319778    Objective Loss 0.319778                                        LR 0.000036    Time 0.027952    
2023-01-06 16:57:24,989 - Epoch: [44][   70/  246]    Overall Loss 0.318467    Objective Loss 0.318467                                        LR 0.000036    Time 0.026720    
2023-01-06 16:57:25,183 - Epoch: [44][   80/  246]    Overall Loss 0.318510    Objective Loss 0.318510                                        LR 0.000036    Time 0.025801    
2023-01-06 16:57:25,376 - Epoch: [44][   90/  246]    Overall Loss 0.318195    Objective Loss 0.318195                                        LR 0.000036    Time 0.025079    
2023-01-06 16:57:25,571 - Epoch: [44][  100/  246]    Overall Loss 0.316525    Objective Loss 0.316525                                        LR 0.000036    Time 0.024512    
2023-01-06 16:57:25,765 - Epoch: [44][  110/  246]    Overall Loss 0.313926    Objective Loss 0.313926                                        LR 0.000036    Time 0.024044    
2023-01-06 16:57:25,958 - Epoch: [44][  120/  246]    Overall Loss 0.315590    Objective Loss 0.315590                                        LR 0.000036    Time 0.023652    
2023-01-06 16:57:26,151 - Epoch: [44][  130/  246]    Overall Loss 0.317095    Objective Loss 0.317095                                        LR 0.000036    Time 0.023310    
2023-01-06 16:57:26,345 - Epoch: [44][  140/  246]    Overall Loss 0.316375    Objective Loss 0.316375                                        LR 0.000036    Time 0.023028    
2023-01-06 16:57:26,538 - Epoch: [44][  150/  246]    Overall Loss 0.315047    Objective Loss 0.315047                                        LR 0.000036    Time 0.022778    
2023-01-06 16:57:26,733 - Epoch: [44][  160/  246]    Overall Loss 0.316292    Objective Loss 0.316292                                        LR 0.000036    Time 0.022570    
2023-01-06 16:57:26,926 - Epoch: [44][  170/  246]    Overall Loss 0.315730    Objective Loss 0.315730                                        LR 0.000036    Time 0.022378    
2023-01-06 16:57:27,121 - Epoch: [44][  180/  246]    Overall Loss 0.315255    Objective Loss 0.315255                                        LR 0.000036    Time 0.022214    
2023-01-06 16:57:27,314 - Epoch: [44][  190/  246]    Overall Loss 0.315458    Objective Loss 0.315458                                        LR 0.000036    Time 0.022062    
2023-01-06 16:57:27,509 - Epoch: [44][  200/  246]    Overall Loss 0.314343    Objective Loss 0.314343                                        LR 0.000036    Time 0.021928    
2023-01-06 16:57:27,703 - Epoch: [44][  210/  246]    Overall Loss 0.313453    Objective Loss 0.313453                                        LR 0.000036    Time 0.021808    
2023-01-06 16:57:27,900 - Epoch: [44][  220/  246]    Overall Loss 0.313413    Objective Loss 0.313413                                        LR 0.000036    Time 0.021710    
2023-01-06 16:57:28,095 - Epoch: [44][  230/  246]    Overall Loss 0.313024    Objective Loss 0.313024                                        LR 0.000036    Time 0.021614    
2023-01-06 16:57:28,305 - Epoch: [44][  240/  246]    Overall Loss 0.312832    Objective Loss 0.312832                                        LR 0.000036    Time 0.021584    
2023-01-06 16:57:28,401 - Epoch: [44][  246/  246]    Overall Loss 0.312348    Objective Loss 0.312348    Top1 89.473684    LR 0.000036    Time 0.021450    
2023-01-06 16:57:28,523 - --- validate (epoch=44)-----------
2023-01-06 16:57:28,523 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:28,981 - Epoch: [44][   10/   28]    Loss 0.322847    Top1 87.890625    
2023-01-06 16:57:29,098 - Epoch: [44][   20/   28]    Loss 0.315476    Top1 87.988281    
2023-01-06 16:57:29,179 - Epoch: [44][   28/   28]    Loss 0.312867    Top1 88.190667    
2023-01-06 16:57:29,319 - ==> Top1: 88.191    Loss: 0.313

2023-01-06 16:57:29,320 - ==> Confusion:
[[ 140    7  292]
 [   6  140  456]
 [  28   36 5881]]

2023-01-06 16:57:29,321 - ==> Best [Top1: 88.663   Sparsity:0.00   Params: 155168 on epoch: 43]
2023-01-06 16:57:29,321 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:29,327 - 

2023-01-06 16:57:29,327 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:29,921 - Epoch: [45][   10/  246]    Overall Loss 0.304517    Objective Loss 0.304517                                        LR 0.000036    Time 0.059363    
2023-01-06 16:57:30,117 - Epoch: [45][   20/  246]    Overall Loss 0.310122    Objective Loss 0.310122                                        LR 0.000036    Time 0.039451    
2023-01-06 16:57:30,310 - Epoch: [45][   30/  246]    Overall Loss 0.309947    Objective Loss 0.309947                                        LR 0.000036    Time 0.032724    
2023-01-06 16:57:30,503 - Epoch: [45][   40/  246]    Overall Loss 0.308042    Objective Loss 0.308042                                        LR 0.000036    Time 0.029344    
2023-01-06 16:57:30,700 - Epoch: [45][   50/  246]    Overall Loss 0.306357    Objective Loss 0.306357                                        LR 0.000036    Time 0.027406    
2023-01-06 16:57:30,893 - Epoch: [45][   60/  246]    Overall Loss 0.305310    Objective Loss 0.305310                                        LR 0.000036    Time 0.026050    
2023-01-06 16:57:31,082 - Epoch: [45][   70/  246]    Overall Loss 0.306549    Objective Loss 0.306549                                        LR 0.000036    Time 0.025026    
2023-01-06 16:57:31,269 - Epoch: [45][   80/  246]    Overall Loss 0.311067    Objective Loss 0.311067                                        LR 0.000036    Time 0.024232    
2023-01-06 16:57:31,456 - Epoch: [45][   90/  246]    Overall Loss 0.310384    Objective Loss 0.310384                                        LR 0.000036    Time 0.023610    
2023-01-06 16:57:31,644 - Epoch: [45][  100/  246]    Overall Loss 0.308472    Objective Loss 0.308472                                        LR 0.000036    Time 0.023127    
2023-01-06 16:57:31,831 - Epoch: [45][  110/  246]    Overall Loss 0.309581    Objective Loss 0.309581                                        LR 0.000036    Time 0.022723    
2023-01-06 16:57:32,020 - Epoch: [45][  120/  246]    Overall Loss 0.309063    Objective Loss 0.309063                                        LR 0.000036    Time 0.022404    
2023-01-06 16:57:32,209 - Epoch: [45][  130/  246]    Overall Loss 0.308872    Objective Loss 0.308872                                        LR 0.000036    Time 0.022132    
2023-01-06 16:57:32,401 - Epoch: [45][  140/  246]    Overall Loss 0.309308    Objective Loss 0.309308                                        LR 0.000036    Time 0.021919    
2023-01-06 16:57:32,587 - Epoch: [45][  150/  246]    Overall Loss 0.309512    Objective Loss 0.309512                                        LR 0.000036    Time 0.021699    
2023-01-06 16:57:32,780 - Epoch: [45][  160/  246]    Overall Loss 0.309048    Objective Loss 0.309048                                        LR 0.000036    Time 0.021541    
2023-01-06 16:57:32,970 - Epoch: [45][  170/  246]    Overall Loss 0.308421    Objective Loss 0.308421                                        LR 0.000036    Time 0.021389    
2023-01-06 16:57:33,160 - Epoch: [45][  180/  246]    Overall Loss 0.309167    Objective Loss 0.309167                                        LR 0.000036    Time 0.021257    
2023-01-06 16:57:33,354 - Epoch: [45][  190/  246]    Overall Loss 0.309198    Objective Loss 0.309198                                        LR 0.000036    Time 0.021155    
2023-01-06 16:57:33,545 - Epoch: [45][  200/  246]    Overall Loss 0.310105    Objective Loss 0.310105                                        LR 0.000036    Time 0.021052    
2023-01-06 16:57:33,738 - Epoch: [45][  210/  246]    Overall Loss 0.310102    Objective Loss 0.310102                                        LR 0.000036    Time 0.020959    
2023-01-06 16:57:33,930 - Epoch: [45][  220/  246]    Overall Loss 0.310243    Objective Loss 0.310243                                        LR 0.000036    Time 0.020878    
2023-01-06 16:57:34,122 - Epoch: [45][  230/  246]    Overall Loss 0.309996    Objective Loss 0.309996                                        LR 0.000036    Time 0.020804    
2023-01-06 16:57:34,331 - Epoch: [45][  240/  246]    Overall Loss 0.309833    Objective Loss 0.309833                                        LR 0.000036    Time 0.020807    
2023-01-06 16:57:34,429 - Epoch: [45][  246/  246]    Overall Loss 0.310328    Objective Loss 0.310328    Top1 86.363636    LR 0.000036    Time 0.020695    
2023-01-06 16:57:34,554 - --- validate (epoch=45)-----------
2023-01-06 16:57:34,554 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:35,015 - Epoch: [45][   10/   28]    Loss 0.310410    Top1 88.906250    
2023-01-06 16:57:35,140 - Epoch: [45][   20/   28]    Loss 0.317678    Top1 88.515625    
2023-01-06 16:57:35,207 - Epoch: [45][   28/   28]    Loss 0.306242    Top1 89.063842    
2023-01-06 16:57:35,342 - ==> Top1: 89.064    Loss: 0.306

2023-01-06 16:57:35,343 - ==> Confusion:
[[ 192    9  238]
 [  14  195  393]
 [  57   53 5835]]

2023-01-06 16:57:35,344 - ==> Best [Top1: 89.064   Sparsity:0.00   Params: 155168 on epoch: 45]
2023-01-06 16:57:35,344 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:35,352 - 

2023-01-06 16:57:35,352 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:36,085 - Epoch: [46][   10/  246]    Overall Loss 0.313024    Objective Loss 0.313024                                        LR 0.000036    Time 0.073250    
2023-01-06 16:57:36,289 - Epoch: [46][   20/  246]    Overall Loss 0.318283    Objective Loss 0.318283                                        LR 0.000036    Time 0.046795    
2023-01-06 16:57:36,493 - Epoch: [46][   30/  246]    Overall Loss 0.311865    Objective Loss 0.311865                                        LR 0.000036    Time 0.038006    
2023-01-06 16:57:36,701 - Epoch: [46][   40/  246]    Overall Loss 0.309386    Objective Loss 0.309386                                        LR 0.000036    Time 0.033678    
2023-01-06 16:57:36,902 - Epoch: [46][   50/  246]    Overall Loss 0.316211    Objective Loss 0.316211                                        LR 0.000036    Time 0.030959    
2023-01-06 16:57:37,103 - Epoch: [46][   60/  246]    Overall Loss 0.315968    Objective Loss 0.315968                                        LR 0.000036    Time 0.029148    
2023-01-06 16:57:37,304 - Epoch: [46][   70/  246]    Overall Loss 0.316510    Objective Loss 0.316510                                        LR 0.000036    Time 0.027852    
2023-01-06 16:57:37,508 - Epoch: [46][   80/  246]    Overall Loss 0.313481    Objective Loss 0.313481                                        LR 0.000036    Time 0.026911    
2023-01-06 16:57:37,710 - Epoch: [46][   90/  246]    Overall Loss 0.315036    Objective Loss 0.315036                                        LR 0.000036    Time 0.026162    
2023-01-06 16:57:37,912 - Epoch: [46][  100/  246]    Overall Loss 0.314374    Objective Loss 0.314374                                        LR 0.000036    Time 0.025558    
2023-01-06 16:57:38,112 - Epoch: [46][  110/  246]    Overall Loss 0.312337    Objective Loss 0.312337                                        LR 0.000036    Time 0.025055    
2023-01-06 16:57:38,305 - Epoch: [46][  120/  246]    Overall Loss 0.313119    Objective Loss 0.313119                                        LR 0.000036    Time 0.024570    
2023-01-06 16:57:38,499 - Epoch: [46][  130/  246]    Overall Loss 0.311355    Objective Loss 0.311355                                        LR 0.000036    Time 0.024170    
2023-01-06 16:57:38,695 - Epoch: [46][  140/  246]    Overall Loss 0.311606    Objective Loss 0.311606                                        LR 0.000036    Time 0.023839    
2023-01-06 16:57:38,889 - Epoch: [46][  150/  246]    Overall Loss 0.313008    Objective Loss 0.313008                                        LR 0.000036    Time 0.023544    
2023-01-06 16:57:39,086 - Epoch: [46][  160/  246]    Overall Loss 0.313112    Objective Loss 0.313112                                        LR 0.000036    Time 0.023297    
2023-01-06 16:57:39,281 - Epoch: [46][  170/  246]    Overall Loss 0.311621    Objective Loss 0.311621                                        LR 0.000036    Time 0.023071    
2023-01-06 16:57:39,475 - Epoch: [46][  180/  246]    Overall Loss 0.311774    Objective Loss 0.311774                                        LR 0.000036    Time 0.022868    
2023-01-06 16:57:39,670 - Epoch: [46][  190/  246]    Overall Loss 0.311432    Objective Loss 0.311432                                        LR 0.000036    Time 0.022687    
2023-01-06 16:57:39,865 - Epoch: [46][  200/  246]    Overall Loss 0.310219    Objective Loss 0.310219                                        LR 0.000036    Time 0.022525    
2023-01-06 16:57:40,060 - Epoch: [46][  210/  246]    Overall Loss 0.308557    Objective Loss 0.308557                                        LR 0.000036    Time 0.022383    
2023-01-06 16:57:40,258 - Epoch: [46][  220/  246]    Overall Loss 0.309235    Objective Loss 0.309235                                        LR 0.000036    Time 0.022263    
2023-01-06 16:57:40,451 - Epoch: [46][  230/  246]    Overall Loss 0.308946    Objective Loss 0.308946                                        LR 0.000036    Time 0.022133    
2023-01-06 16:57:40,656 - Epoch: [46][  240/  246]    Overall Loss 0.308718    Objective Loss 0.308718                                        LR 0.000036    Time 0.022064    
2023-01-06 16:57:40,752 - Epoch: [46][  246/  246]    Overall Loss 0.308721    Objective Loss 0.308721    Top1 87.799043    LR 0.000036    Time 0.021915    
2023-01-06 16:57:40,905 - --- validate (epoch=46)-----------
2023-01-06 16:57:40,905 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:41,365 - Epoch: [46][   10/   28]    Loss 0.308108    Top1 88.984375    
2023-01-06 16:57:41,482 - Epoch: [46][   20/   28]    Loss 0.318328    Top1 88.710938    
2023-01-06 16:57:41,550 - Epoch: [46][   28/   28]    Loss 0.313852    Top1 88.920699    
2023-01-06 16:57:41,708 - ==> Top1: 88.921    Loss: 0.314

2023-01-06 16:57:41,708 - ==> Confusion:
[[ 220    8  211]
 [  24  192  386]
 [  84   61 5800]]

2023-01-06 16:57:41,709 - ==> Best [Top1: 89.064   Sparsity:0.00   Params: 155168 on epoch: 45]
2023-01-06 16:57:41,709 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:41,715 - 

2023-01-06 16:57:41,715 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:42,281 - Epoch: [47][   10/  246]    Overall Loss 0.313990    Objective Loss 0.313990                                        LR 0.000036    Time 0.056475    
2023-01-06 16:57:42,475 - Epoch: [47][   20/  246]    Overall Loss 0.310644    Objective Loss 0.310644                                        LR 0.000036    Time 0.037925    
2023-01-06 16:57:42,672 - Epoch: [47][   30/  246]    Overall Loss 0.312008    Objective Loss 0.312008                                        LR 0.000036    Time 0.031837    
2023-01-06 16:57:42,868 - Epoch: [47][   40/  246]    Overall Loss 0.311952    Objective Loss 0.311952                                        LR 0.000036    Time 0.028765    
2023-01-06 16:57:43,061 - Epoch: [47][   50/  246]    Overall Loss 0.316173    Objective Loss 0.316173                                        LR 0.000036    Time 0.026872    
2023-01-06 16:57:43,255 - Epoch: [47][   60/  246]    Overall Loss 0.313127    Objective Loss 0.313127                                        LR 0.000036    Time 0.025611    
2023-01-06 16:57:43,448 - Epoch: [47][   70/  246]    Overall Loss 0.310988    Objective Loss 0.310988                                        LR 0.000036    Time 0.024713    
2023-01-06 16:57:43,641 - Epoch: [47][   80/  246]    Overall Loss 0.311512    Objective Loss 0.311512                                        LR 0.000036    Time 0.024034    
2023-01-06 16:57:43,834 - Epoch: [47][   90/  246]    Overall Loss 0.310614    Objective Loss 0.310614                                        LR 0.000036    Time 0.023503    
2023-01-06 16:57:44,027 - Epoch: [47][  100/  246]    Overall Loss 0.309736    Objective Loss 0.309736                                        LR 0.000036    Time 0.023078    
2023-01-06 16:57:44,221 - Epoch: [47][  110/  246]    Overall Loss 0.309333    Objective Loss 0.309333                                        LR 0.000036    Time 0.022737    
2023-01-06 16:57:44,414 - Epoch: [47][  120/  246]    Overall Loss 0.309973    Objective Loss 0.309973                                        LR 0.000036    Time 0.022446    
2023-01-06 16:57:44,606 - Epoch: [47][  130/  246]    Overall Loss 0.309981    Objective Loss 0.309981                                        LR 0.000036    Time 0.022193    
2023-01-06 16:57:44,798 - Epoch: [47][  140/  246]    Overall Loss 0.310706    Objective Loss 0.310706                                        LR 0.000036    Time 0.021978    
2023-01-06 16:57:44,990 - Epoch: [47][  150/  246]    Overall Loss 0.310394    Objective Loss 0.310394                                        LR 0.000036    Time 0.021788    
2023-01-06 16:57:45,182 - Epoch: [47][  160/  246]    Overall Loss 0.309888    Objective Loss 0.309888                                        LR 0.000036    Time 0.021626    
2023-01-06 16:57:45,374 - Epoch: [47][  170/  246]    Overall Loss 0.309984    Objective Loss 0.309984                                        LR 0.000036    Time 0.021482    
2023-01-06 16:57:45,566 - Epoch: [47][  180/  246]    Overall Loss 0.309663    Objective Loss 0.309663                                        LR 0.000036    Time 0.021356    
2023-01-06 16:57:45,759 - Epoch: [47][  190/  246]    Overall Loss 0.309682    Objective Loss 0.309682                                        LR 0.000036    Time 0.021243    
2023-01-06 16:57:45,952 - Epoch: [47][  200/  246]    Overall Loss 0.309212    Objective Loss 0.309212                                        LR 0.000036    Time 0.021144    
2023-01-06 16:57:46,144 - Epoch: [47][  210/  246]    Overall Loss 0.310324    Objective Loss 0.310324                                        LR 0.000036    Time 0.021052    
2023-01-06 16:57:46,340 - Epoch: [47][  220/  246]    Overall Loss 0.310163    Objective Loss 0.310163                                        LR 0.000036    Time 0.020984    
2023-01-06 16:57:46,535 - Epoch: [47][  230/  246]    Overall Loss 0.309893    Objective Loss 0.309893                                        LR 0.000036    Time 0.020916    
2023-01-06 16:57:46,745 - Epoch: [47][  240/  246]    Overall Loss 0.309281    Objective Loss 0.309281                                        LR 0.000036    Time 0.020918    
2023-01-06 16:57:46,841 - Epoch: [47][  246/  246]    Overall Loss 0.308835    Objective Loss 0.308835    Top1 90.191388    LR 0.000036    Time 0.020796    
2023-01-06 16:57:46,971 - --- validate (epoch=47)-----------
2023-01-06 16:57:46,972 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:47,431 - Epoch: [47][   10/   28]    Loss 0.324767    Top1 88.164062    
2023-01-06 16:57:47,549 - Epoch: [47][   20/   28]    Loss 0.312556    Top1 88.671875    
2023-01-06 16:57:47,618 - Epoch: [47][   28/   28]    Loss 0.309031    Top1 88.877756    
2023-01-06 16:57:47,774 - ==> Top1: 88.878    Loss: 0.309

2023-01-06 16:57:47,774 - ==> Confusion:
[[ 192    8  239]
 [  14  175  413]
 [  55   48 5842]]

2023-01-06 16:57:47,775 - ==> Best [Top1: 89.064   Sparsity:0.00   Params: 155168 on epoch: 45]
2023-01-06 16:57:47,775 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:47,781 - 

2023-01-06 16:57:47,781 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:48,506 - Epoch: [48][   10/  246]    Overall Loss 0.314677    Objective Loss 0.314677                                        LR 0.000036    Time 0.072387    
2023-01-06 16:57:48,711 - Epoch: [48][   20/  246]    Overall Loss 0.305904    Objective Loss 0.305904                                        LR 0.000036    Time 0.046423    
2023-01-06 16:57:48,906 - Epoch: [48][   30/  246]    Overall Loss 0.301138    Objective Loss 0.301138                                        LR 0.000036    Time 0.037457    
2023-01-06 16:57:49,110 - Epoch: [48][   40/  246]    Overall Loss 0.301723    Objective Loss 0.301723                                        LR 0.000036    Time 0.033161    
2023-01-06 16:57:49,306 - Epoch: [48][   50/  246]    Overall Loss 0.306630    Objective Loss 0.306630                                        LR 0.000036    Time 0.030447    
2023-01-06 16:57:49,509 - Epoch: [48][   60/  246]    Overall Loss 0.307180    Objective Loss 0.307180                                        LR 0.000036    Time 0.028751    
2023-01-06 16:57:49,706 - Epoch: [48][   70/  246]    Overall Loss 0.308446    Objective Loss 0.308446                                        LR 0.000036    Time 0.027458    
2023-01-06 16:57:49,910 - Epoch: [48][   80/  246]    Overall Loss 0.308610    Objective Loss 0.308610                                        LR 0.000036    Time 0.026574    
2023-01-06 16:57:50,108 - Epoch: [48][   90/  246]    Overall Loss 0.308161    Objective Loss 0.308161                                        LR 0.000036    Time 0.025811    
2023-01-06 16:57:50,313 - Epoch: [48][  100/  246]    Overall Loss 0.307722    Objective Loss 0.307722                                        LR 0.000036    Time 0.025276    
2023-01-06 16:57:50,512 - Epoch: [48][  110/  246]    Overall Loss 0.307976    Objective Loss 0.307976                                        LR 0.000036    Time 0.024781    
2023-01-06 16:57:50,716 - Epoch: [48][  120/  246]    Overall Loss 0.307874    Objective Loss 0.307874                                        LR 0.000036    Time 0.024418    
2023-01-06 16:57:50,915 - Epoch: [48][  130/  246]    Overall Loss 0.309942    Objective Loss 0.309942                                        LR 0.000036    Time 0.024063    
2023-01-06 16:57:51,117 - Epoch: [48][  140/  246]    Overall Loss 0.309706    Objective Loss 0.309706                                        LR 0.000036    Time 0.023780    
2023-01-06 16:57:51,320 - Epoch: [48][  150/  246]    Overall Loss 0.310471    Objective Loss 0.310471                                        LR 0.000036    Time 0.023543    
2023-01-06 16:57:51,534 - Epoch: [48][  160/  246]    Overall Loss 0.309741    Objective Loss 0.309741                                        LR 0.000036    Time 0.023405    
2023-01-06 16:57:51,736 - Epoch: [48][  170/  246]    Overall Loss 0.310242    Objective Loss 0.310242                                        LR 0.000036    Time 0.023213    
2023-01-06 16:57:51,936 - Epoch: [48][  180/  246]    Overall Loss 0.309622    Objective Loss 0.309622                                        LR 0.000036    Time 0.023033    
2023-01-06 16:57:52,106 - Epoch: [48][  190/  246]    Overall Loss 0.308855    Objective Loss 0.308855                                        LR 0.000036    Time 0.022718    
2023-01-06 16:57:52,273 - Epoch: [48][  200/  246]    Overall Loss 0.308991    Objective Loss 0.308991                                        LR 0.000036    Time 0.022413    
2023-01-06 16:57:52,439 - Epoch: [48][  210/  246]    Overall Loss 0.309903    Objective Loss 0.309903                                        LR 0.000036    Time 0.022133    
2023-01-06 16:57:52,606 - Epoch: [48][  220/  246]    Overall Loss 0.309626    Objective Loss 0.309626                                        LR 0.000036    Time 0.021885    
2023-01-06 16:57:52,774 - Epoch: [48][  230/  246]    Overall Loss 0.310920    Objective Loss 0.310920                                        LR 0.000036    Time 0.021664    
2023-01-06 16:57:52,995 - Epoch: [48][  240/  246]    Overall Loss 0.309569    Objective Loss 0.309569                                        LR 0.000036    Time 0.021677    
2023-01-06 16:57:53,090 - Epoch: [48][  246/  246]    Overall Loss 0.309362    Objective Loss 0.309362    Top1 89.952153    LR 0.000036    Time 0.021537    
2023-01-06 16:57:53,224 - --- validate (epoch=48)-----------
2023-01-06 16:57:53,224 - 6986 samples (256 per mini-batch)
2023-01-06 16:57:53,691 - Epoch: [48][   10/   28]    Loss 0.327020    Top1 88.398438    
2023-01-06 16:57:53,824 - Epoch: [48][   20/   28]    Loss 0.314585    Top1 88.769531    
2023-01-06 16:57:53,895 - Epoch: [48][   28/   28]    Loss 0.311202    Top1 89.049528    
2023-01-06 16:57:54,052 - ==> Top1: 89.050    Loss: 0.311

2023-01-06 16:57:54,053 - ==> Confusion:
[[ 186    7  246]
 [  15  176  411]
 [  47   39 5859]]

2023-01-06 16:57:54,054 - ==> Best [Top1: 89.064   Sparsity:0.00   Params: 155168 on epoch: 45]
2023-01-06 16:57:54,054 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:57:54,061 - 

2023-01-06 16:57:54,061 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:57:54,654 - Epoch: [49][   10/  246]    Overall Loss 0.302546    Objective Loss 0.302546                                        LR 0.000036    Time 0.059270    
2023-01-06 16:57:54,857 - Epoch: [49][   20/  246]    Overall Loss 0.304947    Objective Loss 0.304947                                        LR 0.000036    Time 0.039744    
2023-01-06 16:57:55,063 - Epoch: [49][   30/  246]    Overall Loss 0.301675    Objective Loss 0.301675                                        LR 0.000036    Time 0.033377    
2023-01-06 16:57:55,266 - Epoch: [49][   40/  246]    Overall Loss 0.303031    Objective Loss 0.303031                                        LR 0.000036    Time 0.030079    
2023-01-06 16:57:55,466 - Epoch: [49][   50/  246]    Overall Loss 0.303276    Objective Loss 0.303276                                        LR 0.000036    Time 0.028070    
2023-01-06 16:57:55,662 - Epoch: [49][   60/  246]    Overall Loss 0.301456    Objective Loss 0.301456                                        LR 0.000036    Time 0.026638    
2023-01-06 16:57:55,860 - Epoch: [49][   70/  246]    Overall Loss 0.303260    Objective Loss 0.303260                                        LR 0.000036    Time 0.025660    
2023-01-06 16:57:56,054 - Epoch: [49][   80/  246]    Overall Loss 0.303255    Objective Loss 0.303255                                        LR 0.000036    Time 0.024879    
2023-01-06 16:57:56,265 - Epoch: [49][   90/  246]    Overall Loss 0.302616    Objective Loss 0.302616                                        LR 0.000036    Time 0.024455    
2023-01-06 16:57:56,471 - Epoch: [49][  100/  246]    Overall Loss 0.301476    Objective Loss 0.301476                                        LR 0.000036    Time 0.024059    
2023-01-06 16:57:56,665 - Epoch: [49][  110/  246]    Overall Loss 0.302772    Objective Loss 0.302772                                        LR 0.000036    Time 0.023636    
2023-01-06 16:57:56,855 - Epoch: [49][  120/  246]    Overall Loss 0.304775    Objective Loss 0.304775                                        LR 0.000036    Time 0.023250    
2023-01-06 16:57:57,048 - Epoch: [49][  130/  246]    Overall Loss 0.304512    Objective Loss 0.304512                                        LR 0.000036    Time 0.022938    
2023-01-06 16:57:57,241 - Epoch: [49][  140/  246]    Overall Loss 0.305851    Objective Loss 0.305851                                        LR 0.000036    Time 0.022681    
2023-01-06 16:57:57,434 - Epoch: [49][  150/  246]    Overall Loss 0.306822    Objective Loss 0.306822                                        LR 0.000036    Time 0.022453    
2023-01-06 16:57:57,626 - Epoch: [49][  160/  246]    Overall Loss 0.308063    Objective Loss 0.308063                                        LR 0.000036    Time 0.022242    
2023-01-06 16:57:57,816 - Epoch: [49][  170/  246]    Overall Loss 0.308182    Objective Loss 0.308182                                        LR 0.000036    Time 0.022054    
2023-01-06 16:57:58,007 - Epoch: [49][  180/  246]    Overall Loss 0.307583    Objective Loss 0.307583                                        LR 0.000036    Time 0.021887    
2023-01-06 16:57:58,199 - Epoch: [49][  190/  246]    Overall Loss 0.306962    Objective Loss 0.306962                                        LR 0.000036    Time 0.021742    
2023-01-06 16:57:58,389 - Epoch: [49][  200/  246]    Overall Loss 0.307046    Objective Loss 0.307046                                        LR 0.000036    Time 0.021606    
2023-01-06 16:57:58,580 - Epoch: [49][  210/  246]    Overall Loss 0.307501    Objective Loss 0.307501                                        LR 0.000036    Time 0.021483    
2023-01-06 16:57:58,771 - Epoch: [49][  220/  246]    Overall Loss 0.307714    Objective Loss 0.307714                                        LR 0.000036    Time 0.021372    
2023-01-06 16:57:58,963 - Epoch: [49][  230/  246]    Overall Loss 0.307209    Objective Loss 0.307209                                        LR 0.000036    Time 0.021276    
2023-01-06 16:57:59,167 - Epoch: [49][  240/  246]    Overall Loss 0.307737    Objective Loss 0.307737                                        LR 0.000036    Time 0.021239    
2023-01-06 16:57:59,264 - Epoch: [49][  246/  246]    Overall Loss 0.307445    Objective Loss 0.307445    Top1 89.952153    LR 0.000036    Time 0.021113    
2023-01-06 16:57:59,398 - --- validate (epoch=49)-----------
2023-01-06 16:57:59,399 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:00,147 - Epoch: [49][   10/   28]    Loss 0.307979    Top1 88.750000    
2023-01-06 16:58:00,259 - Epoch: [49][   20/   28]    Loss 0.310834    Top1 88.769531    
2023-01-06 16:58:00,327 - Epoch: [49][   28/   28]    Loss 0.309067    Top1 88.677355    
2023-01-06 16:58:00,477 - ==> Top1: 88.677    Loss: 0.309

2023-01-06 16:58:00,477 - ==> Confusion:
[[ 170    7  262]
 [  12  186  404]
 [  44   62 5839]]

2023-01-06 16:58:00,478 - ==> Best [Top1: 89.064   Sparsity:0.00   Params: 155168 on epoch: 45]
2023-01-06 16:58:00,479 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:00,484 - 

2023-01-06 16:58:00,485 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:01,068 - Epoch: [50][   10/  246]    Overall Loss 0.295613    Objective Loss 0.295613                                        LR 0.000036    Time 0.058260    
2023-01-06 16:58:01,267 - Epoch: [50][   20/  246]    Overall Loss 0.303248    Objective Loss 0.303248                                        LR 0.000036    Time 0.039061    
2023-01-06 16:58:01,473 - Epoch: [50][   30/  246]    Overall Loss 0.303783    Objective Loss 0.303783                                        LR 0.000036    Time 0.032884    
2023-01-06 16:58:01,671 - Epoch: [50][   40/  246]    Overall Loss 0.307171    Objective Loss 0.307171                                        LR 0.000036    Time 0.029581    
2023-01-06 16:58:01,878 - Epoch: [50][   50/  246]    Overall Loss 0.304849    Objective Loss 0.304849                                        LR 0.000036    Time 0.027780    
2023-01-06 16:58:02,076 - Epoch: [50][   60/  246]    Overall Loss 0.304126    Objective Loss 0.304126                                        LR 0.000036    Time 0.026451    
2023-01-06 16:58:02,269 - Epoch: [50][   70/  246]    Overall Loss 0.302054    Objective Loss 0.302054                                        LR 0.000036    Time 0.025416    
2023-01-06 16:58:02,457 - Epoch: [50][   80/  246]    Overall Loss 0.302674    Objective Loss 0.302674                                        LR 0.000036    Time 0.024595    
2023-01-06 16:58:02,655 - Epoch: [50][   90/  246]    Overall Loss 0.299319    Objective Loss 0.299319                                        LR 0.000036    Time 0.024053    
2023-01-06 16:58:02,847 - Epoch: [50][  100/  246]    Overall Loss 0.300945    Objective Loss 0.300945                                        LR 0.000036    Time 0.023569    
2023-01-06 16:58:03,042 - Epoch: [50][  110/  246]    Overall Loss 0.301400    Objective Loss 0.301400                                        LR 0.000036    Time 0.023195    
2023-01-06 16:58:03,236 - Epoch: [50][  120/  246]    Overall Loss 0.300353    Objective Loss 0.300353                                        LR 0.000036    Time 0.022872    
2023-01-06 16:58:03,428 - Epoch: [50][  130/  246]    Overall Loss 0.299546    Objective Loss 0.299546                                        LR 0.000036    Time 0.022586    
2023-01-06 16:58:03,615 - Epoch: [50][  140/  246]    Overall Loss 0.299610    Objective Loss 0.299610                                        LR 0.000036    Time 0.022304    
2023-01-06 16:58:03,801 - Epoch: [50][  150/  246]    Overall Loss 0.300007    Objective Loss 0.300007                                        LR 0.000036    Time 0.022060    
2023-01-06 16:58:03,987 - Epoch: [50][  160/  246]    Overall Loss 0.300307    Objective Loss 0.300307                                        LR 0.000036    Time 0.021841    
2023-01-06 16:58:04,168 - Epoch: [50][  170/  246]    Overall Loss 0.301660    Objective Loss 0.301660                                        LR 0.000036    Time 0.021616    
2023-01-06 16:58:04,354 - Epoch: [50][  180/  246]    Overall Loss 0.301608    Objective Loss 0.301608                                        LR 0.000036    Time 0.021447    
2023-01-06 16:58:04,542 - Epoch: [50][  190/  246]    Overall Loss 0.302253    Objective Loss 0.302253                                        LR 0.000036    Time 0.021306    
2023-01-06 16:58:04,738 - Epoch: [50][  200/  246]    Overall Loss 0.302942    Objective Loss 0.302942                                        LR 0.000036    Time 0.021219    
2023-01-06 16:58:04,932 - Epoch: [50][  210/  246]    Overall Loss 0.302412    Objective Loss 0.302412                                        LR 0.000036    Time 0.021132    
2023-01-06 16:58:05,127 - Epoch: [50][  220/  246]    Overall Loss 0.303085    Objective Loss 0.303085                                        LR 0.000036    Time 0.021057    
2023-01-06 16:58:05,323 - Epoch: [50][  230/  246]    Overall Loss 0.303714    Objective Loss 0.303714                                        LR 0.000036    Time 0.020990    
2023-01-06 16:58:05,530 - Epoch: [50][  240/  246]    Overall Loss 0.302980    Objective Loss 0.302980                                        LR 0.000036    Time 0.020977    
2023-01-06 16:58:05,626 - Epoch: [50][  246/  246]    Overall Loss 0.302731    Objective Loss 0.302731    Top1 90.430622    LR 0.000036    Time 0.020854    
2023-01-06 16:58:05,763 - --- validate (epoch=50)-----------
2023-01-06 16:58:05,763 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:06,217 - Epoch: [50][   10/   28]    Loss 0.307299    Top1 89.218750    
2023-01-06 16:58:06,353 - Epoch: [50][   20/   28]    Loss 0.303163    Top1 89.218750    
2023-01-06 16:58:06,420 - Epoch: [50][   28/   28]    Loss 0.302698    Top1 89.192671    
2023-01-06 16:58:06,572 - ==> Top1: 89.193    Loss: 0.303

2023-01-06 16:58:06,572 - ==> Confusion:
[[ 195    8  236]
 [  13  211  378]
 [  58   62 5825]]

2023-01-06 16:58:06,574 - ==> Best [Top1: 89.193   Sparsity:0.00   Params: 155168 on epoch: 50]
2023-01-06 16:58:06,574 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:06,581 - 

2023-01-06 16:58:06,581 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:07,276 - Epoch: [51][   10/  246]    Overall Loss 0.291816    Objective Loss 0.291816                                        LR 0.000036    Time 0.069381    
2023-01-06 16:58:07,464 - Epoch: [51][   20/  246]    Overall Loss 0.293504    Objective Loss 0.293504                                        LR 0.000036    Time 0.044067    
2023-01-06 16:58:07,657 - Epoch: [51][   30/  246]    Overall Loss 0.296679    Objective Loss 0.296679                                        LR 0.000036    Time 0.035817    
2023-01-06 16:58:07,853 - Epoch: [51][   40/  246]    Overall Loss 0.301787    Objective Loss 0.301787                                        LR 0.000036    Time 0.031742    
2023-01-06 16:58:08,046 - Epoch: [51][   50/  246]    Overall Loss 0.303537    Objective Loss 0.303537                                        LR 0.000036    Time 0.029261    
2023-01-06 16:58:08,244 - Epoch: [51][   60/  246]    Overall Loss 0.305928    Objective Loss 0.305928                                        LR 0.000036    Time 0.027663    
2023-01-06 16:58:08,454 - Epoch: [51][   70/  246]    Overall Loss 0.305647    Objective Loss 0.305647                                        LR 0.000036    Time 0.026707    
2023-01-06 16:58:08,691 - Epoch: [51][   80/  246]    Overall Loss 0.305170    Objective Loss 0.305170                                        LR 0.000036    Time 0.026329    
2023-01-06 16:58:08,922 - Epoch: [51][   90/  246]    Overall Loss 0.305395    Objective Loss 0.305395                                        LR 0.000036    Time 0.025961    
2023-01-06 16:58:09,157 - Epoch: [51][  100/  246]    Overall Loss 0.307069    Objective Loss 0.307069                                        LR 0.000036    Time 0.025713    
2023-01-06 16:58:09,391 - Epoch: [51][  110/  246]    Overall Loss 0.305546    Objective Loss 0.305546                                        LR 0.000036    Time 0.025485    
2023-01-06 16:58:09,631 - Epoch: [51][  120/  246]    Overall Loss 0.306065    Objective Loss 0.306065                                        LR 0.000036    Time 0.025357    
2023-01-06 16:58:09,858 - Epoch: [51][  130/  246]    Overall Loss 0.305614    Objective Loss 0.305614                                        LR 0.000036    Time 0.025148    
2023-01-06 16:58:10,087 - Epoch: [51][  140/  246]    Overall Loss 0.305323    Objective Loss 0.305323                                        LR 0.000036    Time 0.024986    
2023-01-06 16:58:10,327 - Epoch: [51][  150/  246]    Overall Loss 0.305781    Objective Loss 0.305781                                        LR 0.000036    Time 0.024915    
2023-01-06 16:58:10,573 - Epoch: [51][  160/  246]    Overall Loss 0.305651    Objective Loss 0.305651                                        LR 0.000036    Time 0.024891    
2023-01-06 16:58:10,812 - Epoch: [51][  170/  246]    Overall Loss 0.304658    Objective Loss 0.304658                                        LR 0.000036    Time 0.024834    
2023-01-06 16:58:11,041 - Epoch: [51][  180/  246]    Overall Loss 0.305204    Objective Loss 0.305204                                        LR 0.000036    Time 0.024724    
2023-01-06 16:58:11,271 - Epoch: [51][  190/  246]    Overall Loss 0.305118    Objective Loss 0.305118                                        LR 0.000036    Time 0.024628    
2023-01-06 16:58:11,493 - Epoch: [51][  200/  246]    Overall Loss 0.304958    Objective Loss 0.304958                                        LR 0.000036    Time 0.024507    
2023-01-06 16:58:11,718 - Epoch: [51][  210/  246]    Overall Loss 0.304977    Objective Loss 0.304977                                        LR 0.000036    Time 0.024408    
2023-01-06 16:58:11,941 - Epoch: [51][  220/  246]    Overall Loss 0.303780    Objective Loss 0.303780                                        LR 0.000036    Time 0.024310    
2023-01-06 16:58:12,165 - Epoch: [51][  230/  246]    Overall Loss 0.304105    Objective Loss 0.304105                                        LR 0.000036    Time 0.024226    
2023-01-06 16:58:12,404 - Epoch: [51][  240/  246]    Overall Loss 0.304661    Objective Loss 0.304661                                        LR 0.000036    Time 0.024211    
2023-01-06 16:58:12,515 - Epoch: [51][  246/  246]    Overall Loss 0.304076    Objective Loss 0.304076    Top1 90.669856    LR 0.000036    Time 0.024069    
2023-01-06 16:58:12,679 - --- validate (epoch=51)-----------
2023-01-06 16:58:12,679 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:13,121 - Epoch: [51][   10/   28]    Loss 0.298237    Top1 88.906250    
2023-01-06 16:58:13,232 - Epoch: [51][   20/   28]    Loss 0.306131    Top1 88.964844    
2023-01-06 16:58:13,299 - Epoch: [51][   28/   28]    Loss 0.309642    Top1 88.806184    
2023-01-06 16:58:13,456 - ==> Top1: 88.806    Loss: 0.310

2023-01-06 16:58:13,456 - ==> Confusion:
[[ 193    9  237]
 [  13  171  418]
 [  61   44 5840]]

2023-01-06 16:58:13,457 - ==> Best [Top1: 89.193   Sparsity:0.00   Params: 155168 on epoch: 50]
2023-01-06 16:58:13,457 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:13,463 - 

2023-01-06 16:58:13,463 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:14,029 - Epoch: [52][   10/  246]    Overall Loss 0.295793    Objective Loss 0.295793                                        LR 0.000036    Time 0.056538    
2023-01-06 16:58:14,208 - Epoch: [52][   20/  246]    Overall Loss 0.301855    Objective Loss 0.301855                                        LR 0.000036    Time 0.037203    
2023-01-06 16:58:14,402 - Epoch: [52][   30/  246]    Overall Loss 0.298031    Objective Loss 0.298031                                        LR 0.000036    Time 0.031230    
2023-01-06 16:58:14,585 - Epoch: [52][   40/  246]    Overall Loss 0.300659    Objective Loss 0.300659                                        LR 0.000036    Time 0.027997    
2023-01-06 16:58:14,777 - Epoch: [52][   50/  246]    Overall Loss 0.299654    Objective Loss 0.299654                                        LR 0.000036    Time 0.026237    
2023-01-06 16:58:14,960 - Epoch: [52][   60/  246]    Overall Loss 0.301295    Objective Loss 0.301295                                        LR 0.000036    Time 0.024905    
2023-01-06 16:58:15,159 - Epoch: [52][   70/  246]    Overall Loss 0.303070    Objective Loss 0.303070                                        LR 0.000036    Time 0.024185    
2023-01-06 16:58:15,350 - Epoch: [52][   80/  246]    Overall Loss 0.303556    Objective Loss 0.303556                                        LR 0.000036    Time 0.023549    
2023-01-06 16:58:15,550 - Epoch: [52][   90/  246]    Overall Loss 0.302211    Objective Loss 0.302211                                        LR 0.000036    Time 0.023147    
2023-01-06 16:58:15,741 - Epoch: [52][  100/  246]    Overall Loss 0.302624    Objective Loss 0.302624                                        LR 0.000036    Time 0.022739    
2023-01-06 16:58:15,942 - Epoch: [52][  110/  246]    Overall Loss 0.303690    Objective Loss 0.303690                                        LR 0.000036    Time 0.022495    
2023-01-06 16:58:16,132 - Epoch: [52][  120/  246]    Overall Loss 0.302973    Objective Loss 0.302973                                        LR 0.000036    Time 0.022205    
2023-01-06 16:58:16,332 - Epoch: [52][  130/  246]    Overall Loss 0.302999    Objective Loss 0.302999                                        LR 0.000036    Time 0.022028    
2023-01-06 16:58:16,523 - Epoch: [52][  140/  246]    Overall Loss 0.303886    Objective Loss 0.303886                                        LR 0.000036    Time 0.021819    
2023-01-06 16:58:16,717 - Epoch: [52][  150/  246]    Overall Loss 0.303162    Objective Loss 0.303162                                        LR 0.000036    Time 0.021656    
2023-01-06 16:58:16,901 - Epoch: [52][  160/  246]    Overall Loss 0.304421    Objective Loss 0.304421                                        LR 0.000036    Time 0.021446    
2023-01-06 16:58:17,091 - Epoch: [52][  170/  246]    Overall Loss 0.303761    Objective Loss 0.303761                                        LR 0.000036    Time 0.021304    
2023-01-06 16:58:17,279 - Epoch: [52][  180/  246]    Overall Loss 0.304418    Objective Loss 0.304418                                        LR 0.000036    Time 0.021161    
2023-01-06 16:58:17,466 - Epoch: [52][  190/  246]    Overall Loss 0.305638    Objective Loss 0.305638                                        LR 0.000036    Time 0.021028    
2023-01-06 16:58:17,649 - Epoch: [52][  200/  246]    Overall Loss 0.305221    Objective Loss 0.305221                                        LR 0.000036    Time 0.020891    
2023-01-06 16:58:17,835 - Epoch: [52][  210/  246]    Overall Loss 0.304494    Objective Loss 0.304494                                        LR 0.000036    Time 0.020783    
2023-01-06 16:58:18,024 - Epoch: [52][  220/  246]    Overall Loss 0.304302    Objective Loss 0.304302                                        LR 0.000036    Time 0.020696    
2023-01-06 16:58:18,199 - Epoch: [52][  230/  246]    Overall Loss 0.304010    Objective Loss 0.304010                                        LR 0.000036    Time 0.020552    
2023-01-06 16:58:18,373 - Epoch: [52][  240/  246]    Overall Loss 0.302960    Objective Loss 0.302960                                        LR 0.000036    Time 0.020421    
2023-01-06 16:58:18,457 - Epoch: [52][  246/  246]    Overall Loss 0.302580    Objective Loss 0.302580    Top1 89.712919    LR 0.000036    Time 0.020263    
2023-01-06 16:58:18,629 - --- validate (epoch=52)-----------
2023-01-06 16:58:18,629 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:19,078 - Epoch: [52][   10/   28]    Loss 0.304103    Top1 88.984375    
2023-01-06 16:58:19,189 - Epoch: [52][   20/   28]    Loss 0.303146    Top1 89.121094    
2023-01-06 16:58:19,258 - Epoch: [52][   28/   28]    Loss 0.299866    Top1 89.049528    
2023-01-06 16:58:19,422 - ==> Top1: 89.050    Loss: 0.300

2023-01-06 16:58:19,422 - ==> Confusion:
[[ 192   15  232]
 [  12  217  373]
 [  51   82 5812]]

2023-01-06 16:58:19,423 - ==> Best [Top1: 89.193   Sparsity:0.00   Params: 155168 on epoch: 50]
2023-01-06 16:58:19,424 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:19,430 - 

2023-01-06 16:58:19,430 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:20,120 - Epoch: [53][   10/  246]    Overall Loss 0.300598    Objective Loss 0.300598                                        LR 0.000036    Time 0.068943    
2023-01-06 16:58:20,299 - Epoch: [53][   20/  246]    Overall Loss 0.313249    Objective Loss 0.313249                                        LR 0.000036    Time 0.043385    
2023-01-06 16:58:20,504 - Epoch: [53][   30/  246]    Overall Loss 0.303519    Objective Loss 0.303519                                        LR 0.000036    Time 0.035734    
2023-01-06 16:58:20,693 - Epoch: [53][   40/  246]    Overall Loss 0.307798    Objective Loss 0.307798                                        LR 0.000036    Time 0.031527    
2023-01-06 16:58:20,889 - Epoch: [53][   50/  246]    Overall Loss 0.306917    Objective Loss 0.306917                                        LR 0.000036    Time 0.029128    
2023-01-06 16:58:21,082 - Epoch: [53][   60/  246]    Overall Loss 0.304828    Objective Loss 0.304828                                        LR 0.000036    Time 0.027487    
2023-01-06 16:58:21,283 - Epoch: [53][   70/  246]    Overall Loss 0.305001    Objective Loss 0.305001                                        LR 0.000036    Time 0.026418    
2023-01-06 16:58:21,478 - Epoch: [53][   80/  246]    Overall Loss 0.303706    Objective Loss 0.303706                                        LR 0.000036    Time 0.025559    
2023-01-06 16:58:21,682 - Epoch: [53][   90/  246]    Overall Loss 0.302288    Objective Loss 0.302288                                        LR 0.000036    Time 0.024977    
2023-01-06 16:58:21,863 - Epoch: [53][  100/  246]    Overall Loss 0.300425    Objective Loss 0.300425                                        LR 0.000036    Time 0.024288    
2023-01-06 16:58:22,041 - Epoch: [53][  110/  246]    Overall Loss 0.301144    Objective Loss 0.301144                                        LR 0.000036    Time 0.023693    
2023-01-06 16:58:22,231 - Epoch: [53][  120/  246]    Overall Loss 0.300283    Objective Loss 0.300283                                        LR 0.000036    Time 0.023302    
2023-01-06 16:58:22,416 - Epoch: [53][  130/  246]    Overall Loss 0.299445    Objective Loss 0.299445                                        LR 0.000036    Time 0.022926    
2023-01-06 16:58:22,614 - Epoch: [53][  140/  246]    Overall Loss 0.298802    Objective Loss 0.298802                                        LR 0.000036    Time 0.022702    
2023-01-06 16:58:22,808 - Epoch: [53][  150/  246]    Overall Loss 0.299504    Objective Loss 0.299504                                        LR 0.000036    Time 0.022481    
2023-01-06 16:58:23,005 - Epoch: [53][  160/  246]    Overall Loss 0.300044    Objective Loss 0.300044                                        LR 0.000036    Time 0.022304    
2023-01-06 16:58:23,199 - Epoch: [53][  170/  246]    Overall Loss 0.302072    Objective Loss 0.302072                                        LR 0.000036    Time 0.022129    
2023-01-06 16:58:23,396 - Epoch: [53][  180/  246]    Overall Loss 0.301762    Objective Loss 0.301762                                        LR 0.000036    Time 0.021998    
2023-01-06 16:58:23,590 - Epoch: [53][  190/  246]    Overall Loss 0.302002    Objective Loss 0.302002                                        LR 0.000036    Time 0.021858    
2023-01-06 16:58:23,788 - Epoch: [53][  200/  246]    Overall Loss 0.301385    Objective Loss 0.301385                                        LR 0.000036    Time 0.021753    
2023-01-06 16:58:23,982 - Epoch: [53][  210/  246]    Overall Loss 0.301082    Objective Loss 0.301082                                        LR 0.000036    Time 0.021640    
2023-01-06 16:58:24,188 - Epoch: [53][  220/  246]    Overall Loss 0.301447    Objective Loss 0.301447                                        LR 0.000036    Time 0.021591    
2023-01-06 16:58:24,398 - Epoch: [53][  230/  246]    Overall Loss 0.301483    Objective Loss 0.301483                                        LR 0.000036    Time 0.021564    
2023-01-06 16:58:24,618 - Epoch: [53][  240/  246]    Overall Loss 0.301059    Objective Loss 0.301059                                        LR 0.000036    Time 0.021578    
2023-01-06 16:58:24,714 - Epoch: [53][  246/  246]    Overall Loss 0.300633    Objective Loss 0.300633    Top1 88.755981    LR 0.000036    Time 0.021443    
2023-01-06 16:58:24,857 - --- validate (epoch=53)-----------
2023-01-06 16:58:24,857 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:25,297 - Epoch: [53][   10/   28]    Loss 0.310994    Top1 89.296875    
2023-01-06 16:58:25,410 - Epoch: [53][   20/   28]    Loss 0.308299    Top1 88.789062    
2023-01-06 16:58:25,479 - Epoch: [53][   28/   28]    Loss 0.306102    Top1 88.748926    
2023-01-06 16:58:25,618 - ==> Top1: 88.749    Loss: 0.306

2023-01-06 16:58:25,618 - ==> Confusion:
[[ 192   13  234]
 [  14  211  377]
 [  56   92 5797]]

2023-01-06 16:58:25,619 - ==> Best [Top1: 89.193   Sparsity:0.00   Params: 155168 on epoch: 50]
2023-01-06 16:58:25,619 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:25,625 - 

2023-01-06 16:58:25,625 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:26,336 - Epoch: [54][   10/  246]    Overall Loss 0.306589    Objective Loss 0.306589                                        LR 0.000036    Time 0.070963    
2023-01-06 16:58:26,528 - Epoch: [54][   20/  246]    Overall Loss 0.304633    Objective Loss 0.304633                                        LR 0.000036    Time 0.045092    
2023-01-06 16:58:26,738 - Epoch: [54][   30/  246]    Overall Loss 0.306325    Objective Loss 0.306325                                        LR 0.000036    Time 0.037037    
2023-01-06 16:58:26,948 - Epoch: [54][   40/  246]    Overall Loss 0.312165    Objective Loss 0.312165                                        LR 0.000036    Time 0.033013    
2023-01-06 16:58:27,160 - Epoch: [54][   50/  246]    Overall Loss 0.311077    Objective Loss 0.311077                                        LR 0.000036    Time 0.030656    
2023-01-06 16:58:27,368 - Epoch: [54][   60/  246]    Overall Loss 0.309197    Objective Loss 0.309197                                        LR 0.000036    Time 0.029006    
2023-01-06 16:58:27,584 - Epoch: [54][   70/  246]    Overall Loss 0.307687    Objective Loss 0.307687                                        LR 0.000036    Time 0.027936    
2023-01-06 16:58:27,797 - Epoch: [54][   80/  246]    Overall Loss 0.308089    Objective Loss 0.308089                                        LR 0.000036    Time 0.027110    
2023-01-06 16:58:28,012 - Epoch: [54][   90/  246]    Overall Loss 0.307228    Objective Loss 0.307228                                        LR 0.000036    Time 0.026484    
2023-01-06 16:58:28,226 - Epoch: [54][  100/  246]    Overall Loss 0.305993    Objective Loss 0.305993                                        LR 0.000036    Time 0.025969    
2023-01-06 16:58:28,442 - Epoch: [54][  110/  246]    Overall Loss 0.304377    Objective Loss 0.304377                                        LR 0.000036    Time 0.025571    
2023-01-06 16:58:28,652 - Epoch: [54][  120/  246]    Overall Loss 0.304193    Objective Loss 0.304193                                        LR 0.000036    Time 0.025179    
2023-01-06 16:58:28,863 - Epoch: [54][  130/  246]    Overall Loss 0.303070    Objective Loss 0.303070                                        LR 0.000036    Time 0.024869    
2023-01-06 16:58:29,075 - Epoch: [54][  140/  246]    Overall Loss 0.303279    Objective Loss 0.303279                                        LR 0.000036    Time 0.024601    
2023-01-06 16:58:29,274 - Epoch: [54][  150/  246]    Overall Loss 0.304072    Objective Loss 0.304072                                        LR 0.000036    Time 0.024283    
2023-01-06 16:58:29,464 - Epoch: [54][  160/  246]    Overall Loss 0.304826    Objective Loss 0.304826                                        LR 0.000036    Time 0.023951    
2023-01-06 16:58:29,661 - Epoch: [54][  170/  246]    Overall Loss 0.304337    Objective Loss 0.304337                                        LR 0.000036    Time 0.023703    
2023-01-06 16:58:29,850 - Epoch: [54][  180/  246]    Overall Loss 0.303541    Objective Loss 0.303541                                        LR 0.000036    Time 0.023431    
2023-01-06 16:58:30,034 - Epoch: [54][  190/  246]    Overall Loss 0.303407    Objective Loss 0.303407                                        LR 0.000036    Time 0.023165    
2023-01-06 16:58:30,221 - Epoch: [54][  200/  246]    Overall Loss 0.303634    Objective Loss 0.303634                                        LR 0.000036    Time 0.022939    
2023-01-06 16:58:30,428 - Epoch: [54][  210/  246]    Overall Loss 0.303186    Objective Loss 0.303186                                        LR 0.000036    Time 0.022831    
2023-01-06 16:58:30,644 - Epoch: [54][  220/  246]    Overall Loss 0.302732    Objective Loss 0.302732                                        LR 0.000036    Time 0.022771    
2023-01-06 16:58:30,849 - Epoch: [54][  230/  246]    Overall Loss 0.302280    Objective Loss 0.302280                                        LR 0.000036    Time 0.022673    
2023-01-06 16:58:31,070 - Epoch: [54][  240/  246]    Overall Loss 0.302458    Objective Loss 0.302458                                        LR 0.000036    Time 0.022649    
2023-01-06 16:58:31,166 - Epoch: [54][  246/  246]    Overall Loss 0.301766    Objective Loss 0.301766    Top1 91.626794    LR 0.000036    Time 0.022486    
2023-01-06 16:58:31,310 - --- validate (epoch=54)-----------
2023-01-06 16:58:31,310 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:31,765 - Epoch: [54][   10/   28]    Loss 0.312229    Top1 89.140625    
2023-01-06 16:58:31,888 - Epoch: [54][   20/   28]    Loss 0.305320    Top1 88.945312    
2023-01-06 16:58:31,955 - Epoch: [54][   28/   28]    Loss 0.304343    Top1 89.235614    
2023-01-06 16:58:32,087 - ==> Top1: 89.236    Loss: 0.304

2023-01-06 16:58:32,087 - ==> Confusion:
[[ 191   10  238]
 [  11  200  391]
 [  53   49 5843]]

2023-01-06 16:58:32,088 - ==> Best [Top1: 89.236   Sparsity:0.00   Params: 155168 on epoch: 54]
2023-01-06 16:58:32,088 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:32,095 - 

2023-01-06 16:58:32,095 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:32,670 - Epoch: [55][   10/  246]    Overall Loss 0.278123    Objective Loss 0.278123                                        LR 0.000036    Time 0.057400    
2023-01-06 16:58:32,860 - Epoch: [55][   20/  246]    Overall Loss 0.288987    Objective Loss 0.288987                                        LR 0.000036    Time 0.038157    
2023-01-06 16:58:33,054 - Epoch: [55][   30/  246]    Overall Loss 0.289084    Objective Loss 0.289084                                        LR 0.000036    Time 0.031889    
2023-01-06 16:58:33,251 - Epoch: [55][   40/  246]    Overall Loss 0.294882    Objective Loss 0.294882                                        LR 0.000036    Time 0.028838    
2023-01-06 16:58:33,444 - Epoch: [55][   50/  246]    Overall Loss 0.291532    Objective Loss 0.291532                                        LR 0.000036    Time 0.026918    
2023-01-06 16:58:33,615 - Epoch: [55][   60/  246]    Overall Loss 0.292239    Objective Loss 0.292239                                        LR 0.000036    Time 0.025283    
2023-01-06 16:58:33,801 - Epoch: [55][   70/  246]    Overall Loss 0.291678    Objective Loss 0.291678                                        LR 0.000036    Time 0.024319    
2023-01-06 16:58:33,998 - Epoch: [55][   80/  246]    Overall Loss 0.294647    Objective Loss 0.294647                                        LR 0.000036    Time 0.023735    
2023-01-06 16:58:34,193 - Epoch: [55][   90/  246]    Overall Loss 0.296766    Objective Loss 0.296766                                        LR 0.000036    Time 0.023263    
2023-01-06 16:58:34,384 - Epoch: [55][  100/  246]    Overall Loss 0.297827    Objective Loss 0.297827                                        LR 0.000036    Time 0.022845    
2023-01-06 16:58:34,575 - Epoch: [55][  110/  246]    Overall Loss 0.299322    Objective Loss 0.299322                                        LR 0.000036    Time 0.022504    
2023-01-06 16:58:34,770 - Epoch: [55][  120/  246]    Overall Loss 0.300876    Objective Loss 0.300876                                        LR 0.000036    Time 0.022252    
2023-01-06 16:58:34,968 - Epoch: [55][  130/  246]    Overall Loss 0.299597    Objective Loss 0.299597                                        LR 0.000036    Time 0.022056    
2023-01-06 16:58:35,181 - Epoch: [55][  140/  246]    Overall Loss 0.300340    Objective Loss 0.300340                                        LR 0.000036    Time 0.022003    
2023-01-06 16:58:35,397 - Epoch: [55][  150/  246]    Overall Loss 0.299795    Objective Loss 0.299795                                        LR 0.000036    Time 0.021973    
2023-01-06 16:58:35,614 - Epoch: [55][  160/  246]    Overall Loss 0.299744    Objective Loss 0.299744                                        LR 0.000036    Time 0.021948    
2023-01-06 16:58:35,834 - Epoch: [55][  170/  246]    Overall Loss 0.299860    Objective Loss 0.299860                                        LR 0.000036    Time 0.021952    
2023-01-06 16:58:36,075 - Epoch: [55][  180/  246]    Overall Loss 0.300570    Objective Loss 0.300570                                        LR 0.000036    Time 0.022069    
2023-01-06 16:58:36,309 - Epoch: [55][  190/  246]    Overall Loss 0.299497    Objective Loss 0.299497                                        LR 0.000036    Time 0.022135    
2023-01-06 16:58:36,533 - Epoch: [55][  200/  246]    Overall Loss 0.298751    Objective Loss 0.298751                                        LR 0.000036    Time 0.022145    
2023-01-06 16:58:36,758 - Epoch: [55][  210/  246]    Overall Loss 0.298596    Objective Loss 0.298596                                        LR 0.000036    Time 0.022161    
2023-01-06 16:58:36,985 - Epoch: [55][  220/  246]    Overall Loss 0.297665    Objective Loss 0.297665                                        LR 0.000036    Time 0.022181    
2023-01-06 16:58:37,217 - Epoch: [55][  230/  246]    Overall Loss 0.297539    Objective Loss 0.297539                                        LR 0.000036    Time 0.022222    
2023-01-06 16:58:37,456 - Epoch: [55][  240/  246]    Overall Loss 0.297649    Objective Loss 0.297649                                        LR 0.000036    Time 0.022290    
2023-01-06 16:58:37,541 - Epoch: [55][  246/  246]    Overall Loss 0.298608    Objective Loss 0.298608    Top1 86.602871    LR 0.000036    Time 0.022093    
2023-01-06 16:58:37,677 - --- validate (epoch=55)-----------
2023-01-06 16:58:37,677 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:38,128 - Epoch: [55][   10/   28]    Loss 0.299355    Top1 89.062500    
2023-01-06 16:58:38,243 - Epoch: [55][   20/   28]    Loss 0.299713    Top1 89.433594    
2023-01-06 16:58:38,313 - Epoch: [55][   28/   28]    Loss 0.305170    Top1 89.249928    
2023-01-06 16:58:38,440 - ==> Top1: 89.250    Loss: 0.305

2023-01-06 16:58:38,441 - ==> Confusion:
[[ 215   13  211]
 [  17  225  360]
 [  70   80 5795]]

2023-01-06 16:58:38,442 - ==> Best [Top1: 89.250   Sparsity:0.00   Params: 155168 on epoch: 55]
2023-01-06 16:58:38,442 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:38,449 - 

2023-01-06 16:58:38,449 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:39,150 - Epoch: [56][   10/  246]    Overall Loss 0.301634    Objective Loss 0.301634                                        LR 0.000036    Time 0.069952    
2023-01-06 16:58:39,317 - Epoch: [56][   20/  246]    Overall Loss 0.300549    Objective Loss 0.300549                                        LR 0.000036    Time 0.043324    
2023-01-06 16:58:39,487 - Epoch: [56][   30/  246]    Overall Loss 0.294593    Objective Loss 0.294593                                        LR 0.000036    Time 0.034527    
2023-01-06 16:58:39,658 - Epoch: [56][   40/  246]    Overall Loss 0.299530    Objective Loss 0.299530                                        LR 0.000036    Time 0.030168    
2023-01-06 16:58:39,831 - Epoch: [56][   50/  246]    Overall Loss 0.300210    Objective Loss 0.300210                                        LR 0.000036    Time 0.027579    
2023-01-06 16:58:40,004 - Epoch: [56][   60/  246]    Overall Loss 0.303692    Objective Loss 0.303692                                        LR 0.000036    Time 0.025871    
2023-01-06 16:58:40,197 - Epoch: [56][   70/  246]    Overall Loss 0.303620    Objective Loss 0.303620                                        LR 0.000036    Time 0.024925    
2023-01-06 16:58:40,396 - Epoch: [56][   80/  246]    Overall Loss 0.301792    Objective Loss 0.301792                                        LR 0.000036    Time 0.024283    
2023-01-06 16:58:40,599 - Epoch: [56][   90/  246]    Overall Loss 0.298807    Objective Loss 0.298807                                        LR 0.000036    Time 0.023840    
2023-01-06 16:58:40,806 - Epoch: [56][  100/  246]    Overall Loss 0.300115    Objective Loss 0.300115                                        LR 0.000036    Time 0.023518    
2023-01-06 16:58:40,999 - Epoch: [56][  110/  246]    Overall Loss 0.299618    Objective Loss 0.299618                                        LR 0.000036    Time 0.023137    
2023-01-06 16:58:41,178 - Epoch: [56][  120/  246]    Overall Loss 0.299763    Objective Loss 0.299763                                        LR 0.000036    Time 0.022696    
2023-01-06 16:58:41,384 - Epoch: [56][  130/  246]    Overall Loss 0.298617    Objective Loss 0.298617                                        LR 0.000036    Time 0.022535    
2023-01-06 16:58:41,615 - Epoch: [56][  140/  246]    Overall Loss 0.297445    Objective Loss 0.297445                                        LR 0.000036    Time 0.022566    
2023-01-06 16:58:41,848 - Epoch: [56][  150/  246]    Overall Loss 0.296369    Objective Loss 0.296369                                        LR 0.000036    Time 0.022613    
2023-01-06 16:58:42,085 - Epoch: [56][  160/  246]    Overall Loss 0.297513    Objective Loss 0.297513                                        LR 0.000036    Time 0.022680    
2023-01-06 16:58:42,339 - Epoch: [56][  170/  246]    Overall Loss 0.297435    Objective Loss 0.297435                                        LR 0.000036    Time 0.022838    
2023-01-06 16:58:42,587 - Epoch: [56][  180/  246]    Overall Loss 0.297660    Objective Loss 0.297660                                        LR 0.000036    Time 0.022941    
2023-01-06 16:58:42,821 - Epoch: [56][  190/  246]    Overall Loss 0.297317    Objective Loss 0.297317                                        LR 0.000036    Time 0.022965    
2023-01-06 16:58:43,055 - Epoch: [56][  200/  246]    Overall Loss 0.297077    Objective Loss 0.297077                                        LR 0.000036    Time 0.022981    
2023-01-06 16:58:43,302 - Epoch: [56][  210/  246]    Overall Loss 0.297413    Objective Loss 0.297413                                        LR 0.000036    Time 0.023060    
2023-01-06 16:58:43,543 - Epoch: [56][  220/  246]    Overall Loss 0.297234    Objective Loss 0.297234                                        LR 0.000036    Time 0.023100    
2023-01-06 16:58:43,788 - Epoch: [56][  230/  246]    Overall Loss 0.298181    Objective Loss 0.298181                                        LR 0.000036    Time 0.023158    
2023-01-06 16:58:44,041 - Epoch: [56][  240/  246]    Overall Loss 0.297628    Objective Loss 0.297628                                        LR 0.000036    Time 0.023247    
2023-01-06 16:58:44,157 - Epoch: [56][  246/  246]    Overall Loss 0.297312    Objective Loss 0.297312    Top1 91.626794    LR 0.000036    Time 0.023151    
2023-01-06 16:58:44,320 - --- validate (epoch=56)-----------
2023-01-06 16:58:44,320 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:44,779 - Epoch: [56][   10/   28]    Loss 0.282619    Top1 89.414062    
2023-01-06 16:58:44,893 - Epoch: [56][   20/   28]    Loss 0.290451    Top1 89.199219    
2023-01-06 16:58:44,965 - Epoch: [56][   28/   28]    Loss 0.301523    Top1 89.078156    
2023-01-06 16:58:45,105 - ==> Top1: 89.078    Loss: 0.302

2023-01-06 16:58:45,106 - ==> Confusion:
[[ 178    9  252]
 [  12  189  401]
 [  38   51 5856]]

2023-01-06 16:58:45,107 - ==> Best [Top1: 89.250   Sparsity:0.00   Params: 155168 on epoch: 55]
2023-01-06 16:58:45,107 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:45,113 - 

2023-01-06 16:58:45,113 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:45,853 - Epoch: [57][   10/  246]    Overall Loss 0.318153    Objective Loss 0.318153                                        LR 0.000036    Time 0.073953    
2023-01-06 16:58:46,056 - Epoch: [57][   20/  246]    Overall Loss 0.305684    Objective Loss 0.305684                                        LR 0.000036    Time 0.047103    
2023-01-06 16:58:46,252 - Epoch: [57][   30/  246]    Overall Loss 0.299573    Objective Loss 0.299573                                        LR 0.000036    Time 0.037923    
2023-01-06 16:58:46,453 - Epoch: [57][   40/  246]    Overall Loss 0.299229    Objective Loss 0.299229                                        LR 0.000036    Time 0.033452    
2023-01-06 16:58:46,649 - Epoch: [57][   50/  246]    Overall Loss 0.298587    Objective Loss 0.298587                                        LR 0.000036    Time 0.030670    
2023-01-06 16:58:46,853 - Epoch: [57][   60/  246]    Overall Loss 0.296881    Objective Loss 0.296881                                        LR 0.000036    Time 0.028962    
2023-01-06 16:58:47,036 - Epoch: [57][   70/  246]    Overall Loss 0.296105    Objective Loss 0.296105                                        LR 0.000036    Time 0.027425    
2023-01-06 16:58:47,220 - Epoch: [57][   80/  246]    Overall Loss 0.298098    Objective Loss 0.298098                                        LR 0.000036    Time 0.026299    
2023-01-06 16:58:47,404 - Epoch: [57][   90/  246]    Overall Loss 0.300584    Objective Loss 0.300584                                        LR 0.000036    Time 0.025411    
2023-01-06 16:58:47,590 - Epoch: [57][  100/  246]    Overall Loss 0.301119    Objective Loss 0.301119                                        LR 0.000036    Time 0.024723    
2023-01-06 16:58:47,768 - Epoch: [57][  110/  246]    Overall Loss 0.300004    Objective Loss 0.300004                                        LR 0.000036    Time 0.024090    
2023-01-06 16:58:47,956 - Epoch: [57][  120/  246]    Overall Loss 0.300164    Objective Loss 0.300164                                        LR 0.000036    Time 0.023650    
2023-01-06 16:58:48,134 - Epoch: [57][  130/  246]    Overall Loss 0.300443    Objective Loss 0.300443                                        LR 0.000036    Time 0.023198    
2023-01-06 16:58:48,323 - Epoch: [57][  140/  246]    Overall Loss 0.299897    Objective Loss 0.299897                                        LR 0.000036    Time 0.022887    
2023-01-06 16:58:48,500 - Epoch: [57][  150/  246]    Overall Loss 0.300330    Objective Loss 0.300330                                        LR 0.000036    Time 0.022537    
2023-01-06 16:58:48,687 - Epoch: [57][  160/  246]    Overall Loss 0.298981    Objective Loss 0.298981                                        LR 0.000036    Time 0.022295    
2023-01-06 16:58:48,865 - Epoch: [57][  170/  246]    Overall Loss 0.301054    Objective Loss 0.301054                                        LR 0.000036    Time 0.022030    
2023-01-06 16:58:49,052 - Epoch: [57][  180/  246]    Overall Loss 0.302739    Objective Loss 0.302739                                        LR 0.000036    Time 0.021842    
2023-01-06 16:58:49,228 - Epoch: [57][  190/  246]    Overall Loss 0.302287    Objective Loss 0.302287                                        LR 0.000036    Time 0.021616    
2023-01-06 16:58:49,416 - Epoch: [57][  200/  246]    Overall Loss 0.301588    Objective Loss 0.301588                                        LR 0.000036    Time 0.021473    
2023-01-06 16:58:49,594 - Epoch: [57][  210/  246]    Overall Loss 0.301629    Objective Loss 0.301629                                        LR 0.000036    Time 0.021299    
2023-01-06 16:58:49,781 - Epoch: [57][  220/  246]    Overall Loss 0.300852    Objective Loss 0.300852                                        LR 0.000036    Time 0.021180    
2023-01-06 16:58:49,966 - Epoch: [57][  230/  246]    Overall Loss 0.299915    Objective Loss 0.299915                                        LR 0.000036    Time 0.021059    
2023-01-06 16:58:50,166 - Epoch: [57][  240/  246]    Overall Loss 0.299035    Objective Loss 0.299035                                        LR 0.000036    Time 0.021014    
2023-01-06 16:58:50,260 - Epoch: [57][  246/  246]    Overall Loss 0.298515    Objective Loss 0.298515    Top1 89.473684    LR 0.000036    Time 0.020884    
2023-01-06 16:58:50,439 - --- validate (epoch=57)-----------
2023-01-06 16:58:50,439 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:50,889 - Epoch: [57][   10/   28]    Loss 0.322872    Top1 88.242188    
2023-01-06 16:58:51,003 - Epoch: [57][   20/   28]    Loss 0.307543    Top1 89.003906    
2023-01-06 16:58:51,072 - Epoch: [57][   28/   28]    Loss 0.300656    Top1 89.450329    
2023-01-06 16:58:51,227 - ==> Top1: 89.450    Loss: 0.301

2023-01-06 16:58:51,228 - ==> Confusion:
[[ 222    8  209]
 [  17  217  368]
 [  71   64 5810]]

2023-01-06 16:58:51,229 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:58:51,229 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:51,236 - 

2023-01-06 16:58:51,236 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:51,790 - Epoch: [58][   10/  246]    Overall Loss 0.301866    Objective Loss 0.301866                                        LR 0.000036    Time 0.055322    
2023-01-06 16:58:51,956 - Epoch: [58][   20/  246]    Overall Loss 0.302604    Objective Loss 0.302604                                        LR 0.000036    Time 0.035928    
2023-01-06 16:58:52,146 - Epoch: [58][   30/  246]    Overall Loss 0.297329    Objective Loss 0.297329                                        LR 0.000036    Time 0.030289    
2023-01-06 16:58:52,337 - Epoch: [58][   40/  246]    Overall Loss 0.301760    Objective Loss 0.301760                                        LR 0.000036    Time 0.027464    
2023-01-06 16:58:52,530 - Epoch: [58][   50/  246]    Overall Loss 0.296640    Objective Loss 0.296640                                        LR 0.000036    Time 0.025828    
2023-01-06 16:58:52,726 - Epoch: [58][   60/  246]    Overall Loss 0.298436    Objective Loss 0.298436                                        LR 0.000036    Time 0.024794    
2023-01-06 16:58:52,948 - Epoch: [58][   70/  246]    Overall Loss 0.297354    Objective Loss 0.297354                                        LR 0.000036    Time 0.024412    
2023-01-06 16:58:53,170 - Epoch: [58][   80/  246]    Overall Loss 0.300290    Objective Loss 0.300290                                        LR 0.000036    Time 0.024126    
2023-01-06 16:58:53,393 - Epoch: [58][   90/  246]    Overall Loss 0.302826    Objective Loss 0.302826                                        LR 0.000036    Time 0.023925    
2023-01-06 16:58:53,614 - Epoch: [58][  100/  246]    Overall Loss 0.301918    Objective Loss 0.301918                                        LR 0.000036    Time 0.023732    
2023-01-06 16:58:53,835 - Epoch: [58][  110/  246]    Overall Loss 0.299698    Objective Loss 0.299698                                        LR 0.000036    Time 0.023583    
2023-01-06 16:58:54,058 - Epoch: [58][  120/  246]    Overall Loss 0.296477    Objective Loss 0.296477                                        LR 0.000036    Time 0.023472    
2023-01-06 16:58:54,280 - Epoch: [58][  130/  246]    Overall Loss 0.296208    Objective Loss 0.296208                                        LR 0.000036    Time 0.023374    
2023-01-06 16:58:54,503 - Epoch: [58][  140/  246]    Overall Loss 0.295051    Objective Loss 0.295051                                        LR 0.000036    Time 0.023290    
2023-01-06 16:58:54,726 - Epoch: [58][  150/  246]    Overall Loss 0.293836    Objective Loss 0.293836                                        LR 0.000036    Time 0.023220    
2023-01-06 16:58:54,949 - Epoch: [58][  160/  246]    Overall Loss 0.293052    Objective Loss 0.293052                                        LR 0.000036    Time 0.023162    
2023-01-06 16:58:55,173 - Epoch: [58][  170/  246]    Overall Loss 0.292556    Objective Loss 0.292556                                        LR 0.000036    Time 0.023114    
2023-01-06 16:58:55,398 - Epoch: [58][  180/  246]    Overall Loss 0.293439    Objective Loss 0.293439                                        LR 0.000036    Time 0.023075    
2023-01-06 16:58:55,621 - Epoch: [58][  190/  246]    Overall Loss 0.294521    Objective Loss 0.294521                                        LR 0.000036    Time 0.023032    
2023-01-06 16:58:55,842 - Epoch: [58][  200/  246]    Overall Loss 0.295477    Objective Loss 0.295477                                        LR 0.000036    Time 0.022984    
2023-01-06 16:58:56,065 - Epoch: [58][  210/  246]    Overall Loss 0.296386    Objective Loss 0.296386                                        LR 0.000036    Time 0.022950    
2023-01-06 16:58:56,287 - Epoch: [58][  220/  246]    Overall Loss 0.296395    Objective Loss 0.296395                                        LR 0.000036    Time 0.022915    
2023-01-06 16:58:56,511 - Epoch: [58][  230/  246]    Overall Loss 0.297233    Objective Loss 0.297233                                        LR 0.000036    Time 0.022890    
2023-01-06 16:58:56,742 - Epoch: [58][  240/  246]    Overall Loss 0.296842    Objective Loss 0.296842                                        LR 0.000036    Time 0.022899    
2023-01-06 16:58:56,839 - Epoch: [58][  246/  246]    Overall Loss 0.296684    Objective Loss 0.296684    Top1 89.473684    LR 0.000036    Time 0.022734    
2023-01-06 16:58:56,987 - --- validate (epoch=58)-----------
2023-01-06 16:58:56,988 - 6986 samples (256 per mini-batch)
2023-01-06 16:58:57,433 - Epoch: [58][   10/   28]    Loss 0.301009    Top1 89.375000    
2023-01-06 16:58:57,545 - Epoch: [58][   20/   28]    Loss 0.300168    Top1 89.003906    
2023-01-06 16:58:57,614 - Epoch: [58][   28/   28]    Loss 0.300086    Top1 89.192671    
2023-01-06 16:58:57,762 - ==> Top1: 89.193    Loss: 0.300

2023-01-06 16:58:57,763 - ==> Confusion:
[[ 198    8  233]
 [  13  197  392]
 [  58   51 5836]]

2023-01-06 16:58:57,764 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:58:57,764 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:58:57,770 - 

2023-01-06 16:58:57,770 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:58:58,474 - Epoch: [59][   10/  246]    Overall Loss 0.284750    Objective Loss 0.284750                                        LR 0.000036    Time 0.070337    
2023-01-06 16:58:58,655 - Epoch: [59][   20/  246]    Overall Loss 0.294105    Objective Loss 0.294105                                        LR 0.000036    Time 0.044179    
2023-01-06 16:58:58,854 - Epoch: [59][   30/  246]    Overall Loss 0.293748    Objective Loss 0.293748                                        LR 0.000036    Time 0.036095    
2023-01-06 16:58:59,048 - Epoch: [59][   40/  246]    Overall Loss 0.293883    Objective Loss 0.293883                                        LR 0.000036    Time 0.031915    
2023-01-06 16:58:59,228 - Epoch: [59][   50/  246]    Overall Loss 0.290602    Objective Loss 0.290602                                        LR 0.000036    Time 0.029119    
2023-01-06 16:58:59,404 - Epoch: [59][   60/  246]    Overall Loss 0.293008    Objective Loss 0.293008                                        LR 0.000036    Time 0.027183    
2023-01-06 16:58:59,579 - Epoch: [59][   70/  246]    Overall Loss 0.292691    Objective Loss 0.292691                                        LR 0.000036    Time 0.025799    
2023-01-06 16:58:59,758 - Epoch: [59][   80/  246]    Overall Loss 0.291152    Objective Loss 0.291152                                        LR 0.000036    Time 0.024811    
2023-01-06 16:58:59,940 - Epoch: [59][   90/  246]    Overall Loss 0.293497    Objective Loss 0.293497                                        LR 0.000036    Time 0.024070    
2023-01-06 16:59:00,113 - Epoch: [59][  100/  246]    Overall Loss 0.294935    Objective Loss 0.294935                                        LR 0.000036    Time 0.023391    
2023-01-06 16:59:00,285 - Epoch: [59][  110/  246]    Overall Loss 0.296937    Objective Loss 0.296937                                        LR 0.000036    Time 0.022824    
2023-01-06 16:59:00,460 - Epoch: [59][  120/  246]    Overall Loss 0.294802    Objective Loss 0.294802                                        LR 0.000036    Time 0.022380    
2023-01-06 16:59:00,634 - Epoch: [59][  130/  246]    Overall Loss 0.294921    Objective Loss 0.294921                                        LR 0.000036    Time 0.021993    
2023-01-06 16:59:00,809 - Epoch: [59][  140/  246]    Overall Loss 0.296228    Objective Loss 0.296228                                        LR 0.000036    Time 0.021667    
2023-01-06 16:59:00,986 - Epoch: [59][  150/  246]    Overall Loss 0.295675    Objective Loss 0.295675                                        LR 0.000036    Time 0.021401    
2023-01-06 16:59:01,179 - Epoch: [59][  160/  246]    Overall Loss 0.295908    Objective Loss 0.295908                                        LR 0.000036    Time 0.021271    
2023-01-06 16:59:01,387 - Epoch: [59][  170/  246]    Overall Loss 0.295559    Objective Loss 0.295559                                        LR 0.000036    Time 0.021241    
2023-01-06 16:59:01,597 - Epoch: [59][  180/  246]    Overall Loss 0.294927    Objective Loss 0.294927                                        LR 0.000036    Time 0.021223    
2023-01-06 16:59:01,805 - Epoch: [59][  190/  246]    Overall Loss 0.294800    Objective Loss 0.294800                                        LR 0.000036    Time 0.021201    
2023-01-06 16:59:02,021 - Epoch: [59][  200/  246]    Overall Loss 0.295037    Objective Loss 0.295037                                        LR 0.000036    Time 0.021218    
2023-01-06 16:59:02,218 - Epoch: [59][  210/  246]    Overall Loss 0.294645    Objective Loss 0.294645                                        LR 0.000036    Time 0.021141    
2023-01-06 16:59:02,411 - Epoch: [59][  220/  246]    Overall Loss 0.296337    Objective Loss 0.296337                                        LR 0.000036    Time 0.021056    
2023-01-06 16:59:02,598 - Epoch: [59][  230/  246]    Overall Loss 0.296139    Objective Loss 0.296139                                        LR 0.000036    Time 0.020952    
2023-01-06 16:59:02,802 - Epoch: [59][  240/  246]    Overall Loss 0.295889    Objective Loss 0.295889                                        LR 0.000036    Time 0.020927    
2023-01-06 16:59:02,897 - Epoch: [59][  246/  246]    Overall Loss 0.296529    Objective Loss 0.296529    Top1 88.995215    LR 0.000036    Time 0.020805    
2023-01-06 16:59:03,040 - --- validate (epoch=59)-----------
2023-01-06 16:59:03,040 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:03,500 - Epoch: [59][   10/   28]    Loss 0.303568    Top1 89.101562    
2023-01-06 16:59:03,624 - Epoch: [59][   20/   28]    Loss 0.307365    Top1 88.750000    
2023-01-06 16:59:03,691 - Epoch: [59][   28/   28]    Loss 0.301961    Top1 88.935013    
2023-01-06 16:59:03,849 - ==> Top1: 88.935    Loss: 0.302

2023-01-06 16:59:03,849 - ==> Confusion:
[[ 181   13  245]
 [  11  208  383]
 [  50   71 5824]]

2023-01-06 16:59:03,850 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:03,850 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:03,856 - 

2023-01-06 16:59:03,856 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:04,407 - Epoch: [60][   10/  246]    Overall Loss 0.313229    Objective Loss 0.313229                                        LR 0.000036    Time 0.055008    
2023-01-06 16:59:04,574 - Epoch: [60][   20/  246]    Overall Loss 0.296633    Objective Loss 0.296633                                        LR 0.000036    Time 0.035761    
2023-01-06 16:59:04,760 - Epoch: [60][   30/  246]    Overall Loss 0.290246    Objective Loss 0.290246                                        LR 0.000036    Time 0.030040    
2023-01-06 16:59:04,949 - Epoch: [60][   40/  246]    Overall Loss 0.292730    Objective Loss 0.292730                                        LR 0.000036    Time 0.027230    
2023-01-06 16:59:05,126 - Epoch: [60][   50/  246]    Overall Loss 0.293215    Objective Loss 0.293215                                        LR 0.000036    Time 0.025321    
2023-01-06 16:59:05,308 - Epoch: [60][   60/  246]    Overall Loss 0.295157    Objective Loss 0.295157                                        LR 0.000036    Time 0.024127    
2023-01-06 16:59:05,490 - Epoch: [60][   70/  246]    Overall Loss 0.295677    Objective Loss 0.295677                                        LR 0.000036    Time 0.023278    
2023-01-06 16:59:05,674 - Epoch: [60][   80/  246]    Overall Loss 0.295470    Objective Loss 0.295470                                        LR 0.000036    Time 0.022658    
2023-01-06 16:59:05,856 - Epoch: [60][   90/  246]    Overall Loss 0.296568    Objective Loss 0.296568                                        LR 0.000036    Time 0.022162    
2023-01-06 16:59:06,026 - Epoch: [60][  100/  246]    Overall Loss 0.296329    Objective Loss 0.296329                                        LR 0.000036    Time 0.021643    
2023-01-06 16:59:06,198 - Epoch: [60][  110/  246]    Overall Loss 0.294680    Objective Loss 0.294680                                        LR 0.000036    Time 0.021241    
2023-01-06 16:59:06,374 - Epoch: [60][  120/  246]    Overall Loss 0.294287    Objective Loss 0.294287                                        LR 0.000036    Time 0.020934    
2023-01-06 16:59:06,545 - Epoch: [60][  130/  246]    Overall Loss 0.291702    Objective Loss 0.291702                                        LR 0.000036    Time 0.020634    
2023-01-06 16:59:06,715 - Epoch: [60][  140/  246]    Overall Loss 0.292241    Objective Loss 0.292241                                        LR 0.000036    Time 0.020376    
2023-01-06 16:59:06,893 - Epoch: [60][  150/  246]    Overall Loss 0.292641    Objective Loss 0.292641                                        LR 0.000036    Time 0.020198    
2023-01-06 16:59:07,073 - Epoch: [60][  160/  246]    Overall Loss 0.292568    Objective Loss 0.292568                                        LR 0.000036    Time 0.020056    
2023-01-06 16:59:07,244 - Epoch: [60][  170/  246]    Overall Loss 0.292181    Objective Loss 0.292181                                        LR 0.000036    Time 0.019882    
2023-01-06 16:59:07,423 - Epoch: [60][  180/  246]    Overall Loss 0.292742    Objective Loss 0.292742                                        LR 0.000036    Time 0.019770    
2023-01-06 16:59:07,606 - Epoch: [60][  190/  246]    Overall Loss 0.292720    Objective Loss 0.292720                                        LR 0.000036    Time 0.019690    
2023-01-06 16:59:07,785 - Epoch: [60][  200/  246]    Overall Loss 0.292852    Objective Loss 0.292852                                        LR 0.000036    Time 0.019602    
2023-01-06 16:59:07,957 - Epoch: [60][  210/  246]    Overall Loss 0.293081    Objective Loss 0.293081                                        LR 0.000036    Time 0.019487    
2023-01-06 16:59:08,126 - Epoch: [60][  220/  246]    Overall Loss 0.293060    Objective Loss 0.293060                                        LR 0.000036    Time 0.019364    
2023-01-06 16:59:08,292 - Epoch: [60][  230/  246]    Overall Loss 0.293111    Objective Loss 0.293111                                        LR 0.000036    Time 0.019243    
2023-01-06 16:59:08,488 - Epoch: [60][  240/  246]    Overall Loss 0.294341    Objective Loss 0.294341                                        LR 0.000036    Time 0.019259    
2023-01-06 16:59:08,580 - Epoch: [60][  246/  246]    Overall Loss 0.294489    Objective Loss 0.294489    Top1 88.755981    LR 0.000036    Time 0.019160    
2023-01-06 16:59:08,736 - --- validate (epoch=60)-----------
2023-01-06 16:59:08,736 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:09,200 - Epoch: [60][   10/   28]    Loss 0.310565    Top1 88.867188    
2023-01-06 16:59:09,314 - Epoch: [60][   20/   28]    Loss 0.297325    Top1 89.394531    
2023-01-06 16:59:09,382 - Epoch: [60][   28/   28]    Loss 0.305830    Top1 89.164042    
2023-01-06 16:59:09,543 - ==> Top1: 89.164    Loss: 0.306

2023-01-06 16:59:09,544 - ==> Confusion:
[[ 194    9  236]
 [  13  178  411]
 [  50   38 5857]]

2023-01-06 16:59:09,545 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:09,545 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:09,551 - 

2023-01-06 16:59:09,551 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:10,263 - Epoch: [61][   10/  246]    Overall Loss 0.297544    Objective Loss 0.297544                                        LR 0.000036    Time 0.071183    
2023-01-06 16:59:10,462 - Epoch: [61][   20/  246]    Overall Loss 0.295417    Objective Loss 0.295417                                        LR 0.000036    Time 0.045523    
2023-01-06 16:59:10,668 - Epoch: [61][   30/  246]    Overall Loss 0.296340    Objective Loss 0.296340                                        LR 0.000036    Time 0.037193    
2023-01-06 16:59:10,875 - Epoch: [61][   40/  246]    Overall Loss 0.289215    Objective Loss 0.289215                                        LR 0.000036    Time 0.033072    
2023-01-06 16:59:11,075 - Epoch: [61][   50/  246]    Overall Loss 0.295115    Objective Loss 0.295115                                        LR 0.000036    Time 0.030450    
2023-01-06 16:59:11,277 - Epoch: [61][   60/  246]    Overall Loss 0.296176    Objective Loss 0.296176                                        LR 0.000036    Time 0.028731    
2023-01-06 16:59:11,484 - Epoch: [61][   70/  246]    Overall Loss 0.298470    Objective Loss 0.298470                                        LR 0.000036    Time 0.027576    
2023-01-06 16:59:11,712 - Epoch: [61][   80/  246]    Overall Loss 0.296533    Objective Loss 0.296533                                        LR 0.000036    Time 0.026977    
2023-01-06 16:59:11,942 - Epoch: [61][   90/  246]    Overall Loss 0.295876    Objective Loss 0.295876                                        LR 0.000036    Time 0.026530    
2023-01-06 16:59:12,155 - Epoch: [61][  100/  246]    Overall Loss 0.294890    Objective Loss 0.294890                                        LR 0.000036    Time 0.025994    
2023-01-06 16:59:12,369 - Epoch: [61][  110/  246]    Overall Loss 0.294676    Objective Loss 0.294676                                        LR 0.000036    Time 0.025572    
2023-01-06 16:59:12,583 - Epoch: [61][  120/  246]    Overall Loss 0.294395    Objective Loss 0.294395                                        LR 0.000036    Time 0.025226    
2023-01-06 16:59:12,822 - Epoch: [61][  130/  246]    Overall Loss 0.293957    Objective Loss 0.293957                                        LR 0.000036    Time 0.025121    
2023-01-06 16:59:13,060 - Epoch: [61][  140/  246]    Overall Loss 0.293715    Objective Loss 0.293715                                        LR 0.000036    Time 0.025021    
2023-01-06 16:59:13,293 - Epoch: [61][  150/  246]    Overall Loss 0.294011    Objective Loss 0.294011                                        LR 0.000036    Time 0.024904    
2023-01-06 16:59:13,540 - Epoch: [61][  160/  246]    Overall Loss 0.293101    Objective Loss 0.293101                                        LR 0.000036    Time 0.024884    
2023-01-06 16:59:13,770 - Epoch: [61][  170/  246]    Overall Loss 0.292005    Objective Loss 0.292005                                        LR 0.000036    Time 0.024770    
2023-01-06 16:59:13,998 - Epoch: [61][  180/  246]    Overall Loss 0.292569    Objective Loss 0.292569                                        LR 0.000036    Time 0.024661    
2023-01-06 16:59:14,197 - Epoch: [61][  190/  246]    Overall Loss 0.293307    Objective Loss 0.293307                                        LR 0.000036    Time 0.024406    
2023-01-06 16:59:14,397 - Epoch: [61][  200/  246]    Overall Loss 0.293905    Objective Loss 0.293905                                        LR 0.000036    Time 0.024185    
2023-01-06 16:59:14,603 - Epoch: [61][  210/  246]    Overall Loss 0.293293    Objective Loss 0.293293                                        LR 0.000036    Time 0.024013    
2023-01-06 16:59:14,832 - Epoch: [61][  220/  246]    Overall Loss 0.293108    Objective Loss 0.293108                                        LR 0.000036    Time 0.023960    
2023-01-06 16:59:15,062 - Epoch: [61][  230/  246]    Overall Loss 0.293558    Objective Loss 0.293558                                        LR 0.000036    Time 0.023915    
2023-01-06 16:59:15,309 - Epoch: [61][  240/  246]    Overall Loss 0.293887    Objective Loss 0.293887                                        LR 0.000036    Time 0.023947    
2023-01-06 16:59:15,422 - Epoch: [61][  246/  246]    Overall Loss 0.293721    Objective Loss 0.293721    Top1 87.559809    LR 0.000036    Time 0.023819    
2023-01-06 16:59:15,563 - --- validate (epoch=61)-----------
2023-01-06 16:59:15,563 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:16,018 - Epoch: [61][   10/   28]    Loss 0.291777    Top1 89.687500    
2023-01-06 16:59:16,148 - Epoch: [61][   20/   28]    Loss 0.281267    Top1 89.824219    
2023-01-06 16:59:16,216 - Epoch: [61][   28/   28]    Loss 0.298558    Top1 89.321500    
2023-01-06 16:59:16,365 - ==> Top1: 89.322    Loss: 0.299

2023-01-06 16:59:16,365 - ==> Confusion:
[[ 207    7  225]
 [  16  180  406]
 [  49   43 5853]]

2023-01-06 16:59:16,366 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:16,366 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:16,372 - 

2023-01-06 16:59:16,373 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:17,085 - Epoch: [62][   10/  246]    Overall Loss 0.270456    Objective Loss 0.270456                                        LR 0.000036    Time 0.071149    
2023-01-06 16:59:17,271 - Epoch: [62][   20/  246]    Overall Loss 0.279363    Objective Loss 0.279363                                        LR 0.000036    Time 0.044846    
2023-01-06 16:59:17,471 - Epoch: [62][   30/  246]    Overall Loss 0.284044    Objective Loss 0.284044                                        LR 0.000036    Time 0.036580    
2023-01-06 16:59:17,682 - Epoch: [62][   40/  246]    Overall Loss 0.289791    Objective Loss 0.289791                                        LR 0.000036    Time 0.032699    
2023-01-06 16:59:17,893 - Epoch: [62][   50/  246]    Overall Loss 0.291762    Objective Loss 0.291762                                        LR 0.000036    Time 0.030359    
2023-01-06 16:59:18,104 - Epoch: [62][   60/  246]    Overall Loss 0.290565    Objective Loss 0.290565                                        LR 0.000036    Time 0.028815    
2023-01-06 16:59:18,310 - Epoch: [62][   70/  246]    Overall Loss 0.285749    Objective Loss 0.285749                                        LR 0.000036    Time 0.027640    
2023-01-06 16:59:18,520 - Epoch: [62][   80/  246]    Overall Loss 0.287718    Objective Loss 0.287718                                        LR 0.000036    Time 0.026802    
2023-01-06 16:59:18,709 - Epoch: [62][   90/  246]    Overall Loss 0.286922    Objective Loss 0.286922                                        LR 0.000036    Time 0.025926    
2023-01-06 16:59:18,900 - Epoch: [62][  100/  246]    Overall Loss 0.286431    Objective Loss 0.286431                                        LR 0.000036    Time 0.025232    
2023-01-06 16:59:19,077 - Epoch: [62][  110/  246]    Overall Loss 0.285286    Objective Loss 0.285286                                        LR 0.000036    Time 0.024550    
2023-01-06 16:59:19,265 - Epoch: [62][  120/  246]    Overall Loss 0.287023    Objective Loss 0.287023                                        LR 0.000036    Time 0.024069    
2023-01-06 16:59:19,446 - Epoch: [62][  130/  246]    Overall Loss 0.288404    Objective Loss 0.288404                                        LR 0.000036    Time 0.023604    
2023-01-06 16:59:19,634 - Epoch: [62][  140/  246]    Overall Loss 0.289513    Objective Loss 0.289513                                        LR 0.000036    Time 0.023261    
2023-01-06 16:59:19,815 - Epoch: [62][  150/  246]    Overall Loss 0.290527    Objective Loss 0.290527                                        LR 0.000036    Time 0.022910    
2023-01-06 16:59:20,003 - Epoch: [62][  160/  246]    Overall Loss 0.290411    Objective Loss 0.290411                                        LR 0.000036    Time 0.022654    
2023-01-06 16:59:20,183 - Epoch: [62][  170/  246]    Overall Loss 0.290367    Objective Loss 0.290367                                        LR 0.000036    Time 0.022379    
2023-01-06 16:59:20,372 - Epoch: [62][  180/  246]    Overall Loss 0.290472    Objective Loss 0.290472                                        LR 0.000036    Time 0.022185    
2023-01-06 16:59:20,553 - Epoch: [62][  190/  246]    Overall Loss 0.290422    Objective Loss 0.290422                                        LR 0.000036    Time 0.021968    
2023-01-06 16:59:20,741 - Epoch: [62][  200/  246]    Overall Loss 0.290494    Objective Loss 0.290494                                        LR 0.000036    Time 0.021807    
2023-01-06 16:59:20,919 - Epoch: [62][  210/  246]    Overall Loss 0.291116    Objective Loss 0.291116                                        LR 0.000036    Time 0.021615    
2023-01-06 16:59:21,114 - Epoch: [62][  220/  246]    Overall Loss 0.292147    Objective Loss 0.292147                                        LR 0.000036    Time 0.021517    
2023-01-06 16:59:21,319 - Epoch: [62][  230/  246]    Overall Loss 0.293230    Objective Loss 0.293230                                        LR 0.000036    Time 0.021472    
2023-01-06 16:59:21,533 - Epoch: [62][  240/  246]    Overall Loss 0.292693    Objective Loss 0.292693                                        LR 0.000036    Time 0.021465    
2023-01-06 16:59:21,629 - Epoch: [62][  246/  246]    Overall Loss 0.292614    Objective Loss 0.292614    Top1 86.124402    LR 0.000036    Time 0.021332    
2023-01-06 16:59:21,760 - --- validate (epoch=62)-----------
2023-01-06 16:59:21,760 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:22,216 - Epoch: [62][   10/   28]    Loss 0.296425    Top1 89.023438    
2023-01-06 16:59:22,328 - Epoch: [62][   20/   28]    Loss 0.296700    Top1 89.042969    
2023-01-06 16:59:22,396 - Epoch: [62][   28/   28]    Loss 0.292194    Top1 89.264243    
2023-01-06 16:59:22,549 - ==> Top1: 89.264    Loss: 0.292

2023-01-06 16:59:22,549 - ==> Confusion:
[[ 216    8  215]
 [  19  217  366]
 [  71   71 5803]]

2023-01-06 16:59:22,550 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:22,550 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:22,556 - 

2023-01-06 16:59:22,556 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:23,109 - Epoch: [63][   10/  246]    Overall Loss 0.294812    Objective Loss 0.294812                                        LR 0.000036    Time 0.055184    
2023-01-06 16:59:23,291 - Epoch: [63][   20/  246]    Overall Loss 0.298391    Objective Loss 0.298391                                        LR 0.000036    Time 0.036665    
2023-01-06 16:59:23,493 - Epoch: [63][   30/  246]    Overall Loss 0.301933    Objective Loss 0.301933                                        LR 0.000036    Time 0.031186    
2023-01-06 16:59:23,697 - Epoch: [63][   40/  246]    Overall Loss 0.301126    Objective Loss 0.301126                                        LR 0.000036    Time 0.028469    
2023-01-06 16:59:23,905 - Epoch: [63][   50/  246]    Overall Loss 0.301948    Objective Loss 0.301948                                        LR 0.000036    Time 0.026920    
2023-01-06 16:59:24,107 - Epoch: [63][   60/  246]    Overall Loss 0.307972    Objective Loss 0.307972                                        LR 0.000036    Time 0.025799    
2023-01-06 16:59:24,310 - Epoch: [63][   70/  246]    Overall Loss 0.318107    Objective Loss 0.318107                                        LR 0.000036    Time 0.025006    
2023-01-06 16:59:24,514 - Epoch: [63][   80/  246]    Overall Loss 0.324352    Objective Loss 0.324352                                        LR 0.000036    Time 0.024424    
2023-01-06 16:59:24,720 - Epoch: [63][   90/  246]    Overall Loss 0.325672    Objective Loss 0.325672                                        LR 0.000036    Time 0.023998    
2023-01-06 16:59:24,925 - Epoch: [63][  100/  246]    Overall Loss 0.323080    Objective Loss 0.323080                                        LR 0.000036    Time 0.023645    
2023-01-06 16:59:25,123 - Epoch: [63][  110/  246]    Overall Loss 0.320884    Objective Loss 0.320884                                        LR 0.000036    Time 0.023293    
2023-01-06 16:59:25,325 - Epoch: [63][  120/  246]    Overall Loss 0.318859    Objective Loss 0.318859                                        LR 0.000036    Time 0.023030    
2023-01-06 16:59:25,524 - Epoch: [63][  130/  246]    Overall Loss 0.317070    Objective Loss 0.317070                                        LR 0.000036    Time 0.022785    
2023-01-06 16:59:25,725 - Epoch: [63][  140/  246]    Overall Loss 0.316557    Objective Loss 0.316557                                        LR 0.000036    Time 0.022593    
2023-01-06 16:59:25,923 - Epoch: [63][  150/  246]    Overall Loss 0.317876    Objective Loss 0.317876                                        LR 0.000036    Time 0.022404    
2023-01-06 16:59:26,103 - Epoch: [63][  160/  246]    Overall Loss 0.319331    Objective Loss 0.319331                                        LR 0.000036    Time 0.022126    
2023-01-06 16:59:26,277 - Epoch: [63][  170/  246]    Overall Loss 0.319668    Objective Loss 0.319668                                        LR 0.000036    Time 0.021847    
2023-01-06 16:59:26,451 - Epoch: [63][  180/  246]    Overall Loss 0.320672    Objective Loss 0.320672                                        LR 0.000036    Time 0.021597    
2023-01-06 16:59:26,626 - Epoch: [63][  190/  246]    Overall Loss 0.320721    Objective Loss 0.320721                                        LR 0.000036    Time 0.021378    
2023-01-06 16:59:26,800 - Epoch: [63][  200/  246]    Overall Loss 0.320868    Objective Loss 0.320868                                        LR 0.000036    Time 0.021178    
2023-01-06 16:59:26,972 - Epoch: [63][  210/  246]    Overall Loss 0.321513    Objective Loss 0.321513                                        LR 0.000036    Time 0.020987    
2023-01-06 16:59:27,143 - Epoch: [63][  220/  246]    Overall Loss 0.322437    Objective Loss 0.322437                                        LR 0.000036    Time 0.020809    
2023-01-06 16:59:27,314 - Epoch: [63][  230/  246]    Overall Loss 0.321865    Objective Loss 0.321865                                        LR 0.000036    Time 0.020646    
2023-01-06 16:59:27,497 - Epoch: [63][  240/  246]    Overall Loss 0.321494    Objective Loss 0.321494                                        LR 0.000036    Time 0.020547    
2023-01-06 16:59:27,585 - Epoch: [63][  246/  246]    Overall Loss 0.320383    Objective Loss 0.320383    Top1 91.387560    LR 0.000036    Time 0.020405    
2023-01-06 16:59:27,729 - --- validate (epoch=63)-----------
2023-01-06 16:59:27,729 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:28,193 - Epoch: [63][   10/   28]    Loss 0.308021    Top1 89.179688    
2023-01-06 16:59:28,315 - Epoch: [63][   20/   28]    Loss 0.302617    Top1 89.414062    
2023-01-06 16:59:28,383 - Epoch: [63][   28/   28]    Loss 0.303636    Top1 89.393072    
2023-01-06 16:59:28,537 - ==> Top1: 89.393    Loss: 0.304

2023-01-06 16:59:28,538 - ==> Confusion:
[[ 231   15  193]
 [  13  236  353]
 [  92   75 5778]]

2023-01-06 16:59:28,539 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:28,539 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:28,545 - 

2023-01-06 16:59:28,545 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:29,249 - Epoch: [64][   10/  246]    Overall Loss 0.323456    Objective Loss 0.323456                                        LR 0.000036    Time 0.070349    
2023-01-06 16:59:29,431 - Epoch: [64][   20/  246]    Overall Loss 0.319991    Objective Loss 0.319991                                        LR 0.000036    Time 0.044236    
2023-01-06 16:59:29,604 - Epoch: [64][   30/  246]    Overall Loss 0.312695    Objective Loss 0.312695                                        LR 0.000036    Time 0.035218    
2023-01-06 16:59:29,784 - Epoch: [64][   40/  246]    Overall Loss 0.308646    Objective Loss 0.308646                                        LR 0.000036    Time 0.030917    
2023-01-06 16:59:29,958 - Epoch: [64][   50/  246]    Overall Loss 0.310333    Objective Loss 0.310333                                        LR 0.000036    Time 0.028205    
2023-01-06 16:59:30,132 - Epoch: [64][   60/  246]    Overall Loss 0.309185    Objective Loss 0.309185                                        LR 0.000036    Time 0.026394    
2023-01-06 16:59:30,305 - Epoch: [64][   70/  246]    Overall Loss 0.317699    Objective Loss 0.317699                                        LR 0.000036    Time 0.025075    
2023-01-06 16:59:30,477 - Epoch: [64][   80/  246]    Overall Loss 0.322351    Objective Loss 0.322351                                        LR 0.000036    Time 0.024079    
2023-01-06 16:59:30,643 - Epoch: [64][   90/  246]    Overall Loss 0.326878    Objective Loss 0.326878                                        LR 0.000036    Time 0.023253    
2023-01-06 16:59:30,814 - Epoch: [64][  100/  246]    Overall Loss 0.329285    Objective Loss 0.329285                                        LR 0.000036    Time 0.022629    
2023-01-06 16:59:30,984 - Epoch: [64][  110/  246]    Overall Loss 0.329695    Objective Loss 0.329695                                        LR 0.000036    Time 0.022120    
2023-01-06 16:59:31,159 - Epoch: [64][  120/  246]    Overall Loss 0.329524    Objective Loss 0.329524                                        LR 0.000036    Time 0.021724    
2023-01-06 16:59:31,325 - Epoch: [64][  130/  246]    Overall Loss 0.328283    Objective Loss 0.328283                                        LR 0.000036    Time 0.021329    
2023-01-06 16:59:31,497 - Epoch: [64][  140/  246]    Overall Loss 0.326925    Objective Loss 0.326925                                        LR 0.000036    Time 0.021031    
2023-01-06 16:59:31,663 - Epoch: [64][  150/  246]    Overall Loss 0.325863    Objective Loss 0.325863                                        LR 0.000036    Time 0.020735    
2023-01-06 16:59:31,833 - Epoch: [64][  160/  246]    Overall Loss 0.325907    Objective Loss 0.325907                                        LR 0.000036    Time 0.020501    
2023-01-06 16:59:31,994 - Epoch: [64][  170/  246]    Overall Loss 0.325775    Objective Loss 0.325775                                        LR 0.000036    Time 0.020240    
2023-01-06 16:59:32,159 - Epoch: [64][  180/  246]    Overall Loss 0.324875    Objective Loss 0.324875                                        LR 0.000036    Time 0.020031    
2023-01-06 16:59:32,322 - Epoch: [64][  190/  246]    Overall Loss 0.323988    Objective Loss 0.323988                                        LR 0.000036    Time 0.019832    
2023-01-06 16:59:32,486 - Epoch: [64][  200/  246]    Overall Loss 0.324559    Objective Loss 0.324559                                        LR 0.000036    Time 0.019659    
2023-01-06 16:59:32,678 - Epoch: [64][  210/  246]    Overall Loss 0.323452    Objective Loss 0.323452                                        LR 0.000036    Time 0.019635    
2023-01-06 16:59:32,871 - Epoch: [64][  220/  246]    Overall Loss 0.323616    Objective Loss 0.323616                                        LR 0.000036    Time 0.019615    
2023-01-06 16:59:33,068 - Epoch: [64][  230/  246]    Overall Loss 0.323272    Objective Loss 0.323272                                        LR 0.000036    Time 0.019618    
2023-01-06 16:59:33,272 - Epoch: [64][  240/  246]    Overall Loss 0.323178    Objective Loss 0.323178                                        LR 0.000036    Time 0.019651    
2023-01-06 16:59:33,369 - Epoch: [64][  246/  246]    Overall Loss 0.322481    Objective Loss 0.322481    Top1 90.430622    LR 0.000036    Time 0.019563    
2023-01-06 16:59:33,552 - --- validate (epoch=64)-----------
2023-01-06 16:59:33,552 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:33,996 - Epoch: [64][   10/   28]    Loss 0.330438    Top1 87.812500    
2023-01-06 16:59:34,114 - Epoch: [64][   20/   28]    Loss 0.320526    Top1 88.671875    
2023-01-06 16:59:34,183 - Epoch: [64][   28/   28]    Loss 0.313838    Top1 88.877756    
2023-01-06 16:59:34,337 - ==> Top1: 88.878    Loss: 0.314

2023-01-06 16:59:34,337 - ==> Confusion:
[[ 165    5  269]
 [   9  158  435]
 [  32   27 5886]]

2023-01-06 16:59:34,338 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:34,338 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:34,344 - 

2023-01-06 16:59:34,344 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:34,907 - Epoch: [65][   10/  246]    Overall Loss 0.329208    Objective Loss 0.329208                                        LR 0.000036    Time 0.056158    
2023-01-06 16:59:35,091 - Epoch: [65][   20/  246]    Overall Loss 0.325369    Objective Loss 0.325369                                        LR 0.000036    Time 0.037280    
2023-01-06 16:59:35,280 - Epoch: [65][   30/  246]    Overall Loss 0.322214    Objective Loss 0.322214                                        LR 0.000036    Time 0.031151    
2023-01-06 16:59:35,470 - Epoch: [65][   40/  246]    Overall Loss 0.315481    Objective Loss 0.315481                                        LR 0.000036    Time 0.028088    
2023-01-06 16:59:35,664 - Epoch: [65][   50/  246]    Overall Loss 0.313401    Objective Loss 0.313401                                        LR 0.000036    Time 0.026351    
2023-01-06 16:59:35,846 - Epoch: [65][   60/  246]    Overall Loss 0.314875    Objective Loss 0.314875                                        LR 0.000036    Time 0.024979    
2023-01-06 16:59:36,048 - Epoch: [65][   70/  246]    Overall Loss 0.315970    Objective Loss 0.315970                                        LR 0.000036    Time 0.024292    
2023-01-06 16:59:36,242 - Epoch: [65][   80/  246]    Overall Loss 0.317150    Objective Loss 0.317150                                        LR 0.000036    Time 0.023682    
2023-01-06 16:59:36,438 - Epoch: [65][   90/  246]    Overall Loss 0.314137    Objective Loss 0.314137                                        LR 0.000036    Time 0.023222    
2023-01-06 16:59:36,637 - Epoch: [65][  100/  246]    Overall Loss 0.313550    Objective Loss 0.313550                                        LR 0.000036    Time 0.022890    
2023-01-06 16:59:36,833 - Epoch: [65][  110/  246]    Overall Loss 0.313625    Objective Loss 0.313625                                        LR 0.000036    Time 0.022583    
2023-01-06 16:59:37,032 - Epoch: [65][  120/  246]    Overall Loss 0.313368    Objective Loss 0.313368                                        LR 0.000036    Time 0.022357    
2023-01-06 16:59:37,230 - Epoch: [65][  130/  246]    Overall Loss 0.312832    Objective Loss 0.312832                                        LR 0.000036    Time 0.022155    
2023-01-06 16:59:37,428 - Epoch: [65][  140/  246]    Overall Loss 0.312790    Objective Loss 0.312790                                        LR 0.000036    Time 0.021988    
2023-01-06 16:59:37,625 - Epoch: [65][  150/  246]    Overall Loss 0.314187    Objective Loss 0.314187                                        LR 0.000036    Time 0.021834    
2023-01-06 16:59:37,822 - Epoch: [65][  160/  246]    Overall Loss 0.314040    Objective Loss 0.314040                                        LR 0.000036    Time 0.021698    
2023-01-06 16:59:38,015 - Epoch: [65][  170/  246]    Overall Loss 0.314839    Objective Loss 0.314839                                        LR 0.000036    Time 0.021553    
2023-01-06 16:59:38,201 - Epoch: [65][  180/  246]    Overall Loss 0.314710    Objective Loss 0.314710                                        LR 0.000036    Time 0.021385    
2023-01-06 16:59:38,395 - Epoch: [65][  190/  246]    Overall Loss 0.314228    Objective Loss 0.314228                                        LR 0.000036    Time 0.021279    
2023-01-06 16:59:38,588 - Epoch: [65][  200/  246]    Overall Loss 0.313674    Objective Loss 0.313674                                        LR 0.000036    Time 0.021178    
2023-01-06 16:59:38,779 - Epoch: [65][  210/  246]    Overall Loss 0.313266    Objective Loss 0.313266                                        LR 0.000036    Time 0.021081    
2023-01-06 16:59:38,971 - Epoch: [65][  220/  246]    Overall Loss 0.313784    Objective Loss 0.313784                                        LR 0.000036    Time 0.020993    
2023-01-06 16:59:39,163 - Epoch: [65][  230/  246]    Overall Loss 0.313600    Objective Loss 0.313600                                        LR 0.000036    Time 0.020913    
2023-01-06 16:59:39,369 - Epoch: [65][  240/  246]    Overall Loss 0.313327    Objective Loss 0.313327                                        LR 0.000036    Time 0.020897    
2023-01-06 16:59:39,465 - Epoch: [65][  246/  246]    Overall Loss 0.313617    Objective Loss 0.313617    Top1 88.516746    LR 0.000036    Time 0.020778    
2023-01-06 16:59:39,592 - --- validate (epoch=65)-----------
2023-01-06 16:59:39,592 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:40,039 - Epoch: [65][   10/   28]    Loss 0.310271    Top1 88.476562    
2023-01-06 16:59:40,171 - Epoch: [65][   20/   28]    Loss 0.314143    Top1 88.515625    
2023-01-06 16:59:40,239 - Epoch: [65][   28/   28]    Loss 0.307200    Top1 88.892070    
2023-01-06 16:59:40,398 - ==> Top1: 88.892    Loss: 0.307

2023-01-06 16:59:40,399 - ==> Confusion:
[[ 164    5  270]
 [   8  158  436]
 [  32   25 5888]]

2023-01-06 16:59:40,400 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:40,400 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:40,406 - 

2023-01-06 16:59:40,406 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:41,112 - Epoch: [66][   10/  246]    Overall Loss 0.300485    Objective Loss 0.300485                                        LR 0.000036    Time 0.070507    
2023-01-06 16:59:41,305 - Epoch: [66][   20/  246]    Overall Loss 0.304840    Objective Loss 0.304840                                        LR 0.000036    Time 0.044884    
2023-01-06 16:59:41,510 - Epoch: [66][   30/  246]    Overall Loss 0.305807    Objective Loss 0.305807                                        LR 0.000036    Time 0.036742    
2023-01-06 16:59:41,708 - Epoch: [66][   40/  246]    Overall Loss 0.306355    Objective Loss 0.306355                                        LR 0.000036    Time 0.032505    
2023-01-06 16:59:41,913 - Epoch: [66][   50/  246]    Overall Loss 0.307469    Objective Loss 0.307469                                        LR 0.000036    Time 0.030095    
2023-01-06 16:59:42,109 - Epoch: [66][   60/  246]    Overall Loss 0.307693    Objective Loss 0.307693                                        LR 0.000036    Time 0.028340    
2023-01-06 16:59:42,313 - Epoch: [66][   70/  246]    Overall Loss 0.307345    Objective Loss 0.307345                                        LR 0.000036    Time 0.027204    
2023-01-06 16:59:42,512 - Epoch: [66][   80/  246]    Overall Loss 0.309044    Objective Loss 0.309044                                        LR 0.000036    Time 0.026287    
2023-01-06 16:59:42,716 - Epoch: [66][   90/  246]    Overall Loss 0.309954    Objective Loss 0.309954                                        LR 0.000036    Time 0.025626    
2023-01-06 16:59:42,915 - Epoch: [66][  100/  246]    Overall Loss 0.308194    Objective Loss 0.308194                                        LR 0.000036    Time 0.025044    
2023-01-06 16:59:43,120 - Epoch: [66][  110/  246]    Overall Loss 0.306163    Objective Loss 0.306163                                        LR 0.000036    Time 0.024629    
2023-01-06 16:59:43,318 - Epoch: [66][  120/  246]    Overall Loss 0.307485    Objective Loss 0.307485                                        LR 0.000036    Time 0.024223    
2023-01-06 16:59:43,525 - Epoch: [66][  130/  246]    Overall Loss 0.308350    Objective Loss 0.308350                                        LR 0.000036    Time 0.023952    
2023-01-06 16:59:43,723 - Epoch: [66][  140/  246]    Overall Loss 0.308339    Objective Loss 0.308339                                        LR 0.000036    Time 0.023655    
2023-01-06 16:59:43,929 - Epoch: [66][  150/  246]    Overall Loss 0.308759    Objective Loss 0.308759                                        LR 0.000036    Time 0.023447    
2023-01-06 16:59:44,125 - Epoch: [66][  160/  246]    Overall Loss 0.309127    Objective Loss 0.309127                                        LR 0.000036    Time 0.023202    
2023-01-06 16:59:44,331 - Epoch: [66][  170/  246]    Overall Loss 0.308963    Objective Loss 0.308963                                        LR 0.000036    Time 0.023046    
2023-01-06 16:59:44,530 - Epoch: [66][  180/  246]    Overall Loss 0.309845    Objective Loss 0.309845                                        LR 0.000036    Time 0.022873    
2023-01-06 16:59:44,737 - Epoch: [66][  190/  246]    Overall Loss 0.309693    Objective Loss 0.309693                                        LR 0.000036    Time 0.022756    
2023-01-06 16:59:44,938 - Epoch: [66][  200/  246]    Overall Loss 0.309269    Objective Loss 0.309269                                        LR 0.000036    Time 0.022619    
2023-01-06 16:59:45,136 - Epoch: [66][  210/  246]    Overall Loss 0.309143    Objective Loss 0.309143                                        LR 0.000036    Time 0.022486    
2023-01-06 16:59:45,333 - Epoch: [66][  220/  246]    Overall Loss 0.308896    Objective Loss 0.308896                                        LR 0.000036    Time 0.022357    
2023-01-06 16:59:45,530 - Epoch: [66][  230/  246]    Overall Loss 0.309121    Objective Loss 0.309121                                        LR 0.000036    Time 0.022239    
2023-01-06 16:59:45,748 - Epoch: [66][  240/  246]    Overall Loss 0.309658    Objective Loss 0.309658                                        LR 0.000036    Time 0.022219    
2023-01-06 16:59:45,843 - Epoch: [66][  246/  246]    Overall Loss 0.309421    Objective Loss 0.309421    Top1 88.755981    LR 0.000036    Time 0.022062    
2023-01-06 16:59:45,977 - --- validate (epoch=66)-----------
2023-01-06 16:59:45,977 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:46,421 - Epoch: [66][   10/   28]    Loss 0.292780    Top1 89.609375    
2023-01-06 16:59:46,531 - Epoch: [66][   20/   28]    Loss 0.306939    Top1 89.179688    
2023-01-06 16:59:46,599 - Epoch: [66][   28/   28]    Loss 0.308580    Top1 89.078156    
2023-01-06 16:59:46,752 - ==> Top1: 89.078    Loss: 0.309

2023-01-06 16:59:46,752 - ==> Confusion:
[[ 189    6  244]
 [  12  172  418]
 [  50   33 5862]]

2023-01-06 16:59:46,753 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:46,753 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:46,759 - 

2023-01-06 16:59:46,759 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:47,353 - Epoch: [67][   10/  246]    Overall Loss 0.302019    Objective Loss 0.302019                                        LR 0.000036    Time 0.059272    
2023-01-06 16:59:47,546 - Epoch: [67][   20/  246]    Overall Loss 0.302655    Objective Loss 0.302655                                        LR 0.000036    Time 0.039272    
2023-01-06 16:59:47,743 - Epoch: [67][   30/  246]    Overall Loss 0.304360    Objective Loss 0.304360                                        LR 0.000036    Time 0.032746    
2023-01-06 16:59:47,943 - Epoch: [67][   40/  246]    Overall Loss 0.304595    Objective Loss 0.304595                                        LR 0.000036    Time 0.029542    
2023-01-06 16:59:48,142 - Epoch: [67][   50/  246]    Overall Loss 0.305556    Objective Loss 0.305556                                        LR 0.000036    Time 0.027614    
2023-01-06 16:59:48,342 - Epoch: [67][   60/  246]    Overall Loss 0.305648    Objective Loss 0.305648                                        LR 0.000036    Time 0.026337    
2023-01-06 16:59:48,539 - Epoch: [67][   70/  246]    Overall Loss 0.309547    Objective Loss 0.309547                                        LR 0.000036    Time 0.025386    
2023-01-06 16:59:48,739 - Epoch: [67][   80/  246]    Overall Loss 0.307063    Objective Loss 0.307063                                        LR 0.000036    Time 0.024709    
2023-01-06 16:59:48,941 - Epoch: [67][   90/  246]    Overall Loss 0.305773    Objective Loss 0.305773                                        LR 0.000036    Time 0.024201    
2023-01-06 16:59:49,148 - Epoch: [67][  100/  246]    Overall Loss 0.305700    Objective Loss 0.305700                                        LR 0.000036    Time 0.023844    
2023-01-06 16:59:49,379 - Epoch: [67][  110/  246]    Overall Loss 0.305884    Objective Loss 0.305884                                        LR 0.000036    Time 0.023774    
2023-01-06 16:59:49,607 - Epoch: [67][  120/  246]    Overall Loss 0.305794    Objective Loss 0.305794                                        LR 0.000036    Time 0.023693    
2023-01-06 16:59:49,834 - Epoch: [67][  130/  246]    Overall Loss 0.305227    Objective Loss 0.305227                                        LR 0.000036    Time 0.023608    
2023-01-06 16:59:50,055 - Epoch: [67][  140/  246]    Overall Loss 0.304665    Objective Loss 0.304665                                        LR 0.000036    Time 0.023502    
2023-01-06 16:59:50,287 - Epoch: [67][  150/  246]    Overall Loss 0.304515    Objective Loss 0.304515                                        LR 0.000036    Time 0.023479    
2023-01-06 16:59:50,516 - Epoch: [67][  160/  246]    Overall Loss 0.305478    Objective Loss 0.305478                                        LR 0.000036    Time 0.023424    
2023-01-06 16:59:50,753 - Epoch: [67][  170/  246]    Overall Loss 0.304873    Objective Loss 0.304873                                        LR 0.000036    Time 0.023437    
2023-01-06 16:59:50,992 - Epoch: [67][  180/  246]    Overall Loss 0.304931    Objective Loss 0.304931                                        LR 0.000036    Time 0.023452    
2023-01-06 16:59:51,233 - Epoch: [67][  190/  246]    Overall Loss 0.304491    Objective Loss 0.304491                                        LR 0.000036    Time 0.023484    
2023-01-06 16:59:51,460 - Epoch: [67][  200/  246]    Overall Loss 0.304710    Objective Loss 0.304710                                        LR 0.000036    Time 0.023444    
2023-01-06 16:59:51,698 - Epoch: [67][  210/  246]    Overall Loss 0.304912    Objective Loss 0.304912                                        LR 0.000036    Time 0.023460    
2023-01-06 16:59:51,923 - Epoch: [67][  220/  246]    Overall Loss 0.305625    Objective Loss 0.305625                                        LR 0.000036    Time 0.023410    
2023-01-06 16:59:52,152 - Epoch: [67][  230/  246]    Overall Loss 0.305862    Objective Loss 0.305862                                        LR 0.000036    Time 0.023386    
2023-01-06 16:59:52,360 - Epoch: [67][  240/  246]    Overall Loss 0.306410    Objective Loss 0.306410                                        LR 0.000036    Time 0.023279    
2023-01-06 16:59:52,456 - Epoch: [67][  246/  246]    Overall Loss 0.306742    Objective Loss 0.306742    Top1 88.995215    LR 0.000036    Time 0.023101    
2023-01-06 16:59:52,597 - --- validate (epoch=67)-----------
2023-01-06 16:59:52,597 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:53,055 - Epoch: [67][   10/   28]    Loss 0.310893    Top1 89.179688    
2023-01-06 16:59:53,169 - Epoch: [67][   20/   28]    Loss 0.306166    Top1 89.140625    
2023-01-06 16:59:53,238 - Epoch: [67][   28/   28]    Loss 0.303656    Top1 89.292871    
2023-01-06 16:59:53,395 - ==> Top1: 89.293    Loss: 0.304

2023-01-06 16:59:53,395 - ==> Confusion:
[[ 186   10  243]
 [  11  193  398]
 [  43   43 5859]]

2023-01-06 16:59:53,396 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:53,396 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:53,402 - 

2023-01-06 16:59:53,402 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:54,107 - Epoch: [68][   10/  246]    Overall Loss 0.284461    Objective Loss 0.284461                                        LR 0.000036    Time 0.070416    
2023-01-06 16:59:54,272 - Epoch: [68][   20/  246]    Overall Loss 0.285870    Objective Loss 0.285870                                        LR 0.000036    Time 0.043405    
2023-01-06 16:59:54,430 - Epoch: [68][   30/  246]    Overall Loss 0.296658    Objective Loss 0.296658                                        LR 0.000036    Time 0.034211    
2023-01-06 16:59:54,589 - Epoch: [68][   40/  246]    Overall Loss 0.299033    Objective Loss 0.299033                                        LR 0.000036    Time 0.029628    
2023-01-06 16:59:54,755 - Epoch: [68][   50/  246]    Overall Loss 0.298901    Objective Loss 0.298901                                        LR 0.000036    Time 0.026998    
2023-01-06 16:59:54,915 - Epoch: [68][   60/  246]    Overall Loss 0.298365    Objective Loss 0.298365                                        LR 0.000036    Time 0.025165    
2023-01-06 16:59:55,075 - Epoch: [68][   70/  246]    Overall Loss 0.297065    Objective Loss 0.297065                                        LR 0.000036    Time 0.023850    
2023-01-06 16:59:55,235 - Epoch: [68][   80/  246]    Overall Loss 0.299210    Objective Loss 0.299210                                        LR 0.000036    Time 0.022866    
2023-01-06 16:59:55,395 - Epoch: [68][   90/  246]    Overall Loss 0.299929    Objective Loss 0.299929                                        LR 0.000036    Time 0.022101    
2023-01-06 16:59:55,557 - Epoch: [68][  100/  246]    Overall Loss 0.300560    Objective Loss 0.300560                                        LR 0.000036    Time 0.021509    
2023-01-06 16:59:55,717 - Epoch: [68][  110/  246]    Overall Loss 0.300982    Objective Loss 0.300982                                        LR 0.000036    Time 0.021004    
2023-01-06 16:59:55,877 - Epoch: [68][  120/  246]    Overall Loss 0.300685    Objective Loss 0.300685                                        LR 0.000036    Time 0.020583    
2023-01-06 16:59:56,037 - Epoch: [68][  130/  246]    Overall Loss 0.300528    Objective Loss 0.300528                                        LR 0.000036    Time 0.020223    
2023-01-06 16:59:56,196 - Epoch: [68][  140/  246]    Overall Loss 0.301866    Objective Loss 0.301866                                        LR 0.000036    Time 0.019918    
2023-01-06 16:59:56,356 - Epoch: [68][  150/  246]    Overall Loss 0.302032    Objective Loss 0.302032                                        LR 0.000036    Time 0.019652    
2023-01-06 16:59:56,516 - Epoch: [68][  160/  246]    Overall Loss 0.304024    Objective Loss 0.304024                                        LR 0.000036    Time 0.019420    
2023-01-06 16:59:56,677 - Epoch: [68][  170/  246]    Overall Loss 0.304811    Objective Loss 0.304811                                        LR 0.000036    Time 0.019222    
2023-01-06 16:59:56,836 - Epoch: [68][  180/  246]    Overall Loss 0.304815    Objective Loss 0.304815                                        LR 0.000036    Time 0.019039    
2023-01-06 16:59:56,996 - Epoch: [68][  190/  246]    Overall Loss 0.304013    Objective Loss 0.304013                                        LR 0.000036    Time 0.018876    
2023-01-06 16:59:57,155 - Epoch: [68][  200/  246]    Overall Loss 0.303541    Objective Loss 0.303541                                        LR 0.000036    Time 0.018727    
2023-01-06 16:59:57,315 - Epoch: [68][  210/  246]    Overall Loss 0.304830    Objective Loss 0.304830                                        LR 0.000036    Time 0.018595    
2023-01-06 16:59:57,476 - Epoch: [68][  220/  246]    Overall Loss 0.304412    Objective Loss 0.304412                                        LR 0.000036    Time 0.018477    
2023-01-06 16:59:57,637 - Epoch: [68][  230/  246]    Overall Loss 0.304383    Objective Loss 0.304383                                        LR 0.000036    Time 0.018372    
2023-01-06 16:59:57,810 - Epoch: [68][  240/  246]    Overall Loss 0.304224    Objective Loss 0.304224                                        LR 0.000036    Time 0.018326    
2023-01-06 16:59:57,899 - Epoch: [68][  246/  246]    Overall Loss 0.304794    Objective Loss 0.304794    Top1 90.430622    LR 0.000036    Time 0.018240    
2023-01-06 16:59:58,024 - --- validate (epoch=68)-----------
2023-01-06 16:59:58,024 - 6986 samples (256 per mini-batch)
2023-01-06 16:59:58,473 - Epoch: [68][   10/   28]    Loss 0.311377    Top1 89.257812    
2023-01-06 16:59:58,592 - Epoch: [68][   20/   28]    Loss 0.308846    Top1 89.355469    
2023-01-06 16:59:58,659 - Epoch: [68][   28/   28]    Loss 0.302750    Top1 89.407386    
2023-01-06 16:59:58,802 - ==> Top1: 89.407    Loss: 0.303

2023-01-06 16:59:58,802 - ==> Confusion:
[[ 211    7  221]
 [  16  192  394]
 [  63   39 5843]]

2023-01-06 16:59:58,803 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 16:59:58,803 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 16:59:58,809 - 

2023-01-06 16:59:58,809 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:59:59,509 - Epoch: [69][   10/  246]    Overall Loss 0.306435    Objective Loss 0.306435                                        LR 0.000036    Time 0.069902    
2023-01-06 16:59:59,676 - Epoch: [69][   20/  246]    Overall Loss 0.307785    Objective Loss 0.307785                                        LR 0.000036    Time 0.043286    
2023-01-06 16:59:59,843 - Epoch: [69][   30/  246]    Overall Loss 0.311600    Objective Loss 0.311600                                        LR 0.000036    Time 0.034408    
2023-01-06 17:00:00,013 - Epoch: [69][   40/  246]    Overall Loss 0.309251    Objective Loss 0.309251                                        LR 0.000036    Time 0.030041    
2023-01-06 17:00:00,185 - Epoch: [69][   50/  246]    Overall Loss 0.305494    Objective Loss 0.305494                                        LR 0.000036    Time 0.027483    
2023-01-06 17:00:00,360 - Epoch: [69][   60/  246]    Overall Loss 0.306370    Objective Loss 0.306370                                        LR 0.000036    Time 0.025807    
2023-01-06 17:00:00,529 - Epoch: [69][   70/  246]    Overall Loss 0.306822    Objective Loss 0.306822                                        LR 0.000036    Time 0.024525    
2023-01-06 17:00:00,703 - Epoch: [69][   80/  246]    Overall Loss 0.305961    Objective Loss 0.305961                                        LR 0.000036    Time 0.023636    
2023-01-06 17:00:00,876 - Epoch: [69][   90/  246]    Overall Loss 0.307173    Objective Loss 0.307173                                        LR 0.000036    Time 0.022928    
2023-01-06 17:00:01,046 - Epoch: [69][  100/  246]    Overall Loss 0.305181    Objective Loss 0.305181                                        LR 0.000036    Time 0.022332    
2023-01-06 17:00:01,224 - Epoch: [69][  110/  246]    Overall Loss 0.304541    Objective Loss 0.304541                                        LR 0.000036    Time 0.021914    
2023-01-06 17:00:01,394 - Epoch: [69][  120/  246]    Overall Loss 0.304212    Objective Loss 0.304212                                        LR 0.000036    Time 0.021500    
2023-01-06 17:00:01,562 - Epoch: [69][  130/  246]    Overall Loss 0.305882    Objective Loss 0.305882                                        LR 0.000036    Time 0.021141    
2023-01-06 17:00:01,739 - Epoch: [69][  140/  246]    Overall Loss 0.304358    Objective Loss 0.304358                                        LR 0.000036    Time 0.020892    
2023-01-06 17:00:01,910 - Epoch: [69][  150/  246]    Overall Loss 0.305540    Objective Loss 0.305540                                        LR 0.000036    Time 0.020633    
2023-01-06 17:00:02,081 - Epoch: [69][  160/  246]    Overall Loss 0.304668    Objective Loss 0.304668                                        LR 0.000036    Time 0.020415    
2023-01-06 17:00:02,254 - Epoch: [69][  170/  246]    Overall Loss 0.303790    Objective Loss 0.303790                                        LR 0.000036    Time 0.020229    
2023-01-06 17:00:02,425 - Epoch: [69][  180/  246]    Overall Loss 0.304840    Objective Loss 0.304840                                        LR 0.000036    Time 0.020052    
2023-01-06 17:00:02,592 - Epoch: [69][  190/  246]    Overall Loss 0.304932    Objective Loss 0.304932                                        LR 0.000036    Time 0.019873    
2023-01-06 17:00:02,758 - Epoch: [69][  200/  246]    Overall Loss 0.304365    Objective Loss 0.304365                                        LR 0.000036    Time 0.019711    
2023-01-06 17:00:02,925 - Epoch: [69][  210/  246]    Overall Loss 0.303742    Objective Loss 0.303742                                        LR 0.000036    Time 0.019564    
2023-01-06 17:00:03,092 - Epoch: [69][  220/  246]    Overall Loss 0.302826    Objective Loss 0.302826                                        LR 0.000036    Time 0.019432    
2023-01-06 17:00:03,259 - Epoch: [69][  230/  246]    Overall Loss 0.301928    Objective Loss 0.301928                                        LR 0.000036    Time 0.019313    
2023-01-06 17:00:03,450 - Epoch: [69][  240/  246]    Overall Loss 0.302837    Objective Loss 0.302837                                        LR 0.000036    Time 0.019303    
2023-01-06 17:00:03,544 - Epoch: [69][  246/  246]    Overall Loss 0.302825    Objective Loss 0.302825    Top1 88.277512    LR 0.000036    Time 0.019212    
2023-01-06 17:00:03,686 - --- validate (epoch=69)-----------
2023-01-06 17:00:03,687 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:04,141 - Epoch: [69][   10/   28]    Loss 0.314252    Top1 87.968750    
2023-01-06 17:00:04,255 - Epoch: [69][   20/   28]    Loss 0.309462    Top1 88.476562    
2023-01-06 17:00:04,323 - Epoch: [69][   28/   28]    Loss 0.313540    Top1 88.376754    
2023-01-06 17:00:04,465 - ==> Top1: 88.377    Loss: 0.314

2023-01-06 17:00:04,465 - ==> Confusion:
[[ 137    5  297]
 [   7  146  449]
 [  20   34 5891]]

2023-01-06 17:00:04,467 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 17:00:04,467 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:04,473 - 

2023-01-06 17:00:04,473 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:05,016 - Epoch: [70][   10/  246]    Overall Loss 0.294178    Objective Loss 0.294178                                        LR 0.000022    Time 0.054226    
2023-01-06 17:00:05,175 - Epoch: [70][   20/  246]    Overall Loss 0.303872    Objective Loss 0.303872                                        LR 0.000022    Time 0.035026    
2023-01-06 17:00:05,339 - Epoch: [70][   30/  246]    Overall Loss 0.304312    Objective Loss 0.304312                                        LR 0.000022    Time 0.028804    
2023-01-06 17:00:05,511 - Epoch: [70][   40/  246]    Overall Loss 0.301884    Objective Loss 0.301884                                        LR 0.000022    Time 0.025913    
2023-01-06 17:00:05,683 - Epoch: [70][   50/  246]    Overall Loss 0.305641    Objective Loss 0.305641                                        LR 0.000022    Time 0.024152    
2023-01-06 17:00:05,853 - Epoch: [70][   60/  246]    Overall Loss 0.305974    Objective Loss 0.305974                                        LR 0.000022    Time 0.022967    
2023-01-06 17:00:06,024 - Epoch: [70][   70/  246]    Overall Loss 0.306259    Objective Loss 0.306259                                        LR 0.000022    Time 0.022124    
2023-01-06 17:00:06,193 - Epoch: [70][   80/  246]    Overall Loss 0.305168    Objective Loss 0.305168                                        LR 0.000022    Time 0.021461    
2023-01-06 17:00:06,363 - Epoch: [70][   90/  246]    Overall Loss 0.303289    Objective Loss 0.303289                                        LR 0.000022    Time 0.020957    
2023-01-06 17:00:06,530 - Epoch: [70][  100/  246]    Overall Loss 0.303434    Objective Loss 0.303434                                        LR 0.000022    Time 0.020534    
2023-01-06 17:00:06,699 - Epoch: [70][  110/  246]    Overall Loss 0.301761    Objective Loss 0.301761                                        LR 0.000022    Time 0.020202    
2023-01-06 17:00:06,866 - Epoch: [70][  120/  246]    Overall Loss 0.301259    Objective Loss 0.301259                                        LR 0.000022    Time 0.019902    
2023-01-06 17:00:07,037 - Epoch: [70][  130/  246]    Overall Loss 0.301439    Objective Loss 0.301439                                        LR 0.000022    Time 0.019688    
2023-01-06 17:00:07,212 - Epoch: [70][  140/  246]    Overall Loss 0.300280    Objective Loss 0.300280                                        LR 0.000022    Time 0.019523    
2023-01-06 17:00:07,396 - Epoch: [70][  150/  246]    Overall Loss 0.300221    Objective Loss 0.300221                                        LR 0.000022    Time 0.019448    
2023-01-06 17:00:07,578 - Epoch: [70][  160/  246]    Overall Loss 0.299780    Objective Loss 0.299780                                        LR 0.000022    Time 0.019371    
2023-01-06 17:00:07,759 - Epoch: [70][  170/  246]    Overall Loss 0.299604    Objective Loss 0.299604                                        LR 0.000022    Time 0.019293    
2023-01-06 17:00:07,931 - Epoch: [70][  180/  246]    Overall Loss 0.298883    Objective Loss 0.298883                                        LR 0.000022    Time 0.019175    
2023-01-06 17:00:08,122 - Epoch: [70][  190/  246]    Overall Loss 0.298901    Objective Loss 0.298901                                        LR 0.000022    Time 0.019167    
2023-01-06 17:00:08,308 - Epoch: [70][  200/  246]    Overall Loss 0.299139    Objective Loss 0.299139                                        LR 0.000022    Time 0.019139    
2023-01-06 17:00:08,495 - Epoch: [70][  210/  246]    Overall Loss 0.299693    Objective Loss 0.299693                                        LR 0.000022    Time 0.019115    
2023-01-06 17:00:08,683 - Epoch: [70][  220/  246]    Overall Loss 0.299855    Objective Loss 0.299855                                        LR 0.000022    Time 0.019098    
2023-01-06 17:00:08,869 - Epoch: [70][  230/  246]    Overall Loss 0.300300    Objective Loss 0.300300                                        LR 0.000022    Time 0.019076    
2023-01-06 17:00:09,064 - Epoch: [70][  240/  246]    Overall Loss 0.300363    Objective Loss 0.300363                                        LR 0.000022    Time 0.019092    
2023-01-06 17:00:09,158 - Epoch: [70][  246/  246]    Overall Loss 0.300013    Objective Loss 0.300013    Top1 88.516746    LR 0.000022    Time 0.019007    
2023-01-06 17:00:09,304 - --- validate (epoch=70)-----------
2023-01-06 17:00:09,304 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:09,749 - Epoch: [70][   10/   28]    Loss 0.294814    Top1 89.179688    
2023-01-06 17:00:09,861 - Epoch: [70][   20/   28]    Loss 0.310275    Top1 88.730469    
2023-01-06 17:00:09,929 - Epoch: [70][   28/   28]    Loss 0.300811    Top1 89.020899    
2023-01-06 17:00:10,062 - ==> Top1: 89.021    Loss: 0.301

2023-01-06 17:00:10,062 - ==> Confusion:
[[ 176   10  253]
 [  11  196  395]
 [  44   54 5847]]

2023-01-06 17:00:10,063 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 17:00:10,063 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:10,069 - 

2023-01-06 17:00:10,070 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:10,758 - Epoch: [71][   10/  246]    Overall Loss 0.290889    Objective Loss 0.290889                                        LR 0.000022    Time 0.068763    
2023-01-06 17:00:10,925 - Epoch: [71][   20/  246]    Overall Loss 0.296255    Objective Loss 0.296255                                        LR 0.000022    Time 0.042689    
2023-01-06 17:00:11,098 - Epoch: [71][   30/  246]    Overall Loss 0.295132    Objective Loss 0.295132                                        LR 0.000022    Time 0.034201    
2023-01-06 17:00:11,274 - Epoch: [71][   40/  246]    Overall Loss 0.293518    Objective Loss 0.293518                                        LR 0.000022    Time 0.030050    
2023-01-06 17:00:11,442 - Epoch: [71][   50/  246]    Overall Loss 0.296046    Objective Loss 0.296046                                        LR 0.000022    Time 0.027386    
2023-01-06 17:00:11,608 - Epoch: [71][   60/  246]    Overall Loss 0.296869    Objective Loss 0.296869                                        LR 0.000022    Time 0.025576    
2023-01-06 17:00:11,776 - Epoch: [71][   70/  246]    Overall Loss 0.300278    Objective Loss 0.300278                                        LR 0.000022    Time 0.024328    
2023-01-06 17:00:11,948 - Epoch: [71][   80/  246]    Overall Loss 0.300676    Objective Loss 0.300676                                        LR 0.000022    Time 0.023421    
2023-01-06 17:00:12,135 - Epoch: [71][   90/  246]    Overall Loss 0.299961    Objective Loss 0.299961                                        LR 0.000022    Time 0.022896    
2023-01-06 17:00:12,326 - Epoch: [71][  100/  246]    Overall Loss 0.301088    Objective Loss 0.301088                                        LR 0.000022    Time 0.022510    
2023-01-06 17:00:12,516 - Epoch: [71][  110/  246]    Overall Loss 0.301278    Objective Loss 0.301278                                        LR 0.000022    Time 0.022192    
2023-01-06 17:00:12,703 - Epoch: [71][  120/  246]    Overall Loss 0.301527    Objective Loss 0.301527                                        LR 0.000022    Time 0.021897    
2023-01-06 17:00:12,893 - Epoch: [71][  130/  246]    Overall Loss 0.301458    Objective Loss 0.301458                                        LR 0.000022    Time 0.021675    
2023-01-06 17:00:13,091 - Epoch: [71][  140/  246]    Overall Loss 0.300445    Objective Loss 0.300445                                        LR 0.000022    Time 0.021537    
2023-01-06 17:00:13,286 - Epoch: [71][  150/  246]    Overall Loss 0.300556    Objective Loss 0.300556                                        LR 0.000022    Time 0.021395    
2023-01-06 17:00:13,487 - Epoch: [71][  160/  246]    Overall Loss 0.301121    Objective Loss 0.301121                                        LR 0.000022    Time 0.021312    
2023-01-06 17:00:13,681 - Epoch: [71][  170/  246]    Overall Loss 0.301100    Objective Loss 0.301100                                        LR 0.000022    Time 0.021199    
2023-01-06 17:00:13,881 - Epoch: [71][  180/  246]    Overall Loss 0.300828    Objective Loss 0.300828                                        LR 0.000022    Time 0.021132    
2023-01-06 17:00:14,074 - Epoch: [71][  190/  246]    Overall Loss 0.299155    Objective Loss 0.299155                                        LR 0.000022    Time 0.021032    
2023-01-06 17:00:14,262 - Epoch: [71][  200/  246]    Overall Loss 0.298548    Objective Loss 0.298548                                        LR 0.000022    Time 0.020917    
2023-01-06 17:00:14,452 - Epoch: [71][  210/  246]    Overall Loss 0.299191    Objective Loss 0.299191                                        LR 0.000022    Time 0.020824    
2023-01-06 17:00:14,642 - Epoch: [71][  220/  246]    Overall Loss 0.299027    Objective Loss 0.299027                                        LR 0.000022    Time 0.020743    
2023-01-06 17:00:14,832 - Epoch: [71][  230/  246]    Overall Loss 0.298333    Objective Loss 0.298333                                        LR 0.000022    Time 0.020665    
2023-01-06 17:00:15,036 - Epoch: [71][  240/  246]    Overall Loss 0.297821    Objective Loss 0.297821                                        LR 0.000022    Time 0.020653    
2023-01-06 17:00:15,132 - Epoch: [71][  246/  246]    Overall Loss 0.297803    Objective Loss 0.297803    Top1 88.755981    LR 0.000022    Time 0.020538    
2023-01-06 17:00:15,280 - --- validate (epoch=71)-----------
2023-01-06 17:00:15,280 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:15,737 - Epoch: [71][   10/   28]    Loss 0.312110    Top1 88.398438    
2023-01-06 17:00:15,854 - Epoch: [71][   20/   28]    Loss 0.304447    Top1 88.789062    
2023-01-06 17:00:15,921 - Epoch: [71][   28/   28]    Loss 0.300321    Top1 89.049528    
2023-01-06 17:00:16,072 - ==> Top1: 89.050    Loss: 0.300

2023-01-06 17:00:16,073 - ==> Confusion:
[[ 174    6  259]
 [  10  173  419]
 [  38   33 5874]]

2023-01-06 17:00:16,074 - ==> Best [Top1: 89.450   Sparsity:0.00   Params: 155168 on epoch: 57]
2023-01-06 17:00:16,074 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:16,080 - 

2023-01-06 17:00:16,080 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:16,634 - Epoch: [72][   10/  246]    Overall Loss 0.306307    Objective Loss 0.306307                                        LR 0.000022    Time 0.055367    
2023-01-06 17:00:16,820 - Epoch: [72][   20/  246]    Overall Loss 0.315532    Objective Loss 0.315532                                        LR 0.000022    Time 0.036930    
2023-01-06 17:00:17,018 - Epoch: [72][   30/  246]    Overall Loss 0.311960    Objective Loss 0.311960                                        LR 0.000022    Time 0.031212    
2023-01-06 17:00:17,221 - Epoch: [72][   40/  246]    Overall Loss 0.306926    Objective Loss 0.306926                                        LR 0.000022    Time 0.028477    
2023-01-06 17:00:17,416 - Epoch: [72][   50/  246]    Overall Loss 0.301799    Objective Loss 0.301799                                        LR 0.000022    Time 0.026679    
2023-01-06 17:00:17,603 - Epoch: [72][   60/  246]    Overall Loss 0.304459    Objective Loss 0.304459                                        LR 0.000022    Time 0.025346    
2023-01-06 17:00:17,787 - Epoch: [72][   70/  246]    Overall Loss 0.301500    Objective Loss 0.301500                                        LR 0.000022    Time 0.024336    
2023-01-06 17:00:17,969 - Epoch: [72][   80/  246]    Overall Loss 0.300277    Objective Loss 0.300277                                        LR 0.000022    Time 0.023571    
2023-01-06 17:00:18,146 - Epoch: [72][   90/  246]    Overall Loss 0.300348    Objective Loss 0.300348                                        LR 0.000022    Time 0.022916    
2023-01-06 17:00:18,334 - Epoch: [72][  100/  246]    Overall Loss 0.298691    Objective Loss 0.298691                                        LR 0.000022    Time 0.022498    
2023-01-06 17:00:18,519 - Epoch: [72][  110/  246]    Overall Loss 0.297421    Objective Loss 0.297421                                        LR 0.000022    Time 0.022134    
2023-01-06 17:00:18,710 - Epoch: [72][  120/  246]    Overall Loss 0.299195    Objective Loss 0.299195                                        LR 0.000022    Time 0.021875    
2023-01-06 17:00:18,891 - Epoch: [72][  130/  246]    Overall Loss 0.297864    Objective Loss 0.297864                                        LR 0.000022    Time 0.021582    
2023-01-06 17:00:19,079 - Epoch: [72][  140/  246]    Overall Loss 0.298342    Objective Loss 0.298342                                        LR 0.000022    Time 0.021385    
2023-01-06 17:00:19,258 - Epoch: [72][  150/  246]    Overall Loss 0.299836    Objective Loss 0.299836                                        LR 0.000022    Time 0.021149    
2023-01-06 17:00:19,444 - Epoch: [72][  160/  246]    Overall Loss 0.299205    Objective Loss 0.299205                                        LR 0.000022    Time 0.020984    
2023-01-06 17:00:19,627 - Epoch: [72][  170/  246]    Overall Loss 0.299042    Objective Loss 0.299042                                        LR 0.000022    Time 0.020828    
2023-01-06 17:00:19,813 - Epoch: [72][  180/  246]    Overall Loss 0.298616    Objective Loss 0.298616                                        LR 0.000022    Time 0.020700    
2023-01-06 17:00:20,002 - Epoch: [72][  190/  246]    Overall Loss 0.298327    Objective Loss 0.298327                                        LR 0.000022    Time 0.020605    
2023-01-06 17:00:20,198 - Epoch: [72][  200/  246]    Overall Loss 0.298170    Objective Loss 0.298170                                        LR 0.000022    Time 0.020549    
2023-01-06 17:00:20,421 - Epoch: [72][  210/  246]    Overall Loss 0.297867    Objective Loss 0.297867                                        LR 0.000022    Time 0.020633    
2023-01-06 17:00:20,664 - Epoch: [72][  220/  246]    Overall Loss 0.298459    Objective Loss 0.298459                                        LR 0.000022    Time 0.020796    
2023-01-06 17:00:20,913 - Epoch: [72][  230/  246]    Overall Loss 0.298265    Objective Loss 0.298265                                        LR 0.000022    Time 0.020971    
2023-01-06 17:00:21,162 - Epoch: [72][  240/  246]    Overall Loss 0.298130    Objective Loss 0.298130                                        LR 0.000022    Time 0.021135    
2023-01-06 17:00:21,273 - Epoch: [72][  246/  246]    Overall Loss 0.298152    Objective Loss 0.298152    Top1 86.842105    LR 0.000022    Time 0.021070    
2023-01-06 17:00:21,453 - --- validate (epoch=72)-----------
2023-01-06 17:00:21,453 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:21,914 - Epoch: [72][   10/   28]    Loss 0.282673    Top1 90.351562    
2023-01-06 17:00:22,027 - Epoch: [72][   20/   28]    Loss 0.303503    Top1 89.355469    
2023-01-06 17:00:22,096 - Epoch: [72][   28/   28]    Loss 0.297267    Top1 89.564844    
2023-01-06 17:00:22,221 - ==> Top1: 89.565    Loss: 0.297

2023-01-06 17:00:22,222 - ==> Confusion:
[[ 211    7  221]
 [  18  206  378]
 [  56   49 5840]]

2023-01-06 17:00:22,223 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:22,223 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:22,230 - 

2023-01-06 17:00:22,230 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:22,949 - Epoch: [73][   10/  246]    Overall Loss 0.290117    Objective Loss 0.290117                                        LR 0.000022    Time 0.071801    
2023-01-06 17:00:23,146 - Epoch: [73][   20/  246]    Overall Loss 0.295875    Objective Loss 0.295875                                        LR 0.000022    Time 0.045734    
2023-01-06 17:00:23,334 - Epoch: [73][   30/  246]    Overall Loss 0.298055    Objective Loss 0.298055                                        LR 0.000022    Time 0.036719    
2023-01-06 17:00:23,496 - Epoch: [73][   40/  246]    Overall Loss 0.297477    Objective Loss 0.297477                                        LR 0.000022    Time 0.031592    
2023-01-06 17:00:23,655 - Epoch: [73][   50/  246]    Overall Loss 0.297581    Objective Loss 0.297581                                        LR 0.000022    Time 0.028437    
2023-01-06 17:00:23,825 - Epoch: [73][   60/  246]    Overall Loss 0.299404    Objective Loss 0.299404                                        LR 0.000022    Time 0.026536    
2023-01-06 17:00:24,019 - Epoch: [73][   70/  246]    Overall Loss 0.297688    Objective Loss 0.297688                                        LR 0.000022    Time 0.025505    
2023-01-06 17:00:24,201 - Epoch: [73][   80/  246]    Overall Loss 0.301161    Objective Loss 0.301161                                        LR 0.000022    Time 0.024590    
2023-01-06 17:00:24,377 - Epoch: [73][   90/  246]    Overall Loss 0.299101    Objective Loss 0.299101                                        LR 0.000022    Time 0.023806    
2023-01-06 17:00:24,556 - Epoch: [73][  100/  246]    Overall Loss 0.299376    Objective Loss 0.299376                                        LR 0.000022    Time 0.023211    
2023-01-06 17:00:24,738 - Epoch: [73][  110/  246]    Overall Loss 0.300245    Objective Loss 0.300245                                        LR 0.000022    Time 0.022753    
2023-01-06 17:00:24,914 - Epoch: [73][  120/  246]    Overall Loss 0.300525    Objective Loss 0.300525                                        LR 0.000022    Time 0.022319    
2023-01-06 17:00:25,078 - Epoch: [73][  130/  246]    Overall Loss 0.300251    Objective Loss 0.300251                                        LR 0.000022    Time 0.021865    
2023-01-06 17:00:25,245 - Epoch: [73][  140/  246]    Overall Loss 0.299716    Objective Loss 0.299716                                        LR 0.000022    Time 0.021492    
2023-01-06 17:00:25,410 - Epoch: [73][  150/  246]    Overall Loss 0.299740    Objective Loss 0.299740                                        LR 0.000022    Time 0.021160    
2023-01-06 17:00:25,574 - Epoch: [73][  160/  246]    Overall Loss 0.298619    Objective Loss 0.298619                                        LR 0.000022    Time 0.020862    
2023-01-06 17:00:25,741 - Epoch: [73][  170/  246]    Overall Loss 0.298145    Objective Loss 0.298145                                        LR 0.000022    Time 0.020611    
2023-01-06 17:00:25,908 - Epoch: [73][  180/  246]    Overall Loss 0.298157    Objective Loss 0.298157                                        LR 0.000022    Time 0.020391    
2023-01-06 17:00:26,075 - Epoch: [73][  190/  246]    Overall Loss 0.296909    Objective Loss 0.296909                                        LR 0.000022    Time 0.020199    
2023-01-06 17:00:26,253 - Epoch: [73][  200/  246]    Overall Loss 0.297495    Objective Loss 0.297495                                        LR 0.000022    Time 0.020073    
2023-01-06 17:00:26,426 - Epoch: [73][  210/  246]    Overall Loss 0.296730    Objective Loss 0.296730                                        LR 0.000022    Time 0.019941    
2023-01-06 17:00:26,599 - Epoch: [73][  220/  246]    Overall Loss 0.296764    Objective Loss 0.296764                                        LR 0.000022    Time 0.019818    
2023-01-06 17:00:26,774 - Epoch: [73][  230/  246]    Overall Loss 0.296562    Objective Loss 0.296562                                        LR 0.000022    Time 0.019719    
2023-01-06 17:00:26,960 - Epoch: [73][  240/  246]    Overall Loss 0.296193    Objective Loss 0.296193                                        LR 0.000022    Time 0.019671    
2023-01-06 17:00:27,050 - Epoch: [73][  246/  246]    Overall Loss 0.296713    Objective Loss 0.296713    Top1 87.559809    LR 0.000022    Time 0.019553    
2023-01-06 17:00:27,193 - --- validate (epoch=73)-----------
2023-01-06 17:00:27,193 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:27,642 - Epoch: [73][   10/   28]    Loss 0.284467    Top1 90.000000    
2023-01-06 17:00:27,757 - Epoch: [73][   20/   28]    Loss 0.293550    Top1 89.550781    
2023-01-06 17:00:27,825 - Epoch: [73][   28/   28]    Loss 0.299055    Top1 89.407386    
2023-01-06 17:00:27,942 - ==> Top1: 89.407    Loss: 0.299

2023-01-06 17:00:27,942 - ==> Confusion:
[[ 198   10  231]
 [  15  224  363]
 [  52   69 5824]]

2023-01-06 17:00:27,943 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:27,943 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:27,949 - 

2023-01-06 17:00:27,949 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:28,645 - Epoch: [74][   10/  246]    Overall Loss 0.304042    Objective Loss 0.304042                                        LR 0.000022    Time 0.069469    
2023-01-06 17:00:28,819 - Epoch: [74][   20/  246]    Overall Loss 0.297529    Objective Loss 0.297529                                        LR 0.000022    Time 0.043432    
2023-01-06 17:00:28,993 - Epoch: [74][   30/  246]    Overall Loss 0.296631    Objective Loss 0.296631                                        LR 0.000022    Time 0.034721    
2023-01-06 17:00:29,170 - Epoch: [74][   40/  246]    Overall Loss 0.297822    Objective Loss 0.297822                                        LR 0.000022    Time 0.030457    
2023-01-06 17:00:29,352 - Epoch: [74][   50/  246]    Overall Loss 0.299047    Objective Loss 0.299047                                        LR 0.000022    Time 0.027999    
2023-01-06 17:00:29,551 - Epoch: [74][   60/  246]    Overall Loss 0.298457    Objective Loss 0.298457                                        LR 0.000022    Time 0.026652    
2023-01-06 17:00:29,744 - Epoch: [74][   70/  246]    Overall Loss 0.296162    Objective Loss 0.296162                                        LR 0.000022    Time 0.025599    
2023-01-06 17:00:29,941 - Epoch: [74][   80/  246]    Overall Loss 0.294200    Objective Loss 0.294200                                        LR 0.000022    Time 0.024854    
2023-01-06 17:00:30,134 - Epoch: [74][   90/  246]    Overall Loss 0.295611    Objective Loss 0.295611                                        LR 0.000022    Time 0.024236    
2023-01-06 17:00:30,324 - Epoch: [74][  100/  246]    Overall Loss 0.293845    Objective Loss 0.293845                                        LR 0.000022    Time 0.023701    
2023-01-06 17:00:30,513 - Epoch: [74][  110/  246]    Overall Loss 0.294009    Objective Loss 0.294009                                        LR 0.000022    Time 0.023263    
2023-01-06 17:00:30,703 - Epoch: [74][  120/  246]    Overall Loss 0.296216    Objective Loss 0.296216                                        LR 0.000022    Time 0.022909    
2023-01-06 17:00:30,890 - Epoch: [74][  130/  246]    Overall Loss 0.295477    Objective Loss 0.295477                                        LR 0.000022    Time 0.022578    
2023-01-06 17:00:31,082 - Epoch: [74][  140/  246]    Overall Loss 0.294987    Objective Loss 0.294987                                        LR 0.000022    Time 0.022317    
2023-01-06 17:00:31,271 - Epoch: [74][  150/  246]    Overall Loss 0.295091    Objective Loss 0.295091                                        LR 0.000022    Time 0.022089    
2023-01-06 17:00:31,461 - Epoch: [74][  160/  246]    Overall Loss 0.293623    Objective Loss 0.293623                                        LR 0.000022    Time 0.021890    
2023-01-06 17:00:31,650 - Epoch: [74][  170/  246]    Overall Loss 0.293469    Objective Loss 0.293469                                        LR 0.000022    Time 0.021715    
2023-01-06 17:00:31,840 - Epoch: [74][  180/  246]    Overall Loss 0.293000    Objective Loss 0.293000                                        LR 0.000022    Time 0.021560    
2023-01-06 17:00:32,029 - Epoch: [74][  190/  246]    Overall Loss 0.294101    Objective Loss 0.294101                                        LR 0.000022    Time 0.021420    
2023-01-06 17:00:32,218 - Epoch: [74][  200/  246]    Overall Loss 0.293798    Objective Loss 0.293798                                        LR 0.000022    Time 0.021289    
2023-01-06 17:00:32,406 - Epoch: [74][  210/  246]    Overall Loss 0.294646    Objective Loss 0.294646                                        LR 0.000022    Time 0.021168    
2023-01-06 17:00:32,596 - Epoch: [74][  220/  246]    Overall Loss 0.295570    Objective Loss 0.295570                                        LR 0.000022    Time 0.021068    
2023-01-06 17:00:32,783 - Epoch: [74][  230/  246]    Overall Loss 0.295630    Objective Loss 0.295630                                        LR 0.000022    Time 0.020964    
2023-01-06 17:00:32,985 - Epoch: [74][  240/  246]    Overall Loss 0.295660    Objective Loss 0.295660                                        LR 0.000022    Time 0.020932    
2023-01-06 17:00:33,084 - Epoch: [74][  246/  246]    Overall Loss 0.295513    Objective Loss 0.295513    Top1 90.191388    LR 0.000022    Time 0.020822    
2023-01-06 17:00:33,234 - --- validate (epoch=74)-----------
2023-01-06 17:00:33,234 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:33,692 - Epoch: [74][   10/   28]    Loss 0.288531    Top1 89.296875    
2023-01-06 17:00:33,808 - Epoch: [74][   20/   28]    Loss 0.288865    Top1 89.414062    
2023-01-06 17:00:33,875 - Epoch: [74][   28/   28]    Loss 0.294283    Top1 89.264243    
2023-01-06 17:00:34,016 - ==> Top1: 89.264    Loss: 0.294

2023-01-06 17:00:34,016 - ==> Confusion:
[[ 186   10  243]
 [  12  208  382]
 [  45   58 5842]]

2023-01-06 17:00:34,017 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:34,018 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:34,023 - 

2023-01-06 17:00:34,024 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:34,597 - Epoch: [75][   10/  246]    Overall Loss 0.293788    Objective Loss 0.293788                                        LR 0.000022    Time 0.057232    
2023-01-06 17:00:34,789 - Epoch: [75][   20/  246]    Overall Loss 0.286784    Objective Loss 0.286784                                        LR 0.000022    Time 0.038195    
2023-01-06 17:00:34,994 - Epoch: [75][   30/  246]    Overall Loss 0.293193    Objective Loss 0.293193                                        LR 0.000022    Time 0.032282    
2023-01-06 17:00:35,217 - Epoch: [75][   40/  246]    Overall Loss 0.292224    Objective Loss 0.292224                                        LR 0.000022    Time 0.029781    
2023-01-06 17:00:35,441 - Epoch: [75][   50/  246]    Overall Loss 0.297406    Objective Loss 0.297406                                        LR 0.000022    Time 0.028291    
2023-01-06 17:00:35,672 - Epoch: [75][   60/  246]    Overall Loss 0.297977    Objective Loss 0.297977                                        LR 0.000022    Time 0.027425    
2023-01-06 17:00:35,901 - Epoch: [75][   70/  246]    Overall Loss 0.296495    Objective Loss 0.296495                                        LR 0.000022    Time 0.026766    
2023-01-06 17:00:36,130 - Epoch: [75][   80/  246]    Overall Loss 0.296858    Objective Loss 0.296858                                        LR 0.000022    Time 0.026283    
2023-01-06 17:00:36,342 - Epoch: [75][   90/  246]    Overall Loss 0.296698    Objective Loss 0.296698                                        LR 0.000022    Time 0.025710    
2023-01-06 17:00:36,560 - Epoch: [75][  100/  246]    Overall Loss 0.297687    Objective Loss 0.297687                                        LR 0.000022    Time 0.025314    
2023-01-06 17:00:36,774 - Epoch: [75][  110/  246]    Overall Loss 0.298889    Objective Loss 0.298889                                        LR 0.000022    Time 0.024952    
2023-01-06 17:00:36,990 - Epoch: [75][  120/  246]    Overall Loss 0.299254    Objective Loss 0.299254                                        LR 0.000022    Time 0.024672    
2023-01-06 17:00:37,213 - Epoch: [75][  130/  246]    Overall Loss 0.297749    Objective Loss 0.297749                                        LR 0.000022    Time 0.024485    
2023-01-06 17:00:37,432 - Epoch: [75][  140/  246]    Overall Loss 0.296672    Objective Loss 0.296672                                        LR 0.000022    Time 0.024294    
2023-01-06 17:00:37,654 - Epoch: [75][  150/  246]    Overall Loss 0.294513    Objective Loss 0.294513                                        LR 0.000022    Time 0.024151    
2023-01-06 17:00:37,876 - Epoch: [75][  160/  246]    Overall Loss 0.295377    Objective Loss 0.295377                                        LR 0.000022    Time 0.024032    
2023-01-06 17:00:38,098 - Epoch: [75][  170/  246]    Overall Loss 0.296016    Objective Loss 0.296016                                        LR 0.000022    Time 0.023919    
2023-01-06 17:00:38,320 - Epoch: [75][  180/  246]    Overall Loss 0.296804    Objective Loss 0.296804                                        LR 0.000022    Time 0.023822    
2023-01-06 17:00:38,537 - Epoch: [75][  190/  246]    Overall Loss 0.296537    Objective Loss 0.296537                                        LR 0.000022    Time 0.023707    
2023-01-06 17:00:38,760 - Epoch: [75][  200/  246]    Overall Loss 0.296183    Objective Loss 0.296183                                        LR 0.000022    Time 0.023632    
2023-01-06 17:00:38,981 - Epoch: [75][  210/  246]    Overall Loss 0.296124    Objective Loss 0.296124                                        LR 0.000022    Time 0.023560    
2023-01-06 17:00:39,191 - Epoch: [75][  220/  246]    Overall Loss 0.295347    Objective Loss 0.295347                                        LR 0.000022    Time 0.023443    
2023-01-06 17:00:39,376 - Epoch: [75][  230/  246]    Overall Loss 0.295297    Objective Loss 0.295297                                        LR 0.000022    Time 0.023223    
2023-01-06 17:00:39,577 - Epoch: [75][  240/  246]    Overall Loss 0.294986    Objective Loss 0.294986                                        LR 0.000022    Time 0.023094    
2023-01-06 17:00:39,673 - Epoch: [75][  246/  246]    Overall Loss 0.294702    Objective Loss 0.294702    Top1 87.559809    LR 0.000022    Time 0.022921    
2023-01-06 17:00:39,806 - --- validate (epoch=75)-----------
2023-01-06 17:00:39,806 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:40,264 - Epoch: [75][   10/   28]    Loss 0.299721    Top1 89.101562    
2023-01-06 17:00:40,396 - Epoch: [75][   20/   28]    Loss 0.295621    Top1 89.472656    
2023-01-06 17:00:40,463 - Epoch: [75][   28/   28]    Loss 0.298709    Top1 89.478958    
2023-01-06 17:00:40,614 - ==> Top1: 89.479    Loss: 0.299

2023-01-06 17:00:40,615 - ==> Confusion:
[[ 213    7  219]
 [  19  206  377]
 [  60   53 5832]]

2023-01-06 17:00:40,616 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:40,616 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:40,622 - 

2023-01-06 17:00:40,622 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:41,332 - Epoch: [76][   10/  246]    Overall Loss 0.295946    Objective Loss 0.295946                                        LR 0.000022    Time 0.070910    
2023-01-06 17:00:41,513 - Epoch: [76][   20/  246]    Overall Loss 0.298927    Objective Loss 0.298927                                        LR 0.000022    Time 0.044483    
2023-01-06 17:00:41,687 - Epoch: [76][   30/  246]    Overall Loss 0.297616    Objective Loss 0.297616                                        LR 0.000022    Time 0.035440    
2023-01-06 17:00:41,887 - Epoch: [76][   40/  246]    Overall Loss 0.296227    Objective Loss 0.296227                                        LR 0.000022    Time 0.031571    
2023-01-06 17:00:42,084 - Epoch: [76][   50/  246]    Overall Loss 0.297884    Objective Loss 0.297884                                        LR 0.000022    Time 0.029187    
2023-01-06 17:00:42,290 - Epoch: [76][   60/  246]    Overall Loss 0.297033    Objective Loss 0.297033                                        LR 0.000022    Time 0.027757    
2023-01-06 17:00:42,497 - Epoch: [76][   70/  246]    Overall Loss 0.299844    Objective Loss 0.299844                                        LR 0.000022    Time 0.026743    
2023-01-06 17:00:42,733 - Epoch: [76][   80/  246]    Overall Loss 0.299543    Objective Loss 0.299543                                        LR 0.000022    Time 0.026340    
2023-01-06 17:00:42,965 - Epoch: [76][   90/  246]    Overall Loss 0.298756    Objective Loss 0.298756                                        LR 0.000022    Time 0.025988    
2023-01-06 17:00:43,186 - Epoch: [76][  100/  246]    Overall Loss 0.298199    Objective Loss 0.298199                                        LR 0.000022    Time 0.025589    
2023-01-06 17:00:43,402 - Epoch: [76][  110/  246]    Overall Loss 0.297441    Objective Loss 0.297441                                        LR 0.000022    Time 0.025228    
2023-01-06 17:00:43,621 - Epoch: [76][  120/  246]    Overall Loss 0.298208    Objective Loss 0.298208                                        LR 0.000022    Time 0.024946    
2023-01-06 17:00:43,853 - Epoch: [76][  130/  246]    Overall Loss 0.298869    Objective Loss 0.298869                                        LR 0.000022    Time 0.024807    
2023-01-06 17:00:44,069 - Epoch: [76][  140/  246]    Overall Loss 0.296922    Objective Loss 0.296922                                        LR 0.000022    Time 0.024577    
2023-01-06 17:00:44,286 - Epoch: [76][  150/  246]    Overall Loss 0.296199    Objective Loss 0.296199                                        LR 0.000022    Time 0.024378    
2023-01-06 17:00:44,504 - Epoch: [76][  160/  246]    Overall Loss 0.294582    Objective Loss 0.294582                                        LR 0.000022    Time 0.024215    
2023-01-06 17:00:44,723 - Epoch: [76][  170/  246]    Overall Loss 0.293819    Objective Loss 0.293819                                        LR 0.000022    Time 0.024080    
2023-01-06 17:00:44,944 - Epoch: [76][  180/  246]    Overall Loss 0.293834    Objective Loss 0.293834                                        LR 0.000022    Time 0.023965    
2023-01-06 17:00:45,163 - Epoch: [76][  190/  246]    Overall Loss 0.294181    Objective Loss 0.294181                                        LR 0.000022    Time 0.023856    
2023-01-06 17:00:45,377 - Epoch: [76][  200/  246]    Overall Loss 0.294568    Objective Loss 0.294568                                        LR 0.000022    Time 0.023729    
2023-01-06 17:00:45,608 - Epoch: [76][  210/  246]    Overall Loss 0.294739    Objective Loss 0.294739                                        LR 0.000022    Time 0.023700    
2023-01-06 17:00:45,826 - Epoch: [76][  220/  246]    Overall Loss 0.295047    Objective Loss 0.295047                                        LR 0.000022    Time 0.023608    
2023-01-06 17:00:46,050 - Epoch: [76][  230/  246]    Overall Loss 0.294558    Objective Loss 0.294558                                        LR 0.000022    Time 0.023553    
2023-01-06 17:00:46,289 - Epoch: [76][  240/  246]    Overall Loss 0.294484    Objective Loss 0.294484                                        LR 0.000022    Time 0.023567    
2023-01-06 17:00:46,404 - Epoch: [76][  246/  246]    Overall Loss 0.294730    Objective Loss 0.294730    Top1 86.363636    LR 0.000022    Time 0.023457    
2023-01-06 17:00:46,549 - --- validate (epoch=76)-----------
2023-01-06 17:00:46,550 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:46,997 - Epoch: [76][   10/   28]    Loss 0.299096    Top1 89.218750    
2023-01-06 17:00:47,116 - Epoch: [76][   20/   28]    Loss 0.300383    Top1 89.335938    
2023-01-06 17:00:47,185 - Epoch: [76][   28/   28]    Loss 0.291689    Top1 89.493272    
2023-01-06 17:00:47,329 - ==> Top1: 89.493    Loss: 0.292

2023-01-06 17:00:47,329 - ==> Confusion:
[[ 207    8  224]
 [  14  201  387]
 [  51   50 5844]]

2023-01-06 17:00:47,330 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:47,330 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:47,336 - 

2023-01-06 17:00:47,336 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:47,918 - Epoch: [77][   10/  246]    Overall Loss 0.277590    Objective Loss 0.277590                                        LR 0.000022    Time 0.058131    
2023-01-06 17:00:48,117 - Epoch: [77][   20/  246]    Overall Loss 0.282821    Objective Loss 0.282821                                        LR 0.000022    Time 0.038967    
2023-01-06 17:00:48,315 - Epoch: [77][   30/  246]    Overall Loss 0.292954    Objective Loss 0.292954                                        LR 0.000022    Time 0.032584    
2023-01-06 17:00:48,514 - Epoch: [77][   40/  246]    Overall Loss 0.294209    Objective Loss 0.294209                                        LR 0.000022    Time 0.029382    
2023-01-06 17:00:48,703 - Epoch: [77][   50/  246]    Overall Loss 0.294743    Objective Loss 0.294743                                        LR 0.000022    Time 0.027285    
2023-01-06 17:00:48,900 - Epoch: [77][   60/  246]    Overall Loss 0.294577    Objective Loss 0.294577                                        LR 0.000022    Time 0.026015    
2023-01-06 17:00:49,093 - Epoch: [77][   70/  246]    Overall Loss 0.296379    Objective Loss 0.296379                                        LR 0.000022    Time 0.025050    
2023-01-06 17:00:49,281 - Epoch: [77][   80/  246]    Overall Loss 0.295500    Objective Loss 0.295500                                        LR 0.000022    Time 0.024270    
2023-01-06 17:00:49,466 - Epoch: [77][   90/  246]    Overall Loss 0.296277    Objective Loss 0.296277                                        LR 0.000022    Time 0.023623    
2023-01-06 17:00:49,654 - Epoch: [77][  100/  246]    Overall Loss 0.296112    Objective Loss 0.296112                                        LR 0.000022    Time 0.023138    
2023-01-06 17:00:49,838 - Epoch: [77][  110/  246]    Overall Loss 0.295181    Objective Loss 0.295181                                        LR 0.000022    Time 0.022698    
2023-01-06 17:00:50,030 - Epoch: [77][  120/  246]    Overall Loss 0.295100    Objective Loss 0.295100                                        LR 0.000022    Time 0.022410    
2023-01-06 17:00:50,234 - Epoch: [77][  130/  246]    Overall Loss 0.294631    Objective Loss 0.294631                                        LR 0.000022    Time 0.022245    
2023-01-06 17:00:50,435 - Epoch: [77][  140/  246]    Overall Loss 0.293681    Objective Loss 0.293681                                        LR 0.000022    Time 0.022091    
2023-01-06 17:00:50,634 - Epoch: [77][  150/  246]    Overall Loss 0.293558    Objective Loss 0.293558                                        LR 0.000022    Time 0.021946    
2023-01-06 17:00:50,836 - Epoch: [77][  160/  246]    Overall Loss 0.293772    Objective Loss 0.293772                                        LR 0.000022    Time 0.021833    
2023-01-06 17:00:51,035 - Epoch: [77][  170/  246]    Overall Loss 0.293394    Objective Loss 0.293394                                        LR 0.000022    Time 0.021714    
2023-01-06 17:00:51,235 - Epoch: [77][  180/  246]    Overall Loss 0.293072    Objective Loss 0.293072                                        LR 0.000022    Time 0.021621    
2023-01-06 17:00:51,435 - Epoch: [77][  190/  246]    Overall Loss 0.292402    Objective Loss 0.292402                                        LR 0.000022    Time 0.021533    
2023-01-06 17:00:51,632 - Epoch: [77][  200/  246]    Overall Loss 0.291930    Objective Loss 0.291930                                        LR 0.000022    Time 0.021437    
2023-01-06 17:00:51,828 - Epoch: [77][  210/  246]    Overall Loss 0.292495    Objective Loss 0.292495                                        LR 0.000022    Time 0.021340    
2023-01-06 17:00:52,025 - Epoch: [77][  220/  246]    Overall Loss 0.292630    Objective Loss 0.292630                                        LR 0.000022    Time 0.021263    
2023-01-06 17:00:52,223 - Epoch: [77][  230/  246]    Overall Loss 0.293320    Objective Loss 0.293320                                        LR 0.000022    Time 0.021199    
2023-01-06 17:00:52,421 - Epoch: [77][  240/  246]    Overall Loss 0.293542    Objective Loss 0.293542                                        LR 0.000022    Time 0.021140    
2023-01-06 17:00:52,511 - Epoch: [77][  246/  246]    Overall Loss 0.293626    Objective Loss 0.293626    Top1 86.602871    LR 0.000022    Time 0.020988    
2023-01-06 17:00:52,656 - --- validate (epoch=77)-----------
2023-01-06 17:00:52,656 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:53,226 - Epoch: [77][   10/   28]    Loss 0.291928    Top1 89.335938    
2023-01-06 17:00:53,359 - Epoch: [77][   20/   28]    Loss 0.289359    Top1 89.570312    
2023-01-06 17:00:53,427 - Epoch: [77][   28/   28]    Loss 0.292929    Top1 89.378758    
2023-01-06 17:00:53,569 - ==> Top1: 89.379    Loss: 0.293

2023-01-06 17:00:53,569 - ==> Confusion:
[[ 199    7  233]
 [  16  210  376]
 [  51   59 5835]]

2023-01-06 17:00:53,570 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:53,570 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:53,576 - 

2023-01-06 17:00:53,576 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:00:54,154 - Epoch: [78][   10/  246]    Overall Loss 0.276714    Objective Loss 0.276714                                        LR 0.000022    Time 0.057701    
2023-01-06 17:00:54,352 - Epoch: [78][   20/  246]    Overall Loss 0.282203    Objective Loss 0.282203                                        LR 0.000022    Time 0.038719    
2023-01-06 17:00:54,547 - Epoch: [78][   30/  246]    Overall Loss 0.285303    Objective Loss 0.285303                                        LR 0.000022    Time 0.032313    
2023-01-06 17:00:54,744 - Epoch: [78][   40/  246]    Overall Loss 0.283418    Objective Loss 0.283418                                        LR 0.000022    Time 0.029143    
2023-01-06 17:00:54,939 - Epoch: [78][   50/  246]    Overall Loss 0.283248    Objective Loss 0.283248                                        LR 0.000022    Time 0.027214    
2023-01-06 17:00:55,138 - Epoch: [78][   60/  246]    Overall Loss 0.285344    Objective Loss 0.285344                                        LR 0.000022    Time 0.025977    
2023-01-06 17:00:55,333 - Epoch: [78][   70/  246]    Overall Loss 0.285297    Objective Loss 0.285297                                        LR 0.000022    Time 0.025057    
2023-01-06 17:00:55,533 - Epoch: [78][   80/  246]    Overall Loss 0.286152    Objective Loss 0.286152                                        LR 0.000022    Time 0.024422    
2023-01-06 17:00:55,732 - Epoch: [78][   90/  246]    Overall Loss 0.287392    Objective Loss 0.287392                                        LR 0.000022    Time 0.023914    
2023-01-06 17:00:55,934 - Epoch: [78][  100/  246]    Overall Loss 0.288149    Objective Loss 0.288149                                        LR 0.000022    Time 0.023536    
2023-01-06 17:00:56,135 - Epoch: [78][  110/  246]    Overall Loss 0.287717    Objective Loss 0.287717                                        LR 0.000022    Time 0.023217    
2023-01-06 17:00:56,332 - Epoch: [78][  120/  246]    Overall Loss 0.288651    Objective Loss 0.288651                                        LR 0.000022    Time 0.022923    
2023-01-06 17:00:56,525 - Epoch: [78][  130/  246]    Overall Loss 0.291432    Objective Loss 0.291432                                        LR 0.000022    Time 0.022641    
2023-01-06 17:00:56,718 - Epoch: [78][  140/  246]    Overall Loss 0.292731    Objective Loss 0.292731                                        LR 0.000022    Time 0.022403    
2023-01-06 17:00:56,911 - Epoch: [78][  150/  246]    Overall Loss 0.291184    Objective Loss 0.291184                                        LR 0.000022    Time 0.022195    
2023-01-06 17:00:57,102 - Epoch: [78][  160/  246]    Overall Loss 0.290534    Objective Loss 0.290534                                        LR 0.000022    Time 0.021995    
2023-01-06 17:00:57,298 - Epoch: [78][  170/  246]    Overall Loss 0.290284    Objective Loss 0.290284                                        LR 0.000022    Time 0.021851    
2023-01-06 17:00:57,494 - Epoch: [78][  180/  246]    Overall Loss 0.290205    Objective Loss 0.290205                                        LR 0.000022    Time 0.021725    
2023-01-06 17:00:57,689 - Epoch: [78][  190/  246]    Overall Loss 0.289287    Objective Loss 0.289287                                        LR 0.000022    Time 0.021608    
2023-01-06 17:00:57,885 - Epoch: [78][  200/  246]    Overall Loss 0.289719    Objective Loss 0.289719                                        LR 0.000022    Time 0.021504    
2023-01-06 17:00:58,080 - Epoch: [78][  210/  246]    Overall Loss 0.290632    Objective Loss 0.290632                                        LR 0.000022    Time 0.021408    
2023-01-06 17:00:58,275 - Epoch: [78][  220/  246]    Overall Loss 0.291151    Objective Loss 0.291151                                        LR 0.000022    Time 0.021318    
2023-01-06 17:00:58,469 - Epoch: [78][  230/  246]    Overall Loss 0.291619    Objective Loss 0.291619                                        LR 0.000022    Time 0.021235    
2023-01-06 17:00:58,677 - Epoch: [78][  240/  246]    Overall Loss 0.292073    Objective Loss 0.292073                                        LR 0.000022    Time 0.021214    
2023-01-06 17:00:58,768 - Epoch: [78][  246/  246]    Overall Loss 0.292200    Objective Loss 0.292200    Top1 88.755981    LR 0.000022    Time 0.021067    
2023-01-06 17:00:58,912 - --- validate (epoch=78)-----------
2023-01-06 17:00:58,913 - 6986 samples (256 per mini-batch)
2023-01-06 17:00:59,377 - Epoch: [78][   10/   28]    Loss 0.288606    Top1 89.140625    
2023-01-06 17:00:59,495 - Epoch: [78][   20/   28]    Loss 0.285621    Top1 89.394531    
2023-01-06 17:00:59,563 - Epoch: [78][   28/   28]    Loss 0.292312    Top1 89.450329    
2023-01-06 17:00:59,717 - ==> Top1: 89.450    Loss: 0.292

2023-01-06 17:00:59,717 - ==> Confusion:
[[ 200   12  227]
 [  14  219  369]
 [  51   64 5830]]

2023-01-06 17:00:59,719 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:00:59,719 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:00:59,725 - 

2023-01-06 17:00:59,725 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:00,420 - Epoch: [79][   10/  246]    Overall Loss 0.275994    Objective Loss 0.275994                                        LR 0.000022    Time 0.069392    
2023-01-06 17:01:00,603 - Epoch: [79][   20/  246]    Overall Loss 0.288791    Objective Loss 0.288791                                        LR 0.000022    Time 0.043860    
2023-01-06 17:01:00,791 - Epoch: [79][   30/  246]    Overall Loss 0.289203    Objective Loss 0.289203                                        LR 0.000022    Time 0.035475    
2023-01-06 17:01:00,983 - Epoch: [79][   40/  246]    Overall Loss 0.286648    Objective Loss 0.286648                                        LR 0.000022    Time 0.031405    
2023-01-06 17:01:01,175 - Epoch: [79][   50/  246]    Overall Loss 0.289332    Objective Loss 0.289332                                        LR 0.000022    Time 0.028958    
2023-01-06 17:01:01,366 - Epoch: [79][   60/  246]    Overall Loss 0.287544    Objective Loss 0.287544                                        LR 0.000022    Time 0.027310    
2023-01-06 17:01:01,557 - Epoch: [79][   70/  246]    Overall Loss 0.289017    Objective Loss 0.289017                                        LR 0.000022    Time 0.026136    
2023-01-06 17:01:01,746 - Epoch: [79][   80/  246]    Overall Loss 0.290950    Objective Loss 0.290950                                        LR 0.000022    Time 0.025223    
2023-01-06 17:01:01,935 - Epoch: [79][   90/  246]    Overall Loss 0.288155    Objective Loss 0.288155                                        LR 0.000022    Time 0.024516    
2023-01-06 17:01:02,134 - Epoch: [79][  100/  246]    Overall Loss 0.287193    Objective Loss 0.287193                                        LR 0.000022    Time 0.024050    
2023-01-06 17:01:02,360 - Epoch: [79][  110/  246]    Overall Loss 0.287452    Objective Loss 0.287452                                        LR 0.000022    Time 0.023915    
2023-01-06 17:01:02,585 - Epoch: [79][  120/  246]    Overall Loss 0.289129    Objective Loss 0.289129                                        LR 0.000022    Time 0.023788    
2023-01-06 17:01:02,812 - Epoch: [79][  130/  246]    Overall Loss 0.289791    Objective Loss 0.289791                                        LR 0.000022    Time 0.023702    
2023-01-06 17:01:03,034 - Epoch: [79][  140/  246]    Overall Loss 0.289950    Objective Loss 0.289950                                        LR 0.000022    Time 0.023592    
2023-01-06 17:01:03,255 - Epoch: [79][  150/  246]    Overall Loss 0.290845    Objective Loss 0.290845                                        LR 0.000022    Time 0.023490    
2023-01-06 17:01:03,476 - Epoch: [79][  160/  246]    Overall Loss 0.290966    Objective Loss 0.290966                                        LR 0.000022    Time 0.023398    
2023-01-06 17:01:03,697 - Epoch: [79][  170/  246]    Overall Loss 0.290681    Objective Loss 0.290681                                        LR 0.000022    Time 0.023324    
2023-01-06 17:01:03,919 - Epoch: [79][  180/  246]    Overall Loss 0.290361    Objective Loss 0.290361                                        LR 0.000022    Time 0.023256    
2023-01-06 17:01:04,144 - Epoch: [79][  190/  246]    Overall Loss 0.290666    Objective Loss 0.290666                                        LR 0.000022    Time 0.023213    
2023-01-06 17:01:04,367 - Epoch: [79][  200/  246]    Overall Loss 0.291102    Objective Loss 0.291102                                        LR 0.000022    Time 0.023166    
2023-01-06 17:01:04,595 - Epoch: [79][  210/  246]    Overall Loss 0.290908    Objective Loss 0.290908                                        LR 0.000022    Time 0.023144    
2023-01-06 17:01:04,820 - Epoch: [79][  220/  246]    Overall Loss 0.291547    Objective Loss 0.291547                                        LR 0.000022    Time 0.023114    
2023-01-06 17:01:05,049 - Epoch: [79][  230/  246]    Overall Loss 0.291263    Objective Loss 0.291263                                        LR 0.000022    Time 0.023101    
2023-01-06 17:01:05,291 - Epoch: [79][  240/  246]    Overall Loss 0.291028    Objective Loss 0.291028                                        LR 0.000022    Time 0.023147    
2023-01-06 17:01:05,397 - Epoch: [79][  246/  246]    Overall Loss 0.290747    Objective Loss 0.290747    Top1 90.909091    LR 0.000022    Time 0.023013    
2023-01-06 17:01:05,534 - --- validate (epoch=79)-----------
2023-01-06 17:01:05,534 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:05,994 - Epoch: [79][   10/   28]    Loss 0.299967    Top1 88.906250    
2023-01-06 17:01:06,125 - Epoch: [79][   20/   28]    Loss 0.304953    Top1 88.789062    
2023-01-06 17:01:06,193 - Epoch: [79][   28/   28]    Loss 0.294806    Top1 89.307186    
2023-01-06 17:01:06,354 - ==> Top1: 89.307    Loss: 0.295

2023-01-06 17:01:06,355 - ==> Confusion:
[[ 186    8  245]
 [  12  216  374]
 [  44   64 5837]]

2023-01-06 17:01:06,356 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:01:06,356 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:06,362 - 

2023-01-06 17:01:06,362 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:06,922 - Epoch: [80][   10/  246]    Overall Loss 0.277835    Objective Loss 0.277835                                        LR 0.000022    Time 0.055984    
2023-01-06 17:01:07,116 - Epoch: [80][   20/  246]    Overall Loss 0.288357    Objective Loss 0.288357                                        LR 0.000022    Time 0.037654    
2023-01-06 17:01:07,315 - Epoch: [80][   30/  246]    Overall Loss 0.293428    Objective Loss 0.293428                                        LR 0.000022    Time 0.031730    
2023-01-06 17:01:07,515 - Epoch: [80][   40/  246]    Overall Loss 0.297797    Objective Loss 0.297797                                        LR 0.000022    Time 0.028769    
2023-01-06 17:01:07,711 - Epoch: [80][   50/  246]    Overall Loss 0.302286    Objective Loss 0.302286                                        LR 0.000022    Time 0.026924    
2023-01-06 17:01:07,911 - Epoch: [80][   60/  246]    Overall Loss 0.300905    Objective Loss 0.300905                                        LR 0.000022    Time 0.025778    
2023-01-06 17:01:08,113 - Epoch: [80][   70/  246]    Overall Loss 0.298622    Objective Loss 0.298622                                        LR 0.000022    Time 0.024972    
2023-01-06 17:01:08,319 - Epoch: [80][   80/  246]    Overall Loss 0.297415    Objective Loss 0.297415                                        LR 0.000022    Time 0.024422    
2023-01-06 17:01:08,546 - Epoch: [80][   90/  246]    Overall Loss 0.296860    Objective Loss 0.296860                                        LR 0.000022    Time 0.024218    
2023-01-06 17:01:08,771 - Epoch: [80][  100/  246]    Overall Loss 0.295446    Objective Loss 0.295446                                        LR 0.000022    Time 0.024042    
2023-01-06 17:01:09,001 - Epoch: [80][  110/  246]    Overall Loss 0.294745    Objective Loss 0.294745                                        LR 0.000022    Time 0.023946    
2023-01-06 17:01:09,237 - Epoch: [80][  120/  246]    Overall Loss 0.293788    Objective Loss 0.293788                                        LR 0.000022    Time 0.023912    
2023-01-06 17:01:09,480 - Epoch: [80][  130/  246]    Overall Loss 0.292925    Objective Loss 0.292925                                        LR 0.000022    Time 0.023939    
2023-01-06 17:01:09,719 - Epoch: [80][  140/  246]    Overall Loss 0.291856    Objective Loss 0.291856                                        LR 0.000022    Time 0.023920    
2023-01-06 17:01:09,964 - Epoch: [80][  150/  246]    Overall Loss 0.291982    Objective Loss 0.291982                                        LR 0.000022    Time 0.023954    
2023-01-06 17:01:10,175 - Epoch: [80][  160/  246]    Overall Loss 0.291596    Objective Loss 0.291596                                        LR 0.000022    Time 0.023768    
2023-01-06 17:01:10,379 - Epoch: [80][  170/  246]    Overall Loss 0.291402    Objective Loss 0.291402                                        LR 0.000022    Time 0.023568    
2023-01-06 17:01:10,575 - Epoch: [80][  180/  246]    Overall Loss 0.292879    Objective Loss 0.292879                                        LR 0.000022    Time 0.023341    
2023-01-06 17:01:10,775 - Epoch: [80][  190/  246]    Overall Loss 0.292508    Objective Loss 0.292508                                        LR 0.000022    Time 0.023166    
2023-01-06 17:01:10,964 - Epoch: [80][  200/  246]    Overall Loss 0.292041    Objective Loss 0.292041                                        LR 0.000022    Time 0.022947    
2023-01-06 17:01:11,155 - Epoch: [80][  210/  246]    Overall Loss 0.291978    Objective Loss 0.291978                                        LR 0.000022    Time 0.022766    
2023-01-06 17:01:11,342 - Epoch: [80][  220/  246]    Overall Loss 0.291482    Objective Loss 0.291482                                        LR 0.000022    Time 0.022579    
2023-01-06 17:01:11,534 - Epoch: [80][  230/  246]    Overall Loss 0.290753    Objective Loss 0.290753                                        LR 0.000022    Time 0.022428    
2023-01-06 17:01:11,742 - Epoch: [80][  240/  246]    Overall Loss 0.290114    Objective Loss 0.290114                                        LR 0.000022    Time 0.022359    
2023-01-06 17:01:11,838 - Epoch: [80][  246/  246]    Overall Loss 0.290706    Objective Loss 0.290706    Top1 88.277512    LR 0.000022    Time 0.022206    
2023-01-06 17:01:11,974 - --- validate (epoch=80)-----------
2023-01-06 17:01:11,974 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:12,442 - Epoch: [80][   10/   28]    Loss 0.295437    Top1 88.984375    
2023-01-06 17:01:12,567 - Epoch: [80][   20/   28]    Loss 0.297294    Top1 89.121094    
2023-01-06 17:01:12,635 - Epoch: [80][   28/   28]    Loss 0.290781    Top1 89.421701    
2023-01-06 17:01:12,769 - ==> Top1: 89.422    Loss: 0.291

2023-01-06 17:01:12,769 - ==> Confusion:
[[ 197   15  227]
 [  14  213  375]
 [  50   58 5837]]

2023-01-06 17:01:12,770 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:01:12,770 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:12,776 - 

2023-01-06 17:01:12,776 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:13,493 - Epoch: [81][   10/  246]    Overall Loss 0.286163    Objective Loss 0.286163                                        LR 0.000022    Time 0.071554    
2023-01-06 17:01:13,694 - Epoch: [81][   20/  246]    Overall Loss 0.293732    Objective Loss 0.293732                                        LR 0.000022    Time 0.045754    
2023-01-06 17:01:13,903 - Epoch: [81][   30/  246]    Overall Loss 0.292078    Objective Loss 0.292078                                        LR 0.000022    Time 0.037408    
2023-01-06 17:01:14,114 - Epoch: [81][   40/  246]    Overall Loss 0.290149    Objective Loss 0.290149                                        LR 0.000022    Time 0.033332    
2023-01-06 17:01:14,326 - Epoch: [81][   50/  246]    Overall Loss 0.291748    Objective Loss 0.291748                                        LR 0.000022    Time 0.030875    
2023-01-06 17:01:14,543 - Epoch: [81][   60/  246]    Overall Loss 0.292978    Objective Loss 0.292978                                        LR 0.000022    Time 0.029341    
2023-01-06 17:01:14,755 - Epoch: [81][   70/  246]    Overall Loss 0.294025    Objective Loss 0.294025                                        LR 0.000022    Time 0.028138    
2023-01-06 17:01:14,968 - Epoch: [81][   80/  246]    Overall Loss 0.291493    Objective Loss 0.291493                                        LR 0.000022    Time 0.027287    
2023-01-06 17:01:15,172 - Epoch: [81][   90/  246]    Overall Loss 0.291777    Objective Loss 0.291777                                        LR 0.000022    Time 0.026509    
2023-01-06 17:01:15,375 - Epoch: [81][  100/  246]    Overall Loss 0.290847    Objective Loss 0.290847                                        LR 0.000022    Time 0.025888    
2023-01-06 17:01:15,595 - Epoch: [81][  110/  246]    Overall Loss 0.288566    Objective Loss 0.288566                                        LR 0.000022    Time 0.025531    
2023-01-06 17:01:15,817 - Epoch: [81][  120/  246]    Overall Loss 0.289544    Objective Loss 0.289544                                        LR 0.000022    Time 0.025253    
2023-01-06 17:01:16,044 - Epoch: [81][  130/  246]    Overall Loss 0.290016    Objective Loss 0.290016                                        LR 0.000022    Time 0.025049    
2023-01-06 17:01:16,272 - Epoch: [81][  140/  246]    Overall Loss 0.290609    Objective Loss 0.290609                                        LR 0.000022    Time 0.024887    
2023-01-06 17:01:16,498 - Epoch: [81][  150/  246]    Overall Loss 0.290408    Objective Loss 0.290408                                        LR 0.000022    Time 0.024729    
2023-01-06 17:01:16,727 - Epoch: [81][  160/  246]    Overall Loss 0.291205    Objective Loss 0.291205                                        LR 0.000022    Time 0.024614    
2023-01-06 17:01:16,947 - Epoch: [81][  170/  246]    Overall Loss 0.289694    Objective Loss 0.289694                                        LR 0.000022    Time 0.024456    
2023-01-06 17:01:17,166 - Epoch: [81][  180/  246]    Overall Loss 0.289134    Objective Loss 0.289134                                        LR 0.000022    Time 0.024314    
2023-01-06 17:01:17,382 - Epoch: [81][  190/  246]    Overall Loss 0.290091    Objective Loss 0.290091                                        LR 0.000022    Time 0.024169    
2023-01-06 17:01:17,601 - Epoch: [81][  200/  246]    Overall Loss 0.289795    Objective Loss 0.289795                                        LR 0.000022    Time 0.024054    
2023-01-06 17:01:17,825 - Epoch: [81][  210/  246]    Overall Loss 0.288718    Objective Loss 0.288718                                        LR 0.000022    Time 0.023971    
2023-01-06 17:01:18,043 - Epoch: [81][  220/  246]    Overall Loss 0.289988    Objective Loss 0.289988                                        LR 0.000022    Time 0.023871    
2023-01-06 17:01:18,266 - Epoch: [81][  230/  246]    Overall Loss 0.289610    Objective Loss 0.289610                                        LR 0.000022    Time 0.023801    
2023-01-06 17:01:18,499 - Epoch: [81][  240/  246]    Overall Loss 0.289436    Objective Loss 0.289436                                        LR 0.000022    Time 0.023781    
2023-01-06 17:01:18,611 - Epoch: [81][  246/  246]    Overall Loss 0.289369    Objective Loss 0.289369    Top1 87.081340    LR 0.000022    Time 0.023654    
2023-01-06 17:01:18,752 - --- validate (epoch=81)-----------
2023-01-06 17:01:18,753 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:19,194 - Epoch: [81][   10/   28]    Loss 0.300178    Top1 89.101562    
2023-01-06 17:01:19,309 - Epoch: [81][   20/   28]    Loss 0.301645    Top1 89.121094    
2023-01-06 17:01:19,380 - Epoch: [81][   28/   28]    Loss 0.292225    Top1 89.350129    
2023-01-06 17:01:19,539 - ==> Top1: 89.350    Loss: 0.292

2023-01-06 17:01:19,539 - ==> Confusion:
[[ 234    8  197]
 [  19  216  367]
 [  88   65 5792]]

2023-01-06 17:01:19,541 - ==> Best [Top1: 89.565   Sparsity:0.00   Params: 155168 on epoch: 72]
2023-01-06 17:01:19,541 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:19,547 - 

2023-01-06 17:01:19,547 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:20,273 - Epoch: [82][   10/  246]    Overall Loss 0.287320    Objective Loss 0.287320                                        LR 0.000022    Time 0.072546    
2023-01-06 17:01:20,474 - Epoch: [82][   20/  246]    Overall Loss 0.288301    Objective Loss 0.288301                                        LR 0.000022    Time 0.046297    
2023-01-06 17:01:20,670 - Epoch: [82][   30/  246]    Overall Loss 0.292616    Objective Loss 0.292616                                        LR 0.000022    Time 0.037387    
2023-01-06 17:01:20,865 - Epoch: [82][   40/  246]    Overall Loss 0.294677    Objective Loss 0.294677                                        LR 0.000022    Time 0.032916    
2023-01-06 17:01:21,032 - Epoch: [82][   50/  246]    Overall Loss 0.296501    Objective Loss 0.296501                                        LR 0.000022    Time 0.029649    
2023-01-06 17:01:21,218 - Epoch: [82][   60/  246]    Overall Loss 0.295419    Objective Loss 0.295419                                        LR 0.000022    Time 0.027807    
2023-01-06 17:01:21,406 - Epoch: [82][   70/  246]    Overall Loss 0.295036    Objective Loss 0.295036                                        LR 0.000022    Time 0.026521    
2023-01-06 17:01:21,593 - Epoch: [82][   80/  246]    Overall Loss 0.293459    Objective Loss 0.293459                                        LR 0.000022    Time 0.025536    
2023-01-06 17:01:21,780 - Epoch: [82][   90/  246]    Overall Loss 0.291581    Objective Loss 0.291581                                        LR 0.000022    Time 0.024774    
2023-01-06 17:01:21,959 - Epoch: [82][  100/  246]    Overall Loss 0.290822    Objective Loss 0.290822                                        LR 0.000022    Time 0.024085    
2023-01-06 17:01:22,144 - Epoch: [82][  110/  246]    Overall Loss 0.290906    Objective Loss 0.290906                                        LR 0.000022    Time 0.023569    
2023-01-06 17:01:22,335 - Epoch: [82][  120/  246]    Overall Loss 0.291250    Objective Loss 0.291250                                        LR 0.000022    Time 0.023191    
2023-01-06 17:01:22,520 - Epoch: [82][  130/  246]    Overall Loss 0.291457    Objective Loss 0.291457                                        LR 0.000022    Time 0.022833    
2023-01-06 17:01:22,709 - Epoch: [82][  140/  246]    Overall Loss 0.291971    Objective Loss 0.291971                                        LR 0.000022    Time 0.022548    
2023-01-06 17:01:22,895 - Epoch: [82][  150/  246]    Overall Loss 0.290942    Objective Loss 0.290942                                        LR 0.000022    Time 0.022279    
2023-01-06 17:01:23,085 - Epoch: [82][  160/  246]    Overall Loss 0.289631    Objective Loss 0.289631                                        LR 0.000022    Time 0.022077    
2023-01-06 17:01:23,274 - Epoch: [82][  170/  246]    Overall Loss 0.289056    Objective Loss 0.289056                                        LR 0.000022    Time 0.021882    
2023-01-06 17:01:23,462 - Epoch: [82][  180/  246]    Overall Loss 0.288217    Objective Loss 0.288217                                        LR 0.000022    Time 0.021714    
2023-01-06 17:01:23,647 - Epoch: [82][  190/  246]    Overall Loss 0.289111    Objective Loss 0.289111                                        LR 0.000022    Time 0.021543    
2023-01-06 17:01:23,837 - Epoch: [82][  200/  246]    Overall Loss 0.289866    Objective Loss 0.289866                                        LR 0.000022    Time 0.021410    
2023-01-06 17:01:24,024 - Epoch: [82][  210/  246]    Overall Loss 0.289907    Objective Loss 0.289907                                        LR 0.000022    Time 0.021280    
2023-01-06 17:01:24,216 - Epoch: [82][  220/  246]    Overall Loss 0.289469    Objective Loss 0.289469                                        LR 0.000022    Time 0.021184    
2023-01-06 17:01:24,405 - Epoch: [82][  230/  246]    Overall Loss 0.289631    Objective Loss 0.289631                                        LR 0.000022    Time 0.021085    
2023-01-06 17:01:24,608 - Epoch: [82][  240/  246]    Overall Loss 0.289561    Objective Loss 0.289561                                        LR 0.000022    Time 0.021048    
2023-01-06 17:01:24,707 - Epoch: [82][  246/  246]    Overall Loss 0.289208    Objective Loss 0.289208    Top1 91.148325    LR 0.000022    Time 0.020936    
2023-01-06 17:01:24,849 - --- validate (epoch=82)-----------
2023-01-06 17:01:24,849 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:25,317 - Epoch: [82][   10/   28]    Loss 0.289429    Top1 89.726562    
2023-01-06 17:01:25,437 - Epoch: [82][   20/   28]    Loss 0.291029    Top1 89.570312    
2023-01-06 17:01:25,506 - Epoch: [82][   28/   28]    Loss 0.292167    Top1 89.665044    
2023-01-06 17:01:25,631 - ==> Top1: 89.665    Loss: 0.292

2023-01-06 17:01:25,631 - ==> Confusion:
[[ 210    9  220]
 [  19  229  354]
 [  53   67 5825]]

2023-01-06 17:01:25,632 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:25,632 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:25,640 - 

2023-01-06 17:01:25,640 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:26,198 - Epoch: [83][   10/  246]    Overall Loss 0.269415    Objective Loss 0.269415                                        LR 0.000022    Time 0.055759    
2023-01-06 17:01:26,378 - Epoch: [83][   20/  246]    Overall Loss 0.272411    Objective Loss 0.272411                                        LR 0.000022    Time 0.036824    
2023-01-06 17:01:26,550 - Epoch: [83][   30/  246]    Overall Loss 0.276433    Objective Loss 0.276433                                        LR 0.000022    Time 0.030300    
2023-01-06 17:01:26,733 - Epoch: [83][   40/  246]    Overall Loss 0.281848    Objective Loss 0.281848                                        LR 0.000022    Time 0.027274    
2023-01-06 17:01:26,908 - Epoch: [83][   50/  246]    Overall Loss 0.284949    Objective Loss 0.284949                                        LR 0.000022    Time 0.025323    
2023-01-06 17:01:27,089 - Epoch: [83][   60/  246]    Overall Loss 0.288917    Objective Loss 0.288917                                        LR 0.000022    Time 0.024103    
2023-01-06 17:01:27,287 - Epoch: [83][   70/  246]    Overall Loss 0.286869    Objective Loss 0.286869                                        LR 0.000022    Time 0.023486    
2023-01-06 17:01:27,494 - Epoch: [83][   80/  246]    Overall Loss 0.286467    Objective Loss 0.286467                                        LR 0.000022    Time 0.023130    
2023-01-06 17:01:27,693 - Epoch: [83][   90/  246]    Overall Loss 0.287208    Objective Loss 0.287208                                        LR 0.000022    Time 0.022773    
2023-01-06 17:01:27,883 - Epoch: [83][  100/  246]    Overall Loss 0.289692    Objective Loss 0.289692                                        LR 0.000022    Time 0.022384    
2023-01-06 17:01:28,070 - Epoch: [83][  110/  246]    Overall Loss 0.288743    Objective Loss 0.288743                                        LR 0.000022    Time 0.022049    
2023-01-06 17:01:28,258 - Epoch: [83][  120/  246]    Overall Loss 0.289340    Objective Loss 0.289340                                        LR 0.000022    Time 0.021773    
2023-01-06 17:01:28,449 - Epoch: [83][  130/  246]    Overall Loss 0.289262    Objective Loss 0.289262                                        LR 0.000022    Time 0.021567    
2023-01-06 17:01:28,645 - Epoch: [83][  140/  246]    Overall Loss 0.288580    Objective Loss 0.288580                                        LR 0.000022    Time 0.021424    
2023-01-06 17:01:28,837 - Epoch: [83][  150/  246]    Overall Loss 0.289079    Objective Loss 0.289079                                        LR 0.000022    Time 0.021275    
2023-01-06 17:01:29,027 - Epoch: [83][  160/  246]    Overall Loss 0.288859    Objective Loss 0.288859                                        LR 0.000022    Time 0.021130    
2023-01-06 17:01:29,216 - Epoch: [83][  170/  246]    Overall Loss 0.288117    Objective Loss 0.288117                                        LR 0.000022    Time 0.020996    
2023-01-06 17:01:29,405 - Epoch: [83][  180/  246]    Overall Loss 0.287313    Objective Loss 0.287313                                        LR 0.000022    Time 0.020877    
2023-01-06 17:01:29,593 - Epoch: [83][  190/  246]    Overall Loss 0.288539    Objective Loss 0.288539                                        LR 0.000022    Time 0.020768    
2023-01-06 17:01:29,785 - Epoch: [83][  200/  246]    Overall Loss 0.287969    Objective Loss 0.287969                                        LR 0.000022    Time 0.020685    
2023-01-06 17:01:29,980 - Epoch: [83][  210/  246]    Overall Loss 0.287843    Objective Loss 0.287843                                        LR 0.000022    Time 0.020628    
2023-01-06 17:01:30,176 - Epoch: [83][  220/  246]    Overall Loss 0.288912    Objective Loss 0.288912                                        LR 0.000022    Time 0.020582    
2023-01-06 17:01:30,372 - Epoch: [83][  230/  246]    Overall Loss 0.288250    Objective Loss 0.288250                                        LR 0.000022    Time 0.020534    
2023-01-06 17:01:30,583 - Epoch: [83][  240/  246]    Overall Loss 0.287897    Objective Loss 0.287897                                        LR 0.000022    Time 0.020556    
2023-01-06 17:01:30,678 - Epoch: [83][  246/  246]    Overall Loss 0.288275    Objective Loss 0.288275    Top1 88.995215    LR 0.000022    Time 0.020442    
2023-01-06 17:01:30,818 - --- validate (epoch=83)-----------
2023-01-06 17:01:30,818 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:31,281 - Epoch: [83][   10/   28]    Loss 0.292668    Top1 89.882812    
2023-01-06 17:01:31,393 - Epoch: [83][   20/   28]    Loss 0.296015    Top1 89.765625    
2023-01-06 17:01:31,463 - Epoch: [83][   28/   28]    Loss 0.298734    Top1 89.622101    
2023-01-06 17:01:31,602 - ==> Top1: 89.622    Loss: 0.299

2023-01-06 17:01:31,603 - ==> Confusion:
[[ 223   11  205]
 [  17  236  349]
 [  73   70 5802]]

2023-01-06 17:01:31,604 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:31,604 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:31,610 - 

2023-01-06 17:01:31,610 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:32,322 - Epoch: [84][   10/  246]    Overall Loss 0.289265    Objective Loss 0.289265                                        LR 0.000022    Time 0.071152    
2023-01-06 17:01:32,517 - Epoch: [84][   20/  246]    Overall Loss 0.283233    Objective Loss 0.283233                                        LR 0.000022    Time 0.045294    
2023-01-06 17:01:32,711 - Epoch: [84][   30/  246]    Overall Loss 0.278685    Objective Loss 0.278685                                        LR 0.000022    Time 0.036648    
2023-01-06 17:01:32,901 - Epoch: [84][   40/  246]    Overall Loss 0.284231    Objective Loss 0.284231                                        LR 0.000022    Time 0.032242    
2023-01-06 17:01:33,094 - Epoch: [84][   50/  246]    Overall Loss 0.289924    Objective Loss 0.289924                                        LR 0.000022    Time 0.029643    
2023-01-06 17:01:33,284 - Epoch: [84][   60/  246]    Overall Loss 0.289826    Objective Loss 0.289826                                        LR 0.000022    Time 0.027861    
2023-01-06 17:01:33,466 - Epoch: [84][   70/  246]    Overall Loss 0.290584    Objective Loss 0.290584                                        LR 0.000022    Time 0.026474    
2023-01-06 17:01:33,642 - Epoch: [84][   80/  246]    Overall Loss 0.290341    Objective Loss 0.290341                                        LR 0.000022    Time 0.025363    
2023-01-06 17:01:33,831 - Epoch: [84][   90/  246]    Overall Loss 0.289682    Objective Loss 0.289682                                        LR 0.000022    Time 0.024640    
2023-01-06 17:01:34,026 - Epoch: [84][  100/  246]    Overall Loss 0.288988    Objective Loss 0.288988                                        LR 0.000022    Time 0.024119    
2023-01-06 17:01:34,222 - Epoch: [84][  110/  246]    Overall Loss 0.289132    Objective Loss 0.289132                                        LR 0.000022    Time 0.023707    
2023-01-06 17:01:34,411 - Epoch: [84][  120/  246]    Overall Loss 0.290096    Objective Loss 0.290096                                        LR 0.000022    Time 0.023298    
2023-01-06 17:01:34,583 - Epoch: [84][  130/  246]    Overall Loss 0.288714    Objective Loss 0.288714                                        LR 0.000022    Time 0.022832    
2023-01-06 17:01:34,754 - Epoch: [84][  140/  246]    Overall Loss 0.287678    Objective Loss 0.287678                                        LR 0.000022    Time 0.022420    
2023-01-06 17:01:34,928 - Epoch: [84][  150/  246]    Overall Loss 0.286659    Objective Loss 0.286659                                        LR 0.000022    Time 0.022080    
2023-01-06 17:01:35,102 - Epoch: [84][  160/  246]    Overall Loss 0.286242    Objective Loss 0.286242                                        LR 0.000022    Time 0.021789    
2023-01-06 17:01:35,274 - Epoch: [84][  170/  246]    Overall Loss 0.287465    Objective Loss 0.287465                                        LR 0.000022    Time 0.021518    
2023-01-06 17:01:35,445 - Epoch: [84][  180/  246]    Overall Loss 0.287778    Objective Loss 0.287778                                        LR 0.000022    Time 0.021269    
2023-01-06 17:01:35,623 - Epoch: [84][  190/  246]    Overall Loss 0.288231    Objective Loss 0.288231                                        LR 0.000022    Time 0.021084    
2023-01-06 17:01:35,825 - Epoch: [84][  200/  246]    Overall Loss 0.287471    Objective Loss 0.287471                                        LR 0.000022    Time 0.021038    
2023-01-06 17:01:36,025 - Epoch: [84][  210/  246]    Overall Loss 0.287173    Objective Loss 0.287173                                        LR 0.000022    Time 0.020988    
2023-01-06 17:01:36,228 - Epoch: [84][  220/  246]    Overall Loss 0.287166    Objective Loss 0.287166                                        LR 0.000022    Time 0.020956    
2023-01-06 17:01:36,430 - Epoch: [84][  230/  246]    Overall Loss 0.287616    Objective Loss 0.287616                                        LR 0.000022    Time 0.020919    
2023-01-06 17:01:36,643 - Epoch: [84][  240/  246]    Overall Loss 0.287656    Objective Loss 0.287656                                        LR 0.000022    Time 0.020938    
2023-01-06 17:01:36,741 - Epoch: [84][  246/  246]    Overall Loss 0.287403    Objective Loss 0.287403    Top1 90.191388    LR 0.000022    Time 0.020822    
2023-01-06 17:01:36,871 - --- validate (epoch=84)-----------
2023-01-06 17:01:36,871 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:37,330 - Epoch: [84][   10/   28]    Loss 0.306393    Top1 88.593750    
2023-01-06 17:01:37,442 - Epoch: [84][   20/   28]    Loss 0.292305    Top1 89.277344    
2023-01-06 17:01:37,510 - Epoch: [84][   28/   28]    Loss 0.288459    Top1 89.421701    
2023-01-06 17:01:37,640 - ==> Top1: 89.422    Loss: 0.288

2023-01-06 17:01:37,640 - ==> Confusion:
[[ 191   10  238]
 [  16  206  380]
 [  44   51 5850]]

2023-01-06 17:01:37,641 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:37,641 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:37,647 - 

2023-01-06 17:01:37,647 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:38,214 - Epoch: [85][   10/  246]    Overall Loss 0.261143    Objective Loss 0.261143                                        LR 0.000022    Time 0.056620    
2023-01-06 17:01:38,410 - Epoch: [85][   20/  246]    Overall Loss 0.264631    Objective Loss 0.264631                                        LR 0.000022    Time 0.038083    
2023-01-06 17:01:38,617 - Epoch: [85][   30/  246]    Overall Loss 0.279379    Objective Loss 0.279379                                        LR 0.000022    Time 0.032261    
2023-01-06 17:01:38,819 - Epoch: [85][   40/  246]    Overall Loss 0.276071    Objective Loss 0.276071                                        LR 0.000022    Time 0.029241    
2023-01-06 17:01:39,027 - Epoch: [85][   50/  246]    Overall Loss 0.278044    Objective Loss 0.278044                                        LR 0.000022    Time 0.027542    
2023-01-06 17:01:39,229 - Epoch: [85][   60/  246]    Overall Loss 0.277014    Objective Loss 0.277014                                        LR 0.000022    Time 0.026306    
2023-01-06 17:01:39,438 - Epoch: [85][   70/  246]    Overall Loss 0.278066    Objective Loss 0.278066                                        LR 0.000022    Time 0.025534    
2023-01-06 17:01:39,639 - Epoch: [85][   80/  246]    Overall Loss 0.277858    Objective Loss 0.277858                                        LR 0.000022    Time 0.024854    
2023-01-06 17:01:39,837 - Epoch: [85][   90/  246]    Overall Loss 0.278145    Objective Loss 0.278145                                        LR 0.000022    Time 0.024288    
2023-01-06 17:01:40,040 - Epoch: [85][  100/  246]    Overall Loss 0.280248    Objective Loss 0.280248                                        LR 0.000022    Time 0.023867    
2023-01-06 17:01:40,240 - Epoch: [85][  110/  246]    Overall Loss 0.281461    Objective Loss 0.281461                                        LR 0.000022    Time 0.023506    
2023-01-06 17:01:40,441 - Epoch: [85][  120/  246]    Overall Loss 0.282588    Objective Loss 0.282588                                        LR 0.000022    Time 0.023218    
2023-01-06 17:01:40,639 - Epoch: [85][  130/  246]    Overall Loss 0.284260    Objective Loss 0.284260                                        LR 0.000022    Time 0.022952    
2023-01-06 17:01:40,844 - Epoch: [85][  140/  246]    Overall Loss 0.284626    Objective Loss 0.284626                                        LR 0.000022    Time 0.022774    
2023-01-06 17:01:41,047 - Epoch: [85][  150/  246]    Overall Loss 0.284349    Objective Loss 0.284349                                        LR 0.000022    Time 0.022610    
2023-01-06 17:01:41,244 - Epoch: [85][  160/  246]    Overall Loss 0.285662    Objective Loss 0.285662                                        LR 0.000022    Time 0.022423    
2023-01-06 17:01:41,403 - Epoch: [85][  170/  246]    Overall Loss 0.285633    Objective Loss 0.285633                                        LR 0.000022    Time 0.022039    
2023-01-06 17:01:41,569 - Epoch: [85][  180/  246]    Overall Loss 0.285034    Objective Loss 0.285034                                        LR 0.000022    Time 0.021738    
2023-01-06 17:01:41,728 - Epoch: [85][  190/  246]    Overall Loss 0.284627    Objective Loss 0.284627                                        LR 0.000022    Time 0.021428    
2023-01-06 17:01:41,891 - Epoch: [85][  200/  246]    Overall Loss 0.284904    Objective Loss 0.284904                                        LR 0.000022    Time 0.021171    
2023-01-06 17:01:42,052 - Epoch: [85][  210/  246]    Overall Loss 0.284283    Objective Loss 0.284283                                        LR 0.000022    Time 0.020925    
2023-01-06 17:01:42,216 - Epoch: [85][  220/  246]    Overall Loss 0.284972    Objective Loss 0.284972                                        LR 0.000022    Time 0.020718    
2023-01-06 17:01:42,380 - Epoch: [85][  230/  246]    Overall Loss 0.285196    Objective Loss 0.285196                                        LR 0.000022    Time 0.020528    
2023-01-06 17:01:42,554 - Epoch: [85][  240/  246]    Overall Loss 0.284990    Objective Loss 0.284990                                        LR 0.000022    Time 0.020399    
2023-01-06 17:01:42,643 - Epoch: [85][  246/  246]    Overall Loss 0.285198    Objective Loss 0.285198    Top1 89.234450    LR 0.000022    Time 0.020261    
2023-01-06 17:01:42,774 - --- validate (epoch=85)-----------
2023-01-06 17:01:42,774 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:43,228 - Epoch: [85][   10/   28]    Loss 0.288539    Top1 89.960938    
2023-01-06 17:01:43,341 - Epoch: [85][   20/   28]    Loss 0.287101    Top1 89.765625    
2023-01-06 17:01:43,413 - Epoch: [85][   28/   28]    Loss 0.292903    Top1 89.493272    
2023-01-06 17:01:43,539 - ==> Top1: 89.493    Loss: 0.293

2023-01-06 17:01:43,539 - ==> Confusion:
[[ 199   10  230]
 [  15  203  384]
 [  45   50 5850]]

2023-01-06 17:01:43,541 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:43,541 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:43,547 - 

2023-01-06 17:01:43,547 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:44,243 - Epoch: [86][   10/  246]    Overall Loss 0.310156    Objective Loss 0.310156                                        LR 0.000022    Time 0.069538    
2023-01-06 17:01:44,435 - Epoch: [86][   20/  246]    Overall Loss 0.303641    Objective Loss 0.303641                                        LR 0.000022    Time 0.044344    
2023-01-06 17:01:44,644 - Epoch: [86][   30/  246]    Overall Loss 0.294538    Objective Loss 0.294538                                        LR 0.000022    Time 0.036500    
2023-01-06 17:01:44,851 - Epoch: [86][   40/  246]    Overall Loss 0.289010    Objective Loss 0.289010                                        LR 0.000022    Time 0.032537    
2023-01-06 17:01:45,055 - Epoch: [86][   50/  246]    Overall Loss 0.291000    Objective Loss 0.291000                                        LR 0.000022    Time 0.030106    
2023-01-06 17:01:45,255 - Epoch: [86][   60/  246]    Overall Loss 0.285888    Objective Loss 0.285888                                        LR 0.000022    Time 0.028424    
2023-01-06 17:01:45,455 - Epoch: [86][   70/  246]    Overall Loss 0.286182    Objective Loss 0.286182                                        LR 0.000022    Time 0.027208    
2023-01-06 17:01:45,657 - Epoch: [86][   80/  246]    Overall Loss 0.286543    Objective Loss 0.286543                                        LR 0.000022    Time 0.026328    
2023-01-06 17:01:45,860 - Epoch: [86][   90/  246]    Overall Loss 0.285139    Objective Loss 0.285139                                        LR 0.000022    Time 0.025659    
2023-01-06 17:01:46,061 - Epoch: [86][  100/  246]    Overall Loss 0.285462    Objective Loss 0.285462                                        LR 0.000022    Time 0.025100    
2023-01-06 17:01:46,253 - Epoch: [86][  110/  246]    Overall Loss 0.286738    Objective Loss 0.286738                                        LR 0.000022    Time 0.024557    
2023-01-06 17:01:46,447 - Epoch: [86][  120/  246]    Overall Loss 0.287081    Objective Loss 0.287081                                        LR 0.000022    Time 0.024124    
2023-01-06 17:01:46,637 - Epoch: [86][  130/  246]    Overall Loss 0.286627    Objective Loss 0.286627                                        LR 0.000022    Time 0.023729    
2023-01-06 17:01:46,831 - Epoch: [86][  140/  246]    Overall Loss 0.286145    Objective Loss 0.286145                                        LR 0.000022    Time 0.023413    
2023-01-06 17:01:47,023 - Epoch: [86][  150/  246]    Overall Loss 0.286006    Objective Loss 0.286006                                        LR 0.000022    Time 0.023129    
2023-01-06 17:01:47,218 - Epoch: [86][  160/  246]    Overall Loss 0.286258    Objective Loss 0.286258                                        LR 0.000022    Time 0.022898    
2023-01-06 17:01:47,410 - Epoch: [86][  170/  246]    Overall Loss 0.286984    Objective Loss 0.286984                                        LR 0.000022    Time 0.022681    
2023-01-06 17:01:47,605 - Epoch: [86][  180/  246]    Overall Loss 0.286231    Objective Loss 0.286231                                        LR 0.000022    Time 0.022500    
2023-01-06 17:01:47,799 - Epoch: [86][  190/  246]    Overall Loss 0.285359    Objective Loss 0.285359                                        LR 0.000022    Time 0.022335    
2023-01-06 17:01:47,992 - Epoch: [86][  200/  246]    Overall Loss 0.284604    Objective Loss 0.284604                                        LR 0.000022    Time 0.022181    
2023-01-06 17:01:48,185 - Epoch: [86][  210/  246]    Overall Loss 0.284317    Objective Loss 0.284317                                        LR 0.000022    Time 0.022045    
2023-01-06 17:01:48,380 - Epoch: [86][  220/  246]    Overall Loss 0.284364    Objective Loss 0.284364                                        LR 0.000022    Time 0.021928    
2023-01-06 17:01:48,579 - Epoch: [86][  230/  246]    Overall Loss 0.285393    Objective Loss 0.285393                                        LR 0.000022    Time 0.021836    
2023-01-06 17:01:48,788 - Epoch: [86][  240/  246]    Overall Loss 0.284552    Objective Loss 0.284552                                        LR 0.000022    Time 0.021798    
2023-01-06 17:01:48,886 - Epoch: [86][  246/  246]    Overall Loss 0.284882    Objective Loss 0.284882    Top1 88.995215    LR 0.000022    Time 0.021664    
2023-01-06 17:01:49,022 - --- validate (epoch=86)-----------
2023-01-06 17:01:49,022 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:49,465 - Epoch: [86][   10/   28]    Loss 0.289363    Top1 89.023438    
2023-01-06 17:01:49,581 - Epoch: [86][   20/   28]    Loss 0.289138    Top1 89.492188    
2023-01-06 17:01:49,649 - Epoch: [86][   28/   28]    Loss 0.288758    Top1 89.564844    
2023-01-06 17:01:49,786 - ==> Top1: 89.565    Loss: 0.289

2023-01-06 17:01:49,786 - ==> Confusion:
[[ 221   13  205]
 [  16  238  348]
 [  73   74 5798]]

2023-01-06 17:01:49,787 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:49,787 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:49,793 - 

2023-01-06 17:01:49,793 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:50,525 - Epoch: [87][   10/  246]    Overall Loss 0.284753    Objective Loss 0.284753                                        LR 0.000022    Time 0.073090    
2023-01-06 17:01:50,731 - Epoch: [87][   20/  246]    Overall Loss 0.281638    Objective Loss 0.281638                                        LR 0.000022    Time 0.046843    
2023-01-06 17:01:50,940 - Epoch: [87][   30/  246]    Overall Loss 0.280535    Objective Loss 0.280535                                        LR 0.000022    Time 0.038159    
2023-01-06 17:01:51,141 - Epoch: [87][   40/  246]    Overall Loss 0.282187    Objective Loss 0.282187                                        LR 0.000022    Time 0.033633    
2023-01-06 17:01:51,350 - Epoch: [87][   50/  246]    Overall Loss 0.280720    Objective Loss 0.280720                                        LR 0.000022    Time 0.031078    
2023-01-06 17:01:51,553 - Epoch: [87][   60/  246]    Overall Loss 0.283528    Objective Loss 0.283528                                        LR 0.000022    Time 0.029276    
2023-01-06 17:01:51,758 - Epoch: [87][   70/  246]    Overall Loss 0.283185    Objective Loss 0.283185                                        LR 0.000022    Time 0.028025    
2023-01-06 17:01:51,959 - Epoch: [87][   80/  246]    Overall Loss 0.284365    Objective Loss 0.284365                                        LR 0.000022    Time 0.027028    
2023-01-06 17:01:52,178 - Epoch: [87][   90/  246]    Overall Loss 0.285056    Objective Loss 0.285056                                        LR 0.000022    Time 0.026457    
2023-01-06 17:01:52,367 - Epoch: [87][  100/  246]    Overall Loss 0.284191    Objective Loss 0.284191                                        LR 0.000022    Time 0.025689    
2023-01-06 17:01:52,540 - Epoch: [87][  110/  246]    Overall Loss 0.285621    Objective Loss 0.285621                                        LR 0.000022    Time 0.024928    
2023-01-06 17:01:52,738 - Epoch: [87][  120/  246]    Overall Loss 0.287234    Objective Loss 0.287234                                        LR 0.000022    Time 0.024501    
2023-01-06 17:01:52,933 - Epoch: [87][  130/  246]    Overall Loss 0.284660    Objective Loss 0.284660                                        LR 0.000022    Time 0.024110    
2023-01-06 17:01:53,131 - Epoch: [87][  140/  246]    Overall Loss 0.285363    Objective Loss 0.285363                                        LR 0.000022    Time 0.023799    
2023-01-06 17:01:53,298 - Epoch: [87][  150/  246]    Overall Loss 0.285897    Objective Loss 0.285897                                        LR 0.000022    Time 0.023323    
2023-01-06 17:01:53,462 - Epoch: [87][  160/  246]    Overall Loss 0.285645    Objective Loss 0.285645                                        LR 0.000022    Time 0.022880    
2023-01-06 17:01:53,650 - Epoch: [87][  170/  246]    Overall Loss 0.285290    Objective Loss 0.285290                                        LR 0.000022    Time 0.022640    
2023-01-06 17:01:53,839 - Epoch: [87][  180/  246]    Overall Loss 0.285577    Objective Loss 0.285577                                        LR 0.000022    Time 0.022429    
2023-01-06 17:01:54,027 - Epoch: [87][  190/  246]    Overall Loss 0.286560    Objective Loss 0.286560                                        LR 0.000022    Time 0.022237    
2023-01-06 17:01:54,215 - Epoch: [87][  200/  246]    Overall Loss 0.286272    Objective Loss 0.286272                                        LR 0.000022    Time 0.022061    
2023-01-06 17:01:54,400 - Epoch: [87][  210/  246]    Overall Loss 0.285214    Objective Loss 0.285214                                        LR 0.000022    Time 0.021892    
2023-01-06 17:01:54,589 - Epoch: [87][  220/  246]    Overall Loss 0.285321    Objective Loss 0.285321                                        LR 0.000022    Time 0.021753    
2023-01-06 17:01:54,779 - Epoch: [87][  230/  246]    Overall Loss 0.285253    Objective Loss 0.285253                                        LR 0.000022    Time 0.021632    
2023-01-06 17:01:54,990 - Epoch: [87][  240/  246]    Overall Loss 0.285457    Objective Loss 0.285457                                        LR 0.000022    Time 0.021610    
2023-01-06 17:01:55,086 - Epoch: [87][  246/  246]    Overall Loss 0.285058    Objective Loss 0.285058    Top1 92.583732    LR 0.000022    Time 0.021472    
2023-01-06 17:01:55,232 - --- validate (epoch=87)-----------
2023-01-06 17:01:55,232 - 6986 samples (256 per mini-batch)
2023-01-06 17:01:55,688 - Epoch: [87][   10/   28]    Loss 0.283640    Top1 89.453125    
2023-01-06 17:01:55,797 - Epoch: [87][   20/   28]    Loss 0.284293    Top1 89.667969    
2023-01-06 17:01:55,864 - Epoch: [87][   28/   28]    Loss 0.291753    Top1 89.536215    
2023-01-06 17:01:56,017 - ==> Top1: 89.536    Loss: 0.292

2023-01-06 17:01:56,017 - ==> Confusion:
[[ 208   11  220]
 [  16  206  380]
 [  52   52 5841]]

2023-01-06 17:01:56,018 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:01:56,018 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:01:56,024 - 

2023-01-06 17:01:56,025 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:01:56,593 - Epoch: [88][   10/  246]    Overall Loss 0.293769    Objective Loss 0.293769                                        LR 0.000022    Time 0.056811    
2023-01-06 17:01:56,788 - Epoch: [88][   20/  246]    Overall Loss 0.291471    Objective Loss 0.291471                                        LR 0.000022    Time 0.038105    
2023-01-06 17:01:56,982 - Epoch: [88][   30/  246]    Overall Loss 0.298257    Objective Loss 0.298257                                        LR 0.000022    Time 0.031872    
2023-01-06 17:01:57,178 - Epoch: [88][   40/  246]    Overall Loss 0.295117    Objective Loss 0.295117                                        LR 0.000022    Time 0.028775    
2023-01-06 17:01:57,370 - Epoch: [88][   50/  246]    Overall Loss 0.293320    Objective Loss 0.293320                                        LR 0.000022    Time 0.026859    
2023-01-06 17:01:57,558 - Epoch: [88][   60/  246]    Overall Loss 0.292453    Objective Loss 0.292453                                        LR 0.000022    Time 0.025507    
2023-01-06 17:01:57,757 - Epoch: [88][   70/  246]    Overall Loss 0.288627    Objective Loss 0.288627                                        LR 0.000022    Time 0.024709    
2023-01-06 17:01:57,969 - Epoch: [88][   80/  246]    Overall Loss 0.286213    Objective Loss 0.286213                                        LR 0.000022    Time 0.024262    
2023-01-06 17:01:58,173 - Epoch: [88][   90/  246]    Overall Loss 0.286340    Objective Loss 0.286340                                        LR 0.000022    Time 0.023824    
2023-01-06 17:01:58,379 - Epoch: [88][  100/  246]    Overall Loss 0.286479    Objective Loss 0.286479                                        LR 0.000022    Time 0.023496    
2023-01-06 17:01:58,573 - Epoch: [88][  110/  246]    Overall Loss 0.286662    Objective Loss 0.286662                                        LR 0.000022    Time 0.023122    
2023-01-06 17:01:58,773 - Epoch: [88][  120/  246]    Overall Loss 0.286006    Objective Loss 0.286006                                        LR 0.000022    Time 0.022864    
2023-01-06 17:01:58,970 - Epoch: [88][  130/  246]    Overall Loss 0.284481    Objective Loss 0.284481                                        LR 0.000022    Time 0.022615    
2023-01-06 17:01:59,172 - Epoch: [88][  140/  246]    Overall Loss 0.284846    Objective Loss 0.284846                                        LR 0.000022    Time 0.022436    
2023-01-06 17:01:59,371 - Epoch: [88][  150/  246]    Overall Loss 0.284259    Objective Loss 0.284259                                        LR 0.000022    Time 0.022255    
2023-01-06 17:01:59,571 - Epoch: [88][  160/  246]    Overall Loss 0.284525    Objective Loss 0.284525                                        LR 0.000022    Time 0.022114    
2023-01-06 17:01:59,743 - Epoch: [88][  170/  246]    Overall Loss 0.285509    Objective Loss 0.285509                                        LR 0.000022    Time 0.021824    
2023-01-06 17:01:59,933 - Epoch: [88][  180/  246]    Overall Loss 0.284852    Objective Loss 0.284852                                        LR 0.000022    Time 0.021665    
2023-01-06 17:02:00,108 - Epoch: [88][  190/  246]    Overall Loss 0.285148    Objective Loss 0.285148                                        LR 0.000022    Time 0.021445    
2023-01-06 17:02:00,298 - Epoch: [88][  200/  246]    Overall Loss 0.285678    Objective Loss 0.285678                                        LR 0.000022    Time 0.021319    
2023-01-06 17:02:00,493 - Epoch: [88][  210/  246]    Overall Loss 0.285300    Objective Loss 0.285300                                        LR 0.000022    Time 0.021232    
2023-01-06 17:02:00,693 - Epoch: [88][  220/  246]    Overall Loss 0.285319    Objective Loss 0.285319                                        LR 0.000022    Time 0.021174    
2023-01-06 17:02:00,888 - Epoch: [88][  230/  246]    Overall Loss 0.285249    Objective Loss 0.285249                                        LR 0.000022    Time 0.021100    
2023-01-06 17:02:01,094 - Epoch: [88][  240/  246]    Overall Loss 0.285106    Objective Loss 0.285106                                        LR 0.000022    Time 0.021078    
2023-01-06 17:02:01,192 - Epoch: [88][  246/  246]    Overall Loss 0.285246    Objective Loss 0.285246    Top1 88.995215    LR 0.000022    Time 0.020960    
2023-01-06 17:02:01,345 - --- validate (epoch=88)-----------
2023-01-06 17:02:01,346 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:01,797 - Epoch: [88][   10/   28]    Loss 0.288336    Top1 89.804688    
2023-01-06 17:02:01,910 - Epoch: [88][   20/   28]    Loss 0.281497    Top1 89.804688    
2023-01-06 17:02:01,978 - Epoch: [88][   28/   28]    Loss 0.287946    Top1 89.593473    
2023-01-06 17:02:02,131 - ==> Top1: 89.593    Loss: 0.288

2023-01-06 17:02:02,131 - ==> Confusion:
[[ 222   11  206]
 [  18  209  375]
 [  66   51 5828]]

2023-01-06 17:02:02,132 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:02:02,132 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:02,138 - 

2023-01-06 17:02:02,138 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:02,845 - Epoch: [89][   10/  246]    Overall Loss 0.293060    Objective Loss 0.293060                                        LR 0.000022    Time 0.070604    
2023-01-06 17:02:03,036 - Epoch: [89][   20/  246]    Overall Loss 0.285996    Objective Loss 0.285996                                        LR 0.000022    Time 0.044808    
2023-01-06 17:02:03,226 - Epoch: [89][   30/  246]    Overall Loss 0.291693    Objective Loss 0.291693                                        LR 0.000022    Time 0.036209    
2023-01-06 17:02:03,415 - Epoch: [89][   40/  246]    Overall Loss 0.288588    Objective Loss 0.288588                                        LR 0.000022    Time 0.031873    
2023-01-06 17:02:03,603 - Epoch: [89][   50/  246]    Overall Loss 0.290765    Objective Loss 0.290765                                        LR 0.000022    Time 0.029250    
2023-01-06 17:02:03,789 - Epoch: [89][   60/  246]    Overall Loss 0.287377    Objective Loss 0.287377                                        LR 0.000022    Time 0.027466    
2023-01-06 17:02:03,976 - Epoch: [89][   70/  246]    Overall Loss 0.284019    Objective Loss 0.284019                                        LR 0.000022    Time 0.026203    
2023-01-06 17:02:04,166 - Epoch: [89][   80/  246]    Overall Loss 0.281647    Objective Loss 0.281647                                        LR 0.000022    Time 0.025295    
2023-01-06 17:02:04,356 - Epoch: [89][   90/  246]    Overall Loss 0.284125    Objective Loss 0.284125                                        LR 0.000022    Time 0.024597    
2023-01-06 17:02:04,546 - Epoch: [89][  100/  246]    Overall Loss 0.283561    Objective Loss 0.283561                                        LR 0.000022    Time 0.024035    
2023-01-06 17:02:04,732 - Epoch: [89][  110/  246]    Overall Loss 0.283123    Objective Loss 0.283123                                        LR 0.000022    Time 0.023531    
2023-01-06 17:02:04,925 - Epoch: [89][  120/  246]    Overall Loss 0.282856    Objective Loss 0.282856                                        LR 0.000022    Time 0.023177    
2023-01-06 17:02:05,114 - Epoch: [89][  130/  246]    Overall Loss 0.283578    Objective Loss 0.283578                                        LR 0.000022    Time 0.022846    
2023-01-06 17:02:05,307 - Epoch: [89][  140/  246]    Overall Loss 0.282691    Objective Loss 0.282691                                        LR 0.000022    Time 0.022588    
2023-01-06 17:02:05,499 - Epoch: [89][  150/  246]    Overall Loss 0.282768    Objective Loss 0.282768                                        LR 0.000022    Time 0.022356    
2023-01-06 17:02:05,692 - Epoch: [89][  160/  246]    Overall Loss 0.283133    Objective Loss 0.283133                                        LR 0.000022    Time 0.022161    
2023-01-06 17:02:05,882 - Epoch: [89][  170/  246]    Overall Loss 0.281817    Objective Loss 0.281817                                        LR 0.000022    Time 0.021968    
2023-01-06 17:02:06,076 - Epoch: [89][  180/  246]    Overall Loss 0.282348    Objective Loss 0.282348                                        LR 0.000022    Time 0.021823    
2023-01-06 17:02:06,266 - Epoch: [89][  190/  246]    Overall Loss 0.283269    Objective Loss 0.283269                                        LR 0.000022    Time 0.021672    
2023-01-06 17:02:06,455 - Epoch: [89][  200/  246]    Overall Loss 0.283066    Objective Loss 0.283066                                        LR 0.000022    Time 0.021530    
2023-01-06 17:02:06,641 - Epoch: [89][  210/  246]    Overall Loss 0.283336    Objective Loss 0.283336                                        LR 0.000022    Time 0.021391    
2023-01-06 17:02:06,826 - Epoch: [89][  220/  246]    Overall Loss 0.284196    Objective Loss 0.284196                                        LR 0.000022    Time 0.021257    
2023-01-06 17:02:07,015 - Epoch: [89][  230/  246]    Overall Loss 0.283420    Objective Loss 0.283420                                        LR 0.000022    Time 0.021152    
2023-01-06 17:02:07,217 - Epoch: [89][  240/  246]    Overall Loss 0.283527    Objective Loss 0.283527                                        LR 0.000022    Time 0.021109    
2023-01-06 17:02:07,314 - Epoch: [89][  246/  246]    Overall Loss 0.283191    Objective Loss 0.283191    Top1 91.626794    LR 0.000022    Time 0.020988    
2023-01-06 17:02:07,460 - --- validate (epoch=89)-----------
2023-01-06 17:02:07,461 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:07,931 - Epoch: [89][   10/   28]    Loss 0.287441    Top1 89.296875    
2023-01-06 17:02:08,054 - Epoch: [89][   20/   28]    Loss 0.287263    Top1 89.667969    
2023-01-06 17:02:08,121 - Epoch: [89][   28/   28]    Loss 0.289858    Top1 89.450329    
2023-01-06 17:02:08,271 - ==> Top1: 89.450    Loss: 0.290

2023-01-06 17:02:08,271 - ==> Confusion:
[[ 219    9  211]
 [  19  195  388]
 [  67   43 5835]]

2023-01-06 17:02:08,272 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:02:08,273 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:08,279 - 

2023-01-06 17:02:08,279 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:09,009 - Epoch: [90][   10/  246]    Overall Loss 0.284180    Objective Loss 0.284180                                        LR 0.000022    Time 0.072927    
2023-01-06 17:02:09,215 - Epoch: [90][   20/  246]    Overall Loss 0.297202    Objective Loss 0.297202                                        LR 0.000022    Time 0.046754    
2023-01-06 17:02:09,414 - Epoch: [90][   30/  246]    Overall Loss 0.295891    Objective Loss 0.295891                                        LR 0.000022    Time 0.037794    
2023-01-06 17:02:09,610 - Epoch: [90][   40/  246]    Overall Loss 0.287096    Objective Loss 0.287096                                        LR 0.000022    Time 0.033239    
2023-01-06 17:02:09,806 - Epoch: [90][   50/  246]    Overall Loss 0.286640    Objective Loss 0.286640                                        LR 0.000022    Time 0.030504    
2023-01-06 17:02:10,004 - Epoch: [90][   60/  246]    Overall Loss 0.280276    Objective Loss 0.280276                                        LR 0.000022    Time 0.028719    
2023-01-06 17:02:10,200 - Epoch: [90][   70/  246]    Overall Loss 0.283353    Objective Loss 0.283353                                        LR 0.000022    Time 0.027408    
2023-01-06 17:02:10,403 - Epoch: [90][   80/  246]    Overall Loss 0.281659    Objective Loss 0.281659                                        LR 0.000022    Time 0.026517    
2023-01-06 17:02:10,599 - Epoch: [90][   90/  246]    Overall Loss 0.281203    Objective Loss 0.281203                                        LR 0.000022    Time 0.025742    
2023-01-06 17:02:10,797 - Epoch: [90][  100/  246]    Overall Loss 0.282028    Objective Loss 0.282028                                        LR 0.000022    Time 0.025148    
2023-01-06 17:02:10,990 - Epoch: [90][  110/  246]    Overall Loss 0.280630    Objective Loss 0.280630                                        LR 0.000022    Time 0.024614    
2023-01-06 17:02:11,189 - Epoch: [90][  120/  246]    Overall Loss 0.280469    Objective Loss 0.280469                                        LR 0.000022    Time 0.024212    
2023-01-06 17:02:11,383 - Epoch: [90][  130/  246]    Overall Loss 0.280329    Objective Loss 0.280329                                        LR 0.000022    Time 0.023840    
2023-01-06 17:02:11,581 - Epoch: [90][  140/  246]    Overall Loss 0.282400    Objective Loss 0.282400                                        LR 0.000022    Time 0.023552    
2023-01-06 17:02:11,775 - Epoch: [90][  150/  246]    Overall Loss 0.282500    Objective Loss 0.282500                                        LR 0.000022    Time 0.023274    
2023-01-06 17:02:11,975 - Epoch: [90][  160/  246]    Overall Loss 0.283547    Objective Loss 0.283547                                        LR 0.000022    Time 0.023064    
2023-01-06 17:02:12,172 - Epoch: [90][  170/  246]    Overall Loss 0.283636    Objective Loss 0.283636                                        LR 0.000022    Time 0.022868    
2023-01-06 17:02:12,365 - Epoch: [90][  180/  246]    Overall Loss 0.283612    Objective Loss 0.283612                                        LR 0.000022    Time 0.022667    
2023-01-06 17:02:12,557 - Epoch: [90][  190/  246]    Overall Loss 0.283553    Objective Loss 0.283553                                        LR 0.000022    Time 0.022484    
2023-01-06 17:02:12,750 - Epoch: [90][  200/  246]    Overall Loss 0.282903    Objective Loss 0.282903                                        LR 0.000022    Time 0.022319    
2023-01-06 17:02:12,941 - Epoch: [90][  210/  246]    Overall Loss 0.283120    Objective Loss 0.283120                                        LR 0.000022    Time 0.022167    
2023-01-06 17:02:13,133 - Epoch: [90][  220/  246]    Overall Loss 0.283694    Objective Loss 0.283694                                        LR 0.000022    Time 0.022030    
2023-01-06 17:02:13,327 - Epoch: [90][  230/  246]    Overall Loss 0.283850    Objective Loss 0.283850                                        LR 0.000022    Time 0.021915    
2023-01-06 17:02:13,528 - Epoch: [90][  240/  246]    Overall Loss 0.284232    Objective Loss 0.284232                                        LR 0.000022    Time 0.021835    
2023-01-06 17:02:13,623 - Epoch: [90][  246/  246]    Overall Loss 0.283370    Objective Loss 0.283370    Top1 91.626794    LR 0.000022    Time 0.021690    
2023-01-06 17:02:13,766 - --- validate (epoch=90)-----------
2023-01-06 17:02:13,766 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:14,215 - Epoch: [90][   10/   28]    Loss 0.304150    Top1 88.710938    
2023-01-06 17:02:14,328 - Epoch: [90][   20/   28]    Loss 0.286573    Top1 89.492188    
2023-01-06 17:02:14,397 - Epoch: [90][   28/   28]    Loss 0.286871    Top1 89.307186    
2023-01-06 17:02:14,537 - ==> Top1: 89.307    Loss: 0.287

2023-01-06 17:02:14,537 - ==> Confusion:
[[ 206    9  224]
 [  16  193  393]
 [  60   45 5840]]

2023-01-06 17:02:14,538 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:02:14,539 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:14,544 - 

2023-01-06 17:02:14,545 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:15,109 - Epoch: [91][   10/  246]    Overall Loss 0.282947    Objective Loss 0.282947                                        LR 0.000022    Time 0.056354    
2023-01-06 17:02:15,303 - Epoch: [91][   20/  246]    Overall Loss 0.277811    Objective Loss 0.277811                                        LR 0.000022    Time 0.037874    
2023-01-06 17:02:15,492 - Epoch: [91][   30/  246]    Overall Loss 0.275303    Objective Loss 0.275303                                        LR 0.000022    Time 0.031526    
2023-01-06 17:02:15,680 - Epoch: [91][   40/  246]    Overall Loss 0.277273    Objective Loss 0.277273                                        LR 0.000022    Time 0.028351    
2023-01-06 17:02:15,866 - Epoch: [91][   50/  246]    Overall Loss 0.277523    Objective Loss 0.277523                                        LR 0.000022    Time 0.026392    
2023-01-06 17:02:16,055 - Epoch: [91][   60/  246]    Overall Loss 0.278187    Objective Loss 0.278187                                        LR 0.000022    Time 0.025135    
2023-01-06 17:02:16,242 - Epoch: [91][   70/  246]    Overall Loss 0.275243    Objective Loss 0.275243                                        LR 0.000022    Time 0.024201    
2023-01-06 17:02:16,432 - Epoch: [91][   80/  246]    Overall Loss 0.274518    Objective Loss 0.274518                                        LR 0.000022    Time 0.023547    
2023-01-06 17:02:16,623 - Epoch: [91][   90/  246]    Overall Loss 0.273182    Objective Loss 0.273182                                        LR 0.000022    Time 0.023054    
2023-01-06 17:02:16,815 - Epoch: [91][  100/  246]    Overall Loss 0.274713    Objective Loss 0.274713                                        LR 0.000022    Time 0.022662    
2023-01-06 17:02:17,009 - Epoch: [91][  110/  246]    Overall Loss 0.276657    Objective Loss 0.276657                                        LR 0.000022    Time 0.022367    
2023-01-06 17:02:17,200 - Epoch: [91][  120/  246]    Overall Loss 0.277390    Objective Loss 0.277390                                        LR 0.000022    Time 0.022086    
2023-01-06 17:02:17,392 - Epoch: [91][  130/  246]    Overall Loss 0.277302    Objective Loss 0.277302                                        LR 0.000022    Time 0.021866    
2023-01-06 17:02:17,582 - Epoch: [91][  140/  246]    Overall Loss 0.276758    Objective Loss 0.276758                                        LR 0.000022    Time 0.021655    
2023-01-06 17:02:17,783 - Epoch: [91][  150/  246]    Overall Loss 0.276857    Objective Loss 0.276857                                        LR 0.000022    Time 0.021551    
2023-01-06 17:02:17,981 - Epoch: [91][  160/  246]    Overall Loss 0.277376    Objective Loss 0.277376                                        LR 0.000022    Time 0.021441    
2023-01-06 17:02:18,182 - Epoch: [91][  170/  246]    Overall Loss 0.278858    Objective Loss 0.278858                                        LR 0.000022    Time 0.021358    
2023-01-06 17:02:18,379 - Epoch: [91][  180/  246]    Overall Loss 0.279535    Objective Loss 0.279535                                        LR 0.000022    Time 0.021264    
2023-01-06 17:02:18,578 - Epoch: [91][  190/  246]    Overall Loss 0.280767    Objective Loss 0.280767                                        LR 0.000022    Time 0.021190    
2023-01-06 17:02:18,776 - Epoch: [91][  200/  246]    Overall Loss 0.281248    Objective Loss 0.281248                                        LR 0.000022    Time 0.021120    
2023-01-06 17:02:18,977 - Epoch: [91][  210/  246]    Overall Loss 0.282158    Objective Loss 0.282158                                        LR 0.000022    Time 0.021066    
2023-01-06 17:02:19,177 - Epoch: [91][  220/  246]    Overall Loss 0.282263    Objective Loss 0.282263                                        LR 0.000022    Time 0.021018    
2023-01-06 17:02:19,377 - Epoch: [91][  230/  246]    Overall Loss 0.281572    Objective Loss 0.281572                                        LR 0.000022    Time 0.020972    
2023-01-06 17:02:19,590 - Epoch: [91][  240/  246]    Overall Loss 0.282132    Objective Loss 0.282132                                        LR 0.000022    Time 0.020982    
2023-01-06 17:02:19,686 - Epoch: [91][  246/  246]    Overall Loss 0.282178    Objective Loss 0.282178    Top1 88.995215    LR 0.000022    Time 0.020862    
2023-01-06 17:02:19,830 - --- validate (epoch=91)-----------
2023-01-06 17:02:19,830 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:20,288 - Epoch: [91][   10/   28]    Loss 0.282274    Top1 89.531250    
2023-01-06 17:02:20,403 - Epoch: [91][   20/   28]    Loss 0.280456    Top1 89.726562    
2023-01-06 17:02:20,470 - Epoch: [91][   28/   28]    Loss 0.287486    Top1 89.478958    
2023-01-06 17:02:20,626 - ==> Top1: 89.479    Loss: 0.287

2023-01-06 17:02:20,626 - ==> Confusion:
[[ 204   11  224]
 [  11  219  372]
 [  47   70 5828]]

2023-01-06 17:02:20,627 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:02:20,627 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:20,633 - 

2023-01-06 17:02:20,633 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:21,356 - Epoch: [92][   10/  246]    Overall Loss 0.281166    Objective Loss 0.281166                                        LR 0.000022    Time 0.072188    
2023-01-06 17:02:21,543 - Epoch: [92][   20/  246]    Overall Loss 0.284702    Objective Loss 0.284702                                        LR 0.000022    Time 0.045409    
2023-01-06 17:02:21,734 - Epoch: [92][   30/  246]    Overall Loss 0.282869    Objective Loss 0.282869                                        LR 0.000022    Time 0.036643    
2023-01-06 17:02:21,927 - Epoch: [92][   40/  246]    Overall Loss 0.283594    Objective Loss 0.283594                                        LR 0.000022    Time 0.032287    
2023-01-06 17:02:22,120 - Epoch: [92][   50/  246]    Overall Loss 0.286267    Objective Loss 0.286267                                        LR 0.000022    Time 0.029682    
2023-01-06 17:02:22,312 - Epoch: [92][   60/  246]    Overall Loss 0.287099    Objective Loss 0.287099                                        LR 0.000022    Time 0.027942    
2023-01-06 17:02:22,502 - Epoch: [92][   70/  246]    Overall Loss 0.285929    Objective Loss 0.285929                                        LR 0.000022    Time 0.026650    
2023-01-06 17:02:22,697 - Epoch: [92][   80/  246]    Overall Loss 0.285686    Objective Loss 0.285686                                        LR 0.000022    Time 0.025754    
2023-01-06 17:02:22,892 - Epoch: [92][   90/  246]    Overall Loss 0.283985    Objective Loss 0.283985                                        LR 0.000022    Time 0.025058    
2023-01-06 17:02:23,090 - Epoch: [92][  100/  246]    Overall Loss 0.283585    Objective Loss 0.283585                                        LR 0.000022    Time 0.024527    
2023-01-06 17:02:23,291 - Epoch: [92][  110/  246]    Overall Loss 0.283973    Objective Loss 0.283973                                        LR 0.000022    Time 0.024117    
2023-01-06 17:02:23,489 - Epoch: [92][  120/  246]    Overall Loss 0.283483    Objective Loss 0.283483                                        LR 0.000022    Time 0.023755    
2023-01-06 17:02:23,688 - Epoch: [92][  130/  246]    Overall Loss 0.284102    Objective Loss 0.284102                                        LR 0.000022    Time 0.023456    
2023-01-06 17:02:23,880 - Epoch: [92][  140/  246]    Overall Loss 0.284984    Objective Loss 0.284984                                        LR 0.000022    Time 0.023148    
2023-01-06 17:02:24,067 - Epoch: [92][  150/  246]    Overall Loss 0.283780    Objective Loss 0.283780                                        LR 0.000022    Time 0.022849    
2023-01-06 17:02:24,257 - Epoch: [92][  160/  246]    Overall Loss 0.283056    Objective Loss 0.283056                                        LR 0.000022    Time 0.022607    
2023-01-06 17:02:24,446 - Epoch: [92][  170/  246]    Overall Loss 0.284295    Objective Loss 0.284295                                        LR 0.000022    Time 0.022390    
2023-01-06 17:02:24,638 - Epoch: [92][  180/  246]    Overall Loss 0.284279    Objective Loss 0.284279                                        LR 0.000022    Time 0.022208    
2023-01-06 17:02:24,824 - Epoch: [92][  190/  246]    Overall Loss 0.284205    Objective Loss 0.284205                                        LR 0.000022    Time 0.022016    
2023-01-06 17:02:25,014 - Epoch: [92][  200/  246]    Overall Loss 0.282184    Objective Loss 0.282184                                        LR 0.000022    Time 0.021863    
2023-01-06 17:02:25,205 - Epoch: [92][  210/  246]    Overall Loss 0.281798    Objective Loss 0.281798                                        LR 0.000022    Time 0.021729    
2023-01-06 17:02:25,392 - Epoch: [92][  220/  246]    Overall Loss 0.281692    Objective Loss 0.281692                                        LR 0.000022    Time 0.021592    
2023-01-06 17:02:25,581 - Epoch: [92][  230/  246]    Overall Loss 0.281260    Objective Loss 0.281260                                        LR 0.000022    Time 0.021462    
2023-01-06 17:02:25,781 - Epoch: [92][  240/  246]    Overall Loss 0.280774    Objective Loss 0.280774                                        LR 0.000022    Time 0.021400    
2023-01-06 17:02:25,876 - Epoch: [92][  246/  246]    Overall Loss 0.281101    Objective Loss 0.281101    Top1 90.191388    LR 0.000022    Time 0.021263    
2023-01-06 17:02:26,057 - --- validate (epoch=92)-----------
2023-01-06 17:02:26,058 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:26,512 - Epoch: [92][   10/   28]    Loss 0.275858    Top1 90.507812    
2023-01-06 17:02:26,627 - Epoch: [92][   20/   28]    Loss 0.283143    Top1 89.648438    
2023-01-06 17:02:26,694 - Epoch: [92][   28/   28]    Loss 0.284781    Top1 89.579158    
2023-01-06 17:02:26,822 - ==> Top1: 89.579    Loss: 0.285

2023-01-06 17:02:26,822 - ==> Confusion:
[[ 199    8  232]
 [  16  219  367]
 [  45   60 5840]]

2023-01-06 17:02:26,824 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 155168 on epoch: 82]
2023-01-06 17:02:26,824 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:26,830 - 

2023-01-06 17:02:26,830 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:27,409 - Epoch: [93][   10/  246]    Overall Loss 0.291453    Objective Loss 0.291453                                        LR 0.000022    Time 0.057871    
2023-01-06 17:02:27,595 - Epoch: [93][   20/  246]    Overall Loss 0.300431    Objective Loss 0.300431                                        LR 0.000022    Time 0.038204    
2023-01-06 17:02:27,793 - Epoch: [93][   30/  246]    Overall Loss 0.298012    Objective Loss 0.298012                                        LR 0.000022    Time 0.032048    
2023-01-06 17:02:27,994 - Epoch: [93][   40/  246]    Overall Loss 0.299249    Objective Loss 0.299249                                        LR 0.000022    Time 0.029071    
2023-01-06 17:02:28,189 - Epoch: [93][   50/  246]    Overall Loss 0.291889    Objective Loss 0.291889                                        LR 0.000022    Time 0.027150    
2023-01-06 17:02:28,377 - Epoch: [93][   60/  246]    Overall Loss 0.290562    Objective Loss 0.290562                                        LR 0.000022    Time 0.025751    
2023-01-06 17:02:28,564 - Epoch: [93][   70/  246]    Overall Loss 0.289961    Objective Loss 0.289961                                        LR 0.000022    Time 0.024738    
2023-01-06 17:02:28,758 - Epoch: [93][   80/  246]    Overall Loss 0.287788    Objective Loss 0.287788                                        LR 0.000022    Time 0.024062    
2023-01-06 17:02:28,958 - Epoch: [93][   90/  246]    Overall Loss 0.287891    Objective Loss 0.287891                                        LR 0.000022    Time 0.023605    
2023-01-06 17:02:29,158 - Epoch: [93][  100/  246]    Overall Loss 0.287472    Objective Loss 0.287472                                        LR 0.000022    Time 0.023241    
2023-01-06 17:02:29,348 - Epoch: [93][  110/  246]    Overall Loss 0.286261    Objective Loss 0.286261                                        LR 0.000022    Time 0.022858    
2023-01-06 17:02:29,540 - Epoch: [93][  120/  246]    Overall Loss 0.284963    Objective Loss 0.284963                                        LR 0.000022    Time 0.022543    
2023-01-06 17:02:29,730 - Epoch: [93][  130/  246]    Overall Loss 0.284674    Objective Loss 0.284674                                        LR 0.000022    Time 0.022274    
2023-01-06 17:02:29,922 - Epoch: [93][  140/  246]    Overall Loss 0.284755    Objective Loss 0.284755                                        LR 0.000022    Time 0.022051    
2023-01-06 17:02:30,117 - Epoch: [93][  150/  246]    Overall Loss 0.285349    Objective Loss 0.285349                                        LR 0.000022    Time 0.021878    
2023-01-06 17:02:30,308 - Epoch: [93][  160/  246]    Overall Loss 0.285331    Objective Loss 0.285331                                        LR 0.000022    Time 0.021700    
2023-01-06 17:02:30,495 - Epoch: [93][  170/  246]    Overall Loss 0.285475    Objective Loss 0.285475                                        LR 0.000022    Time 0.021519    
2023-01-06 17:02:30,686 - Epoch: [93][  180/  246]    Overall Loss 0.285755    Objective Loss 0.285755                                        LR 0.000022    Time 0.021383    
2023-01-06 17:02:30,873 - Epoch: [93][  190/  246]    Overall Loss 0.285766    Objective Loss 0.285766                                        LR 0.000022    Time 0.021244    
2023-01-06 17:02:31,065 - Epoch: [93][  200/  246]    Overall Loss 0.284838    Objective Loss 0.284838                                        LR 0.000022    Time 0.021138    
2023-01-06 17:02:31,246 - Epoch: [93][  210/  246]    Overall Loss 0.284722    Objective Loss 0.284722                                        LR 0.000022    Time 0.020993    
2023-01-06 17:02:31,438 - Epoch: [93][  220/  246]    Overall Loss 0.283763    Objective Loss 0.283763                                        LR 0.000022    Time 0.020909    
2023-01-06 17:02:31,627 - Epoch: [93][  230/  246]    Overall Loss 0.283184    Objective Loss 0.283184                                        LR 0.000022    Time 0.020817    
2023-01-06 17:02:31,831 - Epoch: [93][  240/  246]    Overall Loss 0.283207    Objective Loss 0.283207                                        LR 0.000022    Time 0.020799    
2023-01-06 17:02:31,926 - Epoch: [93][  246/  246]    Overall Loss 0.282565    Objective Loss 0.282565    Top1 91.387560    LR 0.000022    Time 0.020679    
2023-01-06 17:02:32,067 - --- validate (epoch=93)-----------
2023-01-06 17:02:32,067 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:32,513 - Epoch: [93][   10/   28]    Loss 0.296662    Top1 88.984375    
2023-01-06 17:02:32,625 - Epoch: [93][   20/   28]    Loss 0.284593    Top1 89.707031    
2023-01-06 17:02:32,692 - Epoch: [93][   28/   28]    Loss 0.284190    Top1 89.707987    
2023-01-06 17:02:32,825 - ==> Top1: 89.708    Loss: 0.284

2023-01-06 17:02:32,825 - ==> Confusion:
[[ 208   12  219]
 [  13  238  351]
 [  55   69 5821]]

2023-01-06 17:02:32,826 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 155168 on epoch: 93]
2023-01-06 17:02:32,826 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:32,833 - 

2023-01-06 17:02:32,834 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:33,530 - Epoch: [94][   10/  246]    Overall Loss 0.272637    Objective Loss 0.272637                                        LR 0.000022    Time 0.069531    
2023-01-06 17:02:33,699 - Epoch: [94][   20/  246]    Overall Loss 0.283901    Objective Loss 0.283901                                        LR 0.000022    Time 0.043237    
2023-01-06 17:02:33,903 - Epoch: [94][   30/  246]    Overall Loss 0.281255    Objective Loss 0.281255                                        LR 0.000022    Time 0.035585    
2023-01-06 17:02:34,118 - Epoch: [94][   40/  246]    Overall Loss 0.281400    Objective Loss 0.281400                                        LR 0.000022    Time 0.032055    
2023-01-06 17:02:34,361 - Epoch: [94][   50/  246]    Overall Loss 0.277981    Objective Loss 0.277981                                        LR 0.000022    Time 0.030493    
2023-01-06 17:02:34,609 - Epoch: [94][   60/  246]    Overall Loss 0.276852    Objective Loss 0.276852                                        LR 0.000022    Time 0.029533    
2023-01-06 17:02:34,852 - Epoch: [94][   70/  246]    Overall Loss 0.276532    Objective Loss 0.276532                                        LR 0.000022    Time 0.028787    
2023-01-06 17:02:35,093 - Epoch: [94][   80/  246]    Overall Loss 0.275369    Objective Loss 0.275369                                        LR 0.000022    Time 0.028187    
2023-01-06 17:02:35,343 - Epoch: [94][   90/  246]    Overall Loss 0.274528    Objective Loss 0.274528                                        LR 0.000022    Time 0.027821    
2023-01-06 17:02:35,583 - Epoch: [94][  100/  246]    Overall Loss 0.274066    Objective Loss 0.274066                                        LR 0.000022    Time 0.027434    
2023-01-06 17:02:35,824 - Epoch: [94][  110/  246]    Overall Loss 0.275160    Objective Loss 0.275160                                        LR 0.000022    Time 0.027113    
2023-01-06 17:02:36,065 - Epoch: [94][  120/  246]    Overall Loss 0.275569    Objective Loss 0.275569                                        LR 0.000022    Time 0.026851    
2023-01-06 17:02:36,306 - Epoch: [94][  130/  246]    Overall Loss 0.277033    Objective Loss 0.277033                                        LR 0.000022    Time 0.026629    
2023-01-06 17:02:36,548 - Epoch: [94][  140/  246]    Overall Loss 0.278874    Objective Loss 0.278874                                        LR 0.000022    Time 0.026455    
2023-01-06 17:02:36,798 - Epoch: [94][  150/  246]    Overall Loss 0.277902    Objective Loss 0.277902                                        LR 0.000022    Time 0.026354    
2023-01-06 17:02:37,044 - Epoch: [94][  160/  246]    Overall Loss 0.278402    Objective Loss 0.278402                                        LR 0.000022    Time 0.026235    
2023-01-06 17:02:37,271 - Epoch: [94][  170/  246]    Overall Loss 0.277982    Objective Loss 0.277982                                        LR 0.000022    Time 0.026018    
2023-01-06 17:02:37,487 - Epoch: [94][  180/  246]    Overall Loss 0.279243    Objective Loss 0.279243                                        LR 0.000022    Time 0.025769    
2023-01-06 17:02:37,704 - Epoch: [94][  190/  246]    Overall Loss 0.279269    Objective Loss 0.279269                                        LR 0.000022    Time 0.025552    
2023-01-06 17:02:37,922 - Epoch: [94][  200/  246]    Overall Loss 0.280283    Objective Loss 0.280283                                        LR 0.000022    Time 0.025362    
2023-01-06 17:02:38,143 - Epoch: [94][  210/  246]    Overall Loss 0.280215    Objective Loss 0.280215                                        LR 0.000022    Time 0.025205    
2023-01-06 17:02:38,364 - Epoch: [94][  220/  246]    Overall Loss 0.279861    Objective Loss 0.279861                                        LR 0.000022    Time 0.025061    
2023-01-06 17:02:38,587 - Epoch: [94][  230/  246]    Overall Loss 0.279737    Objective Loss 0.279737                                        LR 0.000022    Time 0.024938    
2023-01-06 17:02:38,824 - Epoch: [94][  240/  246]    Overall Loss 0.279967    Objective Loss 0.279967                                        LR 0.000022    Time 0.024887    
2023-01-06 17:02:38,937 - Epoch: [94][  246/  246]    Overall Loss 0.280182    Objective Loss 0.280182    Top1 89.234450    LR 0.000022    Time 0.024738    
2023-01-06 17:02:39,084 - --- validate (epoch=94)-----------
2023-01-06 17:02:39,084 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:39,530 - Epoch: [94][   10/   28]    Loss 0.292638    Top1 89.257812    
2023-01-06 17:02:39,645 - Epoch: [94][   20/   28]    Loss 0.288771    Top1 89.472656    
2023-01-06 17:02:39,715 - Epoch: [94][   28/   28]    Loss 0.283733    Top1 89.622101    
2023-01-06 17:02:39,872 - ==> Top1: 89.622    Loss: 0.284

2023-01-06 17:02:39,872 - ==> Confusion:
[[ 208    8  223]
 [  17  220  365]
 [  51   61 5833]]

2023-01-06 17:02:39,873 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 155168 on epoch: 93]
2023-01-06 17:02:39,874 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:39,879 - 

2023-01-06 17:02:39,879 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:40,600 - Epoch: [95][   10/  246]    Overall Loss 0.279805    Objective Loss 0.279805                                        LR 0.000022    Time 0.071943    
2023-01-06 17:02:40,792 - Epoch: [95][   20/  246]    Overall Loss 0.281675    Objective Loss 0.281675                                        LR 0.000022    Time 0.045592    
2023-01-06 17:02:40,986 - Epoch: [95][   30/  246]    Overall Loss 0.284004    Objective Loss 0.284004                                        LR 0.000022    Time 0.036826    
2023-01-06 17:02:41,177 - Epoch: [95][   40/  246]    Overall Loss 0.286344    Objective Loss 0.286344                                        LR 0.000022    Time 0.032390    
2023-01-06 17:02:41,372 - Epoch: [95][   50/  246]    Overall Loss 0.284977    Objective Loss 0.284977                                        LR 0.000022    Time 0.029799    
2023-01-06 17:02:41,572 - Epoch: [95][   60/  246]    Overall Loss 0.282819    Objective Loss 0.282819                                        LR 0.000022    Time 0.028173    
2023-01-06 17:02:41,783 - Epoch: [95][   70/  246]    Overall Loss 0.281422    Objective Loss 0.281422                                        LR 0.000022    Time 0.027149    
2023-01-06 17:02:42,020 - Epoch: [95][   80/  246]    Overall Loss 0.284959    Objective Loss 0.284959                                        LR 0.000022    Time 0.026711    
2023-01-06 17:02:42,234 - Epoch: [95][   90/  246]    Overall Loss 0.283198    Objective Loss 0.283198                                        LR 0.000022    Time 0.026114    
2023-01-06 17:02:42,463 - Epoch: [95][  100/  246]    Overall Loss 0.283757    Objective Loss 0.283757                                        LR 0.000022    Time 0.025786    
2023-01-06 17:02:42,691 - Epoch: [95][  110/  246]    Overall Loss 0.281439    Objective Loss 0.281439                                        LR 0.000022    Time 0.025511    
2023-01-06 17:02:42,922 - Epoch: [95][  120/  246]    Overall Loss 0.279324    Objective Loss 0.279324                                        LR 0.000022    Time 0.025308    
2023-01-06 17:02:43,152 - Epoch: [95][  130/  246]    Overall Loss 0.278672    Objective Loss 0.278672                                        LR 0.000022    Time 0.025127    
2023-01-06 17:02:43,382 - Epoch: [95][  140/  246]    Overall Loss 0.278085    Objective Loss 0.278085                                        LR 0.000022    Time 0.024975    
2023-01-06 17:02:43,616 - Epoch: [95][  150/  246]    Overall Loss 0.278406    Objective Loss 0.278406                                        LR 0.000022    Time 0.024863    
2023-01-06 17:02:43,824 - Epoch: [95][  160/  246]    Overall Loss 0.278378    Objective Loss 0.278378                                        LR 0.000022    Time 0.024607    
2023-01-06 17:02:44,016 - Epoch: [95][  170/  246]    Overall Loss 0.278344    Objective Loss 0.278344                                        LR 0.000022    Time 0.024289    
2023-01-06 17:02:44,213 - Epoch: [95][  180/  246]    Overall Loss 0.277701    Objective Loss 0.277701                                        LR 0.000022    Time 0.024031    
2023-01-06 17:02:44,414 - Epoch: [95][  190/  246]    Overall Loss 0.277948    Objective Loss 0.277948                                        LR 0.000022    Time 0.023823    
2023-01-06 17:02:44,617 - Epoch: [95][  200/  246]    Overall Loss 0.277959    Objective Loss 0.277959                                        LR 0.000022    Time 0.023644    
2023-01-06 17:02:44,815 - Epoch: [95][  210/  246]    Overall Loss 0.278615    Objective Loss 0.278615                                        LR 0.000022    Time 0.023460    
2023-01-06 17:02:44,999 - Epoch: [95][  220/  246]    Overall Loss 0.278588    Objective Loss 0.278588                                        LR 0.000022    Time 0.023226    
2023-01-06 17:02:45,176 - Epoch: [95][  230/  246]    Overall Loss 0.279322    Objective Loss 0.279322                                        LR 0.000022    Time 0.022984    
2023-01-06 17:02:45,369 - Epoch: [95][  240/  246]    Overall Loss 0.279625    Objective Loss 0.279625                                        LR 0.000022    Time 0.022830    
2023-01-06 17:02:45,466 - Epoch: [95][  246/  246]    Overall Loss 0.280061    Objective Loss 0.280061    Top1 87.799043    LR 0.000022    Time 0.022666    
2023-01-06 17:02:45,601 - --- validate (epoch=95)-----------
2023-01-06 17:02:45,601 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:46,049 - Epoch: [95][   10/   28]    Loss 0.260828    Top1 91.015625    
2023-01-06 17:02:46,161 - Epoch: [95][   20/   28]    Loss 0.276892    Top1 89.843750    
2023-01-06 17:02:46,229 - Epoch: [95][   28/   28]    Loss 0.282856    Top1 89.478958    
2023-01-06 17:02:46,388 - ==> Top1: 89.479    Loss: 0.283

2023-01-06 17:02:46,389 - ==> Confusion:
[[ 193   11  235]
 [  15  215  372]
 [  38   64 5843]]

2023-01-06 17:02:46,390 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 155168 on epoch: 93]
2023-01-06 17:02:46,390 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:46,396 - 

2023-01-06 17:02:46,396 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:46,979 - Epoch: [96][   10/  246]    Overall Loss 0.250892    Objective Loss 0.250892                                        LR 0.000022    Time 0.058175    
2023-01-06 17:02:47,167 - Epoch: [96][   20/  246]    Overall Loss 0.269539    Objective Loss 0.269539                                        LR 0.000022    Time 0.038458    
2023-01-06 17:02:47,361 - Epoch: [96][   30/  246]    Overall Loss 0.268008    Objective Loss 0.268008                                        LR 0.000022    Time 0.032109    
2023-01-06 17:02:47,554 - Epoch: [96][   40/  246]    Overall Loss 0.267530    Objective Loss 0.267530                                        LR 0.000022    Time 0.028894    
2023-01-06 17:02:47,748 - Epoch: [96][   50/  246]    Overall Loss 0.270161    Objective Loss 0.270161                                        LR 0.000022    Time 0.026981    
2023-01-06 17:02:47,940 - Epoch: [96][   60/  246]    Overall Loss 0.271106    Objective Loss 0.271106                                        LR 0.000022    Time 0.025665    
2023-01-06 17:02:48,134 - Epoch: [96][   70/  246]    Overall Loss 0.273570    Objective Loss 0.273570                                        LR 0.000022    Time 0.024757    
2023-01-06 17:02:48,322 - Epoch: [96][   80/  246]    Overall Loss 0.274769    Objective Loss 0.274769                                        LR 0.000022    Time 0.024016    
2023-01-06 17:02:48,514 - Epoch: [96][   90/  246]    Overall Loss 0.276069    Objective Loss 0.276069                                        LR 0.000022    Time 0.023479    
2023-01-06 17:02:48,711 - Epoch: [96][  100/  246]    Overall Loss 0.280577    Objective Loss 0.280577                                        LR 0.000022    Time 0.023093    
2023-01-06 17:02:48,907 - Epoch: [96][  110/  246]    Overall Loss 0.280300    Objective Loss 0.280300                                        LR 0.000022    Time 0.022773    
2023-01-06 17:02:49,112 - Epoch: [96][  120/  246]    Overall Loss 0.281288    Objective Loss 0.281288                                        LR 0.000022    Time 0.022580    
2023-01-06 17:02:49,336 - Epoch: [96][  130/  246]    Overall Loss 0.281633    Objective Loss 0.281633                                        LR 0.000022    Time 0.022558    
2023-01-06 17:02:49,574 - Epoch: [96][  140/  246]    Overall Loss 0.281575    Objective Loss 0.281575                                        LR 0.000022    Time 0.022650    
2023-01-06 17:02:49,805 - Epoch: [96][  150/  246]    Overall Loss 0.281353    Objective Loss 0.281353                                        LR 0.000022    Time 0.022674    
2023-01-06 17:02:50,035 - Epoch: [96][  160/  246]    Overall Loss 0.280640    Objective Loss 0.280640                                        LR 0.000022    Time 0.022691    
2023-01-06 17:02:50,235 - Epoch: [96][  170/  246]    Overall Loss 0.281116    Objective Loss 0.281116                                        LR 0.000022    Time 0.022531    
2023-01-06 17:02:50,437 - Epoch: [96][  180/  246]    Overall Loss 0.280020    Objective Loss 0.280020                                        LR 0.000022    Time 0.022397    
2023-01-06 17:02:50,632 - Epoch: [96][  190/  246]    Overall Loss 0.280705    Objective Loss 0.280705                                        LR 0.000022    Time 0.022243    
2023-01-06 17:02:50,833 - Epoch: [96][  200/  246]    Overall Loss 0.280094    Objective Loss 0.280094                                        LR 0.000022    Time 0.022134    
2023-01-06 17:02:51,032 - Epoch: [96][  210/  246]    Overall Loss 0.279832    Objective Loss 0.279832                                        LR 0.000022    Time 0.022029    
2023-01-06 17:02:51,228 - Epoch: [96][  220/  246]    Overall Loss 0.279455    Objective Loss 0.279455                                        LR 0.000022    Time 0.021913    
2023-01-06 17:02:51,424 - Epoch: [96][  230/  246]    Overall Loss 0.279682    Objective Loss 0.279682                                        LR 0.000022    Time 0.021814    
2023-01-06 17:02:51,625 - Epoch: [96][  240/  246]    Overall Loss 0.279300    Objective Loss 0.279300                                        LR 0.000022    Time 0.021741    
2023-01-06 17:02:51,722 - Epoch: [96][  246/  246]    Overall Loss 0.279194    Objective Loss 0.279194    Top1 91.626794    LR 0.000022    Time 0.021603    
2023-01-06 17:02:51,851 - --- validate (epoch=96)-----------
2023-01-06 17:02:51,851 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:52,306 - Epoch: [96][   10/   28]    Loss 0.278965    Top1 90.625000    
2023-01-06 17:02:52,438 - Epoch: [96][   20/   28]    Loss 0.288869    Top1 89.667969    
2023-01-06 17:02:52,505 - Epoch: [96][   28/   28]    Loss 0.295094    Top1 89.378758    
2023-01-06 17:02:52,665 - ==> Top1: 89.379    Loss: 0.295

2023-01-06 17:02:52,666 - ==> Confusion:
[[ 188   16  235]
 [   8  246  348]
 [  49   86 5810]]

2023-01-06 17:02:52,667 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 155168 on epoch: 93]
2023-01-06 17:02:52,667 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:52,673 - 

2023-01-06 17:02:52,673 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:02:53,357 - Epoch: [97][   10/  246]    Overall Loss 0.277865    Objective Loss 0.277865                                        LR 0.000022    Time 0.068313    
2023-01-06 17:02:53,541 - Epoch: [97][   20/  246]    Overall Loss 0.285068    Objective Loss 0.285068                                        LR 0.000022    Time 0.043332    
2023-01-06 17:02:53,735 - Epoch: [97][   30/  246]    Overall Loss 0.283303    Objective Loss 0.283303                                        LR 0.000022    Time 0.035334    
2023-01-06 17:02:53,935 - Epoch: [97][   40/  246]    Overall Loss 0.282925    Objective Loss 0.282925                                        LR 0.000022    Time 0.031498    
2023-01-06 17:02:54,138 - Epoch: [97][   50/  246]    Overall Loss 0.277554    Objective Loss 0.277554                                        LR 0.000022    Time 0.029248    
2023-01-06 17:02:54,346 - Epoch: [97][   60/  246]    Overall Loss 0.276031    Objective Loss 0.276031                                        LR 0.000022    Time 0.027834    
2023-01-06 17:02:54,545 - Epoch: [97][   70/  246]    Overall Loss 0.279075    Objective Loss 0.279075                                        LR 0.000022    Time 0.026698    
2023-01-06 17:02:54,751 - Epoch: [97][   80/  246]    Overall Loss 0.277511    Objective Loss 0.277511                                        LR 0.000022    Time 0.025933    
2023-01-06 17:02:54,981 - Epoch: [97][   90/  246]    Overall Loss 0.280158    Objective Loss 0.280158                                        LR 0.000022    Time 0.025603    
2023-01-06 17:02:55,209 - Epoch: [97][  100/  246]    Overall Loss 0.278094    Objective Loss 0.278094                                        LR 0.000022    Time 0.025311    
2023-01-06 17:02:55,441 - Epoch: [97][  110/  246]    Overall Loss 0.279379    Objective Loss 0.279379                                        LR 0.000022    Time 0.025119    
2023-01-06 17:02:55,668 - Epoch: [97][  120/  246]    Overall Loss 0.277813    Objective Loss 0.277813                                        LR 0.000022    Time 0.024916    
2023-01-06 17:02:55,900 - Epoch: [97][  130/  246]    Overall Loss 0.278292    Objective Loss 0.278292                                        LR 0.000022    Time 0.024782    
2023-01-06 17:02:56,120 - Epoch: [97][  140/  246]    Overall Loss 0.279191    Objective Loss 0.279191                                        LR 0.000022    Time 0.024574    
2023-01-06 17:02:56,350 - Epoch: [97][  150/  246]    Overall Loss 0.278301    Objective Loss 0.278301                                        LR 0.000022    Time 0.024469    
2023-01-06 17:02:56,579 - Epoch: [97][  160/  246]    Overall Loss 0.278481    Objective Loss 0.278481                                        LR 0.000022    Time 0.024370    
2023-01-06 17:02:56,810 - Epoch: [97][  170/  246]    Overall Loss 0.278301    Objective Loss 0.278301                                        LR 0.000022    Time 0.024289    
2023-01-06 17:02:57,040 - Epoch: [97][  180/  246]    Overall Loss 0.278042    Objective Loss 0.278042                                        LR 0.000022    Time 0.024218    
2023-01-06 17:02:57,278 - Epoch: [97][  190/  246]    Overall Loss 0.278164    Objective Loss 0.278164                                        LR 0.000022    Time 0.024192    
2023-01-06 17:02:57,516 - Epoch: [97][  200/  246]    Overall Loss 0.278542    Objective Loss 0.278542                                        LR 0.000022    Time 0.024171    
2023-01-06 17:02:57,751 - Epoch: [97][  210/  246]    Overall Loss 0.279140    Objective Loss 0.279140                                        LR 0.000022    Time 0.024134    
2023-01-06 17:02:57,986 - Epoch: [97][  220/  246]    Overall Loss 0.278600    Objective Loss 0.278600                                        LR 0.000022    Time 0.024103    
2023-01-06 17:02:58,220 - Epoch: [97][  230/  246]    Overall Loss 0.278414    Objective Loss 0.278414                                        LR 0.000022    Time 0.024073    
2023-01-06 17:02:58,427 - Epoch: [97][  240/  246]    Overall Loss 0.278519    Objective Loss 0.278519                                        LR 0.000022    Time 0.023929    
2023-01-06 17:02:58,516 - Epoch: [97][  246/  246]    Overall Loss 0.278494    Objective Loss 0.278494    Top1 88.755981    LR 0.000022    Time 0.023707    
2023-01-06 17:02:58,649 - --- validate (epoch=97)-----------
2023-01-06 17:02:58,649 - 6986 samples (256 per mini-batch)
2023-01-06 17:02:59,109 - Epoch: [97][   10/   28]    Loss 0.296294    Top1 89.257812    
2023-01-06 17:02:59,233 - Epoch: [97][   20/   28]    Loss 0.300404    Top1 88.730469    
2023-01-06 17:02:59,300 - Epoch: [97][   28/   28]    Loss 0.281790    Top1 89.493272    
2023-01-06 17:02:59,446 - ==> Top1: 89.493    Loss: 0.282

2023-01-06 17:02:59,447 - ==> Confusion:
[[ 203   10  226]
 [  13  203  386]
 [  50   49 5846]]

2023-01-06 17:02:59,448 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 155168 on epoch: 93]
2023-01-06 17:02:59,448 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:02:59,454 - 

2023-01-06 17:02:59,454 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:00,001 - Epoch: [98][   10/  246]    Overall Loss 0.260625    Objective Loss 0.260625                                        LR 0.000022    Time 0.054638    
2023-01-06 17:03:00,161 - Epoch: [98][   20/  246]    Overall Loss 0.275459    Objective Loss 0.275459                                        LR 0.000022    Time 0.035277    
2023-01-06 17:03:00,330 - Epoch: [98][   30/  246]    Overall Loss 0.278793    Objective Loss 0.278793                                        LR 0.000022    Time 0.029141    
2023-01-06 17:03:00,503 - Epoch: [98][   40/  246]    Overall Loss 0.274949    Objective Loss 0.274949                                        LR 0.000022    Time 0.026191    
2023-01-06 17:03:00,678 - Epoch: [98][   50/  246]    Overall Loss 0.274288    Objective Loss 0.274288                                        LR 0.000022    Time 0.024427    
2023-01-06 17:03:00,845 - Epoch: [98][   60/  246]    Overall Loss 0.277561    Objective Loss 0.277561                                        LR 0.000022    Time 0.023144    
2023-01-06 17:03:01,011 - Epoch: [98][   70/  246]    Overall Loss 0.276741    Objective Loss 0.276741                                        LR 0.000022    Time 0.022209    
2023-01-06 17:03:01,180 - Epoch: [98][   80/  246]    Overall Loss 0.275699    Objective Loss 0.275699                                        LR 0.000022    Time 0.021535    
2023-01-06 17:03:01,354 - Epoch: [98][   90/  246]    Overall Loss 0.274246    Objective Loss 0.274246                                        LR 0.000022    Time 0.021069    
2023-01-06 17:03:01,531 - Epoch: [98][  100/  246]    Overall Loss 0.275123    Objective Loss 0.275123                                        LR 0.000022    Time 0.020729    
2023-01-06 17:03:01,696 - Epoch: [98][  110/  246]    Overall Loss 0.275827    Objective Loss 0.275827                                        LR 0.000022    Time 0.020341    
2023-01-06 17:03:01,861 - Epoch: [98][  120/  246]    Overall Loss 0.275341    Objective Loss 0.275341                                        LR 0.000022    Time 0.020024    
2023-01-06 17:03:02,031 - Epoch: [98][  130/  246]    Overall Loss 0.276391    Objective Loss 0.276391                                        LR 0.000022    Time 0.019788    
2023-01-06 17:03:02,199 - Epoch: [98][  140/  246]    Overall Loss 0.275042    Objective Loss 0.275042                                        LR 0.000022    Time 0.019574    
2023-01-06 17:03:02,377 - Epoch: [98][  150/  246]    Overall Loss 0.276496    Objective Loss 0.276496                                        LR 0.000022    Time 0.019452    
2023-01-06 17:03:02,550 - Epoch: [98][  160/  246]    Overall Loss 0.276055    Objective Loss 0.276055                                        LR 0.000022    Time 0.019314    
2023-01-06 17:03:02,724 - Epoch: [98][  170/  246]    Overall Loss 0.277186    Objective Loss 0.277186                                        LR 0.000022    Time 0.019200    
2023-01-06 17:03:02,890 - Epoch: [98][  180/  246]    Overall Loss 0.277285    Objective Loss 0.277285                                        LR 0.000022    Time 0.019056    
2023-01-06 17:03:03,059 - Epoch: [98][  190/  246]    Overall Loss 0.277724    Objective Loss 0.277724                                        LR 0.000022    Time 0.018940    
2023-01-06 17:03:03,223 - Epoch: [98][  200/  246]    Overall Loss 0.277972    Objective Loss 0.277972                                        LR 0.000022    Time 0.018808    
2023-01-06 17:03:03,390 - Epoch: [98][  210/  246]    Overall Loss 0.277507    Objective Loss 0.277507                                        LR 0.000022    Time 0.018708    
2023-01-06 17:03:03,556 - Epoch: [98][  220/  246]    Overall Loss 0.277379    Objective Loss 0.277379                                        LR 0.000022    Time 0.018612    
2023-01-06 17:03:03,723 - Epoch: [98][  230/  246]    Overall Loss 0.277038    Objective Loss 0.277038                                        LR 0.000022    Time 0.018527    
2023-01-06 17:03:03,903 - Epoch: [98][  240/  246]    Overall Loss 0.277929    Objective Loss 0.277929                                        LR 0.000022    Time 0.018503    
2023-01-06 17:03:03,992 - Epoch: [98][  246/  246]    Overall Loss 0.277550    Objective Loss 0.277550    Top1 90.430622    LR 0.000022    Time 0.018413    
2023-01-06 17:03:04,136 - --- validate (epoch=98)-----------
2023-01-06 17:03:04,136 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:04,595 - Epoch: [98][   10/   28]    Loss 0.301363    Top1 88.945312    
2023-01-06 17:03:04,717 - Epoch: [98][   20/   28]    Loss 0.283673    Top1 89.687500    
2023-01-06 17:03:04,785 - Epoch: [98][   28/   28]    Loss 0.283930    Top1 89.765245    
2023-01-06 17:03:04,927 - ==> Top1: 89.765    Loss: 0.284

2023-01-06 17:03:04,927 - ==> Confusion:
[[ 228   11  200]
 [  18  240  344]
 [  70   72 5803]]

2023-01-06 17:03:04,928 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:04,928 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:04,935 - 

2023-01-06 17:03:04,936 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:05,636 - Epoch: [99][   10/  246]    Overall Loss 0.318297    Objective Loss 0.318297                                        LR 0.000022    Time 0.069978    
2023-01-06 17:03:05,808 - Epoch: [99][   20/  246]    Overall Loss 0.296297    Objective Loss 0.296297                                        LR 0.000022    Time 0.043554    
2023-01-06 17:03:05,985 - Epoch: [99][   30/  246]    Overall Loss 0.296420    Objective Loss 0.296420                                        LR 0.000022    Time 0.034926    
2023-01-06 17:03:06,157 - Epoch: [99][   40/  246]    Overall Loss 0.293431    Objective Loss 0.293431                                        LR 0.000022    Time 0.030504    
2023-01-06 17:03:06,338 - Epoch: [99][   50/  246]    Overall Loss 0.289966    Objective Loss 0.289966                                        LR 0.000022    Time 0.028000    
2023-01-06 17:03:06,536 - Epoch: [99][   60/  246]    Overall Loss 0.288621    Objective Loss 0.288621                                        LR 0.000022    Time 0.026639    
2023-01-06 17:03:06,735 - Epoch: [99][   70/  246]    Overall Loss 0.284619    Objective Loss 0.284619                                        LR 0.000022    Time 0.025663    
2023-01-06 17:03:06,933 - Epoch: [99][   80/  246]    Overall Loss 0.280731    Objective Loss 0.280731                                        LR 0.000022    Time 0.024934    
2023-01-06 17:03:07,128 - Epoch: [99][   90/  246]    Overall Loss 0.281861    Objective Loss 0.281861                                        LR 0.000022    Time 0.024324    
2023-01-06 17:03:07,328 - Epoch: [99][  100/  246]    Overall Loss 0.282534    Objective Loss 0.282534                                        LR 0.000022    Time 0.023886    
2023-01-06 17:03:07,523 - Epoch: [99][  110/  246]    Overall Loss 0.281565    Objective Loss 0.281565                                        LR 0.000022    Time 0.023481    
2023-01-06 17:03:07,721 - Epoch: [99][  120/  246]    Overall Loss 0.280454    Objective Loss 0.280454                                        LR 0.000022    Time 0.023177    
2023-01-06 17:03:07,917 - Epoch: [99][  130/  246]    Overall Loss 0.279604    Objective Loss 0.279604                                        LR 0.000022    Time 0.022895    
2023-01-06 17:03:08,116 - Epoch: [99][  140/  246]    Overall Loss 0.278989    Objective Loss 0.278989                                        LR 0.000022    Time 0.022677    
2023-01-06 17:03:08,309 - Epoch: [99][  150/  246]    Overall Loss 0.278982    Objective Loss 0.278982                                        LR 0.000022    Time 0.022456    
2023-01-06 17:03:08,508 - Epoch: [99][  160/  246]    Overall Loss 0.277796    Objective Loss 0.277796                                        LR 0.000022    Time 0.022292    
2023-01-06 17:03:08,703 - Epoch: [99][  170/  246]    Overall Loss 0.277596    Objective Loss 0.277596                                        LR 0.000022    Time 0.022127    
2023-01-06 17:03:08,902 - Epoch: [99][  180/  246]    Overall Loss 0.277964    Objective Loss 0.277964                                        LR 0.000022    Time 0.021997    
2023-01-06 17:03:09,097 - Epoch: [99][  190/  246]    Overall Loss 0.277254    Objective Loss 0.277254                                        LR 0.000022    Time 0.021867    
2023-01-06 17:03:09,297 - Epoch: [99][  200/  246]    Overall Loss 0.277523    Objective Loss 0.277523                                        LR 0.000022    Time 0.021770    
2023-01-06 17:03:09,488 - Epoch: [99][  210/  246]    Overall Loss 0.276949    Objective Loss 0.276949                                        LR 0.000022    Time 0.021641    
2023-01-06 17:03:09,687 - Epoch: [99][  220/  246]    Overall Loss 0.277020    Objective Loss 0.277020                                        LR 0.000022    Time 0.021560    
2023-01-06 17:03:09,881 - Epoch: [99][  230/  246]    Overall Loss 0.277987    Objective Loss 0.277987                                        LR 0.000022    Time 0.021466    
2023-01-06 17:03:10,093 - Epoch: [99][  240/  246]    Overall Loss 0.278300    Objective Loss 0.278300                                        LR 0.000022    Time 0.021454    
2023-01-06 17:03:10,189 - Epoch: [99][  246/  246]    Overall Loss 0.277766    Objective Loss 0.277766    Top1 90.669856    LR 0.000022    Time 0.021320    
2023-01-06 17:03:10,330 - --- validate (epoch=99)-----------
2023-01-06 17:03:10,330 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:10,795 - Epoch: [99][   10/   28]    Loss 0.259188    Top1 90.546875    
2023-01-06 17:03:10,914 - Epoch: [99][   20/   28]    Loss 0.272331    Top1 90.039062    
2023-01-06 17:03:10,981 - Epoch: [99][   28/   28]    Loss 0.285235    Top1 89.750930    
2023-01-06 17:03:11,118 - ==> Top1: 89.751    Loss: 0.285

2023-01-06 17:03:11,119 - ==> Confusion:
[[ 225    9  205]
 [  17  220  365]
 [  62   58 5825]]

2023-01-06 17:03:11,121 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:11,121 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:11,130 - 

2023-01-06 17:03:11,130 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:11,698 - Epoch: [100][   10/  246]    Overall Loss 0.286719    Objective Loss 0.286719                                        LR 0.000013    Time 0.056764    
2023-01-06 17:03:11,885 - Epoch: [100][   20/  246]    Overall Loss 0.282577    Objective Loss 0.282577                                        LR 0.000013    Time 0.037693    
2023-01-06 17:03:12,075 - Epoch: [100][   30/  246]    Overall Loss 0.280576    Objective Loss 0.280576                                        LR 0.000013    Time 0.031434    
2023-01-06 17:03:12,260 - Epoch: [100][   40/  246]    Overall Loss 0.276939    Objective Loss 0.276939                                        LR 0.000013    Time 0.028210    
2023-01-06 17:03:12,457 - Epoch: [100][   50/  246]    Overall Loss 0.275434    Objective Loss 0.275434                                        LR 0.000013    Time 0.026490    
2023-01-06 17:03:12,651 - Epoch: [100][   60/  246]    Overall Loss 0.273718    Objective Loss 0.273718                                        LR 0.000013    Time 0.025296    
2023-01-06 17:03:12,846 - Epoch: [100][   70/  246]    Overall Loss 0.273804    Objective Loss 0.273804                                        LR 0.000013    Time 0.024474    
2023-01-06 17:03:13,053 - Epoch: [100][   80/  246]    Overall Loss 0.274591    Objective Loss 0.274591                                        LR 0.000013    Time 0.023990    
2023-01-06 17:03:13,253 - Epoch: [100][   90/  246]    Overall Loss 0.274305    Objective Loss 0.274305                                        LR 0.000013    Time 0.023542    
2023-01-06 17:03:13,452 - Epoch: [100][  100/  246]    Overall Loss 0.274386    Objective Loss 0.274386                                        LR 0.000013    Time 0.023171    
2023-01-06 17:03:13,645 - Epoch: [100][  110/  246]    Overall Loss 0.275323    Objective Loss 0.275323                                        LR 0.000013    Time 0.022823    
2023-01-06 17:03:13,842 - Epoch: [100][  120/  246]    Overall Loss 0.274960    Objective Loss 0.274960                                        LR 0.000013    Time 0.022555    
2023-01-06 17:03:14,037 - Epoch: [100][  130/  246]    Overall Loss 0.274631    Objective Loss 0.274631                                        LR 0.000013    Time 0.022317    
2023-01-06 17:03:14,203 - Epoch: [100][  140/  246]    Overall Loss 0.275374    Objective Loss 0.275374                                        LR 0.000013    Time 0.021911    
2023-01-06 17:03:14,405 - Epoch: [100][  150/  246]    Overall Loss 0.274896    Objective Loss 0.274896                                        LR 0.000013    Time 0.021792    
2023-01-06 17:03:14,629 - Epoch: [100][  160/  246]    Overall Loss 0.274941    Objective Loss 0.274941                                        LR 0.000013    Time 0.021827    
2023-01-06 17:03:14,844 - Epoch: [100][  170/  246]    Overall Loss 0.274328    Objective Loss 0.274328                                        LR 0.000013    Time 0.021805    
2023-01-06 17:03:15,042 - Epoch: [100][  180/  246]    Overall Loss 0.274480    Objective Loss 0.274480                                        LR 0.000013    Time 0.021690    
2023-01-06 17:03:15,234 - Epoch: [100][  190/  246]    Overall Loss 0.274938    Objective Loss 0.274938                                        LR 0.000013    Time 0.021556    
2023-01-06 17:03:15,426 - Epoch: [100][  200/  246]    Overall Loss 0.274401    Objective Loss 0.274401                                        LR 0.000013    Time 0.021438    
2023-01-06 17:03:15,617 - Epoch: [100][  210/  246]    Overall Loss 0.274512    Objective Loss 0.274512                                        LR 0.000013    Time 0.021327    
2023-01-06 17:03:15,807 - Epoch: [100][  220/  246]    Overall Loss 0.274513    Objective Loss 0.274513                                        LR 0.000013    Time 0.021217    
2023-01-06 17:03:16,001 - Epoch: [100][  230/  246]    Overall Loss 0.274660    Objective Loss 0.274660                                        LR 0.000013    Time 0.021136    
2023-01-06 17:03:16,205 - Epoch: [100][  240/  246]    Overall Loss 0.274992    Objective Loss 0.274992                                        LR 0.000013    Time 0.021107    
2023-01-06 17:03:16,302 - Epoch: [100][  246/  246]    Overall Loss 0.274795    Objective Loss 0.274795    Top1 90.909091    LR 0.000013    Time 0.020985    
2023-01-06 17:03:16,435 - --- validate (epoch=100)-----------
2023-01-06 17:03:16,436 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:16,888 - Epoch: [100][   10/   28]    Loss 0.267380    Top1 90.117188    
2023-01-06 17:03:17,004 - Epoch: [100][   20/   28]    Loss 0.287658    Top1 89.492188    
2023-01-06 17:03:17,073 - Epoch: [100][   28/   28]    Loss 0.285238    Top1 89.579158    
2023-01-06 17:03:17,242 - ==> Top1: 89.579    Loss: 0.285

2023-01-06 17:03:17,242 - ==> Confusion:
[[ 205   10  224]
 [  14  211  377]
 [  46   57 5842]]

2023-01-06 17:03:17,244 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:17,244 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:17,250 - 

2023-01-06 17:03:17,250 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:17,952 - Epoch: [101][   10/  246]    Overall Loss 0.266506    Objective Loss 0.266506                                        LR 0.000013    Time 0.070140    
2023-01-06 17:03:18,133 - Epoch: [101][   20/  246]    Overall Loss 0.265652    Objective Loss 0.265652                                        LR 0.000013    Time 0.044117    
2023-01-06 17:03:18,326 - Epoch: [101][   30/  246]    Overall Loss 0.265101    Objective Loss 0.265101                                        LR 0.000013    Time 0.035803    
2023-01-06 17:03:18,519 - Epoch: [101][   40/  246]    Overall Loss 0.268557    Objective Loss 0.268557                                        LR 0.000013    Time 0.031686    
2023-01-06 17:03:18,712 - Epoch: [101][   50/  246]    Overall Loss 0.271229    Objective Loss 0.271229                                        LR 0.000013    Time 0.029203    
2023-01-06 17:03:18,905 - Epoch: [101][   60/  246]    Overall Loss 0.270569    Objective Loss 0.270569                                        LR 0.000013    Time 0.027534    
2023-01-06 17:03:19,097 - Epoch: [101][   70/  246]    Overall Loss 0.273499    Objective Loss 0.273499                                        LR 0.000013    Time 0.026347    
2023-01-06 17:03:19,283 - Epoch: [101][   80/  246]    Overall Loss 0.270474    Objective Loss 0.270474                                        LR 0.000013    Time 0.025369    
2023-01-06 17:03:19,469 - Epoch: [101][   90/  246]    Overall Loss 0.272036    Objective Loss 0.272036                                        LR 0.000013    Time 0.024615    
2023-01-06 17:03:19,656 - Epoch: [101][  100/  246]    Overall Loss 0.274105    Objective Loss 0.274105                                        LR 0.000013    Time 0.024020    
2023-01-06 17:03:19,842 - Epoch: [101][  110/  246]    Overall Loss 0.274316    Objective Loss 0.274316                                        LR 0.000013    Time 0.023525    
2023-01-06 17:03:20,028 - Epoch: [101][  120/  246]    Overall Loss 0.274593    Objective Loss 0.274593                                        LR 0.000013    Time 0.023109    
2023-01-06 17:03:20,208 - Epoch: [101][  130/  246]    Overall Loss 0.274528    Objective Loss 0.274528                                        LR 0.000013    Time 0.022712    
2023-01-06 17:03:20,398 - Epoch: [101][  140/  246]    Overall Loss 0.273812    Objective Loss 0.273812                                        LR 0.000013    Time 0.022445    
2023-01-06 17:03:20,591 - Epoch: [101][  150/  246]    Overall Loss 0.273562    Objective Loss 0.273562                                        LR 0.000013    Time 0.022235    
2023-01-06 17:03:20,787 - Epoch: [101][  160/  246]    Overall Loss 0.274052    Objective Loss 0.274052                                        LR 0.000013    Time 0.022066    
2023-01-06 17:03:20,980 - Epoch: [101][  170/  246]    Overall Loss 0.274643    Objective Loss 0.274643                                        LR 0.000013    Time 0.021900    
2023-01-06 17:03:21,169 - Epoch: [101][  180/  246]    Overall Loss 0.274266    Objective Loss 0.274266                                        LR 0.000013    Time 0.021732    
2023-01-06 17:03:21,341 - Epoch: [101][  190/  246]    Overall Loss 0.275105    Objective Loss 0.275105                                        LR 0.000013    Time 0.021494    
2023-01-06 17:03:21,514 - Epoch: [101][  200/  246]    Overall Loss 0.275164    Objective Loss 0.275164                                        LR 0.000013    Time 0.021284    
2023-01-06 17:03:21,690 - Epoch: [101][  210/  246]    Overall Loss 0.276277    Objective Loss 0.276277                                        LR 0.000013    Time 0.021105    
2023-01-06 17:03:21,881 - Epoch: [101][  220/  246]    Overall Loss 0.276520    Objective Loss 0.276520                                        LR 0.000013    Time 0.021011    
2023-01-06 17:03:22,061 - Epoch: [101][  230/  246]    Overall Loss 0.276655    Objective Loss 0.276655                                        LR 0.000013    Time 0.020880    
2023-01-06 17:03:22,249 - Epoch: [101][  240/  246]    Overall Loss 0.276076    Objective Loss 0.276076                                        LR 0.000013    Time 0.020790    
2023-01-06 17:03:22,337 - Epoch: [101][  246/  246]    Overall Loss 0.275708    Objective Loss 0.275708    Top1 91.148325    LR 0.000013    Time 0.020640    
2023-01-06 17:03:22,476 - --- validate (epoch=101)-----------
2023-01-06 17:03:22,477 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:22,945 - Epoch: [101][   10/   28]    Loss 0.283093    Top1 89.765625    
2023-01-06 17:03:23,077 - Epoch: [101][   20/   28]    Loss 0.286292    Top1 89.492188    
2023-01-06 17:03:23,145 - Epoch: [101][   28/   28]    Loss 0.279230    Top1 89.693673    
2023-01-06 17:03:23,311 - ==> Top1: 89.694    Loss: 0.279

2023-01-06 17:03:23,312 - ==> Confusion:
[[ 221   10  208]
 [  19  221  362]
 [  61   60 5824]]

2023-01-06 17:03:23,313 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:23,313 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:23,319 - 

2023-01-06 17:03:23,319 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:24,019 - Epoch: [102][   10/  246]    Overall Loss 0.250969    Objective Loss 0.250969                                        LR 0.000013    Time 0.069916    
2023-01-06 17:03:24,181 - Epoch: [102][   20/  246]    Overall Loss 0.265052    Objective Loss 0.265052                                        LR 0.000013    Time 0.043055    
2023-01-06 17:03:24,345 - Epoch: [102][   30/  246]    Overall Loss 0.265200    Objective Loss 0.265200                                        LR 0.000013    Time 0.034128    
2023-01-06 17:03:24,530 - Epoch: [102][   40/  246]    Overall Loss 0.267638    Objective Loss 0.267638                                        LR 0.000013    Time 0.030192    
2023-01-06 17:03:24,703 - Epoch: [102][   50/  246]    Overall Loss 0.268177    Objective Loss 0.268177                                        LR 0.000013    Time 0.027610    
2023-01-06 17:03:24,881 - Epoch: [102][   60/  246]    Overall Loss 0.271474    Objective Loss 0.271474                                        LR 0.000013    Time 0.025968    
2023-01-06 17:03:25,060 - Epoch: [102][   70/  246]    Overall Loss 0.274766    Objective Loss 0.274766                                        LR 0.000013    Time 0.024811    
2023-01-06 17:03:25,260 - Epoch: [102][   80/  246]    Overall Loss 0.275018    Objective Loss 0.275018                                        LR 0.000013    Time 0.024205    
2023-01-06 17:03:25,444 - Epoch: [102][   90/  246]    Overall Loss 0.275024    Objective Loss 0.275024                                        LR 0.000013    Time 0.023561    
2023-01-06 17:03:25,640 - Epoch: [102][  100/  246]    Overall Loss 0.275478    Objective Loss 0.275478                                        LR 0.000013    Time 0.023159    
2023-01-06 17:03:25,825 - Epoch: [102][  110/  246]    Overall Loss 0.275236    Objective Loss 0.275236                                        LR 0.000013    Time 0.022732    
2023-01-06 17:03:26,014 - Epoch: [102][  120/  246]    Overall Loss 0.274940    Objective Loss 0.274940                                        LR 0.000013    Time 0.022412    
2023-01-06 17:03:26,196 - Epoch: [102][  130/  246]    Overall Loss 0.275559    Objective Loss 0.275559                                        LR 0.000013    Time 0.022083    
2023-01-06 17:03:26,384 - Epoch: [102][  140/  246]    Overall Loss 0.273474    Objective Loss 0.273474                                        LR 0.000013    Time 0.021845    
2023-01-06 17:03:26,574 - Epoch: [102][  150/  246]    Overall Loss 0.272124    Objective Loss 0.272124                                        LR 0.000013    Time 0.021654    
2023-01-06 17:03:26,768 - Epoch: [102][  160/  246]    Overall Loss 0.271787    Objective Loss 0.271787                                        LR 0.000013    Time 0.021508    
2023-01-06 17:03:26,962 - Epoch: [102][  170/  246]    Overall Loss 0.272705    Objective Loss 0.272705                                        LR 0.000013    Time 0.021386    
2023-01-06 17:03:27,155 - Epoch: [102][  180/  246]    Overall Loss 0.273780    Objective Loss 0.273780                                        LR 0.000013    Time 0.021267    
2023-01-06 17:03:27,346 - Epoch: [102][  190/  246]    Overall Loss 0.274510    Objective Loss 0.274510                                        LR 0.000013    Time 0.021153    
2023-01-06 17:03:27,541 - Epoch: [102][  200/  246]    Overall Loss 0.274897    Objective Loss 0.274897                                        LR 0.000013    Time 0.021056    
2023-01-06 17:03:27,737 - Epoch: [102][  210/  246]    Overall Loss 0.275839    Objective Loss 0.275839                                        LR 0.000013    Time 0.020985    
2023-01-06 17:03:27,929 - Epoch: [102][  220/  246]    Overall Loss 0.275593    Objective Loss 0.275593                                        LR 0.000013    Time 0.020902    
2023-01-06 17:03:28,109 - Epoch: [102][  230/  246]    Overall Loss 0.276035    Objective Loss 0.276035                                        LR 0.000013    Time 0.020775    
2023-01-06 17:03:28,290 - Epoch: [102][  240/  246]    Overall Loss 0.275961    Objective Loss 0.275961                                        LR 0.000013    Time 0.020655    
2023-01-06 17:03:28,382 - Epoch: [102][  246/  246]    Overall Loss 0.275897    Objective Loss 0.275897    Top1 88.755981    LR 0.000013    Time 0.020527    
2023-01-06 17:03:28,515 - --- validate (epoch=102)-----------
2023-01-06 17:03:28,515 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:28,965 - Epoch: [102][   10/   28]    Loss 0.268333    Top1 90.429688    
2023-01-06 17:03:29,087 - Epoch: [102][   20/   28]    Loss 0.280562    Top1 89.746094    
2023-01-06 17:03:29,155 - Epoch: [102][   28/   28]    Loss 0.288008    Top1 89.636416    
2023-01-06 17:03:29,318 - ==> Top1: 89.636    Loss: 0.288

2023-01-06 17:03:29,319 - ==> Confusion:
[[ 218    9  212]
 [  15  221  366]
 [  52   70 5823]]

2023-01-06 17:03:29,320 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:29,320 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:29,326 - 

2023-01-06 17:03:29,326 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:29,874 - Epoch: [103][   10/  246]    Overall Loss 0.258595    Objective Loss 0.258595                                        LR 0.000013    Time 0.054738    
2023-01-06 17:03:30,044 - Epoch: [103][   20/  246]    Overall Loss 0.267220    Objective Loss 0.267220                                        LR 0.000013    Time 0.035827    
2023-01-06 17:03:30,211 - Epoch: [103][   30/  246]    Overall Loss 0.271856    Objective Loss 0.271856                                        LR 0.000013    Time 0.029454    
2023-01-06 17:03:30,383 - Epoch: [103][   40/  246]    Overall Loss 0.271583    Objective Loss 0.271583                                        LR 0.000013    Time 0.026365    
2023-01-06 17:03:30,548 - Epoch: [103][   50/  246]    Overall Loss 0.268204    Objective Loss 0.268204                                        LR 0.000013    Time 0.024382    
2023-01-06 17:03:30,724 - Epoch: [103][   60/  246]    Overall Loss 0.267612    Objective Loss 0.267612                                        LR 0.000013    Time 0.023253    
2023-01-06 17:03:30,888 - Epoch: [103][   70/  246]    Overall Loss 0.268822    Objective Loss 0.268822                                        LR 0.000013    Time 0.022276    
2023-01-06 17:03:31,059 - Epoch: [103][   80/  246]    Overall Loss 0.269983    Objective Loss 0.269983                                        LR 0.000013    Time 0.021623    
2023-01-06 17:03:31,224 - Epoch: [103][   90/  246]    Overall Loss 0.268613    Objective Loss 0.268613                                        LR 0.000013    Time 0.021042    
2023-01-06 17:03:31,396 - Epoch: [103][  100/  246]    Overall Loss 0.269225    Objective Loss 0.269225                                        LR 0.000013    Time 0.020659    
2023-01-06 17:03:31,570 - Epoch: [103][  110/  246]    Overall Loss 0.270920    Objective Loss 0.270920                                        LR 0.000013    Time 0.020360    
2023-01-06 17:03:31,746 - Epoch: [103][  120/  246]    Overall Loss 0.271614    Objective Loss 0.271614                                        LR 0.000013    Time 0.020127    
2023-01-06 17:03:31,925 - Epoch: [103][  130/  246]    Overall Loss 0.272716    Objective Loss 0.272716                                        LR 0.000013    Time 0.019951    
2023-01-06 17:03:32,109 - Epoch: [103][  140/  246]    Overall Loss 0.272956    Objective Loss 0.272956                                        LR 0.000013    Time 0.019835    
2023-01-06 17:03:32,289 - Epoch: [103][  150/  246]    Overall Loss 0.271919    Objective Loss 0.271919                                        LR 0.000013    Time 0.019711    
2023-01-06 17:03:32,481 - Epoch: [103][  160/  246]    Overall Loss 0.273634    Objective Loss 0.273634                                        LR 0.000013    Time 0.019677    
2023-01-06 17:03:32,674 - Epoch: [103][  170/  246]    Overall Loss 0.273321    Objective Loss 0.273321                                        LR 0.000013    Time 0.019651    
2023-01-06 17:03:32,864 - Epoch: [103][  180/  246]    Overall Loss 0.274058    Objective Loss 0.274058                                        LR 0.000013    Time 0.019614    
2023-01-06 17:03:33,050 - Epoch: [103][  190/  246]    Overall Loss 0.273328    Objective Loss 0.273328                                        LR 0.000013    Time 0.019558    
2023-01-06 17:03:33,243 - Epoch: [103][  200/  246]    Overall Loss 0.273338    Objective Loss 0.273338                                        LR 0.000013    Time 0.019544    
2023-01-06 17:03:33,413 - Epoch: [103][  210/  246]    Overall Loss 0.273552    Objective Loss 0.273552                                        LR 0.000013    Time 0.019422    
2023-01-06 17:03:33,581 - Epoch: [103][  220/  246]    Overall Loss 0.274015    Objective Loss 0.274015                                        LR 0.000013    Time 0.019301    
2023-01-06 17:03:33,755 - Epoch: [103][  230/  246]    Overall Loss 0.274697    Objective Loss 0.274697                                        LR 0.000013    Time 0.019218    
2023-01-06 17:03:33,941 - Epoch: [103][  240/  246]    Overall Loss 0.274598    Objective Loss 0.274598                                        LR 0.000013    Time 0.019190    
2023-01-06 17:03:34,029 - Epoch: [103][  246/  246]    Overall Loss 0.275027    Objective Loss 0.275027    Top1 86.842105    LR 0.000013    Time 0.019078    
2023-01-06 17:03:34,152 - --- validate (epoch=103)-----------
2023-01-06 17:03:34,152 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:34,616 - Epoch: [103][   10/   28]    Loss 0.295901    Top1 88.945312    
2023-01-06 17:03:34,751 - Epoch: [103][   20/   28]    Loss 0.284796    Top1 89.570312    
2023-01-06 17:03:34,821 - Epoch: [103][   28/   28]    Loss 0.281296    Top1 89.679359    
2023-01-06 17:03:34,968 - ==> Top1: 89.679    Loss: 0.281

2023-01-06 17:03:34,969 - ==> Confusion:
[[ 204   10  225]
 [  14  211  377]
 [  43   52 5850]]

2023-01-06 17:03:34,970 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:34,970 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:34,976 - 

2023-01-06 17:03:34,976 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:35,683 - Epoch: [104][   10/  246]    Overall Loss 0.277759    Objective Loss 0.277759                                        LR 0.000013    Time 0.070561    
2023-01-06 17:03:35,890 - Epoch: [104][   20/  246]    Overall Loss 0.278282    Objective Loss 0.278282                                        LR 0.000013    Time 0.045636    
2023-01-06 17:03:36,099 - Epoch: [104][   30/  246]    Overall Loss 0.280416    Objective Loss 0.280416                                        LR 0.000013    Time 0.037389    
2023-01-06 17:03:36,308 - Epoch: [104][   40/  246]    Overall Loss 0.278104    Objective Loss 0.278104                                        LR 0.000013    Time 0.033236    
2023-01-06 17:03:36,512 - Epoch: [104][   50/  246]    Overall Loss 0.274917    Objective Loss 0.274917                                        LR 0.000013    Time 0.030667    
2023-01-06 17:03:36,714 - Epoch: [104][   60/  246]    Overall Loss 0.272818    Objective Loss 0.272818                                        LR 0.000013    Time 0.028919    
2023-01-06 17:03:36,923 - Epoch: [104][   70/  246]    Overall Loss 0.275343    Objective Loss 0.275343                                        LR 0.000013    Time 0.027775    
2023-01-06 17:03:37,128 - Epoch: [104][   80/  246]    Overall Loss 0.276355    Objective Loss 0.276355                                        LR 0.000013    Time 0.026853    
2023-01-06 17:03:37,313 - Epoch: [104][   90/  246]    Overall Loss 0.276809    Objective Loss 0.276809                                        LR 0.000013    Time 0.025922    
2023-01-06 17:03:37,507 - Epoch: [104][  100/  246]    Overall Loss 0.275076    Objective Loss 0.275076                                        LR 0.000013    Time 0.025244    
2023-01-06 17:03:37,703 - Epoch: [104][  110/  246]    Overall Loss 0.276191    Objective Loss 0.276191                                        LR 0.000013    Time 0.024722    
2023-01-06 17:03:37,893 - Epoch: [104][  120/  246]    Overall Loss 0.276306    Objective Loss 0.276306                                        LR 0.000013    Time 0.024245    
2023-01-06 17:03:38,092 - Epoch: [104][  130/  246]    Overall Loss 0.275782    Objective Loss 0.275782                                        LR 0.000013    Time 0.023908    
2023-01-06 17:03:38,263 - Epoch: [104][  140/  246]    Overall Loss 0.276108    Objective Loss 0.276108                                        LR 0.000013    Time 0.023416    
2023-01-06 17:03:38,463 - Epoch: [104][  150/  246]    Overall Loss 0.275979    Objective Loss 0.275979                                        LR 0.000013    Time 0.023184    
2023-01-06 17:03:38,638 - Epoch: [104][  160/  246]    Overall Loss 0.275855    Objective Loss 0.275855                                        LR 0.000013    Time 0.022831    
2023-01-06 17:03:38,812 - Epoch: [104][  170/  246]    Overall Loss 0.276224    Objective Loss 0.276224                                        LR 0.000013    Time 0.022507    
2023-01-06 17:03:38,985 - Epoch: [104][  180/  246]    Overall Loss 0.276173    Objective Loss 0.276173                                        LR 0.000013    Time 0.022218    
2023-01-06 17:03:39,145 - Epoch: [104][  190/  246]    Overall Loss 0.275460    Objective Loss 0.275460                                        LR 0.000013    Time 0.021888    
2023-01-06 17:03:39,306 - Epoch: [104][  200/  246]    Overall Loss 0.275336    Objective Loss 0.275336                                        LR 0.000013    Time 0.021596    
2023-01-06 17:03:39,466 - Epoch: [104][  210/  246]    Overall Loss 0.274347    Objective Loss 0.274347                                        LR 0.000013    Time 0.021326    
2023-01-06 17:03:39,626 - Epoch: [104][  220/  246]    Overall Loss 0.274316    Objective Loss 0.274316                                        LR 0.000013    Time 0.021083    
2023-01-06 17:03:39,786 - Epoch: [104][  230/  246]    Overall Loss 0.274307    Objective Loss 0.274307                                        LR 0.000013    Time 0.020859    
2023-01-06 17:03:39,962 - Epoch: [104][  240/  246]    Overall Loss 0.274488    Objective Loss 0.274488                                        LR 0.000013    Time 0.020722    
2023-01-06 17:03:40,051 - Epoch: [104][  246/  246]    Overall Loss 0.274298    Objective Loss 0.274298    Top1 89.234450    LR 0.000013    Time 0.020578    
2023-01-06 17:03:40,177 - --- validate (epoch=104)-----------
2023-01-06 17:03:40,177 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:40,630 - Epoch: [104][   10/   28]    Loss 0.301616    Top1 88.828125    
2023-01-06 17:03:40,742 - Epoch: [104][   20/   28]    Loss 0.281597    Top1 89.824219    
2023-01-06 17:03:40,811 - Epoch: [104][   28/   28]    Loss 0.281903    Top1 89.722302    
2023-01-06 17:03:40,973 - ==> Top1: 89.722    Loss: 0.282

2023-01-06 17:03:40,973 - ==> Confusion:
[[ 216   14  209]
 [  14  241  347]
 [  60   74 5811]]

2023-01-06 17:03:40,974 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:40,974 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:40,980 - 

2023-01-06 17:03:40,980 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:41,690 - Epoch: [105][   10/  246]    Overall Loss 0.274653    Objective Loss 0.274653                                        LR 0.000013    Time 0.070946    
2023-01-06 17:03:41,848 - Epoch: [105][   20/  246]    Overall Loss 0.274664    Objective Loss 0.274664                                        LR 0.000013    Time 0.043332    
2023-01-06 17:03:42,008 - Epoch: [105][   30/  246]    Overall Loss 0.273230    Objective Loss 0.273230                                        LR 0.000013    Time 0.034217    
2023-01-06 17:03:42,177 - Epoch: [105][   40/  246]    Overall Loss 0.267380    Objective Loss 0.267380                                        LR 0.000013    Time 0.029889    
2023-01-06 17:03:42,350 - Epoch: [105][   50/  246]    Overall Loss 0.274341    Objective Loss 0.274341                                        LR 0.000013    Time 0.027355    
2023-01-06 17:03:42,519 - Epoch: [105][   60/  246]    Overall Loss 0.278421    Objective Loss 0.278421                                        LR 0.000013    Time 0.025605    
2023-01-06 17:03:42,688 - Epoch: [105][   70/  246]    Overall Loss 0.277944    Objective Loss 0.277944                                        LR 0.000013    Time 0.024361    
2023-01-06 17:03:42,856 - Epoch: [105][   80/  246]    Overall Loss 0.275791    Objective Loss 0.275791                                        LR 0.000013    Time 0.023413    
2023-01-06 17:03:43,024 - Epoch: [105][   90/  246]    Overall Loss 0.274611    Objective Loss 0.274611                                        LR 0.000013    Time 0.022669    
2023-01-06 17:03:43,193 - Epoch: [105][  100/  246]    Overall Loss 0.275660    Objective Loss 0.275660                                        LR 0.000013    Time 0.022092    
2023-01-06 17:03:43,370 - Epoch: [105][  110/  246]    Overall Loss 0.273002    Objective Loss 0.273002                                        LR 0.000013    Time 0.021691    
2023-01-06 17:03:43,568 - Epoch: [105][  120/  246]    Overall Loss 0.272698    Objective Loss 0.272698                                        LR 0.000013    Time 0.021529    
2023-01-06 17:03:43,766 - Epoch: [105][  130/  246]    Overall Loss 0.272893    Objective Loss 0.272893                                        LR 0.000013    Time 0.021392    
2023-01-06 17:03:43,965 - Epoch: [105][  140/  246]    Overall Loss 0.274922    Objective Loss 0.274922                                        LR 0.000013    Time 0.021280    
2023-01-06 17:03:44,155 - Epoch: [105][  150/  246]    Overall Loss 0.274544    Objective Loss 0.274544                                        LR 0.000013    Time 0.021125    
2023-01-06 17:03:44,353 - Epoch: [105][  160/  246]    Overall Loss 0.274698    Objective Loss 0.274698                                        LR 0.000013    Time 0.021040    
2023-01-06 17:03:44,554 - Epoch: [105][  170/  246]    Overall Loss 0.274583    Objective Loss 0.274583                                        LR 0.000013    Time 0.020984    
2023-01-06 17:03:44,752 - Epoch: [105][  180/  246]    Overall Loss 0.274729    Objective Loss 0.274729                                        LR 0.000013    Time 0.020917    
2023-01-06 17:03:44,953 - Epoch: [105][  190/  246]    Overall Loss 0.274385    Objective Loss 0.274385                                        LR 0.000013    Time 0.020872    
2023-01-06 17:03:45,145 - Epoch: [105][  200/  246]    Overall Loss 0.273655    Objective Loss 0.273655                                        LR 0.000013    Time 0.020788    
2023-01-06 17:03:45,339 - Epoch: [105][  210/  246]    Overall Loss 0.273956    Objective Loss 0.273956                                        LR 0.000013    Time 0.020718    
2023-01-06 17:03:45,534 - Epoch: [105][  220/  246]    Overall Loss 0.274147    Objective Loss 0.274147                                        LR 0.000013    Time 0.020661    
2023-01-06 17:03:45,727 - Epoch: [105][  230/  246]    Overall Loss 0.274161    Objective Loss 0.274161                                        LR 0.000013    Time 0.020599    
2023-01-06 17:03:45,934 - Epoch: [105][  240/  246]    Overall Loss 0.274220    Objective Loss 0.274220                                        LR 0.000013    Time 0.020604    
2023-01-06 17:03:46,019 - Epoch: [105][  246/  246]    Overall Loss 0.274025    Objective Loss 0.274025    Top1 90.191388    LR 0.000013    Time 0.020444    
2023-01-06 17:03:46,161 - --- validate (epoch=105)-----------
2023-01-06 17:03:46,162 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:46,602 - Epoch: [105][   10/   28]    Loss 0.276825    Top1 90.039062    
2023-01-06 17:03:46,715 - Epoch: [105][   20/   28]    Loss 0.279511    Top1 89.648438    
2023-01-06 17:03:46,784 - Epoch: [105][   28/   28]    Loss 0.277156    Top1 89.707987    
2023-01-06 17:03:46,917 - ==> Top1: 89.708    Loss: 0.277

2023-01-06 17:03:46,917 - ==> Confusion:
[[ 216   11  212]
 [  18  219  365]
 [  58   55 5832]]

2023-01-06 17:03:46,919 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:46,919 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:46,925 - 

2023-01-06 17:03:46,925 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:47,480 - Epoch: [106][   10/  246]    Overall Loss 0.250161    Objective Loss 0.250161                                        LR 0.000013    Time 0.055470    
2023-01-06 17:03:47,660 - Epoch: [106][   20/  246]    Overall Loss 0.268991    Objective Loss 0.268991                                        LR 0.000013    Time 0.036700    
2023-01-06 17:03:47,860 - Epoch: [106][   30/  246]    Overall Loss 0.271336    Objective Loss 0.271336                                        LR 0.000013    Time 0.031105    
2023-01-06 17:03:48,075 - Epoch: [106][   40/  246]    Overall Loss 0.273724    Objective Loss 0.273724                                        LR 0.000013    Time 0.028712    
2023-01-06 17:03:48,282 - Epoch: [106][   50/  246]    Overall Loss 0.272427    Objective Loss 0.272427                                        LR 0.000013    Time 0.027096    
2023-01-06 17:03:48,485 - Epoch: [106][   60/  246]    Overall Loss 0.269381    Objective Loss 0.269381                                        LR 0.000013    Time 0.025949    
2023-01-06 17:03:48,696 - Epoch: [106][   70/  246]    Overall Loss 0.266666    Objective Loss 0.266666                                        LR 0.000013    Time 0.025255    
2023-01-06 17:03:48,920 - Epoch: [106][   80/  246]    Overall Loss 0.267624    Objective Loss 0.267624                                        LR 0.000013    Time 0.024893    
2023-01-06 17:03:49,130 - Epoch: [106][   90/  246]    Overall Loss 0.267817    Objective Loss 0.267817                                        LR 0.000013    Time 0.024453    
2023-01-06 17:03:49,314 - Epoch: [106][  100/  246]    Overall Loss 0.268062    Objective Loss 0.268062                                        LR 0.000013    Time 0.023844    
2023-01-06 17:03:49,493 - Epoch: [106][  110/  246]    Overall Loss 0.267767    Objective Loss 0.267767                                        LR 0.000013    Time 0.023306    
2023-01-06 17:03:49,689 - Epoch: [106][  120/  246]    Overall Loss 0.266800    Objective Loss 0.266800                                        LR 0.000013    Time 0.022995    
2023-01-06 17:03:49,905 - Epoch: [106][  130/  246]    Overall Loss 0.267951    Objective Loss 0.267951                                        LR 0.000013    Time 0.022885    
2023-01-06 17:03:50,122 - Epoch: [106][  140/  246]    Overall Loss 0.268633    Objective Loss 0.268633                                        LR 0.000013    Time 0.022798    
2023-01-06 17:03:50,345 - Epoch: [106][  150/  246]    Overall Loss 0.269646    Objective Loss 0.269646                                        LR 0.000013    Time 0.022761    
2023-01-06 17:03:50,579 - Epoch: [106][  160/  246]    Overall Loss 0.269910    Objective Loss 0.269910                                        LR 0.000013    Time 0.022796    
2023-01-06 17:03:50,806 - Epoch: [106][  170/  246]    Overall Loss 0.271468    Objective Loss 0.271468                                        LR 0.000013    Time 0.022789    
2023-01-06 17:03:51,019 - Epoch: [106][  180/  246]    Overall Loss 0.272974    Objective Loss 0.272974                                        LR 0.000013    Time 0.022704    
2023-01-06 17:03:51,231 - Epoch: [106][  190/  246]    Overall Loss 0.273229    Objective Loss 0.273229                                        LR 0.000013    Time 0.022624    
2023-01-06 17:03:51,443 - Epoch: [106][  200/  246]    Overall Loss 0.273776    Objective Loss 0.273776                                        LR 0.000013    Time 0.022549    
2023-01-06 17:03:51,652 - Epoch: [106][  210/  246]    Overall Loss 0.274311    Objective Loss 0.274311                                        LR 0.000013    Time 0.022469    
2023-01-06 17:03:51,861 - Epoch: [106][  220/  246]    Overall Loss 0.273445    Objective Loss 0.273445                                        LR 0.000013    Time 0.022393    
2023-01-06 17:03:52,084 - Epoch: [106][  230/  246]    Overall Loss 0.273803    Objective Loss 0.273803                                        LR 0.000013    Time 0.022389    
2023-01-06 17:03:52,324 - Epoch: [106][  240/  246]    Overall Loss 0.272971    Objective Loss 0.272971                                        LR 0.000013    Time 0.022453    
2023-01-06 17:03:52,433 - Epoch: [106][  246/  246]    Overall Loss 0.272527    Objective Loss 0.272527    Top1 92.105263    LR 0.000013    Time 0.022349    
2023-01-06 17:03:52,613 - --- validate (epoch=106)-----------
2023-01-06 17:03:52,613 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:53,074 - Epoch: [106][   10/   28]    Loss 0.275813    Top1 89.609375    
2023-01-06 17:03:53,184 - Epoch: [106][   20/   28]    Loss 0.281206    Top1 89.589844    
2023-01-06 17:03:53,252 - Epoch: [106][   28/   28]    Loss 0.279936    Top1 89.593473    
2023-01-06 17:03:53,401 - ==> Top1: 89.593    Loss: 0.280

2023-01-06 17:03:53,401 - ==> Confusion:
[[ 202    9  228]
 [  13  223  366]
 [  51   60 5834]]

2023-01-06 17:03:53,402 - ==> Best [Top1: 89.765   Sparsity:0.00   Params: 155168 on epoch: 98]
2023-01-06 17:03:53,403 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:53,408 - 

2023-01-06 17:03:53,409 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:03:54,130 - Epoch: [107][   10/  246]    Overall Loss 0.277589    Objective Loss 0.277589                                        LR 0.000013    Time 0.072072    
2023-01-06 17:03:54,322 - Epoch: [107][   20/  246]    Overall Loss 0.284806    Objective Loss 0.284806                                        LR 0.000013    Time 0.045638    
2023-01-06 17:03:54,516 - Epoch: [107][   30/  246]    Overall Loss 0.282052    Objective Loss 0.282052                                        LR 0.000013    Time 0.036879    
2023-01-06 17:03:54,709 - Epoch: [107][   40/  246]    Overall Loss 0.276524    Objective Loss 0.276524                                        LR 0.000013    Time 0.032478    
2023-01-06 17:03:54,908 - Epoch: [107][   50/  246]    Overall Loss 0.273625    Objective Loss 0.273625                                        LR 0.000013    Time 0.029946    
2023-01-06 17:03:55,104 - Epoch: [107][   60/  246]    Overall Loss 0.276584    Objective Loss 0.276584                                        LR 0.000013    Time 0.028219    
2023-01-06 17:03:55,300 - Epoch: [107][   70/  246]    Overall Loss 0.276989    Objective Loss 0.276989                                        LR 0.000013    Time 0.026973    
2023-01-06 17:03:55,491 - Epoch: [107][   80/  246]    Overall Loss 0.276987    Objective Loss 0.276987                                        LR 0.000013    Time 0.025989    
2023-01-06 17:03:55,692 - Epoch: [107][   90/  246]    Overall Loss 0.276138    Objective Loss 0.276138                                        LR 0.000013    Time 0.025329    
2023-01-06 17:03:55,882 - Epoch: [107][  100/  246]    Overall Loss 0.274772    Objective Loss 0.274772                                        LR 0.000013    Time 0.024697    
2023-01-06 17:03:56,075 - Epoch: [107][  110/  246]    Overall Loss 0.276790    Objective Loss 0.276790                                        LR 0.000013    Time 0.024202    
2023-01-06 17:03:56,261 - Epoch: [107][  120/  246]    Overall Loss 0.277110    Objective Loss 0.277110                                        LR 0.000013    Time 0.023728    
2023-01-06 17:03:56,443 - Epoch: [107][  130/  246]    Overall Loss 0.276838    Objective Loss 0.276838                                        LR 0.000013    Time 0.023301    
2023-01-06 17:03:56,628 - Epoch: [107][  140/  246]    Overall Loss 0.276805    Objective Loss 0.276805                                        LR 0.000013    Time 0.022959    
2023-01-06 17:03:56,813 - Epoch: [107][  150/  246]    Overall Loss 0.275913    Objective Loss 0.275913                                        LR 0.000013    Time 0.022661    
2023-01-06 17:03:56,999 - Epoch: [107][  160/  246]    Overall Loss 0.274598    Objective Loss 0.274598                                        LR 0.000013    Time 0.022403    
2023-01-06 17:03:57,186 - Epoch: [107][  170/  246]    Overall Loss 0.275048    Objective Loss 0.275048                                        LR 0.000013    Time 0.022181    
2023-01-06 17:03:57,370 - Epoch: [107][  180/  246]    Overall Loss 0.274525    Objective Loss 0.274525                                        LR 0.000013    Time 0.021971    
2023-01-06 17:03:57,560 - Epoch: [107][  190/  246]    Overall Loss 0.274736    Objective Loss 0.274736                                        LR 0.000013    Time 0.021814    
2023-01-06 17:03:57,750 - Epoch: [107][  200/  246]    Overall Loss 0.274109    Objective Loss 0.274109                                        LR 0.000013    Time 0.021669    
2023-01-06 17:03:57,930 - Epoch: [107][  210/  246]    Overall Loss 0.274605    Objective Loss 0.274605                                        LR 0.000013    Time 0.021494    
2023-01-06 17:03:58,106 - Epoch: [107][  220/  246]    Overall Loss 0.274828    Objective Loss 0.274828                                        LR 0.000013    Time 0.021316    
2023-01-06 17:03:58,282 - Epoch: [107][  230/  246]    Overall Loss 0.274552    Objective Loss 0.274552                                        LR 0.000013    Time 0.021154    
2023-01-06 17:03:58,476 - Epoch: [107][  240/  246]    Overall Loss 0.274539    Objective Loss 0.274539                                        LR 0.000013    Time 0.021078    
2023-01-06 17:03:58,564 - Epoch: [107][  246/  246]    Overall Loss 0.274360    Objective Loss 0.274360    Top1 90.430622    LR 0.000013    Time 0.020922    
2023-01-06 17:03:58,711 - --- validate (epoch=107)-----------
2023-01-06 17:03:58,711 - 6986 samples (256 per mini-batch)
2023-01-06 17:03:59,162 - Epoch: [107][   10/   28]    Loss 0.268992    Top1 89.882812    
2023-01-06 17:03:59,273 - Epoch: [107][   20/   28]    Loss 0.272520    Top1 90.058594    
2023-01-06 17:03:59,341 - Epoch: [107][   28/   28]    Loss 0.280057    Top1 89.908388    
2023-01-06 17:03:59,483 - ==> Top1: 89.908    Loss: 0.280

2023-01-06 17:03:59,484 - ==> Confusion:
[[ 218   11  210]
 [  16  230  356]
 [  51   61 5833]]

2023-01-06 17:03:59,485 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 155168 on epoch: 107]
2023-01-06 17:03:59,485 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:03:59,492 - 

2023-01-06 17:03:59,493 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:00,060 - Epoch: [108][   10/  246]    Overall Loss 0.253426    Objective Loss 0.253426                                        LR 0.000013    Time 0.056685    
2023-01-06 17:04:00,242 - Epoch: [108][   20/  246]    Overall Loss 0.267315    Objective Loss 0.267315                                        LR 0.000013    Time 0.037416    
2023-01-06 17:04:00,438 - Epoch: [108][   30/  246]    Overall Loss 0.265712    Objective Loss 0.265712                                        LR 0.000013    Time 0.031456    
2023-01-06 17:04:00,663 - Epoch: [108][   40/  246]    Overall Loss 0.267774    Objective Loss 0.267774                                        LR 0.000013    Time 0.029204    
2023-01-06 17:04:00,894 - Epoch: [108][   50/  246]    Overall Loss 0.269116    Objective Loss 0.269116                                        LR 0.000013    Time 0.027989    
2023-01-06 17:04:01,123 - Epoch: [108][   60/  246]    Overall Loss 0.266665    Objective Loss 0.266665                                        LR 0.000013    Time 0.027122    
2023-01-06 17:04:01,371 - Epoch: [108][   70/  246]    Overall Loss 0.266544    Objective Loss 0.266544                                        LR 0.000013    Time 0.026780    
2023-01-06 17:04:01,608 - Epoch: [108][   80/  246]    Overall Loss 0.266925    Objective Loss 0.266925                                        LR 0.000013    Time 0.026378    
2023-01-06 17:04:01,857 - Epoch: [108][   90/  246]    Overall Loss 0.265995    Objective Loss 0.265995                                        LR 0.000013    Time 0.026207    
2023-01-06 17:04:02,050 - Epoch: [108][  100/  246]    Overall Loss 0.267076    Objective Loss 0.267076                                        LR 0.000013    Time 0.025490    
2023-01-06 17:04:02,235 - Epoch: [108][  110/  246]    Overall Loss 0.267669    Objective Loss 0.267669                                        LR 0.000013    Time 0.024852    
2023-01-06 17:04:02,416 - Epoch: [108][  120/  246]    Overall Loss 0.267033    Objective Loss 0.267033                                        LR 0.000013    Time 0.024272    
2023-01-06 17:04:02,594 - Epoch: [108][  130/  246]    Overall Loss 0.267956    Objective Loss 0.267956                                        LR 0.000013    Time 0.023770    
2023-01-06 17:04:02,772 - Epoch: [108][  140/  246]    Overall Loss 0.267688    Objective Loss 0.267688                                        LR 0.000013    Time 0.023339    
2023-01-06 17:04:02,950 - Epoch: [108][  150/  246]    Overall Loss 0.269120    Objective Loss 0.269120                                        LR 0.000013    Time 0.022966    
2023-01-06 17:04:03,129 - Epoch: [108][  160/  246]    Overall Loss 0.269743    Objective Loss 0.269743                                        LR 0.000013    Time 0.022652    
2023-01-06 17:04:03,306 - Epoch: [108][  170/  246]    Overall Loss 0.269793    Objective Loss 0.269793                                        LR 0.000013    Time 0.022355    
2023-01-06 17:04:03,482 - Epoch: [108][  180/  246]    Overall Loss 0.269822    Objective Loss 0.269822                                        LR 0.000013    Time 0.022088    
2023-01-06 17:04:03,653 - Epoch: [108][  190/  246]    Overall Loss 0.269995    Objective Loss 0.269995                                        LR 0.000013    Time 0.021826    
2023-01-06 17:04:03,828 - Epoch: [108][  200/  246]    Overall Loss 0.270359    Objective Loss 0.270359                                        LR 0.000013    Time 0.021607    
2023-01-06 17:04:04,001 - Epoch: [108][  210/  246]    Overall Loss 0.271339    Objective Loss 0.271339                                        LR 0.000013    Time 0.021400    
2023-01-06 17:04:04,184 - Epoch: [108][  220/  246]    Overall Loss 0.272682    Objective Loss 0.272682                                        LR 0.000013    Time 0.021260    
2023-01-06 17:04:04,368 - Epoch: [108][  230/  246]    Overall Loss 0.272980    Objective Loss 0.272980                                        LR 0.000013    Time 0.021134    
2023-01-06 17:04:04,562 - Epoch: [108][  240/  246]    Overall Loss 0.273277    Objective Loss 0.273277                                        LR 0.000013    Time 0.021059    
2023-01-06 17:04:04,656 - Epoch: [108][  246/  246]    Overall Loss 0.273058    Objective Loss 0.273058    Top1 90.669856    LR 0.000013    Time 0.020927    
2023-01-06 17:04:04,808 - --- validate (epoch=108)-----------
2023-01-06 17:04:04,809 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:05,262 - Epoch: [108][   10/   28]    Loss 0.291832    Top1 89.453125    
2023-01-06 17:04:05,375 - Epoch: [108][   20/   28]    Loss 0.291236    Top1 89.394531    
2023-01-06 17:04:05,442 - Epoch: [108][   28/   28]    Loss 0.281122    Top1 89.750930    
2023-01-06 17:04:05,581 - ==> Top1: 89.751    Loss: 0.281

2023-01-06 17:04:05,581 - ==> Confusion:
[[ 204   13  222]
 [  17  225  360]
 [  46   58 5841]]

2023-01-06 17:04:05,582 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 155168 on epoch: 107]
2023-01-06 17:04:05,582 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:05,589 - 

2023-01-06 17:04:05,589 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:06,263 - Epoch: [109][   10/  246]    Overall Loss 0.292921    Objective Loss 0.292921                                        LR 0.000013    Time 0.067369    
2023-01-06 17:04:06,435 - Epoch: [109][   20/  246]    Overall Loss 0.280816    Objective Loss 0.280816                                        LR 0.000013    Time 0.042286    
2023-01-06 17:04:06,633 - Epoch: [109][   30/  246]    Overall Loss 0.276030    Objective Loss 0.276030                                        LR 0.000013    Time 0.034763    
2023-01-06 17:04:06,833 - Epoch: [109][   40/  246]    Overall Loss 0.274287    Objective Loss 0.274287                                        LR 0.000013    Time 0.031060    
2023-01-06 17:04:07,052 - Epoch: [109][   50/  246]    Overall Loss 0.271815    Objective Loss 0.271815                                        LR 0.000013    Time 0.029219    
2023-01-06 17:04:07,262 - Epoch: [109][   60/  246]    Overall Loss 0.272801    Objective Loss 0.272801                                        LR 0.000013    Time 0.027842    
2023-01-06 17:04:07,435 - Epoch: [109][   70/  246]    Overall Loss 0.271543    Objective Loss 0.271543                                        LR 0.000013    Time 0.026332    
2023-01-06 17:04:07,599 - Epoch: [109][   80/  246]    Overall Loss 0.274912    Objective Loss 0.274912                                        LR 0.000013    Time 0.025092    
2023-01-06 17:04:07,763 - Epoch: [109][   90/  246]    Overall Loss 0.275626    Objective Loss 0.275626                                        LR 0.000013    Time 0.024123    
2023-01-06 17:04:07,928 - Epoch: [109][  100/  246]    Overall Loss 0.273731    Objective Loss 0.273731                                        LR 0.000013    Time 0.023350    
2023-01-06 17:04:08,092 - Epoch: [109][  110/  246]    Overall Loss 0.273485    Objective Loss 0.273485                                        LR 0.000013    Time 0.022721    
2023-01-06 17:04:08,256 - Epoch: [109][  120/  246]    Overall Loss 0.273030    Objective Loss 0.273030                                        LR 0.000013    Time 0.022188    
2023-01-06 17:04:08,420 - Epoch: [109][  130/  246]    Overall Loss 0.273065    Objective Loss 0.273065                                        LR 0.000013    Time 0.021743    
2023-01-06 17:04:08,584 - Epoch: [109][  140/  246]    Overall Loss 0.272544    Objective Loss 0.272544                                        LR 0.000013    Time 0.021360    
2023-01-06 17:04:08,749 - Epoch: [109][  150/  246]    Overall Loss 0.273416    Objective Loss 0.273416                                        LR 0.000013    Time 0.021035    
2023-01-06 17:04:08,913 - Epoch: [109][  160/  246]    Overall Loss 0.273423    Objective Loss 0.273423                                        LR 0.000013    Time 0.020739    
2023-01-06 17:04:09,076 - Epoch: [109][  170/  246]    Overall Loss 0.273214    Objective Loss 0.273214                                        LR 0.000013    Time 0.020479    
2023-01-06 17:04:09,240 - Epoch: [109][  180/  246]    Overall Loss 0.273485    Objective Loss 0.273485                                        LR 0.000013    Time 0.020249    
2023-01-06 17:04:09,404 - Epoch: [109][  190/  246]    Overall Loss 0.272284    Objective Loss 0.272284                                        LR 0.000013    Time 0.020044    
2023-01-06 17:04:09,567 - Epoch: [109][  200/  246]    Overall Loss 0.271778    Objective Loss 0.271778                                        LR 0.000013    Time 0.019858    
2023-01-06 17:04:09,731 - Epoch: [109][  210/  246]    Overall Loss 0.271251    Objective Loss 0.271251                                        LR 0.000013    Time 0.019691    
2023-01-06 17:04:09,895 - Epoch: [109][  220/  246]    Overall Loss 0.270474    Objective Loss 0.270474                                        LR 0.000013    Time 0.019538    
2023-01-06 17:04:10,059 - Epoch: [109][  230/  246]    Overall Loss 0.270406    Objective Loss 0.270406                                        LR 0.000013    Time 0.019402    
2023-01-06 17:04:10,256 - Epoch: [109][  240/  246]    Overall Loss 0.270271    Objective Loss 0.270271                                        LR 0.000013    Time 0.019413    
2023-01-06 17:04:10,353 - Epoch: [109][  246/  246]    Overall Loss 0.271054    Objective Loss 0.271054    Top1 87.799043    LR 0.000013    Time 0.019334    
2023-01-06 17:04:10,485 - --- validate (epoch=109)-----------
2023-01-06 17:04:10,485 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:10,931 - Epoch: [109][   10/   28]    Loss 0.279035    Top1 89.921875    
2023-01-06 17:04:11,053 - Epoch: [109][   20/   28]    Loss 0.275675    Top1 89.960938    
2023-01-06 17:04:11,120 - Epoch: [109][   28/   28]    Loss 0.280758    Top1 89.894074    
2023-01-06 17:04:11,257 - ==> Top1: 89.894    Loss: 0.281

2023-01-06 17:04:11,257 - ==> Confusion:
[[ 233   13  193]
 [  18  244  340]
 [  67   75 5803]]

2023-01-06 17:04:11,258 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 155168 on epoch: 107]
2023-01-06 17:04:11,258 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:11,264 - 

2023-01-06 17:04:11,264 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:11,814 - Epoch: [110][   10/  246]    Overall Loss 0.280287    Objective Loss 0.280287                                        LR 0.000013    Time 0.054868    
2023-01-06 17:04:11,986 - Epoch: [110][   20/  246]    Overall Loss 0.287688    Objective Loss 0.287688                                        LR 0.000013    Time 0.036052    
2023-01-06 17:04:12,177 - Epoch: [110][   30/  246]    Overall Loss 0.285644    Objective Loss 0.285644                                        LR 0.000013    Time 0.030383    
2023-01-06 17:04:12,366 - Epoch: [110][   40/  246]    Overall Loss 0.284987    Objective Loss 0.284987                                        LR 0.000013    Time 0.027497    
2023-01-06 17:04:12,551 - Epoch: [110][   50/  246]    Overall Loss 0.283520    Objective Loss 0.283520                                        LR 0.000013    Time 0.025690    
2023-01-06 17:04:12,745 - Epoch: [110][   60/  246]    Overall Loss 0.284607    Objective Loss 0.284607                                        LR 0.000013    Time 0.024634    
2023-01-06 17:04:12,931 - Epoch: [110][   70/  246]    Overall Loss 0.283606    Objective Loss 0.283606                                        LR 0.000013    Time 0.023773    
2023-01-06 17:04:13,111 - Epoch: [110][   80/  246]    Overall Loss 0.282477    Objective Loss 0.282477                                        LR 0.000013    Time 0.023047    
2023-01-06 17:04:13,286 - Epoch: [110][   90/  246]    Overall Loss 0.280149    Objective Loss 0.280149                                        LR 0.000013    Time 0.022430    
2023-01-06 17:04:13,465 - Epoch: [110][  100/  246]    Overall Loss 0.279449    Objective Loss 0.279449                                        LR 0.000013    Time 0.021970    
2023-01-06 17:04:13,642 - Epoch: [110][  110/  246]    Overall Loss 0.277866    Objective Loss 0.277866                                        LR 0.000013    Time 0.021581    
2023-01-06 17:04:13,819 - Epoch: [110][  120/  246]    Overall Loss 0.276913    Objective Loss 0.276913                                        LR 0.000013    Time 0.021254    
2023-01-06 17:04:14,000 - Epoch: [110][  130/  246]    Overall Loss 0.275295    Objective Loss 0.275295                                        LR 0.000013    Time 0.021005    
2023-01-06 17:04:14,174 - Epoch: [110][  140/  246]    Overall Loss 0.274466    Objective Loss 0.274466                                        LR 0.000013    Time 0.020752    
2023-01-06 17:04:14,342 - Epoch: [110][  150/  246]    Overall Loss 0.275043    Objective Loss 0.275043                                        LR 0.000013    Time 0.020481    
2023-01-06 17:04:14,511 - Epoch: [110][  160/  246]    Overall Loss 0.273547    Objective Loss 0.273547                                        LR 0.000013    Time 0.020258    
2023-01-06 17:04:14,687 - Epoch: [110][  170/  246]    Overall Loss 0.272042    Objective Loss 0.272042                                        LR 0.000013    Time 0.020100    
2023-01-06 17:04:14,863 - Epoch: [110][  180/  246]    Overall Loss 0.271427    Objective Loss 0.271427                                        LR 0.000013    Time 0.019959    
2023-01-06 17:04:15,038 - Epoch: [110][  190/  246]    Overall Loss 0.270573    Objective Loss 0.270573                                        LR 0.000013    Time 0.019826    
2023-01-06 17:04:15,213 - Epoch: [110][  200/  246]    Overall Loss 0.271965    Objective Loss 0.271965                                        LR 0.000013    Time 0.019710    
2023-01-06 17:04:15,395 - Epoch: [110][  210/  246]    Overall Loss 0.271952    Objective Loss 0.271952                                        LR 0.000013    Time 0.019635    
2023-01-06 17:04:15,579 - Epoch: [110][  220/  246]    Overall Loss 0.271853    Objective Loss 0.271853                                        LR 0.000013    Time 0.019579    
2023-01-06 17:04:15,759 - Epoch: [110][  230/  246]    Overall Loss 0.271239    Objective Loss 0.271239                                        LR 0.000013    Time 0.019509    
2023-01-06 17:04:15,946 - Epoch: [110][  240/  246]    Overall Loss 0.271765    Objective Loss 0.271765                                        LR 0.000013    Time 0.019473    
2023-01-06 17:04:16,035 - Epoch: [110][  246/  246]    Overall Loss 0.271856    Objective Loss 0.271856    Top1 91.626794    LR 0.000013    Time 0.019359    
2023-01-06 17:04:16,156 - --- validate (epoch=110)-----------
2023-01-06 17:04:16,156 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:16,601 - Epoch: [110][   10/   28]    Loss 0.274261    Top1 90.156250    
2023-01-06 17:04:16,717 - Epoch: [110][   20/   28]    Loss 0.270338    Top1 90.410156    
2023-01-06 17:04:16,787 - Epoch: [110][   28/   28]    Loss 0.278477    Top1 90.008589    
2023-01-06 17:04:16,933 - ==> Top1: 90.009    Loss: 0.278

2023-01-06 17:04:16,933 - ==> Confusion:
[[ 233   16  190]
 [  18  261  323]
 [  76   75 5794]]

2023-01-06 17:04:16,934 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 155168 on epoch: 110]
2023-01-06 17:04:16,934 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:16,941 - 

2023-01-06 17:04:16,942 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:17,646 - Epoch: [111][   10/  246]    Overall Loss 0.259235    Objective Loss 0.259235                                        LR 0.000013    Time 0.070352    
2023-01-06 17:04:17,842 - Epoch: [111][   20/  246]    Overall Loss 0.271932    Objective Loss 0.271932                                        LR 0.000013    Time 0.044990    
2023-01-06 17:04:18,034 - Epoch: [111][   30/  246]    Overall Loss 0.270600    Objective Loss 0.270600                                        LR 0.000013    Time 0.036377    
2023-01-06 17:04:18,212 - Epoch: [111][   40/  246]    Overall Loss 0.268963    Objective Loss 0.268963                                        LR 0.000013    Time 0.031715    
2023-01-06 17:04:18,387 - Epoch: [111][   50/  246]    Overall Loss 0.270432    Objective Loss 0.270432                                        LR 0.000013    Time 0.028864    
2023-01-06 17:04:18,568 - Epoch: [111][   60/  246]    Overall Loss 0.272402    Objective Loss 0.272402                                        LR 0.000013    Time 0.027071    
2023-01-06 17:04:18,759 - Epoch: [111][   70/  246]    Overall Loss 0.271375    Objective Loss 0.271375                                        LR 0.000013    Time 0.025927    
2023-01-06 17:04:18,974 - Epoch: [111][   80/  246]    Overall Loss 0.271769    Objective Loss 0.271769                                        LR 0.000013    Time 0.025363    
2023-01-06 17:04:19,203 - Epoch: [111][   90/  246]    Overall Loss 0.271420    Objective Loss 0.271420                                        LR 0.000013    Time 0.025089    
2023-01-06 17:04:19,411 - Epoch: [111][  100/  246]    Overall Loss 0.272069    Objective Loss 0.272069                                        LR 0.000013    Time 0.024651    
2023-01-06 17:04:19,614 - Epoch: [111][  110/  246]    Overall Loss 0.271145    Objective Loss 0.271145                                        LR 0.000013    Time 0.024255    
2023-01-06 17:04:19,817 - Epoch: [111][  120/  246]    Overall Loss 0.269966    Objective Loss 0.269966                                        LR 0.000013    Time 0.023920    
2023-01-06 17:04:20,012 - Epoch: [111][  130/  246]    Overall Loss 0.269955    Objective Loss 0.269955                                        LR 0.000013    Time 0.023581    
2023-01-06 17:04:20,190 - Epoch: [111][  140/  246]    Overall Loss 0.269850    Objective Loss 0.269850                                        LR 0.000013    Time 0.023160    
2023-01-06 17:04:20,358 - Epoch: [111][  150/  246]    Overall Loss 0.269854    Objective Loss 0.269854                                        LR 0.000013    Time 0.022736    
2023-01-06 17:04:20,529 - Epoch: [111][  160/  246]    Overall Loss 0.269458    Objective Loss 0.269458                                        LR 0.000013    Time 0.022382    
2023-01-06 17:04:20,696 - Epoch: [111][  170/  246]    Overall Loss 0.268720    Objective Loss 0.268720                                        LR 0.000013    Time 0.022047    
2023-01-06 17:04:20,868 - Epoch: [111][  180/  246]    Overall Loss 0.269754    Objective Loss 0.269754                                        LR 0.000013    Time 0.021774    
2023-01-06 17:04:21,033 - Epoch: [111][  190/  246]    Overall Loss 0.271094    Objective Loss 0.271094                                        LR 0.000013    Time 0.021496    
2023-01-06 17:04:21,199 - Epoch: [111][  200/  246]    Overall Loss 0.271720    Objective Loss 0.271720                                        LR 0.000013    Time 0.021251    
2023-01-06 17:04:21,394 - Epoch: [111][  210/  246]    Overall Loss 0.271336    Objective Loss 0.271336                                        LR 0.000013    Time 0.021163    
2023-01-06 17:04:21,594 - Epoch: [111][  220/  246]    Overall Loss 0.270453    Objective Loss 0.270453                                        LR 0.000013    Time 0.021113    
2023-01-06 17:04:21,800 - Epoch: [111][  230/  246]    Overall Loss 0.270341    Objective Loss 0.270341                                        LR 0.000013    Time 0.021085    
2023-01-06 17:04:22,008 - Epoch: [111][  240/  246]    Overall Loss 0.270839    Objective Loss 0.270839                                        LR 0.000013    Time 0.021072    
2023-01-06 17:04:22,108 - Epoch: [111][  246/  246]    Overall Loss 0.271350    Objective Loss 0.271350    Top1 90.430622    LR 0.000013    Time 0.020963    
2023-01-06 17:04:22,241 - --- validate (epoch=111)-----------
2023-01-06 17:04:22,241 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:22,704 - Epoch: [111][   10/   28]    Loss 0.293068    Top1 89.062500    
2023-01-06 17:04:22,821 - Epoch: [111][   20/   28]    Loss 0.272578    Top1 89.941406    
2023-01-06 17:04:22,892 - Epoch: [111][   28/   28]    Loss 0.280056    Top1 89.836816    
2023-01-06 17:04:23,033 - ==> Top1: 89.837    Loss: 0.280

2023-01-06 17:04:23,033 - ==> Confusion:
[[ 228   15  196]
 [  19  238  345]
 [  68   67 5810]]

2023-01-06 17:04:23,035 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 155168 on epoch: 110]
2023-01-06 17:04:23,035 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:23,041 - 

2023-01-06 17:04:23,041 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:23,754 - Epoch: [112][   10/  246]    Overall Loss 0.269662    Objective Loss 0.269662                                        LR 0.000013    Time 0.071256    
2023-01-06 17:04:23,927 - Epoch: [112][   20/  246]    Overall Loss 0.264267    Objective Loss 0.264267                                        LR 0.000013    Time 0.044189    
2023-01-06 17:04:24,115 - Epoch: [112][   30/  246]    Overall Loss 0.271046    Objective Loss 0.271046                                        LR 0.000013    Time 0.035731    
2023-01-06 17:04:24,324 - Epoch: [112][   40/  246]    Overall Loss 0.273932    Objective Loss 0.273932                                        LR 0.000013    Time 0.032014    
2023-01-06 17:04:24,532 - Epoch: [112][   50/  246]    Overall Loss 0.274723    Objective Loss 0.274723                                        LR 0.000013    Time 0.029762    
2023-01-06 17:04:24,741 - Epoch: [112][   60/  246]    Overall Loss 0.275442    Objective Loss 0.275442                                        LR 0.000013    Time 0.028274    
2023-01-06 17:04:24,935 - Epoch: [112][   70/  246]    Overall Loss 0.276306    Objective Loss 0.276306                                        LR 0.000013    Time 0.027003    
2023-01-06 17:04:25,139 - Epoch: [112][   80/  246]    Overall Loss 0.276091    Objective Loss 0.276091                                        LR 0.000013    Time 0.026177    
2023-01-06 17:04:25,343 - Epoch: [112][   90/  246]    Overall Loss 0.275563    Objective Loss 0.275563                                        LR 0.000013    Time 0.025529    
2023-01-06 17:04:25,509 - Epoch: [112][  100/  246]    Overall Loss 0.277383    Objective Loss 0.277383                                        LR 0.000013    Time 0.024630    
2023-01-06 17:04:25,681 - Epoch: [112][  110/  246]    Overall Loss 0.278086    Objective Loss 0.278086                                        LR 0.000013    Time 0.023950    
2023-01-06 17:04:25,851 - Epoch: [112][  120/  246]    Overall Loss 0.277428    Objective Loss 0.277428                                        LR 0.000013    Time 0.023369    
2023-01-06 17:04:26,023 - Epoch: [112][  130/  246]    Overall Loss 0.276949    Objective Loss 0.276949                                        LR 0.000013    Time 0.022891    
2023-01-06 17:04:26,193 - Epoch: [112][  140/  246]    Overall Loss 0.276886    Objective Loss 0.276886                                        LR 0.000013    Time 0.022467    
2023-01-06 17:04:26,366 - Epoch: [112][  150/  246]    Overall Loss 0.276216    Objective Loss 0.276216                                        LR 0.000013    Time 0.022122    
2023-01-06 17:04:26,540 - Epoch: [112][  160/  246]    Overall Loss 0.275113    Objective Loss 0.275113                                        LR 0.000013    Time 0.021820    
2023-01-06 17:04:26,712 - Epoch: [112][  170/  246]    Overall Loss 0.274420    Objective Loss 0.274420                                        LR 0.000013    Time 0.021551    
2023-01-06 17:04:26,880 - Epoch: [112][  180/  246]    Overall Loss 0.274051    Objective Loss 0.274051                                        LR 0.000013    Time 0.021282    
2023-01-06 17:04:27,050 - Epoch: [112][  190/  246]    Overall Loss 0.273359    Objective Loss 0.273359                                        LR 0.000013    Time 0.021053    
2023-01-06 17:04:27,216 - Epoch: [112][  200/  246]    Overall Loss 0.274438    Objective Loss 0.274438                                        LR 0.000013    Time 0.020829    
2023-01-06 17:04:27,382 - Epoch: [112][  210/  246]    Overall Loss 0.273210    Objective Loss 0.273210                                        LR 0.000013    Time 0.020630    
2023-01-06 17:04:27,549 - Epoch: [112][  220/  246]    Overall Loss 0.272750    Objective Loss 0.272750                                        LR 0.000013    Time 0.020445    
2023-01-06 17:04:27,716 - Epoch: [112][  230/  246]    Overall Loss 0.271893    Objective Loss 0.271893                                        LR 0.000013    Time 0.020283    
2023-01-06 17:04:27,897 - Epoch: [112][  240/  246]    Overall Loss 0.272154    Objective Loss 0.272154                                        LR 0.000013    Time 0.020189    
2023-01-06 17:04:27,987 - Epoch: [112][  246/  246]    Overall Loss 0.271819    Objective Loss 0.271819    Top1 89.473684    LR 0.000013    Time 0.020065    
2023-01-06 17:04:28,127 - --- validate (epoch=112)-----------
2023-01-06 17:04:28,127 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:28,583 - Epoch: [112][   10/   28]    Loss 0.274324    Top1 89.570312    
2023-01-06 17:04:28,698 - Epoch: [112][   20/   28]    Loss 0.273381    Top1 89.765625    
2023-01-06 17:04:28,768 - Epoch: [112][   28/   28]    Loss 0.277153    Top1 89.636416    
2023-01-06 17:04:28,913 - ==> Top1: 89.636    Loss: 0.277

2023-01-06 17:04:28,913 - ==> Confusion:
[[ 222   13  204]
 [  19  214  369]
 [  65   54 5826]]

2023-01-06 17:04:28,915 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 155168 on epoch: 110]
2023-01-06 17:04:28,915 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:28,921 - 

2023-01-06 17:04:28,921 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:29,489 - Epoch: [113][   10/  246]    Overall Loss 0.246549    Objective Loss 0.246549                                        LR 0.000013    Time 0.056721    
2023-01-06 17:04:29,660 - Epoch: [113][   20/  246]    Overall Loss 0.261616    Objective Loss 0.261616                                        LR 0.000013    Time 0.036873    
2023-01-06 17:04:29,836 - Epoch: [113][   30/  246]    Overall Loss 0.267673    Objective Loss 0.267673                                        LR 0.000013    Time 0.030441    
2023-01-06 17:04:30,038 - Epoch: [113][   40/  246]    Overall Loss 0.269933    Objective Loss 0.269933                                        LR 0.000013    Time 0.027871    
2023-01-06 17:04:30,232 - Epoch: [113][   50/  246]    Overall Loss 0.270743    Objective Loss 0.270743                                        LR 0.000013    Time 0.026161    
2023-01-06 17:04:30,426 - Epoch: [113][   60/  246]    Overall Loss 0.270997    Objective Loss 0.270997                                        LR 0.000013    Time 0.025028    
2023-01-06 17:04:30,622 - Epoch: [113][   70/  246]    Overall Loss 0.268471    Objective Loss 0.268471                                        LR 0.000013    Time 0.024245    
2023-01-06 17:04:30,819 - Epoch: [113][   80/  246]    Overall Loss 0.269987    Objective Loss 0.269987                                        LR 0.000013    Time 0.023671    
2023-01-06 17:04:31,013 - Epoch: [113][   90/  246]    Overall Loss 0.270563    Objective Loss 0.270563                                        LR 0.000013    Time 0.023198    
2023-01-06 17:04:31,208 - Epoch: [113][  100/  246]    Overall Loss 0.269417    Objective Loss 0.269417                                        LR 0.000013    Time 0.022825    
2023-01-06 17:04:31,404 - Epoch: [113][  110/  246]    Overall Loss 0.271935    Objective Loss 0.271935                                        LR 0.000013    Time 0.022530    
2023-01-06 17:04:31,593 - Epoch: [113][  120/  246]    Overall Loss 0.270644    Objective Loss 0.270644                                        LR 0.000013    Time 0.022220    
2023-01-06 17:04:31,791 - Epoch: [113][  130/  246]    Overall Loss 0.270302    Objective Loss 0.270302                                        LR 0.000013    Time 0.022031    
2023-01-06 17:04:31,980 - Epoch: [113][  140/  246]    Overall Loss 0.270524    Objective Loss 0.270524                                        LR 0.000013    Time 0.021805    
2023-01-06 17:04:32,169 - Epoch: [113][  150/  246]    Overall Loss 0.271391    Objective Loss 0.271391                                        LR 0.000013    Time 0.021606    
2023-01-06 17:04:32,362 - Epoch: [113][  160/  246]    Overall Loss 0.270950    Objective Loss 0.270950                                        LR 0.000013    Time 0.021465    
2023-01-06 17:04:32,550 - Epoch: [113][  170/  246]    Overall Loss 0.270919    Objective Loss 0.270919                                        LR 0.000013    Time 0.021301    
2023-01-06 17:04:32,737 - Epoch: [113][  180/  246]    Overall Loss 0.270916    Objective Loss 0.270916                                        LR 0.000013    Time 0.021156    
2023-01-06 17:04:32,930 - Epoch: [113][  190/  246]    Overall Loss 0.270195    Objective Loss 0.270195                                        LR 0.000013    Time 0.021054    
2023-01-06 17:04:33,131 - Epoch: [113][  200/  246]    Overall Loss 0.270610    Objective Loss 0.270610                                        LR 0.000013    Time 0.021007    
2023-01-06 17:04:33,313 - Epoch: [113][  210/  246]    Overall Loss 0.270579    Objective Loss 0.270579                                        LR 0.000013    Time 0.020872    
2023-01-06 17:04:33,499 - Epoch: [113][  220/  246]    Overall Loss 0.270341    Objective Loss 0.270341                                        LR 0.000013    Time 0.020767    
2023-01-06 17:04:33,695 - Epoch: [113][  230/  246]    Overall Loss 0.270997    Objective Loss 0.270997                                        LR 0.000013    Time 0.020713    
2023-01-06 17:04:33,896 - Epoch: [113][  240/  246]    Overall Loss 0.270977    Objective Loss 0.270977                                        LR 0.000013    Time 0.020687    
2023-01-06 17:04:33,994 - Epoch: [113][  246/  246]    Overall Loss 0.270514    Objective Loss 0.270514    Top1 91.866029    LR 0.000013    Time 0.020578    
2023-01-06 17:04:34,133 - --- validate (epoch=113)-----------
2023-01-06 17:04:34,133 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:34,582 - Epoch: [113][   10/   28]    Loss 0.305778    Top1 88.593750    
2023-01-06 17:04:34,695 - Epoch: [113][   20/   28]    Loss 0.275546    Top1 90.058594    
2023-01-06 17:04:34,762 - Epoch: [113][   28/   28]    Loss 0.276568    Top1 89.994274    
2023-01-06 17:04:34,884 - ==> Top1: 89.994    Loss: 0.277

2023-01-06 17:04:34,884 - ==> Confusion:
[[ 212   17  210]
 [  12  247  343]
 [  47   70 5828]]

2023-01-06 17:04:34,885 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 155168 on epoch: 110]
2023-01-06 17:04:34,885 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:34,891 - 

2023-01-06 17:04:34,892 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:35,656 - Epoch: [114][   10/  246]    Overall Loss 0.280362    Objective Loss 0.280362                                        LR 0.000013    Time 0.076389    
2023-01-06 17:04:35,879 - Epoch: [114][   20/  246]    Overall Loss 0.277358    Objective Loss 0.277358                                        LR 0.000013    Time 0.049217    
2023-01-06 17:04:36,103 - Epoch: [114][   30/  246]    Overall Loss 0.276324    Objective Loss 0.276324                                        LR 0.000013    Time 0.040267    
2023-01-06 17:04:36,331 - Epoch: [114][   40/  246]    Overall Loss 0.276867    Objective Loss 0.276867                                        LR 0.000013    Time 0.035908    
2023-01-06 17:04:36,543 - Epoch: [114][   50/  246]    Overall Loss 0.271430    Objective Loss 0.271430                                        LR 0.000013    Time 0.032960    
2023-01-06 17:04:36,771 - Epoch: [114][   60/  246]    Overall Loss 0.271261    Objective Loss 0.271261                                        LR 0.000013    Time 0.031252    
2023-01-06 17:04:37,003 - Epoch: [114][   70/  246]    Overall Loss 0.268041    Objective Loss 0.268041                                        LR 0.000013    Time 0.030096    
2023-01-06 17:04:37,232 - Epoch: [114][   80/  246]    Overall Loss 0.267302    Objective Loss 0.267302                                        LR 0.000013    Time 0.029162    
2023-01-06 17:04:37,463 - Epoch: [114][   90/  246]    Overall Loss 0.269065    Objective Loss 0.269065                                        LR 0.000013    Time 0.028484    
2023-01-06 17:04:37,707 - Epoch: [114][  100/  246]    Overall Loss 0.270281    Objective Loss 0.270281                                        LR 0.000013    Time 0.028068    
2023-01-06 17:04:37,948 - Epoch: [114][  110/  246]    Overall Loss 0.269705    Objective Loss 0.269705                                        LR 0.000013    Time 0.027704    
2023-01-06 17:04:38,197 - Epoch: [114][  120/  246]    Overall Loss 0.269667    Objective Loss 0.269667                                        LR 0.000013    Time 0.027463    
2023-01-06 17:04:38,455 - Epoch: [114][  130/  246]    Overall Loss 0.270077    Objective Loss 0.270077                                        LR 0.000013    Time 0.027332    
2023-01-06 17:04:38,691 - Epoch: [114][  140/  246]    Overall Loss 0.271300    Objective Loss 0.271300                                        LR 0.000013    Time 0.027066    
2023-01-06 17:04:38,931 - Epoch: [114][  150/  246]    Overall Loss 0.270614    Objective Loss 0.270614                                        LR 0.000013    Time 0.026854    
2023-01-06 17:04:39,147 - Epoch: [114][  160/  246]    Overall Loss 0.270310    Objective Loss 0.270310                                        LR 0.000013    Time 0.026525    
2023-01-06 17:04:39,344 - Epoch: [114][  170/  246]    Overall Loss 0.270821    Objective Loss 0.270821                                        LR 0.000013    Time 0.026120    
2023-01-06 17:04:39,541 - Epoch: [114][  180/  246]    Overall Loss 0.269424    Objective Loss 0.269424                                        LR 0.000013    Time 0.025763    
2023-01-06 17:04:39,745 - Epoch: [114][  190/  246]    Overall Loss 0.268802    Objective Loss 0.268802                                        LR 0.000013    Time 0.025478    
2023-01-06 17:04:39,937 - Epoch: [114][  200/  246]    Overall Loss 0.268659    Objective Loss 0.268659                                        LR 0.000013    Time 0.025159    
2023-01-06 17:04:40,139 - Epoch: [114][  210/  246]    Overall Loss 0.269508    Objective Loss 0.269508                                        LR 0.000013    Time 0.024925    
2023-01-06 17:04:40,334 - Epoch: [114][  220/  246]    Overall Loss 0.269586    Objective Loss 0.269586                                        LR 0.000013    Time 0.024676    
2023-01-06 17:04:40,538 - Epoch: [114][  230/  246]    Overall Loss 0.270103    Objective Loss 0.270103                                        LR 0.000013    Time 0.024488    
2023-01-06 17:04:40,748 - Epoch: [114][  240/  246]    Overall Loss 0.269352    Objective Loss 0.269352                                        LR 0.000013    Time 0.024342    
2023-01-06 17:04:40,845 - Epoch: [114][  246/  246]    Overall Loss 0.269906    Objective Loss 0.269906    Top1 88.755981    LR 0.000013    Time 0.024141    
2023-01-06 17:04:40,987 - --- validate (epoch=114)-----------
2023-01-06 17:04:40,987 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:41,442 - Epoch: [114][   10/   28]    Loss 0.273744    Top1 90.468750    
2023-01-06 17:04:41,552 - Epoch: [114][   20/   28]    Loss 0.277387    Top1 90.078125    
2023-01-06 17:04:41,619 - Epoch: [114][   28/   28]    Loss 0.276005    Top1 90.037217    
2023-01-06 17:04:41,782 - ==> Top1: 90.037    Loss: 0.276

2023-01-06 17:04:41,782 - ==> Confusion:
[[ 235   13  191]
 [  18  251  333]
 [  73   68 5804]]

2023-01-06 17:04:41,783 - ==> Best [Top1: 90.037   Sparsity:0.00   Params: 155168 on epoch: 114]
2023-01-06 17:04:41,783 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:41,791 - 

2023-01-06 17:04:41,791 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:42,512 - Epoch: [115][   10/  246]    Overall Loss 0.250660    Objective Loss 0.250660                                        LR 0.000013    Time 0.072045    
2023-01-06 17:04:42,697 - Epoch: [115][   20/  246]    Overall Loss 0.257275    Objective Loss 0.257275                                        LR 0.000013    Time 0.045246    
2023-01-06 17:04:42,900 - Epoch: [115][   30/  246]    Overall Loss 0.258491    Objective Loss 0.258491                                        LR 0.000013    Time 0.036912    
2023-01-06 17:04:43,088 - Epoch: [115][   40/  246]    Overall Loss 0.264400    Objective Loss 0.264400                                        LR 0.000013    Time 0.032383    
2023-01-06 17:04:43,272 - Epoch: [115][   50/  246]    Overall Loss 0.262994    Objective Loss 0.262994                                        LR 0.000013    Time 0.029571    
2023-01-06 17:04:43,469 - Epoch: [115][   60/  246]    Overall Loss 0.262697    Objective Loss 0.262697                                        LR 0.000013    Time 0.027928    
2023-01-06 17:04:43,672 - Epoch: [115][   70/  246]    Overall Loss 0.263701    Objective Loss 0.263701                                        LR 0.000013    Time 0.026834    
2023-01-06 17:04:43,849 - Epoch: [115][   80/  246]    Overall Loss 0.264435    Objective Loss 0.264435                                        LR 0.000013    Time 0.025681    
2023-01-06 17:04:44,040 - Epoch: [115][   90/  246]    Overall Loss 0.263718    Objective Loss 0.263718                                        LR 0.000013    Time 0.024944    
2023-01-06 17:04:44,228 - Epoch: [115][  100/  246]    Overall Loss 0.262921    Objective Loss 0.262921                                        LR 0.000013    Time 0.024332    
2023-01-06 17:04:44,423 - Epoch: [115][  110/  246]    Overall Loss 0.263090    Objective Loss 0.263090                                        LR 0.000013    Time 0.023889    
2023-01-06 17:04:44,616 - Epoch: [115][  120/  246]    Overall Loss 0.264704    Objective Loss 0.264704                                        LR 0.000013    Time 0.023506    
2023-01-06 17:04:44,809 - Epoch: [115][  130/  246]    Overall Loss 0.264835    Objective Loss 0.264835                                        LR 0.000013    Time 0.023175    
2023-01-06 17:04:45,001 - Epoch: [115][  140/  246]    Overall Loss 0.264824    Objective Loss 0.264824                                        LR 0.000013    Time 0.022886    
2023-01-06 17:04:45,191 - Epoch: [115][  150/  246]    Overall Loss 0.264222    Objective Loss 0.264222                                        LR 0.000013    Time 0.022625    
2023-01-06 17:04:45,381 - Epoch: [115][  160/  246]    Overall Loss 0.265061    Objective Loss 0.265061                                        LR 0.000013    Time 0.022396    
2023-01-06 17:04:45,573 - Epoch: [115][  170/  246]    Overall Loss 0.266436    Objective Loss 0.266436                                        LR 0.000013    Time 0.022206    
2023-01-06 17:04:45,765 - Epoch: [115][  180/  246]    Overall Loss 0.266330    Objective Loss 0.266330                                        LR 0.000013    Time 0.022040    
2023-01-06 17:04:45,957 - Epoch: [115][  190/  246]    Overall Loss 0.266263    Objective Loss 0.266263                                        LR 0.000013    Time 0.021886    
2023-01-06 17:04:46,147 - Epoch: [115][  200/  246]    Overall Loss 0.266702    Objective Loss 0.266702                                        LR 0.000013    Time 0.021741    
2023-01-06 17:04:46,338 - Epoch: [115][  210/  246]    Overall Loss 0.267437    Objective Loss 0.267437                                        LR 0.000013    Time 0.021613    
2023-01-06 17:04:46,529 - Epoch: [115][  220/  246]    Overall Loss 0.268038    Objective Loss 0.268038                                        LR 0.000013    Time 0.021498    
2023-01-06 17:04:46,722 - Epoch: [115][  230/  246]    Overall Loss 0.268615    Objective Loss 0.268615                                        LR 0.000013    Time 0.021398    
2023-01-06 17:04:46,928 - Epoch: [115][  240/  246]    Overall Loss 0.268648    Objective Loss 0.268648                                        LR 0.000013    Time 0.021364    
2023-01-06 17:04:47,025 - Epoch: [115][  246/  246]    Overall Loss 0.268487    Objective Loss 0.268487    Top1 91.387560    LR 0.000013    Time 0.021236    
2023-01-06 17:04:47,169 - --- validate (epoch=115)-----------
2023-01-06 17:04:47,169 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:47,619 - Epoch: [115][   10/   28]    Loss 0.276109    Top1 89.804688    
2023-01-06 17:04:47,730 - Epoch: [115][   20/   28]    Loss 0.270969    Top1 89.960938    
2023-01-06 17:04:47,796 - Epoch: [115][   28/   28]    Loss 0.281537    Top1 89.679359    
2023-01-06 17:04:47,943 - ==> Top1: 89.679    Loss: 0.282

2023-01-06 17:04:47,944 - ==> Confusion:
[[ 222   12  205]
 [  18  209  375]
 [  64   47 5834]]

2023-01-06 17:04:47,945 - ==> Best [Top1: 90.037   Sparsity:0.00   Params: 155168 on epoch: 114]
2023-01-06 17:04:47,945 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:47,951 - 

2023-01-06 17:04:47,951 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:48,505 - Epoch: [116][   10/  246]    Overall Loss 0.281669    Objective Loss 0.281669                                        LR 0.000013    Time 0.055360    
2023-01-06 17:04:48,704 - Epoch: [116][   20/  246]    Overall Loss 0.277194    Objective Loss 0.277194                                        LR 0.000013    Time 0.037573    
2023-01-06 17:04:48,900 - Epoch: [116][   30/  246]    Overall Loss 0.276448    Objective Loss 0.276448                                        LR 0.000013    Time 0.031578    
2023-01-06 17:04:49,087 - Epoch: [116][   40/  246]    Overall Loss 0.274830    Objective Loss 0.274830                                        LR 0.000013    Time 0.028344    
2023-01-06 17:04:49,266 - Epoch: [116][   50/  246]    Overall Loss 0.273533    Objective Loss 0.273533                                        LR 0.000013    Time 0.026249    
2023-01-06 17:04:49,435 - Epoch: [116][   60/  246]    Overall Loss 0.268989    Objective Loss 0.268989                                        LR 0.000013    Time 0.024692    
2023-01-06 17:04:49,640 - Epoch: [116][   70/  246]    Overall Loss 0.273663    Objective Loss 0.273663                                        LR 0.000013    Time 0.024077    
2023-01-06 17:04:49,868 - Epoch: [116][   80/  246]    Overall Loss 0.271384    Objective Loss 0.271384                                        LR 0.000013    Time 0.023920    
2023-01-06 17:04:50,099 - Epoch: [116][   90/  246]    Overall Loss 0.270347    Objective Loss 0.270347                                        LR 0.000013    Time 0.023819    
2023-01-06 17:04:50,300 - Epoch: [116][  100/  246]    Overall Loss 0.271121    Objective Loss 0.271121                                        LR 0.000013    Time 0.023449    
2023-01-06 17:04:50,489 - Epoch: [116][  110/  246]    Overall Loss 0.268107    Objective Loss 0.268107                                        LR 0.000013    Time 0.023032    
2023-01-06 17:04:50,680 - Epoch: [116][  120/  246]    Overall Loss 0.268436    Objective Loss 0.268436                                        LR 0.000013    Time 0.022702    
2023-01-06 17:04:50,863 - Epoch: [116][  130/  246]    Overall Loss 0.268512    Objective Loss 0.268512                                        LR 0.000013    Time 0.022356    
2023-01-06 17:04:51,048 - Epoch: [116][  140/  246]    Overall Loss 0.267445    Objective Loss 0.267445                                        LR 0.000013    Time 0.022079    
2023-01-06 17:04:51,232 - Epoch: [116][  150/  246]    Overall Loss 0.267230    Objective Loss 0.267230                                        LR 0.000013    Time 0.021829    
2023-01-06 17:04:51,414 - Epoch: [116][  160/  246]    Overall Loss 0.267563    Objective Loss 0.267563                                        LR 0.000013    Time 0.021606    
2023-01-06 17:04:51,600 - Epoch: [116][  170/  246]    Overall Loss 0.268008    Objective Loss 0.268008                                        LR 0.000013    Time 0.021422    
2023-01-06 17:04:51,784 - Epoch: [116][  180/  246]    Overall Loss 0.267883    Objective Loss 0.267883                                        LR 0.000013    Time 0.021253    
2023-01-06 17:04:51,952 - Epoch: [116][  190/  246]    Overall Loss 0.267244    Objective Loss 0.267244                                        LR 0.000013    Time 0.021017    
2023-01-06 17:04:52,108 - Epoch: [116][  200/  246]    Overall Loss 0.267810    Objective Loss 0.267810                                        LR 0.000013    Time 0.020746    
2023-01-06 17:04:52,267 - Epoch: [116][  210/  246]    Overall Loss 0.267911    Objective Loss 0.267911                                        LR 0.000013    Time 0.020510    
2023-01-06 17:04:52,418 - Epoch: [116][  220/  246]    Overall Loss 0.269549    Objective Loss 0.269549                                        LR 0.000013    Time 0.020264    
2023-01-06 17:04:52,583 - Epoch: [116][  230/  246]    Overall Loss 0.269661    Objective Loss 0.269661                                        LR 0.000013    Time 0.020102    
2023-01-06 17:04:52,755 - Epoch: [116][  240/  246]    Overall Loss 0.269015    Objective Loss 0.269015                                        LR 0.000013    Time 0.019979    
2023-01-06 17:04:52,845 - Epoch: [116][  246/  246]    Overall Loss 0.268766    Objective Loss 0.268766    Top1 91.148325    LR 0.000013    Time 0.019857    
2023-01-06 17:04:53,008 - --- validate (epoch=116)-----------
2023-01-06 17:04:53,008 - 6986 samples (256 per mini-batch)
2023-01-06 17:04:53,461 - Epoch: [116][   10/   28]    Loss 0.275838    Top1 90.078125    
2023-01-06 17:04:53,576 - Epoch: [116][   20/   28]    Loss 0.281389    Top1 89.726562    
2023-01-06 17:04:53,643 - Epoch: [116][   28/   28]    Loss 0.275648    Top1 90.051532    
2023-01-06 17:04:53,806 - ==> Top1: 90.052    Loss: 0.276

2023-01-06 17:04:53,807 - ==> Confusion:
[[ 218   12  209]
 [  14  251  337]
 [  58   65 5822]]

2023-01-06 17:04:53,808 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:04:53,808 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:04:53,815 - 

2023-01-06 17:04:53,815 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:04:54,527 - Epoch: [117][   10/  246]    Overall Loss 0.259271    Objective Loss 0.259271                                        LR 0.000013    Time 0.071122    
2023-01-06 17:04:54,710 - Epoch: [117][   20/  246]    Overall Loss 0.261333    Objective Loss 0.261333                                        LR 0.000013    Time 0.044679    
2023-01-06 17:04:54,909 - Epoch: [117][   30/  246]    Overall Loss 0.264343    Objective Loss 0.264343                                        LR 0.000013    Time 0.036406    
2023-01-06 17:04:55,130 - Epoch: [117][   40/  246]    Overall Loss 0.267059    Objective Loss 0.267059                                        LR 0.000013    Time 0.032817    
2023-01-06 17:04:55,352 - Epoch: [117][   50/  246]    Overall Loss 0.270501    Objective Loss 0.270501                                        LR 0.000013    Time 0.030697    
2023-01-06 17:04:55,576 - Epoch: [117][   60/  246]    Overall Loss 0.272582    Objective Loss 0.272582                                        LR 0.000013    Time 0.029299    
2023-01-06 17:04:55,798 - Epoch: [117][   70/  246]    Overall Loss 0.273531    Objective Loss 0.273531                                        LR 0.000013    Time 0.028283    
2023-01-06 17:04:56,018 - Epoch: [117][   80/  246]    Overall Loss 0.274202    Objective Loss 0.274202                                        LR 0.000013    Time 0.027493    
2023-01-06 17:04:56,222 - Epoch: [117][   90/  246]    Overall Loss 0.273904    Objective Loss 0.273904                                        LR 0.000013    Time 0.026705    
2023-01-06 17:04:56,441 - Epoch: [117][  100/  246]    Overall Loss 0.274837    Objective Loss 0.274837                                        LR 0.000013    Time 0.026217    
2023-01-06 17:04:56,665 - Epoch: [117][  110/  246]    Overall Loss 0.272304    Objective Loss 0.272304                                        LR 0.000013    Time 0.025869    
2023-01-06 17:04:56,886 - Epoch: [117][  120/  246]    Overall Loss 0.273221    Objective Loss 0.273221                                        LR 0.000013    Time 0.025549    
2023-01-06 17:04:57,110 - Epoch: [117][  130/  246]    Overall Loss 0.272769    Objective Loss 0.272769                                        LR 0.000013    Time 0.025300    
2023-01-06 17:04:57,326 - Epoch: [117][  140/  246]    Overall Loss 0.272562    Objective Loss 0.272562                                        LR 0.000013    Time 0.025036    
2023-01-06 17:04:57,549 - Epoch: [117][  150/  246]    Overall Loss 0.272921    Objective Loss 0.272921                                        LR 0.000013    Time 0.024852    
2023-01-06 17:04:57,771 - Epoch: [117][  160/  246]    Overall Loss 0.271612    Objective Loss 0.271612                                        LR 0.000013    Time 0.024679    
2023-01-06 17:04:57,991 - Epoch: [117][  170/  246]    Overall Loss 0.271891    Objective Loss 0.271891                                        LR 0.000013    Time 0.024524    
2023-01-06 17:04:58,207 - Epoch: [117][  180/  246]    Overall Loss 0.272316    Objective Loss 0.272316                                        LR 0.000013    Time 0.024357    
2023-01-06 17:04:58,421 - Epoch: [117][  190/  246]    Overall Loss 0.271849    Objective Loss 0.271849                                        LR 0.000013    Time 0.024202    
2023-01-06 17:04:58,634 - Epoch: [117][  200/  246]    Overall Loss 0.272388    Objective Loss 0.272388                                        LR 0.000013    Time 0.024051    
2023-01-06 17:04:58,847 - Epoch: [117][  210/  246]    Overall Loss 0.272188    Objective Loss 0.272188                                        LR 0.000013    Time 0.023918    
2023-01-06 17:04:59,060 - Epoch: [117][  220/  246]    Overall Loss 0.272257    Objective Loss 0.272257                                        LR 0.000013    Time 0.023797    
2023-01-06 17:04:59,271 - Epoch: [117][  230/  246]    Overall Loss 0.271783    Objective Loss 0.271783                                        LR 0.000013    Time 0.023679    
2023-01-06 17:04:59,496 - Epoch: [117][  240/  246]    Overall Loss 0.271206    Objective Loss 0.271206                                        LR 0.000013    Time 0.023626    
2023-01-06 17:04:59,590 - Epoch: [117][  246/  246]    Overall Loss 0.270305    Objective Loss 0.270305    Top1 90.430622    LR 0.000013    Time 0.023432    
2023-01-06 17:04:59,736 - --- validate (epoch=117)-----------
2023-01-06 17:04:59,736 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:00,210 - Epoch: [117][   10/   28]    Loss 0.293698    Top1 88.671875    
2023-01-06 17:05:00,343 - Epoch: [117][   20/   28]    Loss 0.286843    Top1 89.257812    
2023-01-06 17:05:00,408 - Epoch: [117][   28/   28]    Loss 0.278394    Top1 89.622101    
2023-01-06 17:05:00,567 - ==> Top1: 89.622    Loss: 0.278

2023-01-06 17:05:00,568 - ==> Confusion:
[[ 202   10  227]
 [  15  201  386]
 [  45   42 5858]]

2023-01-06 17:05:00,569 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:00,569 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:00,575 - 

2023-01-06 17:05:00,575 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:01,146 - Epoch: [118][   10/  246]    Overall Loss 0.280785    Objective Loss 0.280785                                        LR 0.000013    Time 0.057035    
2023-01-06 17:05:01,342 - Epoch: [118][   20/  246]    Overall Loss 0.273728    Objective Loss 0.273728                                        LR 0.000013    Time 0.038257    
2023-01-06 17:05:01,536 - Epoch: [118][   30/  246]    Overall Loss 0.272843    Objective Loss 0.272843                                        LR 0.000013    Time 0.031957    
2023-01-06 17:05:01,755 - Epoch: [118][   40/  246]    Overall Loss 0.272057    Objective Loss 0.272057                                        LR 0.000013    Time 0.029443    
2023-01-06 17:05:01,980 - Epoch: [118][   50/  246]    Overall Loss 0.269871    Objective Loss 0.269871                                        LR 0.000013    Time 0.028039    
2023-01-06 17:05:02,220 - Epoch: [118][   60/  246]    Overall Loss 0.270145    Objective Loss 0.270145                                        LR 0.000013    Time 0.027368    
2023-01-06 17:05:02,447 - Epoch: [118][   70/  246]    Overall Loss 0.267871    Objective Loss 0.267871                                        LR 0.000013    Time 0.026691    
2023-01-06 17:05:02,680 - Epoch: [118][   80/  246]    Overall Loss 0.269396    Objective Loss 0.269396                                        LR 0.000013    Time 0.026262    
2023-01-06 17:05:02,913 - Epoch: [118][   90/  246]    Overall Loss 0.269204    Objective Loss 0.269204                                        LR 0.000013    Time 0.025922    
2023-01-06 17:05:03,147 - Epoch: [118][  100/  246]    Overall Loss 0.269535    Objective Loss 0.269535                                        LR 0.000013    Time 0.025667    
2023-01-06 17:05:03,384 - Epoch: [118][  110/  246]    Overall Loss 0.269327    Objective Loss 0.269327                                        LR 0.000013    Time 0.025465    
2023-01-06 17:05:03,622 - Epoch: [118][  120/  246]    Overall Loss 0.269799    Objective Loss 0.269799                                        LR 0.000013    Time 0.025322    
2023-01-06 17:05:03,859 - Epoch: [118][  130/  246]    Overall Loss 0.268437    Objective Loss 0.268437                                        LR 0.000013    Time 0.025193    
2023-01-06 17:05:04,056 - Epoch: [118][  140/  246]    Overall Loss 0.268522    Objective Loss 0.268522                                        LR 0.000013    Time 0.024799    
2023-01-06 17:05:04,252 - Epoch: [118][  150/  246]    Overall Loss 0.268088    Objective Loss 0.268088                                        LR 0.000013    Time 0.024449    
2023-01-06 17:05:04,455 - Epoch: [118][  160/  246]    Overall Loss 0.269244    Objective Loss 0.269244                                        LR 0.000013    Time 0.024188    
2023-01-06 17:05:04,684 - Epoch: [118][  170/  246]    Overall Loss 0.269321    Objective Loss 0.269321                                        LR 0.000013    Time 0.024108    
2023-01-06 17:05:04,923 - Epoch: [118][  180/  246]    Overall Loss 0.269248    Objective Loss 0.269248                                        LR 0.000013    Time 0.024096    
2023-01-06 17:05:05,166 - Epoch: [118][  190/  246]    Overall Loss 0.269147    Objective Loss 0.269147                                        LR 0.000013    Time 0.024103    
2023-01-06 17:05:05,405 - Epoch: [118][  200/  246]    Overall Loss 0.268885    Objective Loss 0.268885                                        LR 0.000013    Time 0.024092    
2023-01-06 17:05:05,648 - Epoch: [118][  210/  246]    Overall Loss 0.268963    Objective Loss 0.268963                                        LR 0.000013    Time 0.024096    
2023-01-06 17:05:05,889 - Epoch: [118][  220/  246]    Overall Loss 0.269183    Objective Loss 0.269183                                        LR 0.000013    Time 0.024095    
2023-01-06 17:05:06,132 - Epoch: [118][  230/  246]    Overall Loss 0.268440    Objective Loss 0.268440                                        LR 0.000013    Time 0.024101    
2023-01-06 17:05:06,380 - Epoch: [118][  240/  246]    Overall Loss 0.268448    Objective Loss 0.268448                                        LR 0.000013    Time 0.024130    
2023-01-06 17:05:06,494 - Epoch: [118][  246/  246]    Overall Loss 0.268353    Objective Loss 0.268353    Top1 90.191388    LR 0.000013    Time 0.024003    
2023-01-06 17:05:06,658 - --- validate (epoch=118)-----------
2023-01-06 17:05:06,659 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:07,108 - Epoch: [118][   10/   28]    Loss 0.281090    Top1 89.492188    
2023-01-06 17:05:07,222 - Epoch: [118][   20/   28]    Loss 0.284532    Top1 89.648438    
2023-01-06 17:05:07,291 - Epoch: [118][   28/   28]    Loss 0.278616    Top1 89.894074    
2023-01-06 17:05:07,438 - ==> Top1: 89.894    Loss: 0.279

2023-01-06 17:05:07,438 - ==> Confusion:
[[ 233   17  189]
 [  14  281  307]
 [  76  103 5766]]

2023-01-06 17:05:07,440 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:07,440 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:07,446 - 

2023-01-06 17:05:07,446 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:08,163 - Epoch: [119][   10/  246]    Overall Loss 0.258095    Objective Loss 0.258095                                        LR 0.000013    Time 0.071612    
2023-01-06 17:05:08,380 - Epoch: [119][   20/  246]    Overall Loss 0.256083    Objective Loss 0.256083                                        LR 0.000013    Time 0.046640    
2023-01-06 17:05:08,578 - Epoch: [119][   30/  246]    Overall Loss 0.261759    Objective Loss 0.261759                                        LR 0.000013    Time 0.037703    
2023-01-06 17:05:08,780 - Epoch: [119][   40/  246]    Overall Loss 0.263365    Objective Loss 0.263365                                        LR 0.000013    Time 0.033303    
2023-01-06 17:05:08,982 - Epoch: [119][   50/  246]    Overall Loss 0.266779    Objective Loss 0.266779                                        LR 0.000013    Time 0.030678    
2023-01-06 17:05:09,182 - Epoch: [119][   60/  246]    Overall Loss 0.266213    Objective Loss 0.266213                                        LR 0.000013    Time 0.028897    
2023-01-06 17:05:09,379 - Epoch: [119][   70/  246]    Overall Loss 0.263569    Objective Loss 0.263569                                        LR 0.000013    Time 0.027569    
2023-01-06 17:05:09,595 - Epoch: [119][   80/  246]    Overall Loss 0.266305    Objective Loss 0.266305                                        LR 0.000013    Time 0.026825    
2023-01-06 17:05:09,802 - Epoch: [119][   90/  246]    Overall Loss 0.266065    Objective Loss 0.266065                                        LR 0.000013    Time 0.026134    
2023-01-06 17:05:10,008 - Epoch: [119][  100/  246]    Overall Loss 0.265964    Objective Loss 0.265964                                        LR 0.000013    Time 0.025585    
2023-01-06 17:05:10,205 - Epoch: [119][  110/  246]    Overall Loss 0.265842    Objective Loss 0.265842                                        LR 0.000013    Time 0.025044    
2023-01-06 17:05:10,408 - Epoch: [119][  120/  246]    Overall Loss 0.264586    Objective Loss 0.264586                                        LR 0.000013    Time 0.024648    
2023-01-06 17:05:10,604 - Epoch: [119][  130/  246]    Overall Loss 0.265594    Objective Loss 0.265594                                        LR 0.000013    Time 0.024256    
2023-01-06 17:05:10,807 - Epoch: [119][  140/  246]    Overall Loss 0.265920    Objective Loss 0.265920                                        LR 0.000013    Time 0.023970    
2023-01-06 17:05:11,003 - Epoch: [119][  150/  246]    Overall Loss 0.266448    Objective Loss 0.266448                                        LR 0.000013    Time 0.023675    
2023-01-06 17:05:11,195 - Epoch: [119][  160/  246]    Overall Loss 0.266259    Objective Loss 0.266259                                        LR 0.000013    Time 0.023393    
2023-01-06 17:05:11,387 - Epoch: [119][  170/  246]    Overall Loss 0.266182    Objective Loss 0.266182                                        LR 0.000013    Time 0.023146    
2023-01-06 17:05:11,590 - Epoch: [119][  180/  246]    Overall Loss 0.265505    Objective Loss 0.265505                                        LR 0.000013    Time 0.022985    
2023-01-06 17:05:11,788 - Epoch: [119][  190/  246]    Overall Loss 0.266774    Objective Loss 0.266774                                        LR 0.000013    Time 0.022816    
2023-01-06 17:05:11,997 - Epoch: [119][  200/  246]    Overall Loss 0.266653    Objective Loss 0.266653                                        LR 0.000013    Time 0.022719    
2023-01-06 17:05:12,183 - Epoch: [119][  210/  246]    Overall Loss 0.267673    Objective Loss 0.267673                                        LR 0.000013    Time 0.022519    
2023-01-06 17:05:12,365 - Epoch: [119][  220/  246]    Overall Loss 0.268328    Objective Loss 0.268328                                        LR 0.000013    Time 0.022322    
2023-01-06 17:05:12,546 - Epoch: [119][  230/  246]    Overall Loss 0.267918    Objective Loss 0.267918                                        LR 0.000013    Time 0.022137    
2023-01-06 17:05:12,741 - Epoch: [119][  240/  246]    Overall Loss 0.268133    Objective Loss 0.268133                                        LR 0.000013    Time 0.022025    
2023-01-06 17:05:12,836 - Epoch: [119][  246/  246]    Overall Loss 0.268270    Objective Loss 0.268270    Top1 89.952153    LR 0.000013    Time 0.021875    
2023-01-06 17:05:13,020 - --- validate (epoch=119)-----------
2023-01-06 17:05:13,020 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:13,465 - Epoch: [119][   10/   28]    Loss 0.267884    Top1 90.351562    
2023-01-06 17:05:13,576 - Epoch: [119][   20/   28]    Loss 0.270634    Top1 90.039062    
2023-01-06 17:05:13,644 - Epoch: [119][   28/   28]    Loss 0.279372    Top1 89.908388    
2023-01-06 17:05:13,785 - ==> Top1: 89.908    Loss: 0.279

2023-01-06 17:05:13,785 - ==> Confusion:
[[ 234   12  193]
 [  18  248  336]
 [  64   82 5799]]

2023-01-06 17:05:13,787 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:13,787 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:13,793 - 

2023-01-06 17:05:13,793 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:14,497 - Epoch: [120][   10/  246]    Overall Loss 0.264528    Objective Loss 0.264528                                        LR 0.000013    Time 0.070353    
2023-01-06 17:05:14,680 - Epoch: [120][   20/  246]    Overall Loss 0.271181    Objective Loss 0.271181                                        LR 0.000013    Time 0.044307    
2023-01-06 17:05:14,869 - Epoch: [120][   30/  246]    Overall Loss 0.273758    Objective Loss 0.273758                                        LR 0.000013    Time 0.035829    
2023-01-06 17:05:15,057 - Epoch: [120][   40/  246]    Overall Loss 0.265620    Objective Loss 0.265620                                        LR 0.000013    Time 0.031564    
2023-01-06 17:05:15,245 - Epoch: [120][   50/  246]    Overall Loss 0.268558    Objective Loss 0.268558                                        LR 0.000013    Time 0.028999    
2023-01-06 17:05:15,436 - Epoch: [120][   60/  246]    Overall Loss 0.269241    Objective Loss 0.269241                                        LR 0.000013    Time 0.027344    
2023-01-06 17:05:15,631 - Epoch: [120][   70/  246]    Overall Loss 0.265914    Objective Loss 0.265914                                        LR 0.000013    Time 0.026217    
2023-01-06 17:05:15,829 - Epoch: [120][   80/  246]    Overall Loss 0.266400    Objective Loss 0.266400                                        LR 0.000013    Time 0.025415    
2023-01-06 17:05:16,025 - Epoch: [120][   90/  246]    Overall Loss 0.267416    Objective Loss 0.267416                                        LR 0.000013    Time 0.024758    
2023-01-06 17:05:16,219 - Epoch: [120][  100/  246]    Overall Loss 0.266942    Objective Loss 0.266942                                        LR 0.000013    Time 0.024221    
2023-01-06 17:05:16,415 - Epoch: [120][  110/  246]    Overall Loss 0.269364    Objective Loss 0.269364                                        LR 0.000013    Time 0.023796    
2023-01-06 17:05:16,605 - Epoch: [120][  120/  246]    Overall Loss 0.269456    Objective Loss 0.269456                                        LR 0.000013    Time 0.023395    
2023-01-06 17:05:16,803 - Epoch: [120][  130/  246]    Overall Loss 0.268278    Objective Loss 0.268278                                        LR 0.000013    Time 0.023117    
2023-01-06 17:05:16,999 - Epoch: [120][  140/  246]    Overall Loss 0.268123    Objective Loss 0.268123                                        LR 0.000013    Time 0.022864    
2023-01-06 17:05:17,193 - Epoch: [120][  150/  246]    Overall Loss 0.268839    Objective Loss 0.268839                                        LR 0.000013    Time 0.022632    
2023-01-06 17:05:17,391 - Epoch: [120][  160/  246]    Overall Loss 0.267965    Objective Loss 0.267965                                        LR 0.000013    Time 0.022449    
2023-01-06 17:05:17,582 - Epoch: [120][  170/  246]    Overall Loss 0.268793    Objective Loss 0.268793                                        LR 0.000013    Time 0.022254    
2023-01-06 17:05:17,780 - Epoch: [120][  180/  246]    Overall Loss 0.269054    Objective Loss 0.269054                                        LR 0.000013    Time 0.022115    
2023-01-06 17:05:17,971 - Epoch: [120][  190/  246]    Overall Loss 0.268545    Objective Loss 0.268545                                        LR 0.000013    Time 0.021952    
2023-01-06 17:05:18,157 - Epoch: [120][  200/  246]    Overall Loss 0.268913    Objective Loss 0.268913                                        LR 0.000013    Time 0.021782    
2023-01-06 17:05:18,343 - Epoch: [120][  210/  246]    Overall Loss 0.269292    Objective Loss 0.269292                                        LR 0.000013    Time 0.021629    
2023-01-06 17:05:18,533 - Epoch: [120][  220/  246]    Overall Loss 0.269982    Objective Loss 0.269982                                        LR 0.000013    Time 0.021511    
2023-01-06 17:05:18,726 - Epoch: [120][  230/  246]    Overall Loss 0.269586    Objective Loss 0.269586                                        LR 0.000013    Time 0.021413    
2023-01-06 17:05:18,933 - Epoch: [120][  240/  246]    Overall Loss 0.268954    Objective Loss 0.268954                                        LR 0.000013    Time 0.021379    
2023-01-06 17:05:19,029 - Epoch: [120][  246/  246]    Overall Loss 0.268774    Objective Loss 0.268774    Top1 92.344498    LR 0.000013    Time 0.021248    
2023-01-06 17:05:19,203 - --- validate (epoch=120)-----------
2023-01-06 17:05:19,203 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:19,650 - Epoch: [120][   10/   28]    Loss 0.268841    Top1 90.117188    
2023-01-06 17:05:19,761 - Epoch: [120][   20/   28]    Loss 0.276926    Top1 89.785156    
2023-01-06 17:05:19,829 - Epoch: [120][   28/   28]    Loss 0.275303    Top1 89.879760    
2023-01-06 17:05:19,966 - ==> Top1: 89.880    Loss: 0.275

2023-01-06 17:05:19,967 - ==> Confusion:
[[ 201   17  221]
 [  13  263  326]
 [  48   82 5815]]

2023-01-06 17:05:19,968 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:19,968 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:19,974 - 

2023-01-06 17:05:19,974 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:20,556 - Epoch: [121][   10/  246]    Overall Loss 0.258903    Objective Loss 0.258903                                        LR 0.000013    Time 0.058111    
2023-01-06 17:05:20,748 - Epoch: [121][   20/  246]    Overall Loss 0.268953    Objective Loss 0.268953                                        LR 0.000013    Time 0.038642    
2023-01-06 17:05:20,936 - Epoch: [121][   30/  246]    Overall Loss 0.274208    Objective Loss 0.274208                                        LR 0.000013    Time 0.032010    
2023-01-06 17:05:21,118 - Epoch: [121][   40/  246]    Overall Loss 0.266460    Objective Loss 0.266460                                        LR 0.000013    Time 0.028567    
2023-01-06 17:05:21,307 - Epoch: [121][   50/  246]    Overall Loss 0.267592    Objective Loss 0.267592                                        LR 0.000013    Time 0.026626    
2023-01-06 17:05:21,499 - Epoch: [121][   60/  246]    Overall Loss 0.268618    Objective Loss 0.268618                                        LR 0.000013    Time 0.025370    
2023-01-06 17:05:21,694 - Epoch: [121][   70/  246]    Overall Loss 0.267483    Objective Loss 0.267483                                        LR 0.000013    Time 0.024526    
2023-01-06 17:05:21,897 - Epoch: [121][   80/  246]    Overall Loss 0.266780    Objective Loss 0.266780                                        LR 0.000013    Time 0.023998    
2023-01-06 17:05:22,093 - Epoch: [121][   90/  246]    Overall Loss 0.267046    Objective Loss 0.267046                                        LR 0.000013    Time 0.023502    
2023-01-06 17:05:22,298 - Epoch: [121][  100/  246]    Overall Loss 0.267609    Objective Loss 0.267609                                        LR 0.000013    Time 0.023200    
2023-01-06 17:05:22,497 - Epoch: [121][  110/  246]    Overall Loss 0.268705    Objective Loss 0.268705                                        LR 0.000013    Time 0.022896    
2023-01-06 17:05:22,702 - Epoch: [121][  120/  246]    Overall Loss 0.268867    Objective Loss 0.268867                                        LR 0.000013    Time 0.022692    
2023-01-06 17:05:22,901 - Epoch: [121][  130/  246]    Overall Loss 0.269756    Objective Loss 0.269756                                        LR 0.000013    Time 0.022475    
2023-01-06 17:05:23,104 - Epoch: [121][  140/  246]    Overall Loss 0.269170    Objective Loss 0.269170                                        LR 0.000013    Time 0.022316    
2023-01-06 17:05:23,303 - Epoch: [121][  150/  246]    Overall Loss 0.269983    Objective Loss 0.269983                                        LR 0.000013    Time 0.022154    
2023-01-06 17:05:23,507 - Epoch: [121][  160/  246]    Overall Loss 0.269751    Objective Loss 0.269751                                        LR 0.000013    Time 0.022045    
2023-01-06 17:05:23,702 - Epoch: [121][  170/  246]    Overall Loss 0.268810    Objective Loss 0.268810                                        LR 0.000013    Time 0.021893    
2023-01-06 17:05:23,896 - Epoch: [121][  180/  246]    Overall Loss 0.268679    Objective Loss 0.268679                                        LR 0.000013    Time 0.021753    
2023-01-06 17:05:24,086 - Epoch: [121][  190/  246]    Overall Loss 0.267950    Objective Loss 0.267950                                        LR 0.000013    Time 0.021602    
2023-01-06 17:05:24,277 - Epoch: [121][  200/  246]    Overall Loss 0.268736    Objective Loss 0.268736                                        LR 0.000013    Time 0.021475    
2023-01-06 17:05:24,469 - Epoch: [121][  210/  246]    Overall Loss 0.267990    Objective Loss 0.267990                                        LR 0.000013    Time 0.021366    
2023-01-06 17:05:24,660 - Epoch: [121][  220/  246]    Overall Loss 0.268566    Objective Loss 0.268566                                        LR 0.000013    Time 0.021265    
2023-01-06 17:05:24,852 - Epoch: [121][  230/  246]    Overall Loss 0.268390    Objective Loss 0.268390                                        LR 0.000013    Time 0.021172    
2023-01-06 17:05:25,055 - Epoch: [121][  240/  246]    Overall Loss 0.267849    Objective Loss 0.267849                                        LR 0.000013    Time 0.021132    
2023-01-06 17:05:25,150 - Epoch: [121][  246/  246]    Overall Loss 0.267902    Objective Loss 0.267902    Top1 89.712919    LR 0.000013    Time 0.021003    
2023-01-06 17:05:25,290 - --- validate (epoch=121)-----------
2023-01-06 17:05:25,291 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:25,747 - Epoch: [121][   10/   28]    Loss 0.250922    Top1 90.820312    
2023-01-06 17:05:25,860 - Epoch: [121][   20/   28]    Loss 0.275700    Top1 90.000000    
2023-01-06 17:05:25,927 - Epoch: [121][   28/   28]    Loss 0.276467    Top1 89.865445    
2023-01-06 17:05:26,095 - ==> Top1: 89.865    Loss: 0.276

2023-01-06 17:05:26,096 - ==> Confusion:
[[ 228   14  197]
 [  15  243  344]
 [  71   67 5807]]

2023-01-06 17:05:26,097 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:26,097 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:26,103 - 

2023-01-06 17:05:26,103 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:26,806 - Epoch: [122][   10/  246]    Overall Loss 0.288828    Objective Loss 0.288828                                        LR 0.000013    Time 0.070247    
2023-01-06 17:05:26,992 - Epoch: [122][   20/  246]    Overall Loss 0.283101    Objective Loss 0.283101                                        LR 0.000013    Time 0.044389    
2023-01-06 17:05:27,191 - Epoch: [122][   30/  246]    Overall Loss 0.280028    Objective Loss 0.280028                                        LR 0.000013    Time 0.036205    
2023-01-06 17:05:27,399 - Epoch: [122][   40/  246]    Overall Loss 0.274380    Objective Loss 0.274380                                        LR 0.000013    Time 0.032343    
2023-01-06 17:05:27,595 - Epoch: [122][   50/  246]    Overall Loss 0.267585    Objective Loss 0.267585                                        LR 0.000013    Time 0.029784    
2023-01-06 17:05:27,800 - Epoch: [122][   60/  246]    Overall Loss 0.262682    Objective Loss 0.262682                                        LR 0.000013    Time 0.028230    
2023-01-06 17:05:28,003 - Epoch: [122][   70/  246]    Overall Loss 0.260325    Objective Loss 0.260325                                        LR 0.000013    Time 0.027087    
2023-01-06 17:05:28,204 - Epoch: [122][   80/  246]    Overall Loss 0.261295    Objective Loss 0.261295                                        LR 0.000013    Time 0.026210    
2023-01-06 17:05:28,412 - Epoch: [122][   90/  246]    Overall Loss 0.261504    Objective Loss 0.261504                                        LR 0.000013    Time 0.025603    
2023-01-06 17:05:28,609 - Epoch: [122][  100/  246]    Overall Loss 0.264005    Objective Loss 0.264005                                        LR 0.000013    Time 0.025016    
2023-01-06 17:05:28,806 - Epoch: [122][  110/  246]    Overall Loss 0.265365    Objective Loss 0.265365                                        LR 0.000013    Time 0.024525    
2023-01-06 17:05:28,996 - Epoch: [122][  120/  246]    Overall Loss 0.264629    Objective Loss 0.264629                                        LR 0.000013    Time 0.024061    
2023-01-06 17:05:29,192 - Epoch: [122][  130/  246]    Overall Loss 0.264436    Objective Loss 0.264436                                        LR 0.000013    Time 0.023720    
2023-01-06 17:05:29,392 - Epoch: [122][  140/  246]    Overall Loss 0.263142    Objective Loss 0.263142                                        LR 0.000013    Time 0.023449    
2023-01-06 17:05:29,603 - Epoch: [122][  150/  246]    Overall Loss 0.263199    Objective Loss 0.263199                                        LR 0.000013    Time 0.023287    
2023-01-06 17:05:29,831 - Epoch: [122][  160/  246]    Overall Loss 0.262962    Objective Loss 0.262962                                        LR 0.000013    Time 0.023258    
2023-01-06 17:05:30,062 - Epoch: [122][  170/  246]    Overall Loss 0.263351    Objective Loss 0.263351                                        LR 0.000013    Time 0.023246    
2023-01-06 17:05:30,298 - Epoch: [122][  180/  246]    Overall Loss 0.263766    Objective Loss 0.263766                                        LR 0.000013    Time 0.023262    
2023-01-06 17:05:30,536 - Epoch: [122][  190/  246]    Overall Loss 0.266135    Objective Loss 0.266135                                        LR 0.000013    Time 0.023290    
2023-01-06 17:05:30,776 - Epoch: [122][  200/  246]    Overall Loss 0.266288    Objective Loss 0.266288                                        LR 0.000013    Time 0.023319    
2023-01-06 17:05:31,010 - Epoch: [122][  210/  246]    Overall Loss 0.266364    Objective Loss 0.266364                                        LR 0.000013    Time 0.023313    
2023-01-06 17:05:31,211 - Epoch: [122][  220/  246]    Overall Loss 0.266589    Objective Loss 0.266589                                        LR 0.000013    Time 0.023163    
2023-01-06 17:05:31,413 - Epoch: [122][  230/  246]    Overall Loss 0.267027    Objective Loss 0.267027                                        LR 0.000013    Time 0.023035    
2023-01-06 17:05:31,619 - Epoch: [122][  240/  246]    Overall Loss 0.267323    Objective Loss 0.267323                                        LR 0.000013    Time 0.022930    
2023-01-06 17:05:31,701 - Epoch: [122][  246/  246]    Overall Loss 0.267933    Objective Loss 0.267933    Top1 88.995215    LR 0.000013    Time 0.022705    
2023-01-06 17:05:31,850 - --- validate (epoch=122)-----------
2023-01-06 17:05:31,851 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:32,297 - Epoch: [122][   10/   28]    Loss 0.265050    Top1 90.585938    
2023-01-06 17:05:32,408 - Epoch: [122][   20/   28]    Loss 0.272521    Top1 90.253906    
2023-01-06 17:05:32,475 - Epoch: [122][   28/   28]    Loss 0.275082    Top1 90.022903    
2023-01-06 17:05:32,617 - ==> Top1: 90.023    Loss: 0.275

2023-01-06 17:05:32,618 - ==> Confusion:
[[ 240   11  188]
 [  20  243  339]
 [  71   68 5806]]

2023-01-06 17:05:32,619 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:32,619 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:32,625 - 

2023-01-06 17:05:32,625 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:33,190 - Epoch: [123][   10/  246]    Overall Loss 0.273710    Objective Loss 0.273710                                        LR 0.000013    Time 0.056430    
2023-01-06 17:05:33,376 - Epoch: [123][   20/  246]    Overall Loss 0.268931    Objective Loss 0.268931                                        LR 0.000013    Time 0.037487    
2023-01-06 17:05:33,574 - Epoch: [123][   30/  246]    Overall Loss 0.266100    Objective Loss 0.266100                                        LR 0.000013    Time 0.031583    
2023-01-06 17:05:33,788 - Epoch: [123][   40/  246]    Overall Loss 0.268291    Objective Loss 0.268291                                        LR 0.000013    Time 0.029015    
2023-01-06 17:05:34,005 - Epoch: [123][   50/  246]    Overall Loss 0.266980    Objective Loss 0.266980                                        LR 0.000013    Time 0.027552    
2023-01-06 17:05:34,206 - Epoch: [123][   60/  246]    Overall Loss 0.264228    Objective Loss 0.264228                                        LR 0.000013    Time 0.026295    
2023-01-06 17:05:34,404 - Epoch: [123][   70/  246]    Overall Loss 0.264560    Objective Loss 0.264560                                        LR 0.000013    Time 0.025369    
2023-01-06 17:05:34,602 - Epoch: [123][   80/  246]    Overall Loss 0.265953    Objective Loss 0.265953                                        LR 0.000013    Time 0.024671    
2023-01-06 17:05:34,814 - Epoch: [123][   90/  246]    Overall Loss 0.266847    Objective Loss 0.266847                                        LR 0.000013    Time 0.024272    
2023-01-06 17:05:35,030 - Epoch: [123][  100/  246]    Overall Loss 0.265292    Objective Loss 0.265292                                        LR 0.000013    Time 0.023998    
2023-01-06 17:05:35,275 - Epoch: [123][  110/  246]    Overall Loss 0.264032    Objective Loss 0.264032                                        LR 0.000013    Time 0.024039    
2023-01-06 17:05:35,511 - Epoch: [123][  120/  246]    Overall Loss 0.265119    Objective Loss 0.265119                                        LR 0.000013    Time 0.024000    
2023-01-06 17:05:35,755 - Epoch: [123][  130/  246]    Overall Loss 0.263171    Objective Loss 0.263171                                        LR 0.000013    Time 0.024029    
2023-01-06 17:05:35,991 - Epoch: [123][  140/  246]    Overall Loss 0.263177    Objective Loss 0.263177                                        LR 0.000013    Time 0.023997    
2023-01-06 17:05:36,240 - Epoch: [123][  150/  246]    Overall Loss 0.263164    Objective Loss 0.263164                                        LR 0.000013    Time 0.024055    
2023-01-06 17:05:36,467 - Epoch: [123][  160/  246]    Overall Loss 0.263425    Objective Loss 0.263425                                        LR 0.000013    Time 0.023967    
2023-01-06 17:05:36,702 - Epoch: [123][  170/  246]    Overall Loss 0.264307    Objective Loss 0.264307                                        LR 0.000013    Time 0.023934    
2023-01-06 17:05:36,930 - Epoch: [123][  180/  246]    Overall Loss 0.263293    Objective Loss 0.263293                                        LR 0.000013    Time 0.023871    
2023-01-06 17:05:37,152 - Epoch: [123][  190/  246]    Overall Loss 0.263648    Objective Loss 0.263648                                        LR 0.000013    Time 0.023778    
2023-01-06 17:05:37,376 - Epoch: [123][  200/  246]    Overall Loss 0.263939    Objective Loss 0.263939                                        LR 0.000013    Time 0.023709    
2023-01-06 17:05:37,600 - Epoch: [123][  210/  246]    Overall Loss 0.264492    Objective Loss 0.264492                                        LR 0.000013    Time 0.023644    
2023-01-06 17:05:37,822 - Epoch: [123][  220/  246]    Overall Loss 0.264610    Objective Loss 0.264610                                        LR 0.000013    Time 0.023575    
2023-01-06 17:05:38,039 - Epoch: [123][  230/  246]    Overall Loss 0.265119    Objective Loss 0.265119                                        LR 0.000013    Time 0.023491    
2023-01-06 17:05:38,277 - Epoch: [123][  240/  246]    Overall Loss 0.265656    Objective Loss 0.265656                                        LR 0.000013    Time 0.023501    
2023-01-06 17:05:38,389 - Epoch: [123][  246/  246]    Overall Loss 0.265903    Objective Loss 0.265903    Top1 89.473684    LR 0.000013    Time 0.023385    
2023-01-06 17:05:38,525 - --- validate (epoch=123)-----------
2023-01-06 17:05:38,525 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:38,987 - Epoch: [123][   10/   28]    Loss 0.279353    Top1 89.882812    
2023-01-06 17:05:39,112 - Epoch: [123][   20/   28]    Loss 0.280246    Top1 89.707031    
2023-01-06 17:05:39,179 - Epoch: [123][   28/   28]    Loss 0.275314    Top1 89.865445    
2023-01-06 17:05:39,339 - ==> Top1: 89.865    Loss: 0.275

2023-01-06 17:05:39,340 - ==> Confusion:
[[ 216   12  211]
 [  16  227  359]
 [  53   57 5835]]

2023-01-06 17:05:39,341 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:39,341 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:39,347 - 

2023-01-06 17:05:39,347 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:40,047 - Epoch: [124][   10/  246]    Overall Loss 0.264047    Objective Loss 0.264047                                        LR 0.000013    Time 0.069856    
2023-01-06 17:05:40,228 - Epoch: [124][   20/  246]    Overall Loss 0.265403    Objective Loss 0.265403                                        LR 0.000013    Time 0.043992    
2023-01-06 17:05:40,433 - Epoch: [124][   30/  246]    Overall Loss 0.265241    Objective Loss 0.265241                                        LR 0.000013    Time 0.036144    
2023-01-06 17:05:40,640 - Epoch: [124][   40/  246]    Overall Loss 0.266139    Objective Loss 0.266139                                        LR 0.000013    Time 0.032265    
2023-01-06 17:05:40,847 - Epoch: [124][   50/  246]    Overall Loss 0.266300    Objective Loss 0.266300                                        LR 0.000013    Time 0.029943    
2023-01-06 17:05:41,051 - Epoch: [124][   60/  246]    Overall Loss 0.269326    Objective Loss 0.269326                                        LR 0.000013    Time 0.028360    
2023-01-06 17:05:41,259 - Epoch: [124][   70/  246]    Overall Loss 0.267524    Objective Loss 0.267524                                        LR 0.000013    Time 0.027271    
2023-01-06 17:05:41,466 - Epoch: [124][   80/  246]    Overall Loss 0.265282    Objective Loss 0.265282                                        LR 0.000013    Time 0.026440    
2023-01-06 17:05:41,675 - Epoch: [124][   90/  246]    Overall Loss 0.263949    Objective Loss 0.263949                                        LR 0.000013    Time 0.025820    
2023-01-06 17:05:41,882 - Epoch: [124][  100/  246]    Overall Loss 0.264025    Objective Loss 0.264025                                        LR 0.000013    Time 0.025308    
2023-01-06 17:05:42,086 - Epoch: [124][  110/  246]    Overall Loss 0.266007    Objective Loss 0.266007                                        LR 0.000013    Time 0.024854    
2023-01-06 17:05:42,284 - Epoch: [124][  120/  246]    Overall Loss 0.265082    Objective Loss 0.265082                                        LR 0.000013    Time 0.024432    
2023-01-06 17:05:42,484 - Epoch: [124][  130/  246]    Overall Loss 0.265173    Objective Loss 0.265173                                        LR 0.000013    Time 0.024091    
2023-01-06 17:05:42,684 - Epoch: [124][  140/  246]    Overall Loss 0.265263    Objective Loss 0.265263                                        LR 0.000013    Time 0.023791    
2023-01-06 17:05:42,874 - Epoch: [124][  150/  246]    Overall Loss 0.265900    Objective Loss 0.265900                                        LR 0.000013    Time 0.023473    
2023-01-06 17:05:43,067 - Epoch: [124][  160/  246]    Overall Loss 0.265176    Objective Loss 0.265176                                        LR 0.000013    Time 0.023208    
2023-01-06 17:05:43,256 - Epoch: [124][  170/  246]    Overall Loss 0.266085    Objective Loss 0.266085                                        LR 0.000013    Time 0.022956    
2023-01-06 17:05:43,450 - Epoch: [124][  180/  246]    Overall Loss 0.266602    Objective Loss 0.266602                                        LR 0.000013    Time 0.022756    
2023-01-06 17:05:43,642 - Epoch: [124][  190/  246]    Overall Loss 0.267254    Objective Loss 0.267254                                        LR 0.000013    Time 0.022565    
2023-01-06 17:05:43,834 - Epoch: [124][  200/  246]    Overall Loss 0.267717    Objective Loss 0.267717                                        LR 0.000013    Time 0.022394    
2023-01-06 17:05:44,023 - Epoch: [124][  210/  246]    Overall Loss 0.267878    Objective Loss 0.267878                                        LR 0.000013    Time 0.022229    
2023-01-06 17:05:44,214 - Epoch: [124][  220/  246]    Overall Loss 0.267629    Objective Loss 0.267629                                        LR 0.000013    Time 0.022082    
2023-01-06 17:05:44,405 - Epoch: [124][  230/  246]    Overall Loss 0.267671    Objective Loss 0.267671                                        LR 0.000013    Time 0.021951    
2023-01-06 17:05:44,606 - Epoch: [124][  240/  246]    Overall Loss 0.267172    Objective Loss 0.267172                                        LR 0.000013    Time 0.021874    
2023-01-06 17:05:44,700 - Epoch: [124][  246/  246]    Overall Loss 0.266687    Objective Loss 0.266687    Top1 90.430622    LR 0.000013    Time 0.021723    
2023-01-06 17:05:44,844 - --- validate (epoch=124)-----------
2023-01-06 17:05:44,845 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:45,288 - Epoch: [124][   10/   28]    Loss 0.277304    Top1 89.687500    
2023-01-06 17:05:45,403 - Epoch: [124][   20/   28]    Loss 0.275819    Top1 89.843750    
2023-01-06 17:05:45,469 - Epoch: [124][   28/   28]    Loss 0.275657    Top1 89.908388    
2023-01-06 17:05:45,617 - ==> Top1: 89.908    Loss: 0.276

2023-01-06 17:05:45,618 - ==> Confusion:
[[ 221   11  207]
 [  17  230  355]
 [  53   62 5830]]

2023-01-06 17:05:45,619 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:45,619 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:45,625 - 

2023-01-06 17:05:45,625 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:46,181 - Epoch: [125][   10/  246]    Overall Loss 0.267528    Objective Loss 0.267528                                        LR 0.000013    Time 0.055560    
2023-01-06 17:05:46,363 - Epoch: [125][   20/  246]    Overall Loss 0.278789    Objective Loss 0.278789                                        LR 0.000013    Time 0.036871    
2023-01-06 17:05:46,561 - Epoch: [125][   30/  246]    Overall Loss 0.274126    Objective Loss 0.274126                                        LR 0.000013    Time 0.031144    
2023-01-06 17:05:46,782 - Epoch: [125][   40/  246]    Overall Loss 0.276852    Objective Loss 0.276852                                        LR 0.000013    Time 0.028876    
2023-01-06 17:05:47,005 - Epoch: [125][   50/  246]    Overall Loss 0.276130    Objective Loss 0.276130                                        LR 0.000013    Time 0.027549    
2023-01-06 17:05:47,230 - Epoch: [125][   60/  246]    Overall Loss 0.276186    Objective Loss 0.276186                                        LR 0.000013    Time 0.026703    
2023-01-06 17:05:47,453 - Epoch: [125][   70/  246]    Overall Loss 0.273652    Objective Loss 0.273652                                        LR 0.000013    Time 0.026066    
2023-01-06 17:05:47,681 - Epoch: [125][   80/  246]    Overall Loss 0.276107    Objective Loss 0.276107                                        LR 0.000013    Time 0.025650    
2023-01-06 17:05:47,906 - Epoch: [125][   90/  246]    Overall Loss 0.273940    Objective Loss 0.273940                                        LR 0.000013    Time 0.025300    
2023-01-06 17:05:48,132 - Epoch: [125][  100/  246]    Overall Loss 0.274175    Objective Loss 0.274175                                        LR 0.000013    Time 0.025031    
2023-01-06 17:05:48,368 - Epoch: [125][  110/  246]    Overall Loss 0.275267    Objective Loss 0.275267                                        LR 0.000013    Time 0.024894    
2023-01-06 17:05:48,619 - Epoch: [125][  120/  246]    Overall Loss 0.274398    Objective Loss 0.274398                                        LR 0.000013    Time 0.024888    
2023-01-06 17:05:48,853 - Epoch: [125][  130/  246]    Overall Loss 0.271551    Objective Loss 0.271551                                        LR 0.000013    Time 0.024758    
2023-01-06 17:05:49,046 - Epoch: [125][  140/  246]    Overall Loss 0.272543    Objective Loss 0.272543                                        LR 0.000013    Time 0.024367    
2023-01-06 17:05:49,237 - Epoch: [125][  150/  246]    Overall Loss 0.270532    Objective Loss 0.270532                                        LR 0.000013    Time 0.024018    
2023-01-06 17:05:49,432 - Epoch: [125][  160/  246]    Overall Loss 0.270212    Objective Loss 0.270212                                        LR 0.000013    Time 0.023730    
2023-01-06 17:05:49,624 - Epoch: [125][  170/  246]    Overall Loss 0.270649    Objective Loss 0.270649                                        LR 0.000013    Time 0.023462    
2023-01-06 17:05:49,818 - Epoch: [125][  180/  246]    Overall Loss 0.270998    Objective Loss 0.270998                                        LR 0.000013    Time 0.023232    
2023-01-06 17:05:50,010 - Epoch: [125][  190/  246]    Overall Loss 0.268156    Objective Loss 0.268156                                        LR 0.000013    Time 0.023018    
2023-01-06 17:05:50,198 - Epoch: [125][  200/  246]    Overall Loss 0.267884    Objective Loss 0.267884                                        LR 0.000013    Time 0.022806    
2023-01-06 17:05:50,392 - Epoch: [125][  210/  246]    Overall Loss 0.267232    Objective Loss 0.267232                                        LR 0.000013    Time 0.022643    
2023-01-06 17:05:50,585 - Epoch: [125][  220/  246]    Overall Loss 0.267507    Objective Loss 0.267507                                        LR 0.000013    Time 0.022487    
2023-01-06 17:05:50,777 - Epoch: [125][  230/  246]    Overall Loss 0.267316    Objective Loss 0.267316                                        LR 0.000013    Time 0.022344    
2023-01-06 17:05:50,979 - Epoch: [125][  240/  246]    Overall Loss 0.267380    Objective Loss 0.267380                                        LR 0.000013    Time 0.022255    
2023-01-06 17:05:51,074 - Epoch: [125][  246/  246]    Overall Loss 0.266967    Objective Loss 0.266967    Top1 91.387560    LR 0.000013    Time 0.022096    
2023-01-06 17:05:51,239 - --- validate (epoch=125)-----------
2023-01-06 17:05:51,240 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:51,701 - Epoch: [125][   10/   28]    Loss 0.280917    Top1 89.687500    
2023-01-06 17:05:51,820 - Epoch: [125][   20/   28]    Loss 0.275216    Top1 89.882812    
2023-01-06 17:05:51,885 - Epoch: [125][   28/   28]    Loss 0.276583    Top1 89.951331    
2023-01-06 17:05:52,045 - ==> Top1: 89.951    Loss: 0.277

2023-01-06 17:05:52,045 - ==> Confusion:
[[ 213   14  212]
 [  13  244  345]
 [  54   64 5827]]

2023-01-06 17:05:52,047 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:52,047 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:52,053 - 

2023-01-06 17:05:52,053 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:52,772 - Epoch: [126][   10/  246]    Overall Loss 0.277681    Objective Loss 0.277681                                        LR 0.000013    Time 0.071848    
2023-01-06 17:05:52,971 - Epoch: [126][   20/  246]    Overall Loss 0.277526    Objective Loss 0.277526                                        LR 0.000013    Time 0.045777    
2023-01-06 17:05:53,171 - Epoch: [126][   30/  246]    Overall Loss 0.269409    Objective Loss 0.269409                                        LR 0.000013    Time 0.037193    
2023-01-06 17:05:53,381 - Epoch: [126][   40/  246]    Overall Loss 0.270666    Objective Loss 0.270666                                        LR 0.000013    Time 0.033125    
2023-01-06 17:05:53,586 - Epoch: [126][   50/  246]    Overall Loss 0.266782    Objective Loss 0.266782                                        LR 0.000013    Time 0.030600    
2023-01-06 17:05:53,794 - Epoch: [126][   60/  246]    Overall Loss 0.267754    Objective Loss 0.267754                                        LR 0.000013    Time 0.028962    
2023-01-06 17:05:54,001 - Epoch: [126][   70/  246]    Overall Loss 0.268001    Objective Loss 0.268001                                        LR 0.000013    Time 0.027775    
2023-01-06 17:05:54,211 - Epoch: [126][   80/  246]    Overall Loss 0.267484    Objective Loss 0.267484                                        LR 0.000013    Time 0.026922    
2023-01-06 17:05:54,418 - Epoch: [126][   90/  246]    Overall Loss 0.267010    Objective Loss 0.267010                                        LR 0.000013    Time 0.026230    
2023-01-06 17:05:54,629 - Epoch: [126][  100/  246]    Overall Loss 0.266323    Objective Loss 0.266323                                        LR 0.000013    Time 0.025709    
2023-01-06 17:05:54,835 - Epoch: [126][  110/  246]    Overall Loss 0.267840    Objective Loss 0.267840                                        LR 0.000013    Time 0.025237    
2023-01-06 17:05:55,043 - Epoch: [126][  120/  246]    Overall Loss 0.267033    Objective Loss 0.267033                                        LR 0.000013    Time 0.024873    
2023-01-06 17:05:55,248 - Epoch: [126][  130/  246]    Overall Loss 0.266772    Objective Loss 0.266772                                        LR 0.000013    Time 0.024532    
2023-01-06 17:05:55,457 - Epoch: [126][  140/  246]    Overall Loss 0.267012    Objective Loss 0.267012                                        LR 0.000013    Time 0.024268    
2023-01-06 17:05:55,664 - Epoch: [126][  150/  246]    Overall Loss 0.267932    Objective Loss 0.267932                                        LR 0.000013    Time 0.024029    
2023-01-06 17:05:55,861 - Epoch: [126][  160/  246]    Overall Loss 0.268422    Objective Loss 0.268422                                        LR 0.000013    Time 0.023758    
2023-01-06 17:05:56,062 - Epoch: [126][  170/  246]    Overall Loss 0.268090    Objective Loss 0.268090                                        LR 0.000013    Time 0.023536    
2023-01-06 17:05:56,259 - Epoch: [126][  180/  246]    Overall Loss 0.268128    Objective Loss 0.268128                                        LR 0.000013    Time 0.023321    
2023-01-06 17:05:56,471 - Epoch: [126][  190/  246]    Overall Loss 0.267939    Objective Loss 0.267939                                        LR 0.000013    Time 0.023208    
2023-01-06 17:05:56,672 - Epoch: [126][  200/  246]    Overall Loss 0.268308    Objective Loss 0.268308                                        LR 0.000013    Time 0.023053    
2023-01-06 17:05:56,880 - Epoch: [126][  210/  246]    Overall Loss 0.267288    Objective Loss 0.267288                                        LR 0.000013    Time 0.022944    
2023-01-06 17:05:57,080 - Epoch: [126][  220/  246]    Overall Loss 0.267911    Objective Loss 0.267911                                        LR 0.000013    Time 0.022805    
2023-01-06 17:05:57,282 - Epoch: [126][  230/  246]    Overall Loss 0.267247    Objective Loss 0.267247                                        LR 0.000013    Time 0.022693    
2023-01-06 17:05:57,488 - Epoch: [126][  240/  246]    Overall Loss 0.267253    Objective Loss 0.267253                                        LR 0.000013    Time 0.022603    
2023-01-06 17:05:57,583 - Epoch: [126][  246/  246]    Overall Loss 0.268227    Objective Loss 0.268227    Top1 88.038278    LR 0.000013    Time 0.022439    
2023-01-06 17:05:57,770 - --- validate (epoch=126)-----------
2023-01-06 17:05:57,771 - 6986 samples (256 per mini-batch)
2023-01-06 17:05:58,215 - Epoch: [126][   10/   28]    Loss 0.273217    Top1 90.273438    
2023-01-06 17:05:58,326 - Epoch: [126][   20/   28]    Loss 0.274755    Top1 90.039062    
2023-01-06 17:05:58,392 - Epoch: [126][   28/   28]    Loss 0.275549    Top1 89.851131    
2023-01-06 17:05:58,555 - ==> Top1: 89.851    Loss: 0.276

2023-01-06 17:05:58,555 - ==> Confusion:
[[ 212   13  214]
 [  13  253  336]
 [  50   83 5812]]

2023-01-06 17:05:58,556 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:05:58,556 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:05:58,562 - 

2023-01-06 17:05:58,562 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:05:59,287 - Epoch: [127][   10/  246]    Overall Loss 0.289453    Objective Loss 0.289453                                        LR 0.000013    Time 0.072346    
2023-01-06 17:05:59,501 - Epoch: [127][   20/  246]    Overall Loss 0.282904    Objective Loss 0.282904                                        LR 0.000013    Time 0.046889    
2023-01-06 17:05:59,705 - Epoch: [127][   30/  246]    Overall Loss 0.277464    Objective Loss 0.277464                                        LR 0.000013    Time 0.038027    
2023-01-06 17:05:59,919 - Epoch: [127][   40/  246]    Overall Loss 0.277625    Objective Loss 0.277625                                        LR 0.000013    Time 0.033880    
2023-01-06 17:06:00,124 - Epoch: [127][   50/  246]    Overall Loss 0.280026    Objective Loss 0.280026                                        LR 0.000013    Time 0.031184    
2023-01-06 17:06:00,338 - Epoch: [127][   60/  246]    Overall Loss 0.277425    Objective Loss 0.277425                                        LR 0.000013    Time 0.029554    
2023-01-06 17:06:00,539 - Epoch: [127][   70/  246]    Overall Loss 0.274682    Objective Loss 0.274682                                        LR 0.000013    Time 0.028200    
2023-01-06 17:06:00,751 - Epoch: [127][   80/  246]    Overall Loss 0.274819    Objective Loss 0.274819                                        LR 0.000013    Time 0.027320    
2023-01-06 17:06:00,953 - Epoch: [127][   90/  246]    Overall Loss 0.274055    Objective Loss 0.274055                                        LR 0.000013    Time 0.026526    
2023-01-06 17:06:01,163 - Epoch: [127][  100/  246]    Overall Loss 0.272303    Objective Loss 0.272303                                        LR 0.000013    Time 0.025971    
2023-01-06 17:06:01,367 - Epoch: [127][  110/  246]    Overall Loss 0.271109    Objective Loss 0.271109                                        LR 0.000013    Time 0.025459    
2023-01-06 17:06:01,584 - Epoch: [127][  120/  246]    Overall Loss 0.269990    Objective Loss 0.269990                                        LR 0.000013    Time 0.025144    
2023-01-06 17:06:01,789 - Epoch: [127][  130/  246]    Overall Loss 0.269039    Objective Loss 0.269039                                        LR 0.000013    Time 0.024781    
2023-01-06 17:06:02,003 - Epoch: [127][  140/  246]    Overall Loss 0.269275    Objective Loss 0.269275                                        LR 0.000013    Time 0.024541    
2023-01-06 17:06:02,208 - Epoch: [127][  150/  246]    Overall Loss 0.268763    Objective Loss 0.268763                                        LR 0.000013    Time 0.024267    
2023-01-06 17:06:02,413 - Epoch: [127][  160/  246]    Overall Loss 0.268183    Objective Loss 0.268183                                        LR 0.000013    Time 0.024026    
2023-01-06 17:06:02,620 - Epoch: [127][  170/  246]    Overall Loss 0.268294    Objective Loss 0.268294                                        LR 0.000013    Time 0.023831    
2023-01-06 17:06:02,826 - Epoch: [127][  180/  246]    Overall Loss 0.268132    Objective Loss 0.268132                                        LR 0.000013    Time 0.023649    
2023-01-06 17:06:03,033 - Epoch: [127][  190/  246]    Overall Loss 0.268452    Objective Loss 0.268452                                        LR 0.000013    Time 0.023490    
2023-01-06 17:06:03,237 - Epoch: [127][  200/  246]    Overall Loss 0.269077    Objective Loss 0.269077                                        LR 0.000013    Time 0.023337    
2023-01-06 17:06:03,446 - Epoch: [127][  210/  246]    Overall Loss 0.268350    Objective Loss 0.268350                                        LR 0.000013    Time 0.023219    
2023-01-06 17:06:03,648 - Epoch: [127][  220/  246]    Overall Loss 0.267766    Objective Loss 0.267766                                        LR 0.000013    Time 0.023079    
2023-01-06 17:06:03,846 - Epoch: [127][  230/  246]    Overall Loss 0.267793    Objective Loss 0.267793                                        LR 0.000013    Time 0.022934    
2023-01-06 17:06:04,056 - Epoch: [127][  240/  246]    Overall Loss 0.268045    Objective Loss 0.268045                                        LR 0.000013    Time 0.022852    
2023-01-06 17:06:04,147 - Epoch: [127][  246/  246]    Overall Loss 0.268066    Objective Loss 0.268066    Top1 92.105263    LR 0.000013    Time 0.022665    
2023-01-06 17:06:04,331 - --- validate (epoch=127)-----------
2023-01-06 17:06:04,331 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:04,780 - Epoch: [127][   10/   28]    Loss 0.262185    Top1 90.351562    
2023-01-06 17:06:04,890 - Epoch: [127][   20/   28]    Loss 0.269972    Top1 90.156250    
2023-01-06 17:06:04,954 - Epoch: [127][   28/   28]    Loss 0.275519    Top1 89.979960    
2023-01-06 17:06:05,097 - ==> Top1: 89.980    Loss: 0.276

2023-01-06 17:06:05,097 - ==> Confusion:
[[ 222   14  203]
 [  16  242  344]
 [  57   66 5822]]

2023-01-06 17:06:05,099 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:05,099 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:05,105 - 

2023-01-06 17:06:05,105 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:05,680 - Epoch: [128][   10/  246]    Overall Loss 0.257093    Objective Loss 0.257093                                        LR 0.000013    Time 0.057467    
2023-01-06 17:06:05,873 - Epoch: [128][   20/  246]    Overall Loss 0.247679    Objective Loss 0.247679                                        LR 0.000013    Time 0.038337    
2023-01-06 17:06:06,062 - Epoch: [128][   30/  246]    Overall Loss 0.256022    Objective Loss 0.256022                                        LR 0.000013    Time 0.031862    
2023-01-06 17:06:06,249 - Epoch: [128][   40/  246]    Overall Loss 0.256997    Objective Loss 0.256997                                        LR 0.000013    Time 0.028548    
2023-01-06 17:06:06,439 - Epoch: [128][   50/  246]    Overall Loss 0.261430    Objective Loss 0.261430                                        LR 0.000013    Time 0.026636    
2023-01-06 17:06:06,626 - Epoch: [128][   60/  246]    Overall Loss 0.258980    Objective Loss 0.258980                                        LR 0.000013    Time 0.025316    
2023-01-06 17:06:06,808 - Epoch: [128][   70/  246]    Overall Loss 0.261449    Objective Loss 0.261449                                        LR 0.000013    Time 0.024294    
2023-01-06 17:06:06,999 - Epoch: [128][   80/  246]    Overall Loss 0.260465    Objective Loss 0.260465                                        LR 0.000013    Time 0.023642    
2023-01-06 17:06:07,192 - Epoch: [128][   90/  246]    Overall Loss 0.261375    Objective Loss 0.261375                                        LR 0.000013    Time 0.023155    
2023-01-06 17:06:07,387 - Epoch: [128][  100/  246]    Overall Loss 0.261352    Objective Loss 0.261352                                        LR 0.000013    Time 0.022783    
2023-01-06 17:06:07,582 - Epoch: [128][  110/  246]    Overall Loss 0.261209    Objective Loss 0.261209                                        LR 0.000013    Time 0.022477    
2023-01-06 17:06:07,773 - Epoch: [128][  120/  246]    Overall Loss 0.262927    Objective Loss 0.262927                                        LR 0.000013    Time 0.022199    
2023-01-06 17:06:07,963 - Epoch: [128][  130/  246]    Overall Loss 0.263322    Objective Loss 0.263322                                        LR 0.000013    Time 0.021946    
2023-01-06 17:06:08,156 - Epoch: [128][  140/  246]    Overall Loss 0.262789    Objective Loss 0.262789                                        LR 0.000013    Time 0.021752    
2023-01-06 17:06:08,348 - Epoch: [128][  150/  246]    Overall Loss 0.263730    Objective Loss 0.263730                                        LR 0.000013    Time 0.021580    
2023-01-06 17:06:08,540 - Epoch: [128][  160/  246]    Overall Loss 0.262712    Objective Loss 0.262712                                        LR 0.000013    Time 0.021429    
2023-01-06 17:06:08,731 - Epoch: [128][  170/  246]    Overall Loss 0.264124    Objective Loss 0.264124                                        LR 0.000013    Time 0.021293    
2023-01-06 17:06:08,921 - Epoch: [128][  180/  246]    Overall Loss 0.264062    Objective Loss 0.264062                                        LR 0.000013    Time 0.021161    
2023-01-06 17:06:09,110 - Epoch: [128][  190/  246]    Overall Loss 0.264592    Objective Loss 0.264592                                        LR 0.000013    Time 0.021042    
2023-01-06 17:06:09,300 - Epoch: [128][  200/  246]    Overall Loss 0.264677    Objective Loss 0.264677                                        LR 0.000013    Time 0.020935    
2023-01-06 17:06:09,489 - Epoch: [128][  210/  246]    Overall Loss 0.264891    Objective Loss 0.264891                                        LR 0.000013    Time 0.020838    
2023-01-06 17:06:09,678 - Epoch: [128][  220/  246]    Overall Loss 0.264950    Objective Loss 0.264950                                        LR 0.000013    Time 0.020748    
2023-01-06 17:06:09,867 - Epoch: [128][  230/  246]    Overall Loss 0.265432    Objective Loss 0.265432                                        LR 0.000013    Time 0.020666    
2023-01-06 17:06:10,070 - Epoch: [128][  240/  246]    Overall Loss 0.265866    Objective Loss 0.265866                                        LR 0.000013    Time 0.020651    
2023-01-06 17:06:10,165 - Epoch: [128][  246/  246]    Overall Loss 0.266089    Objective Loss 0.266089    Top1 90.191388    LR 0.000013    Time 0.020533    
2023-01-06 17:06:10,308 - --- validate (epoch=128)-----------
2023-01-06 17:06:10,308 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:10,758 - Epoch: [128][   10/   28]    Loss 0.281811    Top1 89.335938    
2023-01-06 17:06:10,867 - Epoch: [128][   20/   28]    Loss 0.274670    Top1 89.804688    
2023-01-06 17:06:10,933 - Epoch: [128][   28/   28]    Loss 0.276854    Top1 89.836816    
2023-01-06 17:06:11,069 - ==> Top1: 89.837    Loss: 0.277

2023-01-06 17:06:11,070 - ==> Confusion:
[[ 216   17  206]
 [  16  247  339]
 [  58   74 5813]]

2023-01-06 17:06:11,071 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:11,071 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:11,077 - 

2023-01-06 17:06:11,077 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:11,810 - Epoch: [129][   10/  246]    Overall Loss 0.251433    Objective Loss 0.251433                                        LR 0.000013    Time 0.073236    
2023-01-06 17:06:12,016 - Epoch: [129][   20/  246]    Overall Loss 0.268047    Objective Loss 0.268047                                        LR 0.000013    Time 0.046877    
2023-01-06 17:06:12,217 - Epoch: [129][   30/  246]    Overall Loss 0.259807    Objective Loss 0.259807                                        LR 0.000013    Time 0.037963    
2023-01-06 17:06:12,422 - Epoch: [129][   40/  246]    Overall Loss 0.259064    Objective Loss 0.259064                                        LR 0.000013    Time 0.033585    
2023-01-06 17:06:12,628 - Epoch: [129][   50/  246]    Overall Loss 0.258627    Objective Loss 0.258627                                        LR 0.000013    Time 0.030967    
2023-01-06 17:06:12,829 - Epoch: [129][   60/  246]    Overall Loss 0.259818    Objective Loss 0.259818                                        LR 0.000013    Time 0.029158    
2023-01-06 17:06:13,039 - Epoch: [129][   70/  246]    Overall Loss 0.260304    Objective Loss 0.260304                                        LR 0.000013    Time 0.027987    
2023-01-06 17:06:13,249 - Epoch: [129][   80/  246]    Overall Loss 0.258438    Objective Loss 0.258438                                        LR 0.000013    Time 0.027108    
2023-01-06 17:06:13,465 - Epoch: [129][   90/  246]    Overall Loss 0.259327    Objective Loss 0.259327                                        LR 0.000013    Time 0.026494    
2023-01-06 17:06:13,676 - Epoch: [129][  100/  246]    Overall Loss 0.259901    Objective Loss 0.259901                                        LR 0.000013    Time 0.025950    
2023-01-06 17:06:13,890 - Epoch: [129][  110/  246]    Overall Loss 0.260653    Objective Loss 0.260653                                        LR 0.000013    Time 0.025532    
2023-01-06 17:06:14,092 - Epoch: [129][  120/  246]    Overall Loss 0.261498    Objective Loss 0.261498                                        LR 0.000013    Time 0.025082    
2023-01-06 17:06:14,291 - Epoch: [129][  130/  246]    Overall Loss 0.262516    Objective Loss 0.262516                                        LR 0.000013    Time 0.024686    
2023-01-06 17:06:14,499 - Epoch: [129][  140/  246]    Overall Loss 0.265299    Objective Loss 0.265299                                        LR 0.000013    Time 0.024404    
2023-01-06 17:06:14,697 - Epoch: [129][  150/  246]    Overall Loss 0.264228    Objective Loss 0.264228                                        LR 0.000013    Time 0.024093    
2023-01-06 17:06:14,895 - Epoch: [129][  160/  246]    Overall Loss 0.263587    Objective Loss 0.263587                                        LR 0.000013    Time 0.023823    
2023-01-06 17:06:15,093 - Epoch: [129][  170/  246]    Overall Loss 0.263897    Objective Loss 0.263897                                        LR 0.000013    Time 0.023582    
2023-01-06 17:06:15,290 - Epoch: [129][  180/  246]    Overall Loss 0.263054    Objective Loss 0.263054                                        LR 0.000013    Time 0.023368    
2023-01-06 17:06:15,488 - Epoch: [129][  190/  246]    Overall Loss 0.264022    Objective Loss 0.264022                                        LR 0.000013    Time 0.023179    
2023-01-06 17:06:15,686 - Epoch: [129][  200/  246]    Overall Loss 0.263547    Objective Loss 0.263547                                        LR 0.000013    Time 0.023007    
2023-01-06 17:06:15,884 - Epoch: [129][  210/  246]    Overall Loss 0.264207    Objective Loss 0.264207                                        LR 0.000013    Time 0.022851    
2023-01-06 17:06:16,085 - Epoch: [129][  220/  246]    Overall Loss 0.265111    Objective Loss 0.265111                                        LR 0.000013    Time 0.022726    
2023-01-06 17:06:16,292 - Epoch: [129][  230/  246]    Overall Loss 0.266437    Objective Loss 0.266437                                        LR 0.000013    Time 0.022636    
2023-01-06 17:06:16,504 - Epoch: [129][  240/  246]    Overall Loss 0.266786    Objective Loss 0.266786                                        LR 0.000013    Time 0.022574    
2023-01-06 17:06:16,598 - Epoch: [129][  246/  246]    Overall Loss 0.266578    Objective Loss 0.266578    Top1 92.583732    LR 0.000013    Time 0.022406    
2023-01-06 17:06:16,748 - --- validate (epoch=129)-----------
2023-01-06 17:06:16,748 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:17,192 - Epoch: [129][   10/   28]    Loss 0.284455    Top1 89.609375    
2023-01-06 17:06:17,302 - Epoch: [129][   20/   28]    Loss 0.279292    Top1 89.746094    
2023-01-06 17:06:17,367 - Epoch: [129][   28/   28]    Loss 0.278262    Top1 89.836816    
2023-01-06 17:06:17,528 - ==> Top1: 89.837    Loss: 0.278

2023-01-06 17:06:17,528 - ==> Confusion:
[[ 228   14  197]
 [  16  241  345]
 [  63   75 5807]]

2023-01-06 17:06:17,529 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:17,529 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:17,535 - 

2023-01-06 17:06:17,535 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:18,239 - Epoch: [130][   10/  246]    Overall Loss 0.245891    Objective Loss 0.245891                                        LR 0.000013    Time 0.070290    
2023-01-06 17:06:18,433 - Epoch: [130][   20/  246]    Overall Loss 0.253599    Objective Loss 0.253599                                        LR 0.000013    Time 0.044843    
2023-01-06 17:06:18,633 - Epoch: [130][   30/  246]    Overall Loss 0.257946    Objective Loss 0.257946                                        LR 0.000013    Time 0.036551    
2023-01-06 17:06:18,827 - Epoch: [130][   40/  246]    Overall Loss 0.260994    Objective Loss 0.260994                                        LR 0.000013    Time 0.032250    
2023-01-06 17:06:19,022 - Epoch: [130][   50/  246]    Overall Loss 0.263019    Objective Loss 0.263019                                        LR 0.000013    Time 0.029682    
2023-01-06 17:06:19,210 - Epoch: [130][   60/  246]    Overall Loss 0.268223    Objective Loss 0.268223                                        LR 0.000013    Time 0.027878    
2023-01-06 17:06:19,399 - Epoch: [130][   70/  246]    Overall Loss 0.269603    Objective Loss 0.269603                                        LR 0.000013    Time 0.026584    
2023-01-06 17:06:19,584 - Epoch: [130][   80/  246]    Overall Loss 0.269988    Objective Loss 0.269988                                        LR 0.000013    Time 0.025571    
2023-01-06 17:06:19,775 - Epoch: [130][   90/  246]    Overall Loss 0.271472    Objective Loss 0.271472                                        LR 0.000013    Time 0.024848    
2023-01-06 17:06:19,967 - Epoch: [130][  100/  246]    Overall Loss 0.270156    Objective Loss 0.270156                                        LR 0.000013    Time 0.024274    
2023-01-06 17:06:20,166 - Epoch: [130][  110/  246]    Overall Loss 0.271001    Objective Loss 0.271001                                        LR 0.000013    Time 0.023874    
2023-01-06 17:06:20,356 - Epoch: [130][  120/  246]    Overall Loss 0.271237    Objective Loss 0.271237                                        LR 0.000013    Time 0.023471    
2023-01-06 17:06:20,547 - Epoch: [130][  130/  246]    Overall Loss 0.269002    Objective Loss 0.269002                                        LR 0.000013    Time 0.023132    
2023-01-06 17:06:20,738 - Epoch: [130][  140/  246]    Overall Loss 0.268529    Objective Loss 0.268529                                        LR 0.000013    Time 0.022840    
2023-01-06 17:06:20,929 - Epoch: [130][  150/  246]    Overall Loss 0.269026    Objective Loss 0.269026                                        LR 0.000013    Time 0.022588    
2023-01-06 17:06:21,120 - Epoch: [130][  160/  246]    Overall Loss 0.269453    Objective Loss 0.269453                                        LR 0.000013    Time 0.022367    
2023-01-06 17:06:21,311 - Epoch: [130][  170/  246]    Overall Loss 0.268856    Objective Loss 0.268856                                        LR 0.000013    Time 0.022171    
2023-01-06 17:06:21,506 - Epoch: [130][  180/  246]    Overall Loss 0.268867    Objective Loss 0.268867                                        LR 0.000013    Time 0.022022    
2023-01-06 17:06:21,699 - Epoch: [130][  190/  246]    Overall Loss 0.268437    Objective Loss 0.268437                                        LR 0.000013    Time 0.021878    
2023-01-06 17:06:21,894 - Epoch: [130][  200/  246]    Overall Loss 0.268452    Objective Loss 0.268452                                        LR 0.000013    Time 0.021754    
2023-01-06 17:06:22,087 - Epoch: [130][  210/  246]    Overall Loss 0.268673    Objective Loss 0.268673                                        LR 0.000013    Time 0.021637    
2023-01-06 17:06:22,281 - Epoch: [130][  220/  246]    Overall Loss 0.268097    Objective Loss 0.268097                                        LR 0.000013    Time 0.021535    
2023-01-06 17:06:22,474 - Epoch: [130][  230/  246]    Overall Loss 0.267748    Objective Loss 0.267748                                        LR 0.000013    Time 0.021437    
2023-01-06 17:06:22,682 - Epoch: [130][  240/  246]    Overall Loss 0.267509    Objective Loss 0.267509                                        LR 0.000013    Time 0.021408    
2023-01-06 17:06:22,776 - Epoch: [130][  246/  246]    Overall Loss 0.266952    Objective Loss 0.266952    Top1 92.583732    LR 0.000013    Time 0.021268    
2023-01-06 17:06:22,921 - --- validate (epoch=130)-----------
2023-01-06 17:06:22,921 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:23,370 - Epoch: [130][   10/   28]    Loss 0.289197    Top1 89.296875    
2023-01-06 17:06:23,483 - Epoch: [130][   20/   28]    Loss 0.284690    Top1 89.707031    
2023-01-06 17:06:23,548 - Epoch: [130][   28/   28]    Loss 0.284335    Top1 89.822502    
2023-01-06 17:06:23,707 - ==> Top1: 89.823    Loss: 0.284

2023-01-06 17:06:23,707 - ==> Confusion:
[[ 229   17  193]
 [  12  278  312]
 [  63  114 5768]]

2023-01-06 17:06:23,708 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:23,708 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:23,714 - 

2023-01-06 17:06:23,714 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:24,282 - Epoch: [131][   10/  246]    Overall Loss 0.264006    Objective Loss 0.264006                                        LR 0.000013    Time 0.056720    
2023-01-06 17:06:24,468 - Epoch: [131][   20/  246]    Overall Loss 0.268124    Objective Loss 0.268124                                        LR 0.000013    Time 0.037624    
2023-01-06 17:06:24,664 - Epoch: [131][   30/  246]    Overall Loss 0.261968    Objective Loss 0.261968                                        LR 0.000013    Time 0.031595    
2023-01-06 17:06:24,868 - Epoch: [131][   40/  246]    Overall Loss 0.263190    Objective Loss 0.263190                                        LR 0.000013    Time 0.028786    
2023-01-06 17:06:25,063 - Epoch: [131][   50/  246]    Overall Loss 0.268230    Objective Loss 0.268230                                        LR 0.000013    Time 0.026933    
2023-01-06 17:06:25,268 - Epoch: [131][   60/  246]    Overall Loss 0.265303    Objective Loss 0.265303                                        LR 0.000013    Time 0.025843    
2023-01-06 17:06:25,464 - Epoch: [131][   70/  246]    Overall Loss 0.262801    Objective Loss 0.262801                                        LR 0.000013    Time 0.024959    
2023-01-06 17:06:25,660 - Epoch: [131][   80/  246]    Overall Loss 0.261173    Objective Loss 0.261173                                        LR 0.000013    Time 0.024276    
2023-01-06 17:06:25,853 - Epoch: [131][   90/  246]    Overall Loss 0.263404    Objective Loss 0.263404                                        LR 0.000013    Time 0.023719    
2023-01-06 17:06:26,048 - Epoch: [131][  100/  246]    Overall Loss 0.264136    Objective Loss 0.264136                                        LR 0.000013    Time 0.023295    
2023-01-06 17:06:26,249 - Epoch: [131][  110/  246]    Overall Loss 0.264160    Objective Loss 0.264160                                        LR 0.000013    Time 0.023006    
2023-01-06 17:06:26,453 - Epoch: [131][  120/  246]    Overall Loss 0.263615    Objective Loss 0.263615                                        LR 0.000013    Time 0.022780    
2023-01-06 17:06:26,679 - Epoch: [131][  130/  246]    Overall Loss 0.264545    Objective Loss 0.264545                                        LR 0.000013    Time 0.022762    
2023-01-06 17:06:26,902 - Epoch: [131][  140/  246]    Overall Loss 0.264950    Objective Loss 0.264950                                        LR 0.000013    Time 0.022730    
2023-01-06 17:06:27,128 - Epoch: [131][  150/  246]    Overall Loss 0.265894    Objective Loss 0.265894                                        LR 0.000013    Time 0.022719    
2023-01-06 17:06:27,348 - Epoch: [131][  160/  246]    Overall Loss 0.266214    Objective Loss 0.266214                                        LR 0.000013    Time 0.022673    
2023-01-06 17:06:27,573 - Epoch: [131][  170/  246]    Overall Loss 0.266391    Objective Loss 0.266391                                        LR 0.000013    Time 0.022660    
2023-01-06 17:06:27,796 - Epoch: [131][  180/  246]    Overall Loss 0.266387    Objective Loss 0.266387                                        LR 0.000013    Time 0.022637    
2023-01-06 17:06:28,014 - Epoch: [131][  190/  246]    Overall Loss 0.265819    Objective Loss 0.265819                                        LR 0.000013    Time 0.022587    
2023-01-06 17:06:28,230 - Epoch: [131][  200/  246]    Overall Loss 0.266003    Objective Loss 0.266003                                        LR 0.000013    Time 0.022537    
2023-01-06 17:06:28,447 - Epoch: [131][  210/  246]    Overall Loss 0.265807    Objective Loss 0.265807                                        LR 0.000013    Time 0.022494    
2023-01-06 17:06:28,641 - Epoch: [131][  220/  246]    Overall Loss 0.266469    Objective Loss 0.266469                                        LR 0.000013    Time 0.022354    
2023-01-06 17:06:28,835 - Epoch: [131][  230/  246]    Overall Loss 0.266620    Objective Loss 0.266620                                        LR 0.000013    Time 0.022223    
2023-01-06 17:06:29,040 - Epoch: [131][  240/  246]    Overall Loss 0.265853    Objective Loss 0.265853                                        LR 0.000013    Time 0.022150    
2023-01-06 17:06:29,134 - Epoch: [131][  246/  246]    Overall Loss 0.265308    Objective Loss 0.265308    Top1 88.995215    LR 0.000013    Time 0.021993    
2023-01-06 17:06:29,268 - --- validate (epoch=131)-----------
2023-01-06 17:06:29,268 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:29,718 - Epoch: [131][   10/   28]    Loss 0.286231    Top1 89.335938    
2023-01-06 17:06:29,832 - Epoch: [131][   20/   28]    Loss 0.272639    Top1 90.078125    
2023-01-06 17:06:29,898 - Epoch: [131][   28/   28]    Loss 0.275587    Top1 89.808188    
2023-01-06 17:06:30,040 - ==> Top1: 89.808    Loss: 0.276

2023-01-06 17:06:30,041 - ==> Confusion:
[[ 204   10  225]
 [  15  219  368]
 [  43   51 5851]]

2023-01-06 17:06:30,042 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:30,042 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:30,048 - 

2023-01-06 17:06:30,048 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:30,775 - Epoch: [132][   10/  246]    Overall Loss 0.284647    Objective Loss 0.284647                                        LR 0.000013    Time 0.072677    
2023-01-06 17:06:30,961 - Epoch: [132][   20/  246]    Overall Loss 0.272386    Objective Loss 0.272386                                        LR 0.000013    Time 0.045586    
2023-01-06 17:06:31,146 - Epoch: [132][   30/  246]    Overall Loss 0.273523    Objective Loss 0.273523                                        LR 0.000013    Time 0.036557    
2023-01-06 17:06:31,343 - Epoch: [132][   40/  246]    Overall Loss 0.269379    Objective Loss 0.269379                                        LR 0.000013    Time 0.032319    
2023-01-06 17:06:31,538 - Epoch: [132][   50/  246]    Overall Loss 0.268469    Objective Loss 0.268469                                        LR 0.000013    Time 0.029761    
2023-01-06 17:06:31,735 - Epoch: [132][   60/  246]    Overall Loss 0.266260    Objective Loss 0.266260                                        LR 0.000013    Time 0.028078    
2023-01-06 17:06:31,917 - Epoch: [132][   70/  246]    Overall Loss 0.267615    Objective Loss 0.267615                                        LR 0.000013    Time 0.026661    
2023-01-06 17:06:32,113 - Epoch: [132][   80/  246]    Overall Loss 0.267944    Objective Loss 0.267944                                        LR 0.000013    Time 0.025769    
2023-01-06 17:06:32,316 - Epoch: [132][   90/  246]    Overall Loss 0.266889    Objective Loss 0.266889                                        LR 0.000013    Time 0.025162    
2023-01-06 17:06:32,547 - Epoch: [132][  100/  246]    Overall Loss 0.265698    Objective Loss 0.265698                                        LR 0.000013    Time 0.024944    
2023-01-06 17:06:32,777 - Epoch: [132][  110/  246]    Overall Loss 0.265187    Objective Loss 0.265187                                        LR 0.000013    Time 0.024770    
2023-01-06 17:06:33,007 - Epoch: [132][  120/  246]    Overall Loss 0.267435    Objective Loss 0.267435                                        LR 0.000013    Time 0.024616    
2023-01-06 17:06:33,233 - Epoch: [132][  130/  246]    Overall Loss 0.266937    Objective Loss 0.266937                                        LR 0.000013    Time 0.024456    
2023-01-06 17:06:33,477 - Epoch: [132][  140/  246]    Overall Loss 0.267264    Objective Loss 0.267264                                        LR 0.000013    Time 0.024454    
2023-01-06 17:06:33,721 - Epoch: [132][  150/  246]    Overall Loss 0.265523    Objective Loss 0.265523                                        LR 0.000013    Time 0.024445    
2023-01-06 17:06:33,962 - Epoch: [132][  160/  246]    Overall Loss 0.264401    Objective Loss 0.264401                                        LR 0.000013    Time 0.024411    
2023-01-06 17:06:34,196 - Epoch: [132][  170/  246]    Overall Loss 0.264643    Objective Loss 0.264643                                        LR 0.000013    Time 0.024346    
2023-01-06 17:06:34,429 - Epoch: [132][  180/  246]    Overall Loss 0.265612    Objective Loss 0.265612                                        LR 0.000013    Time 0.024284    
2023-01-06 17:06:34,673 - Epoch: [132][  190/  246]    Overall Loss 0.266557    Objective Loss 0.266557                                        LR 0.000013    Time 0.024291    
2023-01-06 17:06:34,911 - Epoch: [132][  200/  246]    Overall Loss 0.267238    Objective Loss 0.267238                                        LR 0.000013    Time 0.024263    
2023-01-06 17:06:35,141 - Epoch: [132][  210/  246]    Overall Loss 0.266262    Objective Loss 0.266262                                        LR 0.000013    Time 0.024201    
2023-01-06 17:06:35,361 - Epoch: [132][  220/  246]    Overall Loss 0.266468    Objective Loss 0.266468                                        LR 0.000013    Time 0.024096    
2023-01-06 17:06:35,579 - Epoch: [132][  230/  246]    Overall Loss 0.266350    Objective Loss 0.266350                                        LR 0.000013    Time 0.023995    
2023-01-06 17:06:35,810 - Epoch: [132][  240/  246]    Overall Loss 0.265892    Objective Loss 0.265892                                        LR 0.000013    Time 0.023958    
2023-01-06 17:06:35,924 - Epoch: [132][  246/  246]    Overall Loss 0.265135    Objective Loss 0.265135    Top1 91.626794    LR 0.000013    Time 0.023836    
2023-01-06 17:06:36,062 - --- validate (epoch=132)-----------
2023-01-06 17:06:36,062 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:36,519 - Epoch: [132][   10/   28]    Loss 0.270738    Top1 90.234375    
2023-01-06 17:06:36,631 - Epoch: [132][   20/   28]    Loss 0.273105    Top1 89.882812    
2023-01-06 17:06:36,699 - Epoch: [132][   28/   28]    Loss 0.276477    Top1 89.693673    
2023-01-06 17:06:36,843 - ==> Top1: 89.694    Loss: 0.276

2023-01-06 17:06:36,843 - ==> Confusion:
[[ 201   13  225]
 [  15  216  371]
 [  42   54 5849]]

2023-01-06 17:06:36,845 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:36,845 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:36,851 - 

2023-01-06 17:06:36,851 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:37,445 - Epoch: [133][   10/  246]    Overall Loss 0.280910    Objective Loss 0.280910                                        LR 0.000013    Time 0.059309    
2023-01-06 17:06:37,647 - Epoch: [133][   20/  246]    Overall Loss 0.268605    Objective Loss 0.268605                                        LR 0.000013    Time 0.039701    
2023-01-06 17:06:37,851 - Epoch: [133][   30/  246]    Overall Loss 0.267411    Objective Loss 0.267411                                        LR 0.000013    Time 0.033237    
2023-01-06 17:06:38,046 - Epoch: [133][   40/  246]    Overall Loss 0.268969    Objective Loss 0.268969                                        LR 0.000013    Time 0.029809    
2023-01-06 17:06:38,241 - Epoch: [133][   50/  246]    Overall Loss 0.266352    Objective Loss 0.266352                                        LR 0.000013    Time 0.027734    
2023-01-06 17:06:38,436 - Epoch: [133][   60/  246]    Overall Loss 0.265665    Objective Loss 0.265665                                        LR 0.000013    Time 0.026362    
2023-01-06 17:06:38,640 - Epoch: [133][   70/  246]    Overall Loss 0.265625    Objective Loss 0.265625                                        LR 0.000013    Time 0.025507    
2023-01-06 17:06:38,837 - Epoch: [133][   80/  246]    Overall Loss 0.264708    Objective Loss 0.264708                                        LR 0.000013    Time 0.024775    
2023-01-06 17:06:39,042 - Epoch: [133][   90/  246]    Overall Loss 0.264907    Objective Loss 0.264907                                        LR 0.000013    Time 0.024297    
2023-01-06 17:06:39,232 - Epoch: [133][  100/  246]    Overall Loss 0.265420    Objective Loss 0.265420                                        LR 0.000013    Time 0.023755    
2023-01-06 17:06:39,422 - Epoch: [133][  110/  246]    Overall Loss 0.266967    Objective Loss 0.266967                                        LR 0.000013    Time 0.023325    
2023-01-06 17:06:39,604 - Epoch: [133][  120/  246]    Overall Loss 0.266538    Objective Loss 0.266538                                        LR 0.000013    Time 0.022895    
2023-01-06 17:06:39,794 - Epoch: [133][  130/  246]    Overall Loss 0.268006    Objective Loss 0.268006                                        LR 0.000013    Time 0.022596    
2023-01-06 17:06:39,974 - Epoch: [133][  140/  246]    Overall Loss 0.267820    Objective Loss 0.267820                                        LR 0.000013    Time 0.022260    
2023-01-06 17:06:40,160 - Epoch: [133][  150/  246]    Overall Loss 0.265817    Objective Loss 0.265817                                        LR 0.000013    Time 0.022013    
2023-01-06 17:06:40,340 - Epoch: [133][  160/  246]    Overall Loss 0.266311    Objective Loss 0.266311                                        LR 0.000013    Time 0.021761    
2023-01-06 17:06:40,530 - Epoch: [133][  170/  246]    Overall Loss 0.267148    Objective Loss 0.267148                                        LR 0.000013    Time 0.021596    
2023-01-06 17:06:40,726 - Epoch: [133][  180/  246]    Overall Loss 0.266750    Objective Loss 0.266750                                        LR 0.000013    Time 0.021486    
2023-01-06 17:06:40,937 - Epoch: [133][  190/  246]    Overall Loss 0.265830    Objective Loss 0.265830                                        LR 0.000013    Time 0.021461    
2023-01-06 17:06:41,163 - Epoch: [133][  200/  246]    Overall Loss 0.265670    Objective Loss 0.265670                                        LR 0.000013    Time 0.021520    
2023-01-06 17:06:41,402 - Epoch: [133][  210/  246]    Overall Loss 0.265811    Objective Loss 0.265811                                        LR 0.000013    Time 0.021627    
2023-01-06 17:06:41,695 - Epoch: [133][  220/  246]    Overall Loss 0.265505    Objective Loss 0.265505                                        LR 0.000013    Time 0.021974    
2023-01-06 17:06:41,875 - Epoch: [133][  230/  246]    Overall Loss 0.265047    Objective Loss 0.265047                                        LR 0.000013    Time 0.021801    
2023-01-06 17:06:42,078 - Epoch: [133][  240/  246]    Overall Loss 0.265306    Objective Loss 0.265306                                        LR 0.000013    Time 0.021737    
2023-01-06 17:06:42,175 - Epoch: [133][  246/  246]    Overall Loss 0.264996    Objective Loss 0.264996    Top1 91.387560    LR 0.000013    Time 0.021599    
2023-01-06 17:06:42,326 - --- validate (epoch=133)-----------
2023-01-06 17:06:42,326 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:42,778 - Epoch: [133][   10/   28]    Loss 0.293635    Top1 89.531250    
2023-01-06 17:06:42,892 - Epoch: [133][   20/   28]    Loss 0.294364    Top1 89.550781    
2023-01-06 17:06:42,957 - Epoch: [133][   28/   28]    Loss 0.298195    Top1 89.350129    
2023-01-06 17:06:43,090 - ==> Top1: 89.350    Loss: 0.298

2023-01-06 17:06:43,090 - ==> Confusion:
[[ 208   15  216]
 [  11  254  337]
 [  69   96 5780]]

2023-01-06 17:06:43,091 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:43,091 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:43,097 - 

2023-01-06 17:06:43,097 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:43,778 - Epoch: [134][   10/  246]    Overall Loss 0.290608    Objective Loss 0.290608                                        LR 0.000013    Time 0.067954    
2023-01-06 17:06:43,939 - Epoch: [134][   20/  246]    Overall Loss 0.284362    Objective Loss 0.284362                                        LR 0.000013    Time 0.042011    
2023-01-06 17:06:44,110 - Epoch: [134][   30/  246]    Overall Loss 0.284282    Objective Loss 0.284282                                        LR 0.000013    Time 0.033691    
2023-01-06 17:06:44,278 - Epoch: [134][   40/  246]    Overall Loss 0.286500    Objective Loss 0.286500                                        LR 0.000013    Time 0.029472    
2023-01-06 17:06:44,445 - Epoch: [134][   50/  246]    Overall Loss 0.283162    Objective Loss 0.283162                                        LR 0.000013    Time 0.026917    
2023-01-06 17:06:44,625 - Epoch: [134][   60/  246]    Overall Loss 0.281738    Objective Loss 0.281738                                        LR 0.000013    Time 0.025422    
2023-01-06 17:06:44,818 - Epoch: [134][   70/  246]    Overall Loss 0.280652    Objective Loss 0.280652                                        LR 0.000013    Time 0.024541    
2023-01-06 17:06:45,007 - Epoch: [134][   80/  246]    Overall Loss 0.278978    Objective Loss 0.278978                                        LR 0.000013    Time 0.023836    
2023-01-06 17:06:45,190 - Epoch: [134][   90/  246]    Overall Loss 0.276893    Objective Loss 0.276893                                        LR 0.000013    Time 0.023216    
2023-01-06 17:06:45,374 - Epoch: [134][  100/  246]    Overall Loss 0.274497    Objective Loss 0.274497                                        LR 0.000013    Time 0.022728    
2023-01-06 17:06:45,552 - Epoch: [134][  110/  246]    Overall Loss 0.272563    Objective Loss 0.272563                                        LR 0.000013    Time 0.022274    
2023-01-06 17:06:45,733 - Epoch: [134][  120/  246]    Overall Loss 0.271567    Objective Loss 0.271567                                        LR 0.000013    Time 0.021923    
2023-01-06 17:06:45,919 - Epoch: [134][  130/  246]    Overall Loss 0.270135    Objective Loss 0.270135                                        LR 0.000013    Time 0.021671    
2023-01-06 17:06:46,104 - Epoch: [134][  140/  246]    Overall Loss 0.269909    Objective Loss 0.269909                                        LR 0.000013    Time 0.021439    
2023-01-06 17:06:46,288 - Epoch: [134][  150/  246]    Overall Loss 0.269512    Objective Loss 0.269512                                        LR 0.000013    Time 0.021237    
2023-01-06 17:06:46,472 - Epoch: [134][  160/  246]    Overall Loss 0.268888    Objective Loss 0.268888                                        LR 0.000013    Time 0.021056    
2023-01-06 17:06:46,659 - Epoch: [134][  170/  246]    Overall Loss 0.268558    Objective Loss 0.268558                                        LR 0.000013    Time 0.020914    
2023-01-06 17:06:46,842 - Epoch: [134][  180/  246]    Overall Loss 0.267463    Objective Loss 0.267463                                        LR 0.000013    Time 0.020767    
2023-01-06 17:06:47,020 - Epoch: [134][  190/  246]    Overall Loss 0.267568    Objective Loss 0.267568                                        LR 0.000013    Time 0.020608    
2023-01-06 17:06:47,206 - Epoch: [134][  200/  246]    Overall Loss 0.268780    Objective Loss 0.268780                                        LR 0.000013    Time 0.020508    
2023-01-06 17:06:47,390 - Epoch: [134][  210/  246]    Overall Loss 0.268833    Objective Loss 0.268833                                        LR 0.000013    Time 0.020407    
2023-01-06 17:06:47,573 - Epoch: [134][  220/  246]    Overall Loss 0.267567    Objective Loss 0.267567                                        LR 0.000013    Time 0.020309    
2023-01-06 17:06:47,754 - Epoch: [134][  230/  246]    Overall Loss 0.266869    Objective Loss 0.266869                                        LR 0.000013    Time 0.020211    
2023-01-06 17:06:47,954 - Epoch: [134][  240/  246]    Overall Loss 0.267531    Objective Loss 0.267531                                        LR 0.000013    Time 0.020199    
2023-01-06 17:06:48,045 - Epoch: [134][  246/  246]    Overall Loss 0.267553    Objective Loss 0.267553    Top1 88.995215    LR 0.000013    Time 0.020076    
2023-01-06 17:06:48,198 - --- validate (epoch=134)-----------
2023-01-06 17:06:48,198 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:48,665 - Epoch: [134][   10/   28]    Loss 0.268063    Top1 90.195312    
2023-01-06 17:06:48,778 - Epoch: [134][   20/   28]    Loss 0.270164    Top1 89.980469    
2023-01-06 17:06:48,844 - Epoch: [134][   28/   28]    Loss 0.274785    Top1 89.793873    
2023-01-06 17:06:48,971 - ==> Top1: 89.794    Loss: 0.275

2023-01-06 17:06:48,971 - ==> Confusion:
[[ 221   12  206]
 [  17  227  358]
 [  63   57 5825]]

2023-01-06 17:06:48,973 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:48,973 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:48,979 - 

2023-01-06 17:06:48,979 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:49,677 - Epoch: [135][   10/  246]    Overall Loss 0.264191    Objective Loss 0.264191                                        LR 0.000013    Time 0.069781    
2023-01-06 17:06:49,847 - Epoch: [135][   20/  246]    Overall Loss 0.262219    Objective Loss 0.262219                                        LR 0.000013    Time 0.043320    
2023-01-06 17:06:50,024 - Epoch: [135][   30/  246]    Overall Loss 0.257960    Objective Loss 0.257960                                        LR 0.000013    Time 0.034793    
2023-01-06 17:06:50,194 - Epoch: [135][   40/  246]    Overall Loss 0.258588    Objective Loss 0.258588                                        LR 0.000013    Time 0.030335    
2023-01-06 17:06:50,371 - Epoch: [135][   50/  246]    Overall Loss 0.259993    Objective Loss 0.259993                                        LR 0.000013    Time 0.027786    
2023-01-06 17:06:50,556 - Epoch: [135][   60/  246]    Overall Loss 0.260363    Objective Loss 0.260363                                        LR 0.000013    Time 0.026232    
2023-01-06 17:06:50,726 - Epoch: [135][   70/  246]    Overall Loss 0.258691    Objective Loss 0.258691                                        LR 0.000013    Time 0.024916    
2023-01-06 17:06:50,898 - Epoch: [135][   80/  246]    Overall Loss 0.261722    Objective Loss 0.261722                                        LR 0.000013    Time 0.023945    
2023-01-06 17:06:51,067 - Epoch: [135][   90/  246]    Overall Loss 0.259248    Objective Loss 0.259248                                        LR 0.000013    Time 0.023155    
2023-01-06 17:06:51,233 - Epoch: [135][  100/  246]    Overall Loss 0.260590    Objective Loss 0.260590                                        LR 0.000013    Time 0.022502    
2023-01-06 17:06:51,408 - Epoch: [135][  110/  246]    Overall Loss 0.261671    Objective Loss 0.261671                                        LR 0.000013    Time 0.022039    
2023-01-06 17:06:51,573 - Epoch: [135][  120/  246]    Overall Loss 0.262313    Objective Loss 0.262313                                        LR 0.000013    Time 0.021579    
2023-01-06 17:06:51,753 - Epoch: [135][  130/  246]    Overall Loss 0.263158    Objective Loss 0.263158                                        LR 0.000013    Time 0.021300    
2023-01-06 17:06:51,941 - Epoch: [135][  140/  246]    Overall Loss 0.261898    Objective Loss 0.261898                                        LR 0.000013    Time 0.021120    
2023-01-06 17:06:52,141 - Epoch: [135][  150/  246]    Overall Loss 0.262629    Objective Loss 0.262629                                        LR 0.000013    Time 0.021043    
2023-01-06 17:06:52,358 - Epoch: [135][  160/  246]    Overall Loss 0.263966    Objective Loss 0.263966                                        LR 0.000013    Time 0.021081    
2023-01-06 17:06:52,587 - Epoch: [135][  170/  246]    Overall Loss 0.264113    Objective Loss 0.264113                                        LR 0.000013    Time 0.021175    
2023-01-06 17:06:52,759 - Epoch: [135][  180/  246]    Overall Loss 0.264907    Objective Loss 0.264907                                        LR 0.000013    Time 0.020949    
2023-01-06 17:06:52,920 - Epoch: [135][  190/  246]    Overall Loss 0.263807    Objective Loss 0.263807                                        LR 0.000013    Time 0.020693    
2023-01-06 17:06:53,080 - Epoch: [135][  200/  246]    Overall Loss 0.263595    Objective Loss 0.263595                                        LR 0.000013    Time 0.020459    
2023-01-06 17:06:53,241 - Epoch: [135][  210/  246]    Overall Loss 0.263567    Objective Loss 0.263567                                        LR 0.000013    Time 0.020249    
2023-01-06 17:06:53,406 - Epoch: [135][  220/  246]    Overall Loss 0.263841    Objective Loss 0.263841                                        LR 0.000013    Time 0.020076    
2023-01-06 17:06:53,574 - Epoch: [135][  230/  246]    Overall Loss 0.264546    Objective Loss 0.264546                                        LR 0.000013    Time 0.019933    
2023-01-06 17:06:53,755 - Epoch: [135][  240/  246]    Overall Loss 0.265268    Objective Loss 0.265268                                        LR 0.000013    Time 0.019857    
2023-01-06 17:06:53,838 - Epoch: [135][  246/  246]    Overall Loss 0.265091    Objective Loss 0.265091    Top1 89.952153    LR 0.000013    Time 0.019707    
2023-01-06 17:06:53,976 - --- validate (epoch=135)-----------
2023-01-06 17:06:53,977 - 6986 samples (256 per mini-batch)
2023-01-06 17:06:54,427 - Epoch: [135][   10/   28]    Loss 0.270937    Top1 89.921875    
2023-01-06 17:06:54,536 - Epoch: [135][   20/   28]    Loss 0.278502    Top1 89.746094    
2023-01-06 17:06:54,602 - Epoch: [135][   28/   28]    Loss 0.272235    Top1 89.879760    
2023-01-06 17:06:54,734 - ==> Top1: 89.880    Loss: 0.272

2023-01-06 17:06:54,734 - ==> Confusion:
[[ 216    9  214]
 [  18  231  353]
 [  55   58 5832]]

2023-01-06 17:06:54,736 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:06:54,736 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:06:54,742 - 

2023-01-06 17:06:54,742 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:06:55,328 - Epoch: [136][   10/  246]    Overall Loss 0.277813    Objective Loss 0.277813                                        LR 0.000013    Time 0.058574    
2023-01-06 17:06:55,530 - Epoch: [136][   20/  246]    Overall Loss 0.264421    Objective Loss 0.264421                                        LR 0.000013    Time 0.039352    
2023-01-06 17:06:55,728 - Epoch: [136][   30/  246]    Overall Loss 0.263160    Objective Loss 0.263160                                        LR 0.000013    Time 0.032815    
2023-01-06 17:06:55,924 - Epoch: [136][   40/  246]    Overall Loss 0.261768    Objective Loss 0.261768                                        LR 0.000013    Time 0.029502    
2023-01-06 17:06:56,099 - Epoch: [136][   50/  246]    Overall Loss 0.259788    Objective Loss 0.259788                                        LR 0.000013    Time 0.027095    
2023-01-06 17:06:56,280 - Epoch: [136][   60/  246]    Overall Loss 0.259430    Objective Loss 0.259430                                        LR 0.000013    Time 0.025597    
2023-01-06 17:06:56,475 - Epoch: [136][   70/  246]    Overall Loss 0.261182    Objective Loss 0.261182                                        LR 0.000013    Time 0.024712    
2023-01-06 17:06:56,683 - Epoch: [136][   80/  246]    Overall Loss 0.261322    Objective Loss 0.261322                                        LR 0.000013    Time 0.024228    
2023-01-06 17:06:56,882 - Epoch: [136][   90/  246]    Overall Loss 0.263738    Objective Loss 0.263738                                        LR 0.000013    Time 0.023740    
2023-01-06 17:06:57,073 - Epoch: [136][  100/  246]    Overall Loss 0.263319    Objective Loss 0.263319                                        LR 0.000013    Time 0.023268    
2023-01-06 17:06:57,266 - Epoch: [136][  110/  246]    Overall Loss 0.263407    Objective Loss 0.263407                                        LR 0.000013    Time 0.022907    
2023-01-06 17:06:57,459 - Epoch: [136][  120/  246]    Overall Loss 0.264444    Objective Loss 0.264444                                        LR 0.000013    Time 0.022606    
2023-01-06 17:06:57,652 - Epoch: [136][  130/  246]    Overall Loss 0.265050    Objective Loss 0.265050                                        LR 0.000013    Time 0.022342    
2023-01-06 17:06:57,831 - Epoch: [136][  140/  246]    Overall Loss 0.265381    Objective Loss 0.265381                                        LR 0.000013    Time 0.022028    
2023-01-06 17:06:58,018 - Epoch: [136][  150/  246]    Overall Loss 0.264420    Objective Loss 0.264420                                        LR 0.000013    Time 0.021802    
2023-01-06 17:06:58,211 - Epoch: [136][  160/  246]    Overall Loss 0.265512    Objective Loss 0.265512                                        LR 0.000013    Time 0.021643    
2023-01-06 17:06:58,403 - Epoch: [136][  170/  246]    Overall Loss 0.266240    Objective Loss 0.266240                                        LR 0.000013    Time 0.021498    
2023-01-06 17:06:58,595 - Epoch: [136][  180/  246]    Overall Loss 0.266959    Objective Loss 0.266959                                        LR 0.000013    Time 0.021370    
2023-01-06 17:06:58,788 - Epoch: [136][  190/  246]    Overall Loss 0.267844    Objective Loss 0.267844                                        LR 0.000013    Time 0.021255    
2023-01-06 17:06:58,980 - Epoch: [136][  200/  246]    Overall Loss 0.267193    Objective Loss 0.267193                                        LR 0.000013    Time 0.021149    
2023-01-06 17:06:59,171 - Epoch: [136][  210/  246]    Overall Loss 0.266796    Objective Loss 0.266796                                        LR 0.000013    Time 0.021051    
2023-01-06 17:06:59,363 - Epoch: [136][  220/  246]    Overall Loss 0.266538    Objective Loss 0.266538                                        LR 0.000013    Time 0.020966    
2023-01-06 17:06:59,555 - Epoch: [136][  230/  246]    Overall Loss 0.266112    Objective Loss 0.266112                                        LR 0.000013    Time 0.020887    
2023-01-06 17:06:59,761 - Epoch: [136][  240/  246]    Overall Loss 0.265983    Objective Loss 0.265983                                        LR 0.000013    Time 0.020874    
2023-01-06 17:06:59,856 - Epoch: [136][  246/  246]    Overall Loss 0.266202    Objective Loss 0.266202    Top1 89.952153    LR 0.000013    Time 0.020749    
2023-01-06 17:06:59,996 - --- validate (epoch=136)-----------
2023-01-06 17:06:59,996 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:00,450 - Epoch: [136][   10/   28]    Loss 0.281529    Top1 89.648438    
2023-01-06 17:07:00,560 - Epoch: [136][   20/   28]    Loss 0.283619    Top1 89.531250    
2023-01-06 17:07:00,626 - Epoch: [136][   28/   28]    Loss 0.278369    Top1 89.965646    
2023-01-06 17:07:00,783 - ==> Top1: 89.966    Loss: 0.278

2023-01-06 17:07:00,784 - ==> Confusion:
[[ 239   14  186]
 [  19  234  349]
 [  73   60 5812]]

2023-01-06 17:07:00,785 - ==> Best [Top1: 90.052   Sparsity:0.00   Params: 155168 on epoch: 116]
2023-01-06 17:07:00,785 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:00,791 - 

2023-01-06 17:07:00,791 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:01,520 - Epoch: [137][   10/  246]    Overall Loss 0.267620    Objective Loss 0.267620                                        LR 0.000013    Time 0.072788    
2023-01-06 17:07:01,711 - Epoch: [137][   20/  246]    Overall Loss 0.271144    Objective Loss 0.271144                                        LR 0.000013    Time 0.045929    
2023-01-06 17:07:01,906 - Epoch: [137][   30/  246]    Overall Loss 0.272415    Objective Loss 0.272415                                        LR 0.000013    Time 0.037110    
2023-01-06 17:07:02,096 - Epoch: [137][   40/  246]    Overall Loss 0.271471    Objective Loss 0.271471                                        LR 0.000013    Time 0.032590    
2023-01-06 17:07:02,285 - Epoch: [137][   50/  246]    Overall Loss 0.269707    Objective Loss 0.269707                                        LR 0.000013    Time 0.029843    
2023-01-06 17:07:02,481 - Epoch: [137][   60/  246]    Overall Loss 0.269474    Objective Loss 0.269474                                        LR 0.000013    Time 0.028117    
2023-01-06 17:07:02,676 - Epoch: [137][   70/  246]    Overall Loss 0.266682    Objective Loss 0.266682                                        LR 0.000013    Time 0.026894    
2023-01-06 17:07:02,873 - Epoch: [137][   80/  246]    Overall Loss 0.269982    Objective Loss 0.269982                                        LR 0.000013    Time 0.025978    
2023-01-06 17:07:03,070 - Epoch: [137][   90/  246]    Overall Loss 0.267272    Objective Loss 0.267272                                        LR 0.000013    Time 0.025280    
2023-01-06 17:07:03,266 - Epoch: [137][  100/  246]    Overall Loss 0.265027    Objective Loss 0.265027                                        LR 0.000013    Time 0.024713    
2023-01-06 17:07:03,470 - Epoch: [137][  110/  246]    Overall Loss 0.265968    Objective Loss 0.265968                                        LR 0.000013    Time 0.024302    
2023-01-06 17:07:03,668 - Epoch: [137][  120/  246]    Overall Loss 0.266891    Objective Loss 0.266891                                        LR 0.000013    Time 0.023920    
2023-01-06 17:07:03,859 - Epoch: [137][  130/  246]    Overall Loss 0.266029    Objective Loss 0.266029                                        LR 0.000013    Time 0.023548    
2023-01-06 17:07:04,049 - Epoch: [137][  140/  246]    Overall Loss 0.265914    Objective Loss 0.265914                                        LR 0.000013    Time 0.023225    
2023-01-06 17:07:04,236 - Epoch: [137][  150/  246]    Overall Loss 0.265909    Objective Loss 0.265909                                        LR 0.000013    Time 0.022915    
2023-01-06 17:07:04,421 - Epoch: [137][  160/  246]    Overall Loss 0.265339    Objective Loss 0.265339                                        LR 0.000013    Time 0.022640    
2023-01-06 17:07:04,605 - Epoch: [137][  170/  246]    Overall Loss 0.264824    Objective Loss 0.264824                                        LR 0.000013    Time 0.022390    
2023-01-06 17:07:04,789 - Epoch: [137][  180/  246]    Overall Loss 0.264219    Objective Loss 0.264219                                        LR 0.000013    Time 0.022164    
2023-01-06 17:07:04,976 - Epoch: [137][  190/  246]    Overall Loss 0.264200    Objective Loss 0.264200                                        LR 0.000013    Time 0.021980    
2023-01-06 17:07:05,156 - Epoch: [137][  200/  246]    Overall Loss 0.264490    Objective Loss 0.264490                                        LR 0.000013    Time 0.021779    
2023-01-06 17:07:05,323 - Epoch: [137][  210/  246]    Overall Loss 0.265186    Objective Loss 0.265186                                        LR 0.000013    Time 0.021537    
2023-01-06 17:07:05,526 - Epoch: [137][  220/  246]    Overall Loss 0.264310    Objective Loss 0.264310                                        LR 0.000013    Time 0.021476    
2023-01-06 17:07:05,738 - Epoch: [137][  230/  246]    Overall Loss 0.265277    Objective Loss 0.265277                                        LR 0.000013    Time 0.021465    
2023-01-06 17:07:05,982 - Epoch: [137][  240/  246]    Overall Loss 0.265456    Objective Loss 0.265456                                        LR 0.000013    Time 0.021586    
2023-01-06 17:07:06,095 - Epoch: [137][  246/  246]    Overall Loss 0.264681    Objective Loss 0.264681    Top1 90.669856    LR 0.000013    Time 0.021516    
2023-01-06 17:07:06,249 - --- validate (epoch=137)-----------
2023-01-06 17:07:06,249 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:06,703 - Epoch: [137][   10/   28]    Loss 0.269986    Top1 90.195312    
2023-01-06 17:07:06,812 - Epoch: [137][   20/   28]    Loss 0.279648    Top1 89.921875    
2023-01-06 17:07:06,877 - Epoch: [137][   28/   28]    Loss 0.277200    Top1 90.108789    
2023-01-06 17:07:07,034 - ==> Top1: 90.109    Loss: 0.277

2023-01-06 17:07:07,034 - ==> Confusion:
[[ 229   16  194]
 [  14  250  338]
 [  56   73 5816]]

2023-01-06 17:07:07,035 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:07,035 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:07,042 - 

2023-01-06 17:07:07,043 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:07,619 - Epoch: [138][   10/  246]    Overall Loss 0.267052    Objective Loss 0.267052                                        LR 0.000013    Time 0.057571    
2023-01-06 17:07:07,809 - Epoch: [138][   20/  246]    Overall Loss 0.272258    Objective Loss 0.272258                                        LR 0.000013    Time 0.038248    
2023-01-06 17:07:08,001 - Epoch: [138][   30/  246]    Overall Loss 0.273215    Objective Loss 0.273215                                        LR 0.000013    Time 0.031895    
2023-01-06 17:07:08,190 - Epoch: [138][   40/  246]    Overall Loss 0.267560    Objective Loss 0.267560                                        LR 0.000013    Time 0.028634    
2023-01-06 17:07:08,383 - Epoch: [138][   50/  246]    Overall Loss 0.270570    Objective Loss 0.270570                                        LR 0.000013    Time 0.026757    
2023-01-06 17:07:08,574 - Epoch: [138][   60/  246]    Overall Loss 0.268703    Objective Loss 0.268703                                        LR 0.000013    Time 0.025472    
2023-01-06 17:07:08,765 - Epoch: [138][   70/  246]    Overall Loss 0.267500    Objective Loss 0.267500                                        LR 0.000013    Time 0.024562    
2023-01-06 17:07:08,954 - Epoch: [138][   80/  246]    Overall Loss 0.269203    Objective Loss 0.269203                                        LR 0.000013    Time 0.023847    
2023-01-06 17:07:09,144 - Epoch: [138][   90/  246]    Overall Loss 0.269616    Objective Loss 0.269616                                        LR 0.000013    Time 0.023304    
2023-01-06 17:07:09,334 - Epoch: [138][  100/  246]    Overall Loss 0.270292    Objective Loss 0.270292                                        LR 0.000013    Time 0.022869    
2023-01-06 17:07:09,522 - Epoch: [138][  110/  246]    Overall Loss 0.268673    Objective Loss 0.268673                                        LR 0.000013    Time 0.022501    
2023-01-06 17:07:09,711 - Epoch: [138][  120/  246]    Overall Loss 0.268213    Objective Loss 0.268213                                        LR 0.000013    Time 0.022196    
2023-01-06 17:07:09,901 - Epoch: [138][  130/  246]    Overall Loss 0.266244    Objective Loss 0.266244                                        LR 0.000013    Time 0.021945    
2023-01-06 17:07:10,091 - Epoch: [138][  140/  246]    Overall Loss 0.265931    Objective Loss 0.265931                                        LR 0.000013    Time 0.021732    
2023-01-06 17:07:10,279 - Epoch: [138][  150/  246]    Overall Loss 0.266015    Objective Loss 0.266015                                        LR 0.000013    Time 0.021537    
2023-01-06 17:07:10,470 - Epoch: [138][  160/  246]    Overall Loss 0.266321    Objective Loss 0.266321                                        LR 0.000013    Time 0.021378    
2023-01-06 17:07:10,658 - Epoch: [138][  170/  246]    Overall Loss 0.265712    Objective Loss 0.265712                                        LR 0.000013    Time 0.021229    
2023-01-06 17:07:10,852 - Epoch: [138][  180/  246]    Overall Loss 0.265024    Objective Loss 0.265024                                        LR 0.000013    Time 0.021125    
2023-01-06 17:07:11,047 - Epoch: [138][  190/  246]    Overall Loss 0.264807    Objective Loss 0.264807                                        LR 0.000013    Time 0.021035    
2023-01-06 17:07:11,244 - Epoch: [138][  200/  246]    Overall Loss 0.264626    Objective Loss 0.264626                                        LR 0.000013    Time 0.020969    
2023-01-06 17:07:11,439 - Epoch: [138][  210/  246]    Overall Loss 0.264445    Objective Loss 0.264445                                        LR 0.000013    Time 0.020898    
2023-01-06 17:07:11,635 - Epoch: [138][  220/  246]    Overall Loss 0.264533    Objective Loss 0.264533                                        LR 0.000013    Time 0.020834    
2023-01-06 17:07:11,829 - Epoch: [138][  230/  246]    Overall Loss 0.264548    Objective Loss 0.264548                                        LR 0.000013    Time 0.020769    
2023-01-06 17:07:12,037 - Epoch: [138][  240/  246]    Overall Loss 0.263679    Objective Loss 0.263679                                        LR 0.000013    Time 0.020771    
2023-01-06 17:07:12,132 - Epoch: [138][  246/  246]    Overall Loss 0.263309    Objective Loss 0.263309    Top1 91.387560    LR 0.000013    Time 0.020650    
2023-01-06 17:07:12,278 - --- validate (epoch=138)-----------
2023-01-06 17:07:12,278 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:12,733 - Epoch: [138][   10/   28]    Loss 0.277197    Top1 89.804688    
2023-01-06 17:07:12,850 - Epoch: [138][   20/   28]    Loss 0.281658    Top1 89.804688    
2023-01-06 17:07:12,915 - Epoch: [138][   28/   28]    Loss 0.273459    Top1 89.951331    
2023-01-06 17:07:13,065 - ==> Top1: 89.951    Loss: 0.273

2023-01-06 17:07:13,066 - ==> Confusion:
[[ 239   12  188]
 [  18  251  333]
 [  83   68 5794]]

2023-01-06 17:07:13,067 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:13,067 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:13,073 - 

2023-01-06 17:07:13,073 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:13,788 - Epoch: [139][   10/  246]    Overall Loss 0.261768    Objective Loss 0.261768                                        LR 0.000013    Time 0.071389    
2023-01-06 17:07:13,999 - Epoch: [139][   20/  246]    Overall Loss 0.265081    Objective Loss 0.265081                                        LR 0.000013    Time 0.046244    
2023-01-06 17:07:14,191 - Epoch: [139][   30/  246]    Overall Loss 0.263648    Objective Loss 0.263648                                        LR 0.000013    Time 0.037208    
2023-01-06 17:07:14,370 - Epoch: [139][   40/  246]    Overall Loss 0.257457    Objective Loss 0.257457                                        LR 0.000013    Time 0.032369    
2023-01-06 17:07:14,553 - Epoch: [139][   50/  246]    Overall Loss 0.261678    Objective Loss 0.261678                                        LR 0.000013    Time 0.029561    
2023-01-06 17:07:14,734 - Epoch: [139][   60/  246]    Overall Loss 0.259907    Objective Loss 0.259907                                        LR 0.000013    Time 0.027634    
2023-01-06 17:07:14,920 - Epoch: [139][   70/  246]    Overall Loss 0.261293    Objective Loss 0.261293                                        LR 0.000013    Time 0.026339    
2023-01-06 17:07:15,102 - Epoch: [139][   80/  246]    Overall Loss 0.263234    Objective Loss 0.263234                                        LR 0.000013    Time 0.025317    
2023-01-06 17:07:15,290 - Epoch: [139][   90/  246]    Overall Loss 0.261946    Objective Loss 0.261946                                        LR 0.000013    Time 0.024594    
2023-01-06 17:07:15,480 - Epoch: [139][  100/  246]    Overall Loss 0.261124    Objective Loss 0.261124                                        LR 0.000013    Time 0.024028    
2023-01-06 17:07:15,667 - Epoch: [139][  110/  246]    Overall Loss 0.261654    Objective Loss 0.261654                                        LR 0.000013    Time 0.023544    
2023-01-06 17:07:15,880 - Epoch: [139][  120/  246]    Overall Loss 0.260946    Objective Loss 0.260946                                        LR 0.000013    Time 0.023353    
2023-01-06 17:07:16,097 - Epoch: [139][  130/  246]    Overall Loss 0.261656    Objective Loss 0.261656                                        LR 0.000013    Time 0.023218    
2023-01-06 17:07:16,316 - Epoch: [139][  140/  246]    Overall Loss 0.261374    Objective Loss 0.261374                                        LR 0.000013    Time 0.023128    
2023-01-06 17:07:16,535 - Epoch: [139][  150/  246]    Overall Loss 0.260686    Objective Loss 0.260686                                        LR 0.000013    Time 0.023039    
2023-01-06 17:07:16,752 - Epoch: [139][  160/  246]    Overall Loss 0.260811    Objective Loss 0.260811                                        LR 0.000013    Time 0.022955    
2023-01-06 17:07:16,969 - Epoch: [139][  170/  246]    Overall Loss 0.260959    Objective Loss 0.260959                                        LR 0.000013    Time 0.022875    
2023-01-06 17:07:17,150 - Epoch: [139][  180/  246]    Overall Loss 0.262123    Objective Loss 0.262123                                        LR 0.000013    Time 0.022610    
2023-01-06 17:07:17,312 - Epoch: [139][  190/  246]    Overall Loss 0.262733    Objective Loss 0.262733                                        LR 0.000013    Time 0.022269    
2023-01-06 17:07:17,475 - Epoch: [139][  200/  246]    Overall Loss 0.262938    Objective Loss 0.262938                                        LR 0.000013    Time 0.021971    
2023-01-06 17:07:17,638 - Epoch: [139][  210/  246]    Overall Loss 0.263323    Objective Loss 0.263323                                        LR 0.000013    Time 0.021701    
2023-01-06 17:07:17,801 - Epoch: [139][  220/  246]    Overall Loss 0.263363    Objective Loss 0.263363                                        LR 0.000013    Time 0.021453    
2023-01-06 17:07:17,962 - Epoch: [139][  230/  246]    Overall Loss 0.263688    Objective Loss 0.263688                                        LR 0.000013    Time 0.021219    
2023-01-06 17:07:18,144 - Epoch: [139][  240/  246]    Overall Loss 0.263451    Objective Loss 0.263451                                        LR 0.000013    Time 0.021089    
2023-01-06 17:07:18,231 - Epoch: [139][  246/  246]    Overall Loss 0.263018    Objective Loss 0.263018    Top1 89.952153    LR 0.000013    Time 0.020929    
2023-01-06 17:07:18,373 - --- validate (epoch=139)-----------
2023-01-06 17:07:18,373 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:18,818 - Epoch: [139][   10/   28]    Loss 0.271016    Top1 90.273438    
2023-01-06 17:07:18,930 - Epoch: [139][   20/   28]    Loss 0.291683    Top1 89.667969    
2023-01-06 17:07:18,998 - Epoch: [139][   28/   28]    Loss 0.279854    Top1 89.937017    
2023-01-06 17:07:19,158 - ==> Top1: 89.937    Loss: 0.280

2023-01-06 17:07:19,159 - ==> Confusion:
[[ 219   15  205]
 [  13  267  322]
 [  56   92 5797]]

2023-01-06 17:07:19,160 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:19,160 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:19,166 - 

2023-01-06 17:07:19,166 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:19,738 - Epoch: [140][   10/  246]    Overall Loss 0.259699    Objective Loss 0.259699                                        LR 0.000008    Time 0.057138    
2023-01-06 17:07:19,938 - Epoch: [140][   20/  246]    Overall Loss 0.263104    Objective Loss 0.263104                                        LR 0.000008    Time 0.038559    
2023-01-06 17:07:20,120 - Epoch: [140][   30/  246]    Overall Loss 0.257720    Objective Loss 0.257720                                        LR 0.000008    Time 0.031749    
2023-01-06 17:07:20,311 - Epoch: [140][   40/  246]    Overall Loss 0.262090    Objective Loss 0.262090                                        LR 0.000008    Time 0.028577    
2023-01-06 17:07:20,503 - Epoch: [140][   50/  246]    Overall Loss 0.258005    Objective Loss 0.258005                                        LR 0.000008    Time 0.026684    
2023-01-06 17:07:20,695 - Epoch: [140][   60/  246]    Overall Loss 0.261078    Objective Loss 0.261078                                        LR 0.000008    Time 0.025434    
2023-01-06 17:07:20,880 - Epoch: [140][   70/  246]    Overall Loss 0.260347    Objective Loss 0.260347                                        LR 0.000008    Time 0.024439    
2023-01-06 17:07:21,071 - Epoch: [140][   80/  246]    Overall Loss 0.260259    Objective Loss 0.260259                                        LR 0.000008    Time 0.023765    
2023-01-06 17:07:21,266 - Epoch: [140][   90/  246]    Overall Loss 0.260743    Objective Loss 0.260743                                        LR 0.000008    Time 0.023294    
2023-01-06 17:07:21,462 - Epoch: [140][  100/  246]    Overall Loss 0.262410    Objective Loss 0.262410                                        LR 0.000008    Time 0.022916    
2023-01-06 17:07:21,647 - Epoch: [140][  110/  246]    Overall Loss 0.263121    Objective Loss 0.263121                                        LR 0.000008    Time 0.022510    
2023-01-06 17:07:21,829 - Epoch: [140][  120/  246]    Overall Loss 0.263319    Objective Loss 0.263319                                        LR 0.000008    Time 0.022148    
2023-01-06 17:07:21,994 - Epoch: [140][  130/  246]    Overall Loss 0.264055    Objective Loss 0.264055                                        LR 0.000008    Time 0.021712    
2023-01-06 17:07:22,186 - Epoch: [140][  140/  246]    Overall Loss 0.264552    Objective Loss 0.264552                                        LR 0.000008    Time 0.021529    
2023-01-06 17:07:22,381 - Epoch: [140][  150/  246]    Overall Loss 0.264389    Objective Loss 0.264389                                        LR 0.000008    Time 0.021393    
2023-01-06 17:07:22,575 - Epoch: [140][  160/  246]    Overall Loss 0.264451    Objective Loss 0.264451                                        LR 0.000008    Time 0.021266    
2023-01-06 17:07:22,771 - Epoch: [140][  170/  246]    Overall Loss 0.263867    Objective Loss 0.263867                                        LR 0.000008    Time 0.021167    
2023-01-06 17:07:22,948 - Epoch: [140][  180/  246]    Overall Loss 0.263818    Objective Loss 0.263818                                        LR 0.000008    Time 0.020974    
2023-01-06 17:07:23,118 - Epoch: [140][  190/  246]    Overall Loss 0.262888    Objective Loss 0.262888                                        LR 0.000008    Time 0.020759    
2023-01-06 17:07:23,278 - Epoch: [140][  200/  246]    Overall Loss 0.262199    Objective Loss 0.262199                                        LR 0.000008    Time 0.020520    
2023-01-06 17:07:23,464 - Epoch: [140][  210/  246]    Overall Loss 0.262548    Objective Loss 0.262548                                        LR 0.000008    Time 0.020428    
2023-01-06 17:07:23,658 - Epoch: [140][  220/  246]    Overall Loss 0.262406    Objective Loss 0.262406                                        LR 0.000008    Time 0.020379    
2023-01-06 17:07:23,853 - Epoch: [140][  230/  246]    Overall Loss 0.262694    Objective Loss 0.262694                                        LR 0.000008    Time 0.020338    
2023-01-06 17:07:24,040 - Epoch: [140][  240/  246]    Overall Loss 0.263109    Objective Loss 0.263109                                        LR 0.000008    Time 0.020271    
2023-01-06 17:07:24,128 - Epoch: [140][  246/  246]    Overall Loss 0.262838    Objective Loss 0.262838    Top1 91.387560    LR 0.000008    Time 0.020132    
2023-01-06 17:07:24,299 - --- validate (epoch=140)-----------
2023-01-06 17:07:24,299 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:24,745 - Epoch: [140][   10/   28]    Loss 0.283203    Top1 89.296875    
2023-01-06 17:07:24,856 - Epoch: [140][   20/   28]    Loss 0.282487    Top1 89.550781    
2023-01-06 17:07:24,923 - Epoch: [140][   28/   28]    Loss 0.275269    Top1 89.822502    
2023-01-06 17:07:25,064 - ==> Top1: 89.823    Loss: 0.275

2023-01-06 17:07:25,064 - ==> Confusion:
[[ 226   10  203]
 [  17  232  353]
 [  72   56 5817]]

2023-01-06 17:07:25,065 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:25,065 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:25,071 - 

2023-01-06 17:07:25,071 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:25,783 - Epoch: [141][   10/  246]    Overall Loss 0.259287    Objective Loss 0.259287                                        LR 0.000008    Time 0.071123    
2023-01-06 17:07:25,974 - Epoch: [141][   20/  246]    Overall Loss 0.261255    Objective Loss 0.261255                                        LR 0.000008    Time 0.045055    
2023-01-06 17:07:26,166 - Epoch: [141][   30/  246]    Overall Loss 0.271253    Objective Loss 0.271253                                        LR 0.000008    Time 0.036446    
2023-01-06 17:07:26,357 - Epoch: [141][   40/  246]    Overall Loss 0.273923    Objective Loss 0.273923                                        LR 0.000008    Time 0.032092    
2023-01-06 17:07:26,548 - Epoch: [141][   50/  246]    Overall Loss 0.270344    Objective Loss 0.270344                                        LR 0.000008    Time 0.029480    
2023-01-06 17:07:26,739 - Epoch: [141][   60/  246]    Overall Loss 0.269858    Objective Loss 0.269858                                        LR 0.000008    Time 0.027740    
2023-01-06 17:07:26,930 - Epoch: [141][   70/  246]    Overall Loss 0.269405    Objective Loss 0.269405                                        LR 0.000008    Time 0.026503    
2023-01-06 17:07:27,122 - Epoch: [141][   80/  246]    Overall Loss 0.269017    Objective Loss 0.269017                                        LR 0.000008    Time 0.025594    
2023-01-06 17:07:27,314 - Epoch: [141][   90/  246]    Overall Loss 0.268701    Objective Loss 0.268701                                        LR 0.000008    Time 0.024878    
2023-01-06 17:07:27,507 - Epoch: [141][  100/  246]    Overall Loss 0.267629    Objective Loss 0.267629                                        LR 0.000008    Time 0.024312    
2023-01-06 17:07:27,698 - Epoch: [141][  110/  246]    Overall Loss 0.268019    Objective Loss 0.268019                                        LR 0.000008    Time 0.023837    
2023-01-06 17:07:27,890 - Epoch: [141][  120/  246]    Overall Loss 0.266734    Objective Loss 0.266734                                        LR 0.000008    Time 0.023450    
2023-01-06 17:07:28,082 - Epoch: [141][  130/  246]    Overall Loss 0.263469    Objective Loss 0.263469                                        LR 0.000008    Time 0.023118    
2023-01-06 17:07:28,274 - Epoch: [141][  140/  246]    Overall Loss 0.262926    Objective Loss 0.262926                                        LR 0.000008    Time 0.022834    
2023-01-06 17:07:28,466 - Epoch: [141][  150/  246]    Overall Loss 0.263257    Objective Loss 0.263257                                        LR 0.000008    Time 0.022593    
2023-01-06 17:07:28,657 - Epoch: [141][  160/  246]    Overall Loss 0.262352    Objective Loss 0.262352                                        LR 0.000008    Time 0.022371    
2023-01-06 17:07:28,848 - Epoch: [141][  170/  246]    Overall Loss 0.262708    Objective Loss 0.262708                                        LR 0.000008    Time 0.022178    
2023-01-06 17:07:29,026 - Epoch: [141][  180/  246]    Overall Loss 0.262549    Objective Loss 0.262549                                        LR 0.000008    Time 0.021931    
2023-01-06 17:07:29,200 - Epoch: [141][  190/  246]    Overall Loss 0.261920    Objective Loss 0.261920                                        LR 0.000008    Time 0.021691    
2023-01-06 17:07:29,380 - Epoch: [141][  200/  246]    Overall Loss 0.262398    Objective Loss 0.262398                                        LR 0.000008    Time 0.021505    
2023-01-06 17:07:29,573 - Epoch: [141][  210/  246]    Overall Loss 0.261588    Objective Loss 0.261588                                        LR 0.000008    Time 0.021396    
2023-01-06 17:07:29,764 - Epoch: [141][  220/  246]    Overall Loss 0.261638    Objective Loss 0.261638                                        LR 0.000008    Time 0.021294    
2023-01-06 17:07:29,956 - Epoch: [141][  230/  246]    Overall Loss 0.262421    Objective Loss 0.262421                                        LR 0.000008    Time 0.021198    
2023-01-06 17:07:30,159 - Epoch: [141][  240/  246]    Overall Loss 0.261747    Objective Loss 0.261747                                        LR 0.000008    Time 0.021160    
2023-01-06 17:07:30,252 - Epoch: [141][  246/  246]    Overall Loss 0.261465    Objective Loss 0.261465    Top1 94.019139    LR 0.000008    Time 0.021022    
2023-01-06 17:07:30,381 - --- validate (epoch=141)-----------
2023-01-06 17:07:30,381 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:30,836 - Epoch: [141][   10/   28]    Loss 0.285258    Top1 89.375000    
2023-01-06 17:07:30,953 - Epoch: [141][   20/   28]    Loss 0.272042    Top1 90.019531    
2023-01-06 17:07:31,018 - Epoch: [141][   28/   28]    Loss 0.273013    Top1 89.908388    
2023-01-06 17:07:31,167 - ==> Top1: 89.908    Loss: 0.273

2023-01-06 17:07:31,167 - ==> Confusion:
[[ 228   11  200]
 [  16  252  334]
 [  70   74 5801]]

2023-01-06 17:07:31,169 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:31,169 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:31,175 - 

2023-01-06 17:07:31,175 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:31,895 - Epoch: [142][   10/  246]    Overall Loss 0.271124    Objective Loss 0.271124                                        LR 0.000008    Time 0.071906    
2023-01-06 17:07:32,092 - Epoch: [142][   20/  246]    Overall Loss 0.267923    Objective Loss 0.267923                                        LR 0.000008    Time 0.045802    
2023-01-06 17:07:32,289 - Epoch: [142][   30/  246]    Overall Loss 0.263571    Objective Loss 0.263571                                        LR 0.000008    Time 0.037095    
2023-01-06 17:07:32,483 - Epoch: [142][   40/  246]    Overall Loss 0.264550    Objective Loss 0.264550                                        LR 0.000008    Time 0.032656    
2023-01-06 17:07:32,686 - Epoch: [142][   50/  246]    Overall Loss 0.263755    Objective Loss 0.263755                                        LR 0.000008    Time 0.030186    
2023-01-06 17:07:32,880 - Epoch: [142][   60/  246]    Overall Loss 0.262472    Objective Loss 0.262472                                        LR 0.000008    Time 0.028372    
2023-01-06 17:07:33,073 - Epoch: [142][   70/  246]    Overall Loss 0.261390    Objective Loss 0.261390                                        LR 0.000008    Time 0.027079    
2023-01-06 17:07:33,277 - Epoch: [142][   80/  246]    Overall Loss 0.259111    Objective Loss 0.259111                                        LR 0.000008    Time 0.026233    
2023-01-06 17:07:33,480 - Epoch: [142][   90/  246]    Overall Loss 0.258934    Objective Loss 0.258934                                        LR 0.000008    Time 0.025568    
2023-01-06 17:07:33,683 - Epoch: [142][  100/  246]    Overall Loss 0.258350    Objective Loss 0.258350                                        LR 0.000008    Time 0.025036    
2023-01-06 17:07:33,887 - Epoch: [142][  110/  246]    Overall Loss 0.258687    Objective Loss 0.258687                                        LR 0.000008    Time 0.024614    
2023-01-06 17:07:34,086 - Epoch: [142][  120/  246]    Overall Loss 0.259373    Objective Loss 0.259373                                        LR 0.000008    Time 0.024216    
2023-01-06 17:07:34,287 - Epoch: [142][  130/  246]    Overall Loss 0.258581    Objective Loss 0.258581                                        LR 0.000008    Time 0.023896    
2023-01-06 17:07:34,477 - Epoch: [142][  140/  246]    Overall Loss 0.257975    Objective Loss 0.257975                                        LR 0.000008    Time 0.023547    
2023-01-06 17:07:34,678 - Epoch: [142][  150/  246]    Overall Loss 0.258000    Objective Loss 0.258000                                        LR 0.000008    Time 0.023315    
2023-01-06 17:07:34,878 - Epoch: [142][  160/  246]    Overall Loss 0.259985    Objective Loss 0.259985                                        LR 0.000008    Time 0.023101    
2023-01-06 17:07:35,074 - Epoch: [142][  170/  246]    Overall Loss 0.260093    Objective Loss 0.260093                                        LR 0.000008    Time 0.022897    
2023-01-06 17:07:35,270 - Epoch: [142][  180/  246]    Overall Loss 0.260427    Objective Loss 0.260427                                        LR 0.000008    Time 0.022711    
2023-01-06 17:07:35,466 - Epoch: [142][  190/  246]    Overall Loss 0.259794    Objective Loss 0.259794                                        LR 0.000008    Time 0.022546    
2023-01-06 17:07:35,660 - Epoch: [142][  200/  246]    Overall Loss 0.260265    Objective Loss 0.260265                                        LR 0.000008    Time 0.022388    
2023-01-06 17:07:35,858 - Epoch: [142][  210/  246]    Overall Loss 0.261216    Objective Loss 0.261216                                        LR 0.000008    Time 0.022259    
2023-01-06 17:07:36,052 - Epoch: [142][  220/  246]    Overall Loss 0.260740    Objective Loss 0.260740                                        LR 0.000008    Time 0.022129    
2023-01-06 17:07:36,247 - Epoch: [142][  230/  246]    Overall Loss 0.260375    Objective Loss 0.260375                                        LR 0.000008    Time 0.022013    
2023-01-06 17:07:36,452 - Epoch: [142][  240/  246]    Overall Loss 0.260844    Objective Loss 0.260844                                        LR 0.000008    Time 0.021947    
2023-01-06 17:07:36,546 - Epoch: [142][  246/  246]    Overall Loss 0.260788    Objective Loss 0.260788    Top1 89.952153    LR 0.000008    Time 0.021794    
2023-01-06 17:07:36,702 - --- validate (epoch=142)-----------
2023-01-06 17:07:36,703 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:37,154 - Epoch: [142][   10/   28]    Loss 0.267621    Top1 90.000000    
2023-01-06 17:07:37,268 - Epoch: [142][   20/   28]    Loss 0.272795    Top1 89.824219    
2023-01-06 17:07:37,334 - Epoch: [142][   28/   28]    Loss 0.268840    Top1 89.908388    
2023-01-06 17:07:37,458 - ==> Top1: 89.908    Loss: 0.269

2023-01-06 17:07:37,458 - ==> Confusion:
[[ 237   12  190]
 [  19  253  330]
 [  72   82 5791]]

2023-01-06 17:07:37,460 - ==> Best [Top1: 90.109   Sparsity:0.00   Params: 155168 on epoch: 137]
2023-01-06 17:07:37,460 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:37,466 - 

2023-01-06 17:07:37,466 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:38,035 - Epoch: [143][   10/  246]    Overall Loss 0.255511    Objective Loss 0.255511                                        LR 0.000008    Time 0.056868    
2023-01-06 17:07:38,234 - Epoch: [143][   20/  246]    Overall Loss 0.259623    Objective Loss 0.259623                                        LR 0.000008    Time 0.038330    
2023-01-06 17:07:38,430 - Epoch: [143][   30/  246]    Overall Loss 0.258409    Objective Loss 0.258409                                        LR 0.000008    Time 0.032093    
2023-01-06 17:07:38,626 - Epoch: [143][   40/  246]    Overall Loss 0.252258    Objective Loss 0.252258                                        LR 0.000008    Time 0.028955    
2023-01-06 17:07:38,821 - Epoch: [143][   50/  246]    Overall Loss 0.253673    Objective Loss 0.253673                                        LR 0.000008    Time 0.027065    
2023-01-06 17:07:39,016 - Epoch: [143][   60/  246]    Overall Loss 0.254095    Objective Loss 0.254095                                        LR 0.000008    Time 0.025790    
2023-01-06 17:07:39,207 - Epoch: [143][   70/  246]    Overall Loss 0.256837    Objective Loss 0.256837                                        LR 0.000008    Time 0.024827    
2023-01-06 17:07:39,399 - Epoch: [143][   80/  246]    Overall Loss 0.258595    Objective Loss 0.258595                                        LR 0.000008    Time 0.024119    
2023-01-06 17:07:39,617 - Epoch: [143][   90/  246]    Overall Loss 0.257010    Objective Loss 0.257010                                        LR 0.000008    Time 0.023858    
2023-01-06 17:07:39,840 - Epoch: [143][  100/  246]    Overall Loss 0.254764    Objective Loss 0.254764                                        LR 0.000008    Time 0.023699    
2023-01-06 17:07:40,061 - Epoch: [143][  110/  246]    Overall Loss 0.256919    Objective Loss 0.256919                                        LR 0.000008    Time 0.023549    
2023-01-06 17:07:40,295 - Epoch: [143][  120/  246]    Overall Loss 0.258415    Objective Loss 0.258415                                        LR 0.000008    Time 0.023531    
2023-01-06 17:07:40,522 - Epoch: [143][  130/  246]    Overall Loss 0.259448    Objective Loss 0.259448                                        LR 0.000008    Time 0.023463    
2023-01-06 17:07:40,727 - Epoch: [143][  140/  246]    Overall Loss 0.259820    Objective Loss 0.259820                                        LR 0.000008    Time 0.023250    
2023-01-06 17:07:40,938 - Epoch: [143][  150/  246]    Overall Loss 0.259845    Objective Loss 0.259845                                        LR 0.000008    Time 0.023106    
2023-01-06 17:07:41,142 - Epoch: [143][  160/  246]    Overall Loss 0.260803    Objective Loss 0.260803                                        LR 0.000008    Time 0.022924    
2023-01-06 17:07:41,343 - Epoch: [143][  170/  246]    Overall Loss 0.260538    Objective Loss 0.260538                                        LR 0.000008    Time 0.022757    
2023-01-06 17:07:41,539 - Epoch: [143][  180/  246]    Overall Loss 0.260379    Objective Loss 0.260379                                        LR 0.000008    Time 0.022570    
2023-01-06 17:07:41,737 - Epoch: [143][  190/  246]    Overall Loss 0.259813    Objective Loss 0.259813                                        LR 0.000008    Time 0.022419    
2023-01-06 17:07:41,939 - Epoch: [143][  200/  246]    Overall Loss 0.260351    Objective Loss 0.260351                                        LR 0.000008    Time 0.022306    
2023-01-06 17:07:42,138 - Epoch: [143][  210/  246]    Overall Loss 0.260926    Objective Loss 0.260926                                        LR 0.000008    Time 0.022191    
2023-01-06 17:07:42,343 - Epoch: [143][  220/  246]    Overall Loss 0.260734    Objective Loss 0.260734                                        LR 0.000008    Time 0.022114    
2023-01-06 17:07:42,575 - Epoch: [143][  230/  246]    Overall Loss 0.261862    Objective Loss 0.261862                                        LR 0.000008    Time 0.022157    
2023-01-06 17:07:42,832 - Epoch: [143][  240/  246]    Overall Loss 0.261516    Objective Loss 0.261516                                        LR 0.000008    Time 0.022302    
2023-01-06 17:07:42,943 - Epoch: [143][  246/  246]    Overall Loss 0.261328    Objective Loss 0.261328    Top1 89.952153    LR 0.000008    Time 0.022212    
2023-01-06 17:07:43,097 - --- validate (epoch=143)-----------
2023-01-06 17:07:43,098 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:43,543 - Epoch: [143][   10/   28]    Loss 0.269778    Top1 90.546875    
2023-01-06 17:07:43,656 - Epoch: [143][   20/   28]    Loss 0.279555    Top1 90.039062    
2023-01-06 17:07:43,723 - Epoch: [143][   28/   28]    Loss 0.276940    Top1 90.166046    
2023-01-06 17:07:43,853 - ==> Top1: 90.166    Loss: 0.277

2023-01-06 17:07:43,853 - ==> Confusion:
[[ 254   13  172]
 [  21  257  324]
 [  89   68 5788]]

2023-01-06 17:07:43,854 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 155168 on epoch: 143]
2023-01-06 17:07:43,855 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:43,862 - 

2023-01-06 17:07:43,862 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:44,573 - Epoch: [144][   10/  246]    Overall Loss 0.257990    Objective Loss 0.257990                                        LR 0.000008    Time 0.071072    
2023-01-06 17:07:44,769 - Epoch: [144][   20/  246]    Overall Loss 0.266979    Objective Loss 0.266979                                        LR 0.000008    Time 0.045311    
2023-01-06 17:07:44,959 - Epoch: [144][   30/  246]    Overall Loss 0.263919    Objective Loss 0.263919                                        LR 0.000008    Time 0.036538    
2023-01-06 17:07:45,157 - Epoch: [144][   40/  246]    Overall Loss 0.260900    Objective Loss 0.260900                                        LR 0.000008    Time 0.032328    
2023-01-06 17:07:45,372 - Epoch: [144][   50/  246]    Overall Loss 0.261965    Objective Loss 0.261965                                        LR 0.000008    Time 0.030149    
2023-01-06 17:07:45,586 - Epoch: [144][   60/  246]    Overall Loss 0.266300    Objective Loss 0.266300                                        LR 0.000008    Time 0.028697    
2023-01-06 17:07:45,802 - Epoch: [144][   70/  246]    Overall Loss 0.264594    Objective Loss 0.264594                                        LR 0.000008    Time 0.027668    
2023-01-06 17:07:46,018 - Epoch: [144][   80/  246]    Overall Loss 0.263403    Objective Loss 0.263403                                        LR 0.000008    Time 0.026905    
2023-01-06 17:07:46,260 - Epoch: [144][   90/  246]    Overall Loss 0.263574    Objective Loss 0.263574                                        LR 0.000008    Time 0.026601    
2023-01-06 17:07:46,512 - Epoch: [144][  100/  246]    Overall Loss 0.263002    Objective Loss 0.263002                                        LR 0.000008    Time 0.026441    
2023-01-06 17:07:46,763 - Epoch: [144][  110/  246]    Overall Loss 0.264977    Objective Loss 0.264977                                        LR 0.000008    Time 0.026299    
2023-01-06 17:07:46,999 - Epoch: [144][  120/  246]    Overall Loss 0.264910    Objective Loss 0.264910                                        LR 0.000008    Time 0.026059    
2023-01-06 17:07:47,242 - Epoch: [144][  130/  246]    Overall Loss 0.264094    Objective Loss 0.264094                                        LR 0.000008    Time 0.025919    
2023-01-06 17:07:47,482 - Epoch: [144][  140/  246]    Overall Loss 0.263366    Objective Loss 0.263366                                        LR 0.000008    Time 0.025777    
2023-01-06 17:07:47,707 - Epoch: [144][  150/  246]    Overall Loss 0.262991    Objective Loss 0.262991                                        LR 0.000008    Time 0.025558    
2023-01-06 17:07:47,932 - Epoch: [144][  160/  246]    Overall Loss 0.263373    Objective Loss 0.263373                                        LR 0.000008    Time 0.025357    
2023-01-06 17:07:48,098 - Epoch: [144][  170/  246]    Overall Loss 0.262492    Objective Loss 0.262492                                        LR 0.000008    Time 0.024837    
2023-01-06 17:07:48,262 - Epoch: [144][  180/  246]    Overall Loss 0.262669    Objective Loss 0.262669                                        LR 0.000008    Time 0.024367    
2023-01-06 17:07:48,426 - Epoch: [144][  190/  246]    Overall Loss 0.262455    Objective Loss 0.262455                                        LR 0.000008    Time 0.023945    
2023-01-06 17:07:48,590 - Epoch: [144][  200/  246]    Overall Loss 0.261900    Objective Loss 0.261900                                        LR 0.000008    Time 0.023567    
2023-01-06 17:07:48,754 - Epoch: [144][  210/  246]    Overall Loss 0.262276    Objective Loss 0.262276                                        LR 0.000008    Time 0.023225    
2023-01-06 17:07:48,918 - Epoch: [144][  220/  246]    Overall Loss 0.262192    Objective Loss 0.262192                                        LR 0.000008    Time 0.022913    
2023-01-06 17:07:49,082 - Epoch: [144][  230/  246]    Overall Loss 0.262124    Objective Loss 0.262124                                        LR 0.000008    Time 0.022630    
2023-01-06 17:07:49,268 - Epoch: [144][  240/  246]    Overall Loss 0.261828    Objective Loss 0.261828                                        LR 0.000008    Time 0.022458    
2023-01-06 17:07:49,365 - Epoch: [144][  246/  246]    Overall Loss 0.261376    Objective Loss 0.261376    Top1 91.387560    LR 0.000008    Time 0.022304    
2023-01-06 17:07:49,499 - --- validate (epoch=144)-----------
2023-01-06 17:07:49,499 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:49,945 - Epoch: [144][   10/   28]    Loss 0.257282    Top1 90.078125    
2023-01-06 17:07:50,056 - Epoch: [144][   20/   28]    Loss 0.263051    Top1 90.312500    
2023-01-06 17:07:50,123 - Epoch: [144][   28/   28]    Loss 0.270658    Top1 90.123103    
2023-01-06 17:07:50,282 - ==> Top1: 90.123    Loss: 0.271

2023-01-06 17:07:50,282 - ==> Confusion:
[[ 239    9  191]
 [  18  246  338]
 [  71   63 5811]]

2023-01-06 17:07:50,284 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 155168 on epoch: 143]
2023-01-06 17:07:50,284 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:50,290 - 

2023-01-06 17:07:50,290 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:50,846 - Epoch: [145][   10/  246]    Overall Loss 0.268952    Objective Loss 0.268952                                        LR 0.000008    Time 0.055529    
2023-01-06 17:07:51,031 - Epoch: [145][   20/  246]    Overall Loss 0.258728    Objective Loss 0.258728                                        LR 0.000008    Time 0.037015    
2023-01-06 17:07:51,221 - Epoch: [145][   30/  246]    Overall Loss 0.256544    Objective Loss 0.256544                                        LR 0.000008    Time 0.030978    
2023-01-06 17:07:51,412 - Epoch: [145][   40/  246]    Overall Loss 0.256721    Objective Loss 0.256721                                        LR 0.000008    Time 0.027996    
2023-01-06 17:07:51,603 - Epoch: [145][   50/  246]    Overall Loss 0.257192    Objective Loss 0.257192                                        LR 0.000008    Time 0.026206    
2023-01-06 17:07:51,793 - Epoch: [145][   60/  246]    Overall Loss 0.256104    Objective Loss 0.256104                                        LR 0.000008    Time 0.024999    
2023-01-06 17:07:51,985 - Epoch: [145][   70/  246]    Overall Loss 0.255810    Objective Loss 0.255810                                        LR 0.000008    Time 0.024177    
2023-01-06 17:07:52,175 - Epoch: [145][   80/  246]    Overall Loss 0.255297    Objective Loss 0.255297                                        LR 0.000008    Time 0.023526    
2023-01-06 17:07:52,364 - Epoch: [145][   90/  246]    Overall Loss 0.256493    Objective Loss 0.256493                                        LR 0.000008    Time 0.023007    
2023-01-06 17:07:52,554 - Epoch: [145][  100/  246]    Overall Loss 0.259110    Objective Loss 0.259110                                        LR 0.000008    Time 0.022596    
2023-01-06 17:07:52,743 - Epoch: [145][  110/  246]    Overall Loss 0.258516    Objective Loss 0.258516                                        LR 0.000008    Time 0.022257    
2023-01-06 17:07:52,927 - Epoch: [145][  120/  246]    Overall Loss 0.257888    Objective Loss 0.257888                                        LR 0.000008    Time 0.021935    
2023-01-06 17:07:53,097 - Epoch: [145][  130/  246]    Overall Loss 0.257915    Objective Loss 0.257915                                        LR 0.000008    Time 0.021549    
2023-01-06 17:07:53,269 - Epoch: [145][  140/  246]    Overall Loss 0.259354    Objective Loss 0.259354                                        LR 0.000008    Time 0.021236    
2023-01-06 17:07:53,439 - Epoch: [145][  150/  246]    Overall Loss 0.260579    Objective Loss 0.260579                                        LR 0.000008    Time 0.020954    
2023-01-06 17:07:53,611 - Epoch: [145][  160/  246]    Overall Loss 0.260600    Objective Loss 0.260600                                        LR 0.000008    Time 0.020717    
2023-01-06 17:07:53,781 - Epoch: [145][  170/  246]    Overall Loss 0.259947    Objective Loss 0.259947                                        LR 0.000008    Time 0.020497    
2023-01-06 17:07:53,959 - Epoch: [145][  180/  246]    Overall Loss 0.259996    Objective Loss 0.259996                                        LR 0.000008    Time 0.020344    
2023-01-06 17:07:54,151 - Epoch: [145][  190/  246]    Overall Loss 0.260837    Objective Loss 0.260837                                        LR 0.000008    Time 0.020281    
2023-01-06 17:07:54,345 - Epoch: [145][  200/  246]    Overall Loss 0.260613    Objective Loss 0.260613                                        LR 0.000008    Time 0.020234    
2023-01-06 17:07:54,536 - Epoch: [145][  210/  246]    Overall Loss 0.260893    Objective Loss 0.260893                                        LR 0.000008    Time 0.020180    
2023-01-06 17:07:54,729 - Epoch: [145][  220/  246]    Overall Loss 0.260524    Objective Loss 0.260524                                        LR 0.000008    Time 0.020140    
2023-01-06 17:07:54,921 - Epoch: [145][  230/  246]    Overall Loss 0.260592    Objective Loss 0.260592                                        LR 0.000008    Time 0.020095    
2023-01-06 17:07:55,122 - Epoch: [145][  240/  246]    Overall Loss 0.260170    Objective Loss 0.260170                                        LR 0.000008    Time 0.020095    
2023-01-06 17:07:55,212 - Epoch: [145][  246/  246]    Overall Loss 0.260641    Objective Loss 0.260641    Top1 89.952153    LR 0.000008    Time 0.019972    
2023-01-06 17:07:55,355 - --- validate (epoch=145)-----------
2023-01-06 17:07:55,355 - 6986 samples (256 per mini-batch)
2023-01-06 17:07:55,808 - Epoch: [145][   10/   28]    Loss 0.291222    Top1 89.609375    
2023-01-06 17:07:55,918 - Epoch: [145][   20/   28]    Loss 0.273244    Top1 90.117188    
2023-01-06 17:07:55,983 - Epoch: [145][   28/   28]    Loss 0.279562    Top1 89.979960    
2023-01-06 17:07:56,100 - ==> Top1: 89.980    Loss: 0.280

2023-01-06 17:07:56,101 - ==> Confusion:
[[ 250   11  178]
 [  17  257  328]
 [  84   82 5779]]

2023-01-06 17:07:56,102 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 155168 on epoch: 143]
2023-01-06 17:07:56,102 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:07:56,108 - 

2023-01-06 17:07:56,108 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:07:56,794 - Epoch: [146][   10/  246]    Overall Loss 0.260005    Objective Loss 0.260005                                        LR 0.000008    Time 0.068523    
2023-01-06 17:07:56,955 - Epoch: [146][   20/  246]    Overall Loss 0.274126    Objective Loss 0.274126                                        LR 0.000008    Time 0.042269    
2023-01-06 17:07:57,119 - Epoch: [146][   30/  246]    Overall Loss 0.267072    Objective Loss 0.267072                                        LR 0.000008    Time 0.033657    
2023-01-06 17:07:57,283 - Epoch: [146][   40/  246]    Overall Loss 0.266073    Objective Loss 0.266073                                        LR 0.000008    Time 0.029331    
2023-01-06 17:07:57,465 - Epoch: [146][   50/  246]    Overall Loss 0.264953    Objective Loss 0.264953                                        LR 0.000008    Time 0.027090    
2023-01-06 17:07:57,648 - Epoch: [146][   60/  246]    Overall Loss 0.261757    Objective Loss 0.261757                                        LR 0.000008    Time 0.025629    
2023-01-06 17:07:57,836 - Epoch: [146][   70/  246]    Overall Loss 0.262658    Objective Loss 0.262658                                        LR 0.000008    Time 0.024645    
2023-01-06 17:07:58,027 - Epoch: [146][   80/  246]    Overall Loss 0.262928    Objective Loss 0.262928                                        LR 0.000008    Time 0.023942    
2023-01-06 17:07:58,216 - Epoch: [146][   90/  246]    Overall Loss 0.263887    Objective Loss 0.263887                                        LR 0.000008    Time 0.023378    
2023-01-06 17:07:58,407 - Epoch: [146][  100/  246]    Overall Loss 0.264942    Objective Loss 0.264942                                        LR 0.000008    Time 0.022953    
2023-01-06 17:07:58,598 - Epoch: [146][  110/  246]    Overall Loss 0.264897    Objective Loss 0.264897                                        LR 0.000008    Time 0.022597    
2023-01-06 17:07:58,789 - Epoch: [146][  120/  246]    Overall Loss 0.266948    Objective Loss 0.266948                                        LR 0.000008    Time 0.022297    
2023-01-06 17:07:58,979 - Epoch: [146][  130/  246]    Overall Loss 0.267753    Objective Loss 0.267753                                        LR 0.000008    Time 0.022044    
2023-01-06 17:07:59,167 - Epoch: [146][  140/  246]    Overall Loss 0.266022    Objective Loss 0.266022                                        LR 0.000008    Time 0.021812    
2023-01-06 17:07:59,352 - Epoch: [146][  150/  246]    Overall Loss 0.265848    Objective Loss 0.265848                                        LR 0.000008    Time 0.021584    
2023-01-06 17:07:59,536 - Epoch: [146][  160/  246]    Overall Loss 0.265632    Objective Loss 0.265632                                        LR 0.000008    Time 0.021384    
2023-01-06 17:07:59,725 - Epoch: [146][  170/  246]    Overall Loss 0.265034    Objective Loss 0.265034                                        LR 0.000008    Time 0.021238    
2023-01-06 17:07:59,916 - Epoch: [146][  180/  246]    Overall Loss 0.263818    Objective Loss 0.263818                                        LR 0.000008    Time 0.021113    
2023-01-06 17:08:00,106 - Epoch: [146][  190/  246]    Overall Loss 0.263473    Objective Loss 0.263473                                        LR 0.000008    Time 0.021002    
2023-01-06 17:08:00,296 - Epoch: [146][  200/  246]    Overall Loss 0.264812    Objective Loss 0.264812                                        LR 0.000008    Time 0.020900    
2023-01-06 17:08:00,485 - Epoch: [146][  210/  246]    Overall Loss 0.264544    Objective Loss 0.264544                                        LR 0.000008    Time 0.020804    
2023-01-06 17:08:00,675 - Epoch: [146][  220/  246]    Overall Loss 0.263435    Objective Loss 0.263435                                        LR 0.000008    Time 0.020720    
2023-01-06 17:08:00,865 - Epoch: [146][  230/  246]    Overall Loss 0.263458    Objective Loss 0.263458                                        LR 0.000008    Time 0.020643    
2023-01-06 17:08:01,069 - Epoch: [146][  240/  246]    Overall Loss 0.262583    Objective Loss 0.262583                                        LR 0.000008    Time 0.020632    
2023-01-06 17:08:01,163 - Epoch: [146][  246/  246]    Overall Loss 0.262118    Objective Loss 0.262118    Top1 91.626794    LR 0.000008    Time 0.020508    
2023-01-06 17:08:01,311 - --- validate (epoch=146)-----------
2023-01-06 17:08:01,311 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:01,768 - Epoch: [146][   10/   28]    Loss 0.289130    Top1 89.570312    
2023-01-06 17:08:01,878 - Epoch: [146][   20/   28]    Loss 0.277946    Top1 90.078125    
2023-01-06 17:08:01,943 - Epoch: [146][   28/   28]    Loss 0.272887    Top1 90.194675    
2023-01-06 17:08:02,085 - ==> Top1: 90.195    Loss: 0.273

2023-01-06 17:08:02,086 - ==> Confusion:
[[ 228   13  198]
 [  17  241  344]
 [  54   59 5832]]

2023-01-06 17:08:02,087 - ==> Best [Top1: 90.195   Sparsity:0.00   Params: 155168 on epoch: 146]
2023-01-06 17:08:02,087 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:02,094 - 

2023-01-06 17:08:02,094 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:02,806 - Epoch: [147][   10/  246]    Overall Loss 0.247620    Objective Loss 0.247620                                        LR 0.000008    Time 0.071091    
2023-01-06 17:08:03,000 - Epoch: [147][   20/  246]    Overall Loss 0.251134    Objective Loss 0.251134                                        LR 0.000008    Time 0.045217    
2023-01-06 17:08:03,187 - Epoch: [147][   30/  246]    Overall Loss 0.258130    Objective Loss 0.258130                                        LR 0.000008    Time 0.036382    
2023-01-06 17:08:03,394 - Epoch: [147][   40/  246]    Overall Loss 0.257996    Objective Loss 0.257996                                        LR 0.000008    Time 0.032453    
2023-01-06 17:08:03,598 - Epoch: [147][   50/  246]    Overall Loss 0.255869    Objective Loss 0.255869                                        LR 0.000008    Time 0.030023    
2023-01-06 17:08:03,805 - Epoch: [147][   60/  246]    Overall Loss 0.256913    Objective Loss 0.256913                                        LR 0.000008    Time 0.028467    
2023-01-06 17:08:04,012 - Epoch: [147][   70/  246]    Overall Loss 0.258774    Objective Loss 0.258774                                        LR 0.000008    Time 0.027346    
2023-01-06 17:08:04,206 - Epoch: [147][   80/  246]    Overall Loss 0.259261    Objective Loss 0.259261                                        LR 0.000008    Time 0.026352    
2023-01-06 17:08:04,413 - Epoch: [147][   90/  246]    Overall Loss 0.261300    Objective Loss 0.261300                                        LR 0.000008    Time 0.025718    
2023-01-06 17:08:04,622 - Epoch: [147][  100/  246]    Overall Loss 0.262595    Objective Loss 0.262595                                        LR 0.000008    Time 0.025236    
2023-01-06 17:08:04,829 - Epoch: [147][  110/  246]    Overall Loss 0.262461    Objective Loss 0.262461                                        LR 0.000008    Time 0.024819    
2023-01-06 17:08:05,008 - Epoch: [147][  120/  246]    Overall Loss 0.262709    Objective Loss 0.262709                                        LR 0.000008    Time 0.024239    
2023-01-06 17:08:05,194 - Epoch: [147][  130/  246]    Overall Loss 0.263153    Objective Loss 0.263153                                        LR 0.000008    Time 0.023800    
2023-01-06 17:08:05,408 - Epoch: [147][  140/  246]    Overall Loss 0.261931    Objective Loss 0.261931                                        LR 0.000008    Time 0.023630    
2023-01-06 17:08:05,618 - Epoch: [147][  150/  246]    Overall Loss 0.261970    Objective Loss 0.261970                                        LR 0.000008    Time 0.023448    
2023-01-06 17:08:05,832 - Epoch: [147][  160/  246]    Overall Loss 0.262198    Objective Loss 0.262198                                        LR 0.000008    Time 0.023317    
2023-01-06 17:08:06,026 - Epoch: [147][  170/  246]    Overall Loss 0.261547    Objective Loss 0.261547                                        LR 0.000008    Time 0.023085    
2023-01-06 17:08:06,218 - Epoch: [147][  180/  246]    Overall Loss 0.261025    Objective Loss 0.261025                                        LR 0.000008    Time 0.022866    
2023-01-06 17:08:06,408 - Epoch: [147][  190/  246]    Overall Loss 0.261265    Objective Loss 0.261265                                        LR 0.000008    Time 0.022660    
2023-01-06 17:08:06,591 - Epoch: [147][  200/  246]    Overall Loss 0.261649    Objective Loss 0.261649                                        LR 0.000008    Time 0.022440    
2023-01-06 17:08:06,787 - Epoch: [147][  210/  246]    Overall Loss 0.261418    Objective Loss 0.261418                                        LR 0.000008    Time 0.022301    
2023-01-06 17:08:06,982 - Epoch: [147][  220/  246]    Overall Loss 0.261735    Objective Loss 0.261735                                        LR 0.000008    Time 0.022174    
2023-01-06 17:08:07,154 - Epoch: [147][  230/  246]    Overall Loss 0.262046    Objective Loss 0.262046                                        LR 0.000008    Time 0.021955    
2023-01-06 17:08:07,343 - Epoch: [147][  240/  246]    Overall Loss 0.262341    Objective Loss 0.262341                                        LR 0.000008    Time 0.021825    
2023-01-06 17:08:07,433 - Epoch: [147][  246/  246]    Overall Loss 0.262095    Objective Loss 0.262095    Top1 90.430622    LR 0.000008    Time 0.021660    
2023-01-06 17:08:07,591 - --- validate (epoch=147)-----------
2023-01-06 17:08:07,591 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:08,052 - Epoch: [147][   10/   28]    Loss 0.269518    Top1 90.234375    
2023-01-06 17:08:08,168 - Epoch: [147][   20/   28]    Loss 0.260241    Top1 90.664062    
2023-01-06 17:08:08,235 - Epoch: [147][   28/   28]    Loss 0.272463    Top1 90.080160    
2023-01-06 17:08:08,389 - ==> Top1: 90.080    Loss: 0.272

2023-01-06 17:08:08,390 - ==> Confusion:
[[ 223   14  202]
 [  16  241  345]
 [  52   64 5829]]

2023-01-06 17:08:08,391 - ==> Best [Top1: 90.195   Sparsity:0.00   Params: 155168 on epoch: 146]
2023-01-06 17:08:08,391 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:08,397 - 

2023-01-06 17:08:08,397 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:08,968 - Epoch: [148][   10/  246]    Overall Loss 0.244687    Objective Loss 0.244687                                        LR 0.000008    Time 0.057016    
2023-01-06 17:08:09,172 - Epoch: [148][   20/  246]    Overall Loss 0.261292    Objective Loss 0.261292                                        LR 0.000008    Time 0.038669    
2023-01-06 17:08:09,374 - Epoch: [148][   30/  246]    Overall Loss 0.257930    Objective Loss 0.257930                                        LR 0.000008    Time 0.032499    
2023-01-06 17:08:09,552 - Epoch: [148][   40/  246]    Overall Loss 0.257892    Objective Loss 0.257892                                        LR 0.000008    Time 0.028828    
2023-01-06 17:08:09,719 - Epoch: [148][   50/  246]    Overall Loss 0.259306    Objective Loss 0.259306                                        LR 0.000008    Time 0.026398    
2023-01-06 17:08:09,893 - Epoch: [148][   60/  246]    Overall Loss 0.259806    Objective Loss 0.259806                                        LR 0.000008    Time 0.024894    
2023-01-06 17:08:10,080 - Epoch: [148][   70/  246]    Overall Loss 0.260896    Objective Loss 0.260896                                        LR 0.000008    Time 0.023999    
2023-01-06 17:08:10,283 - Epoch: [148][   80/  246]    Overall Loss 0.262969    Objective Loss 0.262969                                        LR 0.000008    Time 0.023528    
2023-01-06 17:08:10,497 - Epoch: [148][   90/  246]    Overall Loss 0.262046    Objective Loss 0.262046                                        LR 0.000008    Time 0.023286    
2023-01-06 17:08:10,714 - Epoch: [148][  100/  246]    Overall Loss 0.262376    Objective Loss 0.262376                                        LR 0.000008    Time 0.023129    
2023-01-06 17:08:10,939 - Epoch: [148][  110/  246]    Overall Loss 0.263884    Objective Loss 0.263884                                        LR 0.000008    Time 0.023061    
2023-01-06 17:08:11,157 - Epoch: [148][  120/  246]    Overall Loss 0.263408    Objective Loss 0.263408                                        LR 0.000008    Time 0.022959    
2023-01-06 17:08:11,396 - Epoch: [148][  130/  246]    Overall Loss 0.262868    Objective Loss 0.262868                                        LR 0.000008    Time 0.023027    
2023-01-06 17:08:11,632 - Epoch: [148][  140/  246]    Overall Loss 0.262485    Objective Loss 0.262485                                        LR 0.000008    Time 0.023062    
2023-01-06 17:08:11,874 - Epoch: [148][  150/  246]    Overall Loss 0.261232    Objective Loss 0.261232                                        LR 0.000008    Time 0.023137    
2023-01-06 17:08:12,101 - Epoch: [148][  160/  246]    Overall Loss 0.261521    Objective Loss 0.261521                                        LR 0.000008    Time 0.023102    
2023-01-06 17:08:12,320 - Epoch: [148][  170/  246]    Overall Loss 0.260819    Objective Loss 0.260819                                        LR 0.000008    Time 0.023029    
2023-01-06 17:08:12,540 - Epoch: [148][  180/  246]    Overall Loss 0.260204    Objective Loss 0.260204                                        LR 0.000008    Time 0.022972    
2023-01-06 17:08:12,768 - Epoch: [148][  190/  246]    Overall Loss 0.260342    Objective Loss 0.260342                                        LR 0.000008    Time 0.022959    
2023-01-06 17:08:12,978 - Epoch: [148][  200/  246]    Overall Loss 0.260730    Objective Loss 0.260730                                        LR 0.000008    Time 0.022860    
2023-01-06 17:08:13,166 - Epoch: [148][  210/  246]    Overall Loss 0.261304    Objective Loss 0.261304                                        LR 0.000008    Time 0.022667    
2023-01-06 17:08:13,357 - Epoch: [148][  220/  246]    Overall Loss 0.260618    Objective Loss 0.260618                                        LR 0.000008    Time 0.022499    
2023-01-06 17:08:13,546 - Epoch: [148][  230/  246]    Overall Loss 0.261390    Objective Loss 0.261390                                        LR 0.000008    Time 0.022344    
2023-01-06 17:08:13,750 - Epoch: [148][  240/  246]    Overall Loss 0.259823    Objective Loss 0.259823                                        LR 0.000008    Time 0.022262    
2023-01-06 17:08:13,843 - Epoch: [148][  246/  246]    Overall Loss 0.259633    Objective Loss 0.259633    Top1 91.866029    LR 0.000008    Time 0.022096    
2023-01-06 17:08:14,008 - --- validate (epoch=148)-----------
2023-01-06 17:08:14,008 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:14,455 - Epoch: [148][   10/   28]    Loss 0.248525    Top1 90.585938    
2023-01-06 17:08:14,568 - Epoch: [148][   20/   28]    Loss 0.275297    Top1 89.941406    
2023-01-06 17:08:14,636 - Epoch: [148][   28/   28]    Loss 0.268507    Top1 90.194675    
2023-01-06 17:08:14,805 - ==> Top1: 90.195    Loss: 0.269

2023-01-06 17:08:14,805 - ==> Confusion:
[[ 234   17  188]
 [  17  274  311]
 [  67   85 5793]]

2023-01-06 17:08:14,806 - ==> Best [Top1: 90.195   Sparsity:0.00   Params: 155168 on epoch: 148]
2023-01-06 17:08:14,807 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:14,814 - 

2023-01-06 17:08:14,814 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:15,538 - Epoch: [149][   10/  246]    Overall Loss 0.247428    Objective Loss 0.247428                                        LR 0.000008    Time 0.072344    
2023-01-06 17:08:15,741 - Epoch: [149][   20/  246]    Overall Loss 0.250158    Objective Loss 0.250158                                        LR 0.000008    Time 0.046309    
2023-01-06 17:08:15,941 - Epoch: [149][   30/  246]    Overall Loss 0.254252    Objective Loss 0.254252                                        LR 0.000008    Time 0.037531    
2023-01-06 17:08:16,147 - Epoch: [149][   40/  246]    Overall Loss 0.257826    Objective Loss 0.257826                                        LR 0.000008    Time 0.033281    
2023-01-06 17:08:16,344 - Epoch: [149][   50/  246]    Overall Loss 0.261659    Objective Loss 0.261659                                        LR 0.000008    Time 0.030565    
2023-01-06 17:08:16,556 - Epoch: [149][   60/  246]    Overall Loss 0.257813    Objective Loss 0.257813                                        LR 0.000008    Time 0.028998    
2023-01-06 17:08:16,759 - Epoch: [149][   70/  246]    Overall Loss 0.258991    Objective Loss 0.258991                                        LR 0.000008    Time 0.027750    
2023-01-06 17:08:16,975 - Epoch: [149][   80/  246]    Overall Loss 0.257265    Objective Loss 0.257265                                        LR 0.000008    Time 0.026970    
2023-01-06 17:08:17,176 - Epoch: [149][   90/  246]    Overall Loss 0.258437    Objective Loss 0.258437                                        LR 0.000008    Time 0.026207    
2023-01-06 17:08:17,391 - Epoch: [149][  100/  246]    Overall Loss 0.258384    Objective Loss 0.258384                                        LR 0.000008    Time 0.025731    
2023-01-06 17:08:17,593 - Epoch: [149][  110/  246]    Overall Loss 0.258145    Objective Loss 0.258145                                        LR 0.000008    Time 0.025223    
2023-01-06 17:08:17,803 - Epoch: [149][  120/  246]    Overall Loss 0.258489    Objective Loss 0.258489                                        LR 0.000008    Time 0.024869    
2023-01-06 17:08:18,006 - Epoch: [149][  130/  246]    Overall Loss 0.259050    Objective Loss 0.259050                                        LR 0.000008    Time 0.024515    
2023-01-06 17:08:18,221 - Epoch: [149][  140/  246]    Overall Loss 0.259125    Objective Loss 0.259125                                        LR 0.000008    Time 0.024298    
2023-01-06 17:08:18,426 - Epoch: [149][  150/  246]    Overall Loss 0.260020    Objective Loss 0.260020                                        LR 0.000008    Time 0.024044    
2023-01-06 17:08:18,639 - Epoch: [149][  160/  246]    Overall Loss 0.259815    Objective Loss 0.259815                                        LR 0.000008    Time 0.023871    
2023-01-06 17:08:18,837 - Epoch: [149][  170/  246]    Overall Loss 0.260231    Objective Loss 0.260231                                        LR 0.000008    Time 0.023628    
2023-01-06 17:08:19,038 - Epoch: [149][  180/  246]    Overall Loss 0.260581    Objective Loss 0.260581                                        LR 0.000008    Time 0.023430    
2023-01-06 17:08:19,241 - Epoch: [149][  190/  246]    Overall Loss 0.260277    Objective Loss 0.260277                                        LR 0.000008    Time 0.023261    
2023-01-06 17:08:19,454 - Epoch: [149][  200/  246]    Overall Loss 0.259320    Objective Loss 0.259320                                        LR 0.000008    Time 0.023163    
2023-01-06 17:08:19,658 - Epoch: [149][  210/  246]    Overall Loss 0.259597    Objective Loss 0.259597                                        LR 0.000008    Time 0.023030    
2023-01-06 17:08:19,872 - Epoch: [149][  220/  246]    Overall Loss 0.259653    Objective Loss 0.259653                                        LR 0.000008    Time 0.022952    
2023-01-06 17:08:20,074 - Epoch: [149][  230/  246]    Overall Loss 0.259878    Objective Loss 0.259878                                        LR 0.000008    Time 0.022833    
2023-01-06 17:08:20,290 - Epoch: [149][  240/  246]    Overall Loss 0.259683    Objective Loss 0.259683                                        LR 0.000008    Time 0.022781    
2023-01-06 17:08:20,386 - Epoch: [149][  246/  246]    Overall Loss 0.259740    Objective Loss 0.259740    Top1 91.387560    LR 0.000008    Time 0.022611    
2023-01-06 17:08:20,533 - --- validate (epoch=149)-----------
2023-01-06 17:08:20,533 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:20,983 - Epoch: [149][   10/   28]    Loss 0.266877    Top1 90.195312    
2023-01-06 17:08:21,102 - Epoch: [149][   20/   28]    Loss 0.269506    Top1 90.234375    
2023-01-06 17:08:21,167 - Epoch: [149][   28/   28]    Loss 0.272524    Top1 90.294875    
2023-01-06 17:08:21,304 - ==> Top1: 90.295    Loss: 0.273

2023-01-06 17:08:21,305 - ==> Confusion:
[[ 238   12  189]
 [  15  261  326]
 [  65   71 5809]]

2023-01-06 17:08:21,306 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:21,306 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:21,313 - 

2023-01-06 17:08:21,314 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:22,066 - Epoch: [150][   10/  246]    Overall Loss 0.253668    Objective Loss 0.253668                                        LR 0.000008    Time 0.075217    
2023-01-06 17:08:22,265 - Epoch: [150][   20/  246]    Overall Loss 0.245024    Objective Loss 0.245024                                        LR 0.000008    Time 0.047502    
2023-01-06 17:08:22,470 - Epoch: [150][   30/  246]    Overall Loss 0.246914    Objective Loss 0.246914                                        LR 0.000008    Time 0.038512    
2023-01-06 17:08:22,668 - Epoch: [150][   40/  246]    Overall Loss 0.253857    Objective Loss 0.253857                                        LR 0.000008    Time 0.033806    
2023-01-06 17:08:22,869 - Epoch: [150][   50/  246]    Overall Loss 0.255513    Objective Loss 0.255513                                        LR 0.000008    Time 0.031072    
2023-01-06 17:08:23,066 - Epoch: [150][   60/  246]    Overall Loss 0.256384    Objective Loss 0.256384                                        LR 0.000008    Time 0.029170    
2023-01-06 17:08:23,266 - Epoch: [150][   70/  246]    Overall Loss 0.261603    Objective Loss 0.261603                                        LR 0.000008    Time 0.027857    
2023-01-06 17:08:23,464 - Epoch: [150][   80/  246]    Overall Loss 0.262647    Objective Loss 0.262647                                        LR 0.000008    Time 0.026834    
2023-01-06 17:08:23,665 - Epoch: [150][   90/  246]    Overall Loss 0.262920    Objective Loss 0.262920                                        LR 0.000008    Time 0.026084    
2023-01-06 17:08:23,862 - Epoch: [150][  100/  246]    Overall Loss 0.261192    Objective Loss 0.261192                                        LR 0.000008    Time 0.025450    
2023-01-06 17:08:24,052 - Epoch: [150][  110/  246]    Overall Loss 0.259826    Objective Loss 0.259826                                        LR 0.000008    Time 0.024853    
2023-01-06 17:08:24,241 - Epoch: [150][  120/  246]    Overall Loss 0.259818    Objective Loss 0.259818                                        LR 0.000008    Time 0.024357    
2023-01-06 17:08:24,430 - Epoch: [150][  130/  246]    Overall Loss 0.260260    Objective Loss 0.260260                                        LR 0.000008    Time 0.023934    
2023-01-06 17:08:24,619 - Epoch: [150][  140/  246]    Overall Loss 0.258679    Objective Loss 0.258679                                        LR 0.000008    Time 0.023570    
2023-01-06 17:08:24,808 - Epoch: [150][  150/  246]    Overall Loss 0.258734    Objective Loss 0.258734                                        LR 0.000008    Time 0.023259    
2023-01-06 17:08:24,997 - Epoch: [150][  160/  246]    Overall Loss 0.257644    Objective Loss 0.257644                                        LR 0.000008    Time 0.022985    
2023-01-06 17:08:25,164 - Epoch: [150][  170/  246]    Overall Loss 0.257183    Objective Loss 0.257183                                        LR 0.000008    Time 0.022610    
2023-01-06 17:08:25,350 - Epoch: [150][  180/  246]    Overall Loss 0.258120    Objective Loss 0.258120                                        LR 0.000008    Time 0.022386    
2023-01-06 17:08:25,539 - Epoch: [150][  190/  246]    Overall Loss 0.258101    Objective Loss 0.258101                                        LR 0.000008    Time 0.022201    
2023-01-06 17:08:25,729 - Epoch: [150][  200/  246]    Overall Loss 0.258295    Objective Loss 0.258295                                        LR 0.000008    Time 0.022038    
2023-01-06 17:08:25,918 - Epoch: [150][  210/  246]    Overall Loss 0.259014    Objective Loss 0.259014                                        LR 0.000008    Time 0.021889    
2023-01-06 17:08:26,108 - Epoch: [150][  220/  246]    Overall Loss 0.258596    Objective Loss 0.258596                                        LR 0.000008    Time 0.021758    
2023-01-06 17:08:26,299 - Epoch: [150][  230/  246]    Overall Loss 0.258914    Objective Loss 0.258914                                        LR 0.000008    Time 0.021636    
2023-01-06 17:08:26,502 - Epoch: [150][  240/  246]    Overall Loss 0.259494    Objective Loss 0.259494                                        LR 0.000008    Time 0.021582    
2023-01-06 17:08:26,596 - Epoch: [150][  246/  246]    Overall Loss 0.259444    Objective Loss 0.259444    Top1 91.387560    LR 0.000008    Time 0.021438    
2023-01-06 17:08:26,727 - --- validate (epoch=150)-----------
2023-01-06 17:08:26,727 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:27,211 - Epoch: [150][   10/   28]    Loss 0.273664    Top1 90.195312    
2023-01-06 17:08:27,346 - Epoch: [150][   20/   28]    Loss 0.274991    Top1 90.097656    
2023-01-06 17:08:27,412 - Epoch: [150][   28/   28]    Loss 0.267998    Top1 90.080160    
2023-01-06 17:08:27,566 - ==> Top1: 90.080    Loss: 0.268

2023-01-06 17:08:27,566 - ==> Confusion:
[[ 234   13  192]
 [  17  242  343]
 [  64   64 5817]]

2023-01-06 17:08:27,567 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:27,567 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:27,573 - 

2023-01-06 17:08:27,573 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:28,136 - Epoch: [151][   10/  246]    Overall Loss 0.249415    Objective Loss 0.249415                                        LR 0.000008    Time 0.056210    
2023-01-06 17:08:28,322 - Epoch: [151][   20/  246]    Overall Loss 0.253348    Objective Loss 0.253348                                        LR 0.000008    Time 0.037359    
2023-01-06 17:08:28,514 - Epoch: [151][   30/  246]    Overall Loss 0.254837    Objective Loss 0.254837                                        LR 0.000008    Time 0.031282    
2023-01-06 17:08:28,710 - Epoch: [151][   40/  246]    Overall Loss 0.256486    Objective Loss 0.256486                                        LR 0.000008    Time 0.028357    
2023-01-06 17:08:28,910 - Epoch: [151][   50/  246]    Overall Loss 0.261289    Objective Loss 0.261289                                        LR 0.000008    Time 0.026680    
2023-01-06 17:08:29,104 - Epoch: [151][   60/  246]    Overall Loss 0.260610    Objective Loss 0.260610                                        LR 0.000008    Time 0.025469    
2023-01-06 17:08:29,304 - Epoch: [151][   70/  246]    Overall Loss 0.259489    Objective Loss 0.259489                                        LR 0.000008    Time 0.024677    
2023-01-06 17:08:29,500 - Epoch: [151][   80/  246]    Overall Loss 0.261545    Objective Loss 0.261545                                        LR 0.000008    Time 0.024035    
2023-01-06 17:08:29,699 - Epoch: [151][   90/  246]    Overall Loss 0.259974    Objective Loss 0.259974                                        LR 0.000008    Time 0.023578    
2023-01-06 17:08:29,895 - Epoch: [151][  100/  246]    Overall Loss 0.259756    Objective Loss 0.259756                                        LR 0.000008    Time 0.023173    
2023-01-06 17:08:30,094 - Epoch: [151][  110/  246]    Overall Loss 0.258150    Objective Loss 0.258150                                        LR 0.000008    Time 0.022871    
2023-01-06 17:08:30,289 - Epoch: [151][  120/  246]    Overall Loss 0.256754    Objective Loss 0.256754                                        LR 0.000008    Time 0.022590    
2023-01-06 17:08:30,479 - Epoch: [151][  130/  246]    Overall Loss 0.257245    Objective Loss 0.257245                                        LR 0.000008    Time 0.022306    
2023-01-06 17:08:30,668 - Epoch: [151][  140/  246]    Overall Loss 0.257524    Objective Loss 0.257524                                        LR 0.000008    Time 0.022066    
2023-01-06 17:08:30,859 - Epoch: [151][  150/  246]    Overall Loss 0.257488    Objective Loss 0.257488                                        LR 0.000008    Time 0.021860    
2023-01-06 17:08:31,048 - Epoch: [151][  160/  246]    Overall Loss 0.259099    Objective Loss 0.259099                                        LR 0.000008    Time 0.021678    
2023-01-06 17:08:31,238 - Epoch: [151][  170/  246]    Overall Loss 0.260016    Objective Loss 0.260016                                        LR 0.000008    Time 0.021518    
2023-01-06 17:08:31,429 - Epoch: [151][  180/  246]    Overall Loss 0.259802    Objective Loss 0.259802                                        LR 0.000008    Time 0.021379    
2023-01-06 17:08:31,619 - Epoch: [151][  190/  246]    Overall Loss 0.259353    Objective Loss 0.259353                                        LR 0.000008    Time 0.021250    
2023-01-06 17:08:31,808 - Epoch: [151][  200/  246]    Overall Loss 0.258582    Objective Loss 0.258582                                        LR 0.000008    Time 0.021131    
2023-01-06 17:08:31,995 - Epoch: [151][  210/  246]    Overall Loss 0.257943    Objective Loss 0.257943                                        LR 0.000008    Time 0.021018    
2023-01-06 17:08:32,184 - Epoch: [151][  220/  246]    Overall Loss 0.258664    Objective Loss 0.258664                                        LR 0.000008    Time 0.020915    
2023-01-06 17:08:32,373 - Epoch: [151][  230/  246]    Overall Loss 0.259409    Objective Loss 0.259409                                        LR 0.000008    Time 0.020830    
2023-01-06 17:08:32,576 - Epoch: [151][  240/  246]    Overall Loss 0.259524    Objective Loss 0.259524                                        LR 0.000008    Time 0.020806    
2023-01-06 17:08:32,670 - Epoch: [151][  246/  246]    Overall Loss 0.259413    Objective Loss 0.259413    Top1 91.866029    LR 0.000008    Time 0.020680    
2023-01-06 17:08:32,803 - --- validate (epoch=151)-----------
2023-01-06 17:08:32,804 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:33,249 - Epoch: [151][   10/   28]    Loss 0.271271    Top1 90.039062    
2023-01-06 17:08:33,369 - Epoch: [151][   20/   28]    Loss 0.271106    Top1 90.117188    
2023-01-06 17:08:33,437 - Epoch: [151][   28/   28]    Loss 0.269810    Top1 90.266247    
2023-01-06 17:08:33,572 - ==> Top1: 90.266    Loss: 0.270

2023-01-06 17:08:33,572 - ==> Confusion:
[[ 236   13  190]
 [  18  258  326]
 [  65   68 5812]]

2023-01-06 17:08:33,574 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:33,574 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:33,580 - 

2023-01-06 17:08:33,580 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:34,306 - Epoch: [152][   10/  246]    Overall Loss 0.257483    Objective Loss 0.257483                                        LR 0.000008    Time 0.072492    
2023-01-06 17:08:34,503 - Epoch: [152][   20/  246]    Overall Loss 0.257578    Objective Loss 0.257578                                        LR 0.000008    Time 0.046051    
2023-01-06 17:08:34,711 - Epoch: [152][   30/  246]    Overall Loss 0.261842    Objective Loss 0.261842                                        LR 0.000008    Time 0.037618    
2023-01-06 17:08:34,922 - Epoch: [152][   40/  246]    Overall Loss 0.260795    Objective Loss 0.260795                                        LR 0.000008    Time 0.033479    
2023-01-06 17:08:35,136 - Epoch: [152][   50/  246]    Overall Loss 0.258710    Objective Loss 0.258710                                        LR 0.000008    Time 0.031064    
2023-01-06 17:08:35,344 - Epoch: [152][   60/  246]    Overall Loss 0.256366    Objective Loss 0.256366                                        LR 0.000008    Time 0.029348    
2023-01-06 17:08:35,545 - Epoch: [152][   70/  246]    Overall Loss 0.256498    Objective Loss 0.256498                                        LR 0.000008    Time 0.028018    
2023-01-06 17:08:35,751 - Epoch: [152][   80/  246]    Overall Loss 0.255257    Objective Loss 0.255257                                        LR 0.000008    Time 0.027080    
2023-01-06 17:08:35,966 - Epoch: [152][   90/  246]    Overall Loss 0.256447    Objective Loss 0.256447                                        LR 0.000008    Time 0.026458    
2023-01-06 17:08:36,178 - Epoch: [152][  100/  246]    Overall Loss 0.257073    Objective Loss 0.257073                                        LR 0.000008    Time 0.025928    
2023-01-06 17:08:36,393 - Epoch: [152][  110/  246]    Overall Loss 0.255777    Objective Loss 0.255777                                        LR 0.000008    Time 0.025526    
2023-01-06 17:08:36,607 - Epoch: [152][  120/  246]    Overall Loss 0.255607    Objective Loss 0.255607                                        LR 0.000008    Time 0.025181    
2023-01-06 17:08:36,825 - Epoch: [152][  130/  246]    Overall Loss 0.255727    Objective Loss 0.255727                                        LR 0.000008    Time 0.024919    
2023-01-06 17:08:37,027 - Epoch: [152][  140/  246]    Overall Loss 0.256717    Objective Loss 0.256717                                        LR 0.000008    Time 0.024574    
2023-01-06 17:08:37,232 - Epoch: [152][  150/  246]    Overall Loss 0.256691    Objective Loss 0.256691                                        LR 0.000008    Time 0.024304    
2023-01-06 17:08:37,441 - Epoch: [152][  160/  246]    Overall Loss 0.257297    Objective Loss 0.257297                                        LR 0.000008    Time 0.024089    
2023-01-06 17:08:37,651 - Epoch: [152][  170/  246]    Overall Loss 0.258063    Objective Loss 0.258063                                        LR 0.000008    Time 0.023905    
2023-01-06 17:08:37,863 - Epoch: [152][  180/  246]    Overall Loss 0.257006    Objective Loss 0.257006                                        LR 0.000008    Time 0.023750    
2023-01-06 17:08:38,061 - Epoch: [152][  190/  246]    Overall Loss 0.255833    Objective Loss 0.255833                                        LR 0.000008    Time 0.023542    
2023-01-06 17:08:38,262 - Epoch: [152][  200/  246]    Overall Loss 0.255298    Objective Loss 0.255298                                        LR 0.000008    Time 0.023368    
2023-01-06 17:08:38,476 - Epoch: [152][  210/  246]    Overall Loss 0.257135    Objective Loss 0.257135                                        LR 0.000008    Time 0.023273    
2023-01-06 17:08:38,676 - Epoch: [152][  220/  246]    Overall Loss 0.258309    Objective Loss 0.258309                                        LR 0.000008    Time 0.023122    
2023-01-06 17:08:38,885 - Epoch: [152][  230/  246]    Overall Loss 0.259030    Objective Loss 0.259030                                        LR 0.000008    Time 0.023022    
2023-01-06 17:08:39,091 - Epoch: [152][  240/  246]    Overall Loss 0.259336    Objective Loss 0.259336                                        LR 0.000008    Time 0.022924    
2023-01-06 17:08:39,190 - Epoch: [152][  246/  246]    Overall Loss 0.259566    Objective Loss 0.259566    Top1 90.191388    LR 0.000008    Time 0.022763    
2023-01-06 17:08:39,363 - --- validate (epoch=152)-----------
2023-01-06 17:08:39,363 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:39,833 - Epoch: [152][   10/   28]    Loss 0.269733    Top1 90.468750    
2023-01-06 17:08:39,955 - Epoch: [152][   20/   28]    Loss 0.270104    Top1 90.332031    
2023-01-06 17:08:40,021 - Epoch: [152][   28/   28]    Loss 0.269321    Top1 90.223304    
2023-01-06 17:08:40,157 - ==> Top1: 90.223    Loss: 0.269

2023-01-06 17:08:40,157 - ==> Confusion:
[[ 252   11  176]
 [  19  242  341]
 [  77   59 5809]]

2023-01-06 17:08:40,159 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:40,159 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:40,165 - 

2023-01-06 17:08:40,165 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:40,721 - Epoch: [153][   10/  246]    Overall Loss 0.239690    Objective Loss 0.239690                                        LR 0.000008    Time 0.055489    
2023-01-06 17:08:40,894 - Epoch: [153][   20/  246]    Overall Loss 0.255509    Objective Loss 0.255509                                        LR 0.000008    Time 0.036389    
2023-01-06 17:08:41,087 - Epoch: [153][   30/  246]    Overall Loss 0.256825    Objective Loss 0.256825                                        LR 0.000008    Time 0.030665    
2023-01-06 17:08:41,276 - Epoch: [153][   40/  246]    Overall Loss 0.259994    Objective Loss 0.259994                                        LR 0.000008    Time 0.027725    
2023-01-06 17:08:41,466 - Epoch: [153][   50/  246]    Overall Loss 0.254862    Objective Loss 0.254862                                        LR 0.000008    Time 0.025976    
2023-01-06 17:08:41,658 - Epoch: [153][   60/  246]    Overall Loss 0.256430    Objective Loss 0.256430                                        LR 0.000008    Time 0.024840    
2023-01-06 17:08:41,849 - Epoch: [153][   70/  246]    Overall Loss 0.255801    Objective Loss 0.255801                                        LR 0.000008    Time 0.024018    
2023-01-06 17:08:42,043 - Epoch: [153][   80/  246]    Overall Loss 0.255552    Objective Loss 0.255552                                        LR 0.000008    Time 0.023433    
2023-01-06 17:08:42,238 - Epoch: [153][   90/  246]    Overall Loss 0.257911    Objective Loss 0.257911                                        LR 0.000008    Time 0.022989    
2023-01-06 17:08:42,433 - Epoch: [153][  100/  246]    Overall Loss 0.259335    Objective Loss 0.259335                                        LR 0.000008    Time 0.022636    
2023-01-06 17:08:42,627 - Epoch: [153][  110/  246]    Overall Loss 0.258217    Objective Loss 0.258217                                        LR 0.000008    Time 0.022339    
2023-01-06 17:08:42,822 - Epoch: [153][  120/  246]    Overall Loss 0.257207    Objective Loss 0.257207                                        LR 0.000008    Time 0.022104    
2023-01-06 17:08:43,018 - Epoch: [153][  130/  246]    Overall Loss 0.256623    Objective Loss 0.256623                                        LR 0.000008    Time 0.021904    
2023-01-06 17:08:43,215 - Epoch: [153][  140/  246]    Overall Loss 0.256524    Objective Loss 0.256524                                        LR 0.000008    Time 0.021750    
2023-01-06 17:08:43,407 - Epoch: [153][  150/  246]    Overall Loss 0.257395    Objective Loss 0.257395                                        LR 0.000008    Time 0.021574    
2023-01-06 17:08:43,603 - Epoch: [153][  160/  246]    Overall Loss 0.258233    Objective Loss 0.258233                                        LR 0.000008    Time 0.021445    
2023-01-06 17:08:43,800 - Epoch: [153][  170/  246]    Overall Loss 0.258688    Objective Loss 0.258688                                        LR 0.000008    Time 0.021344    
2023-01-06 17:08:43,998 - Epoch: [153][  180/  246]    Overall Loss 0.258903    Objective Loss 0.258903                                        LR 0.000008    Time 0.021258    
2023-01-06 17:08:44,210 - Epoch: [153][  190/  246]    Overall Loss 0.259467    Objective Loss 0.259467                                        LR 0.000008    Time 0.021253    
2023-01-06 17:08:44,435 - Epoch: [153][  200/  246]    Overall Loss 0.259342    Objective Loss 0.259342                                        LR 0.000008    Time 0.021309    
2023-01-06 17:08:44,682 - Epoch: [153][  210/  246]    Overall Loss 0.259445    Objective Loss 0.259445                                        LR 0.000008    Time 0.021469    
2023-01-06 17:08:44,916 - Epoch: [153][  220/  246]    Overall Loss 0.259213    Objective Loss 0.259213                                        LR 0.000008    Time 0.021545    
2023-01-06 17:08:45,136 - Epoch: [153][  230/  246]    Overall Loss 0.259484    Objective Loss 0.259484                                        LR 0.000008    Time 0.021566    
2023-01-06 17:08:45,341 - Epoch: [153][  240/  246]    Overall Loss 0.260028    Objective Loss 0.260028                                        LR 0.000008    Time 0.021518    
2023-01-06 17:08:45,435 - Epoch: [153][  246/  246]    Overall Loss 0.260414    Objective Loss 0.260414    Top1 90.669856    LR 0.000008    Time 0.021375    
2023-01-06 17:08:45,615 - --- validate (epoch=153)-----------
2023-01-06 17:08:45,615 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:46,070 - Epoch: [153][   10/   28]    Loss 0.249546    Top1 91.367188    
2023-01-06 17:08:46,184 - Epoch: [153][   20/   28]    Loss 0.273447    Top1 90.410156    
2023-01-06 17:08:46,251 - Epoch: [153][   28/   28]    Loss 0.276142    Top1 90.123103    
2023-01-06 17:08:46,386 - ==> Top1: 90.123    Loss: 0.276

2023-01-06 17:08:46,386 - ==> Confusion:
[[ 241    9  189]
 [  19  226  357]
 [  66   50 5829]]

2023-01-06 17:08:46,387 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:46,388 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:46,393 - 

2023-01-06 17:08:46,394 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:47,097 - Epoch: [154][   10/  246]    Overall Loss 0.256452    Objective Loss 0.256452                                        LR 0.000008    Time 0.070324    
2023-01-06 17:08:47,304 - Epoch: [154][   20/  246]    Overall Loss 0.258292    Objective Loss 0.258292                                        LR 0.000008    Time 0.045460    
2023-01-06 17:08:47,498 - Epoch: [154][   30/  246]    Overall Loss 0.250217    Objective Loss 0.250217                                        LR 0.000008    Time 0.036771    
2023-01-06 17:08:47,698 - Epoch: [154][   40/  246]    Overall Loss 0.250044    Objective Loss 0.250044                                        LR 0.000008    Time 0.032567    
2023-01-06 17:08:47,893 - Epoch: [154][   50/  246]    Overall Loss 0.254456    Objective Loss 0.254456                                        LR 0.000008    Time 0.029949    
2023-01-06 17:08:48,085 - Epoch: [154][   60/  246]    Overall Loss 0.256208    Objective Loss 0.256208                                        LR 0.000008    Time 0.028156    
2023-01-06 17:08:48,280 - Epoch: [154][   70/  246]    Overall Loss 0.256785    Objective Loss 0.256785                                        LR 0.000008    Time 0.026901    
2023-01-06 17:08:48,473 - Epoch: [154][   80/  246]    Overall Loss 0.257749    Objective Loss 0.257749                                        LR 0.000008    Time 0.025949    
2023-01-06 17:08:48,667 - Epoch: [154][   90/  246]    Overall Loss 0.257151    Objective Loss 0.257151                                        LR 0.000008    Time 0.025220    
2023-01-06 17:08:48,856 - Epoch: [154][  100/  246]    Overall Loss 0.258337    Objective Loss 0.258337                                        LR 0.000008    Time 0.024584    
2023-01-06 17:08:49,046 - Epoch: [154][  110/  246]    Overall Loss 0.259799    Objective Loss 0.259799                                        LR 0.000008    Time 0.024069    
2023-01-06 17:08:49,234 - Epoch: [154][  120/  246]    Overall Loss 0.260175    Objective Loss 0.260175                                        LR 0.000008    Time 0.023630    
2023-01-06 17:08:49,423 - Epoch: [154][  130/  246]    Overall Loss 0.259900    Objective Loss 0.259900                                        LR 0.000008    Time 0.023264    
2023-01-06 17:08:49,612 - Epoch: [154][  140/  246]    Overall Loss 0.260335    Objective Loss 0.260335                                        LR 0.000008    Time 0.022950    
2023-01-06 17:08:49,801 - Epoch: [154][  150/  246]    Overall Loss 0.260290    Objective Loss 0.260290                                        LR 0.000008    Time 0.022675    
2023-01-06 17:08:49,989 - Epoch: [154][  160/  246]    Overall Loss 0.260342    Objective Loss 0.260342                                        LR 0.000008    Time 0.022435    
2023-01-06 17:08:50,180 - Epoch: [154][  170/  246]    Overall Loss 0.259264    Objective Loss 0.259264                                        LR 0.000008    Time 0.022232    
2023-01-06 17:08:50,369 - Epoch: [154][  180/  246]    Overall Loss 0.258869    Objective Loss 0.258869                                        LR 0.000008    Time 0.022047    
2023-01-06 17:08:50,557 - Epoch: [154][  190/  246]    Overall Loss 0.259477    Objective Loss 0.259477                                        LR 0.000008    Time 0.021876    
2023-01-06 17:08:50,743 - Epoch: [154][  200/  246]    Overall Loss 0.259497    Objective Loss 0.259497                                        LR 0.000008    Time 0.021710    
2023-01-06 17:08:50,931 - Epoch: [154][  210/  246]    Overall Loss 0.260049    Objective Loss 0.260049                                        LR 0.000008    Time 0.021571    
2023-01-06 17:08:51,121 - Epoch: [154][  220/  246]    Overall Loss 0.260321    Objective Loss 0.260321                                        LR 0.000008    Time 0.021452    
2023-01-06 17:08:51,310 - Epoch: [154][  230/  246]    Overall Loss 0.260556    Objective Loss 0.260556                                        LR 0.000008    Time 0.021338    
2023-01-06 17:08:51,511 - Epoch: [154][  240/  246]    Overall Loss 0.259880    Objective Loss 0.259880                                        LR 0.000008    Time 0.021284    
2023-01-06 17:08:51,605 - Epoch: [154][  246/  246]    Overall Loss 0.260010    Objective Loss 0.260010    Top1 88.038278    LR 0.000008    Time 0.021146    
2023-01-06 17:08:51,746 - --- validate (epoch=154)-----------
2023-01-06 17:08:51,746 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:52,201 - Epoch: [154][   10/   28]    Loss 0.274600    Top1 90.117188    
2023-01-06 17:08:52,307 - Epoch: [154][   20/   28]    Loss 0.266384    Top1 90.039062    
2023-01-06 17:08:52,372 - Epoch: [154][   28/   28]    Loss 0.268181    Top1 89.965646    
2023-01-06 17:08:52,516 - ==> Top1: 89.966    Loss: 0.268

2023-01-06 17:08:52,517 - ==> Confusion:
[[ 223   11  205]
 [  17  225  360]
 [  52   56 5837]]

2023-01-06 17:08:52,518 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:52,518 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:52,524 - 

2023-01-06 17:08:52,524 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:08:53,260 - Epoch: [155][   10/  246]    Overall Loss 0.273678    Objective Loss 0.273678                                        LR 0.000008    Time 0.073493    
2023-01-06 17:08:53,477 - Epoch: [155][   20/  246]    Overall Loss 0.263001    Objective Loss 0.263001                                        LR 0.000008    Time 0.047580    
2023-01-06 17:08:53,688 - Epoch: [155][   30/  246]    Overall Loss 0.256559    Objective Loss 0.256559                                        LR 0.000008    Time 0.038742    
2023-01-06 17:08:53,900 - Epoch: [155][   40/  246]    Overall Loss 0.260248    Objective Loss 0.260248                                        LR 0.000008    Time 0.034331    
2023-01-06 17:08:54,099 - Epoch: [155][   50/  246]    Overall Loss 0.262088    Objective Loss 0.262088                                        LR 0.000008    Time 0.031454    
2023-01-06 17:08:54,299 - Epoch: [155][   60/  246]    Overall Loss 0.263278    Objective Loss 0.263278                                        LR 0.000008    Time 0.029525    
2023-01-06 17:08:54,512 - Epoch: [155][   70/  246]    Overall Loss 0.264990    Objective Loss 0.264990                                        LR 0.000008    Time 0.028347    
2023-01-06 17:08:54,736 - Epoch: [155][   80/  246]    Overall Loss 0.262280    Objective Loss 0.262280                                        LR 0.000008    Time 0.027604    
2023-01-06 17:08:54,957 - Epoch: [155][   90/  246]    Overall Loss 0.262080    Objective Loss 0.262080                                        LR 0.000008    Time 0.026989    
2023-01-06 17:08:55,180 - Epoch: [155][  100/  246]    Overall Loss 0.261254    Objective Loss 0.261254                                        LR 0.000008    Time 0.026517    
2023-01-06 17:08:55,409 - Epoch: [155][  110/  246]    Overall Loss 0.259927    Objective Loss 0.259927                                        LR 0.000008    Time 0.026181    
2023-01-06 17:08:55,609 - Epoch: [155][  120/  246]    Overall Loss 0.258413    Objective Loss 0.258413                                        LR 0.000008    Time 0.025661    
2023-01-06 17:08:55,811 - Epoch: [155][  130/  246]    Overall Loss 0.259482    Objective Loss 0.259482                                        LR 0.000008    Time 0.025235    
2023-01-06 17:08:56,015 - Epoch: [155][  140/  246]    Overall Loss 0.258404    Objective Loss 0.258404                                        LR 0.000008    Time 0.024888    
2023-01-06 17:08:56,207 - Epoch: [155][  150/  246]    Overall Loss 0.258523    Objective Loss 0.258523                                        LR 0.000008    Time 0.024507    
2023-01-06 17:08:56,416 - Epoch: [155][  160/  246]    Overall Loss 0.258180    Objective Loss 0.258180                                        LR 0.000008    Time 0.024279    
2023-01-06 17:08:56,646 - Epoch: [155][  170/  246]    Overall Loss 0.259442    Objective Loss 0.259442                                        LR 0.000008    Time 0.024202    
2023-01-06 17:08:56,887 - Epoch: [155][  180/  246]    Overall Loss 0.259317    Objective Loss 0.259317                                        LR 0.000008    Time 0.024195    
2023-01-06 17:08:57,127 - Epoch: [155][  190/  246]    Overall Loss 0.259350    Objective Loss 0.259350                                        LR 0.000008    Time 0.024180    
2023-01-06 17:08:57,384 - Epoch: [155][  200/  246]    Overall Loss 0.259964    Objective Loss 0.259964                                        LR 0.000008    Time 0.024253    
2023-01-06 17:08:57,634 - Epoch: [155][  210/  246]    Overall Loss 0.260332    Objective Loss 0.260332                                        LR 0.000008    Time 0.024288    
2023-01-06 17:08:57,883 - Epoch: [155][  220/  246]    Overall Loss 0.260014    Objective Loss 0.260014                                        LR 0.000008    Time 0.024311    
2023-01-06 17:08:58,136 - Epoch: [155][  230/  246]    Overall Loss 0.260071    Objective Loss 0.260071                                        LR 0.000008    Time 0.024354    
2023-01-06 17:08:58,396 - Epoch: [155][  240/  246]    Overall Loss 0.259841    Objective Loss 0.259841                                        LR 0.000008    Time 0.024419    
2023-01-06 17:08:58,508 - Epoch: [155][  246/  246]    Overall Loss 0.259496    Objective Loss 0.259496    Top1 89.712919    LR 0.000008    Time 0.024279    
2023-01-06 17:08:58,646 - --- validate (epoch=155)-----------
2023-01-06 17:08:58,646 - 6986 samples (256 per mini-batch)
2023-01-06 17:08:59,108 - Epoch: [155][   10/   28]    Loss 0.276941    Top1 89.882812    
2023-01-06 17:08:59,242 - Epoch: [155][   20/   28]    Loss 0.277396    Top1 89.765625    
2023-01-06 17:08:59,311 - Epoch: [155][   28/   28]    Loss 0.273240    Top1 90.065846    
2023-01-06 17:08:59,457 - ==> Top1: 90.066    Loss: 0.273

2023-01-06 17:08:59,457 - ==> Confusion:
[[ 234   13  192]
 [  18  239  345]
 [  67   59 5819]]

2023-01-06 17:08:59,459 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:08:59,459 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:08:59,465 - 

2023-01-06 17:08:59,465 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:00,021 - Epoch: [156][   10/  246]    Overall Loss 0.260906    Objective Loss 0.260906                                        LR 0.000008    Time 0.055526    
2023-01-06 17:09:00,208 - Epoch: [156][   20/  246]    Overall Loss 0.262497    Objective Loss 0.262497                                        LR 0.000008    Time 0.037101    
2023-01-06 17:09:00,403 - Epoch: [156][   30/  246]    Overall Loss 0.260035    Objective Loss 0.260035                                        LR 0.000008    Time 0.031206    
2023-01-06 17:09:00,597 - Epoch: [156][   40/  246]    Overall Loss 0.261459    Objective Loss 0.261459                                        LR 0.000008    Time 0.028270    
2023-01-06 17:09:00,794 - Epoch: [156][   50/  246]    Overall Loss 0.259178    Objective Loss 0.259178                                        LR 0.000008    Time 0.026533    
2023-01-06 17:09:00,989 - Epoch: [156][   60/  246]    Overall Loss 0.259475    Objective Loss 0.259475                                        LR 0.000008    Time 0.025367    
2023-01-06 17:09:01,185 - Epoch: [156][   70/  246]    Overall Loss 0.260133    Objective Loss 0.260133                                        LR 0.000008    Time 0.024536    
2023-01-06 17:09:01,382 - Epoch: [156][   80/  246]    Overall Loss 0.260025    Objective Loss 0.260025                                        LR 0.000008    Time 0.023918    
2023-01-06 17:09:01,587 - Epoch: [156][   90/  246]    Overall Loss 0.259197    Objective Loss 0.259197                                        LR 0.000008    Time 0.023538    
2023-01-06 17:09:01,806 - Epoch: [156][  100/  246]    Overall Loss 0.259595    Objective Loss 0.259595                                        LR 0.000008    Time 0.023372    
2023-01-06 17:09:02,025 - Epoch: [156][  110/  246]    Overall Loss 0.259768    Objective Loss 0.259768                                        LR 0.000008    Time 0.023236    
2023-01-06 17:09:02,244 - Epoch: [156][  120/  246]    Overall Loss 0.259828    Objective Loss 0.259828                                        LR 0.000008    Time 0.023116    
2023-01-06 17:09:02,463 - Epoch: [156][  130/  246]    Overall Loss 0.260004    Objective Loss 0.260004                                        LR 0.000008    Time 0.023019    
2023-01-06 17:09:02,681 - Epoch: [156][  140/  246]    Overall Loss 0.261163    Objective Loss 0.261163                                        LR 0.000008    Time 0.022931    
2023-01-06 17:09:02,895 - Epoch: [156][  150/  246]    Overall Loss 0.260134    Objective Loss 0.260134                                        LR 0.000008    Time 0.022827    
2023-01-06 17:09:03,112 - Epoch: [156][  160/  246]    Overall Loss 0.259958    Objective Loss 0.259958                                        LR 0.000008    Time 0.022750    
2023-01-06 17:09:03,331 - Epoch: [156][  170/  246]    Overall Loss 0.259039    Objective Loss 0.259039                                        LR 0.000008    Time 0.022701    
2023-01-06 17:09:03,551 - Epoch: [156][  180/  246]    Overall Loss 0.259824    Objective Loss 0.259824                                        LR 0.000008    Time 0.022661    
2023-01-06 17:09:03,770 - Epoch: [156][  190/  246]    Overall Loss 0.259954    Objective Loss 0.259954                                        LR 0.000008    Time 0.022616    
2023-01-06 17:09:03,989 - Epoch: [156][  200/  246]    Overall Loss 0.260198    Objective Loss 0.260198                                        LR 0.000008    Time 0.022579    
2023-01-06 17:09:04,209 - Epoch: [156][  210/  246]    Overall Loss 0.260155    Objective Loss 0.260155                                        LR 0.000008    Time 0.022546    
2023-01-06 17:09:04,428 - Epoch: [156][  220/  246]    Overall Loss 0.260014    Objective Loss 0.260014                                        LR 0.000008    Time 0.022517    
2023-01-06 17:09:04,653 - Epoch: [156][  230/  246]    Overall Loss 0.260534    Objective Loss 0.260534                                        LR 0.000008    Time 0.022516    
2023-01-06 17:09:04,888 - Epoch: [156][  240/  246]    Overall Loss 0.260660    Objective Loss 0.260660                                        LR 0.000008    Time 0.022554    
2023-01-06 17:09:04,999 - Epoch: [156][  246/  246]    Overall Loss 0.260873    Objective Loss 0.260873    Top1 90.669856    LR 0.000008    Time 0.022453    
2023-01-06 17:09:05,150 - --- validate (epoch=156)-----------
2023-01-06 17:09:05,150 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:05,605 - Epoch: [156][   10/   28]    Loss 0.266176    Top1 90.742188    
2023-01-06 17:09:05,717 - Epoch: [156][   20/   28]    Loss 0.264274    Top1 90.742188    
2023-01-06 17:09:05,785 - Epoch: [156][   28/   28]    Loss 0.272885    Top1 90.123103    
2023-01-06 17:09:05,942 - ==> Top1: 90.123    Loss: 0.273

2023-01-06 17:09:05,942 - ==> Confusion:
[[ 244   11  184]
 [  18  251  333]
 [  73   71 5801]]

2023-01-06 17:09:05,943 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:09:05,943 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:05,949 - 

2023-01-06 17:09:05,950 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:06,638 - Epoch: [157][   10/  246]    Overall Loss 0.252025    Objective Loss 0.252025                                        LR 0.000008    Time 0.068757    
2023-01-06 17:09:06,807 - Epoch: [157][   20/  246]    Overall Loss 0.253596    Objective Loss 0.253596                                        LR 0.000008    Time 0.042806    
2023-01-06 17:09:06,967 - Epoch: [157][   30/  246]    Overall Loss 0.254218    Objective Loss 0.254218                                        LR 0.000008    Time 0.033853    
2023-01-06 17:09:07,132 - Epoch: [157][   40/  246]    Overall Loss 0.255653    Objective Loss 0.255653                                        LR 0.000008    Time 0.029512    
2023-01-06 17:09:07,313 - Epoch: [157][   50/  246]    Overall Loss 0.257472    Objective Loss 0.257472                                        LR 0.000008    Time 0.027216    
2023-01-06 17:09:07,494 - Epoch: [157][   60/  246]    Overall Loss 0.259757    Objective Loss 0.259757                                        LR 0.000008    Time 0.025692    
2023-01-06 17:09:07,697 - Epoch: [157][   70/  246]    Overall Loss 0.260256    Objective Loss 0.260256                                        LR 0.000008    Time 0.024917    
2023-01-06 17:09:07,896 - Epoch: [157][   80/  246]    Overall Loss 0.259984    Objective Loss 0.259984                                        LR 0.000008    Time 0.024289    
2023-01-06 17:09:08,092 - Epoch: [157][   90/  246]    Overall Loss 0.259201    Objective Loss 0.259201                                        LR 0.000008    Time 0.023767    
2023-01-06 17:09:08,292 - Epoch: [157][  100/  246]    Overall Loss 0.260538    Objective Loss 0.260538                                        LR 0.000008    Time 0.023381    
2023-01-06 17:09:08,487 - Epoch: [157][  110/  246]    Overall Loss 0.262115    Objective Loss 0.262115                                        LR 0.000008    Time 0.023027    
2023-01-06 17:09:08,679 - Epoch: [157][  120/  246]    Overall Loss 0.261942    Objective Loss 0.261942                                        LR 0.000008    Time 0.022702    
2023-01-06 17:09:08,874 - Epoch: [157][  130/  246]    Overall Loss 0.262165    Objective Loss 0.262165                                        LR 0.000008    Time 0.022456    
2023-01-06 17:09:09,064 - Epoch: [157][  140/  246]    Overall Loss 0.261494    Objective Loss 0.261494                                        LR 0.000008    Time 0.022205    
2023-01-06 17:09:09,260 - Epoch: [157][  150/  246]    Overall Loss 0.261279    Objective Loss 0.261279                                        LR 0.000008    Time 0.022027    
2023-01-06 17:09:09,459 - Epoch: [157][  160/  246]    Overall Loss 0.261402    Objective Loss 0.261402                                        LR 0.000008    Time 0.021894    
2023-01-06 17:09:09,655 - Epoch: [157][  170/  246]    Overall Loss 0.261045    Objective Loss 0.261045                                        LR 0.000008    Time 0.021761    
2023-01-06 17:09:09,851 - Epoch: [157][  180/  246]    Overall Loss 0.261189    Objective Loss 0.261189                                        LR 0.000008    Time 0.021637    
2023-01-06 17:09:10,040 - Epoch: [157][  190/  246]    Overall Loss 0.261926    Objective Loss 0.261926                                        LR 0.000008    Time 0.021490    
2023-01-06 17:09:10,233 - Epoch: [157][  200/  246]    Overall Loss 0.261076    Objective Loss 0.261076                                        LR 0.000008    Time 0.021380    
2023-01-06 17:09:10,432 - Epoch: [157][  210/  246]    Overall Loss 0.261893    Objective Loss 0.261893                                        LR 0.000008    Time 0.021309    
2023-01-06 17:09:10,624 - Epoch: [157][  220/  246]    Overall Loss 0.261033    Objective Loss 0.261033                                        LR 0.000008    Time 0.021211    
2023-01-06 17:09:10,824 - Epoch: [157][  230/  246]    Overall Loss 0.261924    Objective Loss 0.261924                                        LR 0.000008    Time 0.021153    
2023-01-06 17:09:11,020 - Epoch: [157][  240/  246]    Overall Loss 0.261572    Objective Loss 0.261572                                        LR 0.000008    Time 0.021090    
2023-01-06 17:09:11,117 - Epoch: [157][  246/  246]    Overall Loss 0.261771    Objective Loss 0.261771    Top1 90.191388    LR 0.000008    Time 0.020968    
2023-01-06 17:09:11,261 - --- validate (epoch=157)-----------
2023-01-06 17:09:11,261 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:11,711 - Epoch: [157][   10/   28]    Loss 0.256946    Top1 90.898438    
2023-01-06 17:09:11,820 - Epoch: [157][   20/   28]    Loss 0.273417    Top1 89.902344    
2023-01-06 17:09:11,886 - Epoch: [157][   28/   28]    Loss 0.272597    Top1 90.022903    
2023-01-06 17:09:12,019 - ==> Top1: 90.023    Loss: 0.273

2023-01-06 17:09:12,020 - ==> Confusion:
[[ 234   12  193]
 [  18  230  354]
 [  67   53 5825]]

2023-01-06 17:09:12,021 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:09:12,021 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:12,027 - 

2023-01-06 17:09:12,027 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:12,632 - Epoch: [158][   10/  246]    Overall Loss 0.268107    Objective Loss 0.268107                                        LR 0.000008    Time 0.060340    
2023-01-06 17:09:12,866 - Epoch: [158][   20/  246]    Overall Loss 0.275253    Objective Loss 0.275253                                        LR 0.000008    Time 0.041847    
2023-01-06 17:09:13,066 - Epoch: [158][   30/  246]    Overall Loss 0.269795    Objective Loss 0.269795                                        LR 0.000008    Time 0.034562    
2023-01-06 17:09:13,259 - Epoch: [158][   40/  246]    Overall Loss 0.266962    Objective Loss 0.266962                                        LR 0.000008    Time 0.030733    
2023-01-06 17:09:13,459 - Epoch: [158][   50/  246]    Overall Loss 0.266831    Objective Loss 0.266831                                        LR 0.000008    Time 0.028589    
2023-01-06 17:09:13,660 - Epoch: [158][   60/  246]    Overall Loss 0.266379    Objective Loss 0.266379                                        LR 0.000008    Time 0.027158    
2023-01-06 17:09:13,859 - Epoch: [158][   70/  246]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.000008    Time 0.026126    
2023-01-06 17:09:14,063 - Epoch: [158][   80/  246]    Overall Loss 0.263203    Objective Loss 0.263203                                        LR 0.000008    Time 0.025402    
2023-01-06 17:09:14,261 - Epoch: [158][   90/  246]    Overall Loss 0.263850    Objective Loss 0.263850                                        LR 0.000008    Time 0.024780    
2023-01-06 17:09:14,452 - Epoch: [158][  100/  246]    Overall Loss 0.263513    Objective Loss 0.263513                                        LR 0.000008    Time 0.024205    
2023-01-06 17:09:14,624 - Epoch: [158][  110/  246]    Overall Loss 0.263114    Objective Loss 0.263114                                        LR 0.000008    Time 0.023556    
2023-01-06 17:09:14,795 - Epoch: [158][  120/  246]    Overall Loss 0.261627    Objective Loss 0.261627                                        LR 0.000008    Time 0.023008    
2023-01-06 17:09:14,963 - Epoch: [158][  130/  246]    Overall Loss 0.262066    Objective Loss 0.262066                                        LR 0.000008    Time 0.022517    
2023-01-06 17:09:15,159 - Epoch: [158][  140/  246]    Overall Loss 0.263222    Objective Loss 0.263222                                        LR 0.000008    Time 0.022298    
2023-01-06 17:09:15,350 - Epoch: [158][  150/  246]    Overall Loss 0.263606    Objective Loss 0.263606                                        LR 0.000008    Time 0.022085    
2023-01-06 17:09:15,547 - Epoch: [158][  160/  246]    Overall Loss 0.264373    Objective Loss 0.264373                                        LR 0.000008    Time 0.021930    
2023-01-06 17:09:15,742 - Epoch: [158][  170/  246]    Overall Loss 0.263238    Objective Loss 0.263238                                        LR 0.000008    Time 0.021785    
2023-01-06 17:09:15,949 - Epoch: [158][  180/  246]    Overall Loss 0.261966    Objective Loss 0.261966                                        LR 0.000008    Time 0.021727    
2023-01-06 17:09:16,152 - Epoch: [158][  190/  246]    Overall Loss 0.261736    Objective Loss 0.261736                                        LR 0.000008    Time 0.021641    
2023-01-06 17:09:16,360 - Epoch: [158][  200/  246]    Overall Loss 0.262154    Objective Loss 0.262154                                        LR 0.000008    Time 0.021597    
2023-01-06 17:09:16,564 - Epoch: [158][  210/  246]    Overall Loss 0.262116    Objective Loss 0.262116                                        LR 0.000008    Time 0.021534    
2023-01-06 17:09:16,760 - Epoch: [158][  220/  246]    Overall Loss 0.261419    Objective Loss 0.261419                                        LR 0.000008    Time 0.021444    
2023-01-06 17:09:16,960 - Epoch: [158][  230/  246]    Overall Loss 0.261377    Objective Loss 0.261377                                        LR 0.000008    Time 0.021383    
2023-01-06 17:09:17,175 - Epoch: [158][  240/  246]    Overall Loss 0.261826    Objective Loss 0.261826                                        LR 0.000008    Time 0.021385    
2023-01-06 17:09:17,270 - Epoch: [158][  246/  246]    Overall Loss 0.261842    Objective Loss 0.261842    Top1 90.191388    LR 0.000008    Time 0.021247    
2023-01-06 17:09:17,411 - --- validate (epoch=158)-----------
2023-01-06 17:09:17,412 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:17,997 - Epoch: [158][   10/   28]    Loss 0.289707    Top1 89.570312    
2023-01-06 17:09:18,109 - Epoch: [158][   20/   28]    Loss 0.272651    Top1 90.136719    
2023-01-06 17:09:18,175 - Epoch: [158][   28/   28]    Loss 0.271810    Top1 90.237618    
2023-01-06 17:09:18,333 - ==> Top1: 90.238    Loss: 0.272

2023-01-06 17:09:18,334 - ==> Confusion:
[[ 243   14  182]
 [  18  254  330]
 [  71   67 5807]]

2023-01-06 17:09:18,335 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:09:18,335 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:18,341 - 

2023-01-06 17:09:18,341 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:18,924 - Epoch: [159][   10/  246]    Overall Loss 0.267060    Objective Loss 0.267060                                        LR 0.000008    Time 0.058255    
2023-01-06 17:09:19,104 - Epoch: [159][   20/  246]    Overall Loss 0.260168    Objective Loss 0.260168                                        LR 0.000008    Time 0.038105    
2023-01-06 17:09:19,304 - Epoch: [159][   30/  246]    Overall Loss 0.263447    Objective Loss 0.263447                                        LR 0.000008    Time 0.032037    
2023-01-06 17:09:19,501 - Epoch: [159][   40/  246]    Overall Loss 0.260827    Objective Loss 0.260827                                        LR 0.000008    Time 0.028954    
2023-01-06 17:09:19,697 - Epoch: [159][   50/  246]    Overall Loss 0.262319    Objective Loss 0.262319                                        LR 0.000008    Time 0.027073    
2023-01-06 17:09:19,899 - Epoch: [159][   60/  246]    Overall Loss 0.260923    Objective Loss 0.260923                                        LR 0.000008    Time 0.025926    
2023-01-06 17:09:20,097 - Epoch: [159][   70/  246]    Overall Loss 0.261896    Objective Loss 0.261896                                        LR 0.000008    Time 0.025009    
2023-01-06 17:09:20,298 - Epoch: [159][   80/  246]    Overall Loss 0.261277    Objective Loss 0.261277                                        LR 0.000008    Time 0.024401    
2023-01-06 17:09:20,488 - Epoch: [159][   90/  246]    Overall Loss 0.262395    Objective Loss 0.262395                                        LR 0.000008    Time 0.023793    
2023-01-06 17:09:20,680 - Epoch: [159][  100/  246]    Overall Loss 0.260226    Objective Loss 0.260226                                        LR 0.000008    Time 0.023327    
2023-01-06 17:09:20,871 - Epoch: [159][  110/  246]    Overall Loss 0.259139    Objective Loss 0.259139                                        LR 0.000008    Time 0.022944    
2023-01-06 17:09:21,061 - Epoch: [159][  120/  246]    Overall Loss 0.258648    Objective Loss 0.258648                                        LR 0.000008    Time 0.022611    
2023-01-06 17:09:21,257 - Epoch: [159][  130/  246]    Overall Loss 0.260171    Objective Loss 0.260171                                        LR 0.000008    Time 0.022372    
2023-01-06 17:09:21,454 - Epoch: [159][  140/  246]    Overall Loss 0.259967    Objective Loss 0.259967                                        LR 0.000008    Time 0.022184    
2023-01-06 17:09:21,650 - Epoch: [159][  150/  246]    Overall Loss 0.259508    Objective Loss 0.259508                                        LR 0.000008    Time 0.022005    
2023-01-06 17:09:21,844 - Epoch: [159][  160/  246]    Overall Loss 0.260516    Objective Loss 0.260516                                        LR 0.000008    Time 0.021841    
2023-01-06 17:09:22,026 - Epoch: [159][  170/  246]    Overall Loss 0.260426    Objective Loss 0.260426                                        LR 0.000008    Time 0.021628    
2023-01-06 17:09:22,225 - Epoch: [159][  180/  246]    Overall Loss 0.260613    Objective Loss 0.260613                                        LR 0.000008    Time 0.021525    
2023-01-06 17:09:22,424 - Epoch: [159][  190/  246]    Overall Loss 0.259682    Objective Loss 0.259682                                        LR 0.000008    Time 0.021438    
2023-01-06 17:09:22,638 - Epoch: [159][  200/  246]    Overall Loss 0.259963    Objective Loss 0.259963                                        LR 0.000008    Time 0.021433    
2023-01-06 17:09:22,875 - Epoch: [159][  210/  246]    Overall Loss 0.260703    Objective Loss 0.260703                                        LR 0.000008    Time 0.021543    
2023-01-06 17:09:23,098 - Epoch: [159][  220/  246]    Overall Loss 0.260023    Objective Loss 0.260023                                        LR 0.000008    Time 0.021572    
2023-01-06 17:09:23,307 - Epoch: [159][  230/  246]    Overall Loss 0.260268    Objective Loss 0.260268                                        LR 0.000008    Time 0.021540    
2023-01-06 17:09:23,498 - Epoch: [159][  240/  246]    Overall Loss 0.260553    Objective Loss 0.260553                                        LR 0.000008    Time 0.021441    
2023-01-06 17:09:23,592 - Epoch: [159][  246/  246]    Overall Loss 0.260113    Objective Loss 0.260113    Top1 92.344498    LR 0.000008    Time 0.021298    
2023-01-06 17:09:23,714 - --- validate (epoch=159)-----------
2023-01-06 17:09:23,714 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:24,158 - Epoch: [159][   10/   28]    Loss 0.281489    Top1 89.804688    
2023-01-06 17:09:24,267 - Epoch: [159][   20/   28]    Loss 0.279426    Top1 89.960938    
2023-01-06 17:09:24,332 - Epoch: [159][   28/   28]    Loss 0.269689    Top1 90.251932    
2023-01-06 17:09:24,477 - ==> Top1: 90.252    Loss: 0.270

2023-01-06 17:09:24,477 - ==> Confusion:
[[ 227   12  200]
 [  13  249  340]
 [  53   63 5829]]

2023-01-06 17:09:24,479 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:09:24,479 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:24,485 - 

2023-01-06 17:09:24,485 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:25,205 - Epoch: [160][   10/  246]    Overall Loss 0.267628    Objective Loss 0.267628                                        LR 0.000008    Time 0.071971    
2023-01-06 17:09:25,407 - Epoch: [160][   20/  246]    Overall Loss 0.263237    Objective Loss 0.263237                                        LR 0.000008    Time 0.046075    
2023-01-06 17:09:25,603 - Epoch: [160][   30/  246]    Overall Loss 0.262250    Objective Loss 0.262250                                        LR 0.000008    Time 0.037212    
2023-01-06 17:09:25,806 - Epoch: [160][   40/  246]    Overall Loss 0.258685    Objective Loss 0.258685                                        LR 0.000008    Time 0.032976    
2023-01-06 17:09:26,001 - Epoch: [160][   50/  246]    Overall Loss 0.258215    Objective Loss 0.258215                                        LR 0.000008    Time 0.030271    
2023-01-06 17:09:26,203 - Epoch: [160][   60/  246]    Overall Loss 0.259263    Objective Loss 0.259263                                        LR 0.000008    Time 0.028593    
2023-01-06 17:09:26,399 - Epoch: [160][   70/  246]    Overall Loss 0.261117    Objective Loss 0.261117                                        LR 0.000008    Time 0.027304    
2023-01-06 17:09:26,603 - Epoch: [160][   80/  246]    Overall Loss 0.263470    Objective Loss 0.263470                                        LR 0.000008    Time 0.026435    
2023-01-06 17:09:26,799 - Epoch: [160][   90/  246]    Overall Loss 0.263148    Objective Loss 0.263148                                        LR 0.000008    Time 0.025677    
2023-01-06 17:09:27,003 - Epoch: [160][  100/  246]    Overall Loss 0.262908    Objective Loss 0.262908                                        LR 0.000008    Time 0.025139    
2023-01-06 17:09:27,197 - Epoch: [160][  110/  246]    Overall Loss 0.262934    Objective Loss 0.262934                                        LR 0.000008    Time 0.024614    
2023-01-06 17:09:27,400 - Epoch: [160][  120/  246]    Overall Loss 0.262201    Objective Loss 0.262201                                        LR 0.000008    Time 0.024253    
2023-01-06 17:09:27,596 - Epoch: [160][  130/  246]    Overall Loss 0.262586    Objective Loss 0.262586                                        LR 0.000008    Time 0.023891    
2023-01-06 17:09:27,795 - Epoch: [160][  140/  246]    Overall Loss 0.262266    Objective Loss 0.262266                                        LR 0.000008    Time 0.023607    
2023-01-06 17:09:27,990 - Epoch: [160][  150/  246]    Overall Loss 0.262406    Objective Loss 0.262406                                        LR 0.000008    Time 0.023331    
2023-01-06 17:09:28,180 - Epoch: [160][  160/  246]    Overall Loss 0.262880    Objective Loss 0.262880                                        LR 0.000008    Time 0.023053    
2023-01-06 17:09:28,372 - Epoch: [160][  170/  246]    Overall Loss 0.261828    Objective Loss 0.261828                                        LR 0.000008    Time 0.022828    
2023-01-06 17:09:28,555 - Epoch: [160][  180/  246]    Overall Loss 0.261496    Objective Loss 0.261496                                        LR 0.000008    Time 0.022572    
2023-01-06 17:09:28,739 - Epoch: [160][  190/  246]    Overall Loss 0.261480    Objective Loss 0.261480                                        LR 0.000008    Time 0.022354    
2023-01-06 17:09:28,930 - Epoch: [160][  200/  246]    Overall Loss 0.260893    Objective Loss 0.260893                                        LR 0.000008    Time 0.022185    
2023-01-06 17:09:29,126 - Epoch: [160][  210/  246]    Overall Loss 0.260467    Objective Loss 0.260467                                        LR 0.000008    Time 0.022062    
2023-01-06 17:09:29,317 - Epoch: [160][  220/  246]    Overall Loss 0.260292    Objective Loss 0.260292                                        LR 0.000008    Time 0.021928    
2023-01-06 17:09:29,516 - Epoch: [160][  230/  246]    Overall Loss 0.259521    Objective Loss 0.259521                                        LR 0.000008    Time 0.021838    
2023-01-06 17:09:29,721 - Epoch: [160][  240/  246]    Overall Loss 0.259606    Objective Loss 0.259606                                        LR 0.000008    Time 0.021781    
2023-01-06 17:09:29,813 - Epoch: [160][  246/  246]    Overall Loss 0.259487    Objective Loss 0.259487    Top1 92.822967    LR 0.000008    Time 0.021621    
2023-01-06 17:09:29,963 - --- validate (epoch=160)-----------
2023-01-06 17:09:29,963 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:30,447 - Epoch: [160][   10/   28]    Loss 0.276095    Top1 89.765625    
2023-01-06 17:09:30,579 - Epoch: [160][   20/   28]    Loss 0.272522    Top1 89.980469    
2023-01-06 17:09:30,645 - Epoch: [160][   28/   28]    Loss 0.269999    Top1 90.194675    
2023-01-06 17:09:30,794 - ==> Top1: 90.195    Loss: 0.270

2023-01-06 17:09:30,795 - ==> Confusion:
[[ 238   12  189]
 [  18  239  345]
 [  63   58 5824]]

2023-01-06 17:09:30,796 - ==> Best [Top1: 90.295   Sparsity:0.00   Params: 155168 on epoch: 149]
2023-01-06 17:09:30,796 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:30,802 - 

2023-01-06 17:09:30,802 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:31,378 - Epoch: [161][   10/  246]    Overall Loss 0.262107    Objective Loss 0.262107                                        LR 0.000008    Time 0.057508    
2023-01-06 17:09:31,577 - Epoch: [161][   20/  246]    Overall Loss 0.247571    Objective Loss 0.247571                                        LR 0.000008    Time 0.038615    
2023-01-06 17:09:31,772 - Epoch: [161][   30/  246]    Overall Loss 0.255093    Objective Loss 0.255093                                        LR 0.000008    Time 0.032256    
2023-01-06 17:09:31,977 - Epoch: [161][   40/  246]    Overall Loss 0.254087    Objective Loss 0.254087                                        LR 0.000008    Time 0.029288    
2023-01-06 17:09:32,167 - Epoch: [161][   50/  246]    Overall Loss 0.256227    Objective Loss 0.256227                                        LR 0.000008    Time 0.027236    
2023-01-06 17:09:32,358 - Epoch: [161][   60/  246]    Overall Loss 0.257043    Objective Loss 0.257043                                        LR 0.000008    Time 0.025875    
2023-01-06 17:09:32,553 - Epoch: [161][   70/  246]    Overall Loss 0.258860    Objective Loss 0.258860                                        LR 0.000008    Time 0.024957    
2023-01-06 17:09:32,746 - Epoch: [161][   80/  246]    Overall Loss 0.257249    Objective Loss 0.257249                                        LR 0.000008    Time 0.024247    
2023-01-06 17:09:32,941 - Epoch: [161][   90/  246]    Overall Loss 0.258700    Objective Loss 0.258700                                        LR 0.000008    Time 0.023711    
2023-01-06 17:09:33,135 - Epoch: [161][  100/  246]    Overall Loss 0.256879    Objective Loss 0.256879                                        LR 0.000008    Time 0.023274    
2023-01-06 17:09:33,330 - Epoch: [161][  110/  246]    Overall Loss 0.259577    Objective Loss 0.259577                                        LR 0.000008    Time 0.022934    
2023-01-06 17:09:33,554 - Epoch: [161][  120/  246]    Overall Loss 0.259242    Objective Loss 0.259242                                        LR 0.000008    Time 0.022881    
2023-01-06 17:09:33,768 - Epoch: [161][  130/  246]    Overall Loss 0.260085    Objective Loss 0.260085                                        LR 0.000008    Time 0.022763    
2023-01-06 17:09:33,982 - Epoch: [161][  140/  246]    Overall Loss 0.260124    Objective Loss 0.260124                                        LR 0.000008    Time 0.022666    
2023-01-06 17:09:34,208 - Epoch: [161][  150/  246]    Overall Loss 0.261131    Objective Loss 0.261131                                        LR 0.000008    Time 0.022654    
2023-01-06 17:09:34,427 - Epoch: [161][  160/  246]    Overall Loss 0.261723    Objective Loss 0.261723                                        LR 0.000008    Time 0.022607    
2023-01-06 17:09:34,646 - Epoch: [161][  170/  246]    Overall Loss 0.261375    Objective Loss 0.261375                                        LR 0.000008    Time 0.022564    
2023-01-06 17:09:34,865 - Epoch: [161][  180/  246]    Overall Loss 0.260352    Objective Loss 0.260352                                        LR 0.000008    Time 0.022523    
2023-01-06 17:09:35,081 - Epoch: [161][  190/  246]    Overall Loss 0.260455    Objective Loss 0.260455                                        LR 0.000008    Time 0.022471    
2023-01-06 17:09:35,292 - Epoch: [161][  200/  246]    Overall Loss 0.260130    Objective Loss 0.260130                                        LR 0.000008    Time 0.022400    
2023-01-06 17:09:35,506 - Epoch: [161][  210/  246]    Overall Loss 0.259189    Objective Loss 0.259189                                        LR 0.000008    Time 0.022351    
2023-01-06 17:09:35,724 - Epoch: [161][  220/  246]    Overall Loss 0.259410    Objective Loss 0.259410                                        LR 0.000008    Time 0.022324    
2023-01-06 17:09:35,941 - Epoch: [161][  230/  246]    Overall Loss 0.259378    Objective Loss 0.259378                                        LR 0.000008    Time 0.022298    
2023-01-06 17:09:36,173 - Epoch: [161][  240/  246]    Overall Loss 0.259248    Objective Loss 0.259248                                        LR 0.000008    Time 0.022333    
2023-01-06 17:09:36,287 - Epoch: [161][  246/  246]    Overall Loss 0.259252    Objective Loss 0.259252    Top1 91.626794    LR 0.000008    Time 0.022248    
2023-01-06 17:09:36,426 - --- validate (epoch=161)-----------
2023-01-06 17:09:36,427 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:36,888 - Epoch: [161][   10/   28]    Loss 0.284412    Top1 89.765625    
2023-01-06 17:09:36,998 - Epoch: [161][   20/   28]    Loss 0.281454    Top1 89.980469    
2023-01-06 17:09:37,064 - Epoch: [161][   28/   28]    Loss 0.274390    Top1 90.352133    
2023-01-06 17:09:37,232 - ==> Top1: 90.352    Loss: 0.274

2023-01-06 17:09:37,232 - ==> Confusion:
[[ 258   12  169]
 [  21  260  321]
 [  83   68 5794]]

2023-01-06 17:09:37,233 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:09:37,233 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:37,241 - 

2023-01-06 17:09:37,241 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:37,983 - Epoch: [162][   10/  246]    Overall Loss 0.266391    Objective Loss 0.266391                                        LR 0.000008    Time 0.074130    
2023-01-06 17:09:38,175 - Epoch: [162][   20/  246]    Overall Loss 0.267204    Objective Loss 0.267204                                        LR 0.000008    Time 0.046572    
2023-01-06 17:09:38,371 - Epoch: [162][   30/  246]    Overall Loss 0.262227    Objective Loss 0.262227                                        LR 0.000008    Time 0.037571    
2023-01-06 17:09:38,568 - Epoch: [162][   40/  246]    Overall Loss 0.257945    Objective Loss 0.257945                                        LR 0.000008    Time 0.033093    
2023-01-06 17:09:38,763 - Epoch: [162][   50/  246]    Overall Loss 0.258892    Objective Loss 0.258892                                        LR 0.000008    Time 0.030370    
2023-01-06 17:09:38,957 - Epoch: [162][   60/  246]    Overall Loss 0.259743    Objective Loss 0.259743                                        LR 0.000008    Time 0.028523    
2023-01-06 17:09:39,153 - Epoch: [162][   70/  246]    Overall Loss 0.258916    Objective Loss 0.258916                                        LR 0.000008    Time 0.027248    
2023-01-06 17:09:39,343 - Epoch: [162][   80/  246]    Overall Loss 0.261031    Objective Loss 0.261031                                        LR 0.000008    Time 0.026206    
2023-01-06 17:09:39,539 - Epoch: [162][   90/  246]    Overall Loss 0.262232    Objective Loss 0.262232                                        LR 0.000008    Time 0.025476    
2023-01-06 17:09:39,735 - Epoch: [162][  100/  246]    Overall Loss 0.260480    Objective Loss 0.260480                                        LR 0.000008    Time 0.024885    
2023-01-06 17:09:39,932 - Epoch: [162][  110/  246]    Overall Loss 0.260950    Objective Loss 0.260950                                        LR 0.000008    Time 0.024410    
2023-01-06 17:09:40,129 - Epoch: [162][  120/  246]    Overall Loss 0.260781    Objective Loss 0.260781                                        LR 0.000008    Time 0.024014    
2023-01-06 17:09:40,325 - Epoch: [162][  130/  246]    Overall Loss 0.260660    Objective Loss 0.260660                                        LR 0.000008    Time 0.023666    
2023-01-06 17:09:40,521 - Epoch: [162][  140/  246]    Overall Loss 0.260311    Objective Loss 0.260311                                        LR 0.000008    Time 0.023373    
2023-01-06 17:09:40,716 - Epoch: [162][  150/  246]    Overall Loss 0.259968    Objective Loss 0.259968                                        LR 0.000008    Time 0.023117    
2023-01-06 17:09:40,912 - Epoch: [162][  160/  246]    Overall Loss 0.260598    Objective Loss 0.260598                                        LR 0.000008    Time 0.022893    
2023-01-06 17:09:41,109 - Epoch: [162][  170/  246]    Overall Loss 0.260287    Objective Loss 0.260287                                        LR 0.000008    Time 0.022700    
2023-01-06 17:09:41,305 - Epoch: [162][  180/  246]    Overall Loss 0.260078    Objective Loss 0.260078                                        LR 0.000008    Time 0.022525    
2023-01-06 17:09:41,500 - Epoch: [162][  190/  246]    Overall Loss 0.259505    Objective Loss 0.259505                                        LR 0.000008    Time 0.022366    
2023-01-06 17:09:41,695 - Epoch: [162][  200/  246]    Overall Loss 0.259340    Objective Loss 0.259340                                        LR 0.000008    Time 0.022223    
2023-01-06 17:09:41,891 - Epoch: [162][  210/  246]    Overall Loss 0.258799    Objective Loss 0.258799                                        LR 0.000008    Time 0.022095    
2023-01-06 17:09:42,087 - Epoch: [162][  220/  246]    Overall Loss 0.258630    Objective Loss 0.258630                                        LR 0.000008    Time 0.021978    
2023-01-06 17:09:42,283 - Epoch: [162][  230/  246]    Overall Loss 0.259052    Objective Loss 0.259052                                        LR 0.000008    Time 0.021871    
2023-01-06 17:09:42,492 - Epoch: [162][  240/  246]    Overall Loss 0.259420    Objective Loss 0.259420                                        LR 0.000008    Time 0.021832    
2023-01-06 17:09:42,589 - Epoch: [162][  246/  246]    Overall Loss 0.259560    Objective Loss 0.259560    Top1 88.995215    LR 0.000008    Time 0.021692    
2023-01-06 17:09:42,726 - --- validate (epoch=162)-----------
2023-01-06 17:09:42,727 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:43,180 - Epoch: [162][   10/   28]    Loss 0.264707    Top1 90.507812    
2023-01-06 17:09:43,293 - Epoch: [162][   20/   28]    Loss 0.271473    Top1 90.195312    
2023-01-06 17:09:43,360 - Epoch: [162][   28/   28]    Loss 0.269887    Top1 90.294875    
2023-01-06 17:09:43,489 - ==> Top1: 90.295    Loss: 0.270

2023-01-06 17:09:43,489 - ==> Confusion:
[[ 247   16  176]
 [  19  265  318]
 [  72   77 5796]]

2023-01-06 17:09:43,491 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:09:43,491 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:43,497 - 

2023-01-06 17:09:43,497 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:44,234 - Epoch: [163][   10/  246]    Overall Loss 0.240733    Objective Loss 0.240733                                        LR 0.000008    Time 0.073638    
2023-01-06 17:09:44,449 - Epoch: [163][   20/  246]    Overall Loss 0.250793    Objective Loss 0.250793                                        LR 0.000008    Time 0.047574    
2023-01-06 17:09:44,669 - Epoch: [163][   30/  246]    Overall Loss 0.253325    Objective Loss 0.253325                                        LR 0.000008    Time 0.039024    
2023-01-06 17:09:44,889 - Epoch: [163][   40/  246]    Overall Loss 0.259085    Objective Loss 0.259085                                        LR 0.000008    Time 0.034760    
2023-01-06 17:09:45,110 - Epoch: [163][   50/  246]    Overall Loss 0.254598    Objective Loss 0.254598                                        LR 0.000008    Time 0.032209    
2023-01-06 17:09:45,333 - Epoch: [163][   60/  246]    Overall Loss 0.253469    Objective Loss 0.253469                                        LR 0.000008    Time 0.030557    
2023-01-06 17:09:45,555 - Epoch: [163][   70/  246]    Overall Loss 0.253330    Objective Loss 0.253330                                        LR 0.000008    Time 0.029351    
2023-01-06 17:09:45,778 - Epoch: [163][   80/  246]    Overall Loss 0.254145    Objective Loss 0.254145                                        LR 0.000008    Time 0.028473    
2023-01-06 17:09:45,986 - Epoch: [163][   90/  246]    Overall Loss 0.254170    Objective Loss 0.254170                                        LR 0.000008    Time 0.027606    
2023-01-06 17:09:46,178 - Epoch: [163][  100/  246]    Overall Loss 0.254638    Objective Loss 0.254638                                        LR 0.000008    Time 0.026768    
2023-01-06 17:09:46,370 - Epoch: [163][  110/  246]    Overall Loss 0.255483    Objective Loss 0.255483                                        LR 0.000008    Time 0.026071    
2023-01-06 17:09:46,563 - Epoch: [163][  120/  246]    Overall Loss 0.255834    Objective Loss 0.255834                                        LR 0.000008    Time 0.025505    
2023-01-06 17:09:46,754 - Epoch: [163][  130/  246]    Overall Loss 0.255607    Objective Loss 0.255607                                        LR 0.000008    Time 0.025012    
2023-01-06 17:09:46,947 - Epoch: [163][  140/  246]    Overall Loss 0.256534    Objective Loss 0.256534                                        LR 0.000008    Time 0.024601    
2023-01-06 17:09:47,139 - Epoch: [163][  150/  246]    Overall Loss 0.257934    Objective Loss 0.257934                                        LR 0.000008    Time 0.024235    
2023-01-06 17:09:47,331 - Epoch: [163][  160/  246]    Overall Loss 0.259094    Objective Loss 0.259094                                        LR 0.000008    Time 0.023922    
2023-01-06 17:09:47,522 - Epoch: [163][  170/  246]    Overall Loss 0.259361    Objective Loss 0.259361                                        LR 0.000008    Time 0.023635    
2023-01-06 17:09:47,712 - Epoch: [163][  180/  246]    Overall Loss 0.258978    Objective Loss 0.258978                                        LR 0.000008    Time 0.023373    
2023-01-06 17:09:47,899 - Epoch: [163][  190/  246]    Overall Loss 0.259576    Objective Loss 0.259576                                        LR 0.000008    Time 0.023130    
2023-01-06 17:09:48,089 - Epoch: [163][  200/  246]    Overall Loss 0.259857    Objective Loss 0.259857                                        LR 0.000008    Time 0.022920    
2023-01-06 17:09:48,277 - Epoch: [163][  210/  246]    Overall Loss 0.260517    Objective Loss 0.260517                                        LR 0.000008    Time 0.022722    
2023-01-06 17:09:48,466 - Epoch: [163][  220/  246]    Overall Loss 0.260661    Objective Loss 0.260661                                        LR 0.000008    Time 0.022547    
2023-01-06 17:09:48,654 - Epoch: [163][  230/  246]    Overall Loss 0.260357    Objective Loss 0.260357                                        LR 0.000008    Time 0.022382    
2023-01-06 17:09:48,856 - Epoch: [163][  240/  246]    Overall Loss 0.260571    Objective Loss 0.260571                                        LR 0.000008    Time 0.022290    
2023-01-06 17:09:48,952 - Epoch: [163][  246/  246]    Overall Loss 0.259973    Objective Loss 0.259973    Top1 92.344498    LR 0.000008    Time 0.022133    
2023-01-06 17:09:49,113 - --- validate (epoch=163)-----------
2023-01-06 17:09:49,113 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:49,565 - Epoch: [163][   10/   28]    Loss 0.297773    Top1 89.218750    
2023-01-06 17:09:49,679 - Epoch: [163][   20/   28]    Loss 0.278993    Top1 89.960938    
2023-01-06 17:09:49,744 - Epoch: [163][   28/   28]    Loss 0.272695    Top1 90.180361    
2023-01-06 17:09:49,888 - ==> Top1: 90.180    Loss: 0.273

2023-01-06 17:09:49,888 - ==> Confusion:
[[ 228   14  197]
 [  15  242  345]
 [  57   58 5830]]

2023-01-06 17:09:49,890 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:09:49,890 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:49,896 - 

2023-01-06 17:09:49,896 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:50,459 - Epoch: [164][   10/  246]    Overall Loss 0.261932    Objective Loss 0.261932                                        LR 0.000008    Time 0.056150    
2023-01-06 17:09:50,656 - Epoch: [164][   20/  246]    Overall Loss 0.250888    Objective Loss 0.250888                                        LR 0.000008    Time 0.037935    
2023-01-06 17:09:50,858 - Epoch: [164][   30/  246]    Overall Loss 0.254725    Objective Loss 0.254725                                        LR 0.000008    Time 0.032016    
2023-01-06 17:09:51,050 - Epoch: [164][   40/  246]    Overall Loss 0.263835    Objective Loss 0.263835                                        LR 0.000008    Time 0.028748    
2023-01-06 17:09:51,251 - Epoch: [164][   50/  246]    Overall Loss 0.263451    Objective Loss 0.263451                                        LR 0.000008    Time 0.027010    
2023-01-06 17:09:51,449 - Epoch: [164][   60/  246]    Overall Loss 0.262651    Objective Loss 0.262651                                        LR 0.000008    Time 0.025800    
2023-01-06 17:09:51,650 - Epoch: [164][   70/  246]    Overall Loss 0.261501    Objective Loss 0.261501                                        LR 0.000008    Time 0.024978    
2023-01-06 17:09:51,847 - Epoch: [164][   80/  246]    Overall Loss 0.259547    Objective Loss 0.259547                                        LR 0.000008    Time 0.024315    
2023-01-06 17:09:52,048 - Epoch: [164][   90/  246]    Overall Loss 0.258336    Objective Loss 0.258336                                        LR 0.000008    Time 0.023847    
2023-01-06 17:09:52,243 - Epoch: [164][  100/  246]    Overall Loss 0.258114    Objective Loss 0.258114                                        LR 0.000008    Time 0.023405    
2023-01-06 17:09:52,442 - Epoch: [164][  110/  246]    Overall Loss 0.258733    Objective Loss 0.258733                                        LR 0.000008    Time 0.023089    
2023-01-06 17:09:52,639 - Epoch: [164][  120/  246]    Overall Loss 0.257128    Objective Loss 0.257128                                        LR 0.000008    Time 0.022798    
2023-01-06 17:09:52,838 - Epoch: [164][  130/  246]    Overall Loss 0.257080    Objective Loss 0.257080                                        LR 0.000008    Time 0.022579    
2023-01-06 17:09:53,037 - Epoch: [164][  140/  246]    Overall Loss 0.257677    Objective Loss 0.257677                                        LR 0.000008    Time 0.022379    
2023-01-06 17:09:53,240 - Epoch: [164][  150/  246]    Overall Loss 0.256994    Objective Loss 0.256994                                        LR 0.000008    Time 0.022240    
2023-01-06 17:09:53,439 - Epoch: [164][  160/  246]    Overall Loss 0.257479    Objective Loss 0.257479                                        LR 0.000008    Time 0.022092    
2023-01-06 17:09:53,640 - Epoch: [164][  170/  246]    Overall Loss 0.257677    Objective Loss 0.257677                                        LR 0.000008    Time 0.021975    
2023-01-06 17:09:53,840 - Epoch: [164][  180/  246]    Overall Loss 0.258140    Objective Loss 0.258140                                        LR 0.000008    Time 0.021859    
2023-01-06 17:09:54,044 - Epoch: [164][  190/  246]    Overall Loss 0.258656    Objective Loss 0.258656                                        LR 0.000008    Time 0.021781    
2023-01-06 17:09:54,240 - Epoch: [164][  200/  246]    Overall Loss 0.259959    Objective Loss 0.259959                                        LR 0.000008    Time 0.021673    
2023-01-06 17:09:54,442 - Epoch: [164][  210/  246]    Overall Loss 0.259682    Objective Loss 0.259682                                        LR 0.000008    Time 0.021598    
2023-01-06 17:09:54,640 - Epoch: [164][  220/  246]    Overall Loss 0.259345    Objective Loss 0.259345                                        LR 0.000008    Time 0.021518    
2023-01-06 17:09:54,843 - Epoch: [164][  230/  246]    Overall Loss 0.259416    Objective Loss 0.259416                                        LR 0.000008    Time 0.021461    
2023-01-06 17:09:55,056 - Epoch: [164][  240/  246]    Overall Loss 0.259078    Objective Loss 0.259078                                        LR 0.000008    Time 0.021451    
2023-01-06 17:09:55,150 - Epoch: [164][  246/  246]    Overall Loss 0.258800    Objective Loss 0.258800    Top1 90.909091    LR 0.000008    Time 0.021312    
2023-01-06 17:09:55,292 - --- validate (epoch=164)-----------
2023-01-06 17:09:55,292 - 6986 samples (256 per mini-batch)
2023-01-06 17:09:55,743 - Epoch: [164][   10/   28]    Loss 0.270488    Top1 90.664062    
2023-01-06 17:09:55,858 - Epoch: [164][   20/   28]    Loss 0.268367    Top1 90.351562    
2023-01-06 17:09:55,924 - Epoch: [164][   28/   28]    Loss 0.270340    Top1 90.166046    
2023-01-06 17:09:56,071 - ==> Top1: 90.166    Loss: 0.270

2023-01-06 17:09:56,072 - ==> Confusion:
[[ 243   12  184]
 [  18  252  332]
 [  76   65 5804]]

2023-01-06 17:09:56,073 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:09:56,073 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:09:56,079 - 

2023-01-06 17:09:56,079 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:09:56,776 - Epoch: [165][   10/  246]    Overall Loss 0.249832    Objective Loss 0.249832                                        LR 0.000008    Time 0.069604    
2023-01-06 17:09:56,966 - Epoch: [165][   20/  246]    Overall Loss 0.255761    Objective Loss 0.255761                                        LR 0.000008    Time 0.044284    
2023-01-06 17:09:57,157 - Epoch: [165][   30/  246]    Overall Loss 0.254494    Objective Loss 0.254494                                        LR 0.000008    Time 0.035880    
2023-01-06 17:09:57,347 - Epoch: [165][   40/  246]    Overall Loss 0.255552    Objective Loss 0.255552                                        LR 0.000008    Time 0.031658    
2023-01-06 17:09:57,538 - Epoch: [165][   50/  246]    Overall Loss 0.255005    Objective Loss 0.255005                                        LR 0.000008    Time 0.029130    
2023-01-06 17:09:57,727 - Epoch: [165][   60/  246]    Overall Loss 0.252556    Objective Loss 0.252556                                        LR 0.000008    Time 0.027430    
2023-01-06 17:09:57,918 - Epoch: [165][   70/  246]    Overall Loss 0.251603    Objective Loss 0.251603                                        LR 0.000008    Time 0.026224    
2023-01-06 17:09:58,105 - Epoch: [165][   80/  246]    Overall Loss 0.252577    Objective Loss 0.252577                                        LR 0.000008    Time 0.025282    
2023-01-06 17:09:58,282 - Epoch: [165][   90/  246]    Overall Loss 0.254192    Objective Loss 0.254192                                        LR 0.000008    Time 0.024439    
2023-01-06 17:09:58,467 - Epoch: [165][  100/  246]    Overall Loss 0.254605    Objective Loss 0.254605                                        LR 0.000008    Time 0.023837    
2023-01-06 17:09:58,648 - Epoch: [165][  110/  246]    Overall Loss 0.255393    Objective Loss 0.255393                                        LR 0.000008    Time 0.023315    
2023-01-06 17:09:58,836 - Epoch: [165][  120/  246]    Overall Loss 0.256120    Objective Loss 0.256120                                        LR 0.000008    Time 0.022937    
2023-01-06 17:09:59,025 - Epoch: [165][  130/  246]    Overall Loss 0.256478    Objective Loss 0.256478                                        LR 0.000008    Time 0.022624    
2023-01-06 17:09:59,216 - Epoch: [165][  140/  246]    Overall Loss 0.257132    Objective Loss 0.257132                                        LR 0.000008    Time 0.022371    
2023-01-06 17:09:59,407 - Epoch: [165][  150/  246]    Overall Loss 0.257897    Objective Loss 0.257897                                        LR 0.000008    Time 0.022150    
2023-01-06 17:09:59,598 - Epoch: [165][  160/  246]    Overall Loss 0.257729    Objective Loss 0.257729                                        LR 0.000008    Time 0.021953    
2023-01-06 17:09:59,788 - Epoch: [165][  170/  246]    Overall Loss 0.258318    Objective Loss 0.258318                                        LR 0.000008    Time 0.021777    
2023-01-06 17:09:59,977 - Epoch: [165][  180/  246]    Overall Loss 0.257305    Objective Loss 0.257305                                        LR 0.000008    Time 0.021618    
2023-01-06 17:10:00,170 - Epoch: [165][  190/  246]    Overall Loss 0.256212    Objective Loss 0.256212                                        LR 0.000008    Time 0.021492    
2023-01-06 17:10:00,359 - Epoch: [165][  200/  246]    Overall Loss 0.257301    Objective Loss 0.257301                                        LR 0.000008    Time 0.021363    
2023-01-06 17:10:00,550 - Epoch: [165][  210/  246]    Overall Loss 0.258305    Objective Loss 0.258305                                        LR 0.000008    Time 0.021252    
2023-01-06 17:10:00,750 - Epoch: [165][  220/  246]    Overall Loss 0.258434    Objective Loss 0.258434                                        LR 0.000008    Time 0.021192    
2023-01-06 17:10:00,939 - Epoch: [165][  230/  246]    Overall Loss 0.259097    Objective Loss 0.259097                                        LR 0.000008    Time 0.021091    
2023-01-06 17:10:01,142 - Epoch: [165][  240/  246]    Overall Loss 0.259112    Objective Loss 0.259112                                        LR 0.000008    Time 0.021058    
2023-01-06 17:10:01,240 - Epoch: [165][  246/  246]    Overall Loss 0.259803    Objective Loss 0.259803    Top1 88.995215    LR 0.000008    Time 0.020941    
2023-01-06 17:10:01,381 - --- validate (epoch=165)-----------
2023-01-06 17:10:01,381 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:01,838 - Epoch: [165][   10/   28]    Loss 0.286477    Top1 89.453125    
2023-01-06 17:10:01,962 - Epoch: [165][   20/   28]    Loss 0.275229    Top1 89.960938    
2023-01-06 17:10:02,027 - Epoch: [165][   28/   28]    Loss 0.273359    Top1 90.065846    
2023-01-06 17:10:02,185 - ==> Top1: 90.066    Loss: 0.273

2023-01-06 17:10:02,185 - ==> Confusion:
[[ 222   15  202]
 [  17  235  350]
 [  54   56 5835]]

2023-01-06 17:10:02,187 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:10:02,187 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:02,193 - 

2023-01-06 17:10:02,193 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:02,774 - Epoch: [166][   10/  246]    Overall Loss 0.275056    Objective Loss 0.275056                                        LR 0.000008    Time 0.058024    
2023-01-06 17:10:02,964 - Epoch: [166][   20/  246]    Overall Loss 0.268899    Objective Loss 0.268899                                        LR 0.000008    Time 0.038506    
2023-01-06 17:10:03,159 - Epoch: [166][   30/  246]    Overall Loss 0.268121    Objective Loss 0.268121                                        LR 0.000008    Time 0.032155    
2023-01-06 17:10:03,356 - Epoch: [166][   40/  246]    Overall Loss 0.267390    Objective Loss 0.267390                                        LR 0.000008    Time 0.029040    
2023-01-06 17:10:03,582 - Epoch: [166][   50/  246]    Overall Loss 0.266951    Objective Loss 0.266951                                        LR 0.000008    Time 0.027728    
2023-01-06 17:10:03,809 - Epoch: [166][   60/  246]    Overall Loss 0.264018    Objective Loss 0.264018                                        LR 0.000008    Time 0.026895    
2023-01-06 17:10:04,073 - Epoch: [166][   70/  246]    Overall Loss 0.260811    Objective Loss 0.260811                                        LR 0.000008    Time 0.026813    
2023-01-06 17:10:04,341 - Epoch: [166][   80/  246]    Overall Loss 0.260536    Objective Loss 0.260536                                        LR 0.000008    Time 0.026805    
2023-01-06 17:10:04,579 - Epoch: [166][   90/  246]    Overall Loss 0.261955    Objective Loss 0.261955                                        LR 0.000008    Time 0.026444    
2023-01-06 17:10:04,763 - Epoch: [166][  100/  246]    Overall Loss 0.262335    Objective Loss 0.262335                                        LR 0.000008    Time 0.025637    
2023-01-06 17:10:04,991 - Epoch: [166][  110/  246]    Overall Loss 0.262178    Objective Loss 0.262178                                        LR 0.000008    Time 0.025375    
2023-01-06 17:10:05,242 - Epoch: [166][  120/  246]    Overall Loss 0.260859    Objective Loss 0.260859                                        LR 0.000008    Time 0.025348    
2023-01-06 17:10:05,483 - Epoch: [166][  130/  246]    Overall Loss 0.261180    Objective Loss 0.261180                                        LR 0.000008    Time 0.025244    
2023-01-06 17:10:05,728 - Epoch: [166][  140/  246]    Overall Loss 0.260429    Objective Loss 0.260429                                        LR 0.000008    Time 0.025189    
2023-01-06 17:10:05,971 - Epoch: [166][  150/  246]    Overall Loss 0.259547    Objective Loss 0.259547                                        LR 0.000008    Time 0.025118    
2023-01-06 17:10:06,216 - Epoch: [166][  160/  246]    Overall Loss 0.259380    Objective Loss 0.259380                                        LR 0.000008    Time 0.025063    
2023-01-06 17:10:06,462 - Epoch: [166][  170/  246]    Overall Loss 0.259912    Objective Loss 0.259912                                        LR 0.000008    Time 0.025033    
2023-01-06 17:10:06,708 - Epoch: [166][  180/  246]    Overall Loss 0.260181    Objective Loss 0.260181                                        LR 0.000008    Time 0.024997    
2023-01-06 17:10:06,955 - Epoch: [166][  190/  246]    Overall Loss 0.259546    Objective Loss 0.259546                                        LR 0.000008    Time 0.024969    
2023-01-06 17:10:07,201 - Epoch: [166][  200/  246]    Overall Loss 0.259449    Objective Loss 0.259449                                        LR 0.000008    Time 0.024940    
2023-01-06 17:10:07,448 - Epoch: [166][  210/  246]    Overall Loss 0.260231    Objective Loss 0.260231                                        LR 0.000008    Time 0.024919    
2023-01-06 17:10:07,695 - Epoch: [166][  220/  246]    Overall Loss 0.260135    Objective Loss 0.260135                                        LR 0.000008    Time 0.024899    
2023-01-06 17:10:07,941 - Epoch: [166][  230/  246]    Overall Loss 0.260101    Objective Loss 0.260101                                        LR 0.000008    Time 0.024878    
2023-01-06 17:10:08,163 - Epoch: [166][  240/  246]    Overall Loss 0.260570    Objective Loss 0.260570                                        LR 0.000008    Time 0.024766    
2023-01-06 17:10:08,253 - Epoch: [166][  246/  246]    Overall Loss 0.260588    Objective Loss 0.260588    Top1 88.995215    LR 0.000008    Time 0.024526    
2023-01-06 17:10:08,384 - --- validate (epoch=166)-----------
2023-01-06 17:10:08,384 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:08,835 - Epoch: [166][   10/   28]    Loss 0.267968    Top1 90.156250    
2023-01-06 17:10:08,949 - Epoch: [166][   20/   28]    Loss 0.278096    Top1 90.000000    
2023-01-06 17:10:09,015 - Epoch: [166][   28/   28]    Loss 0.281620    Top1 89.851131    
2023-01-06 17:10:09,174 - ==> Top1: 89.851    Loss: 0.282

2023-01-06 17:10:09,175 - ==> Confusion:
[[ 250   10  179]
 [  21  270  311]
 [  95   93 5757]]

2023-01-06 17:10:09,176 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:10:09,176 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:09,182 - 

2023-01-06 17:10:09,182 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:09,914 - Epoch: [167][   10/  246]    Overall Loss 0.256564    Objective Loss 0.256564                                        LR 0.000008    Time 0.073088    
2023-01-06 17:10:10,112 - Epoch: [167][   20/  246]    Overall Loss 0.248449    Objective Loss 0.248449                                        LR 0.000008    Time 0.046366    
2023-01-06 17:10:10,310 - Epoch: [167][   30/  246]    Overall Loss 0.256877    Objective Loss 0.256877                                        LR 0.000008    Time 0.037478    
2023-01-06 17:10:10,516 - Epoch: [167][   40/  246]    Overall Loss 0.258138    Objective Loss 0.258138                                        LR 0.000008    Time 0.033261    
2023-01-06 17:10:10,714 - Epoch: [167][   50/  246]    Overall Loss 0.260613    Objective Loss 0.260613                                        LR 0.000008    Time 0.030550    
2023-01-06 17:10:10,914 - Epoch: [167][   60/  246]    Overall Loss 0.261290    Objective Loss 0.261290                                        LR 0.000008    Time 0.028794    
2023-01-06 17:10:11,109 - Epoch: [167][   70/  246]    Overall Loss 0.261065    Objective Loss 0.261065                                        LR 0.000008    Time 0.027456    
2023-01-06 17:10:11,310 - Epoch: [167][   80/  246]    Overall Loss 0.262476    Objective Loss 0.262476                                        LR 0.000008    Time 0.026529    
2023-01-06 17:10:11,503 - Epoch: [167][   90/  246]    Overall Loss 0.260436    Objective Loss 0.260436                                        LR 0.000008    Time 0.025731    
2023-01-06 17:10:11,705 - Epoch: [167][  100/  246]    Overall Loss 0.258143    Objective Loss 0.258143                                        LR 0.000008    Time 0.025167    
2023-01-06 17:10:11,899 - Epoch: [167][  110/  246]    Overall Loss 0.258291    Objective Loss 0.258291                                        LR 0.000008    Time 0.024641    
2023-01-06 17:10:12,100 - Epoch: [167][  120/  246]    Overall Loss 0.258593    Objective Loss 0.258593                                        LR 0.000008    Time 0.024259    
2023-01-06 17:10:12,293 - Epoch: [167][  130/  246]    Overall Loss 0.259414    Objective Loss 0.259414                                        LR 0.000008    Time 0.023875    
2023-01-06 17:10:12,492 - Epoch: [167][  140/  246]    Overall Loss 0.260171    Objective Loss 0.260171                                        LR 0.000008    Time 0.023593    
2023-01-06 17:10:12,686 - Epoch: [167][  150/  246]    Overall Loss 0.259526    Objective Loss 0.259526                                        LR 0.000008    Time 0.023305    
2023-01-06 17:10:12,885 - Epoch: [167][  160/  246]    Overall Loss 0.258960    Objective Loss 0.258960                                        LR 0.000008    Time 0.023093    
2023-01-06 17:10:13,078 - Epoch: [167][  170/  246]    Overall Loss 0.258809    Objective Loss 0.258809                                        LR 0.000008    Time 0.022870    
2023-01-06 17:10:13,278 - Epoch: [167][  180/  246]    Overall Loss 0.258923    Objective Loss 0.258923                                        LR 0.000008    Time 0.022708    
2023-01-06 17:10:13,471 - Epoch: [167][  190/  246]    Overall Loss 0.259247    Objective Loss 0.259247                                        LR 0.000008    Time 0.022526    
2023-01-06 17:10:13,671 - Epoch: [167][  200/  246]    Overall Loss 0.258929    Objective Loss 0.258929                                        LR 0.000008    Time 0.022396    
2023-01-06 17:10:13,864 - Epoch: [167][  210/  246]    Overall Loss 0.259140    Objective Loss 0.259140                                        LR 0.000008    Time 0.022246    
2023-01-06 17:10:14,059 - Epoch: [167][  220/  246]    Overall Loss 0.259096    Objective Loss 0.259096                                        LR 0.000008    Time 0.022123    
2023-01-06 17:10:14,260 - Epoch: [167][  230/  246]    Overall Loss 0.259620    Objective Loss 0.259620                                        LR 0.000008    Time 0.022033    
2023-01-06 17:10:14,478 - Epoch: [167][  240/  246]    Overall Loss 0.260220    Objective Loss 0.260220                                        LR 0.000008    Time 0.022019    
2023-01-06 17:10:14,573 - Epoch: [167][  246/  246]    Overall Loss 0.260214    Objective Loss 0.260214    Top1 90.669856    LR 0.000008    Time 0.021869    
2023-01-06 17:10:14,706 - --- validate (epoch=167)-----------
2023-01-06 17:10:14,706 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:15,149 - Epoch: [167][   10/   28]    Loss 0.267954    Top1 91.015625    
2023-01-06 17:10:15,262 - Epoch: [167][   20/   28]    Loss 0.273124    Top1 90.312500    
2023-01-06 17:10:15,327 - Epoch: [167][   28/   28]    Loss 0.271467    Top1 90.208989    
2023-01-06 17:10:15,481 - ==> Top1: 90.209    Loss: 0.271

2023-01-06 17:10:15,482 - ==> Confusion:
[[ 232   17  190]
 [  18  250  334]
 [  55   70 5820]]

2023-01-06 17:10:15,483 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 161]
2023-01-06 17:10:15,483 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:15,489 - 

2023-01-06 17:10:15,489 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:16,218 - Epoch: [168][   10/  246]    Overall Loss 0.260346    Objective Loss 0.260346                                        LR 0.000008    Time 0.072788    
2023-01-06 17:10:16,415 - Epoch: [168][   20/  246]    Overall Loss 0.260141    Objective Loss 0.260141                                        LR 0.000008    Time 0.046266    
2023-01-06 17:10:16,631 - Epoch: [168][   30/  246]    Overall Loss 0.258187    Objective Loss 0.258187                                        LR 0.000008    Time 0.038030    
2023-01-06 17:10:16,861 - Epoch: [168][   40/  246]    Overall Loss 0.257221    Objective Loss 0.257221                                        LR 0.000008    Time 0.034243    
2023-01-06 17:10:17,091 - Epoch: [168][   50/  246]    Overall Loss 0.256923    Objective Loss 0.256923                                        LR 0.000008    Time 0.031998    
2023-01-06 17:10:17,329 - Epoch: [168][   60/  246]    Overall Loss 0.254418    Objective Loss 0.254418                                        LR 0.000008    Time 0.030622    
2023-01-06 17:10:17,569 - Epoch: [168][   70/  246]    Overall Loss 0.253630    Objective Loss 0.253630                                        LR 0.000008    Time 0.029666    
2023-01-06 17:10:17,813 - Epoch: [168][   80/  246]    Overall Loss 0.253949    Objective Loss 0.253949                                        LR 0.000008    Time 0.028997    
2023-01-06 17:10:18,061 - Epoch: [168][   90/  246]    Overall Loss 0.252943    Objective Loss 0.252943                                        LR 0.000008    Time 0.028521    
2023-01-06 17:10:18,263 - Epoch: [168][  100/  246]    Overall Loss 0.253507    Objective Loss 0.253507                                        LR 0.000008    Time 0.027678    
2023-01-06 17:10:18,450 - Epoch: [168][  110/  246]    Overall Loss 0.254974    Objective Loss 0.254974                                        LR 0.000008    Time 0.026861    
2023-01-06 17:10:18,646 - Epoch: [168][  120/  246]    Overall Loss 0.254243    Objective Loss 0.254243                                        LR 0.000008    Time 0.026247    
2023-01-06 17:10:18,843 - Epoch: [168][  130/  246]    Overall Loss 0.252654    Objective Loss 0.252654                                        LR 0.000008    Time 0.025741    
2023-01-06 17:10:19,038 - Epoch: [168][  140/  246]    Overall Loss 0.252546    Objective Loss 0.252546                                        LR 0.000008    Time 0.025296    
2023-01-06 17:10:19,250 - Epoch: [168][  150/  246]    Overall Loss 0.252385    Objective Loss 0.252385                                        LR 0.000008    Time 0.025022    
2023-01-06 17:10:19,458 - Epoch: [168][  160/  246]    Overall Loss 0.253883    Objective Loss 0.253883                                        LR 0.000008    Time 0.024752    
2023-01-06 17:10:19,681 - Epoch: [168][  170/  246]    Overall Loss 0.254912    Objective Loss 0.254912                                        LR 0.000008    Time 0.024605    
2023-01-06 17:10:19,890 - Epoch: [168][  180/  246]    Overall Loss 0.255791    Objective Loss 0.255791                                        LR 0.000008    Time 0.024397    
2023-01-06 17:10:20,102 - Epoch: [168][  190/  246]    Overall Loss 0.255365    Objective Loss 0.255365                                        LR 0.000008    Time 0.024231    
2023-01-06 17:10:20,347 - Epoch: [168][  200/  246]    Overall Loss 0.255813    Objective Loss 0.255813                                        LR 0.000008    Time 0.024240    
2023-01-06 17:10:20,581 - Epoch: [168][  210/  246]    Overall Loss 0.256562    Objective Loss 0.256562                                        LR 0.000008    Time 0.024197    
2023-01-06 17:10:20,826 - Epoch: [168][  220/  246]    Overall Loss 0.256818    Objective Loss 0.256818                                        LR 0.000008    Time 0.024211    
2023-01-06 17:10:21,061 - Epoch: [168][  230/  246]    Overall Loss 0.257429    Objective Loss 0.257429                                        LR 0.000008    Time 0.024177    
2023-01-06 17:10:21,313 - Epoch: [168][  240/  246]    Overall Loss 0.257811    Objective Loss 0.257811                                        LR 0.000008    Time 0.024219    
2023-01-06 17:10:21,426 - Epoch: [168][  246/  246]    Overall Loss 0.258313    Objective Loss 0.258313    Top1 88.516746    LR 0.000008    Time 0.024085    
2023-01-06 17:10:21,557 - --- validate (epoch=168)-----------
2023-01-06 17:10:21,557 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:22,057 - Epoch: [168][   10/   28]    Loss 0.258720    Top1 90.664062    
2023-01-06 17:10:22,205 - Epoch: [168][   20/   28]    Loss 0.267936    Top1 90.351562    
2023-01-06 17:10:22,276 - Epoch: [168][   28/   28]    Loss 0.267705    Top1 90.352133    
2023-01-06 17:10:22,404 - ==> Top1: 90.352    Loss: 0.268

2023-01-06 17:10:22,404 - ==> Confusion:
[[ 267   14  158]
 [  22  262  318]
 [  90   72 5783]]

2023-01-06 17:10:22,406 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:22,406 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:22,413 - 

2023-01-06 17:10:22,413 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:22,986 - Epoch: [169][   10/  246]    Overall Loss 0.246642    Objective Loss 0.246642                                        LR 0.000008    Time 0.057175    
2023-01-06 17:10:23,189 - Epoch: [169][   20/  246]    Overall Loss 0.248613    Objective Loss 0.248613                                        LR 0.000008    Time 0.038741    
2023-01-06 17:10:23,387 - Epoch: [169][   30/  246]    Overall Loss 0.249766    Objective Loss 0.249766                                        LR 0.000008    Time 0.032401    
2023-01-06 17:10:23,579 - Epoch: [169][   40/  246]    Overall Loss 0.252033    Objective Loss 0.252033                                        LR 0.000008    Time 0.029100    
2023-01-06 17:10:23,771 - Epoch: [169][   50/  246]    Overall Loss 0.253233    Objective Loss 0.253233                                        LR 0.000008    Time 0.027116    
2023-01-06 17:10:23,966 - Epoch: [169][   60/  246]    Overall Loss 0.255494    Objective Loss 0.255494                                        LR 0.000008    Time 0.025831    
2023-01-06 17:10:24,164 - Epoch: [169][   70/  246]    Overall Loss 0.255824    Objective Loss 0.255824                                        LR 0.000008    Time 0.024966    
2023-01-06 17:10:24,365 - Epoch: [169][   80/  246]    Overall Loss 0.257078    Objective Loss 0.257078                                        LR 0.000008    Time 0.024356    
2023-01-06 17:10:24,563 - Epoch: [169][   90/  246]    Overall Loss 0.256638    Objective Loss 0.256638                                        LR 0.000008    Time 0.023842    
2023-01-06 17:10:24,760 - Epoch: [169][  100/  246]    Overall Loss 0.256714    Objective Loss 0.256714                                        LR 0.000008    Time 0.023422    
2023-01-06 17:10:24,955 - Epoch: [169][  110/  246]    Overall Loss 0.256307    Objective Loss 0.256307                                        LR 0.000008    Time 0.023071    
2023-01-06 17:10:25,157 - Epoch: [169][  120/  246]    Overall Loss 0.256362    Objective Loss 0.256362                                        LR 0.000008    Time 0.022823    
2023-01-06 17:10:25,352 - Epoch: [169][  130/  246]    Overall Loss 0.257364    Objective Loss 0.257364                                        LR 0.000008    Time 0.022570    
2023-01-06 17:10:25,554 - Epoch: [169][  140/  246]    Overall Loss 0.256779    Objective Loss 0.256779                                        LR 0.000008    Time 0.022393    
2023-01-06 17:10:25,753 - Epoch: [169][  150/  246]    Overall Loss 0.257930    Objective Loss 0.257930                                        LR 0.000008    Time 0.022227    
2023-01-06 17:10:25,954 - Epoch: [169][  160/  246]    Overall Loss 0.258638    Objective Loss 0.258638                                        LR 0.000008    Time 0.022095    
2023-01-06 17:10:26,153 - Epoch: [169][  170/  246]    Overall Loss 0.258463    Objective Loss 0.258463                                        LR 0.000008    Time 0.021958    
2023-01-06 17:10:26,353 - Epoch: [169][  180/  246]    Overall Loss 0.258963    Objective Loss 0.258963                                        LR 0.000008    Time 0.021850    
2023-01-06 17:10:26,558 - Epoch: [169][  190/  246]    Overall Loss 0.258810    Objective Loss 0.258810                                        LR 0.000008    Time 0.021775    
2023-01-06 17:10:26,762 - Epoch: [169][  200/  246]    Overall Loss 0.259380    Objective Loss 0.259380                                        LR 0.000008    Time 0.021706    
2023-01-06 17:10:26,961 - Epoch: [169][  210/  246]    Overall Loss 0.258449    Objective Loss 0.258449                                        LR 0.000008    Time 0.021620    
2023-01-06 17:10:27,159 - Epoch: [169][  220/  246]    Overall Loss 0.258137    Objective Loss 0.258137                                        LR 0.000008    Time 0.021536    
2023-01-06 17:10:27,354 - Epoch: [169][  230/  246]    Overall Loss 0.258271    Objective Loss 0.258271                                        LR 0.000008    Time 0.021447    
2023-01-06 17:10:27,567 - Epoch: [169][  240/  246]    Overall Loss 0.258543    Objective Loss 0.258543                                        LR 0.000008    Time 0.021439    
2023-01-06 17:10:27,663 - Epoch: [169][  246/  246]    Overall Loss 0.258584    Objective Loss 0.258584    Top1 91.387560    LR 0.000008    Time 0.021304    
2023-01-06 17:10:27,796 - --- validate (epoch=169)-----------
2023-01-06 17:10:27,796 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:28,281 - Epoch: [169][   10/   28]    Loss 0.272532    Top1 90.000000    
2023-01-06 17:10:28,392 - Epoch: [169][   20/   28]    Loss 0.269472    Top1 90.253906    
2023-01-06 17:10:28,457 - Epoch: [169][   28/   28]    Loss 0.269626    Top1 90.180361    
2023-01-06 17:10:28,602 - ==> Top1: 90.180    Loss: 0.270

2023-01-06 17:10:28,602 - ==> Confusion:
[[ 241   17  181]
 [  18  258  326]
 [  71   73 5801]]

2023-01-06 17:10:28,603 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:28,603 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:28,609 - 

2023-01-06 17:10:28,610 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:29,318 - Epoch: [170][   10/  246]    Overall Loss 0.254211    Objective Loss 0.254211                                        LR 0.000008    Time 0.070771    
2023-01-06 17:10:29,512 - Epoch: [170][   20/  246]    Overall Loss 0.261984    Objective Loss 0.261984                                        LR 0.000008    Time 0.044993    
2023-01-06 17:10:29,710 - Epoch: [170][   30/  246]    Overall Loss 0.262603    Objective Loss 0.262603                                        LR 0.000008    Time 0.036600    
2023-01-06 17:10:29,908 - Epoch: [170][   40/  246]    Overall Loss 0.266467    Objective Loss 0.266467                                        LR 0.000008    Time 0.032391    
2023-01-06 17:10:30,107 - Epoch: [170][   50/  246]    Overall Loss 0.267808    Objective Loss 0.267808                                        LR 0.000008    Time 0.029889    
2023-01-06 17:10:30,299 - Epoch: [170][   60/  246]    Overall Loss 0.265185    Objective Loss 0.265185                                        LR 0.000008    Time 0.028103    
2023-01-06 17:10:30,488 - Epoch: [170][   70/  246]    Overall Loss 0.260986    Objective Loss 0.260986                                        LR 0.000008    Time 0.026783    
2023-01-06 17:10:30,689 - Epoch: [170][   80/  246]    Overall Loss 0.261323    Objective Loss 0.261323                                        LR 0.000008    Time 0.025936    
2023-01-06 17:10:30,907 - Epoch: [170][   90/  246]    Overall Loss 0.259775    Objective Loss 0.259775                                        LR 0.000008    Time 0.025470    
2023-01-06 17:10:31,113 - Epoch: [170][  100/  246]    Overall Loss 0.258894    Objective Loss 0.258894                                        LR 0.000008    Time 0.024981    
2023-01-06 17:10:31,318 - Epoch: [170][  110/  246]    Overall Loss 0.257830    Objective Loss 0.257830                                        LR 0.000008    Time 0.024566    
2023-01-06 17:10:31,547 - Epoch: [170][  120/  246]    Overall Loss 0.257728    Objective Loss 0.257728                                        LR 0.000008    Time 0.024426    
2023-01-06 17:10:31,779 - Epoch: [170][  130/  246]    Overall Loss 0.258513    Objective Loss 0.258513                                        LR 0.000008    Time 0.024328    
2023-01-06 17:10:32,009 - Epoch: [170][  140/  246]    Overall Loss 0.256944    Objective Loss 0.256944                                        LR 0.000008    Time 0.024229    
2023-01-06 17:10:32,226 - Epoch: [170][  150/  246]    Overall Loss 0.257969    Objective Loss 0.257969                                        LR 0.000008    Time 0.024056    
2023-01-06 17:10:32,462 - Epoch: [170][  160/  246]    Overall Loss 0.258555    Objective Loss 0.258555                                        LR 0.000008    Time 0.024027    
2023-01-06 17:10:32,694 - Epoch: [170][  170/  246]    Overall Loss 0.258012    Objective Loss 0.258012                                        LR 0.000008    Time 0.023977    
2023-01-06 17:10:32,932 - Epoch: [170][  180/  246]    Overall Loss 0.258228    Objective Loss 0.258228                                        LR 0.000008    Time 0.023965    
2023-01-06 17:10:33,163 - Epoch: [170][  190/  246]    Overall Loss 0.257742    Objective Loss 0.257742                                        LR 0.000008    Time 0.023908    
2023-01-06 17:10:33,400 - Epoch: [170][  200/  246]    Overall Loss 0.257924    Objective Loss 0.257924                                        LR 0.000008    Time 0.023896    
2023-01-06 17:10:33,639 - Epoch: [170][  210/  246]    Overall Loss 0.258077    Objective Loss 0.258077                                        LR 0.000008    Time 0.023894    
2023-01-06 17:10:33,841 - Epoch: [170][  220/  246]    Overall Loss 0.258557    Objective Loss 0.258557                                        LR 0.000008    Time 0.023723    
2023-01-06 17:10:34,040 - Epoch: [170][  230/  246]    Overall Loss 0.258234    Objective Loss 0.258234                                        LR 0.000008    Time 0.023557    
2023-01-06 17:10:34,244 - Epoch: [170][  240/  246]    Overall Loss 0.258388    Objective Loss 0.258388                                        LR 0.000008    Time 0.023422    
2023-01-06 17:10:34,332 - Epoch: [170][  246/  246]    Overall Loss 0.258560    Objective Loss 0.258560    Top1 91.148325    LR 0.000008    Time 0.023208    
2023-01-06 17:10:34,510 - --- validate (epoch=170)-----------
2023-01-06 17:10:34,510 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:34,975 - Epoch: [170][   10/   28]    Loss 0.271320    Top1 90.000000    
2023-01-06 17:10:35,087 - Epoch: [170][   20/   28]    Loss 0.273671    Top1 90.000000    
2023-01-06 17:10:35,152 - Epoch: [170][   28/   28]    Loss 0.269519    Top1 90.180361    
2023-01-06 17:10:35,280 - ==> Top1: 90.180    Loss: 0.270

2023-01-06 17:10:35,280 - ==> Confusion:
[[ 239   13  187]
 [  18  239  345]
 [  64   59 5822]]

2023-01-06 17:10:35,282 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:35,282 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:35,288 - 

2023-01-06 17:10:35,288 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:35,857 - Epoch: [171][   10/  246]    Overall Loss 0.245062    Objective Loss 0.245062                                        LR 0.000008    Time 0.056829    
2023-01-06 17:10:36,049 - Epoch: [171][   20/  246]    Overall Loss 0.261195    Objective Loss 0.261195                                        LR 0.000008    Time 0.037992    
2023-01-06 17:10:36,241 - Epoch: [171][   30/  246]    Overall Loss 0.259284    Objective Loss 0.259284                                        LR 0.000008    Time 0.031716    
2023-01-06 17:10:36,436 - Epoch: [171][   40/  246]    Overall Loss 0.261853    Objective Loss 0.261853                                        LR 0.000008    Time 0.028657    
2023-01-06 17:10:36,638 - Epoch: [171][   50/  246]    Overall Loss 0.259404    Objective Loss 0.259404                                        LR 0.000008    Time 0.026960    
2023-01-06 17:10:36,825 - Epoch: [171][   60/  246]    Overall Loss 0.259483    Objective Loss 0.259483                                        LR 0.000008    Time 0.025573    
2023-01-06 17:10:37,029 - Epoch: [171][   70/  246]    Overall Loss 0.260590    Objective Loss 0.260590                                        LR 0.000008    Time 0.024837    
2023-01-06 17:10:37,223 - Epoch: [171][   80/  246]    Overall Loss 0.259844    Objective Loss 0.259844                                        LR 0.000008    Time 0.024154    
2023-01-06 17:10:37,418 - Epoch: [171][   90/  246]    Overall Loss 0.260165    Objective Loss 0.260165                                        LR 0.000008    Time 0.023630    
2023-01-06 17:10:37,613 - Epoch: [171][  100/  246]    Overall Loss 0.260037    Objective Loss 0.260037                                        LR 0.000008    Time 0.023213    
2023-01-06 17:10:37,829 - Epoch: [171][  110/  246]    Overall Loss 0.260341    Objective Loss 0.260341                                        LR 0.000008    Time 0.023062    
2023-01-06 17:10:38,029 - Epoch: [171][  120/  246]    Overall Loss 0.259172    Objective Loss 0.259172                                        LR 0.000008    Time 0.022801    
2023-01-06 17:10:38,223 - Epoch: [171][  130/  246]    Overall Loss 0.259751    Objective Loss 0.259751                                        LR 0.000008    Time 0.022536    
2023-01-06 17:10:38,418 - Epoch: [171][  140/  246]    Overall Loss 0.257886    Objective Loss 0.257886                                        LR 0.000008    Time 0.022320    
2023-01-06 17:10:38,611 - Epoch: [171][  150/  246]    Overall Loss 0.257181    Objective Loss 0.257181                                        LR 0.000008    Time 0.022115    
2023-01-06 17:10:38,829 - Epoch: [171][  160/  246]    Overall Loss 0.258681    Objective Loss 0.258681                                        LR 0.000008    Time 0.022094    
2023-01-06 17:10:39,028 - Epoch: [171][  170/  246]    Overall Loss 0.259053    Objective Loss 0.259053                                        LR 0.000008    Time 0.021964    
2023-01-06 17:10:39,232 - Epoch: [171][  180/  246]    Overall Loss 0.259640    Objective Loss 0.259640                                        LR 0.000008    Time 0.021871    
2023-01-06 17:10:39,421 - Epoch: [171][  190/  246]    Overall Loss 0.260355    Objective Loss 0.260355                                        LR 0.000008    Time 0.021714    
2023-01-06 17:10:39,610 - Epoch: [171][  200/  246]    Overall Loss 0.260914    Objective Loss 0.260914                                        LR 0.000008    Time 0.021571    
2023-01-06 17:10:39,795 - Epoch: [171][  210/  246]    Overall Loss 0.259645    Objective Loss 0.259645                                        LR 0.000008    Time 0.021426    
2023-01-06 17:10:39,984 - Epoch: [171][  220/  246]    Overall Loss 0.259381    Objective Loss 0.259381                                        LR 0.000008    Time 0.021308    
2023-01-06 17:10:40,171 - Epoch: [171][  230/  246]    Overall Loss 0.259088    Objective Loss 0.259088                                        LR 0.000008    Time 0.021191    
2023-01-06 17:10:40,373 - Epoch: [171][  240/  246]    Overall Loss 0.259070    Objective Loss 0.259070                                        LR 0.000008    Time 0.021147    
2023-01-06 17:10:40,468 - Epoch: [171][  246/  246]    Overall Loss 0.258380    Objective Loss 0.258380    Top1 89.473684    LR 0.000008    Time 0.021017    
2023-01-06 17:10:40,598 - --- validate (epoch=171)-----------
2023-01-06 17:10:40,599 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:41,073 - Epoch: [171][   10/   28]    Loss 0.278846    Top1 89.765625    
2023-01-06 17:10:41,201 - Epoch: [171][   20/   28]    Loss 0.274786    Top1 89.824219    
2023-01-06 17:10:41,268 - Epoch: [171][   28/   28]    Loss 0.269051    Top1 90.166046    
2023-01-06 17:10:41,392 - ==> Top1: 90.166    Loss: 0.269

2023-01-06 17:10:41,392 - ==> Confusion:
[[ 243   15  181]
 [  18  244  340]
 [  69   64 5812]]

2023-01-06 17:10:41,394 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:41,394 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:41,400 - 

2023-01-06 17:10:41,400 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:42,104 - Epoch: [172][   10/  246]    Overall Loss 0.262470    Objective Loss 0.262470                                        LR 0.000008    Time 0.070339    
2023-01-06 17:10:42,292 - Epoch: [172][   20/  246]    Overall Loss 0.257608    Objective Loss 0.257608                                        LR 0.000008    Time 0.044546    
2023-01-06 17:10:42,482 - Epoch: [172][   30/  246]    Overall Loss 0.256666    Objective Loss 0.256666                                        LR 0.000008    Time 0.036020    
2023-01-06 17:10:42,666 - Epoch: [172][   40/  246]    Overall Loss 0.255310    Objective Loss 0.255310                                        LR 0.000008    Time 0.031598    
2023-01-06 17:10:42,835 - Epoch: [172][   50/  246]    Overall Loss 0.255618    Objective Loss 0.255618                                        LR 0.000008    Time 0.028670    
2023-01-06 17:10:43,006 - Epoch: [172][   60/  246]    Overall Loss 0.254615    Objective Loss 0.254615                                        LR 0.000008    Time 0.026730    
2023-01-06 17:10:43,209 - Epoch: [172][   70/  246]    Overall Loss 0.255693    Objective Loss 0.255693                                        LR 0.000008    Time 0.025801    
2023-01-06 17:10:43,407 - Epoch: [172][   80/  246]    Overall Loss 0.253117    Objective Loss 0.253117                                        LR 0.000008    Time 0.025048    
2023-01-06 17:10:43,613 - Epoch: [172][   90/  246]    Overall Loss 0.252958    Objective Loss 0.252958                                        LR 0.000008    Time 0.024555    
2023-01-06 17:10:43,813 - Epoch: [172][  100/  246]    Overall Loss 0.253927    Objective Loss 0.253927                                        LR 0.000008    Time 0.024091    
2023-01-06 17:10:44,017 - Epoch: [172][  110/  246]    Overall Loss 0.254640    Objective Loss 0.254640                                        LR 0.000008    Time 0.023752    
2023-01-06 17:10:44,216 - Epoch: [172][  120/  246]    Overall Loss 0.254783    Objective Loss 0.254783                                        LR 0.000008    Time 0.023426    
2023-01-06 17:10:44,414 - Epoch: [172][  130/  246]    Overall Loss 0.255509    Objective Loss 0.255509                                        LR 0.000008    Time 0.023148    
2023-01-06 17:10:44,608 - Epoch: [172][  140/  246]    Overall Loss 0.256286    Objective Loss 0.256286                                        LR 0.000008    Time 0.022879    
2023-01-06 17:10:44,803 - Epoch: [172][  150/  246]    Overall Loss 0.255821    Objective Loss 0.255821                                        LR 0.000008    Time 0.022646    
2023-01-06 17:10:44,977 - Epoch: [172][  160/  246]    Overall Loss 0.257031    Objective Loss 0.257031                                        LR 0.000008    Time 0.022317    
2023-01-06 17:10:45,170 - Epoch: [172][  170/  246]    Overall Loss 0.257237    Objective Loss 0.257237                                        LR 0.000008    Time 0.022137    
2023-01-06 17:10:45,362 - Epoch: [172][  180/  246]    Overall Loss 0.258069    Objective Loss 0.258069                                        LR 0.000008    Time 0.021972    
2023-01-06 17:10:45,565 - Epoch: [172][  190/  246]    Overall Loss 0.257937    Objective Loss 0.257937                                        LR 0.000008    Time 0.021884    
2023-01-06 17:10:45,777 - Epoch: [172][  200/  246]    Overall Loss 0.258309    Objective Loss 0.258309                                        LR 0.000008    Time 0.021847    
2023-01-06 17:10:45,964 - Epoch: [172][  210/  246]    Overall Loss 0.258134    Objective Loss 0.258134                                        LR 0.000008    Time 0.021697    
2023-01-06 17:10:46,128 - Epoch: [172][  220/  246]    Overall Loss 0.258247    Objective Loss 0.258247                                        LR 0.000008    Time 0.021451    
2023-01-06 17:10:46,293 - Epoch: [172][  230/  246]    Overall Loss 0.258292    Objective Loss 0.258292                                        LR 0.000008    Time 0.021234    
2023-01-06 17:10:46,480 - Epoch: [172][  240/  246]    Overall Loss 0.258048    Objective Loss 0.258048                                        LR 0.000008    Time 0.021127    
2023-01-06 17:10:46,574 - Epoch: [172][  246/  246]    Overall Loss 0.258019    Objective Loss 0.258019    Top1 92.583732    LR 0.000008    Time 0.020994    
2023-01-06 17:10:46,709 - --- validate (epoch=172)-----------
2023-01-06 17:10:46,709 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:47,161 - Epoch: [172][   10/   28]    Loss 0.266884    Top1 90.195312    
2023-01-06 17:10:47,276 - Epoch: [172][   20/   28]    Loss 0.266936    Top1 90.332031    
2023-01-06 17:10:47,342 - Epoch: [172][   28/   28]    Loss 0.274668    Top1 90.151732    
2023-01-06 17:10:47,492 - ==> Top1: 90.152    Loss: 0.275

2023-01-06 17:10:47,492 - ==> Confusion:
[[ 228   19  192]
 [  18  257  327]
 [  58   74 5813]]

2023-01-06 17:10:47,494 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:47,494 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:47,500 - 

2023-01-06 17:10:47,500 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:48,213 - Epoch: [173][   10/  246]    Overall Loss 0.259852    Objective Loss 0.259852                                        LR 0.000008    Time 0.071277    
2023-01-06 17:10:48,380 - Epoch: [173][   20/  246]    Overall Loss 0.258499    Objective Loss 0.258499                                        LR 0.000008    Time 0.043970    
2023-01-06 17:10:48,564 - Epoch: [173][   30/  246]    Overall Loss 0.259000    Objective Loss 0.259000                                        LR 0.000008    Time 0.035443    
2023-01-06 17:10:48,751 - Epoch: [173][   40/  246]    Overall Loss 0.261325    Objective Loss 0.261325                                        LR 0.000008    Time 0.031226    
2023-01-06 17:10:48,951 - Epoch: [173][   50/  246]    Overall Loss 0.259398    Objective Loss 0.259398                                        LR 0.000008    Time 0.028977    
2023-01-06 17:10:49,145 - Epoch: [173][   60/  246]    Overall Loss 0.258418    Objective Loss 0.258418                                        LR 0.000008    Time 0.027385    
2023-01-06 17:10:49,341 - Epoch: [173][   70/  246]    Overall Loss 0.258480    Objective Loss 0.258480                                        LR 0.000008    Time 0.026259    
2023-01-06 17:10:49,530 - Epoch: [173][   80/  246]    Overall Loss 0.258372    Objective Loss 0.258372                                        LR 0.000008    Time 0.025331    
2023-01-06 17:10:49,721 - Epoch: [173][   90/  246]    Overall Loss 0.258717    Objective Loss 0.258717                                        LR 0.000008    Time 0.024633    
2023-01-06 17:10:49,920 - Epoch: [173][  100/  246]    Overall Loss 0.258692    Objective Loss 0.258692                                        LR 0.000008    Time 0.024146    
2023-01-06 17:10:50,129 - Epoch: [173][  110/  246]    Overall Loss 0.259883    Objective Loss 0.259883                                        LR 0.000008    Time 0.023850    
2023-01-06 17:10:50,337 - Epoch: [173][  120/  246]    Overall Loss 0.257872    Objective Loss 0.257872                                        LR 0.000008    Time 0.023587    
2023-01-06 17:10:50,543 - Epoch: [173][  130/  246]    Overall Loss 0.257945    Objective Loss 0.257945                                        LR 0.000008    Time 0.023354    
2023-01-06 17:10:50,730 - Epoch: [173][  140/  246]    Overall Loss 0.257117    Objective Loss 0.257117                                        LR 0.000008    Time 0.023025    
2023-01-06 17:10:50,929 - Epoch: [173][  150/  246]    Overall Loss 0.258573    Objective Loss 0.258573                                        LR 0.000008    Time 0.022809    
2023-01-06 17:10:51,141 - Epoch: [173][  160/  246]    Overall Loss 0.258527    Objective Loss 0.258527                                        LR 0.000008    Time 0.022708    
2023-01-06 17:10:51,349 - Epoch: [173][  170/  246]    Overall Loss 0.258414    Objective Loss 0.258414                                        LR 0.000008    Time 0.022593    
2023-01-06 17:10:51,557 - Epoch: [173][  180/  246]    Overall Loss 0.258696    Objective Loss 0.258696                                        LR 0.000008    Time 0.022490    
2023-01-06 17:10:51,766 - Epoch: [173][  190/  246]    Overall Loss 0.258321    Objective Loss 0.258321                                        LR 0.000008    Time 0.022405    
2023-01-06 17:10:51,974 - Epoch: [173][  200/  246]    Overall Loss 0.258811    Objective Loss 0.258811                                        LR 0.000008    Time 0.022328    
2023-01-06 17:10:52,183 - Epoch: [173][  210/  246]    Overall Loss 0.258985    Objective Loss 0.258985                                        LR 0.000008    Time 0.022258    
2023-01-06 17:10:52,390 - Epoch: [173][  220/  246]    Overall Loss 0.258994    Objective Loss 0.258994                                        LR 0.000008    Time 0.022185    
2023-01-06 17:10:52,598 - Epoch: [173][  230/  246]    Overall Loss 0.258748    Objective Loss 0.258748                                        LR 0.000008    Time 0.022122    
2023-01-06 17:10:52,817 - Epoch: [173][  240/  246]    Overall Loss 0.258219    Objective Loss 0.258219                                        LR 0.000008    Time 0.022113    
2023-01-06 17:10:52,915 - Epoch: [173][  246/  246]    Overall Loss 0.258080    Objective Loss 0.258080    Top1 91.626794    LR 0.000008    Time 0.021968    
2023-01-06 17:10:53,059 - --- validate (epoch=173)-----------
2023-01-06 17:10:53,060 - 6986 samples (256 per mini-batch)
2023-01-06 17:10:53,513 - Epoch: [173][   10/   28]    Loss 0.261442    Top1 90.664062    
2023-01-06 17:10:53,628 - Epoch: [173][   20/   28]    Loss 0.272159    Top1 90.058594    
2023-01-06 17:10:53,695 - Epoch: [173][   28/   28]    Loss 0.267460    Top1 90.223304    
2023-01-06 17:10:53,847 - ==> Top1: 90.223    Loss: 0.267

2023-01-06 17:10:53,847 - ==> Confusion:
[[ 233   15  191]
 [  19  250  333]
 [  63   62 5820]]

2023-01-06 17:10:53,848 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:10:53,849 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:10:53,855 - 

2023-01-06 17:10:53,855 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:10:54,422 - Epoch: [174][   10/  246]    Overall Loss 0.251042    Objective Loss 0.251042                                        LR 0.000008    Time 0.056632    
2023-01-06 17:10:54,606 - Epoch: [174][   20/  246]    Overall Loss 0.246049    Objective Loss 0.246049                                        LR 0.000008    Time 0.037513    
2023-01-06 17:10:54,812 - Epoch: [174][   30/  246]    Overall Loss 0.248529    Objective Loss 0.248529                                        LR 0.000008    Time 0.031843    
2023-01-06 17:10:55,025 - Epoch: [174][   40/  246]    Overall Loss 0.248190    Objective Loss 0.248190                                        LR 0.000008    Time 0.029202    
2023-01-06 17:10:55,232 - Epoch: [174][   50/  246]    Overall Loss 0.248020    Objective Loss 0.248020                                        LR 0.000008    Time 0.027497    
2023-01-06 17:10:55,478 - Epoch: [174][   60/  246]    Overall Loss 0.248489    Objective Loss 0.248489                                        LR 0.000008    Time 0.027012    
2023-01-06 17:10:55,713 - Epoch: [174][   70/  246]    Overall Loss 0.251547    Objective Loss 0.251547                                        LR 0.000008    Time 0.026507    
2023-01-06 17:10:55,948 - Epoch: [174][   80/  246]    Overall Loss 0.251777    Objective Loss 0.251777                                        LR 0.000008    Time 0.026116    
2023-01-06 17:10:56,181 - Epoch: [174][   90/  246]    Overall Loss 0.251759    Objective Loss 0.251759                                        LR 0.000008    Time 0.025801    
2023-01-06 17:10:56,418 - Epoch: [174][  100/  246]    Overall Loss 0.254107    Objective Loss 0.254107                                        LR 0.000008    Time 0.025584    
2023-01-06 17:10:56,646 - Epoch: [174][  110/  246]    Overall Loss 0.253873    Objective Loss 0.253873                                        LR 0.000008    Time 0.025325    
2023-01-06 17:10:56,871 - Epoch: [174][  120/  246]    Overall Loss 0.254473    Objective Loss 0.254473                                        LR 0.000008    Time 0.025093    
2023-01-06 17:10:57,106 - Epoch: [174][  130/  246]    Overall Loss 0.253404    Objective Loss 0.253404                                        LR 0.000008    Time 0.024960    
2023-01-06 17:10:57,344 - Epoch: [174][  140/  246]    Overall Loss 0.254888    Objective Loss 0.254888                                        LR 0.000008    Time 0.024879    
2023-01-06 17:10:57,584 - Epoch: [174][  150/  246]    Overall Loss 0.255676    Objective Loss 0.255676                                        LR 0.000008    Time 0.024817    
2023-01-06 17:10:57,826 - Epoch: [174][  160/  246]    Overall Loss 0.256224    Objective Loss 0.256224                                        LR 0.000008    Time 0.024774    
2023-01-06 17:10:58,068 - Epoch: [174][  170/  246]    Overall Loss 0.257597    Objective Loss 0.257597                                        LR 0.000008    Time 0.024734    
2023-01-06 17:10:58,299 - Epoch: [174][  180/  246]    Overall Loss 0.257320    Objective Loss 0.257320                                        LR 0.000008    Time 0.024642    
2023-01-06 17:10:58,522 - Epoch: [174][  190/  246]    Overall Loss 0.256880    Objective Loss 0.256880                                        LR 0.000008    Time 0.024517    
2023-01-06 17:10:58,749 - Epoch: [174][  200/  246]    Overall Loss 0.257279    Objective Loss 0.257279                                        LR 0.000008    Time 0.024422    
2023-01-06 17:10:58,978 - Epoch: [174][  210/  246]    Overall Loss 0.256479    Objective Loss 0.256479                                        LR 0.000008    Time 0.024350    
2023-01-06 17:10:59,216 - Epoch: [174][  220/  246]    Overall Loss 0.256441    Objective Loss 0.256441                                        LR 0.000008    Time 0.024321    
2023-01-06 17:10:59,448 - Epoch: [174][  230/  246]    Overall Loss 0.256535    Objective Loss 0.256535                                        LR 0.000008    Time 0.024271    
2023-01-06 17:10:59,659 - Epoch: [174][  240/  246]    Overall Loss 0.256413    Objective Loss 0.256413                                        LR 0.000008    Time 0.024138    
2023-01-06 17:10:59,757 - Epoch: [174][  246/  246]    Overall Loss 0.256332    Objective Loss 0.256332    Top1 93.062201    LR 0.000008    Time 0.023945    
2023-01-06 17:10:59,904 - --- validate (epoch=174)-----------
2023-01-06 17:10:59,905 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:00,375 - Epoch: [174][   10/   28]    Loss 0.268652    Top1 90.234375    
2023-01-06 17:11:00,493 - Epoch: [174][   20/   28]    Loss 0.270269    Top1 90.312500    
2023-01-06 17:11:00,560 - Epoch: [174][   28/   28]    Loss 0.271276    Top1 90.294875    
2023-01-06 17:11:00,728 - ==> Top1: 90.295    Loss: 0.271

2023-01-06 17:11:00,728 - ==> Confusion:
[[ 238   16  185]
 [  15  264  323]
 [  62   77 5806]]

2023-01-06 17:11:00,730 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:00,730 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:00,736 - 

2023-01-06 17:11:00,736 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:01,469 - Epoch: [175][   10/  246]    Overall Loss 0.243706    Objective Loss 0.243706                                        LR 0.000008    Time 0.073250    
2023-01-06 17:11:01,669 - Epoch: [175][   20/  246]    Overall Loss 0.241466    Objective Loss 0.241466                                        LR 0.000008    Time 0.046589    
2023-01-06 17:11:01,856 - Epoch: [175][   30/  246]    Overall Loss 0.248870    Objective Loss 0.248870                                        LR 0.000008    Time 0.037292    
2023-01-06 17:11:02,053 - Epoch: [175][   40/  246]    Overall Loss 0.252721    Objective Loss 0.252721                                        LR 0.000008    Time 0.032900    
2023-01-06 17:11:02,230 - Epoch: [175][   50/  246]    Overall Loss 0.254636    Objective Loss 0.254636                                        LR 0.000008    Time 0.029808    
2023-01-06 17:11:02,429 - Epoch: [175][   60/  246]    Overall Loss 0.256149    Objective Loss 0.256149                                        LR 0.000008    Time 0.028148    
2023-01-06 17:11:02,614 - Epoch: [175][   70/  246]    Overall Loss 0.255341    Objective Loss 0.255341                                        LR 0.000008    Time 0.026739    
2023-01-06 17:11:02,825 - Epoch: [175][   80/  246]    Overall Loss 0.255431    Objective Loss 0.255431                                        LR 0.000008    Time 0.026041    
2023-01-06 17:11:03,034 - Epoch: [175][   90/  246]    Overall Loss 0.255488    Objective Loss 0.255488                                        LR 0.000008    Time 0.025459    
2023-01-06 17:11:03,244 - Epoch: [175][  100/  246]    Overall Loss 0.255717    Objective Loss 0.255717                                        LR 0.000008    Time 0.025010    
2023-01-06 17:11:03,452 - Epoch: [175][  110/  246]    Overall Loss 0.255605    Objective Loss 0.255605                                        LR 0.000008    Time 0.024624    
2023-01-06 17:11:03,663 - Epoch: [175][  120/  246]    Overall Loss 0.256683    Objective Loss 0.256683                                        LR 0.000008    Time 0.024329    
2023-01-06 17:11:03,872 - Epoch: [175][  130/  246]    Overall Loss 0.255291    Objective Loss 0.255291                                        LR 0.000008    Time 0.024061    
2023-01-06 17:11:04,076 - Epoch: [175][  140/  246]    Overall Loss 0.256605    Objective Loss 0.256605                                        LR 0.000008    Time 0.023799    
2023-01-06 17:11:04,268 - Epoch: [175][  150/  246]    Overall Loss 0.257657    Objective Loss 0.257657                                        LR 0.000008    Time 0.023478    
2023-01-06 17:11:04,456 - Epoch: [175][  160/  246]    Overall Loss 0.257013    Objective Loss 0.257013                                        LR 0.000008    Time 0.023184    
2023-01-06 17:11:04,637 - Epoch: [175][  170/  246]    Overall Loss 0.255605    Objective Loss 0.255605                                        LR 0.000008    Time 0.022885    
2023-01-06 17:11:04,845 - Epoch: [175][  180/  246]    Overall Loss 0.255360    Objective Loss 0.255360                                        LR 0.000008    Time 0.022768    
2023-01-06 17:11:05,046 - Epoch: [175][  190/  246]    Overall Loss 0.254352    Objective Loss 0.254352                                        LR 0.000008    Time 0.022623    
2023-01-06 17:11:05,254 - Epoch: [175][  200/  246]    Overall Loss 0.255164    Objective Loss 0.255164                                        LR 0.000008    Time 0.022531    
2023-01-06 17:11:05,454 - Epoch: [175][  210/  246]    Overall Loss 0.255506    Objective Loss 0.255506                                        LR 0.000008    Time 0.022409    
2023-01-06 17:11:05,662 - Epoch: [175][  220/  246]    Overall Loss 0.256148    Objective Loss 0.256148                                        LR 0.000008    Time 0.022332    
2023-01-06 17:11:05,861 - Epoch: [175][  230/  246]    Overall Loss 0.256111    Objective Loss 0.256111                                        LR 0.000008    Time 0.022228    
2023-01-06 17:11:06,074 - Epoch: [175][  240/  246]    Overall Loss 0.256104    Objective Loss 0.256104                                        LR 0.000008    Time 0.022188    
2023-01-06 17:11:06,170 - Epoch: [175][  246/  246]    Overall Loss 0.255964    Objective Loss 0.255964    Top1 87.320574    LR 0.000008    Time 0.022036    
2023-01-06 17:11:06,319 - --- validate (epoch=175)-----------
2023-01-06 17:11:06,319 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:06,804 - Epoch: [175][   10/   28]    Loss 0.287084    Top1 88.984375    
2023-01-06 17:11:06,927 - Epoch: [175][   20/   28]    Loss 0.278031    Top1 89.941406    
2023-01-06 17:11:06,994 - Epoch: [175][   28/   28]    Loss 0.275817    Top1 90.008589    
2023-01-06 17:11:07,142 - ==> Top1: 90.009    Loss: 0.276

2023-01-06 17:11:07,142 - ==> Confusion:
[[ 203   12  224]
 [  12  234  356]
 [  38   56 5851]]

2023-01-06 17:11:07,144 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:07,144 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:07,149 - 

2023-01-06 17:11:07,150 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:07,716 - Epoch: [176][   10/  246]    Overall Loss 0.221153    Objective Loss 0.221153                                        LR 0.000008    Time 0.056573    
2023-01-06 17:11:07,907 - Epoch: [176][   20/  246]    Overall Loss 0.244116    Objective Loss 0.244116                                        LR 0.000008    Time 0.037807    
2023-01-06 17:11:08,094 - Epoch: [176][   30/  246]    Overall Loss 0.252120    Objective Loss 0.252120                                        LR 0.000008    Time 0.031445    
2023-01-06 17:11:08,295 - Epoch: [176][   40/  246]    Overall Loss 0.250795    Objective Loss 0.250795                                        LR 0.000008    Time 0.028592    
2023-01-06 17:11:08,490 - Epoch: [176][   50/  246]    Overall Loss 0.252526    Objective Loss 0.252526                                        LR 0.000008    Time 0.026765    
2023-01-06 17:11:08,680 - Epoch: [176][   60/  246]    Overall Loss 0.254116    Objective Loss 0.254116                                        LR 0.000008    Time 0.025468    
2023-01-06 17:11:08,872 - Epoch: [176][   70/  246]    Overall Loss 0.253205    Objective Loss 0.253205                                        LR 0.000008    Time 0.024568    
2023-01-06 17:11:09,061 - Epoch: [176][   80/  246]    Overall Loss 0.255243    Objective Loss 0.255243                                        LR 0.000008    Time 0.023857    
2023-01-06 17:11:09,248 - Epoch: [176][   90/  246]    Overall Loss 0.256979    Objective Loss 0.256979                                        LR 0.000008    Time 0.023280    
2023-01-06 17:11:09,439 - Epoch: [176][  100/  246]    Overall Loss 0.256515    Objective Loss 0.256515                                        LR 0.000008    Time 0.022853    
2023-01-06 17:11:09,626 - Epoch: [176][  110/  246]    Overall Loss 0.256324    Objective Loss 0.256324                                        LR 0.000008    Time 0.022473    
2023-01-06 17:11:09,811 - Epoch: [176][  120/  246]    Overall Loss 0.257550    Objective Loss 0.257550                                        LR 0.000008    Time 0.022140    
2023-01-06 17:11:09,995 - Epoch: [176][  130/  246]    Overall Loss 0.256942    Objective Loss 0.256942                                        LR 0.000008    Time 0.021852    
2023-01-06 17:11:10,185 - Epoch: [176][  140/  246]    Overall Loss 0.257573    Objective Loss 0.257573                                        LR 0.000008    Time 0.021640    
2023-01-06 17:11:10,371 - Epoch: [176][  150/  246]    Overall Loss 0.256240    Objective Loss 0.256240                                        LR 0.000008    Time 0.021438    
2023-01-06 17:11:10,567 - Epoch: [176][  160/  246]    Overall Loss 0.255303    Objective Loss 0.255303                                        LR 0.000008    Time 0.021320    
2023-01-06 17:11:10,760 - Epoch: [176][  170/  246]    Overall Loss 0.255298    Objective Loss 0.255298                                        LR 0.000008    Time 0.021199    
2023-01-06 17:11:10,950 - Epoch: [176][  180/  246]    Overall Loss 0.254897    Objective Loss 0.254897                                        LR 0.000008    Time 0.021075    
2023-01-06 17:11:11,145 - Epoch: [176][  190/  246]    Overall Loss 0.255622    Objective Loss 0.255622                                        LR 0.000008    Time 0.020993    
2023-01-06 17:11:11,338 - Epoch: [176][  200/  246]    Overall Loss 0.256167    Objective Loss 0.256167                                        LR 0.000008    Time 0.020905    
2023-01-06 17:11:11,532 - Epoch: [176][  210/  246]    Overall Loss 0.256595    Objective Loss 0.256595                                        LR 0.000008    Time 0.020831    
2023-01-06 17:11:11,742 - Epoch: [176][  220/  246]    Overall Loss 0.257396    Objective Loss 0.257396                                        LR 0.000008    Time 0.020836    
2023-01-06 17:11:11,951 - Epoch: [176][  230/  246]    Overall Loss 0.256882    Objective Loss 0.256882                                        LR 0.000008    Time 0.020838    
2023-01-06 17:11:12,181 - Epoch: [176][  240/  246]    Overall Loss 0.257223    Objective Loss 0.257223                                        LR 0.000008    Time 0.020927    
2023-01-06 17:11:12,289 - Epoch: [176][  246/  246]    Overall Loss 0.257219    Objective Loss 0.257219    Top1 91.148325    LR 0.000008    Time 0.020856    
2023-01-06 17:11:12,468 - --- validate (epoch=176)-----------
2023-01-06 17:11:12,468 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:12,931 - Epoch: [176][   10/   28]    Loss 0.267851    Top1 90.195312    
2023-01-06 17:11:13,053 - Epoch: [176][   20/   28]    Loss 0.266646    Top1 90.214844    
2023-01-06 17:11:13,120 - Epoch: [176][   28/   28]    Loss 0.267527    Top1 90.151732    
2023-01-06 17:11:13,278 - ==> Top1: 90.152    Loss: 0.268

2023-01-06 17:11:13,278 - ==> Confusion:
[[ 240   18  181]
 [  18  257  327]
 [  64   80 5801]]

2023-01-06 17:11:13,280 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:13,280 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:13,286 - 

2023-01-06 17:11:13,286 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:13,984 - Epoch: [177][   10/  246]    Overall Loss 0.249179    Objective Loss 0.249179                                        LR 0.000008    Time 0.069791    
2023-01-06 17:11:14,156 - Epoch: [177][   20/  246]    Overall Loss 0.252617    Objective Loss 0.252617                                        LR 0.000008    Time 0.043364    
2023-01-06 17:11:14,320 - Epoch: [177][   30/  246]    Overall Loss 0.254527    Objective Loss 0.254527                                        LR 0.000008    Time 0.034373    
2023-01-06 17:11:14,486 - Epoch: [177][   40/  246]    Overall Loss 0.253768    Objective Loss 0.253768                                        LR 0.000008    Time 0.029912    
2023-01-06 17:11:14,653 - Epoch: [177][   50/  246]    Overall Loss 0.258972    Objective Loss 0.258972                                        LR 0.000008    Time 0.027280    
2023-01-06 17:11:14,824 - Epoch: [177][   60/  246]    Overall Loss 0.261207    Objective Loss 0.261207                                        LR 0.000008    Time 0.025575    
2023-01-06 17:11:14,995 - Epoch: [177][   70/  246]    Overall Loss 0.261913    Objective Loss 0.261913                                        LR 0.000008    Time 0.024354    
2023-01-06 17:11:15,160 - Epoch: [177][   80/  246]    Overall Loss 0.259919    Objective Loss 0.259919                                        LR 0.000008    Time 0.023365    
2023-01-06 17:11:15,327 - Epoch: [177][   90/  246]    Overall Loss 0.260528    Objective Loss 0.260528                                        LR 0.000008    Time 0.022627    
2023-01-06 17:11:15,496 - Epoch: [177][  100/  246]    Overall Loss 0.258670    Objective Loss 0.258670                                        LR 0.000008    Time 0.022052    
2023-01-06 17:11:15,666 - Epoch: [177][  110/  246]    Overall Loss 0.258894    Objective Loss 0.258894                                        LR 0.000008    Time 0.021592    
2023-01-06 17:11:15,838 - Epoch: [177][  120/  246]    Overall Loss 0.259093    Objective Loss 0.259093                                        LR 0.000008    Time 0.021220    
2023-01-06 17:11:16,008 - Epoch: [177][  130/  246]    Overall Loss 0.258088    Objective Loss 0.258088                                        LR 0.000008    Time 0.020896    
2023-01-06 17:11:16,193 - Epoch: [177][  140/  246]    Overall Loss 0.257384    Objective Loss 0.257384                                        LR 0.000008    Time 0.020719    
2023-01-06 17:11:16,406 - Epoch: [177][  150/  246]    Overall Loss 0.256384    Objective Loss 0.256384                                        LR 0.000008    Time 0.020756    
2023-01-06 17:11:16,611 - Epoch: [177][  160/  246]    Overall Loss 0.255103    Objective Loss 0.255103                                        LR 0.000008    Time 0.020736    
2023-01-06 17:11:16,805 - Epoch: [177][  170/  246]    Overall Loss 0.254713    Objective Loss 0.254713                                        LR 0.000008    Time 0.020655    
2023-01-06 17:11:17,010 - Epoch: [177][  180/  246]    Overall Loss 0.253584    Objective Loss 0.253584                                        LR 0.000008    Time 0.020644    
2023-01-06 17:11:17,207 - Epoch: [177][  190/  246]    Overall Loss 0.253507    Objective Loss 0.253507                                        LR 0.000008    Time 0.020595    
2023-01-06 17:11:17,412 - Epoch: [177][  200/  246]    Overall Loss 0.252956    Objective Loss 0.252956                                        LR 0.000008    Time 0.020588    
2023-01-06 17:11:17,609 - Epoch: [177][  210/  246]    Overall Loss 0.253759    Objective Loss 0.253759                                        LR 0.000008    Time 0.020542    
2023-01-06 17:11:17,823 - Epoch: [177][  220/  246]    Overall Loss 0.253669    Objective Loss 0.253669                                        LR 0.000008    Time 0.020580    
2023-01-06 17:11:18,053 - Epoch: [177][  230/  246]    Overall Loss 0.254090    Objective Loss 0.254090                                        LR 0.000008    Time 0.020682    
2023-01-06 17:11:18,286 - Epoch: [177][  240/  246]    Overall Loss 0.254891    Objective Loss 0.254891                                        LR 0.000008    Time 0.020792    
2023-01-06 17:11:18,382 - Epoch: [177][  246/  246]    Overall Loss 0.255161    Objective Loss 0.255161    Top1 91.387560    LR 0.000008    Time 0.020675    
2023-01-06 17:11:18,517 - --- validate (epoch=177)-----------
2023-01-06 17:11:18,517 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:18,986 - Epoch: [177][   10/   28]    Loss 0.266603    Top1 90.078125    
2023-01-06 17:11:19,105 - Epoch: [177][   20/   28]    Loss 0.270006    Top1 90.332031    
2023-01-06 17:11:19,172 - Epoch: [177][   28/   28]    Loss 0.274274    Top1 90.194675    
2023-01-06 17:11:19,304 - ==> Top1: 90.195    Loss: 0.274

2023-01-06 17:11:19,304 - ==> Confusion:
[[ 239    9  191]
 [  17  244  341]
 [  60   67 5818]]

2023-01-06 17:11:19,305 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:19,305 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:19,311 - 

2023-01-06 17:11:19,311 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:20,029 - Epoch: [178][   10/  246]    Overall Loss 0.248602    Objective Loss 0.248602                                        LR 0.000008    Time 0.071663    
2023-01-06 17:11:20,219 - Epoch: [178][   20/  246]    Overall Loss 0.264149    Objective Loss 0.264149                                        LR 0.000008    Time 0.045345    
2023-01-06 17:11:20,411 - Epoch: [178][   30/  246]    Overall Loss 0.261819    Objective Loss 0.261819                                        LR 0.000008    Time 0.036610    
2023-01-06 17:11:20,599 - Epoch: [178][   40/  246]    Overall Loss 0.266304    Objective Loss 0.266304                                        LR 0.000008    Time 0.032146    
2023-01-06 17:11:20,793 - Epoch: [178][   50/  246]    Overall Loss 0.261280    Objective Loss 0.261280                                        LR 0.000008    Time 0.029578    
2023-01-06 17:11:20,987 - Epoch: [178][   60/  246]    Overall Loss 0.260779    Objective Loss 0.260779                                        LR 0.000008    Time 0.027877    
2023-01-06 17:11:21,184 - Epoch: [178][   70/  246]    Overall Loss 0.261160    Objective Loss 0.261160                                        LR 0.000008    Time 0.026699    
2023-01-06 17:11:21,379 - Epoch: [178][   80/  246]    Overall Loss 0.263610    Objective Loss 0.263610                                        LR 0.000008    Time 0.025801    
2023-01-06 17:11:21,570 - Epoch: [178][   90/  246]    Overall Loss 0.261182    Objective Loss 0.261182                                        LR 0.000008    Time 0.025049    
2023-01-06 17:11:21,766 - Epoch: [178][  100/  246]    Overall Loss 0.259144    Objective Loss 0.259144                                        LR 0.000008    Time 0.024507    
2023-01-06 17:11:21,962 - Epoch: [178][  110/  246]    Overall Loss 0.258489    Objective Loss 0.258489                                        LR 0.000008    Time 0.024056    
2023-01-06 17:11:22,159 - Epoch: [178][  120/  246]    Overall Loss 0.257604    Objective Loss 0.257604                                        LR 0.000008    Time 0.023685    
2023-01-06 17:11:22,356 - Epoch: [178][  130/  246]    Overall Loss 0.256925    Objective Loss 0.256925                                        LR 0.000008    Time 0.023374    
2023-01-06 17:11:22,566 - Epoch: [178][  140/  246]    Overall Loss 0.256682    Objective Loss 0.256682                                        LR 0.000008    Time 0.023205    
2023-01-06 17:11:22,786 - Epoch: [178][  150/  246]    Overall Loss 0.256315    Objective Loss 0.256315                                        LR 0.000008    Time 0.023121    
2023-01-06 17:11:22,998 - Epoch: [178][  160/  246]    Overall Loss 0.257872    Objective Loss 0.257872                                        LR 0.000008    Time 0.022999    
2023-01-06 17:11:23,214 - Epoch: [178][  170/  246]    Overall Loss 0.257124    Objective Loss 0.257124                                        LR 0.000008    Time 0.022912    
2023-01-06 17:11:23,439 - Epoch: [178][  180/  246]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.000008    Time 0.022888    
2023-01-06 17:11:23,667 - Epoch: [178][  190/  246]    Overall Loss 0.257102    Objective Loss 0.257102                                        LR 0.000008    Time 0.022882    
2023-01-06 17:11:23,891 - Epoch: [178][  200/  246]    Overall Loss 0.257091    Objective Loss 0.257091                                        LR 0.000008    Time 0.022857    
2023-01-06 17:11:24,117 - Epoch: [178][  210/  246]    Overall Loss 0.257024    Objective Loss 0.257024                                        LR 0.000008    Time 0.022841    
2023-01-06 17:11:24,328 - Epoch: [178][  220/  246]    Overall Loss 0.256492    Objective Loss 0.256492                                        LR 0.000008    Time 0.022757    
2023-01-06 17:11:24,540 - Epoch: [178][  230/  246]    Overall Loss 0.255410    Objective Loss 0.255410                                        LR 0.000008    Time 0.022691    
2023-01-06 17:11:24,771 - Epoch: [178][  240/  246]    Overall Loss 0.255721    Objective Loss 0.255721                                        LR 0.000008    Time 0.022704    
2023-01-06 17:11:24,862 - Epoch: [178][  246/  246]    Overall Loss 0.256383    Objective Loss 0.256383    Top1 87.799043    LR 0.000008    Time 0.022522    
2023-01-06 17:11:25,003 - --- validate (epoch=178)-----------
2023-01-06 17:11:25,003 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:25,452 - Epoch: [178][   10/   28]    Loss 0.269955    Top1 90.000000    
2023-01-06 17:11:25,566 - Epoch: [178][   20/   28]    Loss 0.271662    Top1 89.726562    
2023-01-06 17:11:25,634 - Epoch: [178][   28/   28]    Loss 0.271891    Top1 90.080160    
2023-01-06 17:11:25,788 - ==> Top1: 90.080    Loss: 0.272

2023-01-06 17:11:25,788 - ==> Confusion:
[[ 235   12  192]
 [  18  257  327]
 [  65   79 5801]]

2023-01-06 17:11:25,790 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:25,790 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:25,796 - 

2023-01-06 17:11:25,796 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:26,392 - Epoch: [179][   10/  246]    Overall Loss 0.261230    Objective Loss 0.261230                                        LR 0.000008    Time 0.059576    
2023-01-06 17:11:26,576 - Epoch: [179][   20/  246]    Overall Loss 0.261749    Objective Loss 0.261749                                        LR 0.000008    Time 0.038950    
2023-01-06 17:11:26,767 - Epoch: [179][   30/  246]    Overall Loss 0.258720    Objective Loss 0.258720                                        LR 0.000008    Time 0.032334    
2023-01-06 17:11:26,959 - Epoch: [179][   40/  246]    Overall Loss 0.252795    Objective Loss 0.252795                                        LR 0.000008    Time 0.029021    
2023-01-06 17:11:27,151 - Epoch: [179][   50/  246]    Overall Loss 0.251001    Objective Loss 0.251001                                        LR 0.000008    Time 0.027066    
2023-01-06 17:11:27,342 - Epoch: [179][   60/  246]    Overall Loss 0.253975    Objective Loss 0.253975                                        LR 0.000008    Time 0.025730    
2023-01-06 17:11:27,533 - Epoch: [179][   70/  246]    Overall Loss 0.254863    Objective Loss 0.254863                                        LR 0.000008    Time 0.024772    
2023-01-06 17:11:27,725 - Epoch: [179][   80/  246]    Overall Loss 0.253994    Objective Loss 0.253994                                        LR 0.000008    Time 0.024063    
2023-01-06 17:11:27,922 - Epoch: [179][   90/  246]    Overall Loss 0.254827    Objective Loss 0.254827                                        LR 0.000008    Time 0.023579    
2023-01-06 17:11:28,117 - Epoch: [179][  100/  246]    Overall Loss 0.257779    Objective Loss 0.257779                                        LR 0.000008    Time 0.023170    
2023-01-06 17:11:28,313 - Epoch: [179][  110/  246]    Overall Loss 0.258234    Objective Loss 0.258234                                        LR 0.000008    Time 0.022843    
2023-01-06 17:11:28,500 - Epoch: [179][  120/  246]    Overall Loss 0.259528    Objective Loss 0.259528                                        LR 0.000008    Time 0.022493    
2023-01-06 17:11:28,688 - Epoch: [179][  130/  246]    Overall Loss 0.260445    Objective Loss 0.260445                                        LR 0.000008    Time 0.022201    
2023-01-06 17:11:28,873 - Epoch: [179][  140/  246]    Overall Loss 0.259558    Objective Loss 0.259558                                        LR 0.000008    Time 0.021933    
2023-01-06 17:11:29,069 - Epoch: [179][  150/  246]    Overall Loss 0.257786    Objective Loss 0.257786                                        LR 0.000008    Time 0.021774    
2023-01-06 17:11:29,264 - Epoch: [179][  160/  246]    Overall Loss 0.256002    Objective Loss 0.256002                                        LR 0.000008    Time 0.021631    
2023-01-06 17:11:29,463 - Epoch: [179][  170/  246]    Overall Loss 0.255939    Objective Loss 0.255939                                        LR 0.000008    Time 0.021526    
2023-01-06 17:11:29,661 - Epoch: [179][  180/  246]    Overall Loss 0.256149    Objective Loss 0.256149                                        LR 0.000008    Time 0.021429    
2023-01-06 17:11:29,861 - Epoch: [179][  190/  246]    Overall Loss 0.255584    Objective Loss 0.255584                                        LR 0.000008    Time 0.021350    
2023-01-06 17:11:30,059 - Epoch: [179][  200/  246]    Overall Loss 0.255928    Objective Loss 0.255928                                        LR 0.000008    Time 0.021272    
2023-01-06 17:11:30,257 - Epoch: [179][  210/  246]    Overall Loss 0.256076    Objective Loss 0.256076                                        LR 0.000008    Time 0.021199    
2023-01-06 17:11:30,455 - Epoch: [179][  220/  246]    Overall Loss 0.256113    Objective Loss 0.256113                                        LR 0.000008    Time 0.021137    
2023-01-06 17:11:30,655 - Epoch: [179][  230/  246]    Overall Loss 0.256322    Objective Loss 0.256322                                        LR 0.000008    Time 0.021083    
2023-01-06 17:11:30,867 - Epoch: [179][  240/  246]    Overall Loss 0.255425    Objective Loss 0.255425                                        LR 0.000008    Time 0.021086    
2023-01-06 17:11:30,964 - Epoch: [179][  246/  246]    Overall Loss 0.255795    Objective Loss 0.255795    Top1 91.866029    LR 0.000008    Time 0.020965    
2023-01-06 17:11:31,085 - --- validate (epoch=179)-----------
2023-01-06 17:11:31,085 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:31,544 - Epoch: [179][   10/   28]    Loss 0.286263    Top1 89.414062    
2023-01-06 17:11:31,663 - Epoch: [179][   20/   28]    Loss 0.274011    Top1 89.902344    
2023-01-06 17:11:31,730 - Epoch: [179][   28/   28]    Loss 0.272862    Top1 89.937017    
2023-01-06 17:11:31,884 - ==> Top1: 89.937    Loss: 0.273

2023-01-06 17:11:31,884 - ==> Confusion:
[[ 218   11  210]
 [  16  227  359]
 [  46   61 5838]]

2023-01-06 17:11:31,885 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:31,885 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:31,891 - 

2023-01-06 17:11:31,891 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:32,573 - Epoch: [180][   10/  246]    Overall Loss 0.257604    Objective Loss 0.257604                                        LR 0.000005    Time 0.068034    
2023-01-06 17:11:32,737 - Epoch: [180][   20/  246]    Overall Loss 0.256656    Objective Loss 0.256656                                        LR 0.000005    Time 0.042237    
2023-01-06 17:11:32,905 - Epoch: [180][   30/  246]    Overall Loss 0.261095    Objective Loss 0.261095                                        LR 0.000005    Time 0.033737    
2023-01-06 17:11:33,076 - Epoch: [180][   40/  246]    Overall Loss 0.255090    Objective Loss 0.255090                                        LR 0.000005    Time 0.029554    
2023-01-06 17:11:33,247 - Epoch: [180][   50/  246]    Overall Loss 0.252704    Objective Loss 0.252704                                        LR 0.000005    Time 0.027072    
2023-01-06 17:11:33,417 - Epoch: [180][   60/  246]    Overall Loss 0.250635    Objective Loss 0.250635                                        LR 0.000005    Time 0.025388    
2023-01-06 17:11:33,583 - Epoch: [180][   70/  246]    Overall Loss 0.253516    Objective Loss 0.253516                                        LR 0.000005    Time 0.024122    
2023-01-06 17:11:33,746 - Epoch: [180][   80/  246]    Overall Loss 0.252366    Objective Loss 0.252366                                        LR 0.000005    Time 0.023146    
2023-01-06 17:11:33,941 - Epoch: [180][   90/  246]    Overall Loss 0.251117    Objective Loss 0.251117                                        LR 0.000005    Time 0.022731    
2023-01-06 17:11:34,115 - Epoch: [180][  100/  246]    Overall Loss 0.253676    Objective Loss 0.253676                                        LR 0.000005    Time 0.022198    
2023-01-06 17:11:34,299 - Epoch: [180][  110/  246]    Overall Loss 0.256081    Objective Loss 0.256081                                        LR 0.000005    Time 0.021849    
2023-01-06 17:11:34,479 - Epoch: [180][  120/  246]    Overall Loss 0.254046    Objective Loss 0.254046                                        LR 0.000005    Time 0.021521    
2023-01-06 17:11:34,683 - Epoch: [180][  130/  246]    Overall Loss 0.253470    Objective Loss 0.253470                                        LR 0.000005    Time 0.021430    
2023-01-06 17:11:34,894 - Epoch: [180][  140/  246]    Overall Loss 0.254766    Objective Loss 0.254766                                        LR 0.000005    Time 0.021406    
2023-01-06 17:11:35,107 - Epoch: [180][  150/  246]    Overall Loss 0.256088    Objective Loss 0.256088                                        LR 0.000005    Time 0.021400    
2023-01-06 17:11:35,308 - Epoch: [180][  160/  246]    Overall Loss 0.256284    Objective Loss 0.256284                                        LR 0.000005    Time 0.021315    
2023-01-06 17:11:35,490 - Epoch: [180][  170/  246]    Overall Loss 0.256665    Objective Loss 0.256665                                        LR 0.000005    Time 0.021128    
2023-01-06 17:11:35,668 - Epoch: [180][  180/  246]    Overall Loss 0.257261    Objective Loss 0.257261                                        LR 0.000005    Time 0.020944    
2023-01-06 17:11:35,850 - Epoch: [180][  190/  246]    Overall Loss 0.257289    Objective Loss 0.257289                                        LR 0.000005    Time 0.020795    
2023-01-06 17:11:36,030 - Epoch: [180][  200/  246]    Overall Loss 0.256689    Objective Loss 0.256689                                        LR 0.000005    Time 0.020652    
2023-01-06 17:11:36,212 - Epoch: [180][  210/  246]    Overall Loss 0.256223    Objective Loss 0.256223                                        LR 0.000005    Time 0.020535    
2023-01-06 17:11:36,389 - Epoch: [180][  220/  246]    Overall Loss 0.256755    Objective Loss 0.256755                                        LR 0.000005    Time 0.020407    
2023-01-06 17:11:36,575 - Epoch: [180][  230/  246]    Overall Loss 0.256149    Objective Loss 0.256149                                        LR 0.000005    Time 0.020324    
2023-01-06 17:11:36,770 - Epoch: [180][  240/  246]    Overall Loss 0.255744    Objective Loss 0.255744                                        LR 0.000005    Time 0.020289    
2023-01-06 17:11:36,861 - Epoch: [180][  246/  246]    Overall Loss 0.255431    Objective Loss 0.255431    Top1 89.234450    LR 0.000005    Time 0.020165    
2023-01-06 17:11:36,997 - --- validate (epoch=180)-----------
2023-01-06 17:11:36,997 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:37,452 - Epoch: [180][   10/   28]    Loss 0.283976    Top1 89.687500    
2023-01-06 17:11:37,576 - Epoch: [180][   20/   28]    Loss 0.273182    Top1 90.117188    
2023-01-06 17:11:37,643 - Epoch: [180][   28/   28]    Loss 0.266531    Top1 90.237618    
2023-01-06 17:11:37,779 - ==> Top1: 90.238    Loss: 0.267

2023-01-06 17:11:37,780 - ==> Confusion:
[[ 242   18  179]
 [  19  262  321]
 [  72   73 5800]]

2023-01-06 17:11:37,781 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:37,781 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:37,787 - 

2023-01-06 17:11:37,787 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:38,347 - Epoch: [181][   10/  246]    Overall Loss 0.256118    Objective Loss 0.256118                                        LR 0.000005    Time 0.055968    
2023-01-06 17:11:38,508 - Epoch: [181][   20/  246]    Overall Loss 0.247812    Objective Loss 0.247812                                        LR 0.000005    Time 0.035998    
2023-01-06 17:11:38,666 - Epoch: [181][   30/  246]    Overall Loss 0.250863    Objective Loss 0.250863                                        LR 0.000005    Time 0.029232    
2023-01-06 17:11:38,837 - Epoch: [181][   40/  246]    Overall Loss 0.248758    Objective Loss 0.248758                                        LR 0.000005    Time 0.026201    
2023-01-06 17:11:39,008 - Epoch: [181][   50/  246]    Overall Loss 0.246570    Objective Loss 0.246570                                        LR 0.000005    Time 0.024368    
2023-01-06 17:11:39,180 - Epoch: [181][   60/  246]    Overall Loss 0.248508    Objective Loss 0.248508                                        LR 0.000005    Time 0.023170    
2023-01-06 17:11:39,345 - Epoch: [181][   70/  246]    Overall Loss 0.246315    Objective Loss 0.246315                                        LR 0.000005    Time 0.022208    
2023-01-06 17:11:39,512 - Epoch: [181][   80/  246]    Overall Loss 0.245823    Objective Loss 0.245823                                        LR 0.000005    Time 0.021516    
2023-01-06 17:11:39,678 - Epoch: [181][   90/  246]    Overall Loss 0.246113    Objective Loss 0.246113                                        LR 0.000005    Time 0.020968    
2023-01-06 17:11:39,847 - Epoch: [181][  100/  246]    Overall Loss 0.247545    Objective Loss 0.247545                                        LR 0.000005    Time 0.020561    
2023-01-06 17:11:40,028 - Epoch: [181][  110/  246]    Overall Loss 0.248283    Objective Loss 0.248283                                        LR 0.000005    Time 0.020331    
2023-01-06 17:11:40,228 - Epoch: [181][  120/  246]    Overall Loss 0.249528    Objective Loss 0.249528                                        LR 0.000005    Time 0.020307    
2023-01-06 17:11:40,424 - Epoch: [181][  130/  246]    Overall Loss 0.252010    Objective Loss 0.252010                                        LR 0.000005    Time 0.020244    
2023-01-06 17:11:40,610 - Epoch: [181][  140/  246]    Overall Loss 0.252764    Objective Loss 0.252764                                        LR 0.000005    Time 0.020129    
2023-01-06 17:11:40,805 - Epoch: [181][  150/  246]    Overall Loss 0.253930    Objective Loss 0.253930                                        LR 0.000005    Time 0.020085    
2023-01-06 17:11:40,992 - Epoch: [181][  160/  246]    Overall Loss 0.253817    Objective Loss 0.253817                                        LR 0.000005    Time 0.019995    
2023-01-06 17:11:41,187 - Epoch: [181][  170/  246]    Overall Loss 0.254740    Objective Loss 0.254740                                        LR 0.000005    Time 0.019965    
2023-01-06 17:11:41,375 - Epoch: [181][  180/  246]    Overall Loss 0.255334    Objective Loss 0.255334                                        LR 0.000005    Time 0.019896    
2023-01-06 17:11:41,570 - Epoch: [181][  190/  246]    Overall Loss 0.255155    Objective Loss 0.255155                                        LR 0.000005    Time 0.019874    
2023-01-06 17:11:41,757 - Epoch: [181][  200/  246]    Overall Loss 0.255451    Objective Loss 0.255451                                        LR 0.000005    Time 0.019815    
2023-01-06 17:11:41,952 - Epoch: [181][  210/  246]    Overall Loss 0.255533    Objective Loss 0.255533                                        LR 0.000005    Time 0.019798    
2023-01-06 17:11:42,144 - Epoch: [181][  220/  246]    Overall Loss 0.254866    Objective Loss 0.254866                                        LR 0.000005    Time 0.019768    
2023-01-06 17:11:42,337 - Epoch: [181][  230/  246]    Overall Loss 0.254712    Objective Loss 0.254712                                        LR 0.000005    Time 0.019746    
2023-01-06 17:11:42,547 - Epoch: [181][  240/  246]    Overall Loss 0.254326    Objective Loss 0.254326                                        LR 0.000005    Time 0.019798    
2023-01-06 17:11:42,643 - Epoch: [181][  246/  246]    Overall Loss 0.254544    Objective Loss 0.254544    Top1 89.234450    LR 0.000005    Time 0.019703    
2023-01-06 17:11:42,774 - --- validate (epoch=181)-----------
2023-01-06 17:11:42,774 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:43,230 - Epoch: [181][   10/   28]    Loss 0.251439    Top1 91.054688    
2023-01-06 17:11:43,346 - Epoch: [181][   20/   28]    Loss 0.260843    Top1 90.644531    
2023-01-06 17:11:43,413 - Epoch: [181][   28/   28]    Loss 0.271781    Top1 90.266247    
2023-01-06 17:11:43,558 - ==> Top1: 90.266    Loss: 0.272

2023-01-06 17:11:43,558 - ==> Confusion:
[[ 235   16  188]
 [  19  257  326]
 [  60   71 5814]]

2023-01-06 17:11:43,559 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:43,559 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:43,565 - 

2023-01-06 17:11:43,565 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:44,254 - Epoch: [182][   10/  246]    Overall Loss 0.246168    Objective Loss 0.246168                                        LR 0.000005    Time 0.068809    
2023-01-06 17:11:44,448 - Epoch: [182][   20/  246]    Overall Loss 0.250502    Objective Loss 0.250502                                        LR 0.000005    Time 0.044093    
2023-01-06 17:11:44,653 - Epoch: [182][   30/  246]    Overall Loss 0.248757    Objective Loss 0.248757                                        LR 0.000005    Time 0.036192    
2023-01-06 17:11:44,852 - Epoch: [182][   40/  246]    Overall Loss 0.246601    Objective Loss 0.246601                                        LR 0.000005    Time 0.032116    
2023-01-06 17:11:45,054 - Epoch: [182][   50/  246]    Overall Loss 0.253698    Objective Loss 0.253698                                        LR 0.000005    Time 0.029734    
2023-01-06 17:11:45,252 - Epoch: [182][   60/  246]    Overall Loss 0.254065    Objective Loss 0.254065                                        LR 0.000005    Time 0.028072    
2023-01-06 17:11:45,450 - Epoch: [182][   70/  246]    Overall Loss 0.254052    Objective Loss 0.254052                                        LR 0.000005    Time 0.026876    
2023-01-06 17:11:45,645 - Epoch: [182][   80/  246]    Overall Loss 0.252574    Objective Loss 0.252574                                        LR 0.000005    Time 0.025957    
2023-01-06 17:11:45,848 - Epoch: [182][   90/  246]    Overall Loss 0.251991    Objective Loss 0.251991                                        LR 0.000005    Time 0.025322    
2023-01-06 17:11:46,052 - Epoch: [182][  100/  246]    Overall Loss 0.252055    Objective Loss 0.252055                                        LR 0.000005    Time 0.024822    
2023-01-06 17:11:46,257 - Epoch: [182][  110/  246]    Overall Loss 0.252282    Objective Loss 0.252282                                        LR 0.000005    Time 0.024424    
2023-01-06 17:11:46,464 - Epoch: [182][  120/  246]    Overall Loss 0.253024    Objective Loss 0.253024                                        LR 0.000005    Time 0.024110    
2023-01-06 17:11:46,670 - Epoch: [182][  130/  246]    Overall Loss 0.252140    Objective Loss 0.252140                                        LR 0.000005    Time 0.023844    
2023-01-06 17:11:46,875 - Epoch: [182][  140/  246]    Overall Loss 0.251485    Objective Loss 0.251485                                        LR 0.000005    Time 0.023598    
2023-01-06 17:11:47,082 - Epoch: [182][  150/  246]    Overall Loss 0.250813    Objective Loss 0.250813                                        LR 0.000005    Time 0.023402    
2023-01-06 17:11:47,284 - Epoch: [182][  160/  246]    Overall Loss 0.251696    Objective Loss 0.251696                                        LR 0.000005    Time 0.023198    
2023-01-06 17:11:47,461 - Epoch: [182][  170/  246]    Overall Loss 0.251471    Objective Loss 0.251471                                        LR 0.000005    Time 0.022877    
2023-01-06 17:11:47,646 - Epoch: [182][  180/  246]    Overall Loss 0.251815    Objective Loss 0.251815                                        LR 0.000005    Time 0.022630    
2023-01-06 17:11:47,822 - Epoch: [182][  190/  246]    Overall Loss 0.252810    Objective Loss 0.252810                                        LR 0.000005    Time 0.022362    
2023-01-06 17:11:47,997 - Epoch: [182][  200/  246]    Overall Loss 0.252692    Objective Loss 0.252692                                        LR 0.000005    Time 0.022120    
2023-01-06 17:11:48,165 - Epoch: [182][  210/  246]    Overall Loss 0.253485    Objective Loss 0.253485                                        LR 0.000005    Time 0.021863    
2023-01-06 17:11:48,334 - Epoch: [182][  220/  246]    Overall Loss 0.252927    Objective Loss 0.252927                                        LR 0.000005    Time 0.021638    
2023-01-06 17:11:48,501 - Epoch: [182][  230/  246]    Overall Loss 0.253857    Objective Loss 0.253857                                        LR 0.000005    Time 0.021423    
2023-01-06 17:11:48,682 - Epoch: [182][  240/  246]    Overall Loss 0.253546    Objective Loss 0.253546                                        LR 0.000005    Time 0.021283    
2023-01-06 17:11:48,774 - Epoch: [182][  246/  246]    Overall Loss 0.253976    Objective Loss 0.253976    Top1 90.430622    LR 0.000005    Time 0.021138    
2023-01-06 17:11:48,903 - --- validate (epoch=182)-----------
2023-01-06 17:11:48,903 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:49,344 - Epoch: [182][   10/   28]    Loss 0.264834    Top1 90.546875    
2023-01-06 17:11:49,454 - Epoch: [182][   20/   28]    Loss 0.267742    Top1 90.468750    
2023-01-06 17:11:49,522 - Epoch: [182][   28/   28]    Loss 0.269212    Top1 90.151732    
2023-01-06 17:11:49,691 - ==> Top1: 90.152    Loss: 0.269

2023-01-06 17:11:49,691 - ==> Confusion:
[[ 228   17  194]
 [  15  276  311]
 [  57   94 5794]]

2023-01-06 17:11:49,693 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:49,693 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:49,699 - 

2023-01-06 17:11:49,699 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:50,416 - Epoch: [183][   10/  246]    Overall Loss 0.254991    Objective Loss 0.254991                                        LR 0.000005    Time 0.071648    
2023-01-06 17:11:50,605 - Epoch: [183][   20/  246]    Overall Loss 0.251834    Objective Loss 0.251834                                        LR 0.000005    Time 0.045239    
2023-01-06 17:11:50,799 - Epoch: [183][   30/  246]    Overall Loss 0.252542    Objective Loss 0.252542                                        LR 0.000005    Time 0.036626    
2023-01-06 17:11:50,997 - Epoch: [183][   40/  246]    Overall Loss 0.256724    Objective Loss 0.256724                                        LR 0.000005    Time 0.032394    
2023-01-06 17:11:51,190 - Epoch: [183][   50/  246]    Overall Loss 0.259748    Objective Loss 0.259748                                        LR 0.000005    Time 0.029765    
2023-01-06 17:11:51,388 - Epoch: [183][   60/  246]    Overall Loss 0.259172    Objective Loss 0.259172                                        LR 0.000005    Time 0.028107    
2023-01-06 17:11:51,582 - Epoch: [183][   70/  246]    Overall Loss 0.258493    Objective Loss 0.258493                                        LR 0.000005    Time 0.026861    
2023-01-06 17:11:51,780 - Epoch: [183][   80/  246]    Overall Loss 0.258234    Objective Loss 0.258234                                        LR 0.000005    Time 0.025946    
2023-01-06 17:11:51,977 - Epoch: [183][   90/  246]    Overall Loss 0.256145    Objective Loss 0.256145                                        LR 0.000005    Time 0.025249    
2023-01-06 17:11:52,164 - Epoch: [183][  100/  246]    Overall Loss 0.256873    Objective Loss 0.256873                                        LR 0.000005    Time 0.024590    
2023-01-06 17:11:52,341 - Epoch: [183][  110/  246]    Overall Loss 0.258122    Objective Loss 0.258122                                        LR 0.000005    Time 0.023962    
2023-01-06 17:11:52,518 - Epoch: [183][  120/  246]    Overall Loss 0.256847    Objective Loss 0.256847                                        LR 0.000005    Time 0.023442    
2023-01-06 17:11:52,697 - Epoch: [183][  130/  246]    Overall Loss 0.257181    Objective Loss 0.257181                                        LR 0.000005    Time 0.023010    
2023-01-06 17:11:52,876 - Epoch: [183][  140/  246]    Overall Loss 0.257475    Objective Loss 0.257475                                        LR 0.000005    Time 0.022642    
2023-01-06 17:11:53,062 - Epoch: [183][  150/  246]    Overall Loss 0.257447    Objective Loss 0.257447                                        LR 0.000005    Time 0.022369    
2023-01-06 17:11:53,246 - Epoch: [183][  160/  246]    Overall Loss 0.257058    Objective Loss 0.257058                                        LR 0.000005    Time 0.022119    
2023-01-06 17:11:53,429 - Epoch: [183][  170/  246]    Overall Loss 0.257125    Objective Loss 0.257125                                        LR 0.000005    Time 0.021892    
2023-01-06 17:11:53,607 - Epoch: [183][  180/  246]    Overall Loss 0.257685    Objective Loss 0.257685                                        LR 0.000005    Time 0.021664    
2023-01-06 17:11:53,785 - Epoch: [183][  190/  246]    Overall Loss 0.257564    Objective Loss 0.257564                                        LR 0.000005    Time 0.021457    
2023-01-06 17:11:53,967 - Epoch: [183][  200/  246]    Overall Loss 0.256394    Objective Loss 0.256394                                        LR 0.000005    Time 0.021294    
2023-01-06 17:11:54,149 - Epoch: [183][  210/  246]    Overall Loss 0.256683    Objective Loss 0.256683                                        LR 0.000005    Time 0.021145    
2023-01-06 17:11:54,333 - Epoch: [183][  220/  246]    Overall Loss 0.256845    Objective Loss 0.256845                                        LR 0.000005    Time 0.021018    
2023-01-06 17:11:54,514 - Epoch: [183][  230/  246]    Overall Loss 0.255403    Objective Loss 0.255403                                        LR 0.000005    Time 0.020890    
2023-01-06 17:11:54,711 - Epoch: [183][  240/  246]    Overall Loss 0.254436    Objective Loss 0.254436                                        LR 0.000005    Time 0.020839    
2023-01-06 17:11:54,806 - Epoch: [183][  246/  246]    Overall Loss 0.254717    Objective Loss 0.254717    Top1 88.755981    LR 0.000005    Time 0.020718    
2023-01-06 17:11:54,940 - --- validate (epoch=183)-----------
2023-01-06 17:11:54,940 - 6986 samples (256 per mini-batch)
2023-01-06 17:11:55,388 - Epoch: [183][   10/   28]    Loss 0.260820    Top1 90.703125    
2023-01-06 17:11:55,502 - Epoch: [183][   20/   28]    Loss 0.267793    Top1 90.410156    
2023-01-06 17:11:55,571 - Epoch: [183][   28/   28]    Loss 0.267270    Top1 90.294875    
2023-01-06 17:11:55,697 - ==> Top1: 90.295    Loss: 0.267

2023-01-06 17:11:55,698 - ==> Confusion:
[[ 240   15  184]
 [  17  272  313]
 [  63   86 5796]]

2023-01-06 17:11:55,699 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:11:55,699 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:11:55,705 - 

2023-01-06 17:11:55,705 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:11:56,280 - Epoch: [184][   10/  246]    Overall Loss 0.250316    Objective Loss 0.250316                                        LR 0.000005    Time 0.057418    
2023-01-06 17:11:56,474 - Epoch: [184][   20/  246]    Overall Loss 0.254506    Objective Loss 0.254506                                        LR 0.000005    Time 0.038368    
2023-01-06 17:11:56,643 - Epoch: [184][   30/  246]    Overall Loss 0.258037    Objective Loss 0.258037                                        LR 0.000005    Time 0.031218    
2023-01-06 17:11:56,819 - Epoch: [184][   40/  246]    Overall Loss 0.253037    Objective Loss 0.253037                                        LR 0.000005    Time 0.027807    
2023-01-06 17:11:57,009 - Epoch: [184][   50/  246]    Overall Loss 0.252672    Objective Loss 0.252672                                        LR 0.000005    Time 0.026039    
2023-01-06 17:11:57,173 - Epoch: [184][   60/  246]    Overall Loss 0.251751    Objective Loss 0.251751                                        LR 0.000005    Time 0.024429    
2023-01-06 17:11:57,347 - Epoch: [184][   70/  246]    Overall Loss 0.251783    Objective Loss 0.251783                                        LR 0.000005    Time 0.023407    
2023-01-06 17:11:57,534 - Epoch: [184][   80/  246]    Overall Loss 0.255751    Objective Loss 0.255751                                        LR 0.000005    Time 0.022818    
2023-01-06 17:11:57,728 - Epoch: [184][   90/  246]    Overall Loss 0.255597    Objective Loss 0.255597                                        LR 0.000005    Time 0.022436    
2023-01-06 17:11:57,933 - Epoch: [184][  100/  246]    Overall Loss 0.255913    Objective Loss 0.255913                                        LR 0.000005    Time 0.022235    
2023-01-06 17:11:58,154 - Epoch: [184][  110/  246]    Overall Loss 0.254317    Objective Loss 0.254317                                        LR 0.000005    Time 0.022227    
2023-01-06 17:11:58,374 - Epoch: [184][  120/  246]    Overall Loss 0.254192    Objective Loss 0.254192                                        LR 0.000005    Time 0.022196    
2023-01-06 17:11:58,594 - Epoch: [184][  130/  246]    Overall Loss 0.254448    Objective Loss 0.254448                                        LR 0.000005    Time 0.022179    
2023-01-06 17:11:58,810 - Epoch: [184][  140/  246]    Overall Loss 0.254349    Objective Loss 0.254349                                        LR 0.000005    Time 0.022140    
2023-01-06 17:11:59,033 - Epoch: [184][  150/  246]    Overall Loss 0.253502    Objective Loss 0.253502                                        LR 0.000005    Time 0.022145    
2023-01-06 17:11:59,249 - Epoch: [184][  160/  246]    Overall Loss 0.254987    Objective Loss 0.254987                                        LR 0.000005    Time 0.022110    
2023-01-06 17:11:59,468 - Epoch: [184][  170/  246]    Overall Loss 0.255464    Objective Loss 0.255464                                        LR 0.000005    Time 0.022091    
2023-01-06 17:11:59,685 - Epoch: [184][  180/  246]    Overall Loss 0.255198    Objective Loss 0.255198                                        LR 0.000005    Time 0.022070    
2023-01-06 17:11:59,909 - Epoch: [184][  190/  246]    Overall Loss 0.255647    Objective Loss 0.255647                                        LR 0.000005    Time 0.022082    
2023-01-06 17:12:00,130 - Epoch: [184][  200/  246]    Overall Loss 0.255458    Objective Loss 0.255458                                        LR 0.000005    Time 0.022085    
2023-01-06 17:12:00,351 - Epoch: [184][  210/  246]    Overall Loss 0.254932    Objective Loss 0.254932                                        LR 0.000005    Time 0.022084    
2023-01-06 17:12:00,581 - Epoch: [184][  220/  246]    Overall Loss 0.254545    Objective Loss 0.254545                                        LR 0.000005    Time 0.022123    
2023-01-06 17:12:00,790 - Epoch: [184][  230/  246]    Overall Loss 0.254302    Objective Loss 0.254302                                        LR 0.000005    Time 0.022066    
2023-01-06 17:12:01,034 - Epoch: [184][  240/  246]    Overall Loss 0.254274    Objective Loss 0.254274                                        LR 0.000005    Time 0.022161    
2023-01-06 17:12:01,144 - Epoch: [184][  246/  246]    Overall Loss 0.253867    Objective Loss 0.253867    Top1 92.583732    LR 0.000005    Time 0.022067    
2023-01-06 17:12:01,320 - --- validate (epoch=184)-----------
2023-01-06 17:12:01,320 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:01,798 - Epoch: [184][   10/   28]    Loss 0.256038    Top1 90.859375    
2023-01-06 17:12:01,929 - Epoch: [184][   20/   28]    Loss 0.269537    Top1 90.000000    
2023-01-06 17:12:02,000 - Epoch: [184][   28/   28]    Loss 0.275648    Top1 89.965646    
2023-01-06 17:12:02,148 - ==> Top1: 89.966    Loss: 0.276

2023-01-06 17:12:02,149 - ==> Confusion:
[[ 218   11  210]
 [  16  224  362]
 [  46   56 5843]]

2023-01-06 17:12:02,150 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:02,150 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:02,156 - 

2023-01-06 17:12:02,156 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:02,870 - Epoch: [185][   10/  246]    Overall Loss 0.268149    Objective Loss 0.268149                                        LR 0.000005    Time 0.071301    
2023-01-06 17:12:03,062 - Epoch: [185][   20/  246]    Overall Loss 0.267127    Objective Loss 0.267127                                        LR 0.000005    Time 0.045209    
2023-01-06 17:12:03,250 - Epoch: [185][   30/  246]    Overall Loss 0.264023    Objective Loss 0.264023                                        LR 0.000005    Time 0.036414    
2023-01-06 17:12:03,439 - Epoch: [185][   40/  246]    Overall Loss 0.263302    Objective Loss 0.263302                                        LR 0.000005    Time 0.032023    
2023-01-06 17:12:03,624 - Epoch: [185][   50/  246]    Overall Loss 0.263912    Objective Loss 0.263912                                        LR 0.000005    Time 0.029312    
2023-01-06 17:12:03,810 - Epoch: [185][   60/  246]    Overall Loss 0.264983    Objective Loss 0.264983                                        LR 0.000005    Time 0.027519    
2023-01-06 17:12:03,998 - Epoch: [185][   70/  246]    Overall Loss 0.262002    Objective Loss 0.262002                                        LR 0.000005    Time 0.026259    
2023-01-06 17:12:04,186 - Epoch: [185][   80/  246]    Overall Loss 0.261404    Objective Loss 0.261404                                        LR 0.000005    Time 0.025331    
2023-01-06 17:12:04,371 - Epoch: [185][   90/  246]    Overall Loss 0.260863    Objective Loss 0.260863                                        LR 0.000005    Time 0.024568    
2023-01-06 17:12:04,559 - Epoch: [185][  100/  246]    Overall Loss 0.260675    Objective Loss 0.260675                                        LR 0.000005    Time 0.023989    
2023-01-06 17:12:04,744 - Epoch: [185][  110/  246]    Overall Loss 0.259885    Objective Loss 0.259885                                        LR 0.000005    Time 0.023481    
2023-01-06 17:12:04,935 - Epoch: [185][  120/  246]    Overall Loss 0.258766    Objective Loss 0.258766                                        LR 0.000005    Time 0.023116    
2023-01-06 17:12:05,127 - Epoch: [185][  130/  246]    Overall Loss 0.259532    Objective Loss 0.259532                                        LR 0.000005    Time 0.022812    
2023-01-06 17:12:05,317 - Epoch: [185][  140/  246]    Overall Loss 0.258698    Objective Loss 0.258698                                        LR 0.000005    Time 0.022534    
2023-01-06 17:12:05,510 - Epoch: [185][  150/  246]    Overall Loss 0.257415    Objective Loss 0.257415                                        LR 0.000005    Time 0.022320    
2023-01-06 17:12:05,701 - Epoch: [185][  160/  246]    Overall Loss 0.256648    Objective Loss 0.256648                                        LR 0.000005    Time 0.022116    
2023-01-06 17:12:05,894 - Epoch: [185][  170/  246]    Overall Loss 0.255945    Objective Loss 0.255945                                        LR 0.000005    Time 0.021947    
2023-01-06 17:12:06,085 - Epoch: [185][  180/  246]    Overall Loss 0.255851    Objective Loss 0.255851                                        LR 0.000005    Time 0.021786    
2023-01-06 17:12:06,276 - Epoch: [185][  190/  246]    Overall Loss 0.256742    Objective Loss 0.256742                                        LR 0.000005    Time 0.021641    
2023-01-06 17:12:06,468 - Epoch: [185][  200/  246]    Overall Loss 0.255992    Objective Loss 0.255992                                        LR 0.000005    Time 0.021518    
2023-01-06 17:12:06,660 - Epoch: [185][  210/  246]    Overall Loss 0.254555    Objective Loss 0.254555                                        LR 0.000005    Time 0.021405    
2023-01-06 17:12:06,853 - Epoch: [185][  220/  246]    Overall Loss 0.254891    Objective Loss 0.254891                                        LR 0.000005    Time 0.021311    
2023-01-06 17:12:07,045 - Epoch: [185][  230/  246]    Overall Loss 0.254155    Objective Loss 0.254155                                        LR 0.000005    Time 0.021214    
2023-01-06 17:12:07,248 - Epoch: [185][  240/  246]    Overall Loss 0.254547    Objective Loss 0.254547                                        LR 0.000005    Time 0.021175    
2023-01-06 17:12:07,346 - Epoch: [185][  246/  246]    Overall Loss 0.254048    Objective Loss 0.254048    Top1 92.105263    LR 0.000005    Time 0.021057    
2023-01-06 17:12:07,489 - --- validate (epoch=185)-----------
2023-01-06 17:12:07,489 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:07,942 - Epoch: [185][   10/   28]    Loss 0.261203    Top1 90.507812    
2023-01-06 17:12:08,059 - Epoch: [185][   20/   28]    Loss 0.267907    Top1 90.410156    
2023-01-06 17:12:08,128 - Epoch: [185][   28/   28]    Loss 0.269091    Top1 90.180361    
2023-01-06 17:12:08,291 - ==> Top1: 90.180    Loss: 0.269

2023-01-06 17:12:08,291 - ==> Confusion:
[[ 236   16  187]
 [  16  250  336]
 [  59   72 5814]]

2023-01-06 17:12:08,293 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:08,293 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:08,299 - 

2023-01-06 17:12:08,299 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:08,862 - Epoch: [186][   10/  246]    Overall Loss 0.241167    Objective Loss 0.241167                                        LR 0.000005    Time 0.056238    
2023-01-06 17:12:09,024 - Epoch: [186][   20/  246]    Overall Loss 0.231366    Objective Loss 0.231366                                        LR 0.000005    Time 0.036226    
2023-01-06 17:12:09,188 - Epoch: [186][   30/  246]    Overall Loss 0.238017    Objective Loss 0.238017                                        LR 0.000005    Time 0.029604    
2023-01-06 17:12:09,355 - Epoch: [186][   40/  246]    Overall Loss 0.243026    Objective Loss 0.243026                                        LR 0.000005    Time 0.026375    
2023-01-06 17:12:09,526 - Epoch: [186][   50/  246]    Overall Loss 0.240898    Objective Loss 0.240898                                        LR 0.000005    Time 0.024512    
2023-01-06 17:12:09,692 - Epoch: [186][   60/  246]    Overall Loss 0.241271    Objective Loss 0.241271                                        LR 0.000005    Time 0.023190    
2023-01-06 17:12:09,855 - Epoch: [186][   70/  246]    Overall Loss 0.244365    Objective Loss 0.244365                                        LR 0.000005    Time 0.022189    
2023-01-06 17:12:10,017 - Epoch: [186][   80/  246]    Overall Loss 0.244830    Objective Loss 0.244830                                        LR 0.000005    Time 0.021440    
2023-01-06 17:12:10,179 - Epoch: [186][   90/  246]    Overall Loss 0.246144    Objective Loss 0.246144                                        LR 0.000005    Time 0.020857    
2023-01-06 17:12:10,341 - Epoch: [186][  100/  246]    Overall Loss 0.246464    Objective Loss 0.246464                                        LR 0.000005    Time 0.020385    
2023-01-06 17:12:10,511 - Epoch: [186][  110/  246]    Overall Loss 0.246742    Objective Loss 0.246742                                        LR 0.000005    Time 0.020079    
2023-01-06 17:12:10,687 - Epoch: [186][  120/  246]    Overall Loss 0.248190    Objective Loss 0.248190                                        LR 0.000005    Time 0.019871    
2023-01-06 17:12:10,867 - Epoch: [186][  130/  246]    Overall Loss 0.248550    Objective Loss 0.248550                                        LR 0.000005    Time 0.019719    
2023-01-06 17:12:11,037 - Epoch: [186][  140/  246]    Overall Loss 0.250601    Objective Loss 0.250601                                        LR 0.000005    Time 0.019529    
2023-01-06 17:12:11,207 - Epoch: [186][  150/  246]    Overall Loss 0.251312    Objective Loss 0.251312                                        LR 0.000005    Time 0.019356    
2023-01-06 17:12:11,374 - Epoch: [186][  160/  246]    Overall Loss 0.252230    Objective Loss 0.252230                                        LR 0.000005    Time 0.019185    
2023-01-06 17:12:11,552 - Epoch: [186][  170/  246]    Overall Loss 0.251711    Objective Loss 0.251711                                        LR 0.000005    Time 0.019102    
2023-01-06 17:12:11,730 - Epoch: [186][  180/  246]    Overall Loss 0.251849    Objective Loss 0.251849                                        LR 0.000005    Time 0.019030    
2023-01-06 17:12:11,909 - Epoch: [186][  190/  246]    Overall Loss 0.252194    Objective Loss 0.252194                                        LR 0.000005    Time 0.018968    
2023-01-06 17:12:12,082 - Epoch: [186][  200/  246]    Overall Loss 0.253118    Objective Loss 0.253118                                        LR 0.000005    Time 0.018883    
2023-01-06 17:12:12,261 - Epoch: [186][  210/  246]    Overall Loss 0.253457    Objective Loss 0.253457                                        LR 0.000005    Time 0.018836    
2023-01-06 17:12:12,437 - Epoch: [186][  220/  246]    Overall Loss 0.253842    Objective Loss 0.253842                                        LR 0.000005    Time 0.018777    
2023-01-06 17:12:12,616 - Epoch: [186][  230/  246]    Overall Loss 0.253671    Objective Loss 0.253671                                        LR 0.000005    Time 0.018738    
2023-01-06 17:12:12,801 - Epoch: [186][  240/  246]    Overall Loss 0.253824    Objective Loss 0.253824                                        LR 0.000005    Time 0.018729    
2023-01-06 17:12:12,889 - Epoch: [186][  246/  246]    Overall Loss 0.254263    Objective Loss 0.254263    Top1 89.712919    LR 0.000005    Time 0.018625    
2023-01-06 17:12:13,031 - --- validate (epoch=186)-----------
2023-01-06 17:12:13,032 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:13,499 - Epoch: [186][   10/   28]    Loss 0.264152    Top1 90.195312    
2023-01-06 17:12:13,618 - Epoch: [186][   20/   28]    Loss 0.273556    Top1 90.000000    
2023-01-06 17:12:13,685 - Epoch: [186][   28/   28]    Loss 0.269995    Top1 90.180361    
2023-01-06 17:12:13,832 - ==> Top1: 90.180    Loss: 0.270

2023-01-06 17:12:13,832 - ==> Confusion:
[[ 234   11  194]
 [  18  242  342]
 [  60   61 5824]]

2023-01-06 17:12:13,833 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:13,833 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:13,839 - 

2023-01-06 17:12:13,839 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:14,539 - Epoch: [187][   10/  246]    Overall Loss 0.241993    Objective Loss 0.241993                                        LR 0.000005    Time 0.069910    
2023-01-06 17:12:14,717 - Epoch: [187][   20/  246]    Overall Loss 0.250446    Objective Loss 0.250446                                        LR 0.000005    Time 0.043802    
2023-01-06 17:12:14,883 - Epoch: [187][   30/  246]    Overall Loss 0.253217    Objective Loss 0.253217                                        LR 0.000005    Time 0.034740    
2023-01-06 17:12:15,062 - Epoch: [187][   40/  246]    Overall Loss 0.248014    Objective Loss 0.248014                                        LR 0.000005    Time 0.030518    
2023-01-06 17:12:15,256 - Epoch: [187][   50/  246]    Overall Loss 0.249347    Objective Loss 0.249347                                        LR 0.000005    Time 0.028284    
2023-01-06 17:12:15,448 - Epoch: [187][   60/  246]    Overall Loss 0.251004    Objective Loss 0.251004                                        LR 0.000005    Time 0.026765    
2023-01-06 17:12:15,634 - Epoch: [187][   70/  246]    Overall Loss 0.252596    Objective Loss 0.252596                                        LR 0.000005    Time 0.025562    
2023-01-06 17:12:15,830 - Epoch: [187][   80/  246]    Overall Loss 0.253466    Objective Loss 0.253466                                        LR 0.000005    Time 0.024816    
2023-01-06 17:12:16,031 - Epoch: [187][   90/  246]    Overall Loss 0.254052    Objective Loss 0.254052                                        LR 0.000005    Time 0.024286    
2023-01-06 17:12:16,232 - Epoch: [187][  100/  246]    Overall Loss 0.252426    Objective Loss 0.252426                                        LR 0.000005    Time 0.023861    
2023-01-06 17:12:16,433 - Epoch: [187][  110/  246]    Overall Loss 0.251995    Objective Loss 0.251995                                        LR 0.000005    Time 0.023516    
2023-01-06 17:12:16,648 - Epoch: [187][  120/  246]    Overall Loss 0.253232    Objective Loss 0.253232                                        LR 0.000005    Time 0.023347    
2023-01-06 17:12:16,847 - Epoch: [187][  130/  246]    Overall Loss 0.253800    Objective Loss 0.253800                                        LR 0.000005    Time 0.023075    
2023-01-06 17:12:17,047 - Epoch: [187][  140/  246]    Overall Loss 0.254779    Objective Loss 0.254779                                        LR 0.000005    Time 0.022853    
2023-01-06 17:12:17,247 - Epoch: [187][  150/  246]    Overall Loss 0.255437    Objective Loss 0.255437                                        LR 0.000005    Time 0.022660    
2023-01-06 17:12:17,446 - Epoch: [187][  160/  246]    Overall Loss 0.255666    Objective Loss 0.255666                                        LR 0.000005    Time 0.022490    
2023-01-06 17:12:17,645 - Epoch: [187][  170/  246]    Overall Loss 0.254856    Objective Loss 0.254856                                        LR 0.000005    Time 0.022331    
2023-01-06 17:12:17,843 - Epoch: [187][  180/  246]    Overall Loss 0.254444    Objective Loss 0.254444                                        LR 0.000005    Time 0.022188    
2023-01-06 17:12:18,039 - Epoch: [187][  190/  246]    Overall Loss 0.254440    Objective Loss 0.254440                                        LR 0.000005    Time 0.022054    
2023-01-06 17:12:18,239 - Epoch: [187][  200/  246]    Overall Loss 0.254807    Objective Loss 0.254807                                        LR 0.000005    Time 0.021948    
2023-01-06 17:12:18,431 - Epoch: [187][  210/  246]    Overall Loss 0.254773    Objective Loss 0.254773                                        LR 0.000005    Time 0.021813    
2023-01-06 17:12:18,653 - Epoch: [187][  220/  246]    Overall Loss 0.254623    Objective Loss 0.254623                                        LR 0.000005    Time 0.021831    
2023-01-06 17:12:18,874 - Epoch: [187][  230/  246]    Overall Loss 0.253103    Objective Loss 0.253103                                        LR 0.000005    Time 0.021839    
2023-01-06 17:12:19,107 - Epoch: [187][  240/  246]    Overall Loss 0.253148    Objective Loss 0.253148                                        LR 0.000005    Time 0.021899    
2023-01-06 17:12:19,202 - Epoch: [187][  246/  246]    Overall Loss 0.253467    Objective Loss 0.253467    Top1 90.669856    LR 0.000005    Time 0.021752    
2023-01-06 17:12:19,334 - --- validate (epoch=187)-----------
2023-01-06 17:12:19,334 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:19,793 - Epoch: [187][   10/   28]    Loss 0.277263    Top1 89.609375    
2023-01-06 17:12:19,913 - Epoch: [187][   20/   28]    Loss 0.272469    Top1 90.214844    
2023-01-06 17:12:19,980 - Epoch: [187][   28/   28]    Loss 0.265149    Top1 90.337818    
2023-01-06 17:12:20,140 - ==> Top1: 90.338    Loss: 0.265

2023-01-06 17:12:20,140 - ==> Confusion:
[[ 234   15  190]
 [  17  264  321]
 [  59   73 5813]]

2023-01-06 17:12:20,141 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:20,141 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:20,147 - 

2023-01-06 17:12:20,147 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:20,852 - Epoch: [188][   10/  246]    Overall Loss 0.257598    Objective Loss 0.257598                                        LR 0.000005    Time 0.070420    
2023-01-06 17:12:21,049 - Epoch: [188][   20/  246]    Overall Loss 0.249974    Objective Loss 0.249974                                        LR 0.000005    Time 0.045044    
2023-01-06 17:12:21,241 - Epoch: [188][   30/  246]    Overall Loss 0.263410    Objective Loss 0.263410                                        LR 0.000005    Time 0.036406    
2023-01-06 17:12:21,440 - Epoch: [188][   40/  246]    Overall Loss 0.265786    Objective Loss 0.265786                                        LR 0.000005    Time 0.032278    
2023-01-06 17:12:21,639 - Epoch: [188][   50/  246]    Overall Loss 0.263814    Objective Loss 0.263814                                        LR 0.000005    Time 0.029784    
2023-01-06 17:12:21,839 - Epoch: [188][   60/  246]    Overall Loss 0.260865    Objective Loss 0.260865                                        LR 0.000005    Time 0.028152    
2023-01-06 17:12:22,037 - Epoch: [188][   70/  246]    Overall Loss 0.259342    Objective Loss 0.259342                                        LR 0.000005    Time 0.026952    
2023-01-06 17:12:22,220 - Epoch: [188][   80/  246]    Overall Loss 0.258990    Objective Loss 0.258990                                        LR 0.000005    Time 0.025863    
2023-01-06 17:12:22,394 - Epoch: [188][   90/  246]    Overall Loss 0.257535    Objective Loss 0.257535                                        LR 0.000005    Time 0.024920    
2023-01-06 17:12:22,566 - Epoch: [188][  100/  246]    Overall Loss 0.256942    Objective Loss 0.256942                                        LR 0.000005    Time 0.024142    
2023-01-06 17:12:22,739 - Epoch: [188][  110/  246]    Overall Loss 0.257458    Objective Loss 0.257458                                        LR 0.000005    Time 0.023523    
2023-01-06 17:12:22,915 - Epoch: [188][  120/  246]    Overall Loss 0.256047    Objective Loss 0.256047                                        LR 0.000005    Time 0.023029    
2023-01-06 17:12:23,098 - Epoch: [188][  130/  246]    Overall Loss 0.255124    Objective Loss 0.255124                                        LR 0.000005    Time 0.022662    
2023-01-06 17:12:23,274 - Epoch: [188][  140/  246]    Overall Loss 0.256492    Objective Loss 0.256492                                        LR 0.000005    Time 0.022295    
2023-01-06 17:12:23,457 - Epoch: [188][  150/  246]    Overall Loss 0.256058    Objective Loss 0.256058                                        LR 0.000005    Time 0.022030    
2023-01-06 17:12:23,638 - Epoch: [188][  160/  246]    Overall Loss 0.255913    Objective Loss 0.255913                                        LR 0.000005    Time 0.021781    
2023-01-06 17:12:23,815 - Epoch: [188][  170/  246]    Overall Loss 0.254815    Objective Loss 0.254815                                        LR 0.000005    Time 0.021535    
2023-01-06 17:12:23,998 - Epoch: [188][  180/  246]    Overall Loss 0.253678    Objective Loss 0.253678                                        LR 0.000005    Time 0.021357    
2023-01-06 17:12:24,187 - Epoch: [188][  190/  246]    Overall Loss 0.254283    Objective Loss 0.254283                                        LR 0.000005    Time 0.021226    
2023-01-06 17:12:24,374 - Epoch: [188][  200/  246]    Overall Loss 0.253393    Objective Loss 0.253393                                        LR 0.000005    Time 0.021099    
2023-01-06 17:12:24,561 - Epoch: [188][  210/  246]    Overall Loss 0.253188    Objective Loss 0.253188                                        LR 0.000005    Time 0.020983    
2023-01-06 17:12:24,748 - Epoch: [188][  220/  246]    Overall Loss 0.253062    Objective Loss 0.253062                                        LR 0.000005    Time 0.020876    
2023-01-06 17:12:24,935 - Epoch: [188][  230/  246]    Overall Loss 0.253776    Objective Loss 0.253776                                        LR 0.000005    Time 0.020778    
2023-01-06 17:12:25,138 - Epoch: [188][  240/  246]    Overall Loss 0.253090    Objective Loss 0.253090                                        LR 0.000005    Time 0.020758    
2023-01-06 17:12:25,231 - Epoch: [188][  246/  246]    Overall Loss 0.252959    Objective Loss 0.252959    Top1 90.430622    LR 0.000005    Time 0.020629    
2023-01-06 17:12:25,374 - --- validate (epoch=188)-----------
2023-01-06 17:12:25,374 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:25,825 - Epoch: [188][   10/   28]    Loss 0.251560    Top1 91.054688    
2023-01-06 17:12:25,939 - Epoch: [188][   20/   28]    Loss 0.254974    Top1 90.937500    
2023-01-06 17:12:26,007 - Epoch: [188][   28/   28]    Loss 0.265228    Top1 90.223304    
2023-01-06 17:12:26,166 - ==> Top1: 90.223    Loss: 0.265

2023-01-06 17:12:26,166 - ==> Confusion:
[[ 245   13  181]
 [  19  267  316]
 [  70   84 5791]]

2023-01-06 17:12:26,168 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:26,168 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:26,174 - 

2023-01-06 17:12:26,174 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:26,748 - Epoch: [189][   10/  246]    Overall Loss 0.250934    Objective Loss 0.250934                                        LR 0.000005    Time 0.057368    
2023-01-06 17:12:26,932 - Epoch: [189][   20/  246]    Overall Loss 0.251890    Objective Loss 0.251890                                        LR 0.000005    Time 0.037782    
2023-01-06 17:12:27,114 - Epoch: [189][   30/  246]    Overall Loss 0.250906    Objective Loss 0.250906                                        LR 0.000005    Time 0.031274    
2023-01-06 17:12:27,308 - Epoch: [189][   40/  246]    Overall Loss 0.255704    Objective Loss 0.255704                                        LR 0.000005    Time 0.028279    
2023-01-06 17:12:27,495 - Epoch: [189][   50/  246]    Overall Loss 0.254658    Objective Loss 0.254658                                        LR 0.000005    Time 0.026362    
2023-01-06 17:12:27,682 - Epoch: [189][   60/  246]    Overall Loss 0.252806    Objective Loss 0.252806                                        LR 0.000005    Time 0.025087    
2023-01-06 17:12:27,863 - Epoch: [189][   70/  246]    Overall Loss 0.254895    Objective Loss 0.254895                                        LR 0.000005    Time 0.024079    
2023-01-06 17:12:28,059 - Epoch: [189][   80/  246]    Overall Loss 0.255971    Objective Loss 0.255971                                        LR 0.000005    Time 0.023511    
2023-01-06 17:12:28,250 - Epoch: [189][   90/  246]    Overall Loss 0.253843    Objective Loss 0.253843                                        LR 0.000005    Time 0.023020    
2023-01-06 17:12:28,446 - Epoch: [189][  100/  246]    Overall Loss 0.254334    Objective Loss 0.254334                                        LR 0.000005    Time 0.022674    
2023-01-06 17:12:28,635 - Epoch: [189][  110/  246]    Overall Loss 0.255093    Objective Loss 0.255093                                        LR 0.000005    Time 0.022330    
2023-01-06 17:12:28,826 - Epoch: [189][  120/  246]    Overall Loss 0.253027    Objective Loss 0.253027                                        LR 0.000005    Time 0.022058    
2023-01-06 17:12:29,014 - Epoch: [189][  130/  246]    Overall Loss 0.251148    Objective Loss 0.251148                                        LR 0.000005    Time 0.021806    
2023-01-06 17:12:29,206 - Epoch: [189][  140/  246]    Overall Loss 0.251620    Objective Loss 0.251620                                        LR 0.000005    Time 0.021617    
2023-01-06 17:12:29,401 - Epoch: [189][  150/  246]    Overall Loss 0.250544    Objective Loss 0.250544                                        LR 0.000005    Time 0.021473    
2023-01-06 17:12:29,601 - Epoch: [189][  160/  246]    Overall Loss 0.252073    Objective Loss 0.252073                                        LR 0.000005    Time 0.021376    
2023-01-06 17:12:29,803 - Epoch: [189][  170/  246]    Overall Loss 0.251581    Objective Loss 0.251581                                        LR 0.000005    Time 0.021304    
2023-01-06 17:12:30,006 - Epoch: [189][  180/  246]    Overall Loss 0.251670    Objective Loss 0.251670                                        LR 0.000005    Time 0.021248    
2023-01-06 17:12:30,207 - Epoch: [189][  190/  246]    Overall Loss 0.252367    Objective Loss 0.252367                                        LR 0.000005    Time 0.021186    
2023-01-06 17:12:30,408 - Epoch: [189][  200/  246]    Overall Loss 0.252555    Objective Loss 0.252555                                        LR 0.000005    Time 0.021131    
2023-01-06 17:12:30,602 - Epoch: [189][  210/  246]    Overall Loss 0.253147    Objective Loss 0.253147                                        LR 0.000005    Time 0.021044    
2023-01-06 17:12:30,794 - Epoch: [189][  220/  246]    Overall Loss 0.253602    Objective Loss 0.253602                                        LR 0.000005    Time 0.020959    
2023-01-06 17:12:30,988 - Epoch: [189][  230/  246]    Overall Loss 0.253844    Objective Loss 0.253844                                        LR 0.000005    Time 0.020890    
2023-01-06 17:12:31,195 - Epoch: [189][  240/  246]    Overall Loss 0.253952    Objective Loss 0.253952                                        LR 0.000005    Time 0.020883    
2023-01-06 17:12:31,291 - Epoch: [189][  246/  246]    Overall Loss 0.254029    Objective Loss 0.254029    Top1 92.344498    LR 0.000005    Time 0.020762    
2023-01-06 17:12:31,434 - --- validate (epoch=189)-----------
2023-01-06 17:12:31,434 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:31,894 - Epoch: [189][   10/   28]    Loss 0.263050    Top1 90.781250    
2023-01-06 17:12:32,008 - Epoch: [189][   20/   28]    Loss 0.272762    Top1 90.273438    
2023-01-06 17:12:32,078 - Epoch: [189][   28/   28]    Loss 0.273041    Top1 90.108789    
2023-01-06 17:12:32,213 - ==> Top1: 90.109    Loss: 0.273

2023-01-06 17:12:32,213 - ==> Confusion:
[[ 248   12  179]
 [  18  273  311]
 [  76   95 5774]]

2023-01-06 17:12:32,215 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:32,215 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:32,221 - 

2023-01-06 17:12:32,221 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:32,946 - Epoch: [190][   10/  246]    Overall Loss 0.248260    Objective Loss 0.248260                                        LR 0.000003    Time 0.072465    
2023-01-06 17:12:33,142 - Epoch: [190][   20/  246]    Overall Loss 0.258460    Objective Loss 0.258460                                        LR 0.000003    Time 0.046002    
2023-01-06 17:12:33,339 - Epoch: [190][   30/  246]    Overall Loss 0.258831    Objective Loss 0.258831                                        LR 0.000003    Time 0.037215    
2023-01-06 17:12:33,541 - Epoch: [190][   40/  246]    Overall Loss 0.265549    Objective Loss 0.265549                                        LR 0.000003    Time 0.032964    
2023-01-06 17:12:33,738 - Epoch: [190][   50/  246]    Overall Loss 0.259119    Objective Loss 0.259119                                        LR 0.000003    Time 0.030261    
2023-01-06 17:12:33,942 - Epoch: [190][   60/  246]    Overall Loss 0.259015    Objective Loss 0.259015                                        LR 0.000003    Time 0.028608    
2023-01-06 17:12:34,138 - Epoch: [190][   70/  246]    Overall Loss 0.259846    Objective Loss 0.259846                                        LR 0.000003    Time 0.027326    
2023-01-06 17:12:34,349 - Epoch: [190][   80/  246]    Overall Loss 0.260349    Objective Loss 0.260349                                        LR 0.000003    Time 0.026535    
2023-01-06 17:12:34,546 - Epoch: [190][   90/  246]    Overall Loss 0.259513    Objective Loss 0.259513                                        LR 0.000003    Time 0.025770    
2023-01-06 17:12:34,750 - Epoch: [190][  100/  246]    Overall Loss 0.258207    Objective Loss 0.258207                                        LR 0.000003    Time 0.025233    
2023-01-06 17:12:34,950 - Epoch: [190][  110/  246]    Overall Loss 0.257566    Objective Loss 0.257566                                        LR 0.000003    Time 0.024754    
2023-01-06 17:12:35,153 - Epoch: [190][  120/  246]    Overall Loss 0.257684    Objective Loss 0.257684                                        LR 0.000003    Time 0.024378    
2023-01-06 17:12:35,352 - Epoch: [190][  130/  246]    Overall Loss 0.256765    Objective Loss 0.256765                                        LR 0.000003    Time 0.024034    
2023-01-06 17:12:35,557 - Epoch: [190][  140/  246]    Overall Loss 0.255828    Objective Loss 0.255828                                        LR 0.000003    Time 0.023780    
2023-01-06 17:12:35,756 - Epoch: [190][  150/  246]    Overall Loss 0.254966    Objective Loss 0.254966                                        LR 0.000003    Time 0.023516    
2023-01-06 17:12:35,963 - Epoch: [190][  160/  246]    Overall Loss 0.253244    Objective Loss 0.253244                                        LR 0.000003    Time 0.023340    
2023-01-06 17:12:36,163 - Epoch: [190][  170/  246]    Overall Loss 0.252697    Objective Loss 0.252697                                        LR 0.000003    Time 0.023139    
2023-01-06 17:12:36,373 - Epoch: [190][  180/  246]    Overall Loss 0.254553    Objective Loss 0.254553                                        LR 0.000003    Time 0.023021    
2023-01-06 17:12:36,578 - Epoch: [190][  190/  246]    Overall Loss 0.255038    Objective Loss 0.255038                                        LR 0.000003    Time 0.022882    
2023-01-06 17:12:36,790 - Epoch: [190][  200/  246]    Overall Loss 0.254615    Objective Loss 0.254615                                        LR 0.000003    Time 0.022797    
2023-01-06 17:12:36,993 - Epoch: [190][  210/  246]    Overall Loss 0.253529    Objective Loss 0.253529                                        LR 0.000003    Time 0.022678    
2023-01-06 17:12:37,197 - Epoch: [190][  220/  246]    Overall Loss 0.252615    Objective Loss 0.252615                                        LR 0.000003    Time 0.022571    
2023-01-06 17:12:37,391 - Epoch: [190][  230/  246]    Overall Loss 0.253317    Objective Loss 0.253317                                        LR 0.000003    Time 0.022435    
2023-01-06 17:12:37,602 - Epoch: [190][  240/  246]    Overall Loss 0.252975    Objective Loss 0.252975                                        LR 0.000003    Time 0.022377    
2023-01-06 17:12:37,700 - Epoch: [190][  246/  246]    Overall Loss 0.252926    Objective Loss 0.252926    Top1 91.866029    LR 0.000003    Time 0.022230    
2023-01-06 17:12:37,840 - --- validate (epoch=190)-----------
2023-01-06 17:12:37,840 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:38,315 - Epoch: [190][   10/   28]    Loss 0.273357    Top1 90.312500    
2023-01-06 17:12:38,427 - Epoch: [190][   20/   28]    Loss 0.260488    Top1 90.625000    
2023-01-06 17:12:38,496 - Epoch: [190][   28/   28]    Loss 0.263857    Top1 90.280561    
2023-01-06 17:12:38,639 - ==> Top1: 90.281    Loss: 0.264

2023-01-06 17:12:38,639 - ==> Confusion:
[[ 241   15  183]
 [  18  266  318]
 [  67   78 5800]]

2023-01-06 17:12:38,640 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:38,640 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:38,646 - 

2023-01-06 17:12:38,646 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:39,395 - Epoch: [191][   10/  246]    Overall Loss 0.245909    Objective Loss 0.245909                                        LR 0.000003    Time 0.074790    
2023-01-06 17:12:39,614 - Epoch: [191][   20/  246]    Overall Loss 0.244553    Objective Loss 0.244553                                        LR 0.000003    Time 0.048321    
2023-01-06 17:12:39,825 - Epoch: [191][   30/  246]    Overall Loss 0.247758    Objective Loss 0.247758                                        LR 0.000003    Time 0.039234    
2023-01-06 17:12:40,040 - Epoch: [191][   40/  246]    Overall Loss 0.250406    Objective Loss 0.250406                                        LR 0.000003    Time 0.034782    
2023-01-06 17:12:40,258 - Epoch: [191][   50/  246]    Overall Loss 0.253775    Objective Loss 0.253775                                        LR 0.000003    Time 0.032176    
2023-01-06 17:12:40,457 - Epoch: [191][   60/  246]    Overall Loss 0.251172    Objective Loss 0.251172                                        LR 0.000003    Time 0.030133    
2023-01-06 17:12:40,665 - Epoch: [191][   70/  246]    Overall Loss 0.251724    Objective Loss 0.251724                                        LR 0.000003    Time 0.028797    
2023-01-06 17:12:40,868 - Epoch: [191][   80/  246]    Overall Loss 0.253336    Objective Loss 0.253336                                        LR 0.000003    Time 0.027728    
2023-01-06 17:12:41,074 - Epoch: [191][   90/  246]    Overall Loss 0.254517    Objective Loss 0.254517                                        LR 0.000003    Time 0.026931    
2023-01-06 17:12:41,285 - Epoch: [191][  100/  246]    Overall Loss 0.254234    Objective Loss 0.254234                                        LR 0.000003    Time 0.026347    
2023-01-06 17:12:41,500 - Epoch: [191][  110/  246]    Overall Loss 0.254844    Objective Loss 0.254844                                        LR 0.000003    Time 0.025904    
2023-01-06 17:12:41,711 - Epoch: [191][  120/  246]    Overall Loss 0.254242    Objective Loss 0.254242                                        LR 0.000003    Time 0.025501    
2023-01-06 17:12:41,926 - Epoch: [191][  130/  246]    Overall Loss 0.253243    Objective Loss 0.253243                                        LR 0.000003    Time 0.025189    
2023-01-06 17:12:42,139 - Epoch: [191][  140/  246]    Overall Loss 0.253674    Objective Loss 0.253674                                        LR 0.000003    Time 0.024907    
2023-01-06 17:12:42,352 - Epoch: [191][  150/  246]    Overall Loss 0.253510    Objective Loss 0.253510                                        LR 0.000003    Time 0.024667    
2023-01-06 17:12:42,564 - Epoch: [191][  160/  246]    Overall Loss 0.253429    Objective Loss 0.253429                                        LR 0.000003    Time 0.024445    
2023-01-06 17:12:42,772 - Epoch: [191][  170/  246]    Overall Loss 0.253451    Objective Loss 0.253451                                        LR 0.000003    Time 0.024230    
2023-01-06 17:12:42,984 - Epoch: [191][  180/  246]    Overall Loss 0.253853    Objective Loss 0.253853                                        LR 0.000003    Time 0.024061    
2023-01-06 17:12:43,179 - Epoch: [191][  190/  246]    Overall Loss 0.254267    Objective Loss 0.254267                                        LR 0.000003    Time 0.023818    
2023-01-06 17:12:43,376 - Epoch: [191][  200/  246]    Overall Loss 0.254213    Objective Loss 0.254213                                        LR 0.000003    Time 0.023611    
2023-01-06 17:12:43,551 - Epoch: [191][  210/  246]    Overall Loss 0.252977    Objective Loss 0.252977                                        LR 0.000003    Time 0.023317    
2023-01-06 17:12:43,728 - Epoch: [191][  220/  246]    Overall Loss 0.252546    Objective Loss 0.252546                                        LR 0.000003    Time 0.023060    
2023-01-06 17:12:43,922 - Epoch: [191][  230/  246]    Overall Loss 0.252417    Objective Loss 0.252417                                        LR 0.000003    Time 0.022899    
2023-01-06 17:12:44,127 - Epoch: [191][  240/  246]    Overall Loss 0.252526    Objective Loss 0.252526                                        LR 0.000003    Time 0.022799    
2023-01-06 17:12:44,224 - Epoch: [191][  246/  246]    Overall Loss 0.253383    Objective Loss 0.253383    Top1 88.516746    LR 0.000003    Time 0.022635    
2023-01-06 17:12:44,369 - --- validate (epoch=191)-----------
2023-01-06 17:12:44,370 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:44,825 - Epoch: [191][   10/   28]    Loss 0.271998    Top1 89.921875    
2023-01-06 17:12:44,956 - Epoch: [191][   20/   28]    Loss 0.269170    Top1 90.156250    
2023-01-06 17:12:45,023 - Epoch: [191][   28/   28]    Loss 0.267133    Top1 90.251932    
2023-01-06 17:12:45,145 - ==> Top1: 90.252    Loss: 0.267

2023-01-06 17:12:45,146 - ==> Confusion:
[[ 247   12  180]
 [  17  259  326]
 [  67   79 5799]]

2023-01-06 17:12:45,147 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:45,147 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:45,153 - 

2023-01-06 17:12:45,153 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:45,706 - Epoch: [192][   10/  246]    Overall Loss 0.255569    Objective Loss 0.255569                                        LR 0.000003    Time 0.055262    
2023-01-06 17:12:45,870 - Epoch: [192][   20/  246]    Overall Loss 0.244705    Objective Loss 0.244705                                        LR 0.000003    Time 0.035779    
2023-01-06 17:12:46,032 - Epoch: [192][   30/  246]    Overall Loss 0.252521    Objective Loss 0.252521                                        LR 0.000003    Time 0.029240    
2023-01-06 17:12:46,198 - Epoch: [192][   40/  246]    Overall Loss 0.253858    Objective Loss 0.253858                                        LR 0.000003    Time 0.026083    
2023-01-06 17:12:46,377 - Epoch: [192][   50/  246]    Overall Loss 0.254711    Objective Loss 0.254711                                        LR 0.000003    Time 0.024432    
2023-01-06 17:12:46,582 - Epoch: [192][   60/  246]    Overall Loss 0.258678    Objective Loss 0.258678                                        LR 0.000003    Time 0.023768    
2023-01-06 17:12:46,811 - Epoch: [192][   70/  246]    Overall Loss 0.255307    Objective Loss 0.255307                                        LR 0.000003    Time 0.023640    
2023-01-06 17:12:47,039 - Epoch: [192][   80/  246]    Overall Loss 0.254322    Objective Loss 0.254322                                        LR 0.000003    Time 0.023528    
2023-01-06 17:12:47,291 - Epoch: [192][   90/  246]    Overall Loss 0.252613    Objective Loss 0.252613                                        LR 0.000003    Time 0.023711    
2023-01-06 17:12:47,548 - Epoch: [192][  100/  246]    Overall Loss 0.253536    Objective Loss 0.253536                                        LR 0.000003    Time 0.023889    
2023-01-06 17:12:47,805 - Epoch: [192][  110/  246]    Overall Loss 0.253429    Objective Loss 0.253429                                        LR 0.000003    Time 0.024030    
2023-01-06 17:12:48,060 - Epoch: [192][  120/  246]    Overall Loss 0.253609    Objective Loss 0.253609                                        LR 0.000003    Time 0.024139    
2023-01-06 17:12:48,259 - Epoch: [192][  130/  246]    Overall Loss 0.254968    Objective Loss 0.254968                                        LR 0.000003    Time 0.023797    
2023-01-06 17:12:48,457 - Epoch: [192][  140/  246]    Overall Loss 0.254426    Objective Loss 0.254426                                        LR 0.000003    Time 0.023508    
2023-01-06 17:12:48,649 - Epoch: [192][  150/  246]    Overall Loss 0.254015    Objective Loss 0.254015                                        LR 0.000003    Time 0.023217    
2023-01-06 17:12:48,825 - Epoch: [192][  160/  246]    Overall Loss 0.254293    Objective Loss 0.254293                                        LR 0.000003    Time 0.022863    
2023-01-06 17:12:48,998 - Epoch: [192][  170/  246]    Overall Loss 0.253114    Objective Loss 0.253114                                        LR 0.000003    Time 0.022535    
2023-01-06 17:12:49,168 - Epoch: [192][  180/  246]    Overall Loss 0.252240    Objective Loss 0.252240                                        LR 0.000003    Time 0.022226    
2023-01-06 17:12:49,341 - Epoch: [192][  190/  246]    Overall Loss 0.252100    Objective Loss 0.252100                                        LR 0.000003    Time 0.021968    
2023-01-06 17:12:49,517 - Epoch: [192][  200/  246]    Overall Loss 0.252553    Objective Loss 0.252553                                        LR 0.000003    Time 0.021744    
2023-01-06 17:12:49,689 - Epoch: [192][  210/  246]    Overall Loss 0.252870    Objective Loss 0.252870                                        LR 0.000003    Time 0.021528    
2023-01-06 17:12:49,867 - Epoch: [192][  220/  246]    Overall Loss 0.253474    Objective Loss 0.253474                                        LR 0.000003    Time 0.021356    
2023-01-06 17:12:50,048 - Epoch: [192][  230/  246]    Overall Loss 0.253555    Objective Loss 0.253555                                        LR 0.000003    Time 0.021215    
2023-01-06 17:12:50,241 - Epoch: [192][  240/  246]    Overall Loss 0.253507    Objective Loss 0.253507                                        LR 0.000003    Time 0.021134    
2023-01-06 17:12:50,328 - Epoch: [192][  246/  246]    Overall Loss 0.253215    Objective Loss 0.253215    Top1 93.062201    LR 0.000003    Time 0.020971    
2023-01-06 17:12:50,476 - --- validate (epoch=192)-----------
2023-01-06 17:12:50,476 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:50,948 - Epoch: [192][   10/   28]    Loss 0.266302    Top1 90.234375    
2023-01-06 17:12:51,057 - Epoch: [192][   20/   28]    Loss 0.274248    Top1 89.707031    
2023-01-06 17:12:51,124 - Epoch: [192][   28/   28]    Loss 0.268519    Top1 90.037217    
2023-01-06 17:12:51,258 - ==> Top1: 90.037    Loss: 0.269

2023-01-06 17:12:51,258 - ==> Confusion:
[[ 226   16  197]
 [  16  247  339]
 [  58   70 5817]]

2023-01-06 17:12:51,260 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:51,260 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:51,266 - 

2023-01-06 17:12:51,266 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:51,977 - Epoch: [193][   10/  246]    Overall Loss 0.234693    Objective Loss 0.234693                                        LR 0.000003    Time 0.071097    
2023-01-06 17:12:52,167 - Epoch: [193][   20/  246]    Overall Loss 0.239821    Objective Loss 0.239821                                        LR 0.000003    Time 0.045005    
2023-01-06 17:12:52,359 - Epoch: [193][   30/  246]    Overall Loss 0.241394    Objective Loss 0.241394                                        LR 0.000003    Time 0.036406    
2023-01-06 17:12:52,551 - Epoch: [193][   40/  246]    Overall Loss 0.244632    Objective Loss 0.244632                                        LR 0.000003    Time 0.032077    
2023-01-06 17:12:52,740 - Epoch: [193][   50/  246]    Overall Loss 0.242038    Objective Loss 0.242038                                        LR 0.000003    Time 0.029447    
2023-01-06 17:12:52,933 - Epoch: [193][   60/  246]    Overall Loss 0.246801    Objective Loss 0.246801                                        LR 0.000003    Time 0.027748    
2023-01-06 17:12:53,125 - Epoch: [193][   70/  246]    Overall Loss 0.248368    Objective Loss 0.248368                                        LR 0.000003    Time 0.026526    
2023-01-06 17:12:53,315 - Epoch: [193][   80/  246]    Overall Loss 0.248668    Objective Loss 0.248668                                        LR 0.000003    Time 0.025569    
2023-01-06 17:12:53,504 - Epoch: [193][   90/  246]    Overall Loss 0.249333    Objective Loss 0.249333                                        LR 0.000003    Time 0.024828    
2023-01-06 17:12:53,693 - Epoch: [193][  100/  246]    Overall Loss 0.248129    Objective Loss 0.248129                                        LR 0.000003    Time 0.024229    
2023-01-06 17:12:53,882 - Epoch: [193][  110/  246]    Overall Loss 0.246786    Objective Loss 0.246786                                        LR 0.000003    Time 0.023744    
2023-01-06 17:12:54,070 - Epoch: [193][  120/  246]    Overall Loss 0.248062    Objective Loss 0.248062                                        LR 0.000003    Time 0.023333    
2023-01-06 17:12:54,259 - Epoch: [193][  130/  246]    Overall Loss 0.249483    Objective Loss 0.249483                                        LR 0.000003    Time 0.022989    
2023-01-06 17:12:54,447 - Epoch: [193][  140/  246]    Overall Loss 0.250609    Objective Loss 0.250609                                        LR 0.000003    Time 0.022689    
2023-01-06 17:12:54,636 - Epoch: [193][  150/  246]    Overall Loss 0.250619    Objective Loss 0.250619                                        LR 0.000003    Time 0.022433    
2023-01-06 17:12:54,824 - Epoch: [193][  160/  246]    Overall Loss 0.250755    Objective Loss 0.250755                                        LR 0.000003    Time 0.022203    
2023-01-06 17:12:55,014 - Epoch: [193][  170/  246]    Overall Loss 0.250942    Objective Loss 0.250942                                        LR 0.000003    Time 0.022011    
2023-01-06 17:12:55,198 - Epoch: [193][  180/  246]    Overall Loss 0.252239    Objective Loss 0.252239                                        LR 0.000003    Time 0.021809    
2023-01-06 17:12:55,388 - Epoch: [193][  190/  246]    Overall Loss 0.251270    Objective Loss 0.251270                                        LR 0.000003    Time 0.021658    
2023-01-06 17:12:55,577 - Epoch: [193][  200/  246]    Overall Loss 0.250882    Objective Loss 0.250882                                        LR 0.000003    Time 0.021519    
2023-01-06 17:12:55,767 - Epoch: [193][  210/  246]    Overall Loss 0.252308    Objective Loss 0.252308                                        LR 0.000003    Time 0.021397    
2023-01-06 17:12:55,958 - Epoch: [193][  220/  246]    Overall Loss 0.253296    Objective Loss 0.253296                                        LR 0.000003    Time 0.021290    
2023-01-06 17:12:56,146 - Epoch: [193][  230/  246]    Overall Loss 0.254159    Objective Loss 0.254159                                        LR 0.000003    Time 0.021183    
2023-01-06 17:12:56,348 - Epoch: [193][  240/  246]    Overall Loss 0.253582    Objective Loss 0.253582                                        LR 0.000003    Time 0.021141    
2023-01-06 17:12:56,444 - Epoch: [193][  246/  246]    Overall Loss 0.253478    Objective Loss 0.253478    Top1 89.952153    LR 0.000003    Time 0.021015    
2023-01-06 17:12:56,581 - --- validate (epoch=193)-----------
2023-01-06 17:12:56,581 - 6986 samples (256 per mini-batch)
2023-01-06 17:12:57,036 - Epoch: [193][   10/   28]    Loss 0.263324    Top1 90.546875    
2023-01-06 17:12:57,149 - Epoch: [193][   20/   28]    Loss 0.264438    Top1 90.292969    
2023-01-06 17:12:57,217 - Epoch: [193][   28/   28]    Loss 0.269654    Top1 90.137418    
2023-01-06 17:12:57,378 - ==> Top1: 90.137    Loss: 0.270

2023-01-06 17:12:57,378 - ==> Confusion:
[[ 247   17  175]
 [  18  246  338]
 [  73   68 5804]]

2023-01-06 17:12:57,379 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:12:57,379 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:12:57,385 - 

2023-01-06 17:12:57,385 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:12:57,954 - Epoch: [194][   10/  246]    Overall Loss 0.254818    Objective Loss 0.254818                                        LR 0.000003    Time 0.056760    
2023-01-06 17:12:58,119 - Epoch: [194][   20/  246]    Overall Loss 0.253613    Objective Loss 0.253613                                        LR 0.000003    Time 0.036608    
2023-01-06 17:12:58,314 - Epoch: [194][   30/  246]    Overall Loss 0.258939    Objective Loss 0.258939                                        LR 0.000003    Time 0.030910    
2023-01-06 17:12:58,514 - Epoch: [194][   40/  246]    Overall Loss 0.262862    Objective Loss 0.262862                                        LR 0.000003    Time 0.028157    
2023-01-06 17:12:58,710 - Epoch: [194][   50/  246]    Overall Loss 0.262030    Objective Loss 0.262030                                        LR 0.000003    Time 0.026437    
2023-01-06 17:12:58,910 - Epoch: [194][   60/  246]    Overall Loss 0.259778    Objective Loss 0.259778                                        LR 0.000003    Time 0.025365    
2023-01-06 17:12:59,107 - Epoch: [194][   70/  246]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.000003    Time 0.024548    
2023-01-06 17:12:59,311 - Epoch: [194][   80/  246]    Overall Loss 0.256753    Objective Loss 0.256753                                        LR 0.000003    Time 0.024029    
2023-01-06 17:12:59,506 - Epoch: [194][   90/  246]    Overall Loss 0.255950    Objective Loss 0.255950                                        LR 0.000003    Time 0.023514    
2023-01-06 17:12:59,702 - Epoch: [194][  100/  246]    Overall Loss 0.254880    Objective Loss 0.254880                                        LR 0.000003    Time 0.023123    
2023-01-06 17:12:59,891 - Epoch: [194][  110/  246]    Overall Loss 0.253852    Objective Loss 0.253852                                        LR 0.000003    Time 0.022737    
2023-01-06 17:13:00,088 - Epoch: [194][  120/  246]    Overall Loss 0.252783    Objective Loss 0.252783                                        LR 0.000003    Time 0.022479    
2023-01-06 17:13:00,276 - Epoch: [194][  130/  246]    Overall Loss 0.251967    Objective Loss 0.251967                                        LR 0.000003    Time 0.022192    
2023-01-06 17:13:00,472 - Epoch: [194][  140/  246]    Overall Loss 0.253906    Objective Loss 0.253906                                        LR 0.000003    Time 0.022003    
2023-01-06 17:13:00,648 - Epoch: [194][  150/  246]    Overall Loss 0.255923    Objective Loss 0.255923                                        LR 0.000003    Time 0.021707    
2023-01-06 17:13:00,816 - Epoch: [194][  160/  246]    Overall Loss 0.255673    Objective Loss 0.255673                                        LR 0.000003    Time 0.021401    
2023-01-06 17:13:00,990 - Epoch: [194][  170/  246]    Overall Loss 0.254805    Objective Loss 0.254805                                        LR 0.000003    Time 0.021163    
2023-01-06 17:13:01,176 - Epoch: [194][  180/  246]    Overall Loss 0.254607    Objective Loss 0.254607                                        LR 0.000003    Time 0.021020    
2023-01-06 17:13:01,356 - Epoch: [194][  190/  246]    Overall Loss 0.255267    Objective Loss 0.255267                                        LR 0.000003    Time 0.020859    
2023-01-06 17:13:01,550 - Epoch: [194][  200/  246]    Overall Loss 0.254887    Objective Loss 0.254887                                        LR 0.000003    Time 0.020781    
2023-01-06 17:13:01,736 - Epoch: [194][  210/  246]    Overall Loss 0.254369    Objective Loss 0.254369                                        LR 0.000003    Time 0.020675    
2023-01-06 17:13:01,928 - Epoch: [194][  220/  246]    Overall Loss 0.254351    Objective Loss 0.254351                                        LR 0.000003    Time 0.020607    
2023-01-06 17:13:02,107 - Epoch: [194][  230/  246]    Overall Loss 0.253356    Objective Loss 0.253356                                        LR 0.000003    Time 0.020491    
2023-01-06 17:13:02,295 - Epoch: [194][  240/  246]    Overall Loss 0.252733    Objective Loss 0.252733                                        LR 0.000003    Time 0.020417    
2023-01-06 17:13:02,390 - Epoch: [194][  246/  246]    Overall Loss 0.252960    Objective Loss 0.252960    Top1 90.191388    LR 0.000003    Time 0.020303    
2023-01-06 17:13:02,545 - --- validate (epoch=194)-----------
2023-01-06 17:13:02,545 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:02,998 - Epoch: [194][   10/   28]    Loss 0.265257    Top1 90.429688    
2023-01-06 17:13:03,114 - Epoch: [194][   20/   28]    Loss 0.272595    Top1 90.097656    
2023-01-06 17:13:03,183 - Epoch: [194][   28/   28]    Loss 0.269097    Top1 90.266247    
2023-01-06 17:13:03,344 - ==> Top1: 90.266    Loss: 0.269

2023-01-06 17:13:03,345 - ==> Confusion:
[[ 243   12  184]
 [  18  254  330]
 [  72   64 5809]]

2023-01-06 17:13:03,346 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:03,346 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:03,352 - 

2023-01-06 17:13:03,352 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:13:04,046 - Epoch: [195][   10/  246]    Overall Loss 0.265394    Objective Loss 0.265394                                        LR 0.000003    Time 0.069327    
2023-01-06 17:13:04,215 - Epoch: [195][   20/  246]    Overall Loss 0.261062    Objective Loss 0.261062                                        LR 0.000003    Time 0.043062    
2023-01-06 17:13:04,391 - Epoch: [195][   30/  246]    Overall Loss 0.263553    Objective Loss 0.263553                                        LR 0.000003    Time 0.034579    
2023-01-06 17:13:04,553 - Epoch: [195][   40/  246]    Overall Loss 0.259148    Objective Loss 0.259148                                        LR 0.000003    Time 0.029977    
2023-01-06 17:13:04,717 - Epoch: [195][   50/  246]    Overall Loss 0.258039    Objective Loss 0.258039                                        LR 0.000003    Time 0.027240    
2023-01-06 17:13:04,882 - Epoch: [195][   60/  246]    Overall Loss 0.255478    Objective Loss 0.255478                                        LR 0.000003    Time 0.025451    
2023-01-06 17:13:05,084 - Epoch: [195][   70/  246]    Overall Loss 0.257487    Objective Loss 0.257487                                        LR 0.000003    Time 0.024688    
2023-01-06 17:13:05,289 - Epoch: [195][   80/  246]    Overall Loss 0.261006    Objective Loss 0.261006                                        LR 0.000003    Time 0.024160    
2023-01-06 17:13:05,504 - Epoch: [195][   90/  246]    Overall Loss 0.259484    Objective Loss 0.259484                                        LR 0.000003    Time 0.023862    
2023-01-06 17:13:05,716 - Epoch: [195][  100/  246]    Overall Loss 0.259634    Objective Loss 0.259634                                        LR 0.000003    Time 0.023581    
2023-01-06 17:13:05,922 - Epoch: [195][  110/  246]    Overall Loss 0.258760    Objective Loss 0.258760                                        LR 0.000003    Time 0.023302    
2023-01-06 17:13:06,134 - Epoch: [195][  120/  246]    Overall Loss 0.259050    Objective Loss 0.259050                                        LR 0.000003    Time 0.023124    
2023-01-06 17:13:06,354 - Epoch: [195][  130/  246]    Overall Loss 0.256726    Objective Loss 0.256726                                        LR 0.000003    Time 0.023031    
2023-01-06 17:13:06,574 - Epoch: [195][  140/  246]    Overall Loss 0.256701    Objective Loss 0.256701                                        LR 0.000003    Time 0.022944    
2023-01-06 17:13:06,795 - Epoch: [195][  150/  246]    Overall Loss 0.255622    Objective Loss 0.255622                                        LR 0.000003    Time 0.022886    
2023-01-06 17:13:07,012 - Epoch: [195][  160/  246]    Overall Loss 0.255762    Objective Loss 0.255762                                        LR 0.000003    Time 0.022808    
2023-01-06 17:13:07,230 - Epoch: [195][  170/  246]    Overall Loss 0.254023    Objective Loss 0.254023                                        LR 0.000003    Time 0.022745    
2023-01-06 17:13:07,447 - Epoch: [195][  180/  246]    Overall Loss 0.253706    Objective Loss 0.253706                                        LR 0.000003    Time 0.022689    
2023-01-06 17:13:07,665 - Epoch: [195][  190/  246]    Overall Loss 0.253510    Objective Loss 0.253510                                        LR 0.000003    Time 0.022636    
2023-01-06 17:13:07,881 - Epoch: [195][  200/  246]    Overall Loss 0.253304    Objective Loss 0.253304                                        LR 0.000003    Time 0.022585    
2023-01-06 17:13:08,096 - Epoch: [195][  210/  246]    Overall Loss 0.252999    Objective Loss 0.252999                                        LR 0.000003    Time 0.022531    
2023-01-06 17:13:08,314 - Epoch: [195][  220/  246]    Overall Loss 0.252859    Objective Loss 0.252859                                        LR 0.000003    Time 0.022494    
2023-01-06 17:13:08,532 - Epoch: [195][  230/  246]    Overall Loss 0.253036    Objective Loss 0.253036                                        LR 0.000003    Time 0.022461    
2023-01-06 17:13:08,769 - Epoch: [195][  240/  246]    Overall Loss 0.252432    Objective Loss 0.252432                                        LR 0.000003    Time 0.022510    
2023-01-06 17:13:08,880 - Epoch: [195][  246/  246]    Overall Loss 0.252762    Objective Loss 0.252762    Top1 90.669856    LR 0.000003    Time 0.022415    
2023-01-06 17:13:09,020 - --- validate (epoch=195)-----------
2023-01-06 17:13:09,020 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:09,483 - Epoch: [195][   10/   28]    Loss 0.254516    Top1 90.898438    
2023-01-06 17:13:09,600 - Epoch: [195][   20/   28]    Loss 0.265659    Top1 90.390625    
2023-01-06 17:13:09,668 - Epoch: [195][   28/   28]    Loss 0.268291    Top1 90.137418    
2023-01-06 17:13:09,815 - ==> Top1: 90.137    Loss: 0.268

2023-01-06 17:13:09,815 - ==> Confusion:
[[ 241   12  186]
 [  19  251  332]
 [  67   73 5805]]

2023-01-06 17:13:09,816 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:09,816 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:09,823 - 

2023-01-06 17:13:09,823 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:13:10,551 - Epoch: [196][   10/  246]    Overall Loss 0.265150    Objective Loss 0.265150                                        LR 0.000003    Time 0.072770    
2023-01-06 17:13:10,737 - Epoch: [196][   20/  246]    Overall Loss 0.258404    Objective Loss 0.258404                                        LR 0.000003    Time 0.045639    
2023-01-06 17:13:10,935 - Epoch: [196][   30/  246]    Overall Loss 0.254398    Objective Loss 0.254398                                        LR 0.000003    Time 0.037008    
2023-01-06 17:13:11,144 - Epoch: [196][   40/  246]    Overall Loss 0.252855    Objective Loss 0.252855                                        LR 0.000003    Time 0.032984    
2023-01-06 17:13:11,369 - Epoch: [196][   50/  246]    Overall Loss 0.252969    Objective Loss 0.252969                                        LR 0.000003    Time 0.030882    
2023-01-06 17:13:11,597 - Epoch: [196][   60/  246]    Overall Loss 0.252199    Objective Loss 0.252199                                        LR 0.000003    Time 0.029528    
2023-01-06 17:13:11,822 - Epoch: [196][   70/  246]    Overall Loss 0.252203    Objective Loss 0.252203                                        LR 0.000003    Time 0.028520    
2023-01-06 17:13:12,032 - Epoch: [196][   80/  246]    Overall Loss 0.251980    Objective Loss 0.251980                                        LR 0.000003    Time 0.027576    
2023-01-06 17:13:12,238 - Epoch: [196][   90/  246]    Overall Loss 0.254195    Objective Loss 0.254195                                        LR 0.000003    Time 0.026794    
2023-01-06 17:13:12,446 - Epoch: [196][  100/  246]    Overall Loss 0.255841    Objective Loss 0.255841                                        LR 0.000003    Time 0.026191    
2023-01-06 17:13:12,653 - Epoch: [196][  110/  246]    Overall Loss 0.256869    Objective Loss 0.256869                                        LR 0.000003    Time 0.025687    
2023-01-06 17:13:12,860 - Epoch: [196][  120/  246]    Overall Loss 0.256631    Objective Loss 0.256631                                        LR 0.000003    Time 0.025269    
2023-01-06 17:13:13,070 - Epoch: [196][  130/  246]    Overall Loss 0.255731    Objective Loss 0.255731                                        LR 0.000003    Time 0.024939    
2023-01-06 17:13:13,279 - Epoch: [196][  140/  246]    Overall Loss 0.255363    Objective Loss 0.255363                                        LR 0.000003    Time 0.024648    
2023-01-06 17:13:13,478 - Epoch: [196][  150/  246]    Overall Loss 0.255017    Objective Loss 0.255017                                        LR 0.000003    Time 0.024328    
2023-01-06 17:13:13,675 - Epoch: [196][  160/  246]    Overall Loss 0.255868    Objective Loss 0.255868                                        LR 0.000003    Time 0.024035    
2023-01-06 17:13:13,871 - Epoch: [196][  170/  246]    Overall Loss 0.255970    Objective Loss 0.255970                                        LR 0.000003    Time 0.023770    
2023-01-06 17:13:14,060 - Epoch: [196][  180/  246]    Overall Loss 0.255700    Objective Loss 0.255700                                        LR 0.000003    Time 0.023499    
2023-01-06 17:13:14,226 - Epoch: [196][  190/  246]    Overall Loss 0.255301    Objective Loss 0.255301                                        LR 0.000003    Time 0.023135    
2023-01-06 17:13:14,392 - Epoch: [196][  200/  246]    Overall Loss 0.255283    Objective Loss 0.255283                                        LR 0.000003    Time 0.022805    
2023-01-06 17:13:14,564 - Epoch: [196][  210/  246]    Overall Loss 0.255297    Objective Loss 0.255297                                        LR 0.000003    Time 0.022538    
2023-01-06 17:13:14,729 - Epoch: [196][  220/  246]    Overall Loss 0.254598    Objective Loss 0.254598                                        LR 0.000003    Time 0.022262    
2023-01-06 17:13:14,899 - Epoch: [196][  230/  246]    Overall Loss 0.254131    Objective Loss 0.254131                                        LR 0.000003    Time 0.022034    
2023-01-06 17:13:15,083 - Epoch: [196][  240/  246]    Overall Loss 0.253333    Objective Loss 0.253333                                        LR 0.000003    Time 0.021880    
2023-01-06 17:13:15,176 - Epoch: [196][  246/  246]    Overall Loss 0.252775    Objective Loss 0.252775    Top1 92.344498    LR 0.000003    Time 0.021721    
2023-01-06 17:13:15,308 - --- validate (epoch=196)-----------
2023-01-06 17:13:15,309 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:15,779 - Epoch: [196][   10/   28]    Loss 0.265190    Top1 90.781250    
2023-01-06 17:13:15,891 - Epoch: [196][   20/   28]    Loss 0.259680    Top1 90.234375    
2023-01-06 17:13:15,958 - Epoch: [196][   28/   28]    Loss 0.266297    Top1 90.123103    
2023-01-06 17:13:16,100 - ==> Top1: 90.123    Loss: 0.266

2023-01-06 17:13:16,101 - ==> Confusion:
[[ 235   17  187]
 [  18  237  347]
 [  57   64 5824]]

2023-01-06 17:13:16,102 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:16,102 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:16,109 - 

2023-01-06 17:13:16,109 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:13:16,681 - Epoch: [197][   10/  246]    Overall Loss 0.251181    Objective Loss 0.251181                                        LR 0.000003    Time 0.057126    
2023-01-06 17:13:16,847 - Epoch: [197][   20/  246]    Overall Loss 0.246276    Objective Loss 0.246276                                        LR 0.000003    Time 0.036862    
2023-01-06 17:13:17,022 - Epoch: [197][   30/  246]    Overall Loss 0.244291    Objective Loss 0.244291                                        LR 0.000003    Time 0.030395    
2023-01-06 17:13:17,206 - Epoch: [197][   40/  246]    Overall Loss 0.248427    Objective Loss 0.248427                                        LR 0.000003    Time 0.027368    
2023-01-06 17:13:17,386 - Epoch: [197][   50/  246]    Overall Loss 0.247006    Objective Loss 0.247006                                        LR 0.000003    Time 0.025470    
2023-01-06 17:13:17,569 - Epoch: [197][   60/  246]    Overall Loss 0.244820    Objective Loss 0.244820                                        LR 0.000003    Time 0.024272    
2023-01-06 17:13:17,739 - Epoch: [197][   70/  246]    Overall Loss 0.243058    Objective Loss 0.243058                                        LR 0.000003    Time 0.023222    
2023-01-06 17:13:17,913 - Epoch: [197][   80/  246]    Overall Loss 0.246200    Objective Loss 0.246200                                        LR 0.000003    Time 0.022491    
2023-01-06 17:13:18,088 - Epoch: [197][   90/  246]    Overall Loss 0.244952    Objective Loss 0.244952                                        LR 0.000003    Time 0.021931    
2023-01-06 17:13:18,264 - Epoch: [197][  100/  246]    Overall Loss 0.245234    Objective Loss 0.245234                                        LR 0.000003    Time 0.021476    
2023-01-06 17:13:18,440 - Epoch: [197][  110/  246]    Overall Loss 0.248181    Objective Loss 0.248181                                        LR 0.000003    Time 0.021123    
2023-01-06 17:13:18,616 - Epoch: [197][  120/  246]    Overall Loss 0.250814    Objective Loss 0.250814                                        LR 0.000003    Time 0.020824    
2023-01-06 17:13:18,802 - Epoch: [197][  130/  246]    Overall Loss 0.249984    Objective Loss 0.249984                                        LR 0.000003    Time 0.020656    
2023-01-06 17:13:18,979 - Epoch: [197][  140/  246]    Overall Loss 0.249706    Objective Loss 0.249706                                        LR 0.000003    Time 0.020431    
2023-01-06 17:13:19,154 - Epoch: [197][  150/  246]    Overall Loss 0.250042    Objective Loss 0.250042                                        LR 0.000003    Time 0.020231    
2023-01-06 17:13:19,326 - Epoch: [197][  160/  246]    Overall Loss 0.250456    Objective Loss 0.250456                                        LR 0.000003    Time 0.020031    
2023-01-06 17:13:19,501 - Epoch: [197][  170/  246]    Overall Loss 0.251370    Objective Loss 0.251370                                        LR 0.000003    Time 0.019879    
2023-01-06 17:13:19,677 - Epoch: [197][  180/  246]    Overall Loss 0.252124    Objective Loss 0.252124                                        LR 0.000003    Time 0.019739    
2023-01-06 17:13:19,868 - Epoch: [197][  190/  246]    Overall Loss 0.252213    Objective Loss 0.252213                                        LR 0.000003    Time 0.019706    
2023-01-06 17:13:20,052 - Epoch: [197][  200/  246]    Overall Loss 0.253053    Objective Loss 0.253053                                        LR 0.000003    Time 0.019636    
2023-01-06 17:13:20,227 - Epoch: [197][  210/  246]    Overall Loss 0.252812    Objective Loss 0.252812                                        LR 0.000003    Time 0.019534    
2023-01-06 17:13:20,395 - Epoch: [197][  220/  246]    Overall Loss 0.252841    Objective Loss 0.252841                                        LR 0.000003    Time 0.019406    
2023-01-06 17:13:20,559 - Epoch: [197][  230/  246]    Overall Loss 0.252513    Objective Loss 0.252513                                        LR 0.000003    Time 0.019278    
2023-01-06 17:13:20,747 - Epoch: [197][  240/  246]    Overall Loss 0.252344    Objective Loss 0.252344                                        LR 0.000003    Time 0.019250    
2023-01-06 17:13:20,832 - Epoch: [197][  246/  246]    Overall Loss 0.252226    Objective Loss 0.252226    Top1 89.712919    LR 0.000003    Time 0.019124    
2023-01-06 17:13:20,966 - --- validate (epoch=197)-----------
2023-01-06 17:13:20,966 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:21,421 - Epoch: [197][   10/   28]    Loss 0.263538    Top1 90.546875    
2023-01-06 17:13:21,537 - Epoch: [197][   20/   28]    Loss 0.268466    Top1 89.980469    
2023-01-06 17:13:21,605 - Epoch: [197][   28/   28]    Loss 0.267607    Top1 90.151732    
2023-01-06 17:13:21,755 - ==> Top1: 90.152    Loss: 0.268

2023-01-06 17:13:21,756 - ==> Confusion:
[[ 230   15  194]
 [  16  234  352]
 [  54   57 5834]]

2023-01-06 17:13:21,757 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:21,757 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:21,763 - 

2023-01-06 17:13:21,763 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:13:22,476 - Epoch: [198][   10/  246]    Overall Loss 0.263229    Objective Loss 0.263229                                        LR 0.000003    Time 0.071181    
2023-01-06 17:13:22,649 - Epoch: [198][   20/  246]    Overall Loss 0.272820    Objective Loss 0.272820                                        LR 0.000003    Time 0.044248    
2023-01-06 17:13:22,824 - Epoch: [198][   30/  246]    Overall Loss 0.260558    Objective Loss 0.260558                                        LR 0.000003    Time 0.035315    
2023-01-06 17:13:22,996 - Epoch: [198][   40/  246]    Overall Loss 0.256722    Objective Loss 0.256722                                        LR 0.000003    Time 0.030776    
2023-01-06 17:13:23,170 - Epoch: [198][   50/  246]    Overall Loss 0.253978    Objective Loss 0.253978                                        LR 0.000003    Time 0.028083    
2023-01-06 17:13:23,345 - Epoch: [198][   60/  246]    Overall Loss 0.254218    Objective Loss 0.254218                                        LR 0.000003    Time 0.026313    
2023-01-06 17:13:23,533 - Epoch: [198][   70/  246]    Overall Loss 0.253654    Objective Loss 0.253654                                        LR 0.000003    Time 0.025241    
2023-01-06 17:13:23,729 - Epoch: [198][   80/  246]    Overall Loss 0.253813    Objective Loss 0.253813                                        LR 0.000003    Time 0.024531    
2023-01-06 17:13:23,922 - Epoch: [198][   90/  246]    Overall Loss 0.255919    Objective Loss 0.255919                                        LR 0.000003    Time 0.023945    
2023-01-06 17:13:24,116 - Epoch: [198][  100/  246]    Overall Loss 0.255157    Objective Loss 0.255157                                        LR 0.000003    Time 0.023487    
2023-01-06 17:13:24,311 - Epoch: [198][  110/  246]    Overall Loss 0.255477    Objective Loss 0.255477                                        LR 0.000003    Time 0.023118    
2023-01-06 17:13:24,506 - Epoch: [198][  120/  246]    Overall Loss 0.254620    Objective Loss 0.254620                                        LR 0.000003    Time 0.022817    
2023-01-06 17:13:24,701 - Epoch: [198][  130/  246]    Overall Loss 0.254435    Objective Loss 0.254435                                        LR 0.000003    Time 0.022558    
2023-01-06 17:13:24,897 - Epoch: [198][  140/  246]    Overall Loss 0.254866    Objective Loss 0.254866                                        LR 0.000003    Time 0.022341    
2023-01-06 17:13:25,093 - Epoch: [198][  150/  246]    Overall Loss 0.255811    Objective Loss 0.255811                                        LR 0.000003    Time 0.022154    
2023-01-06 17:13:25,289 - Epoch: [198][  160/  246]    Overall Loss 0.255849    Objective Loss 0.255849                                        LR 0.000003    Time 0.021993    
2023-01-06 17:13:25,485 - Epoch: [198][  170/  246]    Overall Loss 0.256340    Objective Loss 0.256340                                        LR 0.000003    Time 0.021848    
2023-01-06 17:13:25,675 - Epoch: [198][  180/  246]    Overall Loss 0.255624    Objective Loss 0.255624                                        LR 0.000003    Time 0.021693    
2023-01-06 17:13:25,871 - Epoch: [198][  190/  246]    Overall Loss 0.255351    Objective Loss 0.255351                                        LR 0.000003    Time 0.021580    
2023-01-06 17:13:26,056 - Epoch: [198][  200/  246]    Overall Loss 0.254649    Objective Loss 0.254649                                        LR 0.000003    Time 0.021418    
2023-01-06 17:13:26,239 - Epoch: [198][  210/  246]    Overall Loss 0.254213    Objective Loss 0.254213                                        LR 0.000003    Time 0.021267    
2023-01-06 17:13:26,441 - Epoch: [198][  220/  246]    Overall Loss 0.253184    Objective Loss 0.253184                                        LR 0.000003    Time 0.021215    
2023-01-06 17:13:26,636 - Epoch: [198][  230/  246]    Overall Loss 0.253040    Objective Loss 0.253040                                        LR 0.000003    Time 0.021141    
2023-01-06 17:13:26,851 - Epoch: [198][  240/  246]    Overall Loss 0.252448    Objective Loss 0.252448                                        LR 0.000003    Time 0.021155    
2023-01-06 17:13:26,948 - Epoch: [198][  246/  246]    Overall Loss 0.252781    Objective Loss 0.252781    Top1 89.473684    LR 0.000003    Time 0.021032    
2023-01-06 17:13:27,075 - --- validate (epoch=198)-----------
2023-01-06 17:13:27,075 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:27,544 - Epoch: [198][   10/   28]    Loss 0.267211    Top1 90.117188    
2023-01-06 17:13:27,663 - Epoch: [198][   20/   28]    Loss 0.263594    Top1 90.332031    
2023-01-06 17:13:27,730 - Epoch: [198][   28/   28]    Loss 0.270013    Top1 90.223304    
2023-01-06 17:13:27,884 - ==> Top1: 90.223    Loss: 0.270

2023-01-06 17:13:27,884 - ==> Confusion:
[[ 242   13  184]
 [  20  245  337]
 [  69   60 5816]]

2023-01-06 17:13:27,885 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:27,885 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:27,891 - 

2023-01-06 17:13:27,891 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 17:13:28,459 - Epoch: [199][   10/  246]    Overall Loss 0.261208    Objective Loss 0.261208                                        LR 0.000003    Time 0.056706    
2023-01-06 17:13:28,655 - Epoch: [199][   20/  246]    Overall Loss 0.255654    Objective Loss 0.255654                                        LR 0.000003    Time 0.038095    
2023-01-06 17:13:28,844 - Epoch: [199][   30/  246]    Overall Loss 0.254205    Objective Loss 0.254205                                        LR 0.000003    Time 0.031683    
2023-01-06 17:13:29,032 - Epoch: [199][   40/  246]    Overall Loss 0.251688    Objective Loss 0.251688                                        LR 0.000003    Time 0.028458    
2023-01-06 17:13:29,226 - Epoch: [199][   50/  246]    Overall Loss 0.251500    Objective Loss 0.251500                                        LR 0.000003    Time 0.026648    
2023-01-06 17:13:29,417 - Epoch: [199][   60/  246]    Overall Loss 0.256233    Objective Loss 0.256233                                        LR 0.000003    Time 0.025379    
2023-01-06 17:13:29,603 - Epoch: [199][   70/  246]    Overall Loss 0.254216    Objective Loss 0.254216                                        LR 0.000003    Time 0.024413    
2023-01-06 17:13:29,791 - Epoch: [199][   80/  246]    Overall Loss 0.256077    Objective Loss 0.256077                                        LR 0.000003    Time 0.023707    
2023-01-06 17:13:29,980 - Epoch: [199][   90/  246]    Overall Loss 0.256494    Objective Loss 0.256494                                        LR 0.000003    Time 0.023160    
2023-01-06 17:13:30,175 - Epoch: [199][  100/  246]    Overall Loss 0.256059    Objective Loss 0.256059                                        LR 0.000003    Time 0.022793    
2023-01-06 17:13:30,372 - Epoch: [199][  110/  246]    Overall Loss 0.255642    Objective Loss 0.255642                                        LR 0.000003    Time 0.022511    
2023-01-06 17:13:30,569 - Epoch: [199][  120/  246]    Overall Loss 0.254549    Objective Loss 0.254549                                        LR 0.000003    Time 0.022274    
2023-01-06 17:13:30,766 - Epoch: [199][  130/  246]    Overall Loss 0.252498    Objective Loss 0.252498                                        LR 0.000003    Time 0.022076    
2023-01-06 17:13:30,963 - Epoch: [199][  140/  246]    Overall Loss 0.251723    Objective Loss 0.251723                                        LR 0.000003    Time 0.021904    
2023-01-06 17:13:31,159 - Epoch: [199][  150/  246]    Overall Loss 0.251415    Objective Loss 0.251415                                        LR 0.000003    Time 0.021744    
2023-01-06 17:13:31,355 - Epoch: [199][  160/  246]    Overall Loss 0.252448    Objective Loss 0.252448                                        LR 0.000003    Time 0.021608    
2023-01-06 17:13:31,541 - Epoch: [199][  170/  246]    Overall Loss 0.252700    Objective Loss 0.252700                                        LR 0.000003    Time 0.021432    
2023-01-06 17:13:31,705 - Epoch: [199][  180/  246]    Overall Loss 0.251693    Objective Loss 0.251693                                        LR 0.000003    Time 0.021149    
2023-01-06 17:13:31,884 - Epoch: [199][  190/  246]    Overall Loss 0.252661    Objective Loss 0.252661                                        LR 0.000003    Time 0.020977    
2023-01-06 17:13:32,079 - Epoch: [199][  200/  246]    Overall Loss 0.252321    Objective Loss 0.252321                                        LR 0.000003    Time 0.020900    
2023-01-06 17:13:32,279 - Epoch: [199][  210/  246]    Overall Loss 0.251435    Objective Loss 0.251435                                        LR 0.000003    Time 0.020854    
2023-01-06 17:13:32,473 - Epoch: [199][  220/  246]    Overall Loss 0.252013    Objective Loss 0.252013                                        LR 0.000003    Time 0.020788    
2023-01-06 17:13:32,660 - Epoch: [199][  230/  246]    Overall Loss 0.252377    Objective Loss 0.252377                                        LR 0.000003    Time 0.020696    
2023-01-06 17:13:32,869 - Epoch: [199][  240/  246]    Overall Loss 0.252177    Objective Loss 0.252177                                        LR 0.000003    Time 0.020705    
2023-01-06 17:13:32,965 - Epoch: [199][  246/  246]    Overall Loss 0.252246    Objective Loss 0.252246    Top1 91.866029    LR 0.000003    Time 0.020589    
2023-01-06 17:13:33,123 - --- validate (epoch=199)-----------
2023-01-06 17:13:33,123 - 6986 samples (256 per mini-batch)
2023-01-06 17:13:33,612 - Epoch: [199][   10/   28]    Loss 0.265244    Top1 89.726562    
2023-01-06 17:13:33,742 - Epoch: [199][   20/   28]    Loss 0.267953    Top1 90.371094    
2023-01-06 17:13:33,809 - Epoch: [199][   28/   28]    Loss 0.268304    Top1 90.294875    
2023-01-06 17:13:33,961 - ==> Top1: 90.295    Loss: 0.268

2023-01-06 17:13:33,962 - ==> Confusion:
[[ 256   13  170]
 [  21  261  320]
 [  81   73 5791]]

2023-01-06 17:13:33,963 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 155168 on epoch: 168]
2023-01-06 17:13:33,963 - Saving checkpoint to: logs/2023.01.06-165156/qat_checkpoint.pth.tar
2023-01-06 17:13:33,969 - --- test ---------------------
2023-01-06 17:13:33,970 - 13117 samples (256 per mini-batch)
2023-01-06 17:13:34,569 - Test: [   10/   52]    Loss 0.216946    Top1 92.421875    
2023-01-06 17:13:34,677 - Test: [   20/   52]    Loss 0.218848    Top1 92.421875    
2023-01-06 17:13:34,790 - Test: [   30/   52]    Loss 0.217798    Top1 92.382812    
2023-01-06 17:13:34,907 - Test: [   40/   52]    Loss 0.227450    Top1 91.982422    
2023-01-06 17:13:35,010 - Test: [   50/   52]    Loss 0.226517    Top1 92.039062    
2023-01-06 17:13:35,026 - Test: [   52/   52]    Loss 0.225554    Top1 92.056110    
2023-01-06 17:13:35,167 - ==> Top1: 92.056    Loss: 0.226

2023-01-06 17:13:35,167 - ==> Confusion:
[[  281    16   264]
 [   23   325   408]
 [  150   181 11469]]

2023-01-06 17:13:35,241 - 
2023-01-06 17:13:35,241 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-165156/2023.01.06-165156.log
