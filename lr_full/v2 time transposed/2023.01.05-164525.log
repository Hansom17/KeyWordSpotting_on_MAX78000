2023-01-05 16:45:25,669 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.05-164525/2023.01.05-164525.log
2023-01-05 16:45:27,712 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-05 16:45:27,712 - Optimizer Args: {'lr': 1e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-05 16:45:39,860 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-05 16:45:39,861 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-05 16:45:39,866 - 

2023-01-05 16:45:39,866 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:40,567 - Epoch: [0][   10/   37]    Overall Loss 1.098417    Objective Loss 1.098417                                        LR 0.000010    Time 0.070081    
2023-01-05 16:45:40,810 - Epoch: [0][   20/   37]    Overall Loss 1.098319    Objective Loss 1.098319                                        LR 0.000010    Time 0.047165    
2023-01-05 16:45:41,078 - Epoch: [0][   30/   37]    Overall Loss 1.098212    Objective Loss 1.098212                                        LR 0.000010    Time 0.040365    
2023-01-05 16:45:41,235 - Epoch: [0][   37/   37]    Overall Loss 1.098125    Objective Loss 1.098125    Top1 56.276151    LR 0.000010    Time 0.036945    
2023-01-05 16:45:41,301 - --- validate (epoch=0)-----------
2023-01-05 16:45:41,301 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:41,516 - Epoch: [0][    5/    5]    Loss 1.097591    Top1 59.064885    
2023-01-05 16:45:41,572 - ==> Top1: 59.065    Loss: 1.098

2023-01-05 16:45:41,573 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:41,574 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 0]
2023-01-05 16:45:41,574 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:41,585 - 

2023-01-05 16:45:41,585 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:42,029 - Epoch: [1][   10/   37]    Overall Loss 1.097403    Objective Loss 1.097403                                        LR 0.000010    Time 0.044321    
2023-01-05 16:45:42,286 - Epoch: [1][   20/   37]    Overall Loss 1.097137    Objective Loss 1.097137                                        LR 0.000010    Time 0.034967    
2023-01-05 16:45:42,542 - Epoch: [1][   30/   37]    Overall Loss 1.096796    Objective Loss 1.096796                                        LR 0.000010    Time 0.031820    
2023-01-05 16:45:42,696 - Epoch: [1][   37/   37]    Overall Loss 1.096486    Objective Loss 1.096486    Top1 56.903766    LR 0.000010    Time 0.029965    
2023-01-05 16:45:42,765 - --- validate (epoch=1)-----------
2023-01-05 16:45:42,766 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:42,973 - Epoch: [1][    5/    5]    Loss 1.094476    Top1 59.064885    
2023-01-05 16:45:43,039 - ==> Top1: 59.065    Loss: 1.094

2023-01-05 16:45:43,039 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:43,041 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 1]
2023-01-05 16:45:43,041 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:43,062 - 

2023-01-05 16:45:43,062 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:43,505 - Epoch: [2][   10/   37]    Overall Loss 1.093807    Objective Loss 1.093807                                        LR 0.000010    Time 0.044293    
2023-01-05 16:45:43,764 - Epoch: [2][   20/   37]    Overall Loss 1.092658    Objective Loss 1.092658                                        LR 0.000010    Time 0.035058    
2023-01-05 16:45:44,020 - Epoch: [2][   30/   37]    Overall Loss 1.091094    Objective Loss 1.091094                                        LR 0.000010    Time 0.031874    
2023-01-05 16:45:44,175 - Epoch: [2][   37/   37]    Overall Loss 1.089714    Objective Loss 1.089714    Top1 58.368201    LR 0.000010    Time 0.030028    
2023-01-05 16:45:44,245 - --- validate (epoch=2)-----------
2023-01-05 16:45:44,245 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:44,466 - Epoch: [2][    5/    5]    Loss 1.081400    Top1 59.064885    
2023-01-05 16:45:44,526 - ==> Top1: 59.065    Loss: 1.081

2023-01-05 16:45:44,527 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:44,528 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 2]
2023-01-05 16:45:44,528 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:44,551 - 

2023-01-05 16:45:44,551 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:44,981 - Epoch: [3][   10/   37]    Overall Loss 1.077926    Objective Loss 1.077926                                        LR 0.000010    Time 0.042956    
2023-01-05 16:45:45,235 - Epoch: [3][   20/   37]    Overall Loss 1.072781    Objective Loss 1.072781                                        LR 0.000010    Time 0.034150    
2023-01-05 16:45:45,489 - Epoch: [3][   30/   37]    Overall Loss 1.066334    Objective Loss 1.066334                                        LR 0.000010    Time 0.031200    
2023-01-05 16:45:45,645 - Epoch: [3][   37/   37]    Overall Loss 1.060314    Objective Loss 1.060314    Top1 58.786611    LR 0.000010    Time 0.029507    
2023-01-05 16:45:45,708 - --- validate (epoch=3)-----------
2023-01-05 16:45:45,708 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:45,926 - Epoch: [3][    5/    5]    Loss 1.023640    Top1 59.064885    
2023-01-05 16:45:45,991 - ==> Top1: 59.065    Loss: 1.024

2023-01-05 16:45:45,992 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:45,993 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 3]
2023-01-05 16:45:45,993 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:46,014 - 

2023-01-05 16:45:46,014 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:46,431 - Epoch: [4][   10/   37]    Overall Loss 1.012951    Objective Loss 1.012951                                        LR 0.000010    Time 0.041666    
2023-01-05 16:45:46,680 - Epoch: [4][   20/   37]    Overall Loss 0.995030    Objective Loss 0.995030                                        LR 0.000010    Time 0.033260    
2023-01-05 16:45:46,940 - Epoch: [4][   30/   37]    Overall Loss 0.973878    Objective Loss 0.973878                                        LR 0.000010    Time 0.030800    
2023-01-05 16:45:47,095 - Epoch: [4][   37/   37]    Overall Loss 0.957930    Objective Loss 0.957930    Top1 55.439331    LR 0.000010    Time 0.029166    
2023-01-05 16:45:47,163 - --- validate (epoch=4)-----------
2023-01-05 16:45:47,163 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:47,387 - Epoch: [4][    5/    5]    Loss 0.858438    Top1 59.064885    
2023-01-05 16:45:47,451 - ==> Top1: 59.065    Loss: 0.858

2023-01-05 16:45:47,451 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:47,453 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 4]
2023-01-05 16:45:47,453 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:47,473 - 

2023-01-05 16:45:47,473 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:47,906 - Epoch: [5][   10/   37]    Overall Loss 0.852250    Objective Loss 0.852250                                        LR 0.000010    Time 0.043213    
2023-01-05 16:45:48,152 - Epoch: [5][   20/   37]    Overall Loss 0.830118    Objective Loss 0.830118                                        LR 0.000010    Time 0.033881    
2023-01-05 16:45:48,408 - Epoch: [5][   30/   37]    Overall Loss 0.815643    Objective Loss 0.815643                                        LR 0.000010    Time 0.031095    
2023-01-05 16:45:48,566 - Epoch: [5][   37/   37]    Overall Loss 0.808465    Objective Loss 0.808465    Top1 58.158996    LR 0.000010    Time 0.029472    
2023-01-05 16:45:48,636 - --- validate (epoch=5)-----------
2023-01-05 16:45:48,636 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:48,856 - Epoch: [5][    5/    5]    Loss 0.768250    Top1 59.064885    
2023-01-05 16:45:48,931 - ==> Top1: 59.065    Loss: 0.768

2023-01-05 16:45:48,931 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:48,932 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 5]
2023-01-05 16:45:48,932 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:48,953 - 

2023-01-05 16:45:48,954 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:49,514 - Epoch: [6][   10/   37]    Overall Loss 0.776018    Objective Loss 0.776018                                        LR 0.000010    Time 0.056012    
2023-01-05 16:45:49,766 - Epoch: [6][   20/   37]    Overall Loss 0.771692    Objective Loss 0.771692                                        LR 0.000010    Time 0.040566    
2023-01-05 16:45:50,017 - Epoch: [6][   30/   37]    Overall Loss 0.769413    Objective Loss 0.769413                                        LR 0.000010    Time 0.035388    
2023-01-05 16:45:50,171 - Epoch: [6][   37/   37]    Overall Loss 0.767916    Objective Loss 0.767916    Top1 52.719665    LR 0.000010    Time 0.032869    
2023-01-05 16:45:50,242 - --- validate (epoch=6)-----------
2023-01-05 16:45:50,242 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:50,460 - Epoch: [6][    5/    5]    Loss 0.757307    Top1 59.064885    
2023-01-05 16:45:50,530 - ==> Top1: 59.065    Loss: 0.757

2023-01-05 16:45:50,531 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:50,532 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 6]
2023-01-05 16:45:50,532 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:50,555 - 

2023-01-05 16:45:50,555 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:50,993 - Epoch: [7][   10/   37]    Overall Loss 0.758061    Objective Loss 0.758061                                        LR 0.000010    Time 0.043724    
2023-01-05 16:45:51,241 - Epoch: [7][   20/   37]    Overall Loss 0.756299    Objective Loss 0.756299                                        LR 0.000010    Time 0.034224    
2023-01-05 16:45:51,498 - Epoch: [7][   30/   37]    Overall Loss 0.754850    Objective Loss 0.754850                                        LR 0.000010    Time 0.031367    
2023-01-05 16:45:51,652 - Epoch: [7][   37/   37]    Overall Loss 0.753909    Objective Loss 0.753909    Top1 59.623431    LR 0.000010    Time 0.029608    
2023-01-05 16:45:51,723 - --- validate (epoch=7)-----------
2023-01-05 16:45:51,723 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:51,958 - Epoch: [7][    5/    5]    Loss 0.748880    Top1 59.064885    
2023-01-05 16:45:52,029 - ==> Top1: 59.065    Loss: 0.749

2023-01-05 16:45:52,029 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:52,030 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 7]
2023-01-05 16:45:52,031 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:52,051 - 

2023-01-05 16:45:52,052 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:52,458 - Epoch: [8][   10/   37]    Overall Loss 0.748463    Objective Loss 0.748463                                        LR 0.000010    Time 0.040507    
2023-01-05 16:45:52,708 - Epoch: [8][   20/   37]    Overall Loss 0.747478    Objective Loss 0.747478                                        LR 0.000010    Time 0.032766    
2023-01-05 16:45:52,949 - Epoch: [8][   30/   37]    Overall Loss 0.746598    Objective Loss 0.746598                                        LR 0.000010    Time 0.029854    
2023-01-05 16:45:53,097 - Epoch: [8][   37/   37]    Overall Loss 0.746005    Objective Loss 0.746005    Top1 56.066946    LR 0.000010    Time 0.028205    
2023-01-05 16:45:53,175 - --- validate (epoch=8)-----------
2023-01-05 16:45:53,175 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:53,386 - Epoch: [8][    5/    5]    Loss 0.743187    Top1 59.064885    
2023-01-05 16:45:53,441 - ==> Top1: 59.065    Loss: 0.743

2023-01-05 16:45:53,441 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:53,442 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 8]
2023-01-05 16:45:53,442 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:53,467 - 

2023-01-05 16:45:53,467 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:53,908 - Epoch: [9][   10/   37]    Overall Loss 0.742111    Objective Loss 0.742111                                        LR 0.000010    Time 0.044010    
2023-01-05 16:45:54,157 - Epoch: [9][   20/   37]    Overall Loss 0.741538    Objective Loss 0.741538                                        LR 0.000010    Time 0.034442    
2023-01-05 16:45:54,407 - Epoch: [9][   30/   37]    Overall Loss 0.740817    Objective Loss 0.740817                                        LR 0.000010    Time 0.031270    
2023-01-05 16:45:54,561 - Epoch: [9][   37/   37]    Overall Loss 0.740327    Objective Loss 0.740327    Top1 58.786611    LR 0.000010    Time 0.029504    
2023-01-05 16:45:54,633 - --- validate (epoch=9)-----------
2023-01-05 16:45:54,634 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:54,854 - Epoch: [9][    5/    5]    Loss 0.737787    Top1 59.064885    
2023-01-05 16:45:54,935 - ==> Top1: 59.065    Loss: 0.738

2023-01-05 16:45:54,935 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:54,936 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 9]
2023-01-05 16:45:54,936 - Saving checkpoint to: logs/2023.01.05-164525/checkpoint.pth.tar
2023-01-05 16:45:54,976 - 

2023-01-05 16:45:54,976 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:55,468 - Epoch: [10][   10/   37]    Overall Loss 0.692023    Objective Loss 0.692023                                        LR 0.000010    Time 0.049091    
2023-01-05 16:45:55,776 - Epoch: [10][   20/   37]    Overall Loss 0.691759    Objective Loss 0.691759                                        LR 0.000010    Time 0.039924    
2023-01-05 16:45:56,085 - Epoch: [10][   30/   37]    Overall Loss 0.691893    Objective Loss 0.691893                                        LR 0.000010    Time 0.036922    
2023-01-05 16:45:56,275 - Epoch: [10][   37/   37]    Overall Loss 0.691546    Objective Loss 0.691546    Top1 60.041841    LR 0.000010    Time 0.035041    
2023-01-05 16:45:56,348 - --- validate (epoch=10)-----------
2023-01-05 16:45:56,348 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:56,601 - Epoch: [10][    5/    5]    Loss 0.690682    Top1 59.064885    
2023-01-05 16:45:56,675 - ==> Top1: 59.065    Loss: 0.691

2023-01-05 16:45:56,675 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:45:56,677 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 10]
2023-01-05 16:45:56,677 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:45:56,688 - 

2023-01-05 16:45:56,688 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:57,204 - Epoch: [11][   10/   37]    Overall Loss 0.691257    Objective Loss 0.691257                                        LR 0.000010    Time 0.051527    
2023-01-05 16:45:57,506 - Epoch: [11][   20/   37]    Overall Loss 0.691447    Objective Loss 0.691447                                        LR 0.000010    Time 0.040650    
2023-01-05 16:45:57,824 - Epoch: [11][   30/   37]    Overall Loss 0.691599    Objective Loss 0.691599                                        LR 0.000010    Time 0.037713    
2023-01-05 16:45:58,034 - Epoch: [11][   37/   37]    Overall Loss 0.691422    Objective Loss 0.691422    Top1 53.765690    LR 0.000010    Time 0.036220    
2023-01-05 16:45:58,106 - --- validate (epoch=11)-----------
2023-01-05 16:45:58,106 - 1048 samples (256 per mini-batch)
2023-01-05 16:45:58,356 - Epoch: [11][    5/    5]    Loss 0.692050    Top1 47.996183    
2023-01-05 16:45:58,430 - ==> Top1: 47.996    Loss: 0.692

2023-01-05 16:45:58,430 - ==> Confusion:
[[370  59   0]
 [486 133   0]
 [  0   0   0]]

2023-01-05 16:45:58,432 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 361664 on epoch: 10]
2023-01-05 16:45:58,432 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:45:58,442 - 

2023-01-05 16:45:58,442 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:45:58,932 - Epoch: [12][   10/   37]    Overall Loss 0.691321    Objective Loss 0.691321                                        LR 0.000010    Time 0.048980    
2023-01-05 16:45:59,223 - Epoch: [12][   20/   37]    Overall Loss 0.691387    Objective Loss 0.691387                                        LR 0.000010    Time 0.039008    
2023-01-05 16:45:59,513 - Epoch: [12][   30/   37]    Overall Loss 0.691135    Objective Loss 0.691135                                        LR 0.000010    Time 0.035664    
2023-01-05 16:45:59,716 - Epoch: [12][   37/   37]    Overall Loss 0.691123    Objective Loss 0.691123    Top1 56.485356    LR 0.000010    Time 0.034388    
2023-01-05 16:45:59,791 - --- validate (epoch=12)-----------
2023-01-05 16:45:59,791 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:00,038 - Epoch: [12][    5/    5]    Loss 0.691216    Top1 61.927481    
2023-01-05 16:46:00,106 - ==> Top1: 61.927    Loss: 0.691

2023-01-05 16:46:00,107 - ==> Confusion:
[[117 312   0]
 [ 87 532   0]
 [  0   0   0]]

2023-01-05 16:46:00,108 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:00,108 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:00,133 - 

2023-01-05 16:46:00,133 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:00,620 - Epoch: [13][   10/   37]    Overall Loss 0.690269    Objective Loss 0.690269                                        LR 0.000010    Time 0.048619    
2023-01-05 16:46:00,903 - Epoch: [13][   20/   37]    Overall Loss 0.690532    Objective Loss 0.690532                                        LR 0.000010    Time 0.038438    
2023-01-05 16:46:01,207 - Epoch: [13][   30/   37]    Overall Loss 0.690497    Objective Loss 0.690497                                        LR 0.000010    Time 0.035738    
2023-01-05 16:46:01,407 - Epoch: [13][   37/   37]    Overall Loss 0.690439    Objective Loss 0.690439    Top1 44.351464    LR 0.000010    Time 0.034377    
2023-01-05 16:46:01,473 - --- validate (epoch=13)-----------
2023-01-05 16:46:01,473 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:01,716 - Epoch: [13][    5/    5]    Loss 0.698401    Top1 41.603053    
2023-01-05 16:46:01,772 - ==> Top1: 41.603    Loss: 0.698

2023-01-05 16:46:01,772 - ==> Confusion:
[[420   9   0]
 [603  16   0]
 [  0   0   0]]

2023-01-05 16:46:01,773 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:01,773 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:01,783 - 

2023-01-05 16:46:01,783 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:02,284 - Epoch: [14][   10/   37]    Overall Loss 0.691346    Objective Loss 0.691346                                        LR 0.000010    Time 0.049938    
2023-01-05 16:46:02,577 - Epoch: [14][   20/   37]    Overall Loss 0.690598    Objective Loss 0.690598                                        LR 0.000010    Time 0.039596    
2023-01-05 16:46:02,881 - Epoch: [14][   30/   37]    Overall Loss 0.690701    Objective Loss 0.690701                                        LR 0.000010    Time 0.036541    
2023-01-05 16:46:03,084 - Epoch: [14][   37/   37]    Overall Loss 0.690547    Objective Loss 0.690547    Top1 58.786611    LR 0.000010    Time 0.035114    
2023-01-05 16:46:03,157 - --- validate (epoch=14)-----------
2023-01-05 16:46:03,157 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:03,404 - Epoch: [14][    5/    5]    Loss 0.688954    Top1 59.064885    
2023-01-05 16:46:03,478 - ==> Top1: 59.065    Loss: 0.689

2023-01-05 16:46:03,479 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-05 16:46:03,480 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:03,480 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:03,489 - 

2023-01-05 16:46:03,489 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:04,002 - Epoch: [15][   10/   37]    Overall Loss 0.689843    Objective Loss 0.689843                                        LR 0.000010    Time 0.051226    
2023-01-05 16:46:04,300 - Epoch: [15][   20/   37]    Overall Loss 0.691191    Objective Loss 0.691191                                        LR 0.000010    Time 0.040457    
2023-01-05 16:46:04,609 - Epoch: [15][   30/   37]    Overall Loss 0.690823    Objective Loss 0.690823                                        LR 0.000010    Time 0.037270    
2023-01-05 16:46:04,812 - Epoch: [15][   37/   37]    Overall Loss 0.690625    Objective Loss 0.690625    Top1 57.112971    LR 0.000010    Time 0.035712    
2023-01-05 16:46:04,884 - --- validate (epoch=15)-----------
2023-01-05 16:46:04,884 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:05,121 - Epoch: [15][    5/    5]    Loss 0.691137    Top1 59.541985    
2023-01-05 16:46:05,185 - ==> Top1: 59.542    Loss: 0.691

2023-01-05 16:46:05,185 - ==> Confusion:
[[ 15 414   0]
 [ 10 609   0]
 [  0   0   0]]

2023-01-05 16:46:05,187 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:05,187 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:05,196 - 

2023-01-05 16:46:05,197 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:05,699 - Epoch: [16][   10/   37]    Overall Loss 0.689496    Objective Loss 0.689496                                        LR 0.000010    Time 0.050186    
2023-01-05 16:46:05,987 - Epoch: [16][   20/   37]    Overall Loss 0.689601    Objective Loss 0.689601                                        LR 0.000010    Time 0.039476    
2023-01-05 16:46:06,288 - Epoch: [16][   30/   37]    Overall Loss 0.689730    Objective Loss 0.689730                                        LR 0.000010    Time 0.036316    
2023-01-05 16:46:06,482 - Epoch: [16][   37/   37]    Overall Loss 0.689795    Objective Loss 0.689795    Top1 50.836820    LR 0.000010    Time 0.034696    
2023-01-05 16:46:06,557 - --- validate (epoch=16)-----------
2023-01-05 16:46:06,557 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:06,811 - Epoch: [16][    5/    5]    Loss 0.693171    Top1 44.751908    
2023-01-05 16:46:06,873 - ==> Top1: 44.752    Loss: 0.693

2023-01-05 16:46:06,873 - ==> Confusion:
[[401  28   0]
 [551  68   0]
 [  0   0   0]]

2023-01-05 16:46:06,874 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:06,874 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:06,884 - 

2023-01-05 16:46:06,884 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:07,384 - Epoch: [17][   10/   37]    Overall Loss 0.689747    Objective Loss 0.689747                                        LR 0.000010    Time 0.049912    
2023-01-05 16:46:07,703 - Epoch: [17][   20/   37]    Overall Loss 0.689910    Objective Loss 0.689910                                        LR 0.000010    Time 0.040877    
2023-01-05 16:46:08,025 - Epoch: [17][   30/   37]    Overall Loss 0.689747    Objective Loss 0.689747                                        LR 0.000010    Time 0.037991    
2023-01-05 16:46:08,225 - Epoch: [17][   37/   37]    Overall Loss 0.689829    Objective Loss 0.689829    Top1 51.882845    LR 0.000010    Time 0.036203    
2023-01-05 16:46:08,300 - --- validate (epoch=17)-----------
2023-01-05 16:46:08,300 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:08,549 - Epoch: [17][    5/    5]    Loss 0.690293    Top1 52.194656    
2023-01-05 16:46:08,611 - ==> Top1: 52.195    Loss: 0.690

2023-01-05 16:46:08,611 - ==> Confusion:
[[353  76   0]
 [425 194   0]
 [  0   0   0]]

2023-01-05 16:46:08,613 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:08,613 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:08,623 - 

2023-01-05 16:46:08,623 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:09,122 - Epoch: [18][   10/   37]    Overall Loss 0.687767    Objective Loss 0.687767                                        LR 0.000010    Time 0.049854    
2023-01-05 16:46:09,414 - Epoch: [18][   20/   37]    Overall Loss 0.689185    Objective Loss 0.689185                                        LR 0.000010    Time 0.039508    
2023-01-05 16:46:09,709 - Epoch: [18][   30/   37]    Overall Loss 0.689868    Objective Loss 0.689868                                        LR 0.000010    Time 0.036151    
2023-01-05 16:46:09,909 - Epoch: [18][   37/   37]    Overall Loss 0.690348    Objective Loss 0.690348    Top1 57.322176    LR 0.000010    Time 0.034725    
2023-01-05 16:46:09,982 - --- validate (epoch=18)-----------
2023-01-05 16:46:09,982 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:10,219 - Epoch: [18][    5/    5]    Loss 0.689438    Top1 58.396947    
2023-01-05 16:46:10,278 - ==> Top1: 58.397    Loss: 0.689

2023-01-05 16:46:10,278 - ==> Confusion:
[[  6 423   0]
 [ 13 606   0]
 [  0   0   0]]

2023-01-05 16:46:10,280 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:10,280 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:10,294 - 

2023-01-05 16:46:10,294 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:10,777 - Epoch: [19][   10/   37]    Overall Loss 0.688527    Objective Loss 0.688527                                        LR 0.000010    Time 0.048180    
2023-01-05 16:46:11,059 - Epoch: [19][   20/   37]    Overall Loss 0.689353    Objective Loss 0.689353                                        LR 0.000010    Time 0.038175    
2023-01-05 16:46:11,344 - Epoch: [19][   30/   37]    Overall Loss 0.688978    Objective Loss 0.688978                                        LR 0.000010    Time 0.034938    
2023-01-05 16:46:11,531 - Epoch: [19][   37/   37]    Overall Loss 0.688985    Objective Loss 0.688985    Top1 49.163180    LR 0.000010    Time 0.033365    
2023-01-05 16:46:11,599 - --- validate (epoch=19)-----------
2023-01-05 16:46:11,599 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:11,844 - Epoch: [19][    5/    5]    Loss 0.691041    Top1 44.847328    
2023-01-05 16:46:11,903 - ==> Top1: 44.847    Loss: 0.691

2023-01-05 16:46:11,903 - ==> Confusion:
[[393  36   0]
 [542  77   0]
 [  0   0   0]]

2023-01-05 16:46:11,904 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:11,904 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:11,914 - 

2023-01-05 16:46:11,914 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:12,390 - Epoch: [20][   10/   37]    Overall Loss 0.689423    Objective Loss 0.689423                                        LR 0.000010    Time 0.047449    
2023-01-05 16:46:12,680 - Epoch: [20][   20/   37]    Overall Loss 0.689211    Objective Loss 0.689211                                        LR 0.000010    Time 0.038224    
2023-01-05 16:46:12,978 - Epoch: [20][   30/   37]    Overall Loss 0.688793    Objective Loss 0.688793                                        LR 0.000010    Time 0.035396    
2023-01-05 16:46:13,164 - Epoch: [20][   37/   37]    Overall Loss 0.688554    Objective Loss 0.688554    Top1 58.158996    LR 0.000010    Time 0.033722    
2023-01-05 16:46:13,227 - --- validate (epoch=20)-----------
2023-01-05 16:46:13,227 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:13,467 - Epoch: [20][    5/    5]    Loss 0.689625    Top1 53.912214    
2023-01-05 16:46:13,537 - ==> Top1: 53.912    Loss: 0.690

2023-01-05 16:46:13,537 - ==> Confusion:
[[331  98   0]
 [385 234   0]
 [  0   0   0]]

2023-01-05 16:46:13,539 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:13,539 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:13,549 - 

2023-01-05 16:46:13,549 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:14,148 - Epoch: [21][   10/   37]    Overall Loss 0.687508    Objective Loss 0.687508                                        LR 0.000010    Time 0.059816    
2023-01-05 16:46:14,439 - Epoch: [21][   20/   37]    Overall Loss 0.687860    Objective Loss 0.687860                                        LR 0.000010    Time 0.044434    
2023-01-05 16:46:14,751 - Epoch: [21][   30/   37]    Overall Loss 0.687669    Objective Loss 0.687669                                        LR 0.000010    Time 0.040029    
2023-01-05 16:46:14,948 - Epoch: [21][   37/   37]    Overall Loss 0.687499    Objective Loss 0.687499    Top1 55.230126    LR 0.000010    Time 0.037759    
2023-01-05 16:46:15,024 - --- validate (epoch=21)-----------
2023-01-05 16:46:15,024 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:15,281 - Epoch: [21][    5/    5]    Loss 0.688281    Top1 57.824427    
2023-01-05 16:46:15,340 - ==> Top1: 57.824    Loss: 0.688

2023-01-05 16:46:15,340 - ==> Confusion:
[[165 264   0]
 [178 441   0]
 [  0   0   0]]

2023-01-05 16:46:15,341 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:15,341 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:15,351 - 

2023-01-05 16:46:15,351 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:15,854 - Epoch: [22][   10/   37]    Overall Loss 0.688765    Objective Loss 0.688765                                        LR 0.000010    Time 0.050239    
2023-01-05 16:46:16,162 - Epoch: [22][   20/   37]    Overall Loss 0.688160    Objective Loss 0.688160                                        LR 0.000010    Time 0.040497    
2023-01-05 16:46:16,475 - Epoch: [22][   30/   37]    Overall Loss 0.687593    Objective Loss 0.687593                                        LR 0.000010    Time 0.037416    
2023-01-05 16:46:16,672 - Epoch: [22][   37/   37]    Overall Loss 0.686929    Objective Loss 0.686929    Top1 57.531381    LR 0.000010    Time 0.035650    
2023-01-05 16:46:16,745 - --- validate (epoch=22)-----------
2023-01-05 16:46:16,745 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:16,986 - Epoch: [22][    5/    5]    Loss 0.688514    Top1 53.912214    
2023-01-05 16:46:17,048 - ==> Top1: 53.912    Loss: 0.689

2023-01-05 16:46:17,049 - ==> Confusion:
[[297 132   0]
 [351 268   0]
 [  0   0   0]]

2023-01-05 16:46:17,050 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:17,050 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:17,059 - 

2023-01-05 16:46:17,059 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:17,521 - Epoch: [23][   10/   37]    Overall Loss 0.684441    Objective Loss 0.684441                                        LR 0.000010    Time 0.046024    
2023-01-05 16:46:17,802 - Epoch: [23][   20/   37]    Overall Loss 0.685850    Objective Loss 0.685850                                        LR 0.000010    Time 0.036930    
2023-01-05 16:46:18,098 - Epoch: [23][   30/   37]    Overall Loss 0.685980    Objective Loss 0.685980                                        LR 0.000010    Time 0.034463    
2023-01-05 16:46:18,291 - Epoch: [23][   37/   37]    Overall Loss 0.685605    Objective Loss 0.685605    Top1 52.928870    LR 0.000010    Time 0.033146    
2023-01-05 16:46:18,366 - --- validate (epoch=23)-----------
2023-01-05 16:46:18,367 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:18,621 - Epoch: [23][    5/    5]    Loss 0.689629    Top1 54.293893    
2023-01-05 16:46:18,684 - ==> Top1: 54.294    Loss: 0.690

2023-01-05 16:46:18,684 - ==> Confusion:
[[309 120   0]
 [359 260   0]
 [  0   0   0]]

2023-01-05 16:46:18,686 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:18,686 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:18,695 - 

2023-01-05 16:46:18,695 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:19,188 - Epoch: [24][   10/   37]    Overall Loss 0.683884    Objective Loss 0.683884                                        LR 0.000010    Time 0.049186    
2023-01-05 16:46:19,480 - Epoch: [24][   20/   37]    Overall Loss 0.685208    Objective Loss 0.685208                                        LR 0.000010    Time 0.039136    
2023-01-05 16:46:19,772 - Epoch: [24][   30/   37]    Overall Loss 0.684764    Objective Loss 0.684764                                        LR 0.000010    Time 0.035823    
2023-01-05 16:46:19,959 - Epoch: [24][   37/   37]    Overall Loss 0.685088    Objective Loss 0.685088    Top1 58.995816    LR 0.000010    Time 0.034106    
2023-01-05 16:46:20,032 - --- validate (epoch=24)-----------
2023-01-05 16:46:20,032 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:20,271 - Epoch: [24][    5/    5]    Loss 0.683020    Top1 56.965649    
2023-01-05 16:46:20,337 - ==> Top1: 56.966    Loss: 0.683

2023-01-05 16:46:20,337 - ==> Confusion:
[[294 135   0]
 [316 303   0]
 [  0   0   0]]

2023-01-05 16:46:20,339 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:20,339 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:20,348 - 

2023-01-05 16:46:20,348 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:20,846 - Epoch: [25][   10/   37]    Overall Loss 0.685709    Objective Loss 0.685709                                        LR 0.000010    Time 0.049716    
2023-01-05 16:46:21,153 - Epoch: [25][   20/   37]    Overall Loss 0.685062    Objective Loss 0.685062                                        LR 0.000010    Time 0.040097    
2023-01-05 16:46:21,457 - Epoch: [25][   30/   37]    Overall Loss 0.684237    Objective Loss 0.684237                                        LR 0.000010    Time 0.036844    
2023-01-05 16:46:21,652 - Epoch: [25][   37/   37]    Overall Loss 0.684506    Objective Loss 0.684506    Top1 60.251046    LR 0.000010    Time 0.035128    
2023-01-05 16:46:21,730 - --- validate (epoch=25)-----------
2023-01-05 16:46:21,730 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:21,968 - Epoch: [25][    5/    5]    Loss 0.690542    Top1 51.240458    
2023-01-05 16:46:22,040 - ==> Top1: 51.240    Loss: 0.691

2023-01-05 16:46:22,041 - ==> Confusion:
[[324 105   0]
 [406 213   0]
 [  0   0   0]]

2023-01-05 16:46:22,042 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:22,042 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:22,052 - 

2023-01-05 16:46:22,052 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:22,536 - Epoch: [26][   10/   37]    Overall Loss 0.686958    Objective Loss 0.686958                                        LR 0.000010    Time 0.048327    
2023-01-05 16:46:22,827 - Epoch: [26][   20/   37]    Overall Loss 0.685573    Objective Loss 0.685573                                        LR 0.000010    Time 0.038662    
2023-01-05 16:46:23,127 - Epoch: [26][   30/   37]    Overall Loss 0.685759    Objective Loss 0.685759                                        LR 0.000010    Time 0.035764    
2023-01-05 16:46:23,321 - Epoch: [26][   37/   37]    Overall Loss 0.685685    Objective Loss 0.685685    Top1 61.087866    LR 0.000010    Time 0.034252    
2023-01-05 16:46:23,399 - --- validate (epoch=26)-----------
2023-01-05 16:46:23,399 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:23,647 - Epoch: [26][    5/    5]    Loss 0.685044    Top1 54.484733    
2023-01-05 16:46:23,713 - ==> Top1: 54.485    Loss: 0.685

2023-01-05 16:46:23,713 - ==> Confusion:
[[293 136   0]
 [341 278   0]
 [  0   0   0]]

2023-01-05 16:46:23,715 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:23,715 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:23,724 - 

2023-01-05 16:46:23,724 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:24,232 - Epoch: [27][   10/   37]    Overall Loss 0.682882    Objective Loss 0.682882                                        LR 0.000010    Time 0.050674    
2023-01-05 16:46:24,542 - Epoch: [27][   20/   37]    Overall Loss 0.683243    Objective Loss 0.683243                                        LR 0.000010    Time 0.040803    
2023-01-05 16:46:24,854 - Epoch: [27][   30/   37]    Overall Loss 0.683611    Objective Loss 0.683611                                        LR 0.000010    Time 0.037604    
2023-01-05 16:46:25,055 - Epoch: [27][   37/   37]    Overall Loss 0.683832    Objective Loss 0.683832    Top1 48.535565    LR 0.000010    Time 0.035890    
2023-01-05 16:46:25,117 - --- validate (epoch=27)-----------
2023-01-05 16:46:25,117 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:25,364 - Epoch: [27][    5/    5]    Loss 0.690313    Top1 48.664122    
2023-01-05 16:46:25,427 - ==> Top1: 48.664    Loss: 0.690

2023-01-05 16:46:25,428 - ==> Confusion:
[[375  54   0]
 [484 135   0]
 [  0   0   0]]

2023-01-05 16:46:25,429 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:25,429 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:25,439 - 

2023-01-05 16:46:25,439 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:25,931 - Epoch: [28][   10/   37]    Overall Loss 0.682028    Objective Loss 0.682028                                        LR 0.000010    Time 0.049146    
2023-01-05 16:46:26,228 - Epoch: [28][   20/   37]    Overall Loss 0.683082    Objective Loss 0.683082                                        LR 0.000010    Time 0.039399    
2023-01-05 16:46:26,523 - Epoch: [28][   30/   37]    Overall Loss 0.682820    Objective Loss 0.682820                                        LR 0.000010    Time 0.036109    
2023-01-05 16:46:26,722 - Epoch: [28][   37/   37]    Overall Loss 0.683093    Objective Loss 0.683093    Top1 52.719665    LR 0.000010    Time 0.034623    
2023-01-05 16:46:26,794 - --- validate (epoch=28)-----------
2023-01-05 16:46:26,795 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:27,034 - Epoch: [28][    5/    5]    Loss 0.685754    Top1 51.145038    
2023-01-05 16:46:27,102 - ==> Top1: 51.145    Loss: 0.686

2023-01-05 16:46:27,102 - ==> Confusion:
[[357  72   0]
 [440 179   0]
 [  0   0   0]]

2023-01-05 16:46:27,104 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:27,104 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:27,114 - 

2023-01-05 16:46:27,114 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:27,605 - Epoch: [29][   10/   37]    Overall Loss 0.684090    Objective Loss 0.684090                                        LR 0.000010    Time 0.048977    
2023-01-05 16:46:27,906 - Epoch: [29][   20/   37]    Overall Loss 0.683236    Objective Loss 0.683236                                        LR 0.000010    Time 0.039563    
2023-01-05 16:46:28,214 - Epoch: [29][   30/   37]    Overall Loss 0.683025    Objective Loss 0.683025                                        LR 0.000010    Time 0.036602    
2023-01-05 16:46:28,408 - Epoch: [29][   37/   37]    Overall Loss 0.682783    Objective Loss 0.682783    Top1 56.694561    LR 0.000010    Time 0.034914    
2023-01-05 16:46:28,499 - --- validate (epoch=29)-----------
2023-01-05 16:46:28,499 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:28,755 - Epoch: [29][    5/    5]    Loss 0.686594    Top1 55.916031    
2023-01-05 16:46:28,834 - ==> Top1: 55.916    Loss: 0.687

2023-01-05 16:46:28,834 - ==> Confusion:
[[288 141   0]
 [321 298   0]
 [  0   0   0]]

2023-01-05 16:46:28,835 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:28,835 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:28,845 - 

2023-01-05 16:46:28,845 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:29,333 - Epoch: [30][   10/   37]    Overall Loss 0.684044    Objective Loss 0.684044                                        LR 0.000010    Time 0.048714    
2023-01-05 16:46:29,627 - Epoch: [30][   20/   37]    Overall Loss 0.683456    Objective Loss 0.683456                                        LR 0.000010    Time 0.039012    
2023-01-05 16:46:29,933 - Epoch: [30][   30/   37]    Overall Loss 0.682015    Objective Loss 0.682015                                        LR 0.000010    Time 0.036202    
2023-01-05 16:46:30,127 - Epoch: [30][   37/   37]    Overall Loss 0.682312    Objective Loss 0.682312    Top1 53.765690    LR 0.000010    Time 0.034595    
2023-01-05 16:46:30,202 - --- validate (epoch=30)-----------
2023-01-05 16:46:30,202 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:30,445 - Epoch: [30][    5/    5]    Loss 0.685882    Top1 56.106870    
2023-01-05 16:46:30,518 - ==> Top1: 56.107    Loss: 0.686

2023-01-05 16:46:30,518 - ==> Confusion:
[[209 220   0]
 [240 379   0]
 [  0   0   0]]

2023-01-05 16:46:30,520 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:30,520 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:30,530 - 

2023-01-05 16:46:30,530 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:31,039 - Epoch: [31][   10/   37]    Overall Loss 0.683689    Objective Loss 0.683689                                        LR 0.000010    Time 0.050770    
2023-01-05 16:46:31,336 - Epoch: [31][   20/   37]    Overall Loss 0.682894    Objective Loss 0.682894                                        LR 0.000010    Time 0.040220    
2023-01-05 16:46:31,631 - Epoch: [31][   30/   37]    Overall Loss 0.682408    Objective Loss 0.682408                                        LR 0.000010    Time 0.036659    
2023-01-05 16:46:31,829 - Epoch: [31][   37/   37]    Overall Loss 0.681537    Objective Loss 0.681537    Top1 58.577406    LR 0.000010    Time 0.035066    
2023-01-05 16:46:31,896 - --- validate (epoch=31)-----------
2023-01-05 16:46:31,897 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:32,147 - Epoch: [31][    5/    5]    Loss 0.688937    Top1 56.870229    
2023-01-05 16:46:32,205 - ==> Top1: 56.870    Loss: 0.689

2023-01-05 16:46:32,205 - ==> Confusion:
[[295 134   0]
 [318 301   0]
 [  0   0   0]]

2023-01-05 16:46:32,207 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:32,207 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:32,216 - 

2023-01-05 16:46:32,216 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:32,707 - Epoch: [32][   10/   37]    Overall Loss 0.679297    Objective Loss 0.679297                                        LR 0.000010    Time 0.049008    
2023-01-05 16:46:32,991 - Epoch: [32][   20/   37]    Overall Loss 0.679706    Objective Loss 0.679706                                        LR 0.000010    Time 0.038567    
2023-01-05 16:46:33,279 - Epoch: [32][   30/   37]    Overall Loss 0.680997    Objective Loss 0.680997                                        LR 0.000010    Time 0.035328    
2023-01-05 16:46:33,473 - Epoch: [32][   37/   37]    Overall Loss 0.681316    Objective Loss 0.681316    Top1 55.020921    LR 0.000010    Time 0.033876    
2023-01-05 16:46:33,555 - --- validate (epoch=32)-----------
2023-01-05 16:46:33,555 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:33,802 - Epoch: [32][    5/    5]    Loss 0.686861    Top1 52.862595    
2023-01-05 16:46:33,861 - ==> Top1: 52.863    Loss: 0.687

2023-01-05 16:46:33,861 - ==> Confusion:
[[319 110   0]
 [384 235   0]
 [  0   0   0]]

2023-01-05 16:46:33,863 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:33,863 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:33,872 - 

2023-01-05 16:46:33,872 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:34,362 - Epoch: [33][   10/   37]    Overall Loss 0.679199    Objective Loss 0.679199                                        LR 0.000010    Time 0.048854    
2023-01-05 16:46:34,664 - Epoch: [33][   20/   37]    Overall Loss 0.679677    Objective Loss 0.679677                                        LR 0.000010    Time 0.039502    
2023-01-05 16:46:34,978 - Epoch: [33][   30/   37]    Overall Loss 0.680136    Objective Loss 0.680136                                        LR 0.000010    Time 0.036784    
2023-01-05 16:46:35,177 - Epoch: [33][   37/   37]    Overall Loss 0.680318    Objective Loss 0.680318    Top1 57.740586    LR 0.000010    Time 0.035206    
2023-01-05 16:46:35,263 - --- validate (epoch=33)-----------
2023-01-05 16:46:35,263 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:35,509 - Epoch: [33][    5/    5]    Loss 0.681173    Top1 58.015267    
2023-01-05 16:46:35,571 - ==> Top1: 58.015    Loss: 0.681

2023-01-05 16:46:35,571 - ==> Confusion:
[[212 217   0]
 [223 396   0]
 [  0   0   0]]

2023-01-05 16:46:35,572 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:35,572 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:35,582 - 

2023-01-05 16:46:35,583 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:36,081 - Epoch: [34][   10/   37]    Overall Loss 0.681303    Objective Loss 0.681303                                        LR 0.000010    Time 0.049771    
2023-01-05 16:46:36,387 - Epoch: [34][   20/   37]    Overall Loss 0.680772    Objective Loss 0.680772                                        LR 0.000010    Time 0.040068    
2023-01-05 16:46:36,695 - Epoch: [34][   30/   37]    Overall Loss 0.679775    Objective Loss 0.679775                                        LR 0.000010    Time 0.036971    
2023-01-05 16:46:36,895 - Epoch: [34][   37/   37]    Overall Loss 0.679456    Objective Loss 0.679456    Top1 62.133891    LR 0.000010    Time 0.035363    
2023-01-05 16:46:36,975 - --- validate (epoch=34)-----------
2023-01-05 16:46:36,975 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:37,234 - Epoch: [34][    5/    5]    Loss 0.681909    Top1 58.683206    
2023-01-05 16:46:37,295 - ==> Top1: 58.683    Loss: 0.682

2023-01-05 16:46:37,295 - ==> Confusion:
[[202 227   0]
 [206 413   0]
 [  0   0   0]]

2023-01-05 16:46:37,296 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:37,296 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:37,306 - 

2023-01-05 16:46:37,306 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:37,805 - Epoch: [35][   10/   37]    Overall Loss 0.678918    Objective Loss 0.678918                                        LR 0.000010    Time 0.049770    
2023-01-05 16:46:38,111 - Epoch: [35][   20/   37]    Overall Loss 0.678569    Objective Loss 0.678569                                        LR 0.000010    Time 0.040164    
2023-01-05 16:46:38,429 - Epoch: [35][   30/   37]    Overall Loss 0.680779    Objective Loss 0.680779                                        LR 0.000010    Time 0.037376    
2023-01-05 16:46:38,624 - Epoch: [35][   37/   37]    Overall Loss 0.680835    Objective Loss 0.680835    Top1 49.372385    LR 0.000010    Time 0.035572    
2023-01-05 16:46:38,698 - --- validate (epoch=35)-----------
2023-01-05 16:46:38,698 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:38,952 - Epoch: [35][    5/    5]    Loss 0.681262    Top1 53.625954    
2023-01-05 16:46:39,021 - ==> Top1: 53.626    Loss: 0.681

2023-01-05 16:46:39,022 - ==> Confusion:
[[330  99   0]
 [387 232   0]
 [  0   0   0]]

2023-01-05 16:46:39,023 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:39,023 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:39,033 - 

2023-01-05 16:46:39,033 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:39,557 - Epoch: [36][   10/   37]    Overall Loss 0.680510    Objective Loss 0.680510                                        LR 0.000010    Time 0.052287    
2023-01-05 16:46:39,854 - Epoch: [36][   20/   37]    Overall Loss 0.678773    Objective Loss 0.678773                                        LR 0.000010    Time 0.040959    
2023-01-05 16:46:40,166 - Epoch: [36][   30/   37]    Overall Loss 0.678669    Objective Loss 0.678669                                        LR 0.000010    Time 0.037651    
2023-01-05 16:46:40,362 - Epoch: [36][   37/   37]    Overall Loss 0.678511    Objective Loss 0.678511    Top1 57.949791    LR 0.000010    Time 0.035800    
2023-01-05 16:46:40,441 - --- validate (epoch=36)-----------
2023-01-05 16:46:40,441 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:40,680 - Epoch: [36][    5/    5]    Loss 0.686420    Top1 55.820611    
2023-01-05 16:46:40,744 - ==> Top1: 55.821    Loss: 0.686

2023-01-05 16:46:40,744 - ==> Confusion:
[[309 120   0]
 [343 276   0]
 [  0   0   0]]

2023-01-05 16:46:40,746 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:40,746 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:40,756 - 

2023-01-05 16:46:40,756 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:41,383 - Epoch: [37][   10/   37]    Overall Loss 0.677145    Objective Loss 0.677145                                        LR 0.000010    Time 0.062582    
2023-01-05 16:46:41,705 - Epoch: [37][   20/   37]    Overall Loss 0.678702    Objective Loss 0.678702                                        LR 0.000010    Time 0.047366    
2023-01-05 16:46:42,025 - Epoch: [37][   30/   37]    Overall Loss 0.678555    Objective Loss 0.678555                                        LR 0.000010    Time 0.042245    
2023-01-05 16:46:42,228 - Epoch: [37][   37/   37]    Overall Loss 0.678358    Objective Loss 0.678358    Top1 54.393305    LR 0.000010    Time 0.039727    
2023-01-05 16:46:42,291 - --- validate (epoch=37)-----------
2023-01-05 16:46:42,291 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:42,555 - Epoch: [37][    5/    5]    Loss 0.682979    Top1 51.812977    
2023-01-05 16:46:42,620 - ==> Top1: 51.813    Loss: 0.683

2023-01-05 16:46:42,620 - ==> Confusion:
[[351  78   0]
 [427 192   0]
 [  0   0   0]]

2023-01-05 16:46:42,622 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:42,622 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:42,632 - 

2023-01-05 16:46:42,632 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:43,142 - Epoch: [38][   10/   37]    Overall Loss 0.679649    Objective Loss 0.679649                                        LR 0.000010    Time 0.050856    
2023-01-05 16:46:43,439 - Epoch: [38][   20/   37]    Overall Loss 0.678279    Objective Loss 0.678279                                        LR 0.000010    Time 0.040281    
2023-01-05 16:46:43,753 - Epoch: [38][   30/   37]    Overall Loss 0.678288    Objective Loss 0.678288                                        LR 0.000010    Time 0.037211    
2023-01-05 16:46:43,953 - Epoch: [38][   37/   37]    Overall Loss 0.677615    Objective Loss 0.677615    Top1 58.577406    LR 0.000010    Time 0.035569    
2023-01-05 16:46:44,015 - --- validate (epoch=38)-----------
2023-01-05 16:46:44,015 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:44,270 - Epoch: [38][    5/    5]    Loss 0.681108    Top1 57.538168    
2023-01-05 16:46:44,328 - ==> Top1: 57.538    Loss: 0.681

2023-01-05 16:46:44,328 - ==> Confusion:
[[309 120   0]
 [325 294   0]
 [  0   0   0]]

2023-01-05 16:46:44,330 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:44,330 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:44,340 - 

2023-01-05 16:46:44,340 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:44,834 - Epoch: [39][   10/   37]    Overall Loss 0.676644    Objective Loss 0.676644                                        LR 0.000010    Time 0.049348    
2023-01-05 16:46:45,137 - Epoch: [39][   20/   37]    Overall Loss 0.677447    Objective Loss 0.677447                                        LR 0.000010    Time 0.039790    
2023-01-05 16:46:45,431 - Epoch: [39][   30/   37]    Overall Loss 0.677039    Objective Loss 0.677039                                        LR 0.000010    Time 0.036318    
2023-01-05 16:46:45,626 - Epoch: [39][   37/   37]    Overall Loss 0.677294    Objective Loss 0.677294    Top1 58.368201    LR 0.000010    Time 0.034710    
2023-01-05 16:46:45,688 - --- validate (epoch=39)-----------
2023-01-05 16:46:45,688 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:45,938 - Epoch: [39][    5/    5]    Loss 0.684438    Top1 59.064885    
2023-01-05 16:46:46,009 - ==> Top1: 59.065    Loss: 0.684

2023-01-05 16:46:46,009 - ==> Confusion:
[[187 242   0]
 [187 432   0]
 [  0   0   0]]

2023-01-05 16:46:46,011 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:46,011 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:46,020 - 

2023-01-05 16:46:46,020 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:46,502 - Epoch: [40][   10/   37]    Overall Loss 0.675085    Objective Loss 0.675085                                        LR 0.000006    Time 0.048099    
2023-01-05 16:46:46,795 - Epoch: [40][   20/   37]    Overall Loss 0.676856    Objective Loss 0.676856                                        LR 0.000006    Time 0.038687    
2023-01-05 16:46:47,112 - Epoch: [40][   30/   37]    Overall Loss 0.675906    Objective Loss 0.675906                                        LR 0.000006    Time 0.036332    
2023-01-05 16:46:47,315 - Epoch: [40][   37/   37]    Overall Loss 0.675773    Objective Loss 0.675773    Top1 62.552301    LR 0.000006    Time 0.034949    
2023-01-05 16:46:47,390 - --- validate (epoch=40)-----------
2023-01-05 16:46:47,390 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:47,641 - Epoch: [40][    5/    5]    Loss 0.677061    Top1 61.068702    
2023-01-05 16:46:47,710 - ==> Top1: 61.069    Loss: 0.677

2023-01-05 16:46:47,710 - ==> Confusion:
[[ 83 346   0]
 [ 62 557   0]
 [  0   0   0]]

2023-01-05 16:46:47,711 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:47,711 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:47,721 - 

2023-01-05 16:46:47,722 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:48,239 - Epoch: [41][   10/   37]    Overall Loss 0.672575    Objective Loss 0.672575                                        LR 0.000006    Time 0.051601    
2023-01-05 16:46:48,542 - Epoch: [41][   20/   37]    Overall Loss 0.675448    Objective Loss 0.675448                                        LR 0.000006    Time 0.040936    
2023-01-05 16:46:48,847 - Epoch: [41][   30/   37]    Overall Loss 0.675041    Objective Loss 0.675041                                        LR 0.000006    Time 0.037441    
2023-01-05 16:46:49,041 - Epoch: [41][   37/   37]    Overall Loss 0.675205    Objective Loss 0.675205    Top1 55.439331    LR 0.000006    Time 0.035594    
2023-01-05 16:46:49,112 - --- validate (epoch=41)-----------
2023-01-05 16:46:49,113 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:49,372 - Epoch: [41][    5/    5]    Loss 0.682759    Top1 53.530534    
2023-01-05 16:46:49,441 - ==> Top1: 53.531    Loss: 0.683

2023-01-05 16:46:49,442 - ==> Confusion:
[[348  81   0]
 [406 213   0]
 [  0   0   0]]

2023-01-05 16:46:49,443 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:49,443 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:49,455 - 

2023-01-05 16:46:49,455 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:49,949 - Epoch: [42][   10/   37]    Overall Loss 0.675945    Objective Loss 0.675945                                        LR 0.000006    Time 0.049296    
2023-01-05 16:46:50,239 - Epoch: [42][   20/   37]    Overall Loss 0.676429    Objective Loss 0.676429                                        LR 0.000006    Time 0.039110    
2023-01-05 16:46:50,537 - Epoch: [42][   30/   37]    Overall Loss 0.675843    Objective Loss 0.675843                                        LR 0.000006    Time 0.036005    
2023-01-05 16:46:50,734 - Epoch: [42][   37/   37]    Overall Loss 0.675360    Objective Loss 0.675360    Top1 56.485356    LR 0.000006    Time 0.034518    
2023-01-05 16:46:50,810 - --- validate (epoch=42)-----------
2023-01-05 16:46:50,811 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:51,060 - Epoch: [42][    5/    5]    Loss 0.675308    Top1 55.820611    
2023-01-05 16:46:51,127 - ==> Top1: 55.821    Loss: 0.675

2023-01-05 16:46:51,127 - ==> Confusion:
[[325 104   0]
 [359 260   0]
 [  0   0   0]]

2023-01-05 16:46:51,128 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:51,128 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:51,138 - 

2023-01-05 16:46:51,138 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:51,641 - Epoch: [43][   10/   37]    Overall Loss 0.673993    Objective Loss 0.673993                                        LR 0.000006    Time 0.050182    
2023-01-05 16:46:51,945 - Epoch: [43][   20/   37]    Overall Loss 0.675223    Objective Loss 0.675223                                        LR 0.000006    Time 0.040289    
2023-01-05 16:46:52,255 - Epoch: [43][   30/   37]    Overall Loss 0.674880    Objective Loss 0.674880                                        LR 0.000006    Time 0.037163    
2023-01-05 16:46:52,457 - Epoch: [43][   37/   37]    Overall Loss 0.674811    Objective Loss 0.674811    Top1 60.251046    LR 0.000006    Time 0.035604    
2023-01-05 16:46:52,535 - --- validate (epoch=43)-----------
2023-01-05 16:46:52,535 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:52,800 - Epoch: [43][    5/    5]    Loss 0.684202    Top1 57.156489    
2023-01-05 16:46:52,871 - ==> Top1: 57.156    Loss: 0.684

2023-01-05 16:46:52,871 - ==> Confusion:
[[311 118   0]
 [331 288   0]
 [  0   0   0]]

2023-01-05 16:46:52,872 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:52,872 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:52,882 - 

2023-01-05 16:46:52,882 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:53,384 - Epoch: [44][   10/   37]    Overall Loss 0.668840    Objective Loss 0.668840                                        LR 0.000006    Time 0.050085    
2023-01-05 16:46:53,686 - Epoch: [44][   20/   37]    Overall Loss 0.674139    Objective Loss 0.674139                                        LR 0.000006    Time 0.040145    
2023-01-05 16:46:54,004 - Epoch: [44][   30/   37]    Overall Loss 0.673986    Objective Loss 0.673986                                        LR 0.000006    Time 0.037334    
2023-01-05 16:46:54,197 - Epoch: [44][   37/   37]    Overall Loss 0.674338    Objective Loss 0.674338    Top1 59.414226    LR 0.000006    Time 0.035495    
2023-01-05 16:46:54,270 - --- validate (epoch=44)-----------
2023-01-05 16:46:54,270 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:54,518 - Epoch: [44][    5/    5]    Loss 0.691082    Top1 52.003817    
2023-01-05 16:46:54,579 - ==> Top1: 52.004    Loss: 0.691

2023-01-05 16:46:54,580 - ==> Confusion:
[[362  67   0]
 [436 183   0]
 [  0   0   0]]

2023-01-05 16:46:54,581 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:54,581 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:54,591 - 

2023-01-05 16:46:54,591 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:55,056 - Epoch: [45][   10/   37]    Overall Loss 0.676397    Objective Loss 0.676397                                        LR 0.000006    Time 0.046466    
2023-01-05 16:46:55,344 - Epoch: [45][   20/   37]    Overall Loss 0.674363    Objective Loss 0.674363                                        LR 0.000006    Time 0.037580    
2023-01-05 16:46:55,638 - Epoch: [45][   30/   37]    Overall Loss 0.673202    Objective Loss 0.673202                                        LR 0.000006    Time 0.034838    
2023-01-05 16:46:55,831 - Epoch: [45][   37/   37]    Overall Loss 0.673760    Objective Loss 0.673760    Top1 59.832636    LR 0.000006    Time 0.033464    
2023-01-05 16:46:55,914 - --- validate (epoch=45)-----------
2023-01-05 16:46:55,914 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:56,160 - Epoch: [45][    5/    5]    Loss 0.675684    Top1 59.446565    
2023-01-05 16:46:56,224 - ==> Top1: 59.447    Loss: 0.676

2023-01-05 16:46:56,224 - ==> Confusion:
[[224 205   0]
 [220 399   0]
 [  0   0   0]]

2023-01-05 16:46:56,226 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:56,226 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:56,235 - 

2023-01-05 16:46:56,236 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:56,751 - Epoch: [46][   10/   37]    Overall Loss 0.676856    Objective Loss 0.676856                                        LR 0.000006    Time 0.051427    
2023-01-05 16:46:57,051 - Epoch: [46][   20/   37]    Overall Loss 0.673788    Objective Loss 0.673788                                        LR 0.000006    Time 0.040682    
2023-01-05 16:46:57,354 - Epoch: [46][   30/   37]    Overall Loss 0.674748    Objective Loss 0.674748                                        LR 0.000006    Time 0.037229    
2023-01-05 16:46:57,550 - Epoch: [46][   37/   37]    Overall Loss 0.674000    Objective Loss 0.674000    Top1 59.623431    LR 0.000006    Time 0.035461    
2023-01-05 16:46:57,619 - --- validate (epoch=46)-----------
2023-01-05 16:46:57,619 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:57,858 - Epoch: [46][    5/    5]    Loss 0.679640    Top1 58.110687    
2023-01-05 16:46:57,924 - ==> Top1: 58.111    Loss: 0.680

2023-01-05 16:46:57,924 - ==> Confusion:
[[299 130   0]
 [309 310   0]
 [  0   0   0]]

2023-01-05 16:46:57,925 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:57,925 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:57,935 - 

2023-01-05 16:46:57,935 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:46:58,449 - Epoch: [47][   10/   37]    Overall Loss 0.674220    Objective Loss 0.674220                                        LR 0.000006    Time 0.051344    
2023-01-05 16:46:58,758 - Epoch: [47][   20/   37]    Overall Loss 0.673341    Objective Loss 0.673341                                        LR 0.000006    Time 0.041073    
2023-01-05 16:46:59,068 - Epoch: [47][   30/   37]    Overall Loss 0.672822    Objective Loss 0.672822                                        LR 0.000006    Time 0.037688    
2023-01-05 16:46:59,265 - Epoch: [47][   37/   37]    Overall Loss 0.671790    Objective Loss 0.671790    Top1 60.041841    LR 0.000006    Time 0.035881    
2023-01-05 16:46:59,337 - --- validate (epoch=47)-----------
2023-01-05 16:46:59,337 - 1048 samples (256 per mini-batch)
2023-01-05 16:46:59,579 - Epoch: [47][    5/    5]    Loss 0.675700    Top1 56.106870    
2023-01-05 16:46:59,636 - ==> Top1: 56.107    Loss: 0.676

2023-01-05 16:46:59,637 - ==> Confusion:
[[334  95   0]
 [365 254   0]
 [  0   0   0]]

2023-01-05 16:46:59,638 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:46:59,639 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:46:59,648 - 

2023-01-05 16:46:59,648 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:00,119 - Epoch: [48][   10/   37]    Overall Loss 0.672002    Objective Loss 0.672002                                        LR 0.000006    Time 0.046944    
2023-01-05 16:47:00,414 - Epoch: [48][   20/   37]    Overall Loss 0.673910    Objective Loss 0.673910                                        LR 0.000006    Time 0.038221    
2023-01-05 16:47:00,723 - Epoch: [48][   30/   37]    Overall Loss 0.672944    Objective Loss 0.672944                                        LR 0.000006    Time 0.035749    
2023-01-05 16:47:00,921 - Epoch: [48][   37/   37]    Overall Loss 0.671661    Objective Loss 0.671661    Top1 63.179916    LR 0.000006    Time 0.034339    
2023-01-05 16:47:00,990 - --- validate (epoch=48)-----------
2023-01-05 16:47:00,990 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:01,239 - Epoch: [48][    5/    5]    Loss 0.678539    Top1 56.870229    
2023-01-05 16:47:01,314 - ==> Top1: 56.870    Loss: 0.679

2023-01-05 16:47:01,314 - ==> Confusion:
[[319 110   0]
 [342 277   0]
 [  0   0   0]]

2023-01-05 16:47:01,316 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:01,316 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:01,325 - 

2023-01-05 16:47:01,325 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:01,809 - Epoch: [49][   10/   37]    Overall Loss 0.674524    Objective Loss 0.674524                                        LR 0.000006    Time 0.048295    
2023-01-05 16:47:02,115 - Epoch: [49][   20/   37]    Overall Loss 0.672491    Objective Loss 0.672491                                        LR 0.000006    Time 0.039411    
2023-01-05 16:47:02,427 - Epoch: [49][   30/   37]    Overall Loss 0.670530    Objective Loss 0.670530                                        LR 0.000006    Time 0.036649    
2023-01-05 16:47:02,622 - Epoch: [49][   37/   37]    Overall Loss 0.670944    Objective Loss 0.670944    Top1 58.786611    LR 0.000006    Time 0.034984    
2023-01-05 16:47:02,685 - --- validate (epoch=49)-----------
2023-01-05 16:47:02,685 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:02,934 - Epoch: [49][    5/    5]    Loss 0.677542    Top1 59.446565    
2023-01-05 16:47:03,001 - ==> Top1: 59.447    Loss: 0.678

2023-01-05 16:47:03,002 - ==> Confusion:
[[294 135   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:47:03,003 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:03,003 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:03,013 - 

2023-01-05 16:47:03,013 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:03,502 - Epoch: [50][   10/   37]    Overall Loss 0.670485    Objective Loss 0.670485                                        LR 0.000006    Time 0.048800    
2023-01-05 16:47:03,799 - Epoch: [50][   20/   37]    Overall Loss 0.670860    Objective Loss 0.670860                                        LR 0.000006    Time 0.039084    
2023-01-05 16:47:04,109 - Epoch: [50][   30/   37]    Overall Loss 0.671651    Objective Loss 0.671651                                        LR 0.000006    Time 0.036379    
2023-01-05 16:47:04,304 - Epoch: [50][   37/   37]    Overall Loss 0.670506    Objective Loss 0.670506    Top1 62.552301    LR 0.000006    Time 0.034739    
2023-01-05 16:47:04,367 - --- validate (epoch=50)-----------
2023-01-05 16:47:04,368 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:04,626 - Epoch: [50][    5/    5]    Loss 0.675406    Top1 55.629771    
2023-01-05 16:47:04,693 - ==> Top1: 55.630    Loss: 0.675

2023-01-05 16:47:04,693 - ==> Confusion:
[[340  89   0]
 [376 243   0]
 [  0   0   0]]

2023-01-05 16:47:04,695 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:04,695 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:04,704 - 

2023-01-05 16:47:04,704 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:05,192 - Epoch: [51][   10/   37]    Overall Loss 0.668619    Objective Loss 0.668619                                        LR 0.000006    Time 0.048657    
2023-01-05 16:47:05,481 - Epoch: [51][   20/   37]    Overall Loss 0.671572    Objective Loss 0.671572                                        LR 0.000006    Time 0.038776    
2023-01-05 16:47:05,786 - Epoch: [51][   30/   37]    Overall Loss 0.671527    Objective Loss 0.671527                                        LR 0.000006    Time 0.035989    
2023-01-05 16:47:05,980 - Epoch: [51][   37/   37]    Overall Loss 0.670681    Objective Loss 0.670681    Top1 60.041841    LR 0.000006    Time 0.034434    
2023-01-05 16:47:06,052 - --- validate (epoch=51)-----------
2023-01-05 16:47:06,052 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:06,294 - Epoch: [51][    5/    5]    Loss 0.672804    Top1 60.019084    
2023-01-05 16:47:06,356 - ==> Top1: 60.019    Loss: 0.673

2023-01-05 16:47:06,356 - ==> Confusion:
[[289 140   0]
 [279 340   0]
 [  0   0   0]]

2023-01-05 16:47:06,357 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:06,358 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:06,367 - 

2023-01-05 16:47:06,367 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:06,846 - Epoch: [52][   10/   37]    Overall Loss 0.671678    Objective Loss 0.671678                                        LR 0.000006    Time 0.047747    
2023-01-05 16:47:07,136 - Epoch: [52][   20/   37]    Overall Loss 0.670516    Objective Loss 0.670516                                        LR 0.000006    Time 0.038379    
2023-01-05 16:47:07,435 - Epoch: [52][   30/   37]    Overall Loss 0.669732    Objective Loss 0.669732                                        LR 0.000006    Time 0.035531    
2023-01-05 16:47:07,630 - Epoch: [52][   37/   37]    Overall Loss 0.669312    Objective Loss 0.669312    Top1 61.087866    LR 0.000006    Time 0.034072    
2023-01-05 16:47:07,702 - --- validate (epoch=52)-----------
2023-01-05 16:47:07,702 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:07,941 - Epoch: [52][    5/    5]    Loss 0.679730    Top1 60.400763    
2023-01-05 16:47:08,006 - ==> Top1: 60.401    Loss: 0.680

2023-01-05 16:47:08,007 - ==> Confusion:
[[293 136   0]
 [279 340   0]
 [  0   0   0]]

2023-01-05 16:47:08,008 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:08,008 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:08,018 - 

2023-01-05 16:47:08,018 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:08,510 - Epoch: [53][   10/   37]    Overall Loss 0.669229    Objective Loss 0.669229                                        LR 0.000006    Time 0.049152    
2023-01-05 16:47:08,803 - Epoch: [53][   20/   37]    Overall Loss 0.667581    Objective Loss 0.667581                                        LR 0.000006    Time 0.039197    
2023-01-05 16:47:09,101 - Epoch: [53][   30/   37]    Overall Loss 0.669082    Objective Loss 0.669082                                        LR 0.000006    Time 0.036045    
2023-01-05 16:47:09,293 - Epoch: [53][   37/   37]    Overall Loss 0.669111    Objective Loss 0.669111    Top1 60.669456    LR 0.000006    Time 0.034421    
2023-01-05 16:47:09,365 - --- validate (epoch=53)-----------
2023-01-05 16:47:09,366 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:09,608 - Epoch: [53][    5/    5]    Loss 0.670145    Top1 59.637405    
2023-01-05 16:47:09,677 - ==> Top1: 59.637    Loss: 0.670

2023-01-05 16:47:09,678 - ==> Confusion:
[[300 129   0]
 [294 325   0]
 [  0   0   0]]

2023-01-05 16:47:09,679 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:09,679 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:09,689 - 

2023-01-05 16:47:09,689 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:10,313 - Epoch: [54][   10/   37]    Overall Loss 0.669780    Objective Loss 0.669780                                        LR 0.000006    Time 0.062305    
2023-01-05 16:47:10,601 - Epoch: [54][   20/   37]    Overall Loss 0.666708    Objective Loss 0.666708                                        LR 0.000006    Time 0.045560    
2023-01-05 16:47:10,902 - Epoch: [54][   30/   37]    Overall Loss 0.667826    Objective Loss 0.667826                                        LR 0.000006    Time 0.040372    
2023-01-05 16:47:11,095 - Epoch: [54][   37/   37]    Overall Loss 0.668588    Objective Loss 0.668588    Top1 57.531381    LR 0.000006    Time 0.037966    
2023-01-05 16:47:11,169 - --- validate (epoch=54)-----------
2023-01-05 16:47:11,169 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:11,417 - Epoch: [54][    5/    5]    Loss 0.679530    Top1 54.770992    
2023-01-05 16:47:11,485 - ==> Top1: 54.771    Loss: 0.680

2023-01-05 16:47:11,485 - ==> Confusion:
[[342  87   0]
 [387 232   0]
 [  0   0   0]]

2023-01-05 16:47:11,486 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:11,486 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:11,496 - 

2023-01-05 16:47:11,496 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:11,992 - Epoch: [55][   10/   37]    Overall Loss 0.666549    Objective Loss 0.666549                                        LR 0.000006    Time 0.049539    
2023-01-05 16:47:12,286 - Epoch: [55][   20/   37]    Overall Loss 0.666413    Objective Loss 0.666413                                        LR 0.000006    Time 0.039330    
2023-01-05 16:47:12,588 - Epoch: [55][   30/   37]    Overall Loss 0.668023    Objective Loss 0.668023                                        LR 0.000006    Time 0.036209    
2023-01-05 16:47:12,783 - Epoch: [55][   37/   37]    Overall Loss 0.668083    Objective Loss 0.668083    Top1 57.531381    LR 0.000006    Time 0.034626    
2023-01-05 16:47:12,856 - --- validate (epoch=55)-----------
2023-01-05 16:47:12,856 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:13,097 - Epoch: [55][    5/    5]    Loss 0.668895    Top1 58.301527    
2023-01-05 16:47:13,164 - ==> Top1: 58.302    Loss: 0.669

2023-01-05 16:47:13,164 - ==> Confusion:
[[324 105   0]
 [332 287   0]
 [  0   0   0]]

2023-01-05 16:47:13,166 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:13,166 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:13,176 - 

2023-01-05 16:47:13,176 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:13,668 - Epoch: [56][   10/   37]    Overall Loss 0.667007    Objective Loss 0.667007                                        LR 0.000006    Time 0.049155    
2023-01-05 16:47:13,958 - Epoch: [56][   20/   37]    Overall Loss 0.666384    Objective Loss 0.666384                                        LR 0.000006    Time 0.039021    
2023-01-05 16:47:14,266 - Epoch: [56][   30/   37]    Overall Loss 0.666797    Objective Loss 0.666797                                        LR 0.000006    Time 0.036268    
2023-01-05 16:47:14,462 - Epoch: [56][   37/   37]    Overall Loss 0.666854    Objective Loss 0.666854    Top1 61.715481    LR 0.000006    Time 0.034701    
2023-01-05 16:47:14,537 - --- validate (epoch=56)-----------
2023-01-05 16:47:14,537 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:14,782 - Epoch: [56][    5/    5]    Loss 0.680268    Top1 60.400763    
2023-01-05 16:47:14,847 - ==> Top1: 60.401    Loss: 0.680

2023-01-05 16:47:14,847 - ==> Confusion:
[[244 185   0]
 [230 389   0]
 [  0   0   0]]

2023-01-05 16:47:14,848 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:14,849 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:14,858 - 

2023-01-05 16:47:14,858 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:15,370 - Epoch: [57][   10/   37]    Overall Loss 0.664554    Objective Loss 0.664554                                        LR 0.000006    Time 0.051076    
2023-01-05 16:47:15,689 - Epoch: [57][   20/   37]    Overall Loss 0.667309    Objective Loss 0.667309                                        LR 0.000006    Time 0.041468    
2023-01-05 16:47:16,001 - Epoch: [57][   30/   37]    Overall Loss 0.666965    Objective Loss 0.666965                                        LR 0.000006    Time 0.038031    
2023-01-05 16:47:16,192 - Epoch: [57][   37/   37]    Overall Loss 0.667021    Objective Loss 0.667021    Top1 60.251046    LR 0.000006    Time 0.036007    
2023-01-05 16:47:16,258 - --- validate (epoch=57)-----------
2023-01-05 16:47:16,259 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:16,505 - Epoch: [57][    5/    5]    Loss 0.670382    Top1 60.782443    
2023-01-05 16:47:16,574 - ==> Top1: 60.782    Loss: 0.670

2023-01-05 16:47:16,575 - ==> Confusion:
[[267 162   0]
 [249 370   0]
 [  0   0   0]]

2023-01-05 16:47:16,576 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:16,576 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:16,586 - 

2023-01-05 16:47:16,586 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:17,083 - Epoch: [58][   10/   37]    Overall Loss 0.666488    Objective Loss 0.666488                                        LR 0.000006    Time 0.049611    
2023-01-05 16:47:17,403 - Epoch: [58][   20/   37]    Overall Loss 0.664922    Objective Loss 0.664922                                        LR 0.000006    Time 0.040781    
2023-01-05 16:47:17,722 - Epoch: [58][   30/   37]    Overall Loss 0.666762    Objective Loss 0.666762                                        LR 0.000006    Time 0.037815    
2023-01-05 16:47:17,918 - Epoch: [58][   37/   37]    Overall Loss 0.666721    Objective Loss 0.666721    Top1 59.414226    LR 0.000006    Time 0.035951    
2023-01-05 16:47:17,988 - --- validate (epoch=58)-----------
2023-01-05 16:47:17,988 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:18,233 - Epoch: [58][    5/    5]    Loss 0.679053    Top1 60.209924    
2023-01-05 16:47:18,302 - ==> Top1: 60.210    Loss: 0.679

2023-01-05 16:47:18,302 - ==> Confusion:
[[310 119   0]
 [298 321   0]
 [  0   0   0]]

2023-01-05 16:47:18,304 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:18,304 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:18,313 - 

2023-01-05 16:47:18,313 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:18,792 - Epoch: [59][   10/   37]    Overall Loss 0.663945    Objective Loss 0.663945                                        LR 0.000006    Time 0.047745    
2023-01-05 16:47:19,094 - Epoch: [59][   20/   37]    Overall Loss 0.665992    Objective Loss 0.665992                                        LR 0.000006    Time 0.038791    
2023-01-05 16:47:19,404 - Epoch: [59][   30/   37]    Overall Loss 0.665853    Objective Loss 0.665853                                        LR 0.000006    Time 0.036184    
2023-01-05 16:47:19,600 - Epoch: [59][   37/   37]    Overall Loss 0.665848    Objective Loss 0.665848    Top1 59.832636    LR 0.000006    Time 0.034636    
2023-01-05 16:47:19,672 - --- validate (epoch=59)-----------
2023-01-05 16:47:19,672 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:19,909 - Epoch: [59][    5/    5]    Loss 0.671520    Top1 56.488550    
2023-01-05 16:47:19,980 - ==> Top1: 56.489    Loss: 0.672

2023-01-05 16:47:19,980 - ==> Confusion:
[[341  88   0]
 [368 251   0]
 [  0   0   0]]

2023-01-05 16:47:19,981 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:19,981 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:19,991 - 

2023-01-05 16:47:19,991 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:20,484 - Epoch: [60][   10/   37]    Overall Loss 0.665349    Objective Loss 0.665349                                        LR 0.000006    Time 0.049181    
2023-01-05 16:47:20,776 - Epoch: [60][   20/   37]    Overall Loss 0.665549    Objective Loss 0.665549                                        LR 0.000006    Time 0.039094    
2023-01-05 16:47:21,073 - Epoch: [60][   30/   37]    Overall Loss 0.667170    Objective Loss 0.667170                                        LR 0.000006    Time 0.035940    
2023-01-05 16:47:21,268 - Epoch: [60][   37/   37]    Overall Loss 0.666686    Objective Loss 0.666686    Top1 57.949791    LR 0.000006    Time 0.034429    
2023-01-05 16:47:21,350 - --- validate (epoch=60)-----------
2023-01-05 16:47:21,350 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:21,599 - Epoch: [60][    5/    5]    Loss 0.664783    Top1 60.114504    
2023-01-05 16:47:21,660 - ==> Top1: 60.115    Loss: 0.665

2023-01-05 16:47:21,660 - ==> Confusion:
[[297 132   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:47:21,662 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:21,662 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:21,671 - 

2023-01-05 16:47:21,671 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:22,165 - Epoch: [61][   10/   37]    Overall Loss 0.663220    Objective Loss 0.663220                                        LR 0.000006    Time 0.049233    
2023-01-05 16:47:22,451 - Epoch: [61][   20/   37]    Overall Loss 0.663628    Objective Loss 0.663628                                        LR 0.000006    Time 0.038828    
2023-01-05 16:47:22,746 - Epoch: [61][   30/   37]    Overall Loss 0.664396    Objective Loss 0.664396                                        LR 0.000006    Time 0.035719    
2023-01-05 16:47:22,934 - Epoch: [61][   37/   37]    Overall Loss 0.664332    Objective Loss 0.664332    Top1 62.133891    LR 0.000006    Time 0.034028    
2023-01-05 16:47:23,002 - --- validate (epoch=61)-----------
2023-01-05 16:47:23,002 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:23,256 - Epoch: [61][    5/    5]    Loss 0.669452    Top1 60.782443    
2023-01-05 16:47:23,326 - ==> Top1: 60.782    Loss: 0.669

2023-01-05 16:47:23,326 - ==> Confusion:
[[293 136   0]
 [275 344   0]
 [  0   0   0]]

2023-01-05 16:47:23,328 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:23,328 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:23,337 - 

2023-01-05 16:47:23,338 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:23,823 - Epoch: [62][   10/   37]    Overall Loss 0.665338    Objective Loss 0.665338                                        LR 0.000006    Time 0.048506    
2023-01-05 16:47:24,113 - Epoch: [62][   20/   37]    Overall Loss 0.664038    Objective Loss 0.664038                                        LR 0.000006    Time 0.038695    
2023-01-05 16:47:24,412 - Epoch: [62][   30/   37]    Overall Loss 0.663657    Objective Loss 0.663657                                        LR 0.000006    Time 0.035753    
2023-01-05 16:47:24,608 - Epoch: [62][   37/   37]    Overall Loss 0.664025    Objective Loss 0.664025    Top1 62.133891    LR 0.000006    Time 0.034278    
2023-01-05 16:47:24,680 - --- validate (epoch=62)-----------
2023-01-05 16:47:24,680 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:24,918 - Epoch: [62][    5/    5]    Loss 0.680597    Top1 61.832061    
2023-01-05 16:47:24,980 - ==> Top1: 61.832    Loss: 0.681

2023-01-05 16:47:24,980 - ==> Confusion:
[[165 264   0]
 [136 483   0]
 [  0   0   0]]

2023-01-05 16:47:24,981 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:24,981 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:24,991 - 

2023-01-05 16:47:24,991 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:25,499 - Epoch: [63][   10/   37]    Overall Loss 0.670571    Objective Loss 0.670571                                        LR 0.000006    Time 0.050772    
2023-01-05 16:47:25,809 - Epoch: [63][   20/   37]    Overall Loss 0.667291    Objective Loss 0.667291                                        LR 0.000006    Time 0.040696    
2023-01-05 16:47:26,115 - Epoch: [63][   30/   37]    Overall Loss 0.664823    Objective Loss 0.664823                                        LR 0.000006    Time 0.037300    
2023-01-05 16:47:26,310 - Epoch: [63][   37/   37]    Overall Loss 0.664998    Objective Loss 0.664998    Top1 58.786611    LR 0.000006    Time 0.035502    
2023-01-05 16:47:26,379 - --- validate (epoch=63)-----------
2023-01-05 16:47:26,380 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:26,622 - Epoch: [63][    5/    5]    Loss 0.682309    Top1 59.923664    
2023-01-05 16:47:26,687 - ==> Top1: 59.924    Loss: 0.682

2023-01-05 16:47:26,687 - ==> Confusion:
[[318 111   0]
 [309 310   0]
 [  0   0   0]]

2023-01-05 16:47:26,688 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:26,688 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:26,699 - 

2023-01-05 16:47:26,699 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:27,179 - Epoch: [64][   10/   37]    Overall Loss 0.669027    Objective Loss 0.669027                                        LR 0.000006    Time 0.047916    
2023-01-05 16:47:27,455 - Epoch: [64][   20/   37]    Overall Loss 0.671798    Objective Loss 0.671798                                        LR 0.000006    Time 0.037758    
2023-01-05 16:47:27,756 - Epoch: [64][   30/   37]    Overall Loss 0.672794    Objective Loss 0.672794                                        LR 0.000006    Time 0.035081    
2023-01-05 16:47:27,947 - Epoch: [64][   37/   37]    Overall Loss 0.673585    Objective Loss 0.673585    Top1 59.414226    LR 0.000006    Time 0.033602    
2023-01-05 16:47:28,025 - --- validate (epoch=64)-----------
2023-01-05 16:47:28,025 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:28,280 - Epoch: [64][    5/    5]    Loss 0.679363    Top1 60.114504    
2023-01-05 16:47:28,346 - ==> Top1: 60.115    Loss: 0.679

2023-01-05 16:47:28,346 - ==> Confusion:
[[276 153   0]
 [265 354   0]
 [  0   0   0]]

2023-01-05 16:47:28,348 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:28,348 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:28,357 - 

2023-01-05 16:47:28,357 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:28,856 - Epoch: [65][   10/   37]    Overall Loss 0.673560    Objective Loss 0.673560                                        LR 0.000006    Time 0.049726    
2023-01-05 16:47:29,162 - Epoch: [65][   20/   37]    Overall Loss 0.674153    Objective Loss 0.674153                                        LR 0.000006    Time 0.040150    
2023-01-05 16:47:29,464 - Epoch: [65][   30/   37]    Overall Loss 0.674060    Objective Loss 0.674060                                        LR 0.000006    Time 0.036829    
2023-01-05 16:47:29,658 - Epoch: [65][   37/   37]    Overall Loss 0.674504    Objective Loss 0.674504    Top1 53.765690    LR 0.000006    Time 0.035099    
2023-01-05 16:47:29,746 - --- validate (epoch=65)-----------
2023-01-05 16:47:29,746 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:29,993 - Epoch: [65][    5/    5]    Loss 0.673660    Top1 59.351145    
2023-01-05 16:47:30,069 - ==> Top1: 59.351    Loss: 0.674

2023-01-05 16:47:30,070 - ==> Confusion:
[[325 104   0]
 [322 297   0]
 [  0   0   0]]

2023-01-05 16:47:30,071 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:30,071 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:30,081 - 

2023-01-05 16:47:30,081 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:30,562 - Epoch: [66][   10/   37]    Overall Loss 0.675598    Objective Loss 0.675598                                        LR 0.000006    Time 0.048009    
2023-01-05 16:47:30,845 - Epoch: [66][   20/   37]    Overall Loss 0.674291    Objective Loss 0.674291                                        LR 0.000006    Time 0.038110    
2023-01-05 16:47:31,133 - Epoch: [66][   30/   37]    Overall Loss 0.674569    Objective Loss 0.674569                                        LR 0.000006    Time 0.035015    
2023-01-05 16:47:31,322 - Epoch: [66][   37/   37]    Overall Loss 0.674238    Objective Loss 0.674238    Top1 62.761506    LR 0.000006    Time 0.033473    
2023-01-05 16:47:31,397 - --- validate (epoch=66)-----------
2023-01-05 16:47:31,397 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:31,642 - Epoch: [66][    5/    5]    Loss 0.679037    Top1 60.305344    
2023-01-05 16:47:31,706 - ==> Top1: 60.305    Loss: 0.679

2023-01-05 16:47:31,706 - ==> Confusion:
[[263 166   0]
 [250 369   0]
 [  0   0   0]]

2023-01-05 16:47:31,707 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:31,708 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:31,718 - 

2023-01-05 16:47:31,718 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:32,208 - Epoch: [67][   10/   37]    Overall Loss 0.675006    Objective Loss 0.675006                                        LR 0.000006    Time 0.048921    
2023-01-05 16:47:32,487 - Epoch: [67][   20/   37]    Overall Loss 0.673155    Objective Loss 0.673155                                        LR 0.000006    Time 0.038393    
2023-01-05 16:47:32,782 - Epoch: [67][   30/   37]    Overall Loss 0.673284    Objective Loss 0.673284                                        LR 0.000006    Time 0.035434    
2023-01-05 16:47:32,976 - Epoch: [67][   37/   37]    Overall Loss 0.673445    Objective Loss 0.673445    Top1 59.832636    LR 0.000006    Time 0.033969    
2023-01-05 16:47:33,055 - --- validate (epoch=67)-----------
2023-01-05 16:47:33,056 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:33,303 - Epoch: [67][    5/    5]    Loss 0.677492    Top1 59.637405    
2023-01-05 16:47:33,373 - ==> Top1: 59.637    Loss: 0.677

2023-01-05 16:47:33,373 - ==> Confusion:
[[270 159   0]
 [264 355   0]
 [  0   0   0]]

2023-01-05 16:47:33,374 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:33,374 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:33,385 - 

2023-01-05 16:47:33,385 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:33,860 - Epoch: [68][   10/   37]    Overall Loss 0.673928    Objective Loss 0.673928                                        LR 0.000006    Time 0.047466    
2023-01-05 16:47:34,141 - Epoch: [68][   20/   37]    Overall Loss 0.672883    Objective Loss 0.672883                                        LR 0.000006    Time 0.037754    
2023-01-05 16:47:34,445 - Epoch: [68][   30/   37]    Overall Loss 0.672978    Objective Loss 0.672978                                        LR 0.000006    Time 0.035293    
2023-01-05 16:47:34,632 - Epoch: [68][   37/   37]    Overall Loss 0.673089    Objective Loss 0.673089    Top1 58.995816    LR 0.000006    Time 0.033658    
2023-01-05 16:47:34,707 - --- validate (epoch=68)-----------
2023-01-05 16:47:34,708 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:34,959 - Epoch: [68][    5/    5]    Loss 0.679116    Top1 60.209924    
2023-01-05 16:47:35,027 - ==> Top1: 60.210    Loss: 0.679

2023-01-05 16:47:35,027 - ==> Confusion:
[[283 146   0]
 [271 348   0]
 [  0   0   0]]

2023-01-05 16:47:35,028 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:35,029 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:35,038 - 

2023-01-05 16:47:35,038 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:35,656 - Epoch: [69][   10/   37]    Overall Loss 0.671436    Objective Loss 0.671436                                        LR 0.000006    Time 0.061642    
2023-01-05 16:47:35,948 - Epoch: [69][   20/   37]    Overall Loss 0.672755    Objective Loss 0.672755                                        LR 0.000006    Time 0.045399    
2023-01-05 16:47:36,244 - Epoch: [69][   30/   37]    Overall Loss 0.672716    Objective Loss 0.672716                                        LR 0.000006    Time 0.040116    
2023-01-05 16:47:36,440 - Epoch: [69][   37/   37]    Overall Loss 0.672365    Objective Loss 0.672365    Top1 59.623431    LR 0.000006    Time 0.037831    
2023-01-05 16:47:36,515 - --- validate (epoch=69)-----------
2023-01-05 16:47:36,515 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:36,765 - Epoch: [69][    5/    5]    Loss 0.677960    Top1 58.683206    
2023-01-05 16:47:36,835 - ==> Top1: 58.683    Loss: 0.678

2023-01-05 16:47:36,835 - ==> Confusion:
[[330  99   0]
 [334 285   0]
 [  0   0   0]]

2023-01-05 16:47:36,837 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:36,837 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:36,846 - 

2023-01-05 16:47:36,846 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:37,315 - Epoch: [70][   10/   37]    Overall Loss 0.673273    Objective Loss 0.673273                                        LR 0.000004    Time 0.046752    
2023-01-05 16:47:37,594 - Epoch: [70][   20/   37]    Overall Loss 0.672174    Objective Loss 0.672174                                        LR 0.000004    Time 0.037350    
2023-01-05 16:47:37,888 - Epoch: [70][   30/   37]    Overall Loss 0.672231    Objective Loss 0.672231                                        LR 0.000004    Time 0.034661    
2023-01-05 16:47:38,081 - Epoch: [70][   37/   37]    Overall Loss 0.672238    Objective Loss 0.672238    Top1 54.393305    LR 0.000004    Time 0.033326    
2023-01-05 16:47:38,153 - --- validate (epoch=70)-----------
2023-01-05 16:47:38,153 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:38,401 - Epoch: [70][    5/    5]    Loss 0.677425    Top1 55.343511    
2023-01-05 16:47:38,460 - ==> Top1: 55.344    Loss: 0.677

2023-01-05 16:47:38,460 - ==> Confusion:
[[344  85   0]
 [383 236   0]
 [  0   0   0]]

2023-01-05 16:47:38,462 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:38,462 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:38,472 - 

2023-01-05 16:47:38,472 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:38,949 - Epoch: [71][   10/   37]    Overall Loss 0.675990    Objective Loss 0.675990                                        LR 0.000004    Time 0.047606    
2023-01-05 16:47:39,236 - Epoch: [71][   20/   37]    Overall Loss 0.673715    Objective Loss 0.673715                                        LR 0.000004    Time 0.038117    
2023-01-05 16:47:39,533 - Epoch: [71][   30/   37]    Overall Loss 0.672461    Objective Loss 0.672461                                        LR 0.000004    Time 0.035298    
2023-01-05 16:47:39,731 - Epoch: [71][   37/   37]    Overall Loss 0.671973    Objective Loss 0.671973    Top1 57.322176    LR 0.000004    Time 0.033982    
2023-01-05 16:47:39,825 - --- validate (epoch=71)-----------
2023-01-05 16:47:39,825 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:40,087 - Epoch: [71][    5/    5]    Loss 0.677305    Top1 57.251908    
2023-01-05 16:47:40,149 - ==> Top1: 57.252    Loss: 0.677

2023-01-05 16:47:40,149 - ==> Confusion:
[[330  99   0]
 [349 270   0]
 [  0   0   0]]

2023-01-05 16:47:40,151 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:40,151 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:40,161 - 

2023-01-05 16:47:40,161 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:40,651 - Epoch: [72][   10/   37]    Overall Loss 0.672463    Objective Loss 0.672463                                        LR 0.000004    Time 0.048862    
2023-01-05 16:47:40,961 - Epoch: [72][   20/   37]    Overall Loss 0.671602    Objective Loss 0.671602                                        LR 0.000004    Time 0.039945    
2023-01-05 16:47:41,272 - Epoch: [72][   30/   37]    Overall Loss 0.672167    Objective Loss 0.672167                                        LR 0.000004    Time 0.036961    
2023-01-05 16:47:41,465 - Epoch: [72][   37/   37]    Overall Loss 0.671307    Objective Loss 0.671307    Top1 64.435146    LR 0.000004    Time 0.035189    
2023-01-05 16:47:41,540 - --- validate (epoch=72)-----------
2023-01-05 16:47:41,540 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:41,794 - Epoch: [72][    5/    5]    Loss 0.672537    Top1 59.160305    
2023-01-05 16:47:41,865 - ==> Top1: 59.160    Loss: 0.673

2023-01-05 16:47:41,865 - ==> Confusion:
[[318 111   0]
 [317 302   0]
 [  0   0   0]]

2023-01-05 16:47:41,867 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:41,867 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:41,877 - 

2023-01-05 16:47:41,877 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:42,387 - Epoch: [73][   10/   37]    Overall Loss 0.671732    Objective Loss 0.671732                                        LR 0.000004    Time 0.050921    
2023-01-05 16:47:42,680 - Epoch: [73][   20/   37]    Overall Loss 0.670273    Objective Loss 0.670273                                        LR 0.000004    Time 0.039956    
2023-01-05 16:47:42,991 - Epoch: [73][   30/   37]    Overall Loss 0.670478    Objective Loss 0.670478                                        LR 0.000004    Time 0.036931    
2023-01-05 16:47:43,186 - Epoch: [73][   37/   37]    Overall Loss 0.670889    Objective Loss 0.670889    Top1 57.531381    LR 0.000004    Time 0.035190    
2023-01-05 16:47:43,267 - --- validate (epoch=73)-----------
2023-01-05 16:47:43,267 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:43,519 - Epoch: [73][    5/    5]    Loss 0.678229    Top1 60.687023    
2023-01-05 16:47:43,585 - ==> Top1: 60.687    Loss: 0.678

2023-01-05 16:47:43,585 - ==> Confusion:
[[302 127   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:47:43,586 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:43,586 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:43,596 - 

2023-01-05 16:47:43,596 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:44,099 - Epoch: [74][   10/   37]    Overall Loss 0.670434    Objective Loss 0.670434                                        LR 0.000004    Time 0.050241    
2023-01-05 16:47:44,390 - Epoch: [74][   20/   37]    Overall Loss 0.671241    Objective Loss 0.671241                                        LR 0.000004    Time 0.039606    
2023-01-05 16:47:44,694 - Epoch: [74][   30/   37]    Overall Loss 0.670976    Objective Loss 0.670976                                        LR 0.000004    Time 0.036547    
2023-01-05 16:47:44,889 - Epoch: [74][   37/   37]    Overall Loss 0.670926    Objective Loss 0.670926    Top1 58.786611    LR 0.000004    Time 0.034891    
2023-01-05 16:47:44,966 - --- validate (epoch=74)-----------
2023-01-05 16:47:44,967 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:45,209 - Epoch: [74][    5/    5]    Loss 0.679370    Top1 58.778626    
2023-01-05 16:47:45,288 - ==> Top1: 58.779    Loss: 0.679

2023-01-05 16:47:45,288 - ==> Confusion:
[[332  97   0]
 [335 284   0]
 [  0   0   0]]

2023-01-05 16:47:45,290 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:45,290 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:45,299 - 

2023-01-05 16:47:45,299 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:45,795 - Epoch: [75][   10/   37]    Overall Loss 0.669151    Objective Loss 0.669151                                        LR 0.000004    Time 0.049492    
2023-01-05 16:47:46,097 - Epoch: [75][   20/   37]    Overall Loss 0.669736    Objective Loss 0.669736                                        LR 0.000004    Time 0.039711    
2023-01-05 16:47:46,409 - Epoch: [75][   30/   37]    Overall Loss 0.671021    Objective Loss 0.671021                                        LR 0.000004    Time 0.036864    
2023-01-05 16:47:46,605 - Epoch: [75][   37/   37]    Overall Loss 0.670572    Objective Loss 0.670572    Top1 58.786611    LR 0.000004    Time 0.035190    
2023-01-05 16:47:46,677 - --- validate (epoch=75)-----------
2023-01-05 16:47:46,677 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:46,935 - Epoch: [75][    5/    5]    Loss 0.679078    Top1 57.251908    
2023-01-05 16:47:47,008 - ==> Top1: 57.252    Loss: 0.679

2023-01-05 16:47:47,009 - ==> Confusion:
[[339  90   0]
 [358 261   0]
 [  0   0   0]]

2023-01-05 16:47:47,010 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:47,010 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:47,020 - 

2023-01-05 16:47:47,020 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:47,522 - Epoch: [76][   10/   37]    Overall Loss 0.671982    Objective Loss 0.671982                                        LR 0.000004    Time 0.050109    
2023-01-05 16:47:47,817 - Epoch: [76][   20/   37]    Overall Loss 0.671141    Objective Loss 0.671141                                        LR 0.000004    Time 0.039823    
2023-01-05 16:47:48,121 - Epoch: [76][   30/   37]    Overall Loss 0.670779    Objective Loss 0.670779                                        LR 0.000004    Time 0.036654    
2023-01-05 16:47:48,317 - Epoch: [76][   37/   37]    Overall Loss 0.670052    Objective Loss 0.670052    Top1 62.343096    LR 0.000004    Time 0.035002    
2023-01-05 16:47:48,375 - --- validate (epoch=76)-----------
2023-01-05 16:47:48,375 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:48,615 - Epoch: [76][    5/    5]    Loss 0.677946    Top1 58.015267    
2023-01-05 16:47:48,685 - ==> Top1: 58.015    Loss: 0.678

2023-01-05 16:47:48,685 - ==> Confusion:
[[332  97   0]
 [343 276   0]
 [  0   0   0]]

2023-01-05 16:47:48,686 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:48,686 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:48,696 - 

2023-01-05 16:47:48,696 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:49,209 - Epoch: [77][   10/   37]    Overall Loss 0.670803    Objective Loss 0.670803                                        LR 0.000004    Time 0.051257    
2023-01-05 16:47:49,500 - Epoch: [77][   20/   37]    Overall Loss 0.671824    Objective Loss 0.671824                                        LR 0.000004    Time 0.040149    
2023-01-05 16:47:49,815 - Epoch: [77][   30/   37]    Overall Loss 0.670205    Objective Loss 0.670205                                        LR 0.000004    Time 0.037254    
2023-01-05 16:47:50,012 - Epoch: [77][   37/   37]    Overall Loss 0.669837    Objective Loss 0.669837    Top1 61.924686    LR 0.000004    Time 0.035518    
2023-01-05 16:47:50,096 - --- validate (epoch=77)-----------
2023-01-05 16:47:50,096 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:50,338 - Epoch: [77][    5/    5]    Loss 0.672282    Top1 61.259542    
2023-01-05 16:47:50,399 - ==> Top1: 61.260    Loss: 0.672

2023-01-05 16:47:50,400 - ==> Confusion:
[[296 133   0]
 [273 346   0]
 [  0   0   0]]

2023-01-05 16:47:50,401 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:50,401 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:50,411 - 

2023-01-05 16:47:50,411 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:50,896 - Epoch: [78][   10/   37]    Overall Loss 0.669414    Objective Loss 0.669414                                        LR 0.000004    Time 0.048415    
2023-01-05 16:47:51,191 - Epoch: [78][   20/   37]    Overall Loss 0.669204    Objective Loss 0.669204                                        LR 0.000004    Time 0.038822    
2023-01-05 16:47:51,498 - Epoch: [78][   30/   37]    Overall Loss 0.669121    Objective Loss 0.669121                                        LR 0.000004    Time 0.036108    
2023-01-05 16:47:51,695 - Epoch: [78][   37/   37]    Overall Loss 0.669585    Objective Loss 0.669585    Top1 61.506276    LR 0.000004    Time 0.034593    
2023-01-05 16:47:51,768 - --- validate (epoch=78)-----------
2023-01-05 16:47:51,769 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:52,020 - Epoch: [78][    5/    5]    Loss 0.675414    Top1 60.305344    
2023-01-05 16:47:52,084 - ==> Top1: 60.305    Loss: 0.675

2023-01-05 16:47:52,084 - ==> Confusion:
[[300 129   0]
 [287 332   0]
 [  0   0   0]]

2023-01-05 16:47:52,085 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:52,086 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:52,095 - 

2023-01-05 16:47:52,095 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:52,592 - Epoch: [79][   10/   37]    Overall Loss 0.664935    Objective Loss 0.664935                                        LR 0.000004    Time 0.049604    
2023-01-05 16:47:52,908 - Epoch: [79][   20/   37]    Overall Loss 0.665821    Objective Loss 0.665821                                        LR 0.000004    Time 0.040560    
2023-01-05 16:47:53,223 - Epoch: [79][   30/   37]    Overall Loss 0.668631    Objective Loss 0.668631                                        LR 0.000004    Time 0.037544    
2023-01-05 16:47:53,417 - Epoch: [79][   37/   37]    Overall Loss 0.669155    Objective Loss 0.669155    Top1 62.133891    LR 0.000004    Time 0.035668    
2023-01-05 16:47:53,486 - --- validate (epoch=79)-----------
2023-01-05 16:47:53,486 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:53,737 - Epoch: [79][    5/    5]    Loss 0.667856    Top1 60.591603    
2023-01-05 16:47:53,804 - ==> Top1: 60.592    Loss: 0.668

2023-01-05 16:47:53,805 - ==> Confusion:
[[297 132   0]
 [281 338   0]
 [  0   0   0]]

2023-01-05 16:47:53,806 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:53,806 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:53,816 - 

2023-01-05 16:47:53,816 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:54,340 - Epoch: [80][   10/   37]    Overall Loss 0.667494    Objective Loss 0.667494                                        LR 0.000004    Time 0.052361    
2023-01-05 16:47:54,662 - Epoch: [80][   20/   37]    Overall Loss 0.668952    Objective Loss 0.668952                                        LR 0.000004    Time 0.042254    
2023-01-05 16:47:54,986 - Epoch: [80][   30/   37]    Overall Loss 0.668384    Objective Loss 0.668384                                        LR 0.000004    Time 0.038939    
2023-01-05 16:47:55,180 - Epoch: [80][   37/   37]    Overall Loss 0.668584    Objective Loss 0.668584    Top1 58.158996    LR 0.000004    Time 0.036816    
2023-01-05 16:47:55,261 - --- validate (epoch=80)-----------
2023-01-05 16:47:55,262 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:55,506 - Epoch: [80][    5/    5]    Loss 0.679478    Top1 61.164122    
2023-01-05 16:47:55,571 - ==> Top1: 61.164    Loss: 0.679

2023-01-05 16:47:55,571 - ==> Confusion:
[[305 124   0]
 [283 336   0]
 [  0   0   0]]

2023-01-05 16:47:55,573 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:55,573 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:55,583 - 

2023-01-05 16:47:55,583 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:56,070 - Epoch: [81][   10/   37]    Overall Loss 0.667738    Objective Loss 0.667738                                        LR 0.000004    Time 0.048568    
2023-01-05 16:47:56,363 - Epoch: [81][   20/   37]    Overall Loss 0.668168    Objective Loss 0.668168                                        LR 0.000004    Time 0.038832    
2023-01-05 16:47:56,665 - Epoch: [81][   30/   37]    Overall Loss 0.667289    Objective Loss 0.667289                                        LR 0.000004    Time 0.035938    
2023-01-05 16:47:56,858 - Epoch: [81][   37/   37]    Overall Loss 0.668258    Objective Loss 0.668258    Top1 58.368201    LR 0.000004    Time 0.034342    
2023-01-05 16:47:56,924 - --- validate (epoch=81)-----------
2023-01-05 16:47:56,924 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:57,171 - Epoch: [81][    5/    5]    Loss 0.671038    Top1 61.259542    
2023-01-05 16:47:57,236 - ==> Top1: 61.260    Loss: 0.671

2023-01-05 16:47:57,236 - ==> Confusion:
[[271 158   0]
 [248 371   0]
 [  0   0   0]]

2023-01-05 16:47:57,238 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:57,238 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:57,247 - 

2023-01-05 16:47:57,247 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:57,759 - Epoch: [82][   10/   37]    Overall Loss 0.669107    Objective Loss 0.669107                                        LR 0.000004    Time 0.051114    
2023-01-05 16:47:58,083 - Epoch: [82][   20/   37]    Overall Loss 0.668271    Objective Loss 0.668271                                        LR 0.000004    Time 0.041720    
2023-01-05 16:47:58,391 - Epoch: [82][   30/   37]    Overall Loss 0.668361    Objective Loss 0.668361                                        LR 0.000004    Time 0.038049    
2023-01-05 16:47:58,586 - Epoch: [82][   37/   37]    Overall Loss 0.668012    Objective Loss 0.668012    Top1 61.924686    LR 0.000004    Time 0.036125    
2023-01-05 16:47:58,662 - --- validate (epoch=82)-----------
2023-01-05 16:47:58,663 - 1048 samples (256 per mini-batch)
2023-01-05 16:47:58,919 - Epoch: [82][    5/    5]    Loss 0.680673    Top1 60.496183    
2023-01-05 16:47:58,987 - ==> Top1: 60.496    Loss: 0.681

2023-01-05 16:47:58,987 - ==> Confusion:
[[282 147   0]
 [267 352   0]
 [  0   0   0]]

2023-01-05 16:47:58,988 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:47:58,988 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:47:58,998 - 

2023-01-05 16:47:58,998 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:47:59,490 - Epoch: [83][   10/   37]    Overall Loss 0.665148    Objective Loss 0.665148                                        LR 0.000004    Time 0.049076    
2023-01-05 16:47:59,793 - Epoch: [83][   20/   37]    Overall Loss 0.667220    Objective Loss 0.667220                                        LR 0.000004    Time 0.039666    
2023-01-05 16:48:00,103 - Epoch: [83][   30/   37]    Overall Loss 0.667207    Objective Loss 0.667207                                        LR 0.000004    Time 0.036771    
2023-01-05 16:48:00,298 - Epoch: [83][   37/   37]    Overall Loss 0.667778    Objective Loss 0.667778    Top1 56.276151    LR 0.000004    Time 0.035092    
2023-01-05 16:48:00,369 - --- validate (epoch=83)-----------
2023-01-05 16:48:00,369 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:00,611 - Epoch: [83][    5/    5]    Loss 0.677050    Top1 59.923664    
2023-01-05 16:48:00,687 - ==> Top1: 59.924    Loss: 0.677

2023-01-05 16:48:00,687 - ==> Confusion:
[[322 107   0]
 [313 306   0]
 [  0   0   0]]

2023-01-05 16:48:00,688 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:00,689 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:00,698 - 

2023-01-05 16:48:00,698 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:01,210 - Epoch: [84][   10/   37]    Overall Loss 0.664890    Objective Loss 0.664890                                        LR 0.000004    Time 0.051113    
2023-01-05 16:48:01,515 - Epoch: [84][   20/   37]    Overall Loss 0.665574    Objective Loss 0.665574                                        LR 0.000004    Time 0.040767    
2023-01-05 16:48:01,828 - Epoch: [84][   30/   37]    Overall Loss 0.667010    Objective Loss 0.667010                                        LR 0.000004    Time 0.037586    
2023-01-05 16:48:02,024 - Epoch: [84][   37/   37]    Overall Loss 0.667007    Objective Loss 0.667007    Top1 57.112971    LR 0.000004    Time 0.035789    
2023-01-05 16:48:02,093 - --- validate (epoch=84)-----------
2023-01-05 16:48:02,093 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:02,348 - Epoch: [84][    5/    5]    Loss 0.674127    Top1 56.393130    
2023-01-05 16:48:02,426 - ==> Top1: 56.393    Loss: 0.674

2023-01-05 16:48:02,427 - ==> Confusion:
[[345  84   0]
 [373 246   0]
 [  0   0   0]]

2023-01-05 16:48:02,428 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:02,428 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:02,438 - 

2023-01-05 16:48:02,438 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:03,050 - Epoch: [85][   10/   37]    Overall Loss 0.668659    Objective Loss 0.668659                                        LR 0.000004    Time 0.061096    
2023-01-05 16:48:03,344 - Epoch: [85][   20/   37]    Overall Loss 0.668150    Objective Loss 0.668150                                        LR 0.000004    Time 0.045233    
2023-01-05 16:48:03,643 - Epoch: [85][   30/   37]    Overall Loss 0.667971    Objective Loss 0.667971                                        LR 0.000004    Time 0.040132    
2023-01-05 16:48:03,837 - Epoch: [85][   37/   37]    Overall Loss 0.666961    Objective Loss 0.666961    Top1 64.644351    LR 0.000004    Time 0.037779    
2023-01-05 16:48:03,911 - --- validate (epoch=85)-----------
2023-01-05 16:48:03,912 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:04,158 - Epoch: [85][    5/    5]    Loss 0.670728    Top1 60.496183    
2023-01-05 16:48:04,221 - ==> Top1: 60.496    Loss: 0.671

2023-01-05 16:48:04,222 - ==> Confusion:
[[305 124   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:48:04,223 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:04,223 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:04,233 - 

2023-01-05 16:48:04,233 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:04,723 - Epoch: [86][   10/   37]    Overall Loss 0.665818    Objective Loss 0.665818                                        LR 0.000004    Time 0.048993    
2023-01-05 16:48:05,015 - Epoch: [86][   20/   37]    Overall Loss 0.666684    Objective Loss 0.666684                                        LR 0.000004    Time 0.039059    
2023-01-05 16:48:05,310 - Epoch: [86][   30/   37]    Overall Loss 0.666186    Objective Loss 0.666186                                        LR 0.000004    Time 0.035847    
2023-01-05 16:48:05,510 - Epoch: [86][   37/   37]    Overall Loss 0.666546    Objective Loss 0.666546    Top1 61.506276    LR 0.000004    Time 0.034475    
2023-01-05 16:48:05,576 - --- validate (epoch=86)-----------
2023-01-05 16:48:05,576 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:05,836 - Epoch: [86][    5/    5]    Loss 0.664944    Top1 59.160305    
2023-01-05 16:48:05,904 - ==> Top1: 59.160    Loss: 0.665

2023-01-05 16:48:05,904 - ==> Confusion:
[[328 101   0]
 [327 292   0]
 [  0   0   0]]

2023-01-05 16:48:05,905 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:05,905 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:05,915 - 

2023-01-05 16:48:05,915 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:06,399 - Epoch: [87][   10/   37]    Overall Loss 0.663819    Objective Loss 0.663819                                        LR 0.000004    Time 0.048276    
2023-01-05 16:48:06,696 - Epoch: [87][   20/   37]    Overall Loss 0.664805    Objective Loss 0.664805                                        LR 0.000004    Time 0.038955    
2023-01-05 16:48:07,002 - Epoch: [87][   30/   37]    Overall Loss 0.665441    Objective Loss 0.665441                                        LR 0.000004    Time 0.036179    
2023-01-05 16:48:07,192 - Epoch: [87][   37/   37]    Overall Loss 0.666225    Objective Loss 0.666225    Top1 57.740586    LR 0.000004    Time 0.034450    
2023-01-05 16:48:07,274 - --- validate (epoch=87)-----------
2023-01-05 16:48:07,274 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:07,512 - Epoch: [87][    5/    5]    Loss 0.671838    Top1 61.259542    
2023-01-05 16:48:07,578 - ==> Top1: 61.260    Loss: 0.672

2023-01-05 16:48:07,579 - ==> Confusion:
[[309 120   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:48:07,580 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:07,580 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:07,590 - 

2023-01-05 16:48:07,590 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:08,098 - Epoch: [88][   10/   37]    Overall Loss 0.663874    Objective Loss 0.663874                                        LR 0.000004    Time 0.050736    
2023-01-05 16:48:08,395 - Epoch: [88][   20/   37]    Overall Loss 0.665267    Objective Loss 0.665267                                        LR 0.000004    Time 0.040164    
2023-01-05 16:48:08,698 - Epoch: [88][   30/   37]    Overall Loss 0.665547    Objective Loss 0.665547                                        LR 0.000004    Time 0.036870    
2023-01-05 16:48:08,894 - Epoch: [88][   37/   37]    Overall Loss 0.665681    Objective Loss 0.665681    Top1 61.297071    LR 0.000004    Time 0.035192    
2023-01-05 16:48:08,965 - --- validate (epoch=88)-----------
2023-01-05 16:48:08,965 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:09,207 - Epoch: [88][    5/    5]    Loss 0.680115    Top1 60.877863    
2023-01-05 16:48:09,272 - ==> Top1: 60.878    Loss: 0.680

2023-01-05 16:48:09,273 - ==> Confusion:
[[285 144   0]
 [266 353   0]
 [  0   0   0]]

2023-01-05 16:48:09,274 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:09,274 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:09,284 - 

2023-01-05 16:48:09,284 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:09,787 - Epoch: [89][   10/   37]    Overall Loss 0.664609    Objective Loss 0.664609                                        LR 0.000004    Time 0.050213    
2023-01-05 16:48:10,084 - Epoch: [89][   20/   37]    Overall Loss 0.664196    Objective Loss 0.664196                                        LR 0.000004    Time 0.039957    
2023-01-05 16:48:10,392 - Epoch: [89][   30/   37]    Overall Loss 0.663792    Objective Loss 0.663792                                        LR 0.000004    Time 0.036836    
2023-01-05 16:48:10,588 - Epoch: [89][   37/   37]    Overall Loss 0.665267    Objective Loss 0.665267    Top1 57.531381    LR 0.000004    Time 0.035158    
2023-01-05 16:48:10,663 - --- validate (epoch=89)-----------
2023-01-05 16:48:10,663 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:10,909 - Epoch: [89][    5/    5]    Loss 0.679265    Top1 60.591603    
2023-01-05 16:48:10,977 - ==> Top1: 60.592    Loss: 0.679

2023-01-05 16:48:10,977 - ==> Confusion:
[[307 122   0]
 [291 328   0]
 [  0   0   0]]

2023-01-05 16:48:10,978 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:10,978 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:10,988 - 

2023-01-05 16:48:10,988 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:11,485 - Epoch: [90][   10/   37]    Overall Loss 0.665150    Objective Loss 0.665150                                        LR 0.000004    Time 0.049619    
2023-01-05 16:48:11,788 - Epoch: [90][   20/   37]    Overall Loss 0.661863    Objective Loss 0.661863                                        LR 0.000004    Time 0.039941    
2023-01-05 16:48:12,092 - Epoch: [90][   30/   37]    Overall Loss 0.663941    Objective Loss 0.663941                                        LR 0.000004    Time 0.036743    
2023-01-05 16:48:12,282 - Epoch: [90][   37/   37]    Overall Loss 0.665000    Objective Loss 0.665000    Top1 56.485356    LR 0.000004    Time 0.034898    
2023-01-05 16:48:12,355 - --- validate (epoch=90)-----------
2023-01-05 16:48:12,355 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:12,606 - Epoch: [90][    5/    5]    Loss 0.669645    Top1 60.114504    
2023-01-05 16:48:12,688 - ==> Top1: 60.115    Loss: 0.670

2023-01-05 16:48:12,688 - ==> Confusion:
[[327 102   0]
 [316 303   0]
 [  0   0   0]]

2023-01-05 16:48:12,690 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:12,690 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:12,699 - 

2023-01-05 16:48:12,700 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:13,173 - Epoch: [91][   10/   37]    Overall Loss 0.661779    Objective Loss 0.661779                                        LR 0.000004    Time 0.047275    
2023-01-05 16:48:13,457 - Epoch: [91][   20/   37]    Overall Loss 0.663841    Objective Loss 0.663841                                        LR 0.000004    Time 0.037791    
2023-01-05 16:48:13,746 - Epoch: [91][   30/   37]    Overall Loss 0.664385    Objective Loss 0.664385                                        LR 0.000004    Time 0.034821    
2023-01-05 16:48:13,944 - Epoch: [91][   37/   37]    Overall Loss 0.664601    Objective Loss 0.664601    Top1 57.531381    LR 0.000004    Time 0.033577    
2023-01-05 16:48:14,018 - --- validate (epoch=91)-----------
2023-01-05 16:48:14,019 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:14,255 - Epoch: [91][    5/    5]    Loss 0.675248    Top1 60.114504    
2023-01-05 16:48:14,312 - ==> Top1: 60.115    Loss: 0.675

2023-01-05 16:48:14,313 - ==> Confusion:
[[326 103   0]
 [315 304   0]
 [  0   0   0]]

2023-01-05 16:48:14,315 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:14,315 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:14,328 - 

2023-01-05 16:48:14,329 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:14,824 - Epoch: [92][   10/   37]    Overall Loss 0.667419    Objective Loss 0.667419                                        LR 0.000004    Time 0.049412    
2023-01-05 16:48:15,118 - Epoch: [92][   20/   37]    Overall Loss 0.665329    Objective Loss 0.665329                                        LR 0.000004    Time 0.039411    
2023-01-05 16:48:15,424 - Epoch: [92][   30/   37]    Overall Loss 0.664649    Objective Loss 0.664649                                        LR 0.000004    Time 0.036442    
2023-01-05 16:48:15,619 - Epoch: [92][   37/   37]    Overall Loss 0.664073    Objective Loss 0.664073    Top1 64.435146    LR 0.000004    Time 0.034835    
2023-01-05 16:48:15,694 - --- validate (epoch=92)-----------
2023-01-05 16:48:15,695 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:15,945 - Epoch: [92][    5/    5]    Loss 0.673244    Top1 60.687023    
2023-01-05 16:48:16,014 - ==> Top1: 60.687    Loss: 0.673

2023-01-05 16:48:16,014 - ==> Confusion:
[[307 122   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:48:16,015 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:16,016 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:16,025 - 

2023-01-05 16:48:16,025 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:16,516 - Epoch: [93][   10/   37]    Overall Loss 0.665456    Objective Loss 0.665456                                        LR 0.000004    Time 0.048971    
2023-01-05 16:48:16,810 - Epoch: [93][   20/   37]    Overall Loss 0.665494    Objective Loss 0.665494                                        LR 0.000004    Time 0.039181    
2023-01-05 16:48:17,118 - Epoch: [93][   30/   37]    Overall Loss 0.664892    Objective Loss 0.664892                                        LR 0.000004    Time 0.036368    
2023-01-05 16:48:17,312 - Epoch: [93][   37/   37]    Overall Loss 0.663618    Objective Loss 0.663618    Top1 63.598326    LR 0.000004    Time 0.034711    
2023-01-05 16:48:17,383 - --- validate (epoch=93)-----------
2023-01-05 16:48:17,383 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:17,636 - Epoch: [93][    5/    5]    Loss 0.670833    Top1 60.305344    
2023-01-05 16:48:17,702 - ==> Top1: 60.305    Loss: 0.671

2023-01-05 16:48:17,702 - ==> Confusion:
[[318 111   0]
 [305 314   0]
 [  0   0   0]]

2023-01-05 16:48:17,704 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:17,704 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:17,713 - 

2023-01-05 16:48:17,713 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:18,211 - Epoch: [94][   10/   37]    Overall Loss 0.664920    Objective Loss 0.664920                                        LR 0.000004    Time 0.049710    
2023-01-05 16:48:18,528 - Epoch: [94][   20/   37]    Overall Loss 0.663509    Objective Loss 0.663509                                        LR 0.000004    Time 0.040583    
2023-01-05 16:48:18,842 - Epoch: [94][   30/   37]    Overall Loss 0.663525    Objective Loss 0.663525                                        LR 0.000004    Time 0.037504    
2023-01-05 16:48:19,036 - Epoch: [94][   37/   37]    Overall Loss 0.663185    Objective Loss 0.663185    Top1 64.225941    LR 0.000004    Time 0.035664    
2023-01-05 16:48:19,113 - --- validate (epoch=94)-----------
2023-01-05 16:48:19,114 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:19,364 - Epoch: [94][    5/    5]    Loss 0.669609    Top1 61.164122    
2023-01-05 16:48:19,429 - ==> Top1: 61.164    Loss: 0.670

2023-01-05 16:48:19,429 - ==> Confusion:
[[302 127   0]
 [280 339   0]
 [  0   0   0]]

2023-01-05 16:48:19,431 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:19,431 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:19,441 - 

2023-01-05 16:48:19,441 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:19,948 - Epoch: [95][   10/   37]    Overall Loss 0.665242    Objective Loss 0.665242                                        LR 0.000004    Time 0.050597    
2023-01-05 16:48:20,252 - Epoch: [95][   20/   37]    Overall Loss 0.661754    Objective Loss 0.661754                                        LR 0.000004    Time 0.040404    
2023-01-05 16:48:20,565 - Epoch: [95][   30/   37]    Overall Loss 0.662971    Objective Loss 0.662971                                        LR 0.000004    Time 0.037346    
2023-01-05 16:48:20,759 - Epoch: [95][   37/   37]    Overall Loss 0.662819    Objective Loss 0.662819    Top1 60.251046    LR 0.000004    Time 0.035542    
2023-01-05 16:48:20,828 - --- validate (epoch=95)-----------
2023-01-05 16:48:20,828 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:21,076 - Epoch: [95][    5/    5]    Loss 0.670276    Top1 60.209924    
2023-01-05 16:48:21,148 - ==> Top1: 60.210    Loss: 0.670

2023-01-05 16:48:21,148 - ==> Confusion:
[[325 104   0]
 [313 306   0]
 [  0   0   0]]

2023-01-05 16:48:21,150 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:21,150 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:21,160 - 

2023-01-05 16:48:21,160 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:21,683 - Epoch: [96][   10/   37]    Overall Loss 0.661732    Objective Loss 0.661732                                        LR 0.000004    Time 0.052197    
2023-01-05 16:48:22,008 - Epoch: [96][   20/   37]    Overall Loss 0.664119    Objective Loss 0.664119                                        LR 0.000004    Time 0.042325    
2023-01-05 16:48:22,328 - Epoch: [96][   30/   37]    Overall Loss 0.663636    Objective Loss 0.663636                                        LR 0.000004    Time 0.038890    
2023-01-05 16:48:22,524 - Epoch: [96][   37/   37]    Overall Loss 0.662448    Objective Loss 0.662448    Top1 61.924686    LR 0.000004    Time 0.036812    
2023-01-05 16:48:22,609 - --- validate (epoch=96)-----------
2023-01-05 16:48:22,609 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:22,862 - Epoch: [96][    5/    5]    Loss 0.669272    Top1 60.305344    
2023-01-05 16:48:22,928 - ==> Top1: 60.305    Loss: 0.669

2023-01-05 16:48:22,928 - ==> Confusion:
[[327 102   0]
 [314 305   0]
 [  0   0   0]]

2023-01-05 16:48:22,930 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:22,930 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:22,940 - 

2023-01-05 16:48:22,940 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:23,438 - Epoch: [97][   10/   37]    Overall Loss 0.664166    Objective Loss 0.664166                                        LR 0.000004    Time 0.049777    
2023-01-05 16:48:23,729 - Epoch: [97][   20/   37]    Overall Loss 0.662134    Objective Loss 0.662134                                        LR 0.000004    Time 0.039420    
2023-01-05 16:48:24,038 - Epoch: [97][   30/   37]    Overall Loss 0.661329    Objective Loss 0.661329                                        LR 0.000004    Time 0.036553    
2023-01-05 16:48:24,234 - Epoch: [97][   37/   37]    Overall Loss 0.661903    Objective Loss 0.661903    Top1 57.322176    LR 0.000004    Time 0.034942    
2023-01-05 16:48:24,308 - --- validate (epoch=97)-----------
2023-01-05 16:48:24,309 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:24,554 - Epoch: [97][    5/    5]    Loss 0.664983    Top1 61.545802    
2023-01-05 16:48:24,616 - ==> Top1: 61.546    Loss: 0.665

2023-01-05 16:48:24,616 - ==> Confusion:
[[312 117   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:48:24,617 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:24,618 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:24,627 - 

2023-01-05 16:48:24,627 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:25,118 - Epoch: [98][   10/   37]    Overall Loss 0.664653    Objective Loss 0.664653                                        LR 0.000004    Time 0.048964    
2023-01-05 16:48:25,409 - Epoch: [98][   20/   37]    Overall Loss 0.662956    Objective Loss 0.662956                                        LR 0.000004    Time 0.039013    
2023-01-05 16:48:25,706 - Epoch: [98][   30/   37]    Overall Loss 0.661859    Objective Loss 0.661859                                        LR 0.000004    Time 0.035894    
2023-01-05 16:48:25,898 - Epoch: [98][   37/   37]    Overall Loss 0.661622    Objective Loss 0.661622    Top1 64.853556    LR 0.000004    Time 0.034295    
2023-01-05 16:48:25,969 - --- validate (epoch=98)-----------
2023-01-05 16:48:25,969 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:26,224 - Epoch: [98][    5/    5]    Loss 0.672772    Top1 60.496183    
2023-01-05 16:48:26,294 - ==> Top1: 60.496    Loss: 0.673

2023-01-05 16:48:26,294 - ==> Confusion:
[[290 139   0]
 [275 344   0]
 [  0   0   0]]

2023-01-05 16:48:26,296 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:26,296 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:26,305 - 

2023-01-05 16:48:26,306 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:26,793 - Epoch: [99][   10/   37]    Overall Loss 0.659364    Objective Loss 0.659364                                        LR 0.000004    Time 0.048605    
2023-01-05 16:48:27,100 - Epoch: [99][   20/   37]    Overall Loss 0.658649    Objective Loss 0.658649                                        LR 0.000004    Time 0.039638    
2023-01-05 16:48:27,407 - Epoch: [99][   30/   37]    Overall Loss 0.659624    Objective Loss 0.659624                                        LR 0.000004    Time 0.036665    
2023-01-05 16:48:27,603 - Epoch: [99][   37/   37]    Overall Loss 0.661236    Objective Loss 0.661236    Top1 58.786611    LR 0.000004    Time 0.035007    
2023-01-05 16:48:27,684 - --- validate (epoch=99)-----------
2023-01-05 16:48:27,684 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:27,923 - Epoch: [99][    5/    5]    Loss 0.666204    Top1 59.446565    
2023-01-05 16:48:27,995 - ==> Top1: 59.447    Loss: 0.666

2023-01-05 16:48:27,995 - ==> Confusion:
[[330  99   0]
 [326 293   0]
 [  0   0   0]]

2023-01-05 16:48:27,997 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:27,997 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:28,007 - 

2023-01-05 16:48:28,007 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:28,481 - Epoch: [100][   10/   37]    Overall Loss 0.660774    Objective Loss 0.660774                                        LR 0.000002    Time 0.047361    
2023-01-05 16:48:28,765 - Epoch: [100][   20/   37]    Overall Loss 0.659110    Objective Loss 0.659110                                        LR 0.000002    Time 0.037836    
2023-01-05 16:48:29,053 - Epoch: [100][   30/   37]    Overall Loss 0.660307    Objective Loss 0.660307                                        LR 0.000002    Time 0.034822    
2023-01-05 16:48:29,248 - Epoch: [100][   37/   37]    Overall Loss 0.660476    Objective Loss 0.660476    Top1 59.205021    LR 0.000002    Time 0.033490    
2023-01-05 16:48:29,320 - --- validate (epoch=100)-----------
2023-01-05 16:48:29,320 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:29,560 - Epoch: [100][    5/    5]    Loss 0.670746    Top1 61.354962    
2023-01-05 16:48:29,625 - ==> Top1: 61.355    Loss: 0.671

2023-01-05 16:48:29,625 - ==> Confusion:
[[300 129   0]
 [276 343   0]
 [  0   0   0]]

2023-01-05 16:48:29,626 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:29,627 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:29,637 - 

2023-01-05 16:48:29,637 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:30,247 - Epoch: [101][   10/   37]    Overall Loss 0.661149    Objective Loss 0.661149                                        LR 0.000002    Time 0.060972    
2023-01-05 16:48:30,535 - Epoch: [101][   20/   37]    Overall Loss 0.661482    Objective Loss 0.661482                                        LR 0.000002    Time 0.044855    
2023-01-05 16:48:30,832 - Epoch: [101][   30/   37]    Overall Loss 0.660327    Objective Loss 0.660327                                        LR 0.000002    Time 0.039796    
2023-01-05 16:48:31,026 - Epoch: [101][   37/   37]    Overall Loss 0.660193    Objective Loss 0.660193    Top1 59.832636    LR 0.000002    Time 0.037495    
2023-01-05 16:48:31,102 - --- validate (epoch=101)-----------
2023-01-05 16:48:31,102 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:31,351 - Epoch: [101][    5/    5]    Loss 0.667620    Top1 60.400763    
2023-01-05 16:48:31,427 - ==> Top1: 60.401    Loss: 0.668

2023-01-05 16:48:31,428 - ==> Confusion:
[[324 105   0]
 [310 309   0]
 [  0   0   0]]

2023-01-05 16:48:31,429 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:31,429 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:31,439 - 

2023-01-05 16:48:31,440 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:31,930 - Epoch: [102][   10/   37]    Overall Loss 0.658541    Objective Loss 0.658541                                        LR 0.000002    Time 0.048967    
2023-01-05 16:48:32,215 - Epoch: [102][   20/   37]    Overall Loss 0.661153    Objective Loss 0.661153                                        LR 0.000002    Time 0.038680    
2023-01-05 16:48:32,519 - Epoch: [102][   30/   37]    Overall Loss 0.660255    Objective Loss 0.660255                                        LR 0.000002    Time 0.035922    
2023-01-05 16:48:32,714 - Epoch: [102][   37/   37]    Overall Loss 0.659701    Objective Loss 0.659701    Top1 62.970711    LR 0.000002    Time 0.034378    
2023-01-05 16:48:32,779 - --- validate (epoch=102)-----------
2023-01-05 16:48:32,779 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:33,018 - Epoch: [102][    5/    5]    Loss 0.670781    Top1 60.782443    
2023-01-05 16:48:33,085 - ==> Top1: 60.782    Loss: 0.671

2023-01-05 16:48:33,086 - ==> Confusion:
[[326 103   0]
 [308 311   0]
 [  0   0   0]]

2023-01-05 16:48:33,087 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:33,087 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:33,097 - 

2023-01-05 16:48:33,097 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:33,581 - Epoch: [103][   10/   37]    Overall Loss 0.660280    Objective Loss 0.660280                                        LR 0.000002    Time 0.048278    
2023-01-05 16:48:33,867 - Epoch: [103][   20/   37]    Overall Loss 0.659984    Objective Loss 0.659984                                        LR 0.000002    Time 0.038412    
2023-01-05 16:48:34,178 - Epoch: [103][   30/   37]    Overall Loss 0.659181    Objective Loss 0.659181                                        LR 0.000002    Time 0.035986    
2023-01-05 16:48:34,370 - Epoch: [103][   37/   37]    Overall Loss 0.659297    Objective Loss 0.659297    Top1 61.715481    LR 0.000002    Time 0.034347    
2023-01-05 16:48:34,445 - --- validate (epoch=103)-----------
2023-01-05 16:48:34,445 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:34,685 - Epoch: [103][    5/    5]    Loss 0.663705    Top1 60.591603    
2023-01-05 16:48:34,751 - ==> Top1: 60.592    Loss: 0.664

2023-01-05 16:48:34,751 - ==> Confusion:
[[325 104   0]
 [309 310   0]
 [  0   0   0]]

2023-01-05 16:48:34,753 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:34,753 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:34,763 - 

2023-01-05 16:48:34,763 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:35,245 - Epoch: [104][   10/   37]    Overall Loss 0.661432    Objective Loss 0.661432                                        LR 0.000002    Time 0.048105    
2023-01-05 16:48:35,534 - Epoch: [104][   20/   37]    Overall Loss 0.659297    Objective Loss 0.659297                                        LR 0.000002    Time 0.038475    
2023-01-05 16:48:35,831 - Epoch: [104][   30/   37]    Overall Loss 0.659560    Objective Loss 0.659560                                        LR 0.000002    Time 0.035537    
2023-01-05 16:48:36,023 - Epoch: [104][   37/   37]    Overall Loss 0.659212    Objective Loss 0.659212    Top1 58.577406    LR 0.000002    Time 0.034000    
2023-01-05 16:48:36,095 - --- validate (epoch=104)-----------
2023-01-05 16:48:36,096 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:36,333 - Epoch: [104][    5/    5]    Loss 0.663723    Top1 61.641221    
2023-01-05 16:48:36,397 - ==> Top1: 61.641    Loss: 0.664

2023-01-05 16:48:36,397 - ==> Confusion:
[[309 120   0]
 [282 337   0]
 [  0   0   0]]

2023-01-05 16:48:36,398 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:36,398 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:36,408 - 

2023-01-05 16:48:36,408 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:36,890 - Epoch: [105][   10/   37]    Overall Loss 0.659667    Objective Loss 0.659667                                        LR 0.000002    Time 0.048080    
2023-01-05 16:48:37,177 - Epoch: [105][   20/   37]    Overall Loss 0.659573    Objective Loss 0.659573                                        LR 0.000002    Time 0.038381    
2023-01-05 16:48:37,485 - Epoch: [105][   30/   37]    Overall Loss 0.659179    Objective Loss 0.659179                                        LR 0.000002    Time 0.035848    
2023-01-05 16:48:37,680 - Epoch: [105][   37/   37]    Overall Loss 0.658987    Objective Loss 0.658987    Top1 59.832636    LR 0.000002    Time 0.034319    
2023-01-05 16:48:37,745 - --- validate (epoch=105)-----------
2023-01-05 16:48:37,745 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:37,985 - Epoch: [105][    5/    5]    Loss 0.658748    Top1 60.114504    
2023-01-05 16:48:38,057 - ==> Top1: 60.115    Loss: 0.659

2023-01-05 16:48:38,057 - ==> Confusion:
[[325 104   0]
 [314 305   0]
 [  0   0   0]]

2023-01-05 16:48:38,059 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:38,059 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:38,069 - 

2023-01-05 16:48:38,069 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:38,564 - Epoch: [106][   10/   37]    Overall Loss 0.659040    Objective Loss 0.659040                                        LR 0.000002    Time 0.049444    
2023-01-05 16:48:38,858 - Epoch: [106][   20/   37]    Overall Loss 0.659750    Objective Loss 0.659750                                        LR 0.000002    Time 0.039242    
2023-01-05 16:48:39,172 - Epoch: [106][   30/   37]    Overall Loss 0.659225    Objective Loss 0.659225                                        LR 0.000002    Time 0.036606    
2023-01-05 16:48:39,365 - Epoch: [106][   37/   37]    Overall Loss 0.658639    Objective Loss 0.658639    Top1 62.761506    LR 0.000002    Time 0.034892    
2023-01-05 16:48:39,435 - --- validate (epoch=106)-----------
2023-01-05 16:48:39,436 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:39,676 - Epoch: [106][    5/    5]    Loss 0.664159    Top1 61.068702    
2023-01-05 16:48:39,749 - ==> Top1: 61.069    Loss: 0.664

2023-01-05 16:48:39,749 - ==> Confusion:
[[322 107   0]
 [301 318   0]
 [  0   0   0]]

2023-01-05 16:48:39,751 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:39,751 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:39,764 - 

2023-01-05 16:48:39,764 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:40,255 - Epoch: [107][   10/   37]    Overall Loss 0.658484    Objective Loss 0.658484                                        LR 0.000002    Time 0.049024    
2023-01-05 16:48:40,550 - Epoch: [107][   20/   37]    Overall Loss 0.660148    Objective Loss 0.660148                                        LR 0.000002    Time 0.039264    
2023-01-05 16:48:40,850 - Epoch: [107][   30/   37]    Overall Loss 0.658870    Objective Loss 0.658870                                        LR 0.000002    Time 0.036135    
2023-01-05 16:48:41,044 - Epoch: [107][   37/   37]    Overall Loss 0.658169    Objective Loss 0.658169    Top1 61.087866    LR 0.000002    Time 0.034552    
2023-01-05 16:48:41,111 - --- validate (epoch=107)-----------
2023-01-05 16:48:41,111 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:41,357 - Epoch: [107][    5/    5]    Loss 0.668649    Top1 60.973282    
2023-01-05 16:48:41,418 - ==> Top1: 60.973    Loss: 0.669

2023-01-05 16:48:41,418 - ==> Confusion:
[[315 114   0]
 [295 324   0]
 [  0   0   0]]

2023-01-05 16:48:41,419 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:41,420 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:41,429 - 

2023-01-05 16:48:41,429 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:41,917 - Epoch: [108][   10/   37]    Overall Loss 0.657747    Objective Loss 0.657747                                        LR 0.000002    Time 0.048652    
2023-01-05 16:48:42,207 - Epoch: [108][   20/   37]    Overall Loss 0.660424    Objective Loss 0.660424                                        LR 0.000002    Time 0.038810    
2023-01-05 16:48:42,515 - Epoch: [108][   30/   37]    Overall Loss 0.659661    Objective Loss 0.659661                                        LR 0.000002    Time 0.036121    
2023-01-05 16:48:42,708 - Epoch: [108][   37/   37]    Overall Loss 0.657661    Objective Loss 0.657661    Top1 62.133891    LR 0.000002    Time 0.034521    
2023-01-05 16:48:42,793 - --- validate (epoch=108)-----------
2023-01-05 16:48:42,794 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:43,037 - Epoch: [108][    5/    5]    Loss 0.667317    Top1 61.164122    
2023-01-05 16:48:43,111 - ==> Top1: 61.164    Loss: 0.667

2023-01-05 16:48:43,111 - ==> Confusion:
[[320 109   0]
 [298 321   0]
 [  0   0   0]]

2023-01-05 16:48:43,112 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:43,113 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:43,122 - 

2023-01-05 16:48:43,122 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:43,640 - Epoch: [109][   10/   37]    Overall Loss 0.660593    Objective Loss 0.660593                                        LR 0.000002    Time 0.051632    
2023-01-05 16:48:43,965 - Epoch: [109][   20/   37]    Overall Loss 0.657620    Objective Loss 0.657620                                        LR 0.000002    Time 0.042083    
2023-01-05 16:48:44,281 - Epoch: [109][   30/   37]    Overall Loss 0.657251    Objective Loss 0.657251                                        LR 0.000002    Time 0.038551    
2023-01-05 16:48:44,476 - Epoch: [109][   37/   37]    Overall Loss 0.657530    Objective Loss 0.657530    Top1 60.878661    LR 0.000002    Time 0.036523    
2023-01-05 16:48:44,547 - --- validate (epoch=109)-----------
2023-01-05 16:48:44,547 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:44,786 - Epoch: [109][    5/    5]    Loss 0.676702    Top1 58.969466    
2023-01-05 16:48:44,852 - ==> Top1: 58.969    Loss: 0.677

2023-01-05 16:48:44,853 - ==> Confusion:
[[335  94   0]
 [336 283   0]
 [  0   0   0]]

2023-01-05 16:48:44,854 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:44,854 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:44,864 - 

2023-01-05 16:48:44,864 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:45,379 - Epoch: [110][   10/   37]    Overall Loss 0.657125    Objective Loss 0.657125                                        LR 0.000002    Time 0.051423    
2023-01-05 16:48:45,667 - Epoch: [110][   20/   37]    Overall Loss 0.659490    Objective Loss 0.659490                                        LR 0.000002    Time 0.040087    
2023-01-05 16:48:45,968 - Epoch: [110][   30/   37]    Overall Loss 0.657633    Objective Loss 0.657633                                        LR 0.000002    Time 0.036697    
2023-01-05 16:48:46,163 - Epoch: [110][   37/   37]    Overall Loss 0.657187    Objective Loss 0.657187    Top1 61.297071    LR 0.000002    Time 0.035016    
2023-01-05 16:48:46,235 - --- validate (epoch=110)-----------
2023-01-05 16:48:46,235 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:46,472 - Epoch: [110][    5/    5]    Loss 0.673884    Top1 60.973282    
2023-01-05 16:48:46,528 - ==> Top1: 60.973    Loss: 0.674

2023-01-05 16:48:46,528 - ==> Confusion:
[[319 110   0]
 [299 320   0]
 [  0   0   0]]

2023-01-05 16:48:46,530 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:46,530 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:46,540 - 

2023-01-05 16:48:46,540 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:47,032 - Epoch: [111][   10/   37]    Overall Loss 0.652942    Objective Loss 0.652942                                        LR 0.000002    Time 0.049123    
2023-01-05 16:48:47,349 - Epoch: [111][   20/   37]    Overall Loss 0.655273    Objective Loss 0.655273                                        LR 0.000002    Time 0.040249    
2023-01-05 16:48:47,663 - Epoch: [111][   30/   37]    Overall Loss 0.656916    Objective Loss 0.656916                                        LR 0.000002    Time 0.037286    
2023-01-05 16:48:47,857 - Epoch: [111][   37/   37]    Overall Loss 0.656602    Objective Loss 0.656602    Top1 66.527197    LR 0.000002    Time 0.035477    
2023-01-05 16:48:47,925 - --- validate (epoch=111)-----------
2023-01-05 16:48:47,925 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:48,174 - Epoch: [111][    5/    5]    Loss 0.668360    Top1 60.496183    
2023-01-05 16:48:48,248 - ==> Top1: 60.496    Loss: 0.668

2023-01-05 16:48:48,248 - ==> Confusion:
[[328 101   0]
 [313 306   0]
 [  0   0   0]]

2023-01-05 16:48:48,250 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:48,250 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:48,259 - 

2023-01-05 16:48:48,259 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:48,763 - Epoch: [112][   10/   37]    Overall Loss 0.659018    Objective Loss 0.659018                                        LR 0.000002    Time 0.050251    
2023-01-05 16:48:49,075 - Epoch: [112][   20/   37]    Overall Loss 0.657848    Objective Loss 0.657848                                        LR 0.000002    Time 0.040694    
2023-01-05 16:48:49,378 - Epoch: [112][   30/   37]    Overall Loss 0.657534    Objective Loss 0.657534                                        LR 0.000002    Time 0.037225    
2023-01-05 16:48:49,572 - Epoch: [112][   37/   37]    Overall Loss 0.656269    Objective Loss 0.656269    Top1 61.297071    LR 0.000002    Time 0.035436    
2023-01-05 16:48:49,639 - --- validate (epoch=112)-----------
2023-01-05 16:48:49,640 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:49,892 - Epoch: [112][    5/    5]    Loss 0.669098    Top1 60.591603    
2023-01-05 16:48:49,967 - ==> Top1: 60.592    Loss: 0.669

2023-01-05 16:48:49,968 - ==> Confusion:
[[331  98   0]
 [315 304   0]
 [  0   0   0]]

2023-01-05 16:48:49,970 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:49,970 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:49,983 - 

2023-01-05 16:48:49,984 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:50,485 - Epoch: [113][   10/   37]    Overall Loss 0.653881    Objective Loss 0.653881                                        LR 0.000002    Time 0.050043    
2023-01-05 16:48:50,798 - Epoch: [113][   20/   37]    Overall Loss 0.654511    Objective Loss 0.654511                                        LR 0.000002    Time 0.040636    
2023-01-05 16:48:51,116 - Epoch: [113][   30/   37]    Overall Loss 0.656430    Objective Loss 0.656430                                        LR 0.000002    Time 0.037686    
2023-01-05 16:48:51,310 - Epoch: [113][   37/   37]    Overall Loss 0.656190    Objective Loss 0.656190    Top1 61.087866    LR 0.000002    Time 0.035795    
2023-01-05 16:48:51,376 - --- validate (epoch=113)-----------
2023-01-05 16:48:51,376 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:51,617 - Epoch: [113][    5/    5]    Loss 0.674971    Top1 60.591603    
2023-01-05 16:48:51,686 - ==> Top1: 60.592    Loss: 0.675

2023-01-05 16:48:51,686 - ==> Confusion:
[[327 102   0]
 [311 308   0]
 [  0   0   0]]

2023-01-05 16:48:51,687 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:51,687 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:51,697 - 

2023-01-05 16:48:51,697 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:52,195 - Epoch: [114][   10/   37]    Overall Loss 0.655879    Objective Loss 0.655879                                        LR 0.000002    Time 0.049638    
2023-01-05 16:48:52,489 - Epoch: [114][   20/   37]    Overall Loss 0.654452    Objective Loss 0.654452                                        LR 0.000002    Time 0.039528    
2023-01-05 16:48:52,802 - Epoch: [114][   30/   37]    Overall Loss 0.654785    Objective Loss 0.654785                                        LR 0.000002    Time 0.036651    
2023-01-05 16:48:52,996 - Epoch: [114][   37/   37]    Overall Loss 0.655249    Objective Loss 0.655249    Top1 64.225941    LR 0.000002    Time 0.034946    
2023-01-05 16:48:53,064 - --- validate (epoch=114)-----------
2023-01-05 16:48:53,064 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:53,315 - Epoch: [114][    5/    5]    Loss 0.665309    Top1 61.259542    
2023-01-05 16:48:53,380 - ==> Top1: 61.260    Loss: 0.665

2023-01-05 16:48:53,380 - ==> Confusion:
[[317 112   0]
 [294 325   0]
 [  0   0   0]]

2023-01-05 16:48:53,382 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:53,382 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:53,392 - 

2023-01-05 16:48:53,392 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:53,880 - Epoch: [115][   10/   37]    Overall Loss 0.656296    Objective Loss 0.656296                                        LR 0.000002    Time 0.048716    
2023-01-05 16:48:54,176 - Epoch: [115][   20/   37]    Overall Loss 0.654161    Objective Loss 0.654161                                        LR 0.000002    Time 0.039120    
2023-01-05 16:48:54,470 - Epoch: [115][   30/   37]    Overall Loss 0.655366    Objective Loss 0.655366                                        LR 0.000002    Time 0.035864    
2023-01-05 16:48:54,664 - Epoch: [115][   37/   37]    Overall Loss 0.655131    Objective Loss 0.655131    Top1 60.878661    LR 0.000002    Time 0.034324    
2023-01-05 16:48:54,745 - --- validate (epoch=115)-----------
2023-01-05 16:48:54,745 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:54,994 - Epoch: [115][    5/    5]    Loss 0.666976    Top1 60.782443    
2023-01-05 16:48:55,059 - ==> Top1: 60.782    Loss: 0.667

2023-01-05 16:48:55,059 - ==> Confusion:
[[322 107   0]
 [304 315   0]
 [  0   0   0]]

2023-01-05 16:48:55,060 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:55,061 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:55,070 - 

2023-01-05 16:48:55,070 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:55,712 - Epoch: [116][   10/   37]    Overall Loss 0.655176    Objective Loss 0.655176                                        LR 0.000002    Time 0.064129    
2023-01-05 16:48:56,010 - Epoch: [116][   20/   37]    Overall Loss 0.656548    Objective Loss 0.656548                                        LR 0.000002    Time 0.046952    
2023-01-05 16:48:56,319 - Epoch: [116][   30/   37]    Overall Loss 0.653969    Objective Loss 0.653969                                        LR 0.000002    Time 0.041573    
2023-01-05 16:48:56,514 - Epoch: [116][   37/   37]    Overall Loss 0.654729    Objective Loss 0.654729    Top1 62.970711    LR 0.000002    Time 0.038982    
2023-01-05 16:48:56,600 - --- validate (epoch=116)-----------
2023-01-05 16:48:56,600 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:56,857 - Epoch: [116][    5/    5]    Loss 0.659159    Top1 60.973282    
2023-01-05 16:48:56,932 - ==> Top1: 60.973    Loss: 0.659

2023-01-05 16:48:56,932 - ==> Confusion:
[[316 113   0]
 [296 323   0]
 [  0   0   0]]

2023-01-05 16:48:56,934 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:56,934 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:56,944 - 

2023-01-05 16:48:56,944 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:57,426 - Epoch: [117][   10/   37]    Overall Loss 0.656503    Objective Loss 0.656503                                        LR 0.000002    Time 0.048077    
2023-01-05 16:48:57,730 - Epoch: [117][   20/   37]    Overall Loss 0.657272    Objective Loss 0.657272                                        LR 0.000002    Time 0.039205    
2023-01-05 16:48:58,036 - Epoch: [117][   30/   37]    Overall Loss 0.654885    Objective Loss 0.654885                                        LR 0.000002    Time 0.036334    
2023-01-05 16:48:58,232 - Epoch: [117][   37/   37]    Overall Loss 0.654355    Objective Loss 0.654355    Top1 59.205021    LR 0.000002    Time 0.034743    
2023-01-05 16:48:58,315 - --- validate (epoch=117)-----------
2023-01-05 16:48:58,315 - 1048 samples (256 per mini-batch)
2023-01-05 16:48:58,565 - Epoch: [117][    5/    5]    Loss 0.664641    Top1 61.068702    
2023-01-05 16:48:58,626 - ==> Top1: 61.069    Loss: 0.665

2023-01-05 16:48:58,627 - ==> Confusion:
[[318 111   0]
 [297 322   0]
 [  0   0   0]]

2023-01-05 16:48:58,628 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:48:58,629 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:48:58,649 - 

2023-01-05 16:48:58,649 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:48:59,153 - Epoch: [118][   10/   37]    Overall Loss 0.653516    Objective Loss 0.653516                                        LR 0.000002    Time 0.050289    
2023-01-05 16:48:59,451 - Epoch: [118][   20/   37]    Overall Loss 0.653057    Objective Loss 0.653057                                        LR 0.000002    Time 0.040014    
2023-01-05 16:48:59,756 - Epoch: [118][   30/   37]    Overall Loss 0.652892    Objective Loss 0.652892                                        LR 0.000002    Time 0.036825    
2023-01-05 16:48:59,950 - Epoch: [118][   37/   37]    Overall Loss 0.654159    Objective Loss 0.654159    Top1 57.949791    LR 0.000002    Time 0.035104    
2023-01-05 16:49:00,038 - --- validate (epoch=118)-----------
2023-01-05 16:49:00,038 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:00,282 - Epoch: [118][    5/    5]    Loss 0.671982    Top1 60.687023    
2023-01-05 16:49:00,344 - ==> Top1: 60.687    Loss: 0.672

2023-01-05 16:49:00,345 - ==> Confusion:
[[323 106   0]
 [306 313   0]
 [  0   0   0]]

2023-01-05 16:49:00,346 - ==> Best [Top1: 61.927   Sparsity:0.00   Params: 361664 on epoch: 12]
2023-01-05 16:49:00,346 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:00,356 - 

2023-01-05 16:49:00,356 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:00,837 - Epoch: [119][   10/   37]    Overall Loss 0.646206    Objective Loss 0.646206                                        LR 0.000002    Time 0.048010    
2023-01-05 16:49:01,123 - Epoch: [119][   20/   37]    Overall Loss 0.651526    Objective Loss 0.651526                                        LR 0.000002    Time 0.038131    
2023-01-05 16:49:01,420 - Epoch: [119][   30/   37]    Overall Loss 0.652975    Objective Loss 0.652975                                        LR 0.000002    Time 0.035313    
2023-01-05 16:49:01,617 - Epoch: [119][   37/   37]    Overall Loss 0.653704    Objective Loss 0.653704    Top1 57.112971    LR 0.000002    Time 0.033930    
2023-01-05 16:49:01,698 - --- validate (epoch=119)-----------
2023-01-05 16:49:01,698 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:01,951 - Epoch: [119][    5/    5]    Loss 0.661655    Top1 62.118321    
2023-01-05 16:49:02,017 - ==> Top1: 62.118    Loss: 0.662

2023-01-05 16:49:02,017 - ==> Confusion:
[[303 126   0]
 [271 348   0]
 [  0   0   0]]

2023-01-05 16:49:02,019 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:02,019 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:02,040 - 

2023-01-05 16:49:02,040 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:02,538 - Epoch: [120][   10/   37]    Overall Loss 0.655340    Objective Loss 0.655340                                        LR 0.000002    Time 0.049777    
2023-01-05 16:49:02,848 - Epoch: [120][   20/   37]    Overall Loss 0.653371    Objective Loss 0.653371                                        LR 0.000002    Time 0.040335    
2023-01-05 16:49:03,159 - Epoch: [120][   30/   37]    Overall Loss 0.653881    Objective Loss 0.653881                                        LR 0.000002    Time 0.037235    
2023-01-05 16:49:03,354 - Epoch: [120][   37/   37]    Overall Loss 0.653396    Objective Loss 0.653396    Top1 65.899582    LR 0.000002    Time 0.035459    
2023-01-05 16:49:03,423 - --- validate (epoch=120)-----------
2023-01-05 16:49:03,424 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:03,672 - Epoch: [120][    5/    5]    Loss 0.668984    Top1 60.973282    
2023-01-05 16:49:03,736 - ==> Top1: 60.973    Loss: 0.669

2023-01-05 16:49:03,736 - ==> Confusion:
[[323 106   0]
 [303 316   0]
 [  0   0   0]]

2023-01-05 16:49:03,738 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:03,738 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:03,748 - 

2023-01-05 16:49:03,748 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:04,246 - Epoch: [121][   10/   37]    Overall Loss 0.651573    Objective Loss 0.651573                                        LR 0.000002    Time 0.049770    
2023-01-05 16:49:04,553 - Epoch: [121][   20/   37]    Overall Loss 0.654668    Objective Loss 0.654668                                        LR 0.000002    Time 0.040183    
2023-01-05 16:49:04,872 - Epoch: [121][   30/   37]    Overall Loss 0.654713    Objective Loss 0.654713                                        LR 0.000002    Time 0.037431    
2023-01-05 16:49:05,069 - Epoch: [121][   37/   37]    Overall Loss 0.652893    Objective Loss 0.652893    Top1 61.087866    LR 0.000002    Time 0.035659    
2023-01-05 16:49:05,141 - --- validate (epoch=121)-----------
2023-01-05 16:49:05,141 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:05,389 - Epoch: [121][    5/    5]    Loss 0.659540    Top1 61.545802    
2023-01-05 16:49:05,462 - ==> Top1: 61.546    Loss: 0.660

2023-01-05 16:49:05,462 - ==> Confusion:
[[318 111   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:49:05,464 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:05,464 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:05,478 - 

2023-01-05 16:49:05,478 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:06,010 - Epoch: [122][   10/   37]    Overall Loss 0.650558    Objective Loss 0.650558                                        LR 0.000002    Time 0.053107    
2023-01-05 16:49:06,341 - Epoch: [122][   20/   37]    Overall Loss 0.651421    Objective Loss 0.651421                                        LR 0.000002    Time 0.043088    
2023-01-05 16:49:06,693 - Epoch: [122][   30/   37]    Overall Loss 0.653478    Objective Loss 0.653478                                        LR 0.000002    Time 0.040424    
2023-01-05 16:49:06,894 - Epoch: [122][   37/   37]    Overall Loss 0.652570    Objective Loss 0.652570    Top1 62.552301    LR 0.000002    Time 0.038220    
2023-01-05 16:49:06,964 - --- validate (epoch=122)-----------
2023-01-05 16:49:06,964 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:07,215 - Epoch: [122][    5/    5]    Loss 0.659458    Top1 60.400763    
2023-01-05 16:49:07,289 - ==> Top1: 60.401    Loss: 0.659

2023-01-05 16:49:07,289 - ==> Confusion:
[[335  94   0]
 [321 298   0]
 [  0   0   0]]

2023-01-05 16:49:07,291 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:07,291 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:07,301 - 

2023-01-05 16:49:07,301 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:07,807 - Epoch: [123][   10/   37]    Overall Loss 0.655710    Objective Loss 0.655710                                        LR 0.000002    Time 0.050490    
2023-01-05 16:49:08,106 - Epoch: [123][   20/   37]    Overall Loss 0.653307    Objective Loss 0.653307                                        LR 0.000002    Time 0.040037    
2023-01-05 16:49:08,420 - Epoch: [123][   30/   37]    Overall Loss 0.652236    Objective Loss 0.652236                                        LR 0.000002    Time 0.037156    
2023-01-05 16:49:08,615 - Epoch: [123][   37/   37]    Overall Loss 0.652281    Objective Loss 0.652281    Top1 64.016736    LR 0.000002    Time 0.035384    
2023-01-05 16:49:08,682 - --- validate (epoch=123)-----------
2023-01-05 16:49:08,682 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:08,947 - Epoch: [123][    5/    5]    Loss 0.679380    Top1 60.496183    
2023-01-05 16:49:09,014 - ==> Top1: 60.496    Loss: 0.679

2023-01-05 16:49:09,014 - ==> Confusion:
[[336  93   0]
 [321 298   0]
 [  0   0   0]]

2023-01-05 16:49:09,016 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:09,016 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:09,025 - 

2023-01-05 16:49:09,026 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:09,539 - Epoch: [124][   10/   37]    Overall Loss 0.647567    Objective Loss 0.647567                                        LR 0.000002    Time 0.051234    
2023-01-05 16:49:09,852 - Epoch: [124][   20/   37]    Overall Loss 0.648709    Objective Loss 0.648709                                        LR 0.000002    Time 0.041250    
2023-01-05 16:49:10,175 - Epoch: [124][   30/   37]    Overall Loss 0.651921    Objective Loss 0.651921                                        LR 0.000002    Time 0.038165    
2023-01-05 16:49:10,374 - Epoch: [124][   37/   37]    Overall Loss 0.652164    Objective Loss 0.652164    Top1 61.924686    LR 0.000002    Time 0.036306    
2023-01-05 16:49:10,452 - --- validate (epoch=124)-----------
2023-01-05 16:49:10,452 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:10,697 - Epoch: [124][    5/    5]    Loss 0.653850    Top1 60.782443    
2023-01-05 16:49:10,766 - ==> Top1: 60.782    Loss: 0.654

2023-01-05 16:49:10,767 - ==> Confusion:
[[324 105   0]
 [306 313   0]
 [  0   0   0]]

2023-01-05 16:49:10,768 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:10,768 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:10,778 - 

2023-01-05 16:49:10,778 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:11,262 - Epoch: [125][   10/   37]    Overall Loss 0.652857    Objective Loss 0.652857                                        LR 0.000002    Time 0.048337    
2023-01-05 16:49:11,553 - Epoch: [125][   20/   37]    Overall Loss 0.653980    Objective Loss 0.653980                                        LR 0.000002    Time 0.038682    
2023-01-05 16:49:11,858 - Epoch: [125][   30/   37]    Overall Loss 0.652132    Objective Loss 0.652132                                        LR 0.000002    Time 0.035949    
2023-01-05 16:49:12,053 - Epoch: [125][   37/   37]    Overall Loss 0.651777    Objective Loss 0.651777    Top1 60.878661    LR 0.000002    Time 0.034397    
2023-01-05 16:49:12,124 - --- validate (epoch=125)-----------
2023-01-05 16:49:12,124 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:12,374 - Epoch: [125][    5/    5]    Loss 0.660347    Top1 60.400763    
2023-01-05 16:49:12,445 - ==> Top1: 60.401    Loss: 0.660

2023-01-05 16:49:12,445 - ==> Confusion:
[[328 101   0]
 [314 305   0]
 [  0   0   0]]

2023-01-05 16:49:12,447 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:12,447 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:12,456 - 

2023-01-05 16:49:12,457 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:12,953 - Epoch: [126][   10/   37]    Overall Loss 0.647804    Objective Loss 0.647804                                        LR 0.000002    Time 0.049574    
2023-01-05 16:49:13,241 - Epoch: [126][   20/   37]    Overall Loss 0.651001    Objective Loss 0.651001                                        LR 0.000002    Time 0.039155    
2023-01-05 16:49:13,547 - Epoch: [126][   30/   37]    Overall Loss 0.650822    Objective Loss 0.650822                                        LR 0.000002    Time 0.036297    
2023-01-05 16:49:13,744 - Epoch: [126][   37/   37]    Overall Loss 0.651207    Objective Loss 0.651207    Top1 58.577406    LR 0.000002    Time 0.034744    
2023-01-05 16:49:13,818 - --- validate (epoch=126)-----------
2023-01-05 16:49:13,818 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:14,067 - Epoch: [126][    5/    5]    Loss 0.649948    Top1 59.923664    
2023-01-05 16:49:14,134 - ==> Top1: 59.924    Loss: 0.650

2023-01-05 16:49:14,134 - ==> Confusion:
[[333  96   0]
 [324 295   0]
 [  0   0   0]]

2023-01-05 16:49:14,136 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:14,136 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:14,146 - 

2023-01-05 16:49:14,146 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:14,656 - Epoch: [127][   10/   37]    Overall Loss 0.651986    Objective Loss 0.651986                                        LR 0.000002    Time 0.050965    
2023-01-05 16:49:14,969 - Epoch: [127][   20/   37]    Overall Loss 0.653015    Objective Loss 0.653015                                        LR 0.000002    Time 0.041082    
2023-01-05 16:49:15,283 - Epoch: [127][   30/   37]    Overall Loss 0.650540    Objective Loss 0.650540                                        LR 0.000002    Time 0.037839    
2023-01-05 16:49:15,479 - Epoch: [127][   37/   37]    Overall Loss 0.650922    Objective Loss 0.650922    Top1 63.807531    LR 0.000002    Time 0.035962    
2023-01-05 16:49:15,558 - --- validate (epoch=127)-----------
2023-01-05 16:49:15,558 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:15,805 - Epoch: [127][    5/    5]    Loss 0.663630    Top1 61.545802    
2023-01-05 16:49:15,867 - ==> Top1: 61.546    Loss: 0.664

2023-01-05 16:49:15,868 - ==> Confusion:
[[324 105   0]
 [298 321   0]
 [  0   0   0]]

2023-01-05 16:49:15,869 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:15,869 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:15,879 - 

2023-01-05 16:49:15,879 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:16,363 - Epoch: [128][   10/   37]    Overall Loss 0.653128    Objective Loss 0.653128                                        LR 0.000002    Time 0.048308    
2023-01-05 16:49:16,660 - Epoch: [128][   20/   37]    Overall Loss 0.653271    Objective Loss 0.653271                                        LR 0.000002    Time 0.038807    
2023-01-05 16:49:16,960 - Epoch: [128][   30/   37]    Overall Loss 0.652375    Objective Loss 0.652375                                        LR 0.000002    Time 0.035875    
2023-01-05 16:49:17,156 - Epoch: [128][   37/   37]    Overall Loss 0.650143    Objective Loss 0.650143    Top1 65.481172    LR 0.000002    Time 0.034367    
2023-01-05 16:49:17,226 - --- validate (epoch=128)-----------
2023-01-05 16:49:17,226 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:17,481 - Epoch: [128][    5/    5]    Loss 0.652709    Top1 60.973282    
2023-01-05 16:49:17,542 - ==> Top1: 60.973    Loss: 0.653

2023-01-05 16:49:17,543 - ==> Confusion:
[[320 109   0]
 [300 319   0]
 [  0   0   0]]

2023-01-05 16:49:17,544 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:17,544 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:17,554 - 

2023-01-05 16:49:17,554 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:18,044 - Epoch: [129][   10/   37]    Overall Loss 0.652422    Objective Loss 0.652422                                        LR 0.000002    Time 0.048928    
2023-01-05 16:49:18,328 - Epoch: [129][   20/   37]    Overall Loss 0.651760    Objective Loss 0.651760                                        LR 0.000002    Time 0.038667    
2023-01-05 16:49:18,630 - Epoch: [129][   30/   37]    Overall Loss 0.650978    Objective Loss 0.650978                                        LR 0.000002    Time 0.035810    
2023-01-05 16:49:18,828 - Epoch: [129][   37/   37]    Overall Loss 0.649789    Objective Loss 0.649789    Top1 62.552301    LR 0.000002    Time 0.034383    
2023-01-05 16:49:18,895 - --- validate (epoch=129)-----------
2023-01-05 16:49:18,895 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:19,139 - Epoch: [129][    5/    5]    Loss 0.672481    Top1 60.305344    
2023-01-05 16:49:19,201 - ==> Top1: 60.305    Loss: 0.672

2023-01-05 16:49:19,202 - ==> Confusion:
[[328 101   0]
 [315 304   0]
 [  0   0   0]]

2023-01-05 16:49:19,203 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:19,203 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:19,213 - 

2023-01-05 16:49:19,213 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:19,713 - Epoch: [130][   10/   37]    Overall Loss 0.649696    Objective Loss 0.649696                                        LR 0.000002    Time 0.049950    
2023-01-05 16:49:20,013 - Epoch: [130][   20/   37]    Overall Loss 0.646743    Objective Loss 0.646743                                        LR 0.000002    Time 0.039916    
2023-01-05 16:49:20,319 - Epoch: [130][   30/   37]    Overall Loss 0.650029    Objective Loss 0.650029                                        LR 0.000002    Time 0.036797    
2023-01-05 16:49:20,515 - Epoch: [130][   37/   37]    Overall Loss 0.649263    Objective Loss 0.649263    Top1 62.133891    LR 0.000002    Time 0.035129    
2023-01-05 16:49:20,594 - --- validate (epoch=130)-----------
2023-01-05 16:49:20,594 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:20,853 - Epoch: [130][    5/    5]    Loss 0.656413    Top1 61.927481    
2023-01-05 16:49:20,922 - ==> Top1: 61.927    Loss: 0.656

2023-01-05 16:49:20,922 - ==> Confusion:
[[305 124   0]
 [275 344   0]
 [  0   0   0]]

2023-01-05 16:49:20,924 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:20,924 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:20,934 - 

2023-01-05 16:49:20,934 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:21,402 - Epoch: [131][   10/   37]    Overall Loss 0.648192    Objective Loss 0.648192                                        LR 0.000002    Time 0.046782    
2023-01-05 16:49:21,690 - Epoch: [131][   20/   37]    Overall Loss 0.648701    Objective Loss 0.648701                                        LR 0.000002    Time 0.037743    
2023-01-05 16:49:21,988 - Epoch: [131][   30/   37]    Overall Loss 0.648259    Objective Loss 0.648259                                        LR 0.000002    Time 0.035071    
2023-01-05 16:49:22,184 - Epoch: [131][   37/   37]    Overall Loss 0.649043    Objective Loss 0.649043    Top1 62.761506    LR 0.000002    Time 0.033734    
2023-01-05 16:49:22,251 - --- validate (epoch=131)-----------
2023-01-05 16:49:22,251 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:22,496 - Epoch: [131][    5/    5]    Loss 0.660892    Top1 61.259542    
2023-01-05 16:49:22,557 - ==> Top1: 61.260    Loss: 0.661

2023-01-05 16:49:22,557 - ==> Confusion:
[[321 108   0]
 [298 321   0]
 [  0   0   0]]

2023-01-05 16:49:22,558 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:22,559 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:22,568 - 

2023-01-05 16:49:22,568 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:23,189 - Epoch: [132][   10/   37]    Overall Loss 0.649200    Objective Loss 0.649200                                        LR 0.000002    Time 0.062014    
2023-01-05 16:49:23,499 - Epoch: [132][   20/   37]    Overall Loss 0.646869    Objective Loss 0.646869                                        LR 0.000002    Time 0.046488    
2023-01-05 16:49:23,812 - Epoch: [132][   30/   37]    Overall Loss 0.650411    Objective Loss 0.650411                                        LR 0.000002    Time 0.041391    
2023-01-05 16:49:24,007 - Epoch: [132][   37/   37]    Overall Loss 0.648670    Objective Loss 0.648670    Top1 65.271967    LR 0.000002    Time 0.038817    
2023-01-05 16:49:24,088 - --- validate (epoch=132)-----------
2023-01-05 16:49:24,089 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:24,347 - Epoch: [132][    5/    5]    Loss 0.654828    Top1 61.068702    
2023-01-05 16:49:24,420 - ==> Top1: 61.069    Loss: 0.655

2023-01-05 16:49:24,420 - ==> Confusion:
[[325 104   0]
 [304 315   0]
 [  0   0   0]]

2023-01-05 16:49:24,422 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:24,422 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:24,432 - 

2023-01-05 16:49:24,432 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:24,916 - Epoch: [133][   10/   37]    Overall Loss 0.643666    Objective Loss 0.643666                                        LR 0.000002    Time 0.048331    
2023-01-05 16:49:25,195 - Epoch: [133][   20/   37]    Overall Loss 0.646137    Objective Loss 0.646137                                        LR 0.000002    Time 0.037989    
2023-01-05 16:49:25,479 - Epoch: [133][   30/   37]    Overall Loss 0.648438    Objective Loss 0.648438                                        LR 0.000002    Time 0.034784    
2023-01-05 16:49:25,666 - Epoch: [133][   37/   37]    Overall Loss 0.648238    Objective Loss 0.648238    Top1 60.669456    LR 0.000002    Time 0.033254    
2023-01-05 16:49:25,742 - --- validate (epoch=133)-----------
2023-01-05 16:49:25,742 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:25,984 - Epoch: [133][    5/    5]    Loss 0.661066    Top1 61.545802    
2023-01-05 16:49:26,052 - ==> Top1: 61.546    Loss: 0.661

2023-01-05 16:49:26,053 - ==> Confusion:
[[315 114   0]
 [289 330   0]
 [  0   0   0]]

2023-01-05 16:49:26,054 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:26,054 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:26,064 - 

2023-01-05 16:49:26,064 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:26,569 - Epoch: [134][   10/   37]    Overall Loss 0.648279    Objective Loss 0.648279                                        LR 0.000002    Time 0.050366    
2023-01-05 16:49:26,868 - Epoch: [134][   20/   37]    Overall Loss 0.648121    Objective Loss 0.648121                                        LR 0.000002    Time 0.040086    
2023-01-05 16:49:27,166 - Epoch: [134][   30/   37]    Overall Loss 0.646715    Objective Loss 0.646715                                        LR 0.000002    Time 0.036656    
2023-01-05 16:49:27,362 - Epoch: [134][   37/   37]    Overall Loss 0.647937    Objective Loss 0.647937    Top1 62.552301    LR 0.000002    Time 0.035026    
2023-01-05 16:49:27,434 - --- validate (epoch=134)-----------
2023-01-05 16:49:27,434 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:27,681 - Epoch: [134][    5/    5]    Loss 0.648739    Top1 61.545802    
2023-01-05 16:49:27,747 - ==> Top1: 61.546    Loss: 0.649

2023-01-05 16:49:27,748 - ==> Confusion:
[[319 110   0]
 [293 326   0]
 [  0   0   0]]

2023-01-05 16:49:27,749 - ==> Best [Top1: 62.118   Sparsity:0.00   Params: 361664 on epoch: 119]
2023-01-05 16:49:27,749 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:27,759 - 

2023-01-05 16:49:27,759 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:28,255 - Epoch: [135][   10/   37]    Overall Loss 0.651261    Objective Loss 0.651261                                        LR 0.000002    Time 0.049532    
2023-01-05 16:49:28,548 - Epoch: [135][   20/   37]    Overall Loss 0.647693    Objective Loss 0.647693                                        LR 0.000002    Time 0.039405    
2023-01-05 16:49:28,846 - Epoch: [135][   30/   37]    Overall Loss 0.646851    Objective Loss 0.646851                                        LR 0.000002    Time 0.036177    
2023-01-05 16:49:29,043 - Epoch: [135][   37/   37]    Overall Loss 0.647739    Objective Loss 0.647739    Top1 62.552301    LR 0.000002    Time 0.034643    
2023-01-05 16:49:29,112 - --- validate (epoch=135)-----------
2023-01-05 16:49:29,112 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:29,361 - Epoch: [135][    5/    5]    Loss 0.660235    Top1 62.213740    
2023-01-05 16:49:29,426 - ==> Top1: 62.214    Loss: 0.660

2023-01-05 16:49:29,426 - ==> Confusion:
[[308 121   0]
 [275 344   0]
 [  0   0   0]]

2023-01-05 16:49:29,428 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:29,428 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:29,448 - 

2023-01-05 16:49:29,449 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:29,944 - Epoch: [136][   10/   37]    Overall Loss 0.648049    Objective Loss 0.648049                                        LR 0.000002    Time 0.049518    
2023-01-05 16:49:30,237 - Epoch: [136][   20/   37]    Overall Loss 0.647432    Objective Loss 0.647432                                        LR 0.000002    Time 0.039346    
2023-01-05 16:49:30,545 - Epoch: [136][   30/   37]    Overall Loss 0.647001    Objective Loss 0.647001                                        LR 0.000002    Time 0.036508    
2023-01-05 16:49:30,741 - Epoch: [136][   37/   37]    Overall Loss 0.647057    Objective Loss 0.647057    Top1 64.016736    LR 0.000002    Time 0.034891    
2023-01-05 16:49:30,818 - --- validate (epoch=136)-----------
2023-01-05 16:49:30,818 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:31,059 - Epoch: [136][    5/    5]    Loss 0.662812    Top1 60.782443    
2023-01-05 16:49:31,125 - ==> Top1: 60.782    Loss: 0.663

2023-01-05 16:49:31,125 - ==> Confusion:
[[327 102   0]
 [309 310   0]
 [  0   0   0]]

2023-01-05 16:49:31,126 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:31,127 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:31,136 - 

2023-01-05 16:49:31,136 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:31,626 - Epoch: [137][   10/   37]    Overall Loss 0.647649    Objective Loss 0.647649                                        LR 0.000002    Time 0.048838    
2023-01-05 16:49:31,922 - Epoch: [137][   20/   37]    Overall Loss 0.645821    Objective Loss 0.645821                                        LR 0.000002    Time 0.039223    
2023-01-05 16:49:32,226 - Epoch: [137][   30/   37]    Overall Loss 0.646676    Objective Loss 0.646676                                        LR 0.000002    Time 0.036257    
2023-01-05 16:49:32,422 - Epoch: [137][   37/   37]    Overall Loss 0.646612    Objective Loss 0.646612    Top1 64.016736    LR 0.000002    Time 0.034678    
2023-01-05 16:49:32,508 - --- validate (epoch=137)-----------
2023-01-05 16:49:32,508 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:32,755 - Epoch: [137][    5/    5]    Loss 0.652753    Top1 61.641221    
2023-01-05 16:49:32,823 - ==> Top1: 61.641    Loss: 0.653

2023-01-05 16:49:32,824 - ==> Confusion:
[[319 110   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:49:32,825 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:32,825 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:32,835 - 

2023-01-05 16:49:32,835 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:33,342 - Epoch: [138][   10/   37]    Overall Loss 0.644781    Objective Loss 0.644781                                        LR 0.000002    Time 0.050639    
2023-01-05 16:49:33,655 - Epoch: [138][   20/   37]    Overall Loss 0.645493    Objective Loss 0.645493                                        LR 0.000002    Time 0.040915    
2023-01-05 16:49:33,969 - Epoch: [138][   30/   37]    Overall Loss 0.646469    Objective Loss 0.646469                                        LR 0.000002    Time 0.037736    
2023-01-05 16:49:34,165 - Epoch: [138][   37/   37]    Overall Loss 0.646331    Objective Loss 0.646331    Top1 61.297071    LR 0.000002    Time 0.035878    
2023-01-05 16:49:34,242 - --- validate (epoch=138)-----------
2023-01-05 16:49:34,242 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:34,485 - Epoch: [138][    5/    5]    Loss 0.653696    Top1 61.927481    
2023-01-05 16:49:34,540 - ==> Top1: 61.927    Loss: 0.654

2023-01-05 16:49:34,541 - ==> Confusion:
[[314 115   0]
 [284 335   0]
 [  0   0   0]]

2023-01-05 16:49:34,542 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:34,542 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:34,552 - 

2023-01-05 16:49:34,552 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:35,055 - Epoch: [139][   10/   37]    Overall Loss 0.646864    Objective Loss 0.646864                                        LR 0.000002    Time 0.050195    
2023-01-05 16:49:35,357 - Epoch: [139][   20/   37]    Overall Loss 0.647087    Objective Loss 0.647087                                        LR 0.000002    Time 0.040188    
2023-01-05 16:49:35,673 - Epoch: [139][   30/   37]    Overall Loss 0.646614    Objective Loss 0.646614                                        LR 0.000002    Time 0.037242    
2023-01-05 16:49:35,870 - Epoch: [139][   37/   37]    Overall Loss 0.645737    Objective Loss 0.645737    Top1 65.062762    LR 0.000002    Time 0.035508    
2023-01-05 16:49:35,945 - --- validate (epoch=139)-----------
2023-01-05 16:49:35,946 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:36,195 - Epoch: [139][    5/    5]    Loss 0.659685    Top1 61.354962    
2023-01-05 16:49:36,280 - ==> Top1: 61.355    Loss: 0.660

2023-01-05 16:49:36,280 - ==> Confusion:
[[324 105   0]
 [300 319   0]
 [  0   0   0]]

2023-01-05 16:49:36,282 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:36,282 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:36,291 - 

2023-01-05 16:49:36,291 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:36,786 - Epoch: [140][   10/   37]    Overall Loss 0.648539    Objective Loss 0.648539                                        LR 0.000001    Time 0.049416    
2023-01-05 16:49:37,096 - Epoch: [140][   20/   37]    Overall Loss 0.645509    Objective Loss 0.645509                                        LR 0.000001    Time 0.040176    
2023-01-05 16:49:37,419 - Epoch: [140][   30/   37]    Overall Loss 0.646432    Objective Loss 0.646432                                        LR 0.000001    Time 0.037519    
2023-01-05 16:49:37,616 - Epoch: [140][   37/   37]    Overall Loss 0.645786    Objective Loss 0.645786    Top1 63.179916    LR 0.000001    Time 0.035759    
2023-01-05 16:49:37,687 - --- validate (epoch=140)-----------
2023-01-05 16:49:37,688 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:37,930 - Epoch: [140][    5/    5]    Loss 0.662213    Top1 61.832061    
2023-01-05 16:49:38,002 - ==> Top1: 61.832    Loss: 0.662

2023-01-05 16:49:38,003 - ==> Confusion:
[[314 115   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:49:38,004 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:38,004 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:38,014 - 

2023-01-05 16:49:38,014 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:38,513 - Epoch: [141][   10/   37]    Overall Loss 0.641808    Objective Loss 0.641808                                        LR 0.000001    Time 0.049782    
2023-01-05 16:49:38,801 - Epoch: [141][   20/   37]    Overall Loss 0.640543    Objective Loss 0.640543                                        LR 0.000001    Time 0.039296    
2023-01-05 16:49:39,104 - Epoch: [141][   30/   37]    Overall Loss 0.645497    Objective Loss 0.645497                                        LR 0.000001    Time 0.036280    
2023-01-05 16:49:39,299 - Epoch: [141][   37/   37]    Overall Loss 0.645255    Objective Loss 0.645255    Top1 64.853556    LR 0.000001    Time 0.034667    
2023-01-05 16:49:39,369 - --- validate (epoch=141)-----------
2023-01-05 16:49:39,369 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:39,614 - Epoch: [141][    5/    5]    Loss 0.661250    Top1 61.545802    
2023-01-05 16:49:39,666 - ==> Top1: 61.546    Loss: 0.661

2023-01-05 16:49:39,666 - ==> Confusion:
[[323 106   0]
 [297 322   0]
 [  0   0   0]]

2023-01-05 16:49:39,668 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:39,668 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:39,678 - 

2023-01-05 16:49:39,678 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:40,149 - Epoch: [142][   10/   37]    Overall Loss 0.642642    Objective Loss 0.642642                                        LR 0.000001    Time 0.047042    
2023-01-05 16:49:40,427 - Epoch: [142][   20/   37]    Overall Loss 0.646879    Objective Loss 0.646879                                        LR 0.000001    Time 0.037303    
2023-01-05 16:49:40,721 - Epoch: [142][   30/   37]    Overall Loss 0.646332    Objective Loss 0.646332                                        LR 0.000001    Time 0.034648    
2023-01-05 16:49:40,908 - Epoch: [142][   37/   37]    Overall Loss 0.645110    Objective Loss 0.645110    Top1 63.807531    LR 0.000001    Time 0.033137    
2023-01-05 16:49:40,978 - --- validate (epoch=142)-----------
2023-01-05 16:49:40,978 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:41,217 - Epoch: [142][    5/    5]    Loss 0.669250    Top1 61.832061    
2023-01-05 16:49:41,294 - ==> Top1: 61.832    Loss: 0.669

2023-01-05 16:49:41,294 - ==> Confusion:
[[314 115   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:49:41,295 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:41,296 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:41,305 - 

2023-01-05 16:49:41,305 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:41,787 - Epoch: [143][   10/   37]    Overall Loss 0.640723    Objective Loss 0.640723                                        LR 0.000001    Time 0.048154    
2023-01-05 16:49:42,076 - Epoch: [143][   20/   37]    Overall Loss 0.642873    Objective Loss 0.642873                                        LR 0.000001    Time 0.038471    
2023-01-05 16:49:42,373 - Epoch: [143][   30/   37]    Overall Loss 0.644435    Objective Loss 0.644435                                        LR 0.000001    Time 0.035532    
2023-01-05 16:49:42,562 - Epoch: [143][   37/   37]    Overall Loss 0.644937    Objective Loss 0.644937    Top1 64.016736    LR 0.000001    Time 0.033912    
2023-01-05 16:49:42,636 - --- validate (epoch=143)-----------
2023-01-05 16:49:42,636 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:42,889 - Epoch: [143][    5/    5]    Loss 0.654708    Top1 61.450382    
2023-01-05 16:49:42,959 - ==> Top1: 61.450    Loss: 0.655

2023-01-05 16:49:42,959 - ==> Confusion:
[[323 106   0]
 [298 321   0]
 [  0   0   0]]

2023-01-05 16:49:42,961 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:42,961 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:42,970 - 

2023-01-05 16:49:42,970 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:43,458 - Epoch: [144][   10/   37]    Overall Loss 0.641568    Objective Loss 0.641568                                        LR 0.000001    Time 0.048703    
2023-01-05 16:49:43,749 - Epoch: [144][   20/   37]    Overall Loss 0.642207    Objective Loss 0.642207                                        LR 0.000001    Time 0.038858    
2023-01-05 16:49:44,043 - Epoch: [144][   30/   37]    Overall Loss 0.643149    Objective Loss 0.643149                                        LR 0.000001    Time 0.035704    
2023-01-05 16:49:44,240 - Epoch: [144][   37/   37]    Overall Loss 0.644650    Objective Loss 0.644650    Top1 60.878661    LR 0.000001    Time 0.034253    
2023-01-05 16:49:44,307 - --- validate (epoch=144)-----------
2023-01-05 16:49:44,308 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:44,547 - Epoch: [144][    5/    5]    Loss 0.645506    Top1 61.832061    
2023-01-05 16:49:44,611 - ==> Top1: 61.832    Loss: 0.646

2023-01-05 16:49:44,611 - ==> Confusion:
[[317 112   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:49:44,612 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:44,612 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:44,622 - 

2023-01-05 16:49:44,622 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:45,123 - Epoch: [145][   10/   37]    Overall Loss 0.646467    Objective Loss 0.646467                                        LR 0.000001    Time 0.050003    
2023-01-05 16:49:45,427 - Epoch: [145][   20/   37]    Overall Loss 0.646944    Objective Loss 0.646944                                        LR 0.000001    Time 0.040153    
2023-01-05 16:49:45,738 - Epoch: [145][   30/   37]    Overall Loss 0.645641    Objective Loss 0.645641                                        LR 0.000001    Time 0.037149    
2023-01-05 16:49:45,934 - Epoch: [145][   37/   37]    Overall Loss 0.644275    Objective Loss 0.644275    Top1 63.179916    LR 0.000001    Time 0.035397    
2023-01-05 16:49:46,010 - --- validate (epoch=145)-----------
2023-01-05 16:49:46,010 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:46,267 - Epoch: [145][    5/    5]    Loss 0.648801    Top1 61.832061    
2023-01-05 16:49:46,327 - ==> Top1: 61.832    Loss: 0.649

2023-01-05 16:49:46,328 - ==> Confusion:
[[313 116   0]
 [284 335   0]
 [  0   0   0]]

2023-01-05 16:49:46,329 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:46,329 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:46,339 - 

2023-01-05 16:49:46,339 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:46,819 - Epoch: [146][   10/   37]    Overall Loss 0.641977    Objective Loss 0.641977                                        LR 0.000001    Time 0.047919    
2023-01-05 16:49:47,112 - Epoch: [146][   20/   37]    Overall Loss 0.643150    Objective Loss 0.643150                                        LR 0.000001    Time 0.038586    
2023-01-05 16:49:47,418 - Epoch: [146][   30/   37]    Overall Loss 0.643911    Objective Loss 0.643911                                        LR 0.000001    Time 0.035926    
2023-01-05 16:49:47,616 - Epoch: [146][   37/   37]    Overall Loss 0.644174    Objective Loss 0.644174    Top1 61.715481    LR 0.000001    Time 0.034476    
2023-01-05 16:49:47,689 - --- validate (epoch=146)-----------
2023-01-05 16:49:47,689 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:47,937 - Epoch: [146][    5/    5]    Loss 0.669675    Top1 61.736641    
2023-01-05 16:49:47,993 - ==> Top1: 61.737    Loss: 0.670

2023-01-05 16:49:47,993 - ==> Confusion:
[[318 111   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:49:47,995 - ==> Best [Top1: 62.214   Sparsity:0.00   Params: 361664 on epoch: 135]
2023-01-05 16:49:47,995 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:48,004 - 

2023-01-05 16:49:48,005 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:48,508 - Epoch: [147][   10/   37]    Overall Loss 0.645790    Objective Loss 0.645790                                        LR 0.000001    Time 0.050281    
2023-01-05 16:49:48,806 - Epoch: [147][   20/   37]    Overall Loss 0.645035    Objective Loss 0.645035                                        LR 0.000001    Time 0.040010    
2023-01-05 16:49:49,113 - Epoch: [147][   30/   37]    Overall Loss 0.644476    Objective Loss 0.644476                                        LR 0.000001    Time 0.036905    
2023-01-05 16:49:49,307 - Epoch: [147][   37/   37]    Overall Loss 0.643954    Objective Loss 0.643954    Top1 61.297071    LR 0.000001    Time 0.035158    
2023-01-05 16:49:49,375 - --- validate (epoch=147)-----------
2023-01-05 16:49:49,375 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:49,635 - Epoch: [147][    5/    5]    Loss 0.650065    Top1 62.500000    
2023-01-05 16:49:49,697 - ==> Top1: 62.500    Loss: 0.650

2023-01-05 16:49:49,697 - ==> Confusion:
[[312 117   0]
 [276 343   0]
 [  0   0   0]]

2023-01-05 16:49:49,699 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 147]
2023-01-05 16:49:49,699 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:49,719 - 

2023-01-05 16:49:49,719 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:50,354 - Epoch: [148][   10/   37]    Overall Loss 0.642619    Objective Loss 0.642619                                        LR 0.000001    Time 0.063384    
2023-01-05 16:49:50,647 - Epoch: [148][   20/   37]    Overall Loss 0.645230    Objective Loss 0.645230                                        LR 0.000001    Time 0.046324    
2023-01-05 16:49:50,959 - Epoch: [148][   30/   37]    Overall Loss 0.644224    Objective Loss 0.644224                                        LR 0.000001    Time 0.041249    
2023-01-05 16:49:51,155 - Epoch: [148][   37/   37]    Overall Loss 0.643633    Objective Loss 0.643633    Top1 64.853556    LR 0.000001    Time 0.038740    
2023-01-05 16:49:51,224 - --- validate (epoch=148)-----------
2023-01-05 16:49:51,225 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:51,474 - Epoch: [148][    5/    5]    Loss 0.653196    Top1 62.500000    
2023-01-05 16:49:51,541 - ==> Top1: 62.500    Loss: 0.653

2023-01-05 16:49:51,541 - ==> Confusion:
[[307 122   0]
 [271 348   0]
 [  0   0   0]]

2023-01-05 16:49:51,542 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:51,543 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:51,568 - 

2023-01-05 16:49:51,568 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:52,075 - Epoch: [149][   10/   37]    Overall Loss 0.649188    Objective Loss 0.649188                                        LR 0.000001    Time 0.050655    
2023-01-05 16:49:52,364 - Epoch: [149][   20/   37]    Overall Loss 0.646724    Objective Loss 0.646724                                        LR 0.000001    Time 0.039728    
2023-01-05 16:49:52,667 - Epoch: [149][   30/   37]    Overall Loss 0.644480    Objective Loss 0.644480                                        LR 0.000001    Time 0.036582    
2023-01-05 16:49:52,865 - Epoch: [149][   37/   37]    Overall Loss 0.643553    Objective Loss 0.643553    Top1 66.317992    LR 0.000001    Time 0.034998    
2023-01-05 16:49:52,937 - --- validate (epoch=149)-----------
2023-01-05 16:49:52,937 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:53,181 - Epoch: [149][    5/    5]    Loss 0.648764    Top1 61.927481    
2023-01-05 16:49:53,246 - ==> Top1: 61.927    Loss: 0.649

2023-01-05 16:49:53,246 - ==> Confusion:
[[322 107   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:49:53,247 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:53,248 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:53,257 - 

2023-01-05 16:49:53,257 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:53,744 - Epoch: [150][   10/   37]    Overall Loss 0.643485    Objective Loss 0.643485                                        LR 0.000001    Time 0.048619    
2023-01-05 16:49:54,024 - Epoch: [150][   20/   37]    Overall Loss 0.643224    Objective Loss 0.643224                                        LR 0.000001    Time 0.038251    
2023-01-05 16:49:54,325 - Epoch: [150][   30/   37]    Overall Loss 0.643159    Objective Loss 0.643159                                        LR 0.000001    Time 0.035488    
2023-01-05 16:49:54,520 - Epoch: [150][   37/   37]    Overall Loss 0.643391    Objective Loss 0.643391    Top1 62.343096    LR 0.000001    Time 0.034030    
2023-01-05 16:49:54,589 - --- validate (epoch=150)-----------
2023-01-05 16:49:54,589 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:54,827 - Epoch: [150][    5/    5]    Loss 0.647422    Top1 61.259542    
2023-01-05 16:49:54,893 - ==> Top1: 61.260    Loss: 0.647

2023-01-05 16:49:54,894 - ==> Confusion:
[[330  99   0]
 [307 312   0]
 [  0   0   0]]

2023-01-05 16:49:54,895 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:54,895 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:54,905 - 

2023-01-05 16:49:54,905 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:55,405 - Epoch: [151][   10/   37]    Overall Loss 0.644538    Objective Loss 0.644538                                        LR 0.000001    Time 0.049862    
2023-01-05 16:49:55,685 - Epoch: [151][   20/   37]    Overall Loss 0.643530    Objective Loss 0.643530                                        LR 0.000001    Time 0.038930    
2023-01-05 16:49:55,999 - Epoch: [151][   30/   37]    Overall Loss 0.642442    Objective Loss 0.642442                                        LR 0.000001    Time 0.036399    
2023-01-05 16:49:56,189 - Epoch: [151][   37/   37]    Overall Loss 0.643163    Objective Loss 0.643163    Top1 55.439331    LR 0.000001    Time 0.034643    
2023-01-05 16:49:56,269 - --- validate (epoch=151)-----------
2023-01-05 16:49:56,269 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:56,521 - Epoch: [151][    5/    5]    Loss 0.653210    Top1 61.450382    
2023-01-05 16:49:56,594 - ==> Top1: 61.450    Loss: 0.653

2023-01-05 16:49:56,595 - ==> Confusion:
[[319 110   0]
 [294 325   0]
 [  0   0   0]]

2023-01-05 16:49:56,596 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:56,596 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:56,606 - 

2023-01-05 16:49:56,606 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:57,093 - Epoch: [152][   10/   37]    Overall Loss 0.646221    Objective Loss 0.646221                                        LR 0.000001    Time 0.048615    
2023-01-05 16:49:57,389 - Epoch: [152][   20/   37]    Overall Loss 0.644531    Objective Loss 0.644531                                        LR 0.000001    Time 0.039010    
2023-01-05 16:49:57,696 - Epoch: [152][   30/   37]    Overall Loss 0.644698    Objective Loss 0.644698                                        LR 0.000001    Time 0.036208    
2023-01-05 16:49:57,889 - Epoch: [152][   37/   37]    Overall Loss 0.642957    Objective Loss 0.642957    Top1 63.807531    LR 0.000001    Time 0.034576    
2023-01-05 16:49:57,959 - --- validate (epoch=152)-----------
2023-01-05 16:49:57,959 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:58,208 - Epoch: [152][    5/    5]    Loss 0.653739    Top1 61.354962    
2023-01-05 16:49:58,273 - ==> Top1: 61.355    Loss: 0.654

2023-01-05 16:49:58,274 - ==> Confusion:
[[321 108   0]
 [297 322   0]
 [  0   0   0]]

2023-01-05 16:49:58,275 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:58,275 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:58,285 - 

2023-01-05 16:49:58,285 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:49:58,794 - Epoch: [153][   10/   37]    Overall Loss 0.643629    Objective Loss 0.643629                                        LR 0.000001    Time 0.050816    
2023-01-05 16:49:59,091 - Epoch: [153][   20/   37]    Overall Loss 0.643627    Objective Loss 0.643627                                        LR 0.000001    Time 0.040205    
2023-01-05 16:49:59,403 - Epoch: [153][   30/   37]    Overall Loss 0.643169    Objective Loss 0.643169                                        LR 0.000001    Time 0.037192    
2023-01-05 16:49:59,597 - Epoch: [153][   37/   37]    Overall Loss 0.642729    Objective Loss 0.642729    Top1 64.225941    LR 0.000001    Time 0.035382    
2023-01-05 16:49:59,669 - --- validate (epoch=153)-----------
2023-01-05 16:49:59,669 - 1048 samples (256 per mini-batch)
2023-01-05 16:49:59,909 - Epoch: [153][    5/    5]    Loss 0.671160    Top1 61.641221    
2023-01-05 16:49:59,973 - ==> Top1: 61.641    Loss: 0.671

2023-01-05 16:49:59,974 - ==> Confusion:
[[316 113   0]
 [289 330   0]
 [  0   0   0]]

2023-01-05 16:49:59,975 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:49:59,975 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:49:59,985 - 

2023-01-05 16:49:59,985 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:00,462 - Epoch: [154][   10/   37]    Overall Loss 0.642464    Objective Loss 0.642464                                        LR 0.000001    Time 0.047650    
2023-01-05 16:50:00,751 - Epoch: [154][   20/   37]    Overall Loss 0.643719    Objective Loss 0.643719                                        LR 0.000001    Time 0.038166    
2023-01-05 16:50:01,063 - Epoch: [154][   30/   37]    Overall Loss 0.642017    Objective Loss 0.642017                                        LR 0.000001    Time 0.035818    
2023-01-05 16:50:01,256 - Epoch: [154][   37/   37]    Overall Loss 0.642608    Objective Loss 0.642608    Top1 61.297071    LR 0.000001    Time 0.034265    
2023-01-05 16:50:01,336 - --- validate (epoch=154)-----------
2023-01-05 16:50:01,337 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:01,576 - Epoch: [154][    5/    5]    Loss 0.662406    Top1 61.545802    
2023-01-05 16:50:01,633 - ==> Top1: 61.546    Loss: 0.662

2023-01-05 16:50:01,633 - ==> Confusion:
[[322 107   0]
 [296 323   0]
 [  0   0   0]]

2023-01-05 16:50:01,635 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:01,635 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:01,644 - 

2023-01-05 16:50:01,644 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:02,151 - Epoch: [155][   10/   37]    Overall Loss 0.644126    Objective Loss 0.644126                                        LR 0.000001    Time 0.050600    
2023-01-05 16:50:02,459 - Epoch: [155][   20/   37]    Overall Loss 0.642976    Objective Loss 0.642976                                        LR 0.000001    Time 0.040648    
2023-01-05 16:50:02,778 - Epoch: [155][   30/   37]    Overall Loss 0.643030    Objective Loss 0.643030                                        LR 0.000001    Time 0.037735    
2023-01-05 16:50:02,973 - Epoch: [155][   37/   37]    Overall Loss 0.642373    Objective Loss 0.642373    Top1 65.899582    LR 0.000001    Time 0.035845    
2023-01-05 16:50:03,043 - --- validate (epoch=155)-----------
2023-01-05 16:50:03,043 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:03,289 - Epoch: [155][    5/    5]    Loss 0.650694    Top1 62.118321    
2023-01-05 16:50:03,355 - ==> Top1: 62.118    Loss: 0.651

2023-01-05 16:50:03,355 - ==> Confusion:
[[322 107   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:50:03,356 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:03,357 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:03,366 - 

2023-01-05 16:50:03,366 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:03,846 - Epoch: [156][   10/   37]    Overall Loss 0.642503    Objective Loss 0.642503                                        LR 0.000001    Time 0.047952    
2023-01-05 16:50:04,140 - Epoch: [156][   20/   37]    Overall Loss 0.642207    Objective Loss 0.642207                                        LR 0.000001    Time 0.038495    
2023-01-05 16:50:04,448 - Epoch: [156][   30/   37]    Overall Loss 0.641079    Objective Loss 0.641079                                        LR 0.000001    Time 0.035862    
2023-01-05 16:50:04,642 - Epoch: [156][   37/   37]    Overall Loss 0.642098    Objective Loss 0.642098    Top1 61.297071    LR 0.000001    Time 0.034310    
2023-01-05 16:50:04,716 - --- validate (epoch=156)-----------
2023-01-05 16:50:04,716 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:04,966 - Epoch: [156][    5/    5]    Loss 0.650614    Top1 61.641221    
2023-01-05 16:50:05,039 - ==> Top1: 61.641    Loss: 0.651

2023-01-05 16:50:05,039 - ==> Confusion:
[[326 103   0]
 [299 320   0]
 [  0   0   0]]

2023-01-05 16:50:05,041 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:05,041 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:05,050 - 

2023-01-05 16:50:05,050 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:05,551 - Epoch: [157][   10/   37]    Overall Loss 0.637740    Objective Loss 0.637740                                        LR 0.000001    Time 0.049943    
2023-01-05 16:50:05,836 - Epoch: [157][   20/   37]    Overall Loss 0.640273    Objective Loss 0.640273                                        LR 0.000001    Time 0.039199    
2023-01-05 16:50:06,139 - Epoch: [157][   30/   37]    Overall Loss 0.638407    Objective Loss 0.638407                                        LR 0.000001    Time 0.036229    
2023-01-05 16:50:06,334 - Epoch: [157][   37/   37]    Overall Loss 0.641647    Objective Loss 0.641647    Top1 59.623431    LR 0.000001    Time 0.034640    
2023-01-05 16:50:06,405 - --- validate (epoch=157)-----------
2023-01-05 16:50:06,405 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:06,654 - Epoch: [157][    5/    5]    Loss 0.648521    Top1 61.641221    
2023-01-05 16:50:06,719 - ==> Top1: 61.641    Loss: 0.649

2023-01-05 16:50:06,719 - ==> Confusion:
[[316 113   0]
 [289 330   0]
 [  0   0   0]]

2023-01-05 16:50:06,721 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:06,721 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:06,730 - 

2023-01-05 16:50:06,731 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:07,246 - Epoch: [158][   10/   37]    Overall Loss 0.644925    Objective Loss 0.644925                                        LR 0.000001    Time 0.051501    
2023-01-05 16:50:07,543 - Epoch: [158][   20/   37]    Overall Loss 0.642124    Objective Loss 0.642124                                        LR 0.000001    Time 0.040573    
2023-01-05 16:50:07,862 - Epoch: [158][   30/   37]    Overall Loss 0.642546    Objective Loss 0.642546                                        LR 0.000001    Time 0.037655    
2023-01-05 16:50:08,057 - Epoch: [158][   37/   37]    Overall Loss 0.641626    Objective Loss 0.641626    Top1 60.669456    LR 0.000001    Time 0.035811    
2023-01-05 16:50:08,127 - --- validate (epoch=158)-----------
2023-01-05 16:50:08,127 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:08,383 - Epoch: [158][    5/    5]    Loss 0.667815    Top1 61.736641    
2023-01-05 16:50:08,453 - ==> Top1: 61.737    Loss: 0.668

2023-01-05 16:50:08,453 - ==> Confusion:
[[319 110   0]
 [291 328   0]
 [  0   0   0]]

2023-01-05 16:50:08,455 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:08,455 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:08,465 - 

2023-01-05 16:50:08,465 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:08,952 - Epoch: [159][   10/   37]    Overall Loss 0.641405    Objective Loss 0.641405                                        LR 0.000001    Time 0.048668    
2023-01-05 16:50:09,237 - Epoch: [159][   20/   37]    Overall Loss 0.642851    Objective Loss 0.642851                                        LR 0.000001    Time 0.038561    
2023-01-05 16:50:09,527 - Epoch: [159][   30/   37]    Overall Loss 0.642071    Objective Loss 0.642071                                        LR 0.000001    Time 0.035362    
2023-01-05 16:50:09,714 - Epoch: [159][   37/   37]    Overall Loss 0.641357    Objective Loss 0.641357    Top1 62.343096    LR 0.000001    Time 0.033717    
2023-01-05 16:50:09,788 - --- validate (epoch=159)-----------
2023-01-05 16:50:09,788 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:10,036 - Epoch: [159][    5/    5]    Loss 0.660220    Top1 61.832061    
2023-01-05 16:50:10,109 - ==> Top1: 61.832    Loss: 0.660

2023-01-05 16:50:10,110 - ==> Confusion:
[[320 109   0]
 [291 328   0]
 [  0   0   0]]

2023-01-05 16:50:10,111 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:10,111 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:10,121 - 

2023-01-05 16:50:10,121 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:10,602 - Epoch: [160][   10/   37]    Overall Loss 0.640507    Objective Loss 0.640507                                        LR 0.000001    Time 0.047989    
2023-01-05 16:50:10,890 - Epoch: [160][   20/   37]    Overall Loss 0.640414    Objective Loss 0.640414                                        LR 0.000001    Time 0.038266    
2023-01-05 16:50:11,196 - Epoch: [160][   30/   37]    Overall Loss 0.641257    Objective Loss 0.641257                                        LR 0.000001    Time 0.035698    
2023-01-05 16:50:11,390 - Epoch: [160][   37/   37]    Overall Loss 0.641199    Objective Loss 0.641199    Top1 62.133891    LR 0.000001    Time 0.034200    
2023-01-05 16:50:11,464 - --- validate (epoch=160)-----------
2023-01-05 16:50:11,465 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:11,702 - Epoch: [160][    5/    5]    Loss 0.639676    Top1 62.404580    
2023-01-05 16:50:11,766 - ==> Top1: 62.405    Loss: 0.640

2023-01-05 16:50:11,767 - ==> Confusion:
[[312 117   0]
 [277 342   0]
 [  0   0   0]]

2023-01-05 16:50:11,768 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:11,768 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:11,778 - 

2023-01-05 16:50:11,778 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:12,279 - Epoch: [161][   10/   37]    Overall Loss 0.640182    Objective Loss 0.640182                                        LR 0.000001    Time 0.050078    
2023-01-05 16:50:12,575 - Epoch: [161][   20/   37]    Overall Loss 0.639698    Objective Loss 0.639698                                        LR 0.000001    Time 0.039771    
2023-01-05 16:50:12,885 - Epoch: [161][   30/   37]    Overall Loss 0.641620    Objective Loss 0.641620                                        LR 0.000001    Time 0.036857    
2023-01-05 16:50:13,083 - Epoch: [161][   37/   37]    Overall Loss 0.640834    Objective Loss 0.640834    Top1 61.087866    LR 0.000001    Time 0.035214    
2023-01-05 16:50:13,152 - --- validate (epoch=161)-----------
2023-01-05 16:50:13,152 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:13,392 - Epoch: [161][    5/    5]    Loss 0.658371    Top1 61.641221    
2023-01-05 16:50:13,465 - ==> Top1: 61.641    Loss: 0.658

2023-01-05 16:50:13,465 - ==> Confusion:
[[321 108   0]
 [294 325   0]
 [  0   0   0]]

2023-01-05 16:50:13,467 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:13,467 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:13,476 - 

2023-01-05 16:50:13,477 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:13,962 - Epoch: [162][   10/   37]    Overall Loss 0.644349    Objective Loss 0.644349                                        LR 0.000001    Time 0.048447    
2023-01-05 16:50:14,241 - Epoch: [162][   20/   37]    Overall Loss 0.640962    Objective Loss 0.640962                                        LR 0.000001    Time 0.037992    
2023-01-05 16:50:14,531 - Epoch: [162][   30/   37]    Overall Loss 0.639286    Objective Loss 0.639286                                        LR 0.000001    Time 0.034984    
2023-01-05 16:50:14,719 - Epoch: [162][   37/   37]    Overall Loss 0.640691    Objective Loss 0.640691    Top1 58.786611    LR 0.000001    Time 0.033450    
2023-01-05 16:50:14,794 - --- validate (epoch=162)-----------
2023-01-05 16:50:14,794 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:15,035 - Epoch: [162][    5/    5]    Loss 0.654079    Top1 62.022901    
2023-01-05 16:50:15,097 - ==> Top1: 62.023    Loss: 0.654

2023-01-05 16:50:15,097 - ==> Confusion:
[[317 112   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:50:15,100 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:15,100 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:15,109 - 

2023-01-05 16:50:15,110 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:15,603 - Epoch: [163][   10/   37]    Overall Loss 0.642969    Objective Loss 0.642969                                        LR 0.000001    Time 0.049271    
2023-01-05 16:50:15,891 - Epoch: [163][   20/   37]    Overall Loss 0.639421    Objective Loss 0.639421                                        LR 0.000001    Time 0.038990    
2023-01-05 16:50:16,176 - Epoch: [163][   30/   37]    Overall Loss 0.639899    Objective Loss 0.639899                                        LR 0.000001    Time 0.035500    
2023-01-05 16:50:16,375 - Epoch: [163][   37/   37]    Overall Loss 0.640275    Objective Loss 0.640275    Top1 60.041841    LR 0.000001    Time 0.034158    
2023-01-05 16:50:16,444 - --- validate (epoch=163)-----------
2023-01-05 16:50:16,444 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:16,683 - Epoch: [163][    5/    5]    Loss 0.649431    Top1 62.309160    
2023-01-05 16:50:16,758 - ==> Top1: 62.309    Loss: 0.649

2023-01-05 16:50:16,758 - ==> Confusion:
[[314 115   0]
 [280 339   0]
 [  0   0   0]]

2023-01-05 16:50:16,760 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:16,760 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:16,770 - 

2023-01-05 16:50:16,770 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:17,386 - Epoch: [164][   10/   37]    Overall Loss 0.642087    Objective Loss 0.642087                                        LR 0.000001    Time 0.061566    
2023-01-05 16:50:17,675 - Epoch: [164][   20/   37]    Overall Loss 0.641243    Objective Loss 0.641243                                        LR 0.000001    Time 0.045202    
2023-01-05 16:50:17,975 - Epoch: [164][   30/   37]    Overall Loss 0.639771    Objective Loss 0.639771                                        LR 0.000001    Time 0.040121    
2023-01-05 16:50:18,169 - Epoch: [164][   37/   37]    Overall Loss 0.639950    Objective Loss 0.639950    Top1 58.995816    LR 0.000001    Time 0.037763    
2023-01-05 16:50:18,253 - --- validate (epoch=164)-----------
2023-01-05 16:50:18,253 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:18,491 - Epoch: [164][    5/    5]    Loss 0.642496    Top1 62.213740    
2023-01-05 16:50:18,567 - ==> Top1: 62.214    Loss: 0.642

2023-01-05 16:50:18,568 - ==> Confusion:
[[316 113   0]
 [283 336   0]
 [  0   0   0]]

2023-01-05 16:50:18,569 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:18,569 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:18,579 - 

2023-01-05 16:50:18,579 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:19,058 - Epoch: [165][   10/   37]    Overall Loss 0.641067    Objective Loss 0.641067                                        LR 0.000001    Time 0.047869    
2023-01-05 16:50:19,335 - Epoch: [165][   20/   37]    Overall Loss 0.639633    Objective Loss 0.639633                                        LR 0.000001    Time 0.037756    
2023-01-05 16:50:19,639 - Epoch: [165][   30/   37]    Overall Loss 0.641274    Objective Loss 0.641274                                        LR 0.000001    Time 0.035291    
2023-01-05 16:50:19,834 - Epoch: [165][   37/   37]    Overall Loss 0.639705    Objective Loss 0.639705    Top1 65.481172    LR 0.000001    Time 0.033875    
2023-01-05 16:50:19,905 - --- validate (epoch=165)-----------
2023-01-05 16:50:19,906 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:20,159 - Epoch: [165][    5/    5]    Loss 0.642742    Top1 61.927481    
2023-01-05 16:50:20,222 - ==> Top1: 61.927    Loss: 0.643

2023-01-05 16:50:20,223 - ==> Confusion:
[[320 109   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:50:20,224 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:20,224 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:20,234 - 

2023-01-05 16:50:20,234 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:20,731 - Epoch: [166][   10/   37]    Overall Loss 0.645140    Objective Loss 0.645140                                        LR 0.000001    Time 0.049607    
2023-01-05 16:50:21,019 - Epoch: [166][   20/   37]    Overall Loss 0.642123    Objective Loss 0.642123                                        LR 0.000001    Time 0.039160    
2023-01-05 16:50:21,312 - Epoch: [166][   30/   37]    Overall Loss 0.640609    Objective Loss 0.640609                                        LR 0.000001    Time 0.035862    
2023-01-05 16:50:21,508 - Epoch: [166][   37/   37]    Overall Loss 0.639471    Objective Loss 0.639471    Top1 64.225941    LR 0.000001    Time 0.034374    
2023-01-05 16:50:21,580 - --- validate (epoch=166)-----------
2023-01-05 16:50:21,581 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:21,824 - Epoch: [166][    5/    5]    Loss 0.652906    Top1 61.832061    
2023-01-05 16:50:21,896 - ==> Top1: 61.832    Loss: 0.653

2023-01-05 16:50:21,897 - ==> Confusion:
[[321 108   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:50:21,898 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:21,898 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:21,909 - 

2023-01-05 16:50:21,909 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:22,402 - Epoch: [167][   10/   37]    Overall Loss 0.636251    Objective Loss 0.636251                                        LR 0.000001    Time 0.049282    
2023-01-05 16:50:22,706 - Epoch: [167][   20/   37]    Overall Loss 0.636329    Objective Loss 0.636329                                        LR 0.000001    Time 0.039791    
2023-01-05 16:50:23,012 - Epoch: [167][   30/   37]    Overall Loss 0.638184    Objective Loss 0.638184                                        LR 0.000001    Time 0.036719    
2023-01-05 16:50:23,208 - Epoch: [167][   37/   37]    Overall Loss 0.639402    Objective Loss 0.639402    Top1 61.715481    LR 0.000001    Time 0.035060    
2023-01-05 16:50:23,292 - --- validate (epoch=167)-----------
2023-01-05 16:50:23,292 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:23,551 - Epoch: [167][    5/    5]    Loss 0.649808    Top1 62.118321    
2023-01-05 16:50:23,614 - ==> Top1: 62.118    Loss: 0.650

2023-01-05 16:50:23,614 - ==> Confusion:
[[317 112   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:50:23,616 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:23,616 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:23,625 - 

2023-01-05 16:50:23,625 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:24,136 - Epoch: [168][   10/   37]    Overall Loss 0.642963    Objective Loss 0.642963                                        LR 0.000001    Time 0.051006    
2023-01-05 16:50:24,421 - Epoch: [168][   20/   37]    Overall Loss 0.640457    Objective Loss 0.640457                                        LR 0.000001    Time 0.039693    
2023-01-05 16:50:24,726 - Epoch: [168][   30/   37]    Overall Loss 0.641053    Objective Loss 0.641053                                        LR 0.000001    Time 0.036616    
2023-01-05 16:50:24,921 - Epoch: [168][   37/   37]    Overall Loss 0.638925    Objective Loss 0.638925    Top1 66.108787    LR 0.000001    Time 0.034961    
2023-01-05 16:50:24,998 - --- validate (epoch=168)-----------
2023-01-05 16:50:24,998 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:25,237 - Epoch: [168][    5/    5]    Loss 0.652443    Top1 61.927481    
2023-01-05 16:50:25,301 - ==> Top1: 61.927    Loss: 0.652

2023-01-05 16:50:25,301 - ==> Confusion:
[[317 112   0]
 [287 332   0]
 [  0   0   0]]

2023-01-05 16:50:25,302 - ==> Best [Top1: 62.500   Sparsity:0.00   Params: 361664 on epoch: 148]
2023-01-05 16:50:25,303 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:25,312 - 

2023-01-05 16:50:25,312 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:25,810 - Epoch: [169][   10/   37]    Overall Loss 0.640122    Objective Loss 0.640122                                        LR 0.000001    Time 0.049722    
2023-01-05 16:50:26,106 - Epoch: [169][   20/   37]    Overall Loss 0.639033    Objective Loss 0.639033                                        LR 0.000001    Time 0.039628    
2023-01-05 16:50:26,415 - Epoch: [169][   30/   37]    Overall Loss 0.638981    Objective Loss 0.638981                                        LR 0.000001    Time 0.036706    
2023-01-05 16:50:26,609 - Epoch: [169][   37/   37]    Overall Loss 0.638821    Objective Loss 0.638821    Top1 65.899582    LR 0.000001    Time 0.034992    
2023-01-05 16:50:26,675 - --- validate (epoch=169)-----------
2023-01-05 16:50:26,675 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:26,922 - Epoch: [169][    5/    5]    Loss 0.662226    Top1 62.595420    
2023-01-05 16:50:26,996 - ==> Top1: 62.595    Loss: 0.662

2023-01-05 16:50:26,996 - ==> Confusion:
[[318 111   0]
 [281 338   0]
 [  0   0   0]]

2023-01-05 16:50:26,997 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:26,998 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:27,018 - 

2023-01-05 16:50:27,018 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:27,501 - Epoch: [170][   10/   37]    Overall Loss 0.640101    Objective Loss 0.640101                                        LR 0.000001    Time 0.048150    
2023-01-05 16:50:27,780 - Epoch: [170][   20/   37]    Overall Loss 0.640430    Objective Loss 0.640430                                        LR 0.000001    Time 0.038025    
2023-01-05 16:50:28,078 - Epoch: [170][   30/   37]    Overall Loss 0.640433    Objective Loss 0.640433                                        LR 0.000001    Time 0.035267    
2023-01-05 16:50:28,268 - Epoch: [170][   37/   37]    Overall Loss 0.638385    Objective Loss 0.638385    Top1 66.527197    LR 0.000001    Time 0.033711    
2023-01-05 16:50:28,344 - --- validate (epoch=170)-----------
2023-01-05 16:50:28,344 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:28,593 - Epoch: [170][    5/    5]    Loss 0.640205    Top1 62.118321    
2023-01-05 16:50:28,654 - ==> Top1: 62.118    Loss: 0.640

2023-01-05 16:50:28,655 - ==> Confusion:
[[313 116   0]
 [281 338   0]
 [  0   0   0]]

2023-01-05 16:50:28,656 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:28,656 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:28,666 - 

2023-01-05 16:50:28,666 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:29,143 - Epoch: [171][   10/   37]    Overall Loss 0.638813    Objective Loss 0.638813                                        LR 0.000001    Time 0.047677    
2023-01-05 16:50:29,426 - Epoch: [171][   20/   37]    Overall Loss 0.640313    Objective Loss 0.640313                                        LR 0.000001    Time 0.037805    
2023-01-05 16:50:29,730 - Epoch: [171][   30/   37]    Overall Loss 0.640192    Objective Loss 0.640192                                        LR 0.000001    Time 0.035298    
2023-01-05 16:50:29,924 - Epoch: [171][   37/   37]    Overall Loss 0.638483    Objective Loss 0.638483    Top1 67.782427    LR 0.000001    Time 0.033862    
2023-01-05 16:50:29,985 - --- validate (epoch=171)-----------
2023-01-05 16:50:29,986 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:30,223 - Epoch: [171][    5/    5]    Loss 0.641605    Top1 62.118321    
2023-01-05 16:50:30,282 - ==> Top1: 62.118    Loss: 0.642

2023-01-05 16:50:30,283 - ==> Confusion:
[[320 109   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:30,284 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:30,284 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:30,294 - 

2023-01-05 16:50:30,294 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:30,799 - Epoch: [172][   10/   37]    Overall Loss 0.645053    Objective Loss 0.645053                                        LR 0.000001    Time 0.050462    
2023-01-05 16:50:31,114 - Epoch: [172][   20/   37]    Overall Loss 0.635629    Objective Loss 0.635629                                        LR 0.000001    Time 0.040845    
2023-01-05 16:50:31,436 - Epoch: [172][   30/   37]    Overall Loss 0.638117    Objective Loss 0.638117                                        LR 0.000001    Time 0.037962    
2023-01-05 16:50:31,630 - Epoch: [172][   37/   37]    Overall Loss 0.638095    Objective Loss 0.638095    Top1 60.878661    LR 0.000001    Time 0.036018    
2023-01-05 16:50:31,700 - --- validate (epoch=172)-----------
2023-01-05 16:50:31,700 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:31,942 - Epoch: [172][    5/    5]    Loss 0.667297    Top1 62.404580    
2023-01-05 16:50:32,011 - ==> Top1: 62.405    Loss: 0.667

2023-01-05 16:50:32,011 - ==> Confusion:
[[323 106   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:32,012 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:32,013 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:32,022 - 

2023-01-05 16:50:32,022 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:32,493 - Epoch: [173][   10/   37]    Overall Loss 0.639968    Objective Loss 0.639968                                        LR 0.000001    Time 0.047045    
2023-01-05 16:50:32,783 - Epoch: [173][   20/   37]    Overall Loss 0.637802    Objective Loss 0.637802                                        LR 0.000001    Time 0.037988    
2023-01-05 16:50:33,070 - Epoch: [173][   30/   37]    Overall Loss 0.637188    Objective Loss 0.637188                                        LR 0.000001    Time 0.034886    
2023-01-05 16:50:33,264 - Epoch: [173][   37/   37]    Overall Loss 0.637803    Objective Loss 0.637803    Top1 61.924686    LR 0.000001    Time 0.033524    
2023-01-05 16:50:33,336 - --- validate (epoch=173)-----------
2023-01-05 16:50:33,337 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:33,580 - Epoch: [173][    5/    5]    Loss 0.639621    Top1 62.022901    
2023-01-05 16:50:33,655 - ==> Top1: 62.023    Loss: 0.640

2023-01-05 16:50:33,655 - ==> Confusion:
[[319 110   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:33,657 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:33,657 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:33,666 - 

2023-01-05 16:50:33,666 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:34,144 - Epoch: [174][   10/   37]    Overall Loss 0.635234    Objective Loss 0.635234                                        LR 0.000001    Time 0.047654    
2023-01-05 16:50:34,439 - Epoch: [174][   20/   37]    Overall Loss 0.639567    Objective Loss 0.639567                                        LR 0.000001    Time 0.038567    
2023-01-05 16:50:34,734 - Epoch: [174][   30/   37]    Overall Loss 0.639151    Objective Loss 0.639151                                        LR 0.000001    Time 0.035519    
2023-01-05 16:50:34,928 - Epoch: [174][   37/   37]    Overall Loss 0.637712    Objective Loss 0.637712    Top1 66.108787    LR 0.000001    Time 0.034034    
2023-01-05 16:50:35,003 - --- validate (epoch=174)-----------
2023-01-05 16:50:35,003 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:35,244 - Epoch: [174][    5/    5]    Loss 0.647486    Top1 62.500000    
2023-01-05 16:50:35,306 - ==> Top1: 62.500    Loss: 0.647

2023-01-05 16:50:35,307 - ==> Confusion:
[[322 107   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:50:35,308 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:35,308 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:35,318 - 

2023-01-05 16:50:35,318 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:35,792 - Epoch: [175][   10/   37]    Overall Loss 0.635473    Objective Loss 0.635473                                        LR 0.000001    Time 0.047329    
2023-01-05 16:50:36,106 - Epoch: [175][   20/   37]    Overall Loss 0.634954    Objective Loss 0.634954                                        LR 0.000001    Time 0.039324    
2023-01-05 16:50:36,415 - Epoch: [175][   30/   37]    Overall Loss 0.636281    Objective Loss 0.636281                                        LR 0.000001    Time 0.036464    
2023-01-05 16:50:36,608 - Epoch: [175][   37/   37]    Overall Loss 0.637560    Objective Loss 0.637560    Top1 59.205021    LR 0.000001    Time 0.034777    
2023-01-05 16:50:36,677 - --- validate (epoch=175)-----------
2023-01-05 16:50:36,677 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:36,920 - Epoch: [175][    5/    5]    Loss 0.636162    Top1 62.118321    
2023-01-05 16:50:36,985 - ==> Top1: 62.118    Loss: 0.636

2023-01-05 16:50:36,986 - ==> Confusion:
[[324 105   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:50:36,987 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:36,987 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:36,997 - 

2023-01-05 16:50:36,997 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:37,479 - Epoch: [176][   10/   37]    Overall Loss 0.637678    Objective Loss 0.637678                                        LR 0.000001    Time 0.048137    
2023-01-05 16:50:37,757 - Epoch: [176][   20/   37]    Overall Loss 0.641306    Objective Loss 0.641306                                        LR 0.000001    Time 0.037926    
2023-01-05 16:50:38,052 - Epoch: [176][   30/   37]    Overall Loss 0.638681    Objective Loss 0.638681                                        LR 0.000001    Time 0.035110    
2023-01-05 16:50:38,245 - Epoch: [176][   37/   37]    Overall Loss 0.637202    Objective Loss 0.637202    Top1 63.389121    LR 0.000001    Time 0.033689    
2023-01-05 16:50:38,313 - --- validate (epoch=176)-----------
2023-01-05 16:50:38,313 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:38,560 - Epoch: [176][    5/    5]    Loss 0.642839    Top1 62.309160    
2023-01-05 16:50:38,625 - ==> Top1: 62.309    Loss: 0.643

2023-01-05 16:50:38,626 - ==> Confusion:
[[320 109   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:50:38,627 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:38,627 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:38,637 - 

2023-01-05 16:50:38,637 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:39,145 - Epoch: [177][   10/   37]    Overall Loss 0.639191    Objective Loss 0.639191                                        LR 0.000001    Time 0.050727    
2023-01-05 16:50:39,457 - Epoch: [177][   20/   37]    Overall Loss 0.641420    Objective Loss 0.641420                                        LR 0.000001    Time 0.040755    
2023-01-05 16:50:39,777 - Epoch: [177][   30/   37]    Overall Loss 0.639437    Objective Loss 0.639437                                        LR 0.000001    Time 0.037831    
2023-01-05 16:50:39,973 - Epoch: [177][   37/   37]    Overall Loss 0.637149    Objective Loss 0.637149    Top1 62.552301    LR 0.000001    Time 0.035953    
2023-01-05 16:50:40,054 - --- validate (epoch=177)-----------
2023-01-05 16:50:40,054 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:40,291 - Epoch: [177][    5/    5]    Loss 0.654337    Top1 62.022901    
2023-01-05 16:50:40,355 - ==> Top1: 62.023    Loss: 0.654

2023-01-05 16:50:40,356 - ==> Confusion:
[[330  99   0]
 [299 320   0]
 [  0   0   0]]

2023-01-05 16:50:40,357 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 169]
2023-01-05 16:50:40,357 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:40,367 - 

2023-01-05 16:50:40,367 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:40,877 - Epoch: [178][   10/   37]    Overall Loss 0.636732    Objective Loss 0.636732                                        LR 0.000001    Time 0.050857    
2023-01-05 16:50:41,174 - Epoch: [178][   20/   37]    Overall Loss 0.638298    Objective Loss 0.638298                                        LR 0.000001    Time 0.040286    
2023-01-05 16:50:41,480 - Epoch: [178][   30/   37]    Overall Loss 0.636412    Objective Loss 0.636412                                        LR 0.000001    Time 0.037039    
2023-01-05 16:50:41,676 - Epoch: [178][   37/   37]    Overall Loss 0.636772    Objective Loss 0.636772    Top1 63.598326    LR 0.000001    Time 0.035314    
2023-01-05 16:50:41,752 - --- validate (epoch=178)-----------
2023-01-05 16:50:41,752 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:41,992 - Epoch: [178][    5/    5]    Loss 0.645399    Top1 62.595420    
2023-01-05 16:50:42,053 - ==> Top1: 62.595    Loss: 0.645

2023-01-05 16:50:42,053 - ==> Confusion:
[[320 109   0]
 [283 336   0]
 [  0   0   0]]

2023-01-05 16:50:42,055 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 178]
2023-01-05 16:50:42,055 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:42,081 - 

2023-01-05 16:50:42,081 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:42,594 - Epoch: [179][   10/   37]    Overall Loss 0.632659    Objective Loss 0.632659                                        LR 0.000001    Time 0.051111    
2023-01-05 16:50:42,890 - Epoch: [179][   20/   37]    Overall Loss 0.634968    Objective Loss 0.634968                                        LR 0.000001    Time 0.040350    
2023-01-05 16:50:43,192 - Epoch: [179][   30/   37]    Overall Loss 0.635462    Objective Loss 0.635462                                        LR 0.000001    Time 0.036959    
2023-01-05 16:50:43,389 - Epoch: [179][   37/   37]    Overall Loss 0.636497    Objective Loss 0.636497    Top1 61.087866    LR 0.000001    Time 0.035287    
2023-01-05 16:50:43,463 - --- validate (epoch=179)-----------
2023-01-05 16:50:43,463 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:43,703 - Epoch: [179][    5/    5]    Loss 0.645808    Top1 62.118321    
2023-01-05 16:50:43,769 - ==> Top1: 62.118    Loss: 0.646

2023-01-05 16:50:43,770 - ==> Confusion:
[[320 109   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:43,771 - ==> Best [Top1: 62.595   Sparsity:0.00   Params: 361664 on epoch: 178]
2023-01-05 16:50:43,771 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:43,781 - 

2023-01-05 16:50:43,781 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:44,271 - Epoch: [180][   10/   37]    Overall Loss 0.642509    Objective Loss 0.642509                                        LR 0.000001    Time 0.048886    
2023-01-05 16:50:44,578 - Epoch: [180][   20/   37]    Overall Loss 0.639331    Objective Loss 0.639331                                        LR 0.000001    Time 0.039781    
2023-01-05 16:50:44,881 - Epoch: [180][   30/   37]    Overall Loss 0.636738    Objective Loss 0.636738                                        LR 0.000001    Time 0.036595    
2023-01-05 16:50:45,076 - Epoch: [180][   37/   37]    Overall Loss 0.636383    Objective Loss 0.636383    Top1 64.644351    LR 0.000001    Time 0.034946    
2023-01-05 16:50:45,148 - --- validate (epoch=180)-----------
2023-01-05 16:50:45,148 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:45,394 - Epoch: [180][    5/    5]    Loss 0.644523    Top1 62.690840    
2023-01-05 16:50:45,457 - ==> Top1: 62.691    Loss: 0.645

2023-01-05 16:50:45,458 - ==> Confusion:
[[326 103   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:45,459 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 180]
2023-01-05 16:50:45,459 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:45,483 - 

2023-01-05 16:50:45,484 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:46,090 - Epoch: [181][   10/   37]    Overall Loss 0.641551    Objective Loss 0.641551                                        LR 0.000001    Time 0.060509    
2023-01-05 16:50:46,380 - Epoch: [181][   20/   37]    Overall Loss 0.636209    Objective Loss 0.636209                                        LR 0.000001    Time 0.044652    
2023-01-05 16:50:46,680 - Epoch: [181][   30/   37]    Overall Loss 0.636798    Objective Loss 0.636798                                        LR 0.000001    Time 0.039719    
2023-01-05 16:50:46,875 - Epoch: [181][   37/   37]    Overall Loss 0.636168    Objective Loss 0.636168    Top1 64.435146    LR 0.000001    Time 0.037461    
2023-01-05 16:50:46,945 - --- validate (epoch=181)-----------
2023-01-05 16:50:46,945 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:47,187 - Epoch: [181][    5/    5]    Loss 0.645622    Top1 62.213740    
2023-01-05 16:50:47,263 - ==> Top1: 62.214    Loss: 0.646

2023-01-05 16:50:47,264 - ==> Confusion:
[[321 108   0]
 [288 331   0]
 [  0   0   0]]

2023-01-05 16:50:47,265 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 180]
2023-01-05 16:50:47,265 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:47,275 - 

2023-01-05 16:50:47,276 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:47,765 - Epoch: [182][   10/   37]    Overall Loss 0.635470    Objective Loss 0.635470                                        LR 0.000001    Time 0.048898    
2023-01-05 16:50:48,063 - Epoch: [182][   20/   37]    Overall Loss 0.633053    Objective Loss 0.633053                                        LR 0.000001    Time 0.039232    
2023-01-05 16:50:48,369 - Epoch: [182][   30/   37]    Overall Loss 0.636085    Objective Loss 0.636085                                        LR 0.000001    Time 0.036314    
2023-01-05 16:50:48,564 - Epoch: [182][   37/   37]    Overall Loss 0.636027    Objective Loss 0.636027    Top1 63.389121    LR 0.000001    Time 0.034706    
2023-01-05 16:50:48,631 - --- validate (epoch=182)-----------
2023-01-05 16:50:48,632 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:48,879 - Epoch: [182][    5/    5]    Loss 0.673304    Top1 62.690840    
2023-01-05 16:50:48,953 - ==> Top1: 62.691    Loss: 0.673

2023-01-05 16:50:48,953 - ==> Confusion:
[[316 113   0]
 [278 341   0]
 [  0   0   0]]

2023-01-05 16:50:48,955 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:48,955 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:48,976 - 

2023-01-05 16:50:48,976 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:49,470 - Epoch: [183][   10/   37]    Overall Loss 0.638370    Objective Loss 0.638370                                        LR 0.000001    Time 0.049306    
2023-01-05 16:50:49,772 - Epoch: [183][   20/   37]    Overall Loss 0.636912    Objective Loss 0.636912                                        LR 0.000001    Time 0.039709    
2023-01-05 16:50:50,086 - Epoch: [183][   30/   37]    Overall Loss 0.637148    Objective Loss 0.637148                                        LR 0.000001    Time 0.036940    
2023-01-05 16:50:50,283 - Epoch: [183][   37/   37]    Overall Loss 0.635769    Objective Loss 0.635769    Top1 63.598326    LR 0.000001    Time 0.035262    
2023-01-05 16:50:50,355 - --- validate (epoch=183)-----------
2023-01-05 16:50:50,355 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:50,595 - Epoch: [183][    5/    5]    Loss 0.651144    Top1 62.309160    
2023-01-05 16:50:50,680 - ==> Top1: 62.309    Loss: 0.651

2023-01-05 16:50:50,681 - ==> Confusion:
[[326 103   0]
 [292 327   0]
 [  0   0   0]]

2023-01-05 16:50:50,682 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:50,682 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:50,692 - 

2023-01-05 16:50:50,692 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:51,179 - Epoch: [184][   10/   37]    Overall Loss 0.636250    Objective Loss 0.636250                                        LR 0.000001    Time 0.048579    
2023-01-05 16:50:51,464 - Epoch: [184][   20/   37]    Overall Loss 0.639716    Objective Loss 0.639716                                        LR 0.000001    Time 0.038488    
2023-01-05 16:50:51,766 - Epoch: [184][   30/   37]    Overall Loss 0.636918    Objective Loss 0.636918                                        LR 0.000001    Time 0.035737    
2023-01-05 16:50:51,957 - Epoch: [184][   37/   37]    Overall Loss 0.635494    Objective Loss 0.635494    Top1 62.970711    LR 0.000001    Time 0.034112    
2023-01-05 16:50:52,027 - --- validate (epoch=184)-----------
2023-01-05 16:50:52,027 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:52,269 - Epoch: [184][    5/    5]    Loss 0.651357    Top1 62.213740    
2023-01-05 16:50:52,335 - ==> Top1: 62.214    Loss: 0.651

2023-01-05 16:50:52,336 - ==> Confusion:
[[323 106   0]
 [290 329   0]
 [  0   0   0]]

2023-01-05 16:50:52,337 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:52,337 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:52,347 - 

2023-01-05 16:50:52,347 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:52,815 - Epoch: [185][   10/   37]    Overall Loss 0.632216    Objective Loss 0.632216                                        LR 0.000001    Time 0.046690    
2023-01-05 16:50:53,090 - Epoch: [185][   20/   37]    Overall Loss 0.634921    Objective Loss 0.634921                                        LR 0.000001    Time 0.037058    
2023-01-05 16:50:53,377 - Epoch: [185][   30/   37]    Overall Loss 0.635324    Objective Loss 0.635324                                        LR 0.000001    Time 0.034261    
2023-01-05 16:50:53,564 - Epoch: [185][   37/   37]    Overall Loss 0.635454    Objective Loss 0.635454    Top1 60.669456    LR 0.000001    Time 0.032828    
2023-01-05 16:50:53,650 - --- validate (epoch=185)-----------
2023-01-05 16:50:53,650 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:53,898 - Epoch: [185][    5/    5]    Loss 0.636836    Top1 62.500000    
2023-01-05 16:50:53,971 - ==> Top1: 62.500    Loss: 0.637

2023-01-05 16:50:53,972 - ==> Confusion:
[[319 110   0]
 [283 336   0]
 [  0   0   0]]

2023-01-05 16:50:53,973 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:53,973 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:53,983 - 

2023-01-05 16:50:53,983 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:54,489 - Epoch: [186][   10/   37]    Overall Loss 0.635766    Objective Loss 0.635766                                        LR 0.000001    Time 0.050545    
2023-01-05 16:50:54,783 - Epoch: [186][   20/   37]    Overall Loss 0.637404    Objective Loss 0.637404                                        LR 0.000001    Time 0.039925    
2023-01-05 16:50:55,083 - Epoch: [186][   30/   37]    Overall Loss 0.635323    Objective Loss 0.635323                                        LR 0.000001    Time 0.036619    
2023-01-05 16:50:55,277 - Epoch: [186][   37/   37]    Overall Loss 0.635370    Objective Loss 0.635370    Top1 63.598326    LR 0.000001    Time 0.034937    
2023-01-05 16:50:55,348 - --- validate (epoch=186)-----------
2023-01-05 16:50:55,348 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:55,591 - Epoch: [186][    5/    5]    Loss 0.644657    Top1 62.500000    
2023-01-05 16:50:55,657 - ==> Top1: 62.500    Loss: 0.645

2023-01-05 16:50:55,657 - ==> Confusion:
[[317 112   0]
 [281 338   0]
 [  0   0   0]]

2023-01-05 16:50:55,659 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:55,659 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:55,669 - 

2023-01-05 16:50:55,669 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:56,141 - Epoch: [187][   10/   37]    Overall Loss 0.637022    Objective Loss 0.637022                                        LR 0.000001    Time 0.047103    
2023-01-05 16:50:56,423 - Epoch: [187][   20/   37]    Overall Loss 0.633185    Objective Loss 0.633185                                        LR 0.000001    Time 0.037647    
2023-01-05 16:50:56,726 - Epoch: [187][   30/   37]    Overall Loss 0.634538    Objective Loss 0.634538                                        LR 0.000001    Time 0.035119    
2023-01-05 16:50:56,922 - Epoch: [187][   37/   37]    Overall Loss 0.635220    Objective Loss 0.635220    Top1 64.225941    LR 0.000001    Time 0.033762    
2023-01-05 16:50:56,999 - --- validate (epoch=187)-----------
2023-01-05 16:50:57,000 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:57,241 - Epoch: [187][    5/    5]    Loss 0.645495    Top1 62.404580    
2023-01-05 16:50:57,305 - ==> Top1: 62.405    Loss: 0.645

2023-01-05 16:50:57,305 - ==> Confusion:
[[315 114   0]
 [280 339   0]
 [  0   0   0]]

2023-01-05 16:50:57,307 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:57,307 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:57,316 - 

2023-01-05 16:50:57,316 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:57,809 - Epoch: [188][   10/   37]    Overall Loss 0.634716    Objective Loss 0.634716                                        LR 0.000001    Time 0.049134    
2023-01-05 16:50:58,104 - Epoch: [188][   20/   37]    Overall Loss 0.634003    Objective Loss 0.634003                                        LR 0.000001    Time 0.039204    
2023-01-05 16:50:58,407 - Epoch: [188][   30/   37]    Overall Loss 0.635209    Objective Loss 0.635209                                        LR 0.000001    Time 0.036227    
2023-01-05 16:50:58,602 - Epoch: [188][   37/   37]    Overall Loss 0.634953    Objective Loss 0.634953    Top1 65.062762    LR 0.000001    Time 0.034631    
2023-01-05 16:50:58,691 - --- validate (epoch=188)-----------
2023-01-05 16:50:58,692 - 1048 samples (256 per mini-batch)
2023-01-05 16:50:58,942 - Epoch: [188][    5/    5]    Loss 0.636827    Top1 62.404580    
2023-01-05 16:50:59,000 - ==> Top1: 62.405    Loss: 0.637

2023-01-05 16:50:59,001 - ==> Confusion:
[[322 107   0]
 [287 332   0]
 [  0   0   0]]

2023-01-05 16:50:59,002 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:50:59,002 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:50:59,012 - 

2023-01-05 16:50:59,012 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:50:59,520 - Epoch: [189][   10/   37]    Overall Loss 0.630486    Objective Loss 0.630486                                        LR 0.000001    Time 0.050682    
2023-01-05 16:50:59,829 - Epoch: [189][   20/   37]    Overall Loss 0.633786    Objective Loss 0.633786                                        LR 0.000001    Time 0.040776    
2023-01-05 16:51:00,149 - Epoch: [189][   30/   37]    Overall Loss 0.634776    Objective Loss 0.634776                                        LR 0.000001    Time 0.037830    
2023-01-05 16:51:00,346 - Epoch: [189][   37/   37]    Overall Loss 0.634769    Objective Loss 0.634769    Top1 67.573222    LR 0.000001    Time 0.035998    
2023-01-05 16:51:00,415 - --- validate (epoch=189)-----------
2023-01-05 16:51:00,415 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:00,663 - Epoch: [189][    5/    5]    Loss 0.654274    Top1 62.595420    
2023-01-05 16:51:00,731 - ==> Top1: 62.595    Loss: 0.654

2023-01-05 16:51:00,732 - ==> Confusion:
[[314 115   0]
 [277 342   0]
 [  0   0   0]]

2023-01-05 16:51:00,733 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:51:00,734 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:00,743 - 

2023-01-05 16:51:00,743 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:01,234 - Epoch: [190][   10/   37]    Overall Loss 0.636238    Objective Loss 0.636238                                        LR 0.000000    Time 0.049027    
2023-01-05 16:51:01,510 - Epoch: [190][   20/   37]    Overall Loss 0.632861    Objective Loss 0.632861                                        LR 0.000000    Time 0.038166    
2023-01-05 16:51:01,797 - Epoch: [190][   30/   37]    Overall Loss 0.635883    Objective Loss 0.635883                                        LR 0.000000    Time 0.035000    
2023-01-05 16:51:01,993 - Epoch: [190][   37/   37]    Overall Loss 0.634602    Objective Loss 0.634602    Top1 61.924686    LR 0.000000    Time 0.033653    
2023-01-05 16:51:02,059 - --- validate (epoch=190)-----------
2023-01-05 16:51:02,060 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:02,302 - Epoch: [190][    5/    5]    Loss 0.637513    Top1 62.404580    
2023-01-05 16:51:02,378 - ==> Top1: 62.405    Loss: 0.638

2023-01-05 16:51:02,379 - ==> Confusion:
[[317 112   0]
 [282 337   0]
 [  0   0   0]]

2023-01-05 16:51:02,380 - ==> Best [Top1: 62.691   Sparsity:0.00   Params: 361664 on epoch: 182]
2023-01-05 16:51:02,380 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:02,390 - 

2023-01-05 16:51:02,391 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:02,880 - Epoch: [191][   10/   37]    Overall Loss 0.637827    Objective Loss 0.637827                                        LR 0.000000    Time 0.048836    
2023-01-05 16:51:03,186 - Epoch: [191][   20/   37]    Overall Loss 0.636391    Objective Loss 0.636391                                        LR 0.000000    Time 0.039716    
2023-01-05 16:51:03,491 - Epoch: [191][   30/   37]    Overall Loss 0.635074    Objective Loss 0.635074                                        LR 0.000000    Time 0.036631    
2023-01-05 16:51:03,698 - Epoch: [191][   37/   37]    Overall Loss 0.634530    Objective Loss 0.634530    Top1 62.343096    LR 0.000000    Time 0.035279    
2023-01-05 16:51:03,774 - --- validate (epoch=191)-----------
2023-01-05 16:51:03,774 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:04,026 - Epoch: [191][    5/    5]    Loss 0.629667    Top1 62.786260    
2023-01-05 16:51:04,097 - ==> Top1: 62.786    Loss: 0.630

2023-01-05 16:51:04,097 - ==> Confusion:
[[317 112   0]
 [278 341   0]
 [  0   0   0]]

2023-01-05 16:51:04,099 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:04,099 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:04,120 - 

2023-01-05 16:51:04,120 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:04,643 - Epoch: [192][   10/   37]    Overall Loss 0.628040    Objective Loss 0.628040                                        LR 0.000000    Time 0.052148    
2023-01-05 16:51:04,955 - Epoch: [192][   20/   37]    Overall Loss 0.633751    Objective Loss 0.633751                                        LR 0.000000    Time 0.041639    
2023-01-05 16:51:05,269 - Epoch: [192][   30/   37]    Overall Loss 0.633849    Objective Loss 0.633849                                        LR 0.000000    Time 0.038217    
2023-01-05 16:51:05,477 - Epoch: [192][   37/   37]    Overall Loss 0.634368    Objective Loss 0.634368    Top1 62.133891    LR 0.000000    Time 0.036613    
2023-01-05 16:51:05,553 - --- validate (epoch=192)-----------
2023-01-05 16:51:05,553 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:05,809 - Epoch: [192][    5/    5]    Loss 0.631050    Top1 62.309160    
2023-01-05 16:51:05,873 - ==> Top1: 62.309    Loss: 0.631

2023-01-05 16:51:05,873 - ==> Confusion:
[[319 110   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:51:05,876 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:05,876 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:05,885 - 

2023-01-05 16:51:05,886 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:06,377 - Epoch: [193][   10/   37]    Overall Loss 0.638590    Objective Loss 0.638590                                        LR 0.000000    Time 0.049069    
2023-01-05 16:51:06,700 - Epoch: [193][   20/   37]    Overall Loss 0.635959    Objective Loss 0.635959                                        LR 0.000000    Time 0.040644    
2023-01-05 16:51:06,999 - Epoch: [193][   30/   37]    Overall Loss 0.634801    Objective Loss 0.634801                                        LR 0.000000    Time 0.037070    
2023-01-05 16:51:07,195 - Epoch: [193][   37/   37]    Overall Loss 0.634314    Objective Loss 0.634314    Top1 62.343096    LR 0.000000    Time 0.035343    
2023-01-05 16:51:07,277 - --- validate (epoch=193)-----------
2023-01-05 16:51:07,278 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:07,535 - Epoch: [193][    5/    5]    Loss 0.646766    Top1 62.500000    
2023-01-05 16:51:07,601 - ==> Top1: 62.500    Loss: 0.647

2023-01-05 16:51:07,602 - ==> Confusion:
[[316 113   0]
 [280 339   0]
 [  0   0   0]]

2023-01-05 16:51:07,603 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:07,603 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:07,615 - 

2023-01-05 16:51:07,615 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:08,098 - Epoch: [194][   10/   37]    Overall Loss 0.637090    Objective Loss 0.637090                                        LR 0.000000    Time 0.048232    
2023-01-05 16:51:08,392 - Epoch: [194][   20/   37]    Overall Loss 0.637229    Objective Loss 0.637229                                        LR 0.000000    Time 0.038778    
2023-01-05 16:51:08,703 - Epoch: [194][   30/   37]    Overall Loss 0.636840    Objective Loss 0.636840                                        LR 0.000000    Time 0.036098    
2023-01-05 16:51:08,904 - Epoch: [194][   37/   37]    Overall Loss 0.634101    Objective Loss 0.634101    Top1 67.573222    LR 0.000000    Time 0.034707    
2023-01-05 16:51:08,977 - --- validate (epoch=194)-----------
2023-01-05 16:51:08,977 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:09,230 - Epoch: [194][    5/    5]    Loss 0.657940    Top1 62.309160    
2023-01-05 16:51:09,296 - ==> Top1: 62.309    Loss: 0.658

2023-01-05 16:51:09,296 - ==> Confusion:
[[319 110   0]
 [285 334   0]
 [  0   0   0]]

2023-01-05 16:51:09,298 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:09,298 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:09,308 - 

2023-01-05 16:51:09,308 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:09,807 - Epoch: [195][   10/   37]    Overall Loss 0.631359    Objective Loss 0.631359                                        LR 0.000000    Time 0.049745    
2023-01-05 16:51:10,100 - Epoch: [195][   20/   37]    Overall Loss 0.632537    Objective Loss 0.632537                                        LR 0.000000    Time 0.039445    
2023-01-05 16:51:10,416 - Epoch: [195][   30/   37]    Overall Loss 0.633355    Objective Loss 0.633355                                        LR 0.000000    Time 0.036809    
2023-01-05 16:51:10,623 - Epoch: [195][   37/   37]    Overall Loss 0.634109    Objective Loss 0.634109    Top1 59.414226    LR 0.000000    Time 0.035425    
2023-01-05 16:51:10,699 - --- validate (epoch=195)-----------
2023-01-05 16:51:10,699 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:10,981 - Epoch: [195][    5/    5]    Loss 0.647525    Top1 62.595420    
2023-01-05 16:51:11,063 - ==> Top1: 62.595    Loss: 0.648

2023-01-05 16:51:11,063 - ==> Confusion:
[[323 106   0]
 [286 333   0]
 [  0   0   0]]

2023-01-05 16:51:11,065 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:11,065 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:11,075 - 

2023-01-05 16:51:11,075 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:11,566 - Epoch: [196][   10/   37]    Overall Loss 0.629146    Objective Loss 0.629146                                        LR 0.000000    Time 0.049045    
2023-01-05 16:51:11,848 - Epoch: [196][   20/   37]    Overall Loss 0.630385    Objective Loss 0.630385                                        LR 0.000000    Time 0.038592    
2023-01-05 16:51:12,154 - Epoch: [196][   30/   37]    Overall Loss 0.630978    Objective Loss 0.630978                                        LR 0.000000    Time 0.035912    
2023-01-05 16:51:12,357 - Epoch: [196][   37/   37]    Overall Loss 0.634209    Objective Loss 0.634209    Top1 62.552301    LR 0.000000    Time 0.034593    
2023-01-05 16:51:12,437 - --- validate (epoch=196)-----------
2023-01-05 16:51:12,437 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:12,691 - Epoch: [196][    5/    5]    Loss 0.654077    Top1 62.690840    
2023-01-05 16:51:12,753 - ==> Top1: 62.691    Loss: 0.654

2023-01-05 16:51:12,754 - ==> Confusion:
[[316 113   0]
 [278 341   0]
 [  0   0   0]]

2023-01-05 16:51:12,756 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 191]
2023-01-05 16:51:12,756 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:12,766 - 

2023-01-05 16:51:12,767 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:13,266 - Epoch: [197][   10/   37]    Overall Loss 0.637119    Objective Loss 0.637119                                        LR 0.000000    Time 0.049873    
2023-01-05 16:51:13,552 - Epoch: [197][   20/   37]    Overall Loss 0.637640    Objective Loss 0.637640                                        LR 0.000000    Time 0.039221    
2023-01-05 16:51:13,857 - Epoch: [197][   30/   37]    Overall Loss 0.636609    Objective Loss 0.636609                                        LR 0.000000    Time 0.036222    
2023-01-05 16:51:14,052 - Epoch: [197][   37/   37]    Overall Loss 0.633852    Objective Loss 0.633852    Top1 64.435146    LR 0.000000    Time 0.034644    
2023-01-05 16:51:14,130 - --- validate (epoch=197)-----------
2023-01-05 16:51:14,131 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:14,663 - Epoch: [197][    5/    5]    Loss 0.666239    Top1 62.786260    
2023-01-05 16:51:14,734 - ==> Top1: 62.786    Loss: 0.666

2023-01-05 16:51:14,734 - ==> Confusion:
[[316 113   0]
 [277 342   0]
 [  0   0   0]]

2023-01-05 16:51:14,737 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 197]
2023-01-05 16:51:14,737 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:14,766 - 

2023-01-05 16:51:14,766 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:15,280 - Epoch: [198][   10/   37]    Overall Loss 0.639053    Objective Loss 0.639053                                        LR 0.000000    Time 0.051211    
2023-01-05 16:51:15,569 - Epoch: [198][   20/   37]    Overall Loss 0.633520    Objective Loss 0.633520                                        LR 0.000000    Time 0.040045    
2023-01-05 16:51:15,875 - Epoch: [198][   30/   37]    Overall Loss 0.633530    Objective Loss 0.633530                                        LR 0.000000    Time 0.036812    
2023-01-05 16:51:16,068 - Epoch: [198][   37/   37]    Overall Loss 0.633826    Objective Loss 0.633826    Top1 63.389121    LR 0.000000    Time 0.035056    
2023-01-05 16:51:16,132 - --- validate (epoch=198)-----------
2023-01-05 16:51:16,133 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:16,373 - Epoch: [198][    5/    5]    Loss 0.644614    Top1 62.595420    
2023-01-05 16:51:16,442 - ==> Top1: 62.595    Loss: 0.645

2023-01-05 16:51:16,442 - ==> Confusion:
[[328 101   0]
 [291 328   0]
 [  0   0   0]]

2023-01-05 16:51:16,444 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 197]
2023-01-05 16:51:16,444 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:16,454 - 

2023-01-05 16:51:16,454 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-05 16:51:16,951 - Epoch: [199][   10/   37]    Overall Loss 0.631601    Objective Loss 0.631601                                        LR 0.000000    Time 0.049583    
2023-01-05 16:51:17,241 - Epoch: [199][   20/   37]    Overall Loss 0.634088    Objective Loss 0.634088                                        LR 0.000000    Time 0.039300    
2023-01-05 16:51:17,539 - Epoch: [199][   30/   37]    Overall Loss 0.634317    Objective Loss 0.634317                                        LR 0.000000    Time 0.036035    
2023-01-05 16:51:17,733 - Epoch: [199][   37/   37]    Overall Loss 0.633610    Objective Loss 0.633610    Top1 64.016736    LR 0.000000    Time 0.034466    
2023-01-05 16:51:17,809 - --- validate (epoch=199)-----------
2023-01-05 16:51:17,809 - 1048 samples (256 per mini-batch)
2023-01-05 16:51:18,052 - Epoch: [199][    5/    5]    Loss 0.656797    Top1 62.500000    
2023-01-05 16:51:18,114 - ==> Top1: 62.500    Loss: 0.657

2023-01-05 16:51:18,114 - ==> Confusion:
[[314 115   0]
 [278 341   0]
 [  0   0   0]]

2023-01-05 16:51:18,116 - ==> Best [Top1: 62.786   Sparsity:0.00   Params: 361664 on epoch: 197]
2023-01-05 16:51:18,116 - Saving checkpoint to: logs/2023.01.05-164525/qat_checkpoint.pth.tar
2023-01-05 16:51:18,125 - --- test ---------------------
2023-01-05 16:51:18,125 - 1317 samples (256 per mini-batch)
2023-01-05 16:51:18,404 - Test: [    6/    6]    Loss 0.658664    Top1 60.060744    
2023-01-05 16:51:18,468 - ==> Top1: 60.061    Loss: 0.659

2023-01-05 16:51:18,468 - ==> Confusion:
[[454 107   0]
 [419 337   0]
 [  0   0   0]]

2023-01-05 16:51:18,479 - 
2023-01-05 16:51:18,479 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.05-164525/2023.01.05-164525.log
