2023-01-06 16:02:24,116 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-160224/2023.01.06-160224.log
2023-01-06 16:02:26,159 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 16:02:26,159 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 16:02:33,232 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-06 16:02:33,232 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 16:02:33,236 - 

2023-01-06 16:02:33,236 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:33,791 - Epoch: [0][   10/   37]    Overall Loss 1.097414    Objective Loss 1.097414                                        LR 0.000060    Time 0.055462    
2023-01-06 16:02:33,909 - Epoch: [0][   20/   37]    Overall Loss 1.096124    Objective Loss 1.096124                                        LR 0.000060    Time 0.033603    
2023-01-06 16:02:34,030 - Epoch: [0][   30/   37]    Overall Loss 1.094392    Objective Loss 1.094392                                        LR 0.000060    Time 0.026422    
2023-01-06 16:02:34,094 - Epoch: [0][   37/   37]    Overall Loss 1.092675    Objective Loss 1.092675    Top1 56.276151    LR 0.000060    Time 0.023149    
2023-01-06 16:02:34,166 - --- validate (epoch=0)-----------
2023-01-06 16:02:34,166 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:34,374 - Epoch: [0][    5/    5]    Loss 1.081208    Top1 59.064885    
2023-01-06 16:02:34,441 - ==> Top1: 59.065    Loss: 1.081

2023-01-06 16:02:34,441 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:34,442 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 0]
2023-01-06 16:02:34,443 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:34,447 - 

2023-01-06 16:02:34,447 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:34,755 - Epoch: [1][   10/   37]    Overall Loss 1.075359    Objective Loss 1.075359                                        LR 0.000060    Time 0.030685    
2023-01-06 16:02:34,888 - Epoch: [1][   20/   37]    Overall Loss 1.065891    Objective Loss 1.065891                                        LR 0.000060    Time 0.021988    
2023-01-06 16:02:35,049 - Epoch: [1][   30/   37]    Overall Loss 1.052432    Objective Loss 1.052432                                        LR 0.000060    Time 0.019992    
2023-01-06 16:02:35,142 - Epoch: [1][   37/   37]    Overall Loss 1.040103    Objective Loss 1.040103    Top1 56.903766    LR 0.000060    Time 0.018714    
2023-01-06 16:02:35,218 - --- validate (epoch=1)-----------
2023-01-06 16:02:35,218 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:35,426 - Epoch: [1][    5/    5]    Loss 0.962635    Top1 59.064885    
2023-01-06 16:02:35,495 - ==> Top1: 59.065    Loss: 0.963

2023-01-06 16:02:35,496 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:35,497 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 1]
2023-01-06 16:02:35,497 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:35,502 - 

2023-01-06 16:02:35,502 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:35,806 - Epoch: [2][   10/   37]    Overall Loss 0.939859    Objective Loss 0.939859                                        LR 0.000060    Time 0.030335    
2023-01-06 16:02:35,913 - Epoch: [2][   20/   37]    Overall Loss 0.909198    Objective Loss 0.909198                                        LR 0.000060    Time 0.020524    
2023-01-06 16:02:36,028 - Epoch: [2][   30/   37]    Overall Loss 0.881264    Objective Loss 0.881264                                        LR 0.000060    Time 0.017448    
2023-01-06 16:02:36,085 - Epoch: [2][   37/   37]    Overall Loss 0.865163    Objective Loss 0.865163    Top1 58.368201    LR 0.000060    Time 0.015681    
2023-01-06 16:02:36,157 - --- validate (epoch=2)-----------
2023-01-06 16:02:36,157 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:36,366 - Epoch: [2][    5/    5]    Loss 0.786473    Top1 59.064885    
2023-01-06 16:02:36,438 - ==> Top1: 59.065    Loss: 0.786

2023-01-06 16:02:36,438 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:36,439 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 2]
2023-01-06 16:02:36,439 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:36,444 - 

2023-01-06 16:02:36,444 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:36,745 - Epoch: [3][   10/   37]    Overall Loss 0.776302    Objective Loss 0.776302                                        LR 0.000060    Time 0.030060    
2023-01-06 16:02:36,843 - Epoch: [3][   20/   37]    Overall Loss 0.766332    Objective Loss 0.766332                                        LR 0.000060    Time 0.019872    
2023-01-06 16:02:36,950 - Epoch: [3][   30/   37]    Overall Loss 0.761610    Objective Loss 0.761610                                        LR 0.000060    Time 0.016805    
2023-01-06 16:02:37,017 - Epoch: [3][   37/   37]    Overall Loss 0.756934    Objective Loss 0.756934    Top1 58.786611    LR 0.000060    Time 0.015439    
2023-01-06 16:02:37,088 - --- validate (epoch=3)-----------
2023-01-06 16:02:37,088 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:37,295 - Epoch: [3][    5/    5]    Loss 0.730887    Top1 59.064885    
2023-01-06 16:02:37,360 - ==> Top1: 59.065    Loss: 0.731

2023-01-06 16:02:37,361 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:37,362 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 3]
2023-01-06 16:02:37,362 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:37,367 - 

2023-01-06 16:02:37,367 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:37,687 - Epoch: [4][   10/   37]    Overall Loss 0.741477    Objective Loss 0.741477                                        LR 0.000060    Time 0.031916    
2023-01-06 16:02:37,808 - Epoch: [4][   20/   37]    Overall Loss 0.738287    Objective Loss 0.738287                                        LR 0.000060    Time 0.021981    
2023-01-06 16:02:37,930 - Epoch: [4][   30/   37]    Overall Loss 0.735005    Objective Loss 0.735005                                        LR 0.000060    Time 0.018709    
2023-01-06 16:02:38,000 - Epoch: [4][   37/   37]    Overall Loss 0.733458    Objective Loss 0.733458    Top1 55.439331    LR 0.000060    Time 0.017063    
2023-01-06 16:02:38,083 - --- validate (epoch=4)-----------
2023-01-06 16:02:38,083 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:38,290 - Epoch: [4][    5/    5]    Loss 0.711407    Top1 59.064885    
2023-01-06 16:02:38,353 - ==> Top1: 59.065    Loss: 0.711

2023-01-06 16:02:38,353 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:38,354 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 4]
2023-01-06 16:02:38,354 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:38,359 - 

2023-01-06 16:02:38,359 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:38,657 - Epoch: [5][   10/   37]    Overall Loss 0.730483    Objective Loss 0.730483                                        LR 0.000060    Time 0.029717    
2023-01-06 16:02:38,752 - Epoch: [5][   20/   37]    Overall Loss 0.726927    Objective Loss 0.726927                                        LR 0.000060    Time 0.019592    
2023-01-06 16:02:38,851 - Epoch: [5][   30/   37]    Overall Loss 0.723391    Objective Loss 0.723391                                        LR 0.000060    Time 0.016362    
2023-01-06 16:02:38,911 - Epoch: [5][   37/   37]    Overall Loss 0.721050    Objective Loss 0.721050    Top1 58.158996    LR 0.000060    Time 0.014879    
2023-01-06 16:02:38,989 - --- validate (epoch=5)-----------
2023-01-06 16:02:38,989 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:39,197 - Epoch: [5][    5/    5]    Loss 0.699960    Top1 59.064885    
2023-01-06 16:02:39,258 - ==> Top1: 59.065    Loss: 0.700

2023-01-06 16:02:39,259 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:39,260 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:02:39,260 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:39,265 - 

2023-01-06 16:02:39,265 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:39,564 - Epoch: [6][   10/   37]    Overall Loss 0.715641    Objective Loss 0.715641                                        LR 0.000060    Time 0.029813    
2023-01-06 16:02:39,657 - Epoch: [6][   20/   37]    Overall Loss 0.712076    Objective Loss 0.712076                                        LR 0.000060    Time 0.019528    
2023-01-06 16:02:39,756 - Epoch: [6][   30/   37]    Overall Loss 0.712491    Objective Loss 0.712491                                        LR 0.000060    Time 0.016331    
2023-01-06 16:02:39,817 - Epoch: [6][   37/   37]    Overall Loss 0.712823    Objective Loss 0.712823    Top1 52.719665    LR 0.000060    Time 0.014867    
2023-01-06 16:02:39,882 - --- validate (epoch=6)-----------
2023-01-06 16:02:39,882 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:40,092 - Epoch: [6][    5/    5]    Loss 0.703725    Top1 59.064885    
2023-01-06 16:02:40,157 - ==> Top1: 59.065    Loss: 0.704

2023-01-06 16:02:40,158 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:40,159 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 6]
2023-01-06 16:02:40,159 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:40,164 - 

2023-01-06 16:02:40,164 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:40,495 - Epoch: [7][   10/   37]    Overall Loss 0.711930    Objective Loss 0.711930                                        LR 0.000060    Time 0.032978    
2023-01-06 16:02:40,621 - Epoch: [7][   20/   37]    Overall Loss 0.710009    Objective Loss 0.710009                                        LR 0.000060    Time 0.022759    
2023-01-06 16:02:40,752 - Epoch: [7][   30/   37]    Overall Loss 0.708820    Objective Loss 0.708820                                        LR 0.000060    Time 0.019547    
2023-01-06 16:02:40,833 - Epoch: [7][   37/   37]    Overall Loss 0.707421    Objective Loss 0.707421    Top1 59.623431    LR 0.000060    Time 0.018021    
2023-01-06 16:02:40,901 - --- validate (epoch=7)-----------
2023-01-06 16:02:40,901 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:41,109 - Epoch: [7][    5/    5]    Loss 0.697027    Top1 59.064885    
2023-01-06 16:02:41,173 - ==> Top1: 59.065    Loss: 0.697

2023-01-06 16:02:41,173 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:41,174 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 7]
2023-01-06 16:02:41,174 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:41,180 - 

2023-01-06 16:02:41,180 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:41,627 - Epoch: [8][   10/   37]    Overall Loss 0.706521    Objective Loss 0.706521                                        LR 0.000060    Time 0.044616    
2023-01-06 16:02:41,742 - Epoch: [8][   20/   37]    Overall Loss 0.704500    Objective Loss 0.704500                                        LR 0.000060    Time 0.028074    
2023-01-06 16:02:41,857 - Epoch: [8][   30/   37]    Overall Loss 0.704000    Objective Loss 0.704000                                        LR 0.000060    Time 0.022529    
2023-01-06 16:02:41,926 - Epoch: [8][   37/   37]    Overall Loss 0.703469    Objective Loss 0.703469    Top1 56.066946    LR 0.000060    Time 0.020129    
2023-01-06 16:02:42,005 - --- validate (epoch=8)-----------
2023-01-06 16:02:42,005 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:42,218 - Epoch: [8][    5/    5]    Loss 0.707166    Top1 59.064885    
2023-01-06 16:02:42,280 - ==> Top1: 59.065    Loss: 0.707

2023-01-06 16:02:42,280 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:42,282 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 8]
2023-01-06 16:02:42,282 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:42,287 - 

2023-01-06 16:02:42,287 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:42,607 - Epoch: [9][   10/   37]    Overall Loss 0.700040    Objective Loss 0.700040                                        LR 0.000060    Time 0.031945    
2023-01-06 16:02:42,713 - Epoch: [9][   20/   37]    Overall Loss 0.702339    Objective Loss 0.702339                                        LR 0.000060    Time 0.021259    
2023-01-06 16:02:42,819 - Epoch: [9][   30/   37]    Overall Loss 0.701636    Objective Loss 0.701636                                        LR 0.000060    Time 0.017680    
2023-01-06 16:02:42,875 - Epoch: [9][   37/   37]    Overall Loss 0.700662    Objective Loss 0.700662    Top1 55.230126    LR 0.000060    Time 0.015856    
2023-01-06 16:02:42,941 - --- validate (epoch=9)-----------
2023-01-06 16:02:42,941 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:43,153 - Epoch: [9][    5/    5]    Loss 0.700207    Top1 59.064885    
2023-01-06 16:02:43,218 - ==> Top1: 59.065    Loss: 0.700

2023-01-06 16:02:43,218 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:43,219 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 9]
2023-01-06 16:02:43,219 - Saving checkpoint to: logs/2023.01.06-160224/checkpoint.pth.tar
2023-01-06 16:02:43,237 - 

2023-01-06 16:02:43,237 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:43,593 - Epoch: [10][   10/   37]    Overall Loss 0.872419    Objective Loss 0.872419                                        LR 0.000060    Time 0.035574    
2023-01-06 16:02:43,740 - Epoch: [10][   20/   37]    Overall Loss 0.797785    Objective Loss 0.797785                                        LR 0.000060    Time 0.025127    
2023-01-06 16:02:43,886 - Epoch: [10][   30/   37]    Overall Loss 0.760164    Objective Loss 0.760164                                        LR 0.000060    Time 0.021579    
2023-01-06 16:02:43,963 - Epoch: [10][   37/   37]    Overall Loss 0.745932    Objective Loss 0.745932    Top1 60.251046    LR 0.000060    Time 0.019594    
2023-01-06 16:02:44,044 - --- validate (epoch=10)-----------
2023-01-06 16:02:44,045 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:44,271 - Epoch: [10][    5/    5]    Loss 0.679821    Top1 59.064885    
2023-01-06 16:02:44,345 - ==> Top1: 59.065    Loss: 0.680

2023-01-06 16:02:44,346 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:44,347 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 10]
2023-01-06 16:02:44,347 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:44,352 - 

2023-01-06 16:02:44,352 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:44,705 - Epoch: [11][   10/   37]    Overall Loss 0.687594    Objective Loss 0.687594                                        LR 0.000060    Time 0.035219    
2023-01-06 16:02:44,843 - Epoch: [11][   20/   37]    Overall Loss 0.684359    Objective Loss 0.684359                                        LR 0.000060    Time 0.024482    
2023-01-06 16:02:44,994 - Epoch: [11][   30/   37]    Overall Loss 0.685018    Objective Loss 0.685018                                        LR 0.000060    Time 0.021351    
2023-01-06 16:02:45,082 - Epoch: [11][   37/   37]    Overall Loss 0.684148    Objective Loss 0.684148    Top1 58.158996    LR 0.000060    Time 0.019675    
2023-01-06 16:02:45,154 - --- validate (epoch=11)-----------
2023-01-06 16:02:45,154 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:45,379 - Epoch: [11][    5/    5]    Loss 0.677952    Top1 59.064885    
2023-01-06 16:02:45,450 - ==> Top1: 59.065    Loss: 0.678

2023-01-06 16:02:45,450 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:45,451 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 11]
2023-01-06 16:02:45,452 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:45,457 - 

2023-01-06 16:02:45,457 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:45,818 - Epoch: [12][   10/   37]    Overall Loss 0.684193    Objective Loss 0.684193                                        LR 0.000060    Time 0.036066    
2023-01-06 16:02:45,983 - Epoch: [12][   20/   37]    Overall Loss 0.683219    Objective Loss 0.683219                                        LR 0.000060    Time 0.026187    
2023-01-06 16:02:46,160 - Epoch: [12][   30/   37]    Overall Loss 0.683894    Objective Loss 0.683894                                        LR 0.000060    Time 0.023344    
2023-01-06 16:02:46,255 - Epoch: [12][   37/   37]    Overall Loss 0.683479    Objective Loss 0.683479    Top1 56.903766    LR 0.000060    Time 0.021492    
2023-01-06 16:02:46,329 - --- validate (epoch=12)-----------
2023-01-06 16:02:46,329 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:46,564 - Epoch: [12][    5/    5]    Loss 0.682157    Top1 59.064885    
2023-01-06 16:02:46,635 - ==> Top1: 59.065    Loss: 0.682

2023-01-06 16:02:46,635 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:46,636 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 12]
2023-01-06 16:02:46,636 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:46,641 - 

2023-01-06 16:02:46,641 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:47,011 - Epoch: [13][   10/   37]    Overall Loss 0.684089    Objective Loss 0.684089                                        LR 0.000060    Time 0.036918    
2023-01-06 16:02:47,189 - Epoch: [13][   20/   37]    Overall Loss 0.682560    Objective Loss 0.682560                                        LR 0.000060    Time 0.027322    
2023-01-06 16:02:47,368 - Epoch: [13][   30/   37]    Overall Loss 0.682243    Objective Loss 0.682243                                        LR 0.000060    Time 0.024167    
2023-01-06 16:02:47,463 - Epoch: [13][   37/   37]    Overall Loss 0.683083    Objective Loss 0.683083    Top1 57.531381    LR 0.000060    Time 0.022157    
2023-01-06 16:02:47,546 - --- validate (epoch=13)-----------
2023-01-06 16:02:47,547 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:47,788 - Epoch: [13][    5/    5]    Loss 0.671028    Top1 59.064885    
2023-01-06 16:02:47,853 - ==> Top1: 59.065    Loss: 0.671

2023-01-06 16:02:47,854 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:47,855 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 13]
2023-01-06 16:02:47,855 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:47,860 - 

2023-01-06 16:02:47,860 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:48,221 - Epoch: [14][   10/   37]    Overall Loss 0.682669    Objective Loss 0.682669                                        LR 0.000060    Time 0.036090    
2023-01-06 16:02:48,370 - Epoch: [14][   20/   37]    Overall Loss 0.684972    Objective Loss 0.684972                                        LR 0.000060    Time 0.025479    
2023-01-06 16:02:48,517 - Epoch: [14][   30/   37]    Overall Loss 0.683924    Objective Loss 0.683924                                        LR 0.000060    Time 0.021877    
2023-01-06 16:02:48,597 - Epoch: [14][   37/   37]    Overall Loss 0.682955    Objective Loss 0.682955    Top1 58.786611    LR 0.000060    Time 0.019878    
2023-01-06 16:02:48,679 - --- validate (epoch=14)-----------
2023-01-06 16:02:48,679 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:48,906 - Epoch: [14][    5/    5]    Loss 0.671159    Top1 59.064885    
2023-01-06 16:02:48,990 - ==> Top1: 59.065    Loss: 0.671

2023-01-06 16:02:48,990 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:48,991 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 14]
2023-01-06 16:02:48,991 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:48,997 - 

2023-01-06 16:02:48,997 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:49,377 - Epoch: [15][   10/   37]    Overall Loss 0.677187    Objective Loss 0.677187                                        LR 0.000060    Time 0.037976    
2023-01-06 16:02:49,541 - Epoch: [15][   20/   37]    Overall Loss 0.683230    Objective Loss 0.683230                                        LR 0.000060    Time 0.027088    
2023-01-06 16:02:49,701 - Epoch: [15][   30/   37]    Overall Loss 0.684165    Objective Loss 0.684165                                        LR 0.000060    Time 0.023345    
2023-01-06 16:02:49,794 - Epoch: [15][   37/   37]    Overall Loss 0.683510    Objective Loss 0.683510    Top1 56.485356    LR 0.000060    Time 0.021456    
2023-01-06 16:02:49,869 - --- validate (epoch=15)-----------
2023-01-06 16:02:49,869 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:50,097 - Epoch: [15][    5/    5]    Loss 0.679450    Top1 59.064885    
2023-01-06 16:02:50,173 - ==> Top1: 59.065    Loss: 0.679

2023-01-06 16:02:50,173 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:50,174 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 15]
2023-01-06 16:02:50,174 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:50,179 - 

2023-01-06 16:02:50,179 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:50,537 - Epoch: [16][   10/   37]    Overall Loss 0.683946    Objective Loss 0.683946                                        LR 0.000060    Time 0.035693    
2023-01-06 16:02:50,688 - Epoch: [16][   20/   37]    Overall Loss 0.682296    Objective Loss 0.682296                                        LR 0.000060    Time 0.025400    
2023-01-06 16:02:50,842 - Epoch: [16][   30/   37]    Overall Loss 0.682395    Objective Loss 0.682395                                        LR 0.000060    Time 0.022032    
2023-01-06 16:02:50,931 - Epoch: [16][   37/   37]    Overall Loss 0.683043    Objective Loss 0.683043    Top1 54.393305    LR 0.000060    Time 0.020267    
2023-01-06 16:02:51,003 - --- validate (epoch=16)-----------
2023-01-06 16:02:51,003 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:51,229 - Epoch: [16][    5/    5]    Loss 0.673597    Top1 59.064885    
2023-01-06 16:02:51,298 - ==> Top1: 59.065    Loss: 0.674

2023-01-06 16:02:51,298 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:51,299 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 16]
2023-01-06 16:02:51,299 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:51,304 - 

2023-01-06 16:02:51,305 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:51,639 - Epoch: [17][   10/   37]    Overall Loss 0.683646    Objective Loss 0.683646                                        LR 0.000060    Time 0.033393    
2023-01-06 16:02:51,781 - Epoch: [17][   20/   37]    Overall Loss 0.682365    Objective Loss 0.682365                                        LR 0.000060    Time 0.023777    
2023-01-06 16:02:51,946 - Epoch: [17][   30/   37]    Overall Loss 0.683421    Objective Loss 0.683421                                        LR 0.000060    Time 0.021320    
2023-01-06 16:02:52,040 - Epoch: [17][   37/   37]    Overall Loss 0.682898    Objective Loss 0.682898    Top1 57.740586    LR 0.000060    Time 0.019842    
2023-01-06 16:02:52,121 - --- validate (epoch=17)-----------
2023-01-06 16:02:52,121 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:52,346 - Epoch: [17][    5/    5]    Loss 0.674870    Top1 59.064885    
2023-01-06 16:02:52,418 - ==> Top1: 59.065    Loss: 0.675

2023-01-06 16:02:52,419 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:52,420 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 17]
2023-01-06 16:02:52,420 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:52,424 - 

2023-01-06 16:02:52,424 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:52,779 - Epoch: [18][   10/   37]    Overall Loss 0.679531    Objective Loss 0.679531                                        LR 0.000060    Time 0.035374    
2023-01-06 16:02:52,922 - Epoch: [18][   20/   37]    Overall Loss 0.683017    Objective Loss 0.683017                                        LR 0.000060    Time 0.024834    
2023-01-06 16:02:53,077 - Epoch: [18][   30/   37]    Overall Loss 0.682872    Objective Loss 0.682872                                        LR 0.000060    Time 0.021696    
2023-01-06 16:02:53,174 - Epoch: [18][   37/   37]    Overall Loss 0.683459    Objective Loss 0.683459    Top1 57.322176    LR 0.000060    Time 0.020209    
2023-01-06 16:02:53,244 - --- validate (epoch=18)-----------
2023-01-06 16:02:53,244 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:53,476 - Epoch: [18][    5/    5]    Loss 0.666028    Top1 59.064885    
2023-01-06 16:02:53,552 - ==> Top1: 59.065    Loss: 0.666

2023-01-06 16:02:53,553 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:53,554 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 18]
2023-01-06 16:02:53,554 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:53,559 - 

2023-01-06 16:02:53,559 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:53,910 - Epoch: [19][   10/   37]    Overall Loss 0.683973    Objective Loss 0.683973                                        LR 0.000060    Time 0.035095    
2023-01-06 16:02:54,063 - Epoch: [19][   20/   37]    Overall Loss 0.681673    Objective Loss 0.681673                                        LR 0.000060    Time 0.025180    
2023-01-06 16:02:54,225 - Epoch: [19][   30/   37]    Overall Loss 0.682772    Objective Loss 0.682772                                        LR 0.000060    Time 0.022168    
2023-01-06 16:02:54,320 - Epoch: [19][   37/   37]    Overall Loss 0.682914    Objective Loss 0.682914    Top1 53.765690    LR 0.000060    Time 0.020532    
2023-01-06 16:02:54,389 - --- validate (epoch=19)-----------
2023-01-06 16:02:54,389 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:54,613 - Epoch: [19][    5/    5]    Loss 0.683471    Top1 59.064885    
2023-01-06 16:02:54,680 - ==> Top1: 59.065    Loss: 0.683

2023-01-06 16:02:54,680 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:54,682 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 19]
2023-01-06 16:02:54,682 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:54,689 - 

2023-01-06 16:02:54,689 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:55,044 - Epoch: [20][   10/   37]    Overall Loss 0.683986    Objective Loss 0.683986                                        LR 0.000060    Time 0.035386    
2023-01-06 16:02:55,192 - Epoch: [20][   20/   37]    Overall Loss 0.682796    Objective Loss 0.682796                                        LR 0.000060    Time 0.025082    
2023-01-06 16:02:55,362 - Epoch: [20][   30/   37]    Overall Loss 0.682448    Objective Loss 0.682448                                        LR 0.000060    Time 0.022365    
2023-01-06 16:02:55,456 - Epoch: [20][   37/   37]    Overall Loss 0.682526    Objective Loss 0.682526    Top1 58.368201    LR 0.000060    Time 0.020684    
2023-01-06 16:02:55,523 - --- validate (epoch=20)-----------
2023-01-06 16:02:55,524 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:55,748 - Epoch: [20][    5/    5]    Loss 0.676504    Top1 59.064885    
2023-01-06 16:02:55,821 - ==> Top1: 59.065    Loss: 0.677

2023-01-06 16:02:55,821 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:55,822 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 20]
2023-01-06 16:02:55,822 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:55,827 - 

2023-01-06 16:02:55,827 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:56,191 - Epoch: [21][   10/   37]    Overall Loss 0.680622    Objective Loss 0.680622                                        LR 0.000060    Time 0.036331    
2023-01-06 16:02:56,351 - Epoch: [21][   20/   37]    Overall Loss 0.684051    Objective Loss 0.684051                                        LR 0.000060    Time 0.026150    
2023-01-06 16:02:56,517 - Epoch: [21][   30/   37]    Overall Loss 0.682321    Objective Loss 0.682321                                        LR 0.000060    Time 0.022943    
2023-01-06 16:02:56,611 - Epoch: [21][   37/   37]    Overall Loss 0.682607    Objective Loss 0.682607    Top1 54.602510    LR 0.000060    Time 0.021149    
2023-01-06 16:02:56,713 - --- validate (epoch=21)-----------
2023-01-06 16:02:56,714 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:56,947 - Epoch: [21][    5/    5]    Loss 0.673121    Top1 59.064885    
2023-01-06 16:02:57,019 - ==> Top1: 59.065    Loss: 0.673

2023-01-06 16:02:57,020 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:57,021 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 21]
2023-01-06 16:02:57,021 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:57,026 - 

2023-01-06 16:02:57,026 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:57,387 - Epoch: [22][   10/   37]    Overall Loss 0.684340    Objective Loss 0.684340                                        LR 0.000060    Time 0.036070    
2023-01-06 16:02:57,533 - Epoch: [22][   20/   37]    Overall Loss 0.683036    Objective Loss 0.683036                                        LR 0.000060    Time 0.025323    
2023-01-06 16:02:57,688 - Epoch: [22][   30/   37]    Overall Loss 0.682430    Objective Loss 0.682430                                        LR 0.000060    Time 0.022031    
2023-01-06 16:02:57,774 - Epoch: [22][   37/   37]    Overall Loss 0.682411    Objective Loss 0.682411    Top1 54.602510    LR 0.000060    Time 0.020174    
2023-01-06 16:02:57,852 - --- validate (epoch=22)-----------
2023-01-06 16:02:57,852 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:58,081 - Epoch: [22][    5/    5]    Loss 0.680344    Top1 59.064885    
2023-01-06 16:02:58,150 - ==> Top1: 59.065    Loss: 0.680

2023-01-06 16:02:58,150 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:58,151 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 22]
2023-01-06 16:02:58,151 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:58,156 - 

2023-01-06 16:02:58,156 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:58,635 - Epoch: [23][   10/   37]    Overall Loss 0.682097    Objective Loss 0.682097                                        LR 0.000060    Time 0.047780    
2023-01-06 16:02:58,803 - Epoch: [23][   20/   37]    Overall Loss 0.681532    Objective Loss 0.681532                                        LR 0.000060    Time 0.032300    
2023-01-06 16:02:58,973 - Epoch: [23][   30/   37]    Overall Loss 0.681805    Objective Loss 0.681805                                        LR 0.000060    Time 0.027168    
2023-01-06 16:02:59,067 - Epoch: [23][   37/   37]    Overall Loss 0.681240    Objective Loss 0.681240    Top1 59.205021    LR 0.000060    Time 0.024566    
2023-01-06 16:02:59,144 - --- validate (epoch=23)-----------
2023-01-06 16:02:59,144 - 1048 samples (256 per mini-batch)
2023-01-06 16:02:59,366 - Epoch: [23][    5/    5]    Loss 0.671314    Top1 59.064885    
2023-01-06 16:02:59,450 - ==> Top1: 59.065    Loss: 0.671

2023-01-06 16:02:59,450 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:02:59,452 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 23]
2023-01-06 16:02:59,452 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:02:59,457 - 

2023-01-06 16:02:59,457 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:02:59,800 - Epoch: [24][   10/   37]    Overall Loss 0.675650    Objective Loss 0.675650                                        LR 0.000060    Time 0.034306    
2023-01-06 16:02:59,965 - Epoch: [24][   20/   37]    Overall Loss 0.678661    Objective Loss 0.678661                                        LR 0.000060    Time 0.025346    
2023-01-06 16:03:00,132 - Epoch: [24][   30/   37]    Overall Loss 0.680508    Objective Loss 0.680508                                        LR 0.000060    Time 0.022448    
2023-01-06 16:03:00,226 - Epoch: [24][   37/   37]    Overall Loss 0.680415    Objective Loss 0.680415    Top1 57.322176    LR 0.000060    Time 0.020741    
2023-01-06 16:03:00,296 - --- validate (epoch=24)-----------
2023-01-06 16:03:00,296 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:00,527 - Epoch: [24][    5/    5]    Loss 0.673562    Top1 59.064885    
2023-01-06 16:03:00,588 - ==> Top1: 59.065    Loss: 0.674

2023-01-06 16:03:00,589 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:03:00,590 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 24]
2023-01-06 16:03:00,590 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:00,595 - 

2023-01-06 16:03:00,595 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:00,947 - Epoch: [25][   10/   37]    Overall Loss 0.680146    Objective Loss 0.680146                                        LR 0.000060    Time 0.035172    
2023-01-06 16:03:01,107 - Epoch: [25][   20/   37]    Overall Loss 0.679855    Objective Loss 0.679855                                        LR 0.000060    Time 0.025568    
2023-01-06 16:03:01,276 - Epoch: [25][   30/   37]    Overall Loss 0.678839    Objective Loss 0.678839                                        LR 0.000060    Time 0.022669    
2023-01-06 16:03:01,371 - Epoch: [25][   37/   37]    Overall Loss 0.679961    Objective Loss 0.679961    Top1 54.393305    LR 0.000060    Time 0.020921    
2023-01-06 16:03:01,446 - --- validate (epoch=25)-----------
2023-01-06 16:03:01,446 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:01,671 - Epoch: [25][    5/    5]    Loss 0.672580    Top1 59.064885    
2023-01-06 16:03:01,733 - ==> Top1: 59.065    Loss: 0.673

2023-01-06 16:03:01,734 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:03:01,735 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 25]
2023-01-06 16:03:01,735 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:01,740 - 

2023-01-06 16:03:01,740 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:02,112 - Epoch: [26][   10/   37]    Overall Loss 0.680150    Objective Loss 0.680150                                        LR 0.000060    Time 0.037150    
2023-01-06 16:03:02,286 - Epoch: [26][   20/   37]    Overall Loss 0.681087    Objective Loss 0.681087                                        LR 0.000060    Time 0.027197    
2023-01-06 16:03:02,452 - Epoch: [26][   30/   37]    Overall Loss 0.679544    Objective Loss 0.679544                                        LR 0.000060    Time 0.023670    
2023-01-06 16:03:02,548 - Epoch: [26][   37/   37]    Overall Loss 0.679627    Objective Loss 0.679627    Top1 55.857741    LR 0.000060    Time 0.021767    
2023-01-06 16:03:02,630 - --- validate (epoch=26)-----------
2023-01-06 16:03:02,630 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:02,852 - Epoch: [26][    5/    5]    Loss 0.672358    Top1 59.064885    
2023-01-06 16:03:02,919 - ==> Top1: 59.065    Loss: 0.672

2023-01-06 16:03:02,919 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:03:02,920 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 26]
2023-01-06 16:03:02,920 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:02,925 - 

2023-01-06 16:03:02,925 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:03,275 - Epoch: [27][   10/   37]    Overall Loss 0.676829    Objective Loss 0.676829                                        LR 0.000060    Time 0.034978    
2023-01-06 16:03:03,441 - Epoch: [27][   20/   37]    Overall Loss 0.676854    Objective Loss 0.676854                                        LR 0.000060    Time 0.025771    
2023-01-06 16:03:03,615 - Epoch: [27][   30/   37]    Overall Loss 0.676288    Objective Loss 0.676288                                        LR 0.000060    Time 0.022959    
2023-01-06 16:03:03,709 - Epoch: [27][   37/   37]    Overall Loss 0.676493    Objective Loss 0.676493    Top1 59.623431    LR 0.000060    Time 0.021131    
2023-01-06 16:03:03,783 - --- validate (epoch=27)-----------
2023-01-06 16:03:03,783 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:04,016 - Epoch: [27][    5/    5]    Loss 0.674948    Top1 59.064885    
2023-01-06 16:03:04,093 - ==> Top1: 59.065    Loss: 0.675

2023-01-06 16:03:04,093 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-06 16:03:04,094 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 46192 on epoch: 27]
2023-01-06 16:03:04,094 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:04,099 - 

2023-01-06 16:03:04,099 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:04,461 - Epoch: [28][   10/   37]    Overall Loss 0.672327    Objective Loss 0.672327                                        LR 0.000060    Time 0.036104    
2023-01-06 16:03:04,620 - Epoch: [28][   20/   37]    Overall Loss 0.673577    Objective Loss 0.673577                                        LR 0.000060    Time 0.025985    
2023-01-06 16:03:04,783 - Epoch: [28][   30/   37]    Overall Loss 0.672491    Objective Loss 0.672491                                        LR 0.000060    Time 0.022761    
2023-01-06 16:03:04,877 - Epoch: [28][   37/   37]    Overall Loss 0.672557    Objective Loss 0.672557    Top1 58.368201    LR 0.000060    Time 0.020985    
2023-01-06 16:03:04,949 - --- validate (epoch=28)-----------
2023-01-06 16:03:04,950 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:05,188 - Epoch: [28][    5/    5]    Loss 0.671442    Top1 59.637405    
2023-01-06 16:03:05,251 - ==> Top1: 59.637    Loss: 0.671

2023-01-06 16:03:05,252 - ==> Confusion:
[[ 15 414   0]
 [  9 610   0]
 [  0   0   0]]

2023-01-06 16:03:05,253 - ==> Best [Top1: 59.637   Sparsity:0.00   Params: 46192 on epoch: 28]
2023-01-06 16:03:05,253 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:05,258 - 

2023-01-06 16:03:05,258 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:05,617 - Epoch: [29][   10/   37]    Overall Loss 0.668790    Objective Loss 0.668790                                        LR 0.000060    Time 0.035840    
2023-01-06 16:03:05,793 - Epoch: [29][   20/   37]    Overall Loss 0.667713    Objective Loss 0.667713                                        LR 0.000060    Time 0.026718    
2023-01-06 16:03:05,974 - Epoch: [29][   30/   37]    Overall Loss 0.667975    Objective Loss 0.667975                                        LR 0.000060    Time 0.023835    
2023-01-06 16:03:06,065 - Epoch: [29][   37/   37]    Overall Loss 0.666701    Objective Loss 0.666701    Top1 57.322176    LR 0.000060    Time 0.021775    
2023-01-06 16:03:06,150 - --- validate (epoch=29)-----------
2023-01-06 16:03:06,150 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:06,375 - Epoch: [29][    5/    5]    Loss 0.651616    Top1 59.732824    
2023-01-06 16:03:06,442 - ==> Top1: 59.733    Loss: 0.652

2023-01-06 16:03:06,443 - ==> Confusion:
[[ 13 416   0]
 [  6 613   0]
 [  0   0   0]]

2023-01-06 16:03:06,444 - ==> Best [Top1: 59.733   Sparsity:0.00   Params: 46192 on epoch: 29]
2023-01-06 16:03:06,444 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:06,449 - 

2023-01-06 16:03:06,449 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:06,817 - Epoch: [30][   10/   37]    Overall Loss 0.660768    Objective Loss 0.660768                                        LR 0.000060    Time 0.036674    
2023-01-06 16:03:06,976 - Epoch: [30][   20/   37]    Overall Loss 0.660800    Objective Loss 0.660800                                        LR 0.000060    Time 0.026266    
2023-01-06 16:03:07,140 - Epoch: [30][   30/   37]    Overall Loss 0.658361    Objective Loss 0.658361                                        LR 0.000060    Time 0.022981    
2023-01-06 16:03:07,235 - Epoch: [30][   37/   37]    Overall Loss 0.656980    Objective Loss 0.656980    Top1 57.740586    LR 0.000060    Time 0.021186    
2023-01-06 16:03:07,315 - --- validate (epoch=30)-----------
2023-01-06 16:03:07,316 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:07,543 - Epoch: [30][    5/    5]    Loss 0.635098    Top1 61.450382    
2023-01-06 16:03:07,613 - ==> Top1: 61.450    Loss: 0.635

2023-01-06 16:03:07,613 - ==> Confusion:
[[ 51 378   0]
 [ 26 593   0]
 [  0   0   0]]

2023-01-06 16:03:07,614 - ==> Best [Top1: 61.450   Sparsity:0.00   Params: 46192 on epoch: 30]
2023-01-06 16:03:07,614 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:07,619 - 

2023-01-06 16:03:07,619 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:07,966 - Epoch: [31][   10/   37]    Overall Loss 0.648026    Objective Loss 0.648026                                        LR 0.000060    Time 0.034641    
2023-01-06 16:03:08,102 - Epoch: [31][   20/   37]    Overall Loss 0.645938    Objective Loss 0.645938                                        LR 0.000060    Time 0.024078    
2023-01-06 16:03:08,239 - Epoch: [31][   30/   37]    Overall Loss 0.642383    Objective Loss 0.642383                                        LR 0.000060    Time 0.020597    
2023-01-06 16:03:08,318 - Epoch: [31][   37/   37]    Overall Loss 0.638849    Objective Loss 0.638849    Top1 66.108787    LR 0.000060    Time 0.018834    
2023-01-06 16:03:08,396 - --- validate (epoch=31)-----------
2023-01-06 16:03:08,396 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:08,625 - Epoch: [31][    5/    5]    Loss 0.608348    Top1 63.549618    
2023-01-06 16:03:08,695 - ==> Top1: 63.550    Loss: 0.608

2023-01-06 16:03:08,696 - ==> Confusion:
[[ 89 340   0]
 [ 42 577   0]
 [  0   0   0]]

2023-01-06 16:03:08,697 - ==> Best [Top1: 63.550   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:03:08,697 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:08,701 - 

2023-01-06 16:03:08,702 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:09,056 - Epoch: [32][   10/   37]    Overall Loss 0.626531    Objective Loss 0.626531                                        LR 0.000060    Time 0.035322    
2023-01-06 16:03:09,202 - Epoch: [32][   20/   37]    Overall Loss 0.623205    Objective Loss 0.623205                                        LR 0.000060    Time 0.024956    
2023-01-06 16:03:09,342 - Epoch: [32][   30/   37]    Overall Loss 0.621049    Objective Loss 0.621049                                        LR 0.000060    Time 0.021300    
2023-01-06 16:03:09,434 - Epoch: [32][   37/   37]    Overall Loss 0.619238    Objective Loss 0.619238    Top1 67.364017    LR 0.000060    Time 0.019740    
2023-01-06 16:03:09,514 - --- validate (epoch=32)-----------
2023-01-06 16:03:09,514 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:09,738 - Epoch: [32][    5/    5]    Loss 0.589008    Top1 67.175573    
2023-01-06 16:03:09,797 - ==> Top1: 67.176    Loss: 0.589

2023-01-06 16:03:09,797 - ==> Confusion:
[[145 284   0]
 [ 60 559   0]
 [  0   0   0]]

2023-01-06 16:03:09,798 - ==> Best [Top1: 67.176   Sparsity:0.00   Params: 46192 on epoch: 32]
2023-01-06 16:03:09,798 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:09,804 - 

2023-01-06 16:03:09,804 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:10,162 - Epoch: [33][   10/   37]    Overall Loss 0.607833    Objective Loss 0.607833                                        LR 0.000060    Time 0.035739    
2023-01-06 16:03:10,323 - Epoch: [33][   20/   37]    Overall Loss 0.603787    Objective Loss 0.603787                                        LR 0.000060    Time 0.025892    
2023-01-06 16:03:10,475 - Epoch: [33][   30/   37]    Overall Loss 0.603600    Objective Loss 0.603600                                        LR 0.000060    Time 0.022327    
2023-01-06 16:03:10,554 - Epoch: [33][   37/   37]    Overall Loss 0.603156    Objective Loss 0.603156    Top1 62.970711    LR 0.000060    Time 0.020220    
2023-01-06 16:03:10,633 - --- validate (epoch=33)-----------
2023-01-06 16:03:10,634 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:10,862 - Epoch: [33][    5/    5]    Loss 0.578315    Top1 66.793893    
2023-01-06 16:03:10,925 - ==> Top1: 66.794    Loss: 0.578

2023-01-06 16:03:10,926 - ==> Confusion:
[[139 290   0]
 [ 58 561   0]
 [  0   0   0]]

2023-01-06 16:03:10,927 - ==> Best [Top1: 67.176   Sparsity:0.00   Params: 46192 on epoch: 32]
2023-01-06 16:03:10,927 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:10,931 - 

2023-01-06 16:03:10,931 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:11,272 - Epoch: [34][   10/   37]    Overall Loss 0.596394    Objective Loss 0.596394                                        LR 0.000060    Time 0.033937    
2023-01-06 16:03:11,410 - Epoch: [34][   20/   37]    Overall Loss 0.599686    Objective Loss 0.599686                                        LR 0.000060    Time 0.023869    
2023-01-06 16:03:11,546 - Epoch: [34][   30/   37]    Overall Loss 0.594461    Objective Loss 0.594461                                        LR 0.000060    Time 0.020449    
2023-01-06 16:03:11,625 - Epoch: [34][   37/   37]    Overall Loss 0.590056    Objective Loss 0.590056    Top1 68.619247    LR 0.000060    Time 0.018703    
2023-01-06 16:03:11,699 - --- validate (epoch=34)-----------
2023-01-06 16:03:11,700 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:11,926 - Epoch: [34][    5/    5]    Loss 0.566897    Top1 70.324427    
2023-01-06 16:03:12,002 - ==> Top1: 70.324    Loss: 0.567

2023-01-06 16:03:12,002 - ==> Confusion:
[[202 227   0]
 [ 84 535   0]
 [  0   0   0]]

2023-01-06 16:03:12,003 - ==> Best [Top1: 70.324   Sparsity:0.00   Params: 46192 on epoch: 34]
2023-01-06 16:03:12,003 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:12,009 - 

2023-01-06 16:03:12,009 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:12,359 - Epoch: [35][   10/   37]    Overall Loss 0.584725    Objective Loss 0.584725                                        LR 0.000060    Time 0.034968    
2023-01-06 16:03:12,509 - Epoch: [35][   20/   37]    Overall Loss 0.577883    Objective Loss 0.577883                                        LR 0.000060    Time 0.024967    
2023-01-06 16:03:12,668 - Epoch: [35][   30/   37]    Overall Loss 0.580705    Objective Loss 0.580705                                        LR 0.000060    Time 0.021894    
2023-01-06 16:03:12,757 - Epoch: [35][   37/   37]    Overall Loss 0.581035    Objective Loss 0.581035    Top1 68.410042    LR 0.000060    Time 0.020151    
2023-01-06 16:03:12,830 - --- validate (epoch=35)-----------
2023-01-06 16:03:12,831 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:13,058 - Epoch: [35][    5/    5]    Loss 0.558085    Top1 70.419847    
2023-01-06 16:03:13,120 - ==> Top1: 70.420    Loss: 0.558

2023-01-06 16:03:13,121 - ==> Confusion:
[[190 239   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 16:03:13,122 - ==> Best [Top1: 70.420   Sparsity:0.00   Params: 46192 on epoch: 35]
2023-01-06 16:03:13,122 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:13,127 - 

2023-01-06 16:03:13,127 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:13,497 - Epoch: [36][   10/   37]    Overall Loss 0.568468    Objective Loss 0.568468                                        LR 0.000060    Time 0.036875    
2023-01-06 16:03:13,642 - Epoch: [36][   20/   37]    Overall Loss 0.568846    Objective Loss 0.568846                                        LR 0.000060    Time 0.025640    
2023-01-06 16:03:13,793 - Epoch: [36][   30/   37]    Overall Loss 0.568768    Objective Loss 0.568768                                        LR 0.000060    Time 0.022068    
2023-01-06 16:03:13,877 - Epoch: [36][   37/   37]    Overall Loss 0.568913    Objective Loss 0.568913    Top1 67.782427    LR 0.000060    Time 0.020158    
2023-01-06 16:03:13,955 - --- validate (epoch=36)-----------
2023-01-06 16:03:13,955 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:14,191 - Epoch: [36][    5/    5]    Loss 0.553298    Top1 71.087786    
2023-01-06 16:03:14,252 - ==> Top1: 71.088    Loss: 0.553

2023-01-06 16:03:14,252 - ==> Confusion:
[[195 234   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 16:03:14,253 - ==> Best [Top1: 71.088   Sparsity:0.00   Params: 46192 on epoch: 36]
2023-01-06 16:03:14,254 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:14,258 - 

2023-01-06 16:03:14,259 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:14,612 - Epoch: [37][   10/   37]    Overall Loss 0.555351    Objective Loss 0.555351                                        LR 0.000060    Time 0.035244    
2023-01-06 16:03:14,756 - Epoch: [37][   20/   37]    Overall Loss 0.558514    Objective Loss 0.558514                                        LR 0.000060    Time 0.024827    
2023-01-06 16:03:14,899 - Epoch: [37][   30/   37]    Overall Loss 0.559616    Objective Loss 0.559616                                        LR 0.000060    Time 0.021284    
2023-01-06 16:03:14,990 - Epoch: [37][   37/   37]    Overall Loss 0.558833    Objective Loss 0.558833    Top1 70.502092    LR 0.000060    Time 0.019727    
2023-01-06 16:03:15,068 - --- validate (epoch=37)-----------
2023-01-06 16:03:15,068 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:15,296 - Epoch: [37][    5/    5]    Loss 0.523553    Top1 73.187023    
2023-01-06 16:03:15,365 - ==> Top1: 73.187    Loss: 0.524

2023-01-06 16:03:15,365 - ==> Confusion:
[[225 204   0]
 [ 77 542   0]
 [  0   0   0]]

2023-01-06 16:03:15,366 - ==> Best [Top1: 73.187   Sparsity:0.00   Params: 46192 on epoch: 37]
2023-01-06 16:03:15,366 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:15,372 - 

2023-01-06 16:03:15,372 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:15,865 - Epoch: [38][   10/   37]    Overall Loss 0.566406    Objective Loss 0.566406                                        LR 0.000060    Time 0.049267    
2023-01-06 16:03:16,021 - Epoch: [38][   20/   37]    Overall Loss 0.558357    Objective Loss 0.558357                                        LR 0.000060    Time 0.032392    
2023-01-06 16:03:16,188 - Epoch: [38][   30/   37]    Overall Loss 0.554509    Objective Loss 0.554509                                        LR 0.000060    Time 0.027163    
2023-01-06 16:03:16,283 - Epoch: [38][   37/   37]    Overall Loss 0.552387    Objective Loss 0.552387    Top1 71.757322    LR 0.000060    Time 0.024593    
2023-01-06 16:03:16,361 - --- validate (epoch=38)-----------
2023-01-06 16:03:16,362 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:16,585 - Epoch: [38][    5/    5]    Loss 0.538912    Top1 74.332061    
2023-01-06 16:03:16,651 - ==> Top1: 74.332    Loss: 0.539

2023-01-06 16:03:16,651 - ==> Confusion:
[[276 153   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:03:16,652 - ==> Best [Top1: 74.332   Sparsity:0.00   Params: 46192 on epoch: 38]
2023-01-06 16:03:16,653 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:16,658 - 

2023-01-06 16:03:16,658 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:17,013 - Epoch: [39][   10/   37]    Overall Loss 0.548316    Objective Loss 0.548316                                        LR 0.000060    Time 0.035444    
2023-01-06 16:03:17,180 - Epoch: [39][   20/   37]    Overall Loss 0.550255    Objective Loss 0.550255                                        LR 0.000060    Time 0.026052    
2023-01-06 16:03:17,351 - Epoch: [39][   30/   37]    Overall Loss 0.546895    Objective Loss 0.546895                                        LR 0.000060    Time 0.023052    
2023-01-06 16:03:17,442 - Epoch: [39][   37/   37]    Overall Loss 0.541882    Objective Loss 0.541882    Top1 78.870293    LR 0.000060    Time 0.021147    
2023-01-06 16:03:17,520 - --- validate (epoch=39)-----------
2023-01-06 16:03:17,520 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:17,747 - Epoch: [39][    5/    5]    Loss 0.550498    Top1 75.667939    
2023-01-06 16:03:17,810 - ==> Top1: 75.668    Loss: 0.550

2023-01-06 16:03:17,810 - ==> Confusion:
[[310 119   0]
 [136 483   0]
 [  0   0   0]]

2023-01-06 16:03:17,811 - ==> Best [Top1: 75.668   Sparsity:0.00   Params: 46192 on epoch: 39]
2023-01-06 16:03:17,811 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:17,817 - 

2023-01-06 16:03:17,817 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:18,158 - Epoch: [40][   10/   37]    Overall Loss 0.533042    Objective Loss 0.533042                                        LR 0.000036    Time 0.034058    
2023-01-06 16:03:18,295 - Epoch: [40][   20/   37]    Overall Loss 0.541303    Objective Loss 0.541303                                        LR 0.000036    Time 0.023840    
2023-01-06 16:03:18,432 - Epoch: [40][   30/   37]    Overall Loss 0.539932    Objective Loss 0.539932                                        LR 0.000036    Time 0.020461    
2023-01-06 16:03:18,518 - Epoch: [40][   37/   37]    Overall Loss 0.535046    Objective Loss 0.535046    Top1 73.012552    LR 0.000036    Time 0.018896    
2023-01-06 16:03:18,611 - --- validate (epoch=40)-----------
2023-01-06 16:03:18,612 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:18,840 - Epoch: [40][    5/    5]    Loss 0.515108    Top1 75.286260    
2023-01-06 16:03:18,916 - ==> Top1: 75.286    Loss: 0.515

2023-01-06 16:03:18,916 - ==> Confusion:
[[254 175   0]
 [ 84 535   0]
 [  0   0   0]]

2023-01-06 16:03:18,917 - ==> Best [Top1: 75.668   Sparsity:0.00   Params: 46192 on epoch: 39]
2023-01-06 16:03:18,918 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:18,922 - 

2023-01-06 16:03:18,922 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:19,295 - Epoch: [41][   10/   37]    Overall Loss 0.520400    Objective Loss 0.520400                                        LR 0.000036    Time 0.037224    
2023-01-06 16:03:19,452 - Epoch: [41][   20/   37]    Overall Loss 0.528624    Objective Loss 0.528624                                        LR 0.000036    Time 0.026396    
2023-01-06 16:03:19,592 - Epoch: [41][   30/   37]    Overall Loss 0.527151    Objective Loss 0.527151                                        LR 0.000036    Time 0.022243    
2023-01-06 16:03:19,671 - Epoch: [41][   37/   37]    Overall Loss 0.528053    Objective Loss 0.528053    Top1 71.966527    LR 0.000036    Time 0.020165    
2023-01-06 16:03:19,748 - --- validate (epoch=41)-----------
2023-01-06 16:03:19,748 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:19,976 - Epoch: [41][    5/    5]    Loss 0.503245    Top1 76.431298    
2023-01-06 16:03:20,051 - ==> Top1: 76.431    Loss: 0.503

2023-01-06 16:03:20,051 - ==> Confusion:
[[293 136   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:03:20,052 - ==> Best [Top1: 76.431   Sparsity:0.00   Params: 46192 on epoch: 41]
2023-01-06 16:03:20,052 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:20,057 - 

2023-01-06 16:03:20,057 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:20,409 - Epoch: [42][   10/   37]    Overall Loss 0.531706    Objective Loss 0.531706                                        LR 0.000036    Time 0.035068    
2023-01-06 16:03:20,555 - Epoch: [42][   20/   37]    Overall Loss 0.533076    Objective Loss 0.533076                                        LR 0.000036    Time 0.024822    
2023-01-06 16:03:20,701 - Epoch: [42][   30/   37]    Overall Loss 0.525741    Objective Loss 0.525741                                        LR 0.000036    Time 0.021404    
2023-01-06 16:03:20,783 - Epoch: [42][   37/   37]    Overall Loss 0.524716    Objective Loss 0.524716    Top1 75.523013    LR 0.000036    Time 0.019553    
2023-01-06 16:03:20,853 - --- validate (epoch=42)-----------
2023-01-06 16:03:20,853 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:21,079 - Epoch: [42][    5/    5]    Loss 0.498392    Top1 76.717557    
2023-01-06 16:03:21,150 - ==> Top1: 76.718    Loss: 0.498

2023-01-06 16:03:21,151 - ==> Confusion:
[[296 133   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:03:21,152 - ==> Best [Top1: 76.718   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:03:21,152 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:21,157 - 

2023-01-06 16:03:21,157 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:21,514 - Epoch: [43][   10/   37]    Overall Loss 0.522943    Objective Loss 0.522943                                        LR 0.000036    Time 0.035642    
2023-01-06 16:03:21,675 - Epoch: [43][   20/   37]    Overall Loss 0.523810    Objective Loss 0.523810                                        LR 0.000036    Time 0.025844    
2023-01-06 16:03:21,828 - Epoch: [43][   30/   37]    Overall Loss 0.524619    Objective Loss 0.524619                                        LR 0.000036    Time 0.022326    
2023-01-06 16:03:21,912 - Epoch: [43][   37/   37]    Overall Loss 0.522724    Objective Loss 0.522724    Top1 75.313808    LR 0.000036    Time 0.020350    
2023-01-06 16:03:21,990 - --- validate (epoch=43)-----------
2023-01-06 16:03:21,990 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:22,218 - Epoch: [43][    5/    5]    Loss 0.487205    Top1 76.240458    
2023-01-06 16:03:22,278 - ==> Top1: 76.240    Loss: 0.487

2023-01-06 16:03:22,278 - ==> Confusion:
[[263 166   0]
 [ 83 536   0]
 [  0   0   0]]

2023-01-06 16:03:22,279 - ==> Best [Top1: 76.718   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:03:22,279 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:22,284 - 

2023-01-06 16:03:22,284 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:22,633 - Epoch: [44][   10/   37]    Overall Loss 0.510650    Objective Loss 0.510650                                        LR 0.000036    Time 0.034848    
2023-01-06 16:03:22,789 - Epoch: [44][   20/   37]    Overall Loss 0.520890    Objective Loss 0.520890                                        LR 0.000036    Time 0.025184    
2023-01-06 16:03:22,944 - Epoch: [44][   30/   37]    Overall Loss 0.514316    Objective Loss 0.514316                                        LR 0.000036    Time 0.021955    
2023-01-06 16:03:23,037 - Epoch: [44][   37/   37]    Overall Loss 0.517749    Objective Loss 0.517749    Top1 73.640167    LR 0.000036    Time 0.020306    
2023-01-06 16:03:23,126 - --- validate (epoch=44)-----------
2023-01-06 16:03:23,127 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:23,355 - Epoch: [44][    5/    5]    Loss 0.492019    Top1 76.145038    
2023-01-06 16:03:23,419 - ==> Top1: 76.145    Loss: 0.492

2023-01-06 16:03:23,420 - ==> Confusion:
[[265 164   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 16:03:23,421 - ==> Best [Top1: 76.718   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:03:23,421 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:23,425 - 

2023-01-06 16:03:23,425 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:23,772 - Epoch: [45][   10/   37]    Overall Loss 0.522723    Objective Loss 0.522723                                        LR 0.000036    Time 0.034624    
2023-01-06 16:03:23,914 - Epoch: [45][   20/   37]    Overall Loss 0.520239    Objective Loss 0.520239                                        LR 0.000036    Time 0.024379    
2023-01-06 16:03:24,055 - Epoch: [45][   30/   37]    Overall Loss 0.516393    Objective Loss 0.516393                                        LR 0.000036    Time 0.020941    
2023-01-06 16:03:24,140 - Epoch: [45][   37/   37]    Overall Loss 0.512982    Objective Loss 0.512982    Top1 74.058577    LR 0.000036    Time 0.019263    
2023-01-06 16:03:24,227 - --- validate (epoch=45)-----------
2023-01-06 16:03:24,227 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:24,457 - Epoch: [45][    5/    5]    Loss 0.496004    Top1 75.190840    
2023-01-06 16:03:24,517 - ==> Top1: 75.191    Loss: 0.496

2023-01-06 16:03:24,518 - ==> Confusion:
[[224 205   0]
 [ 55 564   0]
 [  0   0   0]]

2023-01-06 16:03:24,519 - ==> Best [Top1: 76.718   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:03:24,519 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:24,523 - 

2023-01-06 16:03:24,524 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:24,883 - Epoch: [46][   10/   37]    Overall Loss 0.514559    Objective Loss 0.514559                                        LR 0.000036    Time 0.035903    
2023-01-06 16:03:25,039 - Epoch: [46][   20/   37]    Overall Loss 0.511951    Objective Loss 0.511951                                        LR 0.000036    Time 0.025698    
2023-01-06 16:03:25,178 - Epoch: [46][   30/   37]    Overall Loss 0.512484    Objective Loss 0.512484                                        LR 0.000036    Time 0.021771    
2023-01-06 16:03:25,270 - Epoch: [46][   37/   37]    Overall Loss 0.512743    Objective Loss 0.512743    Top1 72.384937    LR 0.000036    Time 0.020126    
2023-01-06 16:03:25,350 - --- validate (epoch=46)-----------
2023-01-06 16:03:25,350 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:25,577 - Epoch: [46][    5/    5]    Loss 0.483065    Top1 77.194656    
2023-01-06 16:03:25,649 - ==> Top1: 77.195    Loss: 0.483

2023-01-06 16:03:25,649 - ==> Confusion:
[[286 143   0]
 [ 96 523   0]
 [  0   0   0]]

2023-01-06 16:03:25,651 - ==> Best [Top1: 77.195   Sparsity:0.00   Params: 46192 on epoch: 46]
2023-01-06 16:03:25,651 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:25,655 - 

2023-01-06 16:03:25,655 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:26,016 - Epoch: [47][   10/   37]    Overall Loss 0.509131    Objective Loss 0.509131                                        LR 0.000036    Time 0.035948    
2023-01-06 16:03:26,153 - Epoch: [47][   20/   37]    Overall Loss 0.510542    Objective Loss 0.510542                                        LR 0.000036    Time 0.024802    
2023-01-06 16:03:26,291 - Epoch: [47][   30/   37]    Overall Loss 0.509789    Objective Loss 0.509789                                        LR 0.000036    Time 0.021121    
2023-01-06 16:03:26,376 - Epoch: [47][   37/   37]    Overall Loss 0.507393    Objective Loss 0.507393    Top1 75.523013    LR 0.000036    Time 0.019422    
2023-01-06 16:03:26,449 - --- validate (epoch=47)-----------
2023-01-06 16:03:26,450 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:26,681 - Epoch: [47][    5/    5]    Loss 0.460049    Top1 78.148855    
2023-01-06 16:03:26,754 - ==> Top1: 78.149    Loss: 0.460

2023-01-06 16:03:26,754 - ==> Confusion:
[[325 104   0]
 [125 494   0]
 [  0   0   0]]

2023-01-06 16:03:26,756 - ==> Best [Top1: 78.149   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:03:26,756 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:26,761 - 

2023-01-06 16:03:26,761 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:27,102 - Epoch: [48][   10/   37]    Overall Loss 0.505146    Objective Loss 0.505146                                        LR 0.000036    Time 0.034059    
2023-01-06 16:03:27,235 - Epoch: [48][   20/   37]    Overall Loss 0.504007    Objective Loss 0.504007                                        LR 0.000036    Time 0.023658    
2023-01-06 16:03:27,374 - Epoch: [48][   30/   37]    Overall Loss 0.504684    Objective Loss 0.504684                                        LR 0.000036    Time 0.020409    
2023-01-06 16:03:27,465 - Epoch: [48][   37/   37]    Overall Loss 0.503948    Objective Loss 0.503948    Top1 72.384937    LR 0.000036    Time 0.018990    
2023-01-06 16:03:27,535 - --- validate (epoch=48)-----------
2023-01-06 16:03:27,536 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:27,764 - Epoch: [48][    5/    5]    Loss 0.485413    Top1 76.812977    
2023-01-06 16:03:27,821 - ==> Top1: 76.813    Loss: 0.485

2023-01-06 16:03:27,822 - ==> Confusion:
[[255 174   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 16:03:27,823 - ==> Best [Top1: 78.149   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:03:27,823 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:27,827 - 

2023-01-06 16:03:27,827 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:28,182 - Epoch: [49][   10/   37]    Overall Loss 0.518636    Objective Loss 0.518636                                        LR 0.000036    Time 0.035425    
2023-01-06 16:03:28,314 - Epoch: [49][   20/   37]    Overall Loss 0.503646    Objective Loss 0.503646                                        LR 0.000036    Time 0.024322    
2023-01-06 16:03:28,451 - Epoch: [49][   30/   37]    Overall Loss 0.500450    Objective Loss 0.500450                                        LR 0.000036    Time 0.020758    
2023-01-06 16:03:28,537 - Epoch: [49][   37/   37]    Overall Loss 0.501716    Objective Loss 0.501716    Top1 79.288703    LR 0.000036    Time 0.019149    
2023-01-06 16:03:28,609 - --- validate (epoch=49)-----------
2023-01-06 16:03:28,609 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:28,836 - Epoch: [49][    5/    5]    Loss 0.474939    Top1 77.958015    
2023-01-06 16:03:28,912 - ==> Top1: 77.958    Loss: 0.475

2023-01-06 16:03:28,912 - ==> Confusion:
[[293 136   0]
 [ 95 524   0]
 [  0   0   0]]

2023-01-06 16:03:28,913 - ==> Best [Top1: 78.149   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:03:28,913 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:28,918 - 

2023-01-06 16:03:28,918 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:29,285 - Epoch: [50][   10/   37]    Overall Loss 0.491437    Objective Loss 0.491437                                        LR 0.000036    Time 0.036653    
2023-01-06 16:03:29,449 - Epoch: [50][   20/   37]    Overall Loss 0.496609    Objective Loss 0.496609                                        LR 0.000036    Time 0.026483    
2023-01-06 16:03:29,620 - Epoch: [50][   30/   37]    Overall Loss 0.502382    Objective Loss 0.502382                                        LR 0.000036    Time 0.023370    
2023-01-06 16:03:29,715 - Epoch: [50][   37/   37]    Overall Loss 0.499192    Objective Loss 0.499192    Top1 78.242678    LR 0.000036    Time 0.021490    
2023-01-06 16:03:29,788 - --- validate (epoch=50)-----------
2023-01-06 16:03:29,788 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:30,011 - Epoch: [50][    5/    5]    Loss 0.488841    Top1 77.958015    
2023-01-06 16:03:30,087 - ==> Top1: 77.958    Loss: 0.489

2023-01-06 16:03:30,087 - ==> Confusion:
[[297 132   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:03:30,088 - ==> Best [Top1: 78.149   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:03:30,088 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:30,093 - 

2023-01-06 16:03:30,093 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:30,450 - Epoch: [51][   10/   37]    Overall Loss 0.495946    Objective Loss 0.495946                                        LR 0.000036    Time 0.035691    
2023-01-06 16:03:30,610 - Epoch: [51][   20/   37]    Overall Loss 0.493081    Objective Loss 0.493081                                        LR 0.000036    Time 0.025802    
2023-01-06 16:03:30,779 - Epoch: [51][   30/   37]    Overall Loss 0.494924    Objective Loss 0.494924                                        LR 0.000036    Time 0.022826    
2023-01-06 16:03:30,874 - Epoch: [51][   37/   37]    Overall Loss 0.494980    Objective Loss 0.494980    Top1 77.196653    LR 0.000036    Time 0.021075    
2023-01-06 16:03:30,937 - --- validate (epoch=51)-----------
2023-01-06 16:03:30,937 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:31,172 - Epoch: [51][    5/    5]    Loss 0.460644    Top1 78.912214    
2023-01-06 16:03:31,240 - ==> Top1: 78.912    Loss: 0.461

2023-01-06 16:03:31,240 - ==> Confusion:
[[309 120   0]
 [101 518   0]
 [  0   0   0]]

2023-01-06 16:03:31,242 - ==> Best [Top1: 78.912   Sparsity:0.00   Params: 46192 on epoch: 51]
2023-01-06 16:03:31,242 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:31,247 - 

2023-01-06 16:03:31,247 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:31,598 - Epoch: [52][   10/   37]    Overall Loss 0.506232    Objective Loss 0.506232                                        LR 0.000036    Time 0.035077    
2023-01-06 16:03:31,768 - Epoch: [52][   20/   37]    Overall Loss 0.498530    Objective Loss 0.498530                                        LR 0.000036    Time 0.025974    
2023-01-06 16:03:31,940 - Epoch: [52][   30/   37]    Overall Loss 0.497085    Objective Loss 0.497085                                        LR 0.000036    Time 0.023040    
2023-01-06 16:03:32,035 - Epoch: [52][   37/   37]    Overall Loss 0.492656    Objective Loss 0.492656    Top1 82.217573    LR 0.000036    Time 0.021240    
2023-01-06 16:03:32,117 - --- validate (epoch=52)-----------
2023-01-06 16:03:32,117 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:32,339 - Epoch: [52][    5/    5]    Loss 0.496393    Top1 79.103053    
2023-01-06 16:03:32,402 - ==> Top1: 79.103    Loss: 0.496

2023-01-06 16:03:32,402 - ==> Confusion:
[[309 120   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:03:32,404 - ==> Best [Top1: 79.103   Sparsity:0.00   Params: 46192 on epoch: 52]
2023-01-06 16:03:32,404 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:32,409 - 

2023-01-06 16:03:32,409 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:32,751 - Epoch: [53][   10/   37]    Overall Loss 0.492709    Objective Loss 0.492709                                        LR 0.000036    Time 0.034079    
2023-01-06 16:03:32,904 - Epoch: [53][   20/   37]    Overall Loss 0.487334    Objective Loss 0.487334                                        LR 0.000036    Time 0.024700    
2023-01-06 16:03:33,073 - Epoch: [53][   30/   37]    Overall Loss 0.490626    Objective Loss 0.490626                                        LR 0.000036    Time 0.022072    
2023-01-06 16:03:33,168 - Epoch: [53][   37/   37]    Overall Loss 0.489858    Objective Loss 0.489858    Top1 76.150628    LR 0.000036    Time 0.020451    
2023-01-06 16:03:33,240 - --- validate (epoch=53)-----------
2023-01-06 16:03:33,240 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:33,694 - Epoch: [53][    5/    5]    Loss 0.470317    Top1 78.912214    
2023-01-06 16:03:33,772 - ==> Top1: 78.912    Loss: 0.470

2023-01-06 16:03:33,772 - ==> Confusion:
[[296 133   0]
 [ 88 531   0]
 [  0   0   0]]

2023-01-06 16:03:33,774 - ==> Best [Top1: 79.103   Sparsity:0.00   Params: 46192 on epoch: 52]
2023-01-06 16:03:33,774 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:33,778 - 

2023-01-06 16:03:33,778 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:34,132 - Epoch: [54][   10/   37]    Overall Loss 0.490568    Objective Loss 0.490568                                        LR 0.000036    Time 0.035308    
2023-01-06 16:03:34,289 - Epoch: [54][   20/   37]    Overall Loss 0.484591    Objective Loss 0.484591                                        LR 0.000036    Time 0.025485    
2023-01-06 16:03:34,449 - Epoch: [54][   30/   37]    Overall Loss 0.489811    Objective Loss 0.489811                                        LR 0.000036    Time 0.022295    
2023-01-06 16:03:34,544 - Epoch: [54][   37/   37]    Overall Loss 0.488428    Objective Loss 0.488428    Top1 77.405858    LR 0.000036    Time 0.020642    
2023-01-06 16:03:34,626 - --- validate (epoch=54)-----------
2023-01-06 16:03:34,626 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:34,849 - Epoch: [54][    5/    5]    Loss 0.456794    Top1 77.480916    
2023-01-06 16:03:34,910 - ==> Top1: 77.481    Loss: 0.457

2023-01-06 16:03:34,911 - ==> Confusion:
[[355  74   0]
 [162 457   0]
 [  0   0   0]]

2023-01-06 16:03:34,912 - ==> Best [Top1: 79.103   Sparsity:0.00   Params: 46192 on epoch: 52]
2023-01-06 16:03:34,912 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:34,916 - 

2023-01-06 16:03:34,916 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:35,258 - Epoch: [55][   10/   37]    Overall Loss 0.482885    Objective Loss 0.482885                                        LR 0.000036    Time 0.034109    
2023-01-06 16:03:35,397 - Epoch: [55][   20/   37]    Overall Loss 0.484590    Objective Loss 0.484590                                        LR 0.000036    Time 0.023988    
2023-01-06 16:03:35,536 - Epoch: [55][   30/   37]    Overall Loss 0.486641    Objective Loss 0.486641                                        LR 0.000036    Time 0.020602    
2023-01-06 16:03:35,615 - Epoch: [55][   37/   37]    Overall Loss 0.484766    Objective Loss 0.484766    Top1 77.824268    LR 0.000036    Time 0.018834    
2023-01-06 16:03:35,695 - --- validate (epoch=55)-----------
2023-01-06 16:03:35,695 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:35,922 - Epoch: [55][    5/    5]    Loss 0.441435    Top1 78.435115    
2023-01-06 16:03:35,984 - ==> Top1: 78.435    Loss: 0.441

2023-01-06 16:03:35,984 - ==> Confusion:
[[277 152   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-06 16:03:35,986 - ==> Best [Top1: 79.103   Sparsity:0.00   Params: 46192 on epoch: 52]
2023-01-06 16:03:35,986 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:35,990 - 

2023-01-06 16:03:35,990 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:36,344 - Epoch: [56][   10/   37]    Overall Loss 0.480052    Objective Loss 0.480052                                        LR 0.000036    Time 0.035340    
2023-01-06 16:03:36,497 - Epoch: [56][   20/   37]    Overall Loss 0.488550    Objective Loss 0.488550                                        LR 0.000036    Time 0.025304    
2023-01-06 16:03:36,661 - Epoch: [56][   30/   37]    Overall Loss 0.485516    Objective Loss 0.485516                                        LR 0.000036    Time 0.022333    
2023-01-06 16:03:36,750 - Epoch: [56][   37/   37]    Overall Loss 0.482431    Objective Loss 0.482431    Top1 76.359833    LR 0.000036    Time 0.020487    
2023-01-06 16:03:36,819 - --- validate (epoch=56)-----------
2023-01-06 16:03:36,820 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:37,049 - Epoch: [56][    5/    5]    Loss 0.441924    Top1 79.961832    
2023-01-06 16:03:37,119 - ==> Top1: 79.962    Loss: 0.442

2023-01-06 16:03:37,119 - ==> Confusion:
[[323 106   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 16:03:37,120 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:03:37,120 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:37,125 - 

2023-01-06 16:03:37,125 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:37,469 - Epoch: [57][   10/   37]    Overall Loss 0.480391    Objective Loss 0.480391                                        LR 0.000036    Time 0.034363    
2023-01-06 16:03:37,615 - Epoch: [57][   20/   37]    Overall Loss 0.486671    Objective Loss 0.486671                                        LR 0.000036    Time 0.024427    
2023-01-06 16:03:37,769 - Epoch: [57][   30/   37]    Overall Loss 0.480240    Objective Loss 0.480240                                        LR 0.000036    Time 0.021404    
2023-01-06 16:03:37,858 - Epoch: [57][   37/   37]    Overall Loss 0.480353    Objective Loss 0.480353    Top1 77.196653    LR 0.000036    Time 0.019771    
2023-01-06 16:03:37,939 - --- validate (epoch=57)-----------
2023-01-06 16:03:37,939 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:38,166 - Epoch: [57][    5/    5]    Loss 0.465074    Top1 78.816794    
2023-01-06 16:03:38,233 - ==> Top1: 78.817    Loss: 0.465

2023-01-06 16:03:38,233 - ==> Confusion:
[[275 154   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 16:03:38,234 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:03:38,235 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:38,239 - 

2023-01-06 16:03:38,239 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:38,618 - Epoch: [58][   10/   37]    Overall Loss 0.476959    Objective Loss 0.476959                                        LR 0.000036    Time 0.037811    
2023-01-06 16:03:38,790 - Epoch: [58][   20/   37]    Overall Loss 0.479832    Objective Loss 0.479832                                        LR 0.000036    Time 0.027434    
2023-01-06 16:03:38,955 - Epoch: [58][   30/   37]    Overall Loss 0.479204    Objective Loss 0.479204                                        LR 0.000036    Time 0.023787    
2023-01-06 16:03:39,049 - Epoch: [58][   37/   37]    Overall Loss 0.479830    Objective Loss 0.479830    Top1 73.849372    LR 0.000036    Time 0.021837    
2023-01-06 16:03:39,125 - --- validate (epoch=58)-----------
2023-01-06 16:03:39,125 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:39,355 - Epoch: [58][    5/    5]    Loss 0.455984    Top1 78.244275    
2023-01-06 16:03:39,423 - ==> Top1: 78.244    Loss: 0.456

2023-01-06 16:03:39,423 - ==> Confusion:
[[269 160   0]
 [ 68 551   0]
 [  0   0   0]]

2023-01-06 16:03:39,424 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:03:39,424 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:39,429 - 

2023-01-06 16:03:39,429 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:39,793 - Epoch: [59][   10/   37]    Overall Loss 0.481503    Objective Loss 0.481503                                        LR 0.000036    Time 0.036301    
2023-01-06 16:03:39,959 - Epoch: [59][   20/   37]    Overall Loss 0.476657    Objective Loss 0.476657                                        LR 0.000036    Time 0.026465    
2023-01-06 16:03:40,128 - Epoch: [59][   30/   37]    Overall Loss 0.477297    Objective Loss 0.477297                                        LR 0.000036    Time 0.023248    
2023-01-06 16:03:40,223 - Epoch: [59][   37/   37]    Overall Loss 0.477905    Objective Loss 0.477905    Top1 77.824268    LR 0.000036    Time 0.021402    
2023-01-06 16:03:40,293 - --- validate (epoch=59)-----------
2023-01-06 16:03:40,293 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:40,518 - Epoch: [59][    5/    5]    Loss 0.468356    Top1 77.385496    
2023-01-06 16:03:40,580 - ==> Top1: 77.385    Loss: 0.468

2023-01-06 16:03:40,580 - ==> Confusion:
[[365  64   0]
 [173 446   0]
 [  0   0   0]]

2023-01-06 16:03:40,581 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:03:40,581 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:40,586 - 

2023-01-06 16:03:40,586 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:40,936 - Epoch: [60][   10/   37]    Overall Loss 0.468510    Objective Loss 0.468510                                        LR 0.000036    Time 0.034928    
2023-01-06 16:03:41,076 - Epoch: [60][   20/   37]    Overall Loss 0.472957    Objective Loss 0.472957                                        LR 0.000036    Time 0.024407    
2023-01-06 16:03:41,223 - Epoch: [60][   30/   37]    Overall Loss 0.473667    Objective Loss 0.473667                                        LR 0.000036    Time 0.021193    
2023-01-06 16:03:41,317 - Epoch: [60][   37/   37]    Overall Loss 0.474234    Objective Loss 0.474234    Top1 78.033473    LR 0.000036    Time 0.019709    
2023-01-06 16:03:41,395 - --- validate (epoch=60)-----------
2023-01-06 16:03:41,396 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:41,629 - Epoch: [60][    5/    5]    Loss 0.447241    Top1 78.721374    
2023-01-06 16:03:41,699 - ==> Top1: 78.721    Loss: 0.447

2023-01-06 16:03:41,700 - ==> Confusion:
[[358  71   0]
 [152 467   0]
 [  0   0   0]]

2023-01-06 16:03:41,701 - ==> Best [Top1: 79.962   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:03:41,701 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:41,707 - 

2023-01-06 16:03:41,707 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:42,062 - Epoch: [61][   10/   37]    Overall Loss 0.467815    Objective Loss 0.467815                                        LR 0.000036    Time 0.035336    
2023-01-06 16:03:42,209 - Epoch: [61][   20/   37]    Overall Loss 0.467549    Objective Loss 0.467549                                        LR 0.000036    Time 0.025036    
2023-01-06 16:03:42,375 - Epoch: [61][   30/   37]    Overall Loss 0.473744    Objective Loss 0.473744                                        LR 0.000036    Time 0.022190    
2023-01-06 16:03:42,468 - Epoch: [61][   37/   37]    Overall Loss 0.471778    Objective Loss 0.471778    Top1 76.987448    LR 0.000036    Time 0.020512    
2023-01-06 16:03:42,547 - --- validate (epoch=61)-----------
2023-01-06 16:03:42,547 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:42,773 - Epoch: [61][    5/    5]    Loss 0.430840    Top1 80.248092    
2023-01-06 16:03:42,836 - ==> Top1: 80.248    Loss: 0.431

2023-01-06 16:03:42,836 - ==> Confusion:
[[314 115   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:03:42,838 - ==> Best [Top1: 80.248   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:03:42,838 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:42,843 - 

2023-01-06 16:03:42,843 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:43,190 - Epoch: [62][   10/   37]    Overall Loss 0.473329    Objective Loss 0.473329                                        LR 0.000036    Time 0.034630    
2023-01-06 16:03:43,327 - Epoch: [62][   20/   37]    Overall Loss 0.468390    Objective Loss 0.468390                                        LR 0.000036    Time 0.024163    
2023-01-06 16:03:43,474 - Epoch: [62][   30/   37]    Overall Loss 0.468816    Objective Loss 0.468816                                        LR 0.000036    Time 0.020995    
2023-01-06 16:03:43,561 - Epoch: [62][   37/   37]    Overall Loss 0.469870    Objective Loss 0.469870    Top1 74.895397    LR 0.000036    Time 0.019362    
2023-01-06 16:03:43,638 - --- validate (epoch=62)-----------
2023-01-06 16:03:43,638 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:43,863 - Epoch: [62][    5/    5]    Loss 0.431684    Top1 78.530534    
2023-01-06 16:03:43,930 - ==> Top1: 78.531    Loss: 0.432

2023-01-06 16:03:43,930 - ==> Confusion:
[[248 181   0]
 [ 44 575   0]
 [  0   0   0]]

2023-01-06 16:03:43,931 - ==> Best [Top1: 80.248   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:03:43,931 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:43,936 - 

2023-01-06 16:03:43,936 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:44,305 - Epoch: [63][   10/   37]    Overall Loss 0.482065    Objective Loss 0.482065                                        LR 0.000036    Time 0.036804    
2023-01-06 16:03:44,458 - Epoch: [63][   20/   37]    Overall Loss 0.471620    Objective Loss 0.471620                                        LR 0.000036    Time 0.026041    
2023-01-06 16:03:44,620 - Epoch: [63][   30/   37]    Overall Loss 0.470222    Objective Loss 0.470222                                        LR 0.000036    Time 0.022759    
2023-01-06 16:03:44,706 - Epoch: [63][   37/   37]    Overall Loss 0.467579    Objective Loss 0.467579    Top1 79.707113    LR 0.000036    Time 0.020764    
2023-01-06 16:03:44,782 - --- validate (epoch=63)-----------
2023-01-06 16:03:44,783 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:45,016 - Epoch: [63][    5/    5]    Loss 0.458500    Top1 79.770992    
2023-01-06 16:03:45,089 - ==> Top1: 79.771    Loss: 0.459

2023-01-06 16:03:45,090 - ==> Confusion:
[[321 108   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 16:03:45,091 - ==> Best [Top1: 80.248   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:03:45,091 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:45,096 - 

2023-01-06 16:03:45,096 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:45,461 - Epoch: [64][   10/   37]    Overall Loss 0.462737    Objective Loss 0.462737                                        LR 0.000036    Time 0.036460    
2023-01-06 16:03:45,622 - Epoch: [64][   20/   37]    Overall Loss 0.464229    Objective Loss 0.464229                                        LR 0.000036    Time 0.026278    
2023-01-06 16:03:45,784 - Epoch: [64][   30/   37]    Overall Loss 0.464268    Objective Loss 0.464268                                        LR 0.000036    Time 0.022888    
2023-01-06 16:03:45,875 - Epoch: [64][   37/   37]    Overall Loss 0.465170    Objective Loss 0.465170    Top1 79.079498    LR 0.000036    Time 0.021025    
2023-01-06 16:03:45,949 - --- validate (epoch=64)-----------
2023-01-06 16:03:45,949 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:46,174 - Epoch: [64][    5/    5]    Loss 0.435915    Top1 80.725191    
2023-01-06 16:03:46,242 - ==> Top1: 80.725    Loss: 0.436

2023-01-06 16:03:46,242 - ==> Confusion:
[[306 123   0]
 [ 79 540   0]
 [  0   0   0]]

2023-01-06 16:03:46,244 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:46,244 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:46,249 - 

2023-01-06 16:03:46,249 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:46,628 - Epoch: [65][   10/   37]    Overall Loss 0.454778    Objective Loss 0.454778                                        LR 0.000036    Time 0.037857    
2023-01-06 16:03:46,786 - Epoch: [65][   20/   37]    Overall Loss 0.463770    Objective Loss 0.463770                                        LR 0.000036    Time 0.026773    
2023-01-06 16:03:46,953 - Epoch: [65][   30/   37]    Overall Loss 0.466294    Objective Loss 0.466294                                        LR 0.000036    Time 0.023379    
2023-01-06 16:03:47,047 - Epoch: [65][   37/   37]    Overall Loss 0.463241    Objective Loss 0.463241    Top1 79.916318    LR 0.000036    Time 0.021511    
2023-01-06 16:03:47,118 - --- validate (epoch=65)-----------
2023-01-06 16:03:47,118 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:47,344 - Epoch: [65][    5/    5]    Loss 0.438187    Top1 79.389313    
2023-01-06 16:03:47,408 - ==> Top1: 79.389    Loss: 0.438

2023-01-06 16:03:47,408 - ==> Confusion:
[[263 166   0]
 [ 50 569   0]
 [  0   0   0]]

2023-01-06 16:03:47,409 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:47,409 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:47,414 - 

2023-01-06 16:03:47,414 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:47,803 - Epoch: [66][   10/   37]    Overall Loss 0.462573    Objective Loss 0.462573                                        LR 0.000036    Time 0.038822    
2023-01-06 16:03:47,971 - Epoch: [66][   20/   37]    Overall Loss 0.469610    Objective Loss 0.469610                                        LR 0.000036    Time 0.027765    
2023-01-06 16:03:48,129 - Epoch: [66][   30/   37]    Overall Loss 0.467388    Objective Loss 0.467388                                        LR 0.000036    Time 0.023758    
2023-01-06 16:03:48,224 - Epoch: [66][   37/   37]    Overall Loss 0.464204    Objective Loss 0.464204    Top1 78.242678    LR 0.000036    Time 0.021817    
2023-01-06 16:03:48,296 - --- validate (epoch=66)-----------
2023-01-06 16:03:48,297 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:48,525 - Epoch: [66][    5/    5]    Loss 0.465898    Top1 79.675573    
2023-01-06 16:03:48,583 - ==> Top1: 79.676    Loss: 0.466

2023-01-06 16:03:48,586 - ==> Confusion:
[[343  86   0]
 [127 492   0]
 [  0   0   0]]

2023-01-06 16:03:48,587 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:48,587 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:48,592 - 

2023-01-06 16:03:48,592 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:48,936 - Epoch: [67][   10/   37]    Overall Loss 0.462459    Objective Loss 0.462459                                        LR 0.000036    Time 0.034353    
2023-01-06 16:03:49,075 - Epoch: [67][   20/   37]    Overall Loss 0.462057    Objective Loss 0.462057                                        LR 0.000036    Time 0.024088    
2023-01-06 16:03:49,216 - Epoch: [67][   30/   37]    Overall Loss 0.461108    Objective Loss 0.461108                                        LR 0.000036    Time 0.020748    
2023-01-06 16:03:49,299 - Epoch: [67][   37/   37]    Overall Loss 0.459176    Objective Loss 0.459176    Top1 77.196653    LR 0.000036    Time 0.019075    
2023-01-06 16:03:49,389 - --- validate (epoch=67)-----------
2023-01-06 16:03:49,389 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:49,617 - Epoch: [67][    5/    5]    Loss 0.425694    Top1 80.248092    
2023-01-06 16:03:49,688 - ==> Top1: 80.248    Loss: 0.426

2023-01-06 16:03:49,689 - ==> Confusion:
[[308 121   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 16:03:49,690 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:49,690 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:49,694 - 

2023-01-06 16:03:49,694 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:50,201 - Epoch: [68][   10/   37]    Overall Loss 0.466903    Objective Loss 0.466903                                        LR 0.000036    Time 0.050574    
2023-01-06 16:03:50,377 - Epoch: [68][   20/   37]    Overall Loss 0.462395    Objective Loss 0.462395                                        LR 0.000036    Time 0.034098    
2023-01-06 16:03:50,545 - Epoch: [68][   30/   37]    Overall Loss 0.457639    Objective Loss 0.457639                                        LR 0.000036    Time 0.028294    
2023-01-06 16:03:50,638 - Epoch: [68][   37/   37]    Overall Loss 0.457905    Objective Loss 0.457905    Top1 80.962343    LR 0.000036    Time 0.025451    
2023-01-06 16:03:50,708 - --- validate (epoch=68)-----------
2023-01-06 16:03:50,708 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:50,939 - Epoch: [68][    5/    5]    Loss 0.436258    Top1 80.534351    
2023-01-06 16:03:51,005 - ==> Top1: 80.534    Loss: 0.436

2023-01-06 16:03:51,005 - ==> Confusion:
[[322 107   0]
 [ 97 522   0]
 [  0   0   0]]

2023-01-06 16:03:51,006 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:51,006 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:51,011 - 

2023-01-06 16:03:51,011 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:51,360 - Epoch: [69][   10/   37]    Overall Loss 0.447110    Objective Loss 0.447110                                        LR 0.000036    Time 0.034824    
2023-01-06 16:03:51,509 - Epoch: [69][   20/   37]    Overall Loss 0.456665    Objective Loss 0.456665                                        LR 0.000036    Time 0.024821    
2023-01-06 16:03:51,661 - Epoch: [69][   30/   37]    Overall Loss 0.457078    Objective Loss 0.457078                                        LR 0.000036    Time 0.021628    
2023-01-06 16:03:51,743 - Epoch: [69][   37/   37]    Overall Loss 0.456251    Objective Loss 0.456251    Top1 81.171548    LR 0.000036    Time 0.019732    
2023-01-06 16:03:51,818 - --- validate (epoch=69)-----------
2023-01-06 16:03:51,818 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:52,052 - Epoch: [69][    5/    5]    Loss 0.468749    Top1 79.961832    
2023-01-06 16:03:52,114 - ==> Top1: 79.962    Loss: 0.469

2023-01-06 16:03:52,114 - ==> Confusion:
[[300 129   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 16:03:52,115 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 46192 on epoch: 64]
2023-01-06 16:03:52,115 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:52,120 - 

2023-01-06 16:03:52,120 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:52,468 - Epoch: [70][   10/   37]    Overall Loss 0.459831    Objective Loss 0.459831                                        LR 0.000022    Time 0.034738    
2023-01-06 16:03:52,616 - Epoch: [70][   20/   37]    Overall Loss 0.453944    Objective Loss 0.453944                                        LR 0.000022    Time 0.024749    
2023-01-06 16:03:52,776 - Epoch: [70][   30/   37]    Overall Loss 0.453906    Objective Loss 0.453906                                        LR 0.000022    Time 0.021826    
2023-01-06 16:03:52,855 - Epoch: [70][   37/   37]    Overall Loss 0.452690    Objective Loss 0.452690    Top1 78.451883    LR 0.000022    Time 0.019832    
2023-01-06 16:03:52,932 - --- validate (epoch=70)-----------
2023-01-06 16:03:52,932 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:53,158 - Epoch: [70][    5/    5]    Loss 0.431238    Top1 81.011450    
2023-01-06 16:03:53,216 - ==> Top1: 81.011    Loss: 0.431

2023-01-06 16:03:53,217 - ==> Confusion:
[[326 103   0]
 [ 96 523   0]
 [  0   0   0]]

2023-01-06 16:03:53,218 - ==> Best [Top1: 81.011   Sparsity:0.00   Params: 46192 on epoch: 70]
2023-01-06 16:03:53,218 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:53,223 - 

2023-01-06 16:03:53,223 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:53,581 - Epoch: [71][   10/   37]    Overall Loss 0.453769    Objective Loss 0.453769                                        LR 0.000022    Time 0.035739    
2023-01-06 16:03:53,739 - Epoch: [71][   20/   37]    Overall Loss 0.453630    Objective Loss 0.453630                                        LR 0.000022    Time 0.025741    
2023-01-06 16:03:53,880 - Epoch: [71][   30/   37]    Overall Loss 0.453365    Objective Loss 0.453365                                        LR 0.000022    Time 0.021854    
2023-01-06 16:03:53,959 - Epoch: [71][   37/   37]    Overall Loss 0.452788    Objective Loss 0.452788    Top1 77.615063    LR 0.000022    Time 0.019845    
2023-01-06 16:03:54,037 - --- validate (epoch=71)-----------
2023-01-06 16:03:54,037 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:54,262 - Epoch: [71][    5/    5]    Loss 0.456063    Top1 80.343511    
2023-01-06 16:03:54,336 - ==> Top1: 80.344    Loss: 0.456

2023-01-06 16:03:54,336 - ==> Confusion:
[[348  81   0]
 [125 494   0]
 [  0   0   0]]

2023-01-06 16:03:54,337 - ==> Best [Top1: 81.011   Sparsity:0.00   Params: 46192 on epoch: 70]
2023-01-06 16:03:54,337 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:54,341 - 

2023-01-06 16:03:54,341 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:54,700 - Epoch: [72][   10/   37]    Overall Loss 0.431643    Objective Loss 0.431643                                        LR 0.000022    Time 0.035746    
2023-01-06 16:03:54,851 - Epoch: [72][   20/   37]    Overall Loss 0.447938    Objective Loss 0.447938                                        LR 0.000022    Time 0.025405    
2023-01-06 16:03:55,002 - Epoch: [72][   30/   37]    Overall Loss 0.449949    Objective Loss 0.449949                                        LR 0.000022    Time 0.021957    
2023-01-06 16:03:55,085 - Epoch: [72][   37/   37]    Overall Loss 0.451906    Objective Loss 0.451906    Top1 78.242678    LR 0.000022    Time 0.020037    
2023-01-06 16:03:55,163 - --- validate (epoch=72)-----------
2023-01-06 16:03:55,163 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:55,389 - Epoch: [72][    5/    5]    Loss 0.415666    Top1 79.484733    
2023-01-06 16:03:55,449 - ==> Top1: 79.485    Loss: 0.416

2023-01-06 16:03:55,449 - ==> Confusion:
[[361  68   0]
 [147 472   0]
 [  0   0   0]]

2023-01-06 16:03:55,450 - ==> Best [Top1: 81.011   Sparsity:0.00   Params: 46192 on epoch: 70]
2023-01-06 16:03:55,450 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:55,454 - 

2023-01-06 16:03:55,455 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:55,821 - Epoch: [73][   10/   37]    Overall Loss 0.454989    Objective Loss 0.454989                                        LR 0.000022    Time 0.036576    
2023-01-06 16:03:55,989 - Epoch: [73][   20/   37]    Overall Loss 0.450380    Objective Loss 0.450380                                        LR 0.000022    Time 0.026595    
2023-01-06 16:03:56,152 - Epoch: [73][   30/   37]    Overall Loss 0.447999    Objective Loss 0.447999                                        LR 0.000022    Time 0.023181    
2023-01-06 16:03:56,243 - Epoch: [73][   37/   37]    Overall Loss 0.450423    Objective Loss 0.450423    Top1 78.451883    LR 0.000022    Time 0.021236    
2023-01-06 16:03:56,322 - --- validate (epoch=73)-----------
2023-01-06 16:03:56,322 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:56,550 - Epoch: [73][    5/    5]    Loss 0.425961    Top1 81.297710    
2023-01-06 16:03:56,633 - ==> Top1: 81.298    Loss: 0.426

2023-01-06 16:03:56,633 - ==> Confusion:
[[336  93   0]
 [103 516   0]
 [  0   0   0]]

2023-01-06 16:03:56,634 - ==> Best [Top1: 81.298   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:03:56,634 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:56,639 - 

2023-01-06 16:03:56,639 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:56,984 - Epoch: [74][   10/   37]    Overall Loss 0.445310    Objective Loss 0.445310                                        LR 0.000022    Time 0.034410    
2023-01-06 16:03:57,126 - Epoch: [74][   20/   37]    Overall Loss 0.450971    Objective Loss 0.450971                                        LR 0.000022    Time 0.024288    
2023-01-06 16:03:57,269 - Epoch: [74][   30/   37]    Overall Loss 0.449823    Objective Loss 0.449823                                        LR 0.000022    Time 0.020952    
2023-01-06 16:03:57,358 - Epoch: [74][   37/   37]    Overall Loss 0.449311    Objective Loss 0.449311    Top1 77.196653    LR 0.000022    Time 0.019380    
2023-01-06 16:03:57,424 - --- validate (epoch=74)-----------
2023-01-06 16:03:57,424 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:57,652 - Epoch: [74][    5/    5]    Loss 0.435301    Top1 79.389313    
2023-01-06 16:03:57,717 - ==> Top1: 79.389    Loss: 0.435

2023-01-06 16:03:57,718 - ==> Confusion:
[[360  69   0]
 [147 472   0]
 [  0   0   0]]

2023-01-06 16:03:57,719 - ==> Best [Top1: 81.298   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:03:57,719 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:57,723 - 

2023-01-06 16:03:57,723 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:58,098 - Epoch: [75][   10/   37]    Overall Loss 0.442127    Objective Loss 0.442127                                        LR 0.000022    Time 0.037430    
2023-01-06 16:03:58,266 - Epoch: [75][   20/   37]    Overall Loss 0.446793    Objective Loss 0.446793                                        LR 0.000022    Time 0.027053    
2023-01-06 16:03:58,431 - Epoch: [75][   30/   37]    Overall Loss 0.448909    Objective Loss 0.448909                                        LR 0.000022    Time 0.023544    
2023-01-06 16:03:58,520 - Epoch: [75][   37/   37]    Overall Loss 0.446709    Objective Loss 0.446709    Top1 78.451883    LR 0.000022    Time 0.021485    
2023-01-06 16:03:58,601 - --- validate (epoch=75)-----------
2023-01-06 16:03:58,602 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:58,831 - Epoch: [75][    5/    5]    Loss 0.437556    Top1 81.774809    
2023-01-06 16:03:58,899 - ==> Top1: 81.775    Loss: 0.438

2023-01-06 16:03:58,900 - ==> Confusion:
[[311 118   0]
 [ 73 546   0]
 [  0   0   0]]

2023-01-06 16:03:58,901 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 46192 on epoch: 75]
2023-01-06 16:03:58,901 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:58,906 - 

2023-01-06 16:03:58,906 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:03:59,249 - Epoch: [76][   10/   37]    Overall Loss 0.445770    Objective Loss 0.445770                                        LR 0.000022    Time 0.034281    
2023-01-06 16:03:59,386 - Epoch: [76][   20/   37]    Overall Loss 0.449575    Objective Loss 0.449575                                        LR 0.000022    Time 0.023935    
2023-01-06 16:03:59,520 - Epoch: [76][   30/   37]    Overall Loss 0.447717    Objective Loss 0.447717                                        LR 0.000022    Time 0.020425    
2023-01-06 16:03:59,602 - Epoch: [76][   37/   37]    Overall Loss 0.446325    Objective Loss 0.446325    Top1 76.778243    LR 0.000022    Time 0.018772    
2023-01-06 16:03:59,681 - --- validate (epoch=76)-----------
2023-01-06 16:03:59,681 - 1048 samples (256 per mini-batch)
2023-01-06 16:03:59,914 - Epoch: [76][    5/    5]    Loss 0.409119    Top1 80.629771    
2023-01-06 16:03:59,983 - ==> Top1: 80.630    Loss: 0.409

2023-01-06 16:03:59,983 - ==> Confusion:
[[360  69   0]
 [134 485   0]
 [  0   0   0]]

2023-01-06 16:03:59,984 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 46192 on epoch: 75]
2023-01-06 16:03:59,985 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:03:59,989 - 

2023-01-06 16:03:59,989 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:00,352 - Epoch: [77][   10/   37]    Overall Loss 0.438190    Objective Loss 0.438190                                        LR 0.000022    Time 0.036277    
2023-01-06 16:04:00,501 - Epoch: [77][   20/   37]    Overall Loss 0.446629    Objective Loss 0.446629                                        LR 0.000022    Time 0.025503    
2023-01-06 16:04:00,653 - Epoch: [77][   30/   37]    Overall Loss 0.447369    Objective Loss 0.447369                                        LR 0.000022    Time 0.022055    
2023-01-06 16:04:00,739 - Epoch: [77][   37/   37]    Overall Loss 0.445624    Objective Loss 0.445624    Top1 80.334728    LR 0.000022    Time 0.020215    
2023-01-06 16:04:00,813 - --- validate (epoch=77)-----------
2023-01-06 16:04:00,813 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:01,049 - Epoch: [77][    5/    5]    Loss 0.446617    Top1 81.297710    
2023-01-06 16:04:01,126 - ==> Top1: 81.298    Loss: 0.447

2023-01-06 16:04:01,127 - ==> Confusion:
[[291 138   0]
 [ 58 561   0]
 [  0   0   0]]

2023-01-06 16:04:01,128 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 46192 on epoch: 75]
2023-01-06 16:04:01,128 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:01,132 - 

2023-01-06 16:04:01,132 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:01,500 - Epoch: [78][   10/   37]    Overall Loss 0.454764    Objective Loss 0.454764                                        LR 0.000022    Time 0.036660    
2023-01-06 16:04:01,657 - Epoch: [78][   20/   37]    Overall Loss 0.449822    Objective Loss 0.449822                                        LR 0.000022    Time 0.026189    
2023-01-06 16:04:01,814 - Epoch: [78][   30/   37]    Overall Loss 0.444927    Objective Loss 0.444927                                        LR 0.000022    Time 0.022660    
2023-01-06 16:04:01,904 - Epoch: [78][   37/   37]    Overall Loss 0.444442    Objective Loss 0.444442    Top1 81.171548    LR 0.000022    Time 0.020821    
2023-01-06 16:04:01,974 - --- validate (epoch=78)-----------
2023-01-06 16:04:01,975 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:02,201 - Epoch: [78][    5/    5]    Loss 0.415622    Top1 79.961832    
2023-01-06 16:04:02,269 - ==> Top1: 79.962    Loss: 0.416

2023-01-06 16:04:02,269 - ==> Confusion:
[[352  77   0]
 [133 486   0]
 [  0   0   0]]

2023-01-06 16:04:02,270 - ==> Best [Top1: 81.775   Sparsity:0.00   Params: 46192 on epoch: 75]
2023-01-06 16:04:02,271 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:02,275 - 

2023-01-06 16:04:02,275 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:02,636 - Epoch: [79][   10/   37]    Overall Loss 0.439255    Objective Loss 0.439255                                        LR 0.000022    Time 0.036037    
2023-01-06 16:04:02,806 - Epoch: [79][   20/   37]    Overall Loss 0.443838    Objective Loss 0.443838                                        LR 0.000022    Time 0.026466    
2023-01-06 16:04:02,984 - Epoch: [79][   30/   37]    Overall Loss 0.442534    Objective Loss 0.442534                                        LR 0.000022    Time 0.023567    
2023-01-06 16:04:03,079 - Epoch: [79][   37/   37]    Overall Loss 0.442416    Objective Loss 0.442416    Top1 79.916318    LR 0.000022    Time 0.021668    
2023-01-06 16:04:03,157 - --- validate (epoch=79)-----------
2023-01-06 16:04:03,157 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:03,382 - Epoch: [79][    5/    5]    Loss 0.439671    Top1 82.156489    
2023-01-06 16:04:03,448 - ==> Top1: 82.156    Loss: 0.440

2023-01-06 16:04:03,448 - ==> Confusion:
[[343  86   0]
 [101 518   0]
 [  0   0   0]]

2023-01-06 16:04:03,450 - ==> Best [Top1: 82.156   Sparsity:0.00   Params: 46192 on epoch: 79]
2023-01-06 16:04:03,450 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:03,454 - 

2023-01-06 16:04:03,455 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:03,817 - Epoch: [80][   10/   37]    Overall Loss 0.451515    Objective Loss 0.451515                                        LR 0.000022    Time 0.036222    
2023-01-06 16:04:03,968 - Epoch: [80][   20/   37]    Overall Loss 0.442979    Objective Loss 0.442979                                        LR 0.000022    Time 0.025591    
2023-01-06 16:04:04,130 - Epoch: [80][   30/   37]    Overall Loss 0.438442    Objective Loss 0.438442                                        LR 0.000022    Time 0.022459    
2023-01-06 16:04:04,218 - Epoch: [80][   37/   37]    Overall Loss 0.440934    Objective Loss 0.440934    Top1 78.033473    LR 0.000022    Time 0.020574    
2023-01-06 16:04:04,286 - --- validate (epoch=80)-----------
2023-01-06 16:04:04,286 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:04,527 - Epoch: [80][    5/    5]    Loss 0.412042    Top1 81.774809    
2023-01-06 16:04:04,603 - ==> Top1: 81.775    Loss: 0.412

2023-01-06 16:04:04,603 - ==> Confusion:
[[348  81   0]
 [110 509   0]
 [  0   0   0]]

2023-01-06 16:04:04,604 - ==> Best [Top1: 82.156   Sparsity:0.00   Params: 46192 on epoch: 79]
2023-01-06 16:04:04,604 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:04,609 - 

2023-01-06 16:04:04,609 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:04,975 - Epoch: [81][   10/   37]    Overall Loss 0.433041    Objective Loss 0.433041                                        LR 0.000022    Time 0.036518    
2023-01-06 16:04:05,124 - Epoch: [81][   20/   37]    Overall Loss 0.437164    Objective Loss 0.437164                                        LR 0.000022    Time 0.025693    
2023-01-06 16:04:05,263 - Epoch: [81][   30/   37]    Overall Loss 0.439639    Objective Loss 0.439639                                        LR 0.000022    Time 0.021765    
2023-01-06 16:04:05,341 - Epoch: [81][   37/   37]    Overall Loss 0.440369    Objective Loss 0.440369    Top1 77.824268    LR 0.000022    Time 0.019745    
2023-01-06 16:04:05,437 - --- validate (epoch=81)-----------
2023-01-06 16:04:05,438 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:05,665 - Epoch: [81][    5/    5]    Loss 0.431554    Top1 82.251908    
2023-01-06 16:04:05,727 - ==> Top1: 82.252    Loss: 0.432

2023-01-06 16:04:05,727 - ==> Confusion:
[[314 115   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 16:04:05,729 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:05,729 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:05,733 - 

2023-01-06 16:04:05,734 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:06,111 - Epoch: [82][   10/   37]    Overall Loss 0.441443    Objective Loss 0.441443                                        LR 0.000022    Time 0.037703    
2023-01-06 16:04:06,284 - Epoch: [82][   20/   37]    Overall Loss 0.437799    Objective Loss 0.437799                                        LR 0.000022    Time 0.027464    
2023-01-06 16:04:06,455 - Epoch: [82][   30/   37]    Overall Loss 0.439608    Objective Loss 0.439608                                        LR 0.000022    Time 0.023950    
2023-01-06 16:04:06,544 - Epoch: [82][   37/   37]    Overall Loss 0.438103    Objective Loss 0.438103    Top1 81.380753    LR 0.000022    Time 0.021820    
2023-01-06 16:04:06,620 - --- validate (epoch=82)-----------
2023-01-06 16:04:06,620 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:06,856 - Epoch: [82][    5/    5]    Loss 0.399360    Top1 82.156489    
2023-01-06 16:04:06,926 - ==> Top1: 82.156    Loss: 0.399

2023-01-06 16:04:06,926 - ==> Confusion:
[[320 109   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-06 16:04:06,927 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:06,927 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:06,932 - 

2023-01-06 16:04:06,932 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:07,307 - Epoch: [83][   10/   37]    Overall Loss 0.431031    Objective Loss 0.431031                                        LR 0.000022    Time 0.037406    
2023-01-06 16:04:07,475 - Epoch: [83][   20/   37]    Overall Loss 0.429914    Objective Loss 0.429914                                        LR 0.000022    Time 0.027080    
2023-01-06 16:04:07,655 - Epoch: [83][   30/   37]    Overall Loss 0.434424    Objective Loss 0.434424                                        LR 0.000022    Time 0.024060    
2023-01-06 16:04:07,746 - Epoch: [83][   37/   37]    Overall Loss 0.438075    Objective Loss 0.438075    Top1 80.334728    LR 0.000022    Time 0.021960    
2023-01-06 16:04:07,823 - --- validate (epoch=83)-----------
2023-01-06 16:04:07,823 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:08,174 - Epoch: [83][    5/    5]    Loss 0.411252    Top1 81.488550    
2023-01-06 16:04:08,236 - ==> Top1: 81.489    Loss: 0.411

2023-01-06 16:04:08,236 - ==> Confusion:
[[329 100   0]
 [ 94 525   0]
 [  0   0   0]]

2023-01-06 16:04:08,238 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:08,238 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:08,242 - 

2023-01-06 16:04:08,243 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:08,605 - Epoch: [84][   10/   37]    Overall Loss 0.423678    Objective Loss 0.423678                                        LR 0.000022    Time 0.036156    
2023-01-06 16:04:08,772 - Epoch: [84][   20/   37]    Overall Loss 0.427426    Objective Loss 0.427426                                        LR 0.000022    Time 0.026424    
2023-01-06 16:04:08,939 - Epoch: [84][   30/   37]    Overall Loss 0.433847    Objective Loss 0.433847                                        LR 0.000022    Time 0.023148    
2023-01-06 16:04:09,028 - Epoch: [84][   37/   37]    Overall Loss 0.436323    Objective Loss 0.436323    Top1 80.125523    LR 0.000022    Time 0.021184    
2023-01-06 16:04:09,104 - --- validate (epoch=84)-----------
2023-01-06 16:04:09,104 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:09,332 - Epoch: [84][    5/    5]    Loss 0.431111    Top1 80.438931    
2023-01-06 16:04:09,393 - ==> Top1: 80.439    Loss: 0.431

2023-01-06 16:04:09,393 - ==> Confusion:
[[363  66   0]
 [139 480   0]
 [  0   0   0]]

2023-01-06 16:04:09,394 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:09,394 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:09,399 - 

2023-01-06 16:04:09,399 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:09,778 - Epoch: [85][   10/   37]    Overall Loss 0.445919    Objective Loss 0.445919                                        LR 0.000022    Time 0.037857    
2023-01-06 16:04:09,950 - Epoch: [85][   20/   37]    Overall Loss 0.440467    Objective Loss 0.440467                                        LR 0.000022    Time 0.027505    
2023-01-06 16:04:10,123 - Epoch: [85][   30/   37]    Overall Loss 0.436763    Objective Loss 0.436763                                        LR 0.000022    Time 0.024100    
2023-01-06 16:04:10,223 - Epoch: [85][   37/   37]    Overall Loss 0.435670    Objective Loss 0.435670    Top1 78.451883    LR 0.000022    Time 0.022234    
2023-01-06 16:04:10,303 - --- validate (epoch=85)-----------
2023-01-06 16:04:10,303 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:10,530 - Epoch: [85][    5/    5]    Loss 0.426106    Top1 79.961832    
2023-01-06 16:04:10,593 - ==> Top1: 79.962    Loss: 0.426

2023-01-06 16:04:10,594 - ==> Confusion:
[[360  69   0]
 [141 478   0]
 [  0   0   0]]

2023-01-06 16:04:10,595 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:10,595 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:10,600 - 

2023-01-06 16:04:10,600 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:10,942 - Epoch: [86][   10/   37]    Overall Loss 0.441028    Objective Loss 0.441028                                        LR 0.000022    Time 0.034193    
2023-01-06 16:04:11,087 - Epoch: [86][   20/   37]    Overall Loss 0.437445    Objective Loss 0.437445                                        LR 0.000022    Time 0.024324    
2023-01-06 16:04:11,239 - Epoch: [86][   30/   37]    Overall Loss 0.432738    Objective Loss 0.432738                                        LR 0.000022    Time 0.021256    
2023-01-06 16:04:11,326 - Epoch: [86][   37/   37]    Overall Loss 0.435255    Objective Loss 0.435255    Top1 77.824268    LR 0.000022    Time 0.019592    
2023-01-06 16:04:11,390 - --- validate (epoch=86)-----------
2023-01-06 16:04:11,391 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:11,621 - Epoch: [86][    5/    5]    Loss 0.422857    Top1 81.583969    
2023-01-06 16:04:11,687 - ==> Top1: 81.584    Loss: 0.423

2023-01-06 16:04:11,687 - ==> Confusion:
[[355  74   0]
 [119 500   0]
 [  0   0   0]]

2023-01-06 16:04:11,688 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:11,688 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:11,693 - 

2023-01-06 16:04:11,693 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:12,054 - Epoch: [87][   10/   37]    Overall Loss 0.443390    Objective Loss 0.443390                                        LR 0.000022    Time 0.036019    
2023-01-06 16:04:12,202 - Epoch: [87][   20/   37]    Overall Loss 0.435178    Objective Loss 0.435178                                        LR 0.000022    Time 0.025395    
2023-01-06 16:04:12,349 - Epoch: [87][   30/   37]    Overall Loss 0.430773    Objective Loss 0.430773                                        LR 0.000022    Time 0.021815    
2023-01-06 16:04:12,438 - Epoch: [87][   37/   37]    Overall Loss 0.435413    Objective Loss 0.435413    Top1 76.569038    LR 0.000022    Time 0.020069    
2023-01-06 16:04:12,516 - --- validate (epoch=87)-----------
2023-01-06 16:04:12,517 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:12,754 - Epoch: [87][    5/    5]    Loss 0.422749    Top1 80.725191    
2023-01-06 16:04:12,818 - ==> Top1: 80.725    Loss: 0.423

2023-01-06 16:04:12,818 - ==> Confusion:
[[357  72   0]
 [130 489   0]
 [  0   0   0]]

2023-01-06 16:04:12,820 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:12,820 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:12,824 - 

2023-01-06 16:04:12,825 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:13,189 - Epoch: [88][   10/   37]    Overall Loss 0.439456    Objective Loss 0.439456                                        LR 0.000022    Time 0.036362    
2023-01-06 16:04:13,346 - Epoch: [88][   20/   37]    Overall Loss 0.441925    Objective Loss 0.441925                                        LR 0.000022    Time 0.026013    
2023-01-06 16:04:13,515 - Epoch: [88][   30/   37]    Overall Loss 0.455667    Objective Loss 0.455667                                        LR 0.000022    Time 0.022964    
2023-01-06 16:04:13,606 - Epoch: [88][   37/   37]    Overall Loss 0.460756    Objective Loss 0.460756    Top1 78.870293    LR 0.000022    Time 0.021074    
2023-01-06 16:04:13,684 - --- validate (epoch=88)-----------
2023-01-06 16:04:13,684 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:13,918 - Epoch: [88][    5/    5]    Loss 0.486303    Top1 81.393130    
2023-01-06 16:04:13,984 - ==> Top1: 81.393    Loss: 0.486

2023-01-06 16:04:13,984 - ==> Confusion:
[[295 134   0]
 [ 61 558   0]
 [  0   0   0]]

2023-01-06 16:04:13,986 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:13,986 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:13,990 - 

2023-01-06 16:04:13,990 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:14,361 - Epoch: [89][   10/   37]    Overall Loss 0.478748    Objective Loss 0.478748                                        LR 0.000022    Time 0.037015    
2023-01-06 16:04:14,514 - Epoch: [89][   20/   37]    Overall Loss 0.477612    Objective Loss 0.477612                                        LR 0.000022    Time 0.026139    
2023-01-06 16:04:14,675 - Epoch: [89][   30/   37]    Overall Loss 0.474802    Objective Loss 0.474802                                        LR 0.000022    Time 0.022791    
2023-01-06 16:04:14,761 - Epoch: [89][   37/   37]    Overall Loss 0.475880    Objective Loss 0.475880    Top1 76.778243    LR 0.000022    Time 0.020788    
2023-01-06 16:04:14,834 - --- validate (epoch=89)-----------
2023-01-06 16:04:14,834 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:15,065 - Epoch: [89][    5/    5]    Loss 0.464114    Top1 81.011450    
2023-01-06 16:04:15,137 - ==> Top1: 81.011    Loss: 0.464

2023-01-06 16:04:15,137 - ==> Confusion:
[[332  97   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:04:15,138 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:15,138 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:15,143 - 

2023-01-06 16:04:15,143 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:15,510 - Epoch: [90][   10/   37]    Overall Loss 0.474415    Objective Loss 0.474415                                        LR 0.000022    Time 0.036605    
2023-01-06 16:04:15,659 - Epoch: [90][   20/   37]    Overall Loss 0.472904    Objective Loss 0.472904                                        LR 0.000022    Time 0.025679    
2023-01-06 16:04:15,808 - Epoch: [90][   30/   37]    Overall Loss 0.473177    Objective Loss 0.473177                                        LR 0.000022    Time 0.022098    
2023-01-06 16:04:15,893 - Epoch: [90][   37/   37]    Overall Loss 0.470006    Objective Loss 0.470006    Top1 79.079498    LR 0.000022    Time 0.020202    
2023-01-06 16:04:15,972 - --- validate (epoch=90)-----------
2023-01-06 16:04:15,972 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:16,200 - Epoch: [90][    5/    5]    Loss 0.442844    Top1 81.297710    
2023-01-06 16:04:16,263 - ==> Top1: 81.298    Loss: 0.443

2023-01-06 16:04:16,263 - ==> Confusion:
[[334  95   0]
 [101 518   0]
 [  0   0   0]]

2023-01-06 16:04:16,264 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:16,264 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:16,269 - 

2023-01-06 16:04:16,269 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:16,614 - Epoch: [91][   10/   37]    Overall Loss 0.466219    Objective Loss 0.466219                                        LR 0.000022    Time 0.034422    
2023-01-06 16:04:16,752 - Epoch: [91][   20/   37]    Overall Loss 0.466905    Objective Loss 0.466905                                        LR 0.000022    Time 0.024055    
2023-01-06 16:04:16,894 - Epoch: [91][   30/   37]    Overall Loss 0.466852    Objective Loss 0.466852                                        LR 0.000022    Time 0.020743    
2023-01-06 16:04:16,977 - Epoch: [91][   37/   37]    Overall Loss 0.466496    Objective Loss 0.466496    Top1 81.589958    LR 0.000022    Time 0.019064    
2023-01-06 16:04:17,052 - --- validate (epoch=91)-----------
2023-01-06 16:04:17,052 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:17,278 - Epoch: [91][    5/    5]    Loss 0.458723    Top1 80.534351    
2023-01-06 16:04:17,364 - ==> Top1: 80.534    Loss: 0.459

2023-01-06 16:04:17,364 - ==> Confusion:
[[338  91   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 16:04:17,365 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:17,365 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:17,370 - 

2023-01-06 16:04:17,370 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:17,722 - Epoch: [92][   10/   37]    Overall Loss 0.467369    Objective Loss 0.467369                                        LR 0.000022    Time 0.035156    
2023-01-06 16:04:17,861 - Epoch: [92][   20/   37]    Overall Loss 0.467762    Objective Loss 0.467762                                        LR 0.000022    Time 0.024531    
2023-01-06 16:04:18,017 - Epoch: [92][   30/   37]    Overall Loss 0.464603    Objective Loss 0.464603                                        LR 0.000022    Time 0.021534    
2023-01-06 16:04:18,111 - Epoch: [92][   37/   37]    Overall Loss 0.463059    Objective Loss 0.463059    Top1 82.635983    LR 0.000022    Time 0.019991    
2023-01-06 16:04:18,189 - --- validate (epoch=92)-----------
2023-01-06 16:04:18,190 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:18,430 - Epoch: [92][    5/    5]    Loss 0.486845    Top1 80.820611    
2023-01-06 16:04:18,499 - ==> Top1: 80.821    Loss: 0.487

2023-01-06 16:04:18,499 - ==> Confusion:
[[346  83   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 16:04:18,501 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:18,501 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:18,505 - 

2023-01-06 16:04:18,506 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:18,891 - Epoch: [93][   10/   37]    Overall Loss 0.462119    Objective Loss 0.462119                                        LR 0.000022    Time 0.038473    
2023-01-06 16:04:19,057 - Epoch: [93][   20/   37]    Overall Loss 0.461146    Objective Loss 0.461146                                        LR 0.000022    Time 0.027485    
2023-01-06 16:04:19,219 - Epoch: [93][   30/   37]    Overall Loss 0.461494    Objective Loss 0.461494                                        LR 0.000022    Time 0.023691    
2023-01-06 16:04:19,297 - Epoch: [93][   37/   37]    Overall Loss 0.460027    Objective Loss 0.460027    Top1 80.334728    LR 0.000022    Time 0.021313    
2023-01-06 16:04:19,375 - --- validate (epoch=93)-----------
2023-01-06 16:04:19,376 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:19,607 - Epoch: [93][    5/    5]    Loss 0.450989    Top1 81.202290    
2023-01-06 16:04:19,678 - ==> Top1: 81.202    Loss: 0.451

2023-01-06 16:04:19,679 - ==> Confusion:
[[343  86   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:04:19,680 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:19,680 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:19,685 - 

2023-01-06 16:04:19,685 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:20,051 - Epoch: [94][   10/   37]    Overall Loss 0.459146    Objective Loss 0.459146                                        LR 0.000022    Time 0.036518    
2023-01-06 16:04:20,198 - Epoch: [94][   20/   37]    Overall Loss 0.460261    Objective Loss 0.460261                                        LR 0.000022    Time 0.025581    
2023-01-06 16:04:20,345 - Epoch: [94][   30/   37]    Overall Loss 0.458296    Objective Loss 0.458296                                        LR 0.000022    Time 0.021941    
2023-01-06 16:04:20,435 - Epoch: [94][   37/   37]    Overall Loss 0.457782    Objective Loss 0.457782    Top1 76.359833    LR 0.000022    Time 0.020215    
2023-01-06 16:04:20,510 - --- validate (epoch=94)-----------
2023-01-06 16:04:20,510 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:20,740 - Epoch: [94][    5/    5]    Loss 0.439293    Top1 80.820611    
2023-01-06 16:04:20,816 - ==> Top1: 80.821    Loss: 0.439

2023-01-06 16:04:20,816 - ==> Confusion:
[[298 131   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 16:04:20,817 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:20,817 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:20,822 - 

2023-01-06 16:04:20,822 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:21,182 - Epoch: [95][   10/   37]    Overall Loss 0.441988    Objective Loss 0.441988                                        LR 0.000022    Time 0.035994    
2023-01-06 16:04:21,337 - Epoch: [95][   20/   37]    Overall Loss 0.448128    Objective Loss 0.448128                                        LR 0.000022    Time 0.025690    
2023-01-06 16:04:21,506 - Epoch: [95][   30/   37]    Overall Loss 0.455635    Objective Loss 0.455635                                        LR 0.000022    Time 0.022749    
2023-01-06 16:04:21,600 - Epoch: [95][   37/   37]    Overall Loss 0.455393    Objective Loss 0.455393    Top1 80.962343    LR 0.000022    Time 0.020981    
2023-01-06 16:04:21,674 - --- validate (epoch=95)-----------
2023-01-06 16:04:21,674 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:21,899 - Epoch: [95][    5/    5]    Loss 0.444443    Top1 81.011450    
2023-01-06 16:04:21,971 - ==> Top1: 81.011    Loss: 0.444

2023-01-06 16:04:21,971 - ==> Confusion:
[[328 101   0]
 [ 98 521   0]
 [  0   0   0]]

2023-01-06 16:04:21,972 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:21,972 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:21,976 - 

2023-01-06 16:04:21,977 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:22,322 - Epoch: [96][   10/   37]    Overall Loss 0.447132    Objective Loss 0.447132                                        LR 0.000022    Time 0.034497    
2023-01-06 16:04:22,482 - Epoch: [96][   20/   37]    Overall Loss 0.452174    Objective Loss 0.452174                                        LR 0.000022    Time 0.025232    
2023-01-06 16:04:22,641 - Epoch: [96][   30/   37]    Overall Loss 0.453403    Objective Loss 0.453403                                        LR 0.000022    Time 0.022110    
2023-01-06 16:04:22,737 - Epoch: [96][   37/   37]    Overall Loss 0.453800    Objective Loss 0.453800    Top1 78.661088    LR 0.000022    Time 0.020494    
2023-01-06 16:04:22,820 - --- validate (epoch=96)-----------
2023-01-06 16:04:22,820 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:23,055 - Epoch: [96][    5/    5]    Loss 0.428693    Top1 80.916031    
2023-01-06 16:04:23,130 - ==> Top1: 80.916    Loss: 0.429

2023-01-06 16:04:23,131 - ==> Confusion:
[[344  85   0]
 [115 504   0]
 [  0   0   0]]

2023-01-06 16:04:23,132 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:23,132 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:23,136 - 

2023-01-06 16:04:23,136 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:23,485 - Epoch: [97][   10/   37]    Overall Loss 0.451725    Objective Loss 0.451725                                        LR 0.000022    Time 0.034781    
2023-01-06 16:04:23,636 - Epoch: [97][   20/   37]    Overall Loss 0.452371    Objective Loss 0.452371                                        LR 0.000022    Time 0.024942    
2023-01-06 16:04:23,795 - Epoch: [97][   30/   37]    Overall Loss 0.452260    Objective Loss 0.452260                                        LR 0.000022    Time 0.021912    
2023-01-06 16:04:23,890 - Epoch: [97][   37/   37]    Overall Loss 0.452284    Objective Loss 0.452284    Top1 78.870293    LR 0.000022    Time 0.020334    
2023-01-06 16:04:23,966 - --- validate (epoch=97)-----------
2023-01-06 16:04:23,966 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:24,196 - Epoch: [97][    5/    5]    Loss 0.423677    Top1 81.106870    
2023-01-06 16:04:24,263 - ==> Top1: 81.107    Loss: 0.424

2023-01-06 16:04:24,263 - ==> Confusion:
[[301 128   0]
 [ 70 549   0]
 [  0   0   0]]

2023-01-06 16:04:24,264 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:24,264 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:24,269 - 

2023-01-06 16:04:24,269 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:24,617 - Epoch: [98][   10/   37]    Overall Loss 0.437697    Objective Loss 0.437697                                        LR 0.000022    Time 0.034752    
2023-01-06 16:04:24,761 - Epoch: [98][   20/   37]    Overall Loss 0.453918    Objective Loss 0.453918                                        LR 0.000022    Time 0.024546    
2023-01-06 16:04:24,904 - Epoch: [98][   30/   37]    Overall Loss 0.452029    Objective Loss 0.452029                                        LR 0.000022    Time 0.021104    
2023-01-06 16:04:24,989 - Epoch: [98][   37/   37]    Overall Loss 0.449686    Objective Loss 0.449686    Top1 81.589958    LR 0.000022    Time 0.019426    
2023-01-06 16:04:25,064 - --- validate (epoch=98)-----------
2023-01-06 16:04:25,064 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:25,294 - Epoch: [98][    5/    5]    Loss 0.435608    Top1 81.870229    
2023-01-06 16:04:25,358 - ==> Top1: 81.870    Loss: 0.436

2023-01-06 16:04:25,358 - ==> Confusion:
[[331  98   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:04:25,359 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:25,359 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:25,364 - 

2023-01-06 16:04:25,364 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:25,847 - Epoch: [99][   10/   37]    Overall Loss 0.456869    Objective Loss 0.456869                                        LR 0.000022    Time 0.048274    
2023-01-06 16:04:25,991 - Epoch: [99][   20/   37]    Overall Loss 0.447540    Objective Loss 0.447540                                        LR 0.000022    Time 0.031279    
2023-01-06 16:04:26,130 - Epoch: [99][   30/   37]    Overall Loss 0.448434    Objective Loss 0.448434                                        LR 0.000022    Time 0.025472    
2023-01-06 16:04:26,215 - Epoch: [99][   37/   37]    Overall Loss 0.448522    Objective Loss 0.448522    Top1 80.125523    LR 0.000022    Time 0.022959    
2023-01-06 16:04:26,286 - --- validate (epoch=99)-----------
2023-01-06 16:04:26,287 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:26,518 - Epoch: [99][    5/    5]    Loss 0.423677    Top1 80.534351    
2023-01-06 16:04:26,586 - ==> Top1: 80.534    Loss: 0.424

2023-01-06 16:04:26,587 - ==> Confusion:
[[352  77   0]
 [127 492   0]
 [  0   0   0]]

2023-01-06 16:04:26,588 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:26,588 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:26,592 - 

2023-01-06 16:04:26,592 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:26,960 - Epoch: [100][   10/   37]    Overall Loss 0.444549    Objective Loss 0.444549                                        LR 0.000013    Time 0.036732    
2023-01-06 16:04:27,139 - Epoch: [100][   20/   37]    Overall Loss 0.445658    Objective Loss 0.445658                                        LR 0.000013    Time 0.027257    
2023-01-06 16:04:27,312 - Epoch: [100][   30/   37]    Overall Loss 0.448835    Objective Loss 0.448835                                        LR 0.000013    Time 0.023870    
2023-01-06 16:04:27,406 - Epoch: [100][   37/   37]    Overall Loss 0.448233    Objective Loss 0.448233    Top1 76.359833    LR 0.000013    Time 0.021904    
2023-01-06 16:04:27,481 - --- validate (epoch=100)-----------
2023-01-06 16:04:27,482 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:27,711 - Epoch: [100][    5/    5]    Loss 0.449163    Top1 80.629771    
2023-01-06 16:04:27,772 - ==> Top1: 80.630    Loss: 0.449

2023-01-06 16:04:27,773 - ==> Confusion:
[[297 132   0]
 [ 71 548   0]
 [  0   0   0]]

2023-01-06 16:04:27,774 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:27,774 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:27,778 - 

2023-01-06 16:04:27,778 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:28,124 - Epoch: [101][   10/   37]    Overall Loss 0.438473    Objective Loss 0.438473                                        LR 0.000013    Time 0.034528    
2023-01-06 16:04:28,266 - Epoch: [101][   20/   37]    Overall Loss 0.441782    Objective Loss 0.441782                                        LR 0.000013    Time 0.024306    
2023-01-06 16:04:28,420 - Epoch: [101][   30/   37]    Overall Loss 0.446717    Objective Loss 0.446717                                        LR 0.000013    Time 0.021333    
2023-01-06 16:04:28,511 - Epoch: [101][   37/   37]    Overall Loss 0.446470    Objective Loss 0.446470    Top1 76.359833    LR 0.000013    Time 0.019758    
2023-01-06 16:04:28,586 - --- validate (epoch=101)-----------
2023-01-06 16:04:28,587 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:28,810 - Epoch: [101][    5/    5]    Loss 0.439211    Top1 81.297710    
2023-01-06 16:04:28,883 - ==> Top1: 81.298    Loss: 0.439

2023-01-06 16:04:28,883 - ==> Confusion:
[[346  83   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 16:04:28,885 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:28,885 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:28,889 - 

2023-01-06 16:04:28,889 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:29,247 - Epoch: [102][   10/   37]    Overall Loss 0.440608    Objective Loss 0.440608                                        LR 0.000013    Time 0.035707    
2023-01-06 16:04:29,394 - Epoch: [102][   20/   37]    Overall Loss 0.447779    Objective Loss 0.447779                                        LR 0.000013    Time 0.025156    
2023-01-06 16:04:29,546 - Epoch: [102][   30/   37]    Overall Loss 0.445898    Objective Loss 0.445898                                        LR 0.000013    Time 0.021823    
2023-01-06 16:04:29,633 - Epoch: [102][   37/   37]    Overall Loss 0.446205    Objective Loss 0.446205    Top1 77.615063    LR 0.000013    Time 0.020058    
2023-01-06 16:04:29,703 - --- validate (epoch=102)-----------
2023-01-06 16:04:29,703 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:29,926 - Epoch: [102][    5/    5]    Loss 0.444328    Top1 81.583969    
2023-01-06 16:04:29,986 - ==> Top1: 81.584    Loss: 0.444

2023-01-06 16:04:29,986 - ==> Confusion:
[[329 100   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-06 16:04:29,987 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:29,987 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:29,991 - 

2023-01-06 16:04:29,992 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:30,345 - Epoch: [103][   10/   37]    Overall Loss 0.455036    Objective Loss 0.455036                                        LR 0.000013    Time 0.035270    
2023-01-06 16:04:30,511 - Epoch: [103][   20/   37]    Overall Loss 0.451328    Objective Loss 0.451328                                        LR 0.000013    Time 0.025854    
2023-01-06 16:04:30,689 - Epoch: [103][   30/   37]    Overall Loss 0.446568    Objective Loss 0.446568                                        LR 0.000013    Time 0.023163    
2023-01-06 16:04:30,783 - Epoch: [103][   37/   37]    Overall Loss 0.445241    Objective Loss 0.445241    Top1 80.125523    LR 0.000013    Time 0.021311    
2023-01-06 16:04:30,848 - --- validate (epoch=103)-----------
2023-01-06 16:04:30,848 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:31,076 - Epoch: [103][    5/    5]    Loss 0.417439    Top1 81.488550    
2023-01-06 16:04:31,142 - ==> Top1: 81.489    Loss: 0.417

2023-01-06 16:04:31,142 - ==> Confusion:
[[347  82   0]
 [112 507   0]
 [  0   0   0]]

2023-01-06 16:04:31,144 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:31,144 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:31,148 - 

2023-01-06 16:04:31,148 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:31,511 - Epoch: [104][   10/   37]    Overall Loss 0.455434    Objective Loss 0.455434                                        LR 0.000013    Time 0.036171    
2023-01-06 16:04:31,667 - Epoch: [104][   20/   37]    Overall Loss 0.450641    Objective Loss 0.450641                                        LR 0.000013    Time 0.025855    
2023-01-06 16:04:31,827 - Epoch: [104][   30/   37]    Overall Loss 0.448131    Objective Loss 0.448131                                        LR 0.000013    Time 0.022578    
2023-01-06 16:04:31,922 - Epoch: [104][   37/   37]    Overall Loss 0.443740    Objective Loss 0.443740    Top1 83.472803    LR 0.000013    Time 0.020874    
2023-01-06 16:04:32,003 - --- validate (epoch=104)-----------
2023-01-06 16:04:32,004 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:32,243 - Epoch: [104][    5/    5]    Loss 0.420274    Top1 82.061069    
2023-01-06 16:04:32,327 - ==> Top1: 82.061    Loss: 0.420

2023-01-06 16:04:32,327 - ==> Confusion:
[[333  96   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:04:32,329 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:32,329 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:32,333 - 

2023-01-06 16:04:32,333 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:32,694 - Epoch: [105][   10/   37]    Overall Loss 0.449342    Objective Loss 0.449342                                        LR 0.000013    Time 0.035953    
2023-01-06 16:04:32,843 - Epoch: [105][   20/   37]    Overall Loss 0.447431    Objective Loss 0.447431                                        LR 0.000013    Time 0.025432    
2023-01-06 16:04:33,005 - Epoch: [105][   30/   37]    Overall Loss 0.446132    Objective Loss 0.446132                                        LR 0.000013    Time 0.022348    
2023-01-06 16:04:33,100 - Epoch: [105][   37/   37]    Overall Loss 0.443177    Objective Loss 0.443177    Top1 78.870293    LR 0.000013    Time 0.020674    
2023-01-06 16:04:33,165 - --- validate (epoch=105)-----------
2023-01-06 16:04:33,165 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:33,392 - Epoch: [105][    5/    5]    Loss 0.411603    Top1 80.057252    
2023-01-06 16:04:33,458 - ==> Top1: 80.057    Loss: 0.412

2023-01-06 16:04:33,459 - ==> Confusion:
[[359  70   0]
 [139 480   0]
 [  0   0   0]]

2023-01-06 16:04:33,460 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:33,460 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:33,464 - 

2023-01-06 16:04:33,464 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:33,824 - Epoch: [106][   10/   37]    Overall Loss 0.447931    Objective Loss 0.447931                                        LR 0.000013    Time 0.035899    
2023-01-06 16:04:33,976 - Epoch: [106][   20/   37]    Overall Loss 0.447438    Objective Loss 0.447438                                        LR 0.000013    Time 0.025523    
2023-01-06 16:04:34,146 - Epoch: [106][   30/   37]    Overall Loss 0.445065    Objective Loss 0.445065                                        LR 0.000013    Time 0.022667    
2023-01-06 16:04:34,238 - Epoch: [106][   37/   37]    Overall Loss 0.443474    Objective Loss 0.443474    Top1 79.497908    LR 0.000013    Time 0.020866    
2023-01-06 16:04:34,310 - --- validate (epoch=106)-----------
2023-01-06 16:04:34,310 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:34,540 - Epoch: [106][    5/    5]    Loss 0.428070    Top1 81.965649    
2023-01-06 16:04:34,610 - ==> Top1: 81.966    Loss: 0.428

2023-01-06 16:04:34,610 - ==> Confusion:
[[319 110   0]
 [ 79 540   0]
 [  0   0   0]]

2023-01-06 16:04:34,611 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:34,611 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:34,616 - 

2023-01-06 16:04:34,616 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:34,998 - Epoch: [107][   10/   37]    Overall Loss 0.443834    Objective Loss 0.443834                                        LR 0.000013    Time 0.038188    
2023-01-06 16:04:35,160 - Epoch: [107][   20/   37]    Overall Loss 0.446371    Objective Loss 0.446371                                        LR 0.000013    Time 0.027174    
2023-01-06 16:04:35,332 - Epoch: [107][   30/   37]    Overall Loss 0.444768    Objective Loss 0.444768                                        LR 0.000013    Time 0.023830    
2023-01-06 16:04:35,426 - Epoch: [107][   37/   37]    Overall Loss 0.441725    Objective Loss 0.441725    Top1 80.125523    LR 0.000013    Time 0.021851    
2023-01-06 16:04:35,508 - --- validate (epoch=107)-----------
2023-01-06 16:04:35,508 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:35,737 - Epoch: [107][    5/    5]    Loss 0.449928    Top1 80.248092    
2023-01-06 16:04:35,804 - ==> Top1: 80.248    Loss: 0.450

2023-01-06 16:04:35,805 - ==> Confusion:
[[352  77   0]
 [130 489   0]
 [  0   0   0]]

2023-01-06 16:04:35,806 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:35,806 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:35,810 - 

2023-01-06 16:04:35,810 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:36,183 - Epoch: [108][   10/   37]    Overall Loss 0.444207    Objective Loss 0.444207                                        LR 0.000013    Time 0.037173    
2023-01-06 16:04:36,339 - Epoch: [108][   20/   37]    Overall Loss 0.446440    Objective Loss 0.446440                                        LR 0.000013    Time 0.026297    
2023-01-06 16:04:36,500 - Epoch: [108][   30/   37]    Overall Loss 0.444894    Objective Loss 0.444894                                        LR 0.000013    Time 0.022894    
2023-01-06 16:04:36,595 - Epoch: [108][   37/   37]    Overall Loss 0.441113    Objective Loss 0.441113    Top1 81.171548    LR 0.000013    Time 0.021142    
2023-01-06 16:04:36,673 - --- validate (epoch=108)-----------
2023-01-06 16:04:36,673 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:36,896 - Epoch: [108][    5/    5]    Loss 0.424593    Top1 81.774809    
2023-01-06 16:04:36,961 - ==> Top1: 81.775    Loss: 0.425

2023-01-06 16:04:36,962 - ==> Confusion:
[[346  83   0]
 [108 511   0]
 [  0   0   0]]

2023-01-06 16:04:36,963 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:36,963 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:36,968 - 

2023-01-06 16:04:36,968 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:37,344 - Epoch: [109][   10/   37]    Overall Loss 0.449918    Objective Loss 0.449918                                        LR 0.000013    Time 0.037581    
2023-01-06 16:04:37,510 - Epoch: [109][   20/   37]    Overall Loss 0.435338    Objective Loss 0.435338                                        LR 0.000013    Time 0.027030    
2023-01-06 16:04:37,678 - Epoch: [109][   30/   37]    Overall Loss 0.437393    Objective Loss 0.437393                                        LR 0.000013    Time 0.023589    
2023-01-06 16:04:37,775 - Epoch: [109][   37/   37]    Overall Loss 0.440317    Objective Loss 0.440317    Top1 79.916318    LR 0.000013    Time 0.021738    
2023-01-06 16:04:37,847 - --- validate (epoch=109)-----------
2023-01-06 16:04:37,847 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:38,079 - Epoch: [109][    5/    5]    Loss 0.434552    Top1 81.488550    
2023-01-06 16:04:38,149 - ==> Top1: 81.489    Loss: 0.435

2023-01-06 16:04:38,150 - ==> Confusion:
[[353  76   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 16:04:38,151 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:38,151 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:38,155 - 

2023-01-06 16:04:38,155 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:38,511 - Epoch: [110][   10/   37]    Overall Loss 0.446632    Objective Loss 0.446632                                        LR 0.000013    Time 0.035487    
2023-01-06 16:04:38,656 - Epoch: [110][   20/   37]    Overall Loss 0.444874    Objective Loss 0.444874                                        LR 0.000013    Time 0.024970    
2023-01-06 16:04:38,810 - Epoch: [110][   30/   37]    Overall Loss 0.440002    Objective Loss 0.440002                                        LR 0.000013    Time 0.021783    
2023-01-06 16:04:38,904 - Epoch: [110][   37/   37]    Overall Loss 0.439316    Objective Loss 0.439316    Top1 81.380753    LR 0.000013    Time 0.020186    
2023-01-06 16:04:38,975 - --- validate (epoch=110)-----------
2023-01-06 16:04:38,976 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:39,204 - Epoch: [110][    5/    5]    Loss 0.476472    Top1 81.202290    
2023-01-06 16:04:39,287 - ==> Top1: 81.202    Loss: 0.476

2023-01-06 16:04:39,288 - ==> Confusion:
[[347  82   0]
 [115 504   0]
 [  0   0   0]]

2023-01-06 16:04:39,289 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:39,289 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:39,293 - 

2023-01-06 16:04:39,293 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:39,653 - Epoch: [111][   10/   37]    Overall Loss 0.425183    Objective Loss 0.425183                                        LR 0.000013    Time 0.035934    
2023-01-06 16:04:39,810 - Epoch: [111][   20/   37]    Overall Loss 0.434655    Objective Loss 0.434655                                        LR 0.000013    Time 0.025786    
2023-01-06 16:04:39,967 - Epoch: [111][   30/   37]    Overall Loss 0.435933    Objective Loss 0.435933                                        LR 0.000013    Time 0.022410    
2023-01-06 16:04:40,061 - Epoch: [111][   37/   37]    Overall Loss 0.438804    Objective Loss 0.438804    Top1 79.497908    LR 0.000013    Time 0.020692    
2023-01-06 16:04:40,140 - --- validate (epoch=111)-----------
2023-01-06 16:04:40,141 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:40,371 - Epoch: [111][    5/    5]    Loss 0.463537    Top1 81.106870    
2023-01-06 16:04:40,431 - ==> Top1: 81.107    Loss: 0.464

2023-01-06 16:04:40,432 - ==> Confusion:
[[355  74   0]
 [124 495   0]
 [  0   0   0]]

2023-01-06 16:04:40,433 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:40,433 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:40,437 - 

2023-01-06 16:04:40,438 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:40,796 - Epoch: [112][   10/   37]    Overall Loss 0.449237    Objective Loss 0.449237                                        LR 0.000013    Time 0.035804    
2023-01-06 16:04:40,934 - Epoch: [112][   20/   37]    Overall Loss 0.440418    Objective Loss 0.440418                                        LR 0.000013    Time 0.024710    
2023-01-06 16:04:41,074 - Epoch: [112][   30/   37]    Overall Loss 0.437711    Objective Loss 0.437711                                        LR 0.000013    Time 0.021123    
2023-01-06 16:04:41,161 - Epoch: [112][   37/   37]    Overall Loss 0.438451    Objective Loss 0.438451    Top1 79.079498    LR 0.000013    Time 0.019474    
2023-01-06 16:04:41,242 - --- validate (epoch=112)-----------
2023-01-06 16:04:41,242 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:41,475 - Epoch: [112][    5/    5]    Loss 0.421664    Top1 81.488550    
2023-01-06 16:04:41,547 - ==> Top1: 81.489    Loss: 0.422

2023-01-06 16:04:41,547 - ==> Confusion:
[[346  83   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:04:41,548 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:41,548 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:41,553 - 

2023-01-06 16:04:41,553 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:41,931 - Epoch: [113][   10/   37]    Overall Loss 0.445171    Objective Loss 0.445171                                        LR 0.000013    Time 0.037703    
2023-01-06 16:04:42,099 - Epoch: [113][   20/   37]    Overall Loss 0.441171    Objective Loss 0.441171                                        LR 0.000013    Time 0.027235    
2023-01-06 16:04:42,267 - Epoch: [113][   30/   37]    Overall Loss 0.442340    Objective Loss 0.442340                                        LR 0.000013    Time 0.023752    
2023-01-06 16:04:42,360 - Epoch: [113][   37/   37]    Overall Loss 0.436962    Objective Loss 0.436962    Top1 81.589958    LR 0.000013    Time 0.021761    
2023-01-06 16:04:42,431 - --- validate (epoch=113)-----------
2023-01-06 16:04:42,431 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:42,931 - Epoch: [113][    5/    5]    Loss 0.428999    Top1 81.679389    
2023-01-06 16:04:43,003 - ==> Top1: 81.679    Loss: 0.429

2023-01-06 16:04:43,003 - ==> Confusion:
[[351  78   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 16:04:43,005 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:43,005 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:43,011 - 

2023-01-06 16:04:43,011 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:43,392 - Epoch: [114][   10/   37]    Overall Loss 0.441432    Objective Loss 0.441432                                        LR 0.000013    Time 0.037951    
2023-01-06 16:04:43,553 - Epoch: [114][   20/   37]    Overall Loss 0.437225    Objective Loss 0.437225                                        LR 0.000013    Time 0.026997    
2023-01-06 16:04:43,697 - Epoch: [114][   30/   37]    Overall Loss 0.436351    Objective Loss 0.436351                                        LR 0.000013    Time 0.022788    
2023-01-06 16:04:43,780 - Epoch: [114][   37/   37]    Overall Loss 0.436933    Objective Loss 0.436933    Top1 80.334728    LR 0.000013    Time 0.020728    
2023-01-06 16:04:43,854 - --- validate (epoch=114)-----------
2023-01-06 16:04:43,855 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:44,078 - Epoch: [114][    5/    5]    Loss 0.397769    Top1 81.965649    
2023-01-06 16:04:44,140 - ==> Top1: 81.966    Loss: 0.398

2023-01-06 16:04:44,140 - ==> Confusion:
[[328 101   0]
 [ 88 531   0]
 [  0   0   0]]

2023-01-06 16:04:44,141 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:44,142 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:44,146 - 

2023-01-06 16:04:44,146 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:44,502 - Epoch: [115][   10/   37]    Overall Loss 0.429057    Objective Loss 0.429057                                        LR 0.000013    Time 0.035558    
2023-01-06 16:04:44,646 - Epoch: [115][   20/   37]    Overall Loss 0.430642    Objective Loss 0.430642                                        LR 0.000013    Time 0.024947    
2023-01-06 16:04:44,788 - Epoch: [115][   30/   37]    Overall Loss 0.435154    Objective Loss 0.435154                                        LR 0.000013    Time 0.021364    
2023-01-06 16:04:44,870 - Epoch: [115][   37/   37]    Overall Loss 0.435894    Objective Loss 0.435894    Top1 81.589958    LR 0.000013    Time 0.019529    
2023-01-06 16:04:44,943 - --- validate (epoch=115)-----------
2023-01-06 16:04:44,943 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:45,175 - Epoch: [115][    5/    5]    Loss 0.445357    Top1 81.870229    
2023-01-06 16:04:45,247 - ==> Top1: 81.870    Loss: 0.445

2023-01-06 16:04:45,247 - ==> Confusion:
[[354  75   0]
 [115 504   0]
 [  0   0   0]]

2023-01-06 16:04:45,249 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:45,249 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:45,253 - 

2023-01-06 16:04:45,253 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:45,592 - Epoch: [116][   10/   37]    Overall Loss 0.431427    Objective Loss 0.431427                                        LR 0.000013    Time 0.033797    
2023-01-06 16:04:45,733 - Epoch: [116][   20/   37]    Overall Loss 0.433932    Objective Loss 0.433932                                        LR 0.000013    Time 0.023949    
2023-01-06 16:04:45,876 - Epoch: [116][   30/   37]    Overall Loss 0.433076    Objective Loss 0.433076                                        LR 0.000013    Time 0.020706    
2023-01-06 16:04:45,954 - Epoch: [116][   37/   37]    Overall Loss 0.435332    Objective Loss 0.435332    Top1 81.799163    LR 0.000013    Time 0.018889    
2023-01-06 16:04:46,032 - --- validate (epoch=116)-----------
2023-01-06 16:04:46,032 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:46,261 - Epoch: [116][    5/    5]    Loss 0.443674    Top1 80.343511    
2023-01-06 16:04:46,329 - ==> Top1: 80.344    Loss: 0.444

2023-01-06 16:04:46,330 - ==> Confusion:
[[355  74   0]
 [132 487   0]
 [  0   0   0]]

2023-01-06 16:04:46,331 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:46,331 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:46,335 - 

2023-01-06 16:04:46,335 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:46,683 - Epoch: [117][   10/   37]    Overall Loss 0.440873    Objective Loss 0.440873                                        LR 0.000013    Time 0.034665    
2023-01-06 16:04:46,820 - Epoch: [117][   20/   37]    Overall Loss 0.443402    Objective Loss 0.443402                                        LR 0.000013    Time 0.024195    
2023-01-06 16:04:46,961 - Epoch: [117][   30/   37]    Overall Loss 0.436863    Objective Loss 0.436863                                        LR 0.000013    Time 0.020815    
2023-01-06 16:04:47,043 - Epoch: [117][   37/   37]    Overall Loss 0.435308    Objective Loss 0.435308    Top1 77.196653    LR 0.000013    Time 0.019088    
2023-01-06 16:04:47,123 - --- validate (epoch=117)-----------
2023-01-06 16:04:47,124 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:47,353 - Epoch: [117][    5/    5]    Loss 0.415100    Top1 81.774809    
2023-01-06 16:04:47,431 - ==> Top1: 81.775    Loss: 0.415

2023-01-06 16:04:47,432 - ==> Confusion:
[[347  82   0]
 [109 510   0]
 [  0   0   0]]

2023-01-06 16:04:47,433 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:47,433 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:47,437 - 

2023-01-06 16:04:47,438 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:47,796 - Epoch: [118][   10/   37]    Overall Loss 0.432316    Objective Loss 0.432316                                        LR 0.000013    Time 0.035740    
2023-01-06 16:04:47,928 - Epoch: [118][   20/   37]    Overall Loss 0.434510    Objective Loss 0.434510                                        LR 0.000013    Time 0.024482    
2023-01-06 16:04:48,062 - Epoch: [118][   30/   37]    Overall Loss 0.432403    Objective Loss 0.432403                                        LR 0.000013    Time 0.020760    
2023-01-06 16:04:48,151 - Epoch: [118][   37/   37]    Overall Loss 0.434933    Objective Loss 0.434933    Top1 78.242678    LR 0.000013    Time 0.019228    
2023-01-06 16:04:48,229 - --- validate (epoch=118)-----------
2023-01-06 16:04:48,229 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:48,460 - Epoch: [118][    5/    5]    Loss 0.435492    Top1 81.679389    
2023-01-06 16:04:48,534 - ==> Top1: 81.679    Loss: 0.435

2023-01-06 16:04:48,534 - ==> Confusion:
[[345  84   0]
 [108 511   0]
 [  0   0   0]]

2023-01-06 16:04:48,535 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:48,535 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:48,539 - 

2023-01-06 16:04:48,540 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:48,890 - Epoch: [119][   10/   37]    Overall Loss 0.427801    Objective Loss 0.427801                                        LR 0.000013    Time 0.034930    
2023-01-06 16:04:49,030 - Epoch: [119][   20/   37]    Overall Loss 0.433685    Objective Loss 0.433685                                        LR 0.000013    Time 0.024471    
2023-01-06 16:04:49,173 - Epoch: [119][   30/   37]    Overall Loss 0.435618    Objective Loss 0.435618                                        LR 0.000013    Time 0.021053    
2023-01-06 16:04:49,260 - Epoch: [119][   37/   37]    Overall Loss 0.433158    Objective Loss 0.433158    Top1 80.753138    LR 0.000013    Time 0.019428    
2023-01-06 16:04:49,338 - --- validate (epoch=119)-----------
2023-01-06 16:04:49,338 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:49,568 - Epoch: [119][    5/    5]    Loss 0.420964    Top1 81.774809    
2023-01-06 16:04:49,626 - ==> Top1: 81.775    Loss: 0.421

2023-01-06 16:04:49,627 - ==> Confusion:
[[332  97   0]
 [ 94 525   0]
 [  0   0   0]]

2023-01-06 16:04:49,628 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:49,628 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:49,633 - 

2023-01-06 16:04:49,633 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:49,989 - Epoch: [120][   10/   37]    Overall Loss 0.440927    Objective Loss 0.440927                                        LR 0.000013    Time 0.035584    
2023-01-06 16:04:50,130 - Epoch: [120][   20/   37]    Overall Loss 0.435068    Objective Loss 0.435068                                        LR 0.000013    Time 0.024794    
2023-01-06 16:04:50,274 - Epoch: [120][   30/   37]    Overall Loss 0.431027    Objective Loss 0.431027                                        LR 0.000013    Time 0.021325    
2023-01-06 16:04:50,359 - Epoch: [120][   37/   37]    Overall Loss 0.433126    Objective Loss 0.433126    Top1 79.079498    LR 0.000013    Time 0.019585    
2023-01-06 16:04:50,441 - --- validate (epoch=120)-----------
2023-01-06 16:04:50,441 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:50,667 - Epoch: [120][    5/    5]    Loss 0.436246    Top1 81.965649    
2023-01-06 16:04:50,730 - ==> Top1: 81.966    Loss: 0.436

2023-01-06 16:04:50,730 - ==> Confusion:
[[330  99   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-06 16:04:50,732 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:50,732 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:50,736 - 

2023-01-06 16:04:50,736 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:51,083 - Epoch: [121][   10/   37]    Overall Loss 0.430552    Objective Loss 0.430552                                        LR 0.000013    Time 0.034606    
2023-01-06 16:04:51,223 - Epoch: [121][   20/   37]    Overall Loss 0.432612    Objective Loss 0.432612                                        LR 0.000013    Time 0.024290    
2023-01-06 16:04:51,364 - Epoch: [121][   30/   37]    Overall Loss 0.436227    Objective Loss 0.436227                                        LR 0.000013    Time 0.020885    
2023-01-06 16:04:51,443 - Epoch: [121][   37/   37]    Overall Loss 0.431783    Objective Loss 0.431783    Top1 83.891213    LR 0.000013    Time 0.019071    
2023-01-06 16:04:51,511 - --- validate (epoch=121)-----------
2023-01-06 16:04:51,511 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:51,738 - Epoch: [121][    5/    5]    Loss 0.392850    Top1 81.393130    
2023-01-06 16:04:51,802 - ==> Top1: 81.393    Loss: 0.393

2023-01-06 16:04:51,802 - ==> Confusion:
[[351  78   0]
 [117 502   0]
 [  0   0   0]]

2023-01-06 16:04:51,803 - ==> Best [Top1: 82.252   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:04:51,804 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:51,808 - 

2023-01-06 16:04:51,808 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:52,166 - Epoch: [122][   10/   37]    Overall Loss 0.434928    Objective Loss 0.434928                                        LR 0.000013    Time 0.035720    
2023-01-06 16:04:52,324 - Epoch: [122][   20/   37]    Overall Loss 0.429852    Objective Loss 0.429852                                        LR 0.000013    Time 0.025714    
2023-01-06 16:04:52,482 - Epoch: [122][   30/   37]    Overall Loss 0.430276    Objective Loss 0.430276                                        LR 0.000013    Time 0.022418    
2023-01-06 16:04:52,577 - Epoch: [122][   37/   37]    Overall Loss 0.430555    Objective Loss 0.430555    Top1 80.962343    LR 0.000013    Time 0.020714    
2023-01-06 16:04:52,645 - --- validate (epoch=122)-----------
2023-01-06 16:04:52,645 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:52,876 - Epoch: [122][    5/    5]    Loss 0.409704    Top1 82.729008    
2023-01-06 16:04:52,950 - ==> Top1: 82.729    Loss: 0.410

2023-01-06 16:04:52,950 - ==> Confusion:
[[348  81   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 16:04:52,952 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:04:52,952 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:52,956 - 

2023-01-06 16:04:52,957 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:53,330 - Epoch: [123][   10/   37]    Overall Loss 0.420428    Objective Loss 0.420428                                        LR 0.000013    Time 0.037237    
2023-01-06 16:04:53,497 - Epoch: [123][   20/   37]    Overall Loss 0.427716    Objective Loss 0.427716                                        LR 0.000013    Time 0.026961    
2023-01-06 16:04:53,645 - Epoch: [123][   30/   37]    Overall Loss 0.426950    Objective Loss 0.426950                                        LR 0.000013    Time 0.022887    
2023-01-06 16:04:53,735 - Epoch: [123][   37/   37]    Overall Loss 0.430314    Objective Loss 0.430314    Top1 81.171548    LR 0.000013    Time 0.020999    
2023-01-06 16:04:53,806 - --- validate (epoch=123)-----------
2023-01-06 16:04:53,807 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:54,033 - Epoch: [123][    5/    5]    Loss 0.452364    Top1 80.820611    
2023-01-06 16:04:54,093 - ==> Top1: 80.821    Loss: 0.452

2023-01-06 16:04:54,094 - ==> Confusion:
[[361  68   0]
 [133 486   0]
 [  0   0   0]]

2023-01-06 16:04:54,095 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:04:54,095 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:54,099 - 

2023-01-06 16:04:54,100 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:54,474 - Epoch: [124][   10/   37]    Overall Loss 0.429147    Objective Loss 0.429147                                        LR 0.000013    Time 0.037347    
2023-01-06 16:04:54,629 - Epoch: [124][   20/   37]    Overall Loss 0.426162    Objective Loss 0.426162                                        LR 0.000013    Time 0.026413    
2023-01-06 16:04:54,789 - Epoch: [124][   30/   37]    Overall Loss 0.432513    Objective Loss 0.432513                                        LR 0.000013    Time 0.022922    
2023-01-06 16:04:54,881 - Epoch: [124][   37/   37]    Overall Loss 0.429858    Objective Loss 0.429858    Top1 79.288703    LR 0.000013    Time 0.021073    
2023-01-06 16:04:54,962 - --- validate (epoch=124)-----------
2023-01-06 16:04:54,963 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:55,205 - Epoch: [124][    5/    5]    Loss 0.397140    Top1 82.729008    
2023-01-06 16:04:55,271 - ==> Top1: 82.729    Loss: 0.397

2023-01-06 16:04:55,271 - ==> Confusion:
[[329 100   0]
 [ 81 538   0]
 [  0   0   0]]

2023-01-06 16:04:55,272 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:04:55,272 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:55,277 - 

2023-01-06 16:04:55,278 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:55,638 - Epoch: [125][   10/   37]    Overall Loss 0.445238    Objective Loss 0.445238                                        LR 0.000013    Time 0.036013    
2023-01-06 16:04:55,805 - Epoch: [125][   20/   37]    Overall Loss 0.442390    Objective Loss 0.442390                                        LR 0.000013    Time 0.026323    
2023-01-06 16:04:55,973 - Epoch: [125][   30/   37]    Overall Loss 0.433303    Objective Loss 0.433303                                        LR 0.000013    Time 0.023138    
2023-01-06 16:04:56,063 - Epoch: [125][   37/   37]    Overall Loss 0.429678    Objective Loss 0.429678    Top1 79.916318    LR 0.000013    Time 0.021172    
2023-01-06 16:04:56,136 - --- validate (epoch=125)-----------
2023-01-06 16:04:56,136 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:56,372 - Epoch: [125][    5/    5]    Loss 0.440970    Top1 80.725191    
2023-01-06 16:04:56,430 - ==> Top1: 80.725    Loss: 0.441

2023-01-06 16:04:56,430 - ==> Confusion:
[[360  69   0]
 [133 486   0]
 [  0   0   0]]

2023-01-06 16:04:56,432 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:04:56,432 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:56,436 - 

2023-01-06 16:04:56,436 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:56,799 - Epoch: [126][   10/   37]    Overall Loss 0.423652    Objective Loss 0.423652                                        LR 0.000013    Time 0.036250    
2023-01-06 16:04:56,960 - Epoch: [126][   20/   37]    Overall Loss 0.425515    Objective Loss 0.425515                                        LR 0.000013    Time 0.026145    
2023-01-06 16:04:57,135 - Epoch: [126][   30/   37]    Overall Loss 0.429580    Objective Loss 0.429580                                        LR 0.000013    Time 0.023232    
2023-01-06 16:04:57,224 - Epoch: [126][   37/   37]    Overall Loss 0.427829    Objective Loss 0.427829    Top1 80.125523    LR 0.000013    Time 0.021250    
2023-01-06 16:04:57,294 - --- validate (epoch=126)-----------
2023-01-06 16:04:57,295 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:57,526 - Epoch: [126][    5/    5]    Loss 0.421051    Top1 81.297710    
2023-01-06 16:04:57,593 - ==> Top1: 81.298    Loss: 0.421

2023-01-06 16:04:57,593 - ==> Confusion:
[[351  78   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 16:04:57,594 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:04:57,594 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:57,599 - 

2023-01-06 16:04:57,599 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:57,951 - Epoch: [127][   10/   37]    Overall Loss 0.437441    Objective Loss 0.437441                                        LR 0.000013    Time 0.035128    
2023-01-06 16:04:58,104 - Epoch: [127][   20/   37]    Overall Loss 0.434354    Objective Loss 0.434354                                        LR 0.000013    Time 0.025194    
2023-01-06 16:04:58,257 - Epoch: [127][   30/   37]    Overall Loss 0.429558    Objective Loss 0.429558                                        LR 0.000013    Time 0.021884    
2023-01-06 16:04:58,348 - Epoch: [127][   37/   37]    Overall Loss 0.428781    Objective Loss 0.428781    Top1 82.635983    LR 0.000013    Time 0.020197    
2023-01-06 16:04:58,425 - --- validate (epoch=127)-----------
2023-01-06 16:04:58,425 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:58,656 - Epoch: [127][    5/    5]    Loss 0.423725    Top1 82.442748    
2023-01-06 16:04:58,729 - ==> Top1: 82.443    Loss: 0.424

2023-01-06 16:04:58,729 - ==> Confusion:
[[344  85   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:04:58,731 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:04:58,731 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:58,735 - 

2023-01-06 16:04:58,735 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:04:59,099 - Epoch: [128][   10/   37]    Overall Loss 0.426933    Objective Loss 0.426933                                        LR 0.000013    Time 0.036326    
2023-01-06 16:04:59,248 - Epoch: [128][   20/   37]    Overall Loss 0.427622    Objective Loss 0.427622                                        LR 0.000013    Time 0.025586    
2023-01-06 16:04:59,396 - Epoch: [128][   30/   37]    Overall Loss 0.429424    Objective Loss 0.429424                                        LR 0.000013    Time 0.021983    
2023-01-06 16:04:59,482 - Epoch: [128][   37/   37]    Overall Loss 0.428003    Objective Loss 0.428003    Top1 81.799163    LR 0.000013    Time 0.020137    
2023-01-06 16:04:59,553 - --- validate (epoch=128)-----------
2023-01-06 16:04:59,554 - 1048 samples (256 per mini-batch)
2023-01-06 16:04:59,783 - Epoch: [128][    5/    5]    Loss 0.396697    Top1 81.583969    
2023-01-06 16:04:59,847 - ==> Top1: 81.584    Loss: 0.397

2023-01-06 16:04:59,848 - ==> Confusion:
[[358  71   0]
 [122 497   0]
 [  0   0   0]]

2023-01-06 16:04:59,849 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:04:59,849 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:04:59,853 - 

2023-01-06 16:04:59,853 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:00,339 - Epoch: [129][   10/   37]    Overall Loss 0.419534    Objective Loss 0.419534                                        LR 0.000013    Time 0.048457    
2023-01-06 16:05:00,483 - Epoch: [129][   20/   37]    Overall Loss 0.422252    Objective Loss 0.422252                                        LR 0.000013    Time 0.031400    
2023-01-06 16:05:00,625 - Epoch: [129][   30/   37]    Overall Loss 0.422454    Objective Loss 0.422454                                        LR 0.000013    Time 0.025650    
2023-01-06 16:05:00,710 - Epoch: [129][   37/   37]    Overall Loss 0.426480    Objective Loss 0.426480    Top1 77.615063    LR 0.000013    Time 0.023095    
2023-01-06 16:05:00,794 - --- validate (epoch=129)-----------
2023-01-06 16:05:00,794 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:01,019 - Epoch: [129][    5/    5]    Loss 0.414652    Top1 81.870229    
2023-01-06 16:05:01,081 - ==> Top1: 81.870    Loss: 0.415

2023-01-06 16:05:01,081 - ==> Confusion:
[[340  89   0]
 [101 518   0]
 [  0   0   0]]

2023-01-06 16:05:01,082 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:05:01,082 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:01,087 - 

2023-01-06 16:05:01,087 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:01,455 - Epoch: [130][   10/   37]    Overall Loss 0.423975    Objective Loss 0.423975                                        LR 0.000013    Time 0.036745    
2023-01-06 16:05:01,614 - Epoch: [130][   20/   37]    Overall Loss 0.417517    Objective Loss 0.417517                                        LR 0.000013    Time 0.026274    
2023-01-06 16:05:01,778 - Epoch: [130][   30/   37]    Overall Loss 0.424947    Objective Loss 0.424947                                        LR 0.000013    Time 0.022950    
2023-01-06 16:05:01,863 - Epoch: [130][   37/   37]    Overall Loss 0.425915    Objective Loss 0.425915    Top1 82.845188    LR 0.000013    Time 0.020897    
2023-01-06 16:05:01,941 - --- validate (epoch=130)-----------
2023-01-06 16:05:01,941 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:02,169 - Epoch: [130][    5/    5]    Loss 0.415464    Top1 82.538168    
2023-01-06 16:05:02,232 - ==> Top1: 82.538    Loss: 0.415

2023-01-06 16:05:02,232 - ==> Confusion:
[[328 101   0]
 [ 82 537   0]
 [  0   0   0]]

2023-01-06 16:05:02,234 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:05:02,234 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:02,238 - 

2023-01-06 16:05:02,238 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:02,607 - Epoch: [131][   10/   37]    Overall Loss 0.437438    Objective Loss 0.437438                                        LR 0.000013    Time 0.036849    
2023-01-06 16:05:02,761 - Epoch: [131][   20/   37]    Overall Loss 0.432344    Objective Loss 0.432344                                        LR 0.000013    Time 0.026082    
2023-01-06 16:05:02,918 - Epoch: [131][   30/   37]    Overall Loss 0.427390    Objective Loss 0.427390                                        LR 0.000013    Time 0.022564    
2023-01-06 16:05:03,004 - Epoch: [131][   37/   37]    Overall Loss 0.425825    Objective Loss 0.425825    Top1 80.753138    LR 0.000013    Time 0.020620    
2023-01-06 16:05:03,087 - --- validate (epoch=131)-----------
2023-01-06 16:05:03,087 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:03,313 - Epoch: [131][    5/    5]    Loss 0.415087    Top1 82.251908    
2023-01-06 16:05:03,376 - ==> Top1: 82.252    Loss: 0.415

2023-01-06 16:05:03,377 - ==> Confusion:
[[345  84   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:05:03,378 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:05:03,378 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:03,383 - 

2023-01-06 16:05:03,383 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:03,735 - Epoch: [132][   10/   37]    Overall Loss 0.430379    Objective Loss 0.430379                                        LR 0.000013    Time 0.035161    
2023-01-06 16:05:03,882 - Epoch: [132][   20/   37]    Overall Loss 0.428253    Objective Loss 0.428253                                        LR 0.000013    Time 0.024930    
2023-01-06 16:05:04,038 - Epoch: [132][   30/   37]    Overall Loss 0.427404    Objective Loss 0.427404                                        LR 0.000013    Time 0.021776    
2023-01-06 16:05:04,123 - Epoch: [132][   37/   37]    Overall Loss 0.424509    Objective Loss 0.424509    Top1 83.263598    LR 0.000013    Time 0.019949    
2023-01-06 16:05:04,192 - --- validate (epoch=132)-----------
2023-01-06 16:05:04,192 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:04,421 - Epoch: [132][    5/    5]    Loss 0.428576    Top1 81.870229    
2023-01-06 16:05:04,488 - ==> Top1: 81.870    Loss: 0.429

2023-01-06 16:05:04,489 - ==> Confusion:
[[353  76   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 16:05:04,490 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:05:04,490 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:04,494 - 

2023-01-06 16:05:04,494 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:04,850 - Epoch: [133][   10/   37]    Overall Loss 0.424147    Objective Loss 0.424147                                        LR 0.000013    Time 0.035487    
2023-01-06 16:05:05,001 - Epoch: [133][   20/   37]    Overall Loss 0.422258    Objective Loss 0.422258                                        LR 0.000013    Time 0.025288    
2023-01-06 16:05:05,156 - Epoch: [133][   30/   37]    Overall Loss 0.425533    Objective Loss 0.425533                                        LR 0.000013    Time 0.022025    
2023-01-06 16:05:05,250 - Epoch: [133][   37/   37]    Overall Loss 0.423877    Objective Loss 0.423877    Top1 80.753138    LR 0.000013    Time 0.020394    
2023-01-06 16:05:05,316 - --- validate (epoch=133)-----------
2023-01-06 16:05:05,317 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:05,550 - Epoch: [133][    5/    5]    Loss 0.462317    Top1 82.442748    
2023-01-06 16:05:05,625 - ==> Top1: 82.443    Loss: 0.462

2023-01-06 16:05:05,625 - ==> Confusion:
[[345  84   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 16:05:05,626 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 124]
2023-01-06 16:05:05,626 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:05,631 - 

2023-01-06 16:05:05,631 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:05,996 - Epoch: [134][   10/   37]    Overall Loss 0.427025    Objective Loss 0.427025                                        LR 0.000013    Time 0.036363    
2023-01-06 16:05:06,154 - Epoch: [134][   20/   37]    Overall Loss 0.422902    Objective Loss 0.422902                                        LR 0.000013    Time 0.026061    
2023-01-06 16:05:06,319 - Epoch: [134][   30/   37]    Overall Loss 0.420575    Objective Loss 0.420575                                        LR 0.000013    Time 0.022871    
2023-01-06 16:05:06,414 - Epoch: [134][   37/   37]    Overall Loss 0.424055    Objective Loss 0.424055    Top1 79.497908    LR 0.000013    Time 0.021124    
2023-01-06 16:05:06,486 - --- validate (epoch=134)-----------
2023-01-06 16:05:06,486 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:06,715 - Epoch: [134][    5/    5]    Loss 0.385483    Top1 82.729008    
2023-01-06 16:05:06,774 - ==> Top1: 82.729    Loss: 0.385

2023-01-06 16:05:06,774 - ==> Confusion:
[[341  88   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-06 16:05:06,776 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 134]
2023-01-06 16:05:06,776 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:06,781 - 

2023-01-06 16:05:06,781 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:07,148 - Epoch: [135][   10/   37]    Overall Loss 0.426099    Objective Loss 0.426099                                        LR 0.000013    Time 0.036710    
2023-01-06 16:05:07,310 - Epoch: [135][   20/   37]    Overall Loss 0.424462    Objective Loss 0.424462                                        LR 0.000013    Time 0.026408    
2023-01-06 16:05:07,479 - Epoch: [135][   30/   37]    Overall Loss 0.422373    Objective Loss 0.422373                                        LR 0.000013    Time 0.023155    
2023-01-06 16:05:07,575 - Epoch: [135][   37/   37]    Overall Loss 0.423993    Objective Loss 0.423993    Top1 78.242678    LR 0.000013    Time 0.021369    
2023-01-06 16:05:07,647 - --- validate (epoch=135)-----------
2023-01-06 16:05:07,648 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:07,876 - Epoch: [135][    5/    5]    Loss 0.433088    Top1 81.774809    
2023-01-06 16:05:07,960 - ==> Top1: 81.775    Loss: 0.433

2023-01-06 16:05:07,960 - ==> Confusion:
[[313 116   0]
 [ 75 544   0]
 [  0   0   0]]

2023-01-06 16:05:07,962 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 134]
2023-01-06 16:05:07,962 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:07,966 - 

2023-01-06 16:05:07,966 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:08,336 - Epoch: [136][   10/   37]    Overall Loss 0.421439    Objective Loss 0.421439                                        LR 0.000013    Time 0.036909    
2023-01-06 16:05:08,496 - Epoch: [136][   20/   37]    Overall Loss 0.420408    Objective Loss 0.420408                                        LR 0.000013    Time 0.026454    
2023-01-06 16:05:08,657 - Epoch: [136][   30/   37]    Overall Loss 0.425125    Objective Loss 0.425125                                        LR 0.000013    Time 0.022954    
2023-01-06 16:05:08,753 - Epoch: [136][   37/   37]    Overall Loss 0.423698    Objective Loss 0.423698    Top1 81.171548    LR 0.000013    Time 0.021198    
2023-01-06 16:05:08,826 - --- validate (epoch=136)-----------
2023-01-06 16:05:08,826 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:09,061 - Epoch: [136][    5/    5]    Loss 0.410780    Top1 81.393130    
2023-01-06 16:05:09,133 - ==> Top1: 81.393    Loss: 0.411

2023-01-06 16:05:09,133 - ==> Confusion:
[[360  69   0]
 [126 493   0]
 [  0   0   0]]

2023-01-06 16:05:09,134 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 134]
2023-01-06 16:05:09,134 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:09,139 - 

2023-01-06 16:05:09,139 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:09,511 - Epoch: [137][   10/   37]    Overall Loss 0.432003    Objective Loss 0.432003                                        LR 0.000013    Time 0.037201    
2023-01-06 16:05:09,681 - Epoch: [137][   20/   37]    Overall Loss 0.423623    Objective Loss 0.423623                                        LR 0.000013    Time 0.027070    
2023-01-06 16:05:09,846 - Epoch: [137][   30/   37]    Overall Loss 0.423761    Objective Loss 0.423761                                        LR 0.000013    Time 0.023513    
2023-01-06 16:05:09,942 - Epoch: [137][   37/   37]    Overall Loss 0.422566    Objective Loss 0.422566    Top1 80.125523    LR 0.000013    Time 0.021657    
2023-01-06 16:05:10,015 - --- validate (epoch=137)-----------
2023-01-06 16:05:10,015 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:10,244 - Epoch: [137][    5/    5]    Loss 0.406905    Top1 82.251908    
2023-01-06 16:05:10,307 - ==> Top1: 82.252    Loss: 0.407

2023-01-06 16:05:10,308 - ==> Confusion:
[[335  94   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:05:10,309 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 134]
2023-01-06 16:05:10,309 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:10,314 - 

2023-01-06 16:05:10,314 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:10,699 - Epoch: [138][   10/   37]    Overall Loss 0.415074    Objective Loss 0.415074                                        LR 0.000013    Time 0.038474    
2023-01-06 16:05:10,875 - Epoch: [138][   20/   37]    Overall Loss 0.416636    Objective Loss 0.416636                                        LR 0.000013    Time 0.027995    
2023-01-06 16:05:11,048 - Epoch: [138][   30/   37]    Overall Loss 0.420586    Objective Loss 0.420586                                        LR 0.000013    Time 0.024437    
2023-01-06 16:05:11,145 - Epoch: [138][   37/   37]    Overall Loss 0.421851    Objective Loss 0.421851    Top1 82.217573    LR 0.000013    Time 0.022406    
2023-01-06 16:05:11,215 - --- validate (epoch=138)-----------
2023-01-06 16:05:11,215 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:11,439 - Epoch: [138][    5/    5]    Loss 0.410393    Top1 82.729008    
2023-01-06 16:05:11,512 - ==> Top1: 82.729    Loss: 0.410

2023-01-06 16:05:11,512 - ==> Confusion:
[[342  87   0]
 [ 94 525   0]
 [  0   0   0]]

2023-01-06 16:05:11,514 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:11,514 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:11,518 - 

2023-01-06 16:05:11,519 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:11,883 - Epoch: [139][   10/   37]    Overall Loss 0.425130    Objective Loss 0.425130                                        LR 0.000013    Time 0.036377    
2023-01-06 16:05:12,040 - Epoch: [139][   20/   37]    Overall Loss 0.426547    Objective Loss 0.426547                                        LR 0.000013    Time 0.025991    
2023-01-06 16:05:12,194 - Epoch: [139][   30/   37]    Overall Loss 0.423109    Objective Loss 0.423109                                        LR 0.000013    Time 0.022455    
2023-01-06 16:05:12,286 - Epoch: [139][   37/   37]    Overall Loss 0.420957    Objective Loss 0.420957    Top1 82.845188    LR 0.000013    Time 0.020695    
2023-01-06 16:05:12,368 - --- validate (epoch=139)-----------
2023-01-06 16:05:12,369 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:12,601 - Epoch: [139][    5/    5]    Loss 0.388024    Top1 81.297710    
2023-01-06 16:05:12,661 - ==> Top1: 81.298    Loss: 0.388

2023-01-06 16:05:12,662 - ==> Confusion:
[[363  66   0]
 [130 489   0]
 [  0   0   0]]

2023-01-06 16:05:12,663 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:12,663 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:12,667 - 

2023-01-06 16:05:12,668 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:13,021 - Epoch: [140][   10/   37]    Overall Loss 0.441770    Objective Loss 0.441770                                        LR 0.000008    Time 0.035308    
2023-01-06 16:05:13,161 - Epoch: [140][   20/   37]    Overall Loss 0.424003    Objective Loss 0.424003                                        LR 0.000008    Time 0.024565    
2023-01-06 16:05:13,307 - Epoch: [140][   30/   37]    Overall Loss 0.422814    Objective Loss 0.422814                                        LR 0.000008    Time 0.021215    
2023-01-06 16:05:13,394 - Epoch: [140][   37/   37]    Overall Loss 0.420510    Objective Loss 0.420510    Top1 78.242678    LR 0.000008    Time 0.019572    
2023-01-06 16:05:13,471 - --- validate (epoch=140)-----------
2023-01-06 16:05:13,472 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:13,705 - Epoch: [140][    5/    5]    Loss 0.422263    Top1 81.870229    
2023-01-06 16:05:13,781 - ==> Top1: 81.870    Loss: 0.422

2023-01-06 16:05:13,781 - ==> Confusion:
[[353  76   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 16:05:13,782 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:13,782 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:13,787 - 

2023-01-06 16:05:13,787 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:14,142 - Epoch: [141][   10/   37]    Overall Loss 0.427351    Objective Loss 0.427351                                        LR 0.000008    Time 0.035419    
2023-01-06 16:05:14,295 - Epoch: [141][   20/   37]    Overall Loss 0.426374    Objective Loss 0.426374                                        LR 0.000008    Time 0.025351    
2023-01-06 16:05:14,453 - Epoch: [141][   30/   37]    Overall Loss 0.422866    Objective Loss 0.422866                                        LR 0.000008    Time 0.022166    
2023-01-06 16:05:14,544 - Epoch: [141][   37/   37]    Overall Loss 0.419423    Objective Loss 0.419423    Top1 82.217573    LR 0.000008    Time 0.020411    
2023-01-06 16:05:14,618 - --- validate (epoch=141)-----------
2023-01-06 16:05:14,618 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:14,843 - Epoch: [141][    5/    5]    Loss 0.415363    Top1 82.347328    
2023-01-06 16:05:14,920 - ==> Top1: 82.347    Loss: 0.415

2023-01-06 16:05:14,920 - ==> Confusion:
[[355  74   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:05:14,921 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:14,922 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:14,926 - 

2023-01-06 16:05:14,926 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:15,306 - Epoch: [142][   10/   37]    Overall Loss 0.420067    Objective Loss 0.420067                                        LR 0.000008    Time 0.037902    
2023-01-06 16:05:15,471 - Epoch: [142][   20/   37]    Overall Loss 0.428921    Objective Loss 0.428921                                        LR 0.000008    Time 0.027209    
2023-01-06 16:05:15,642 - Epoch: [142][   30/   37]    Overall Loss 0.420740    Objective Loss 0.420740                                        LR 0.000008    Time 0.023815    
2023-01-06 16:05:15,737 - Epoch: [142][   37/   37]    Overall Loss 0.419799    Objective Loss 0.419799    Top1 79.497908    LR 0.000008    Time 0.021876    
2023-01-06 16:05:15,808 - --- validate (epoch=142)-----------
2023-01-06 16:05:15,808 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:16,032 - Epoch: [142][    5/    5]    Loss 0.428090    Top1 82.251908    
2023-01-06 16:05:16,098 - ==> Top1: 82.252    Loss: 0.428

2023-01-06 16:05:16,098 - ==> Confusion:
[[332  97   0]
 [ 89 530   0]
 [  0   0   0]]

2023-01-06 16:05:16,100 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:16,100 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:16,104 - 

2023-01-06 16:05:16,104 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:16,470 - Epoch: [143][   10/   37]    Overall Loss 0.408577    Objective Loss 0.408577                                        LR 0.000008    Time 0.036451    
2023-01-06 16:05:16,627 - Epoch: [143][   20/   37]    Overall Loss 0.409331    Objective Loss 0.409331                                        LR 0.000008    Time 0.026093    
2023-01-06 16:05:16,775 - Epoch: [143][   30/   37]    Overall Loss 0.413885    Objective Loss 0.413885                                        LR 0.000008    Time 0.022278    
2023-01-06 16:05:16,864 - Epoch: [143][   37/   37]    Overall Loss 0.419813    Objective Loss 0.419813    Top1 80.543933    LR 0.000008    Time 0.020470    
2023-01-06 16:05:16,935 - --- validate (epoch=143)-----------
2023-01-06 16:05:16,935 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:17,165 - Epoch: [143][    5/    5]    Loss 0.407668    Top1 82.633588    
2023-01-06 16:05:17,236 - ==> Top1: 82.634    Loss: 0.408

2023-01-06 16:05:17,236 - ==> Confusion:
[[353  76   0]
 [106 513   0]
 [  0   0   0]]

2023-01-06 16:05:17,238 - ==> Best [Top1: 82.729   Sparsity:0.00   Params: 46192 on epoch: 138]
2023-01-06 16:05:17,238 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:17,242 - 

2023-01-06 16:05:17,243 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:17,721 - Epoch: [144][   10/   37]    Overall Loss 0.409082    Objective Loss 0.409082                                        LR 0.000008    Time 0.047750    
2023-01-06 16:05:17,882 - Epoch: [144][   20/   37]    Overall Loss 0.415786    Objective Loss 0.415786                                        LR 0.000008    Time 0.031906    
2023-01-06 16:05:18,041 - Epoch: [144][   30/   37]    Overall Loss 0.415274    Objective Loss 0.415274                                        LR 0.000008    Time 0.026565    
2023-01-06 16:05:18,121 - Epoch: [144][   37/   37]    Overall Loss 0.418889    Objective Loss 0.418889    Top1 79.288703    LR 0.000008    Time 0.023693    
2023-01-06 16:05:18,201 - --- validate (epoch=144)-----------
2023-01-06 16:05:18,201 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:18,430 - Epoch: [144][    5/    5]    Loss 0.422844    Top1 82.824427    
2023-01-06 16:05:18,489 - ==> Top1: 82.824    Loss: 0.423

2023-01-06 16:05:18,490 - ==> Confusion:
[[329 100   0]
 [ 80 539   0]
 [  0   0   0]]

2023-01-06 16:05:18,491 - ==> Best [Top1: 82.824   Sparsity:0.00   Params: 46192 on epoch: 144]
2023-01-06 16:05:18,491 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:18,496 - 

2023-01-06 16:05:18,497 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:18,858 - Epoch: [145][   10/   37]    Overall Loss 0.423240    Objective Loss 0.423240                                        LR 0.000008    Time 0.036040    
2023-01-06 16:05:19,019 - Epoch: [145][   20/   37]    Overall Loss 0.423027    Objective Loss 0.423027                                        LR 0.000008    Time 0.026076    
2023-01-06 16:05:19,187 - Epoch: [145][   30/   37]    Overall Loss 0.419811    Objective Loss 0.419811                                        LR 0.000008    Time 0.022920    
2023-01-06 16:05:19,283 - Epoch: [145][   37/   37]    Overall Loss 0.419056    Objective Loss 0.419056    Top1 78.451883    LR 0.000008    Time 0.021169    
2023-01-06 16:05:19,353 - --- validate (epoch=145)-----------
2023-01-06 16:05:19,354 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:19,581 - Epoch: [145][    5/    5]    Loss 0.371987    Top1 81.870229    
2023-01-06 16:05:19,650 - ==> Top1: 81.870    Loss: 0.372

2023-01-06 16:05:19,650 - ==> Confusion:
[[355  74   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:05:19,651 - ==> Best [Top1: 82.824   Sparsity:0.00   Params: 46192 on epoch: 144]
2023-01-06 16:05:19,651 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:19,656 - 

2023-01-06 16:05:19,656 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:20,014 - Epoch: [146][   10/   37]    Overall Loss 0.421564    Objective Loss 0.421564                                        LR 0.000008    Time 0.035756    
2023-01-06 16:05:20,176 - Epoch: [146][   20/   37]    Overall Loss 0.416555    Objective Loss 0.416555                                        LR 0.000008    Time 0.025931    
2023-01-06 16:05:20,337 - Epoch: [146][   30/   37]    Overall Loss 0.416336    Objective Loss 0.416336                                        LR 0.000008    Time 0.022657    
2023-01-06 16:05:20,427 - Epoch: [146][   37/   37]    Overall Loss 0.418251    Objective Loss 0.418251    Top1 80.753138    LR 0.000008    Time 0.020804    
2023-01-06 16:05:20,503 - --- validate (epoch=146)-----------
2023-01-06 16:05:20,504 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:20,733 - Epoch: [146][    5/    5]    Loss 0.436397    Top1 82.919847    
2023-01-06 16:05:20,806 - ==> Top1: 82.920    Loss: 0.436

2023-01-06 16:05:20,806 - ==> Confusion:
[[342  87   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:05:20,807 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:20,807 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:20,812 - 

2023-01-06 16:05:20,812 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:21,187 - Epoch: [147][   10/   37]    Overall Loss 0.417718    Objective Loss 0.417718                                        LR 0.000008    Time 0.037407    
2023-01-06 16:05:21,343 - Epoch: [147][   20/   37]    Overall Loss 0.416718    Objective Loss 0.416718                                        LR 0.000008    Time 0.026462    
2023-01-06 16:05:21,499 - Epoch: [147][   30/   37]    Overall Loss 0.420442    Objective Loss 0.420442                                        LR 0.000008    Time 0.022851    
2023-01-06 16:05:21,586 - Epoch: [147][   37/   37]    Overall Loss 0.417963    Objective Loss 0.417963    Top1 80.753138    LR 0.000008    Time 0.020866    
2023-01-06 16:05:21,664 - --- validate (epoch=147)-----------
2023-01-06 16:05:21,664 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:21,890 - Epoch: [147][    5/    5]    Loss 0.401733    Top1 82.251908    
2023-01-06 16:05:21,955 - ==> Top1: 82.252    Loss: 0.402

2023-01-06 16:05:21,955 - ==> Confusion:
[[359  70   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:05:21,957 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:21,957 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:21,961 - 

2023-01-06 16:05:21,962 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:22,326 - Epoch: [148][   10/   37]    Overall Loss 0.414065    Objective Loss 0.414065                                        LR 0.000008    Time 0.036374    
2023-01-06 16:05:22,505 - Epoch: [148][   20/   37]    Overall Loss 0.413706    Objective Loss 0.413706                                        LR 0.000008    Time 0.027130    
2023-01-06 16:05:22,681 - Epoch: [148][   30/   37]    Overall Loss 0.412886    Objective Loss 0.412886                                        LR 0.000008    Time 0.023870    
2023-01-06 16:05:22,775 - Epoch: [148][   37/   37]    Overall Loss 0.418182    Objective Loss 0.418182    Top1 77.405858    LR 0.000008    Time 0.021902    
2023-01-06 16:05:22,848 - --- validate (epoch=148)-----------
2023-01-06 16:05:22,848 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:23,083 - Epoch: [148][    5/    5]    Loss 0.391895    Top1 82.824427    
2023-01-06 16:05:23,148 - ==> Top1: 82.824    Loss: 0.392

2023-01-06 16:05:23,149 - ==> Confusion:
[[318 111   0]
 [ 69 550   0]
 [  0   0   0]]

2023-01-06 16:05:23,150 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:23,150 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:23,154 - 

2023-01-06 16:05:23,154 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:23,533 - Epoch: [149][   10/   37]    Overall Loss 0.426415    Objective Loss 0.426415                                        LR 0.000008    Time 0.037825    
2023-01-06 16:05:23,700 - Epoch: [149][   20/   37]    Overall Loss 0.418556    Objective Loss 0.418556                                        LR 0.000008    Time 0.027223    
2023-01-06 16:05:23,862 - Epoch: [149][   30/   37]    Overall Loss 0.418366    Objective Loss 0.418366                                        LR 0.000008    Time 0.023532    
2023-01-06 16:05:23,960 - Epoch: [149][   37/   37]    Overall Loss 0.417119    Objective Loss 0.417119    Top1 81.171548    LR 0.000008    Time 0.021722    
2023-01-06 16:05:24,038 - --- validate (epoch=149)-----------
2023-01-06 16:05:24,038 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:24,267 - Epoch: [149][    5/    5]    Loss 0.386114    Top1 82.251908    
2023-01-06 16:05:24,347 - ==> Top1: 82.252    Loss: 0.386

2023-01-06 16:05:24,347 - ==> Confusion:
[[338  91   0]
 [ 95 524   0]
 [  0   0   0]]

2023-01-06 16:05:24,348 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:24,348 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:24,353 - 

2023-01-06 16:05:24,353 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:24,734 - Epoch: [150][   10/   37]    Overall Loss 0.414274    Objective Loss 0.414274                                        LR 0.000008    Time 0.038004    
2023-01-06 16:05:24,915 - Epoch: [150][   20/   37]    Overall Loss 0.416969    Objective Loss 0.416969                                        LR 0.000008    Time 0.027944    
2023-01-06 16:05:25,104 - Epoch: [150][   30/   37]    Overall Loss 0.412728    Objective Loss 0.412728                                        LR 0.000008    Time 0.024905    
2023-01-06 16:05:25,216 - Epoch: [150][   37/   37]    Overall Loss 0.417171    Objective Loss 0.417171    Top1 77.196653    LR 0.000008    Time 0.023221    
2023-01-06 16:05:25,294 - --- validate (epoch=150)-----------
2023-01-06 16:05:25,294 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:25,523 - Epoch: [150][    5/    5]    Loss 0.387825    Top1 82.347328    
2023-01-06 16:05:25,592 - ==> Top1: 82.347    Loss: 0.388

2023-01-06 16:05:25,592 - ==> Confusion:
[[355  74   0]
 [111 508   0]
 [  0   0   0]]

2023-01-06 16:05:25,593 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:25,593 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:25,598 - 

2023-01-06 16:05:25,598 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:25,957 - Epoch: [151][   10/   37]    Overall Loss 0.424129    Objective Loss 0.424129                                        LR 0.000008    Time 0.035782    
2023-01-06 16:05:26,100 - Epoch: [151][   20/   37]    Overall Loss 0.426919    Objective Loss 0.426919                                        LR 0.000008    Time 0.025040    
2023-01-06 16:05:26,247 - Epoch: [151][   30/   37]    Overall Loss 0.418092    Objective Loss 0.418092                                        LR 0.000008    Time 0.021571    
2023-01-06 16:05:26,341 - Epoch: [151][   37/   37]    Overall Loss 0.416613    Objective Loss 0.416613    Top1 78.451883    LR 0.000008    Time 0.020015    
2023-01-06 16:05:26,416 - --- validate (epoch=151)-----------
2023-01-06 16:05:26,416 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:26,645 - Epoch: [151][    5/    5]    Loss 0.399999    Top1 82.156489    
2023-01-06 16:05:26,717 - ==> Top1: 82.156    Loss: 0.400

2023-01-06 16:05:26,717 - ==> Confusion:
[[340  89   0]
 [ 98 521   0]
 [  0   0   0]]

2023-01-06 16:05:26,718 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:26,718 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:26,723 - 

2023-01-06 16:05:26,723 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:27,078 - Epoch: [152][   10/   37]    Overall Loss 0.416143    Objective Loss 0.416143                                        LR 0.000008    Time 0.035467    
2023-01-06 16:05:27,232 - Epoch: [152][   20/   37]    Overall Loss 0.415995    Objective Loss 0.415995                                        LR 0.000008    Time 0.025425    
2023-01-06 16:05:27,395 - Epoch: [152][   30/   37]    Overall Loss 0.417548    Objective Loss 0.417548                                        LR 0.000008    Time 0.022323    
2023-01-06 16:05:27,489 - Epoch: [152][   37/   37]    Overall Loss 0.415900    Objective Loss 0.415900    Top1 83.263598    LR 0.000008    Time 0.020623    
2023-01-06 16:05:27,559 - --- validate (epoch=152)-----------
2023-01-06 16:05:27,560 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:27,784 - Epoch: [152][    5/    5]    Loss 0.421011    Top1 81.870229    
2023-01-06 16:05:27,855 - ==> Top1: 81.870    Loss: 0.421

2023-01-06 16:05:27,856 - ==> Confusion:
[[365  64   0]
 [126 493   0]
 [  0   0   0]]

2023-01-06 16:05:27,857 - ==> Best [Top1: 82.920   Sparsity:0.00   Params: 46192 on epoch: 146]
2023-01-06 16:05:27,857 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:27,861 - 

2023-01-06 16:05:27,862 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:28,212 - Epoch: [153][   10/   37]    Overall Loss 0.418469    Objective Loss 0.418469                                        LR 0.000008    Time 0.034956    
2023-01-06 16:05:28,358 - Epoch: [153][   20/   37]    Overall Loss 0.413409    Objective Loss 0.413409                                        LR 0.000008    Time 0.024747    
2023-01-06 16:05:28,515 - Epoch: [153][   30/   37]    Overall Loss 0.414627    Objective Loss 0.414627                                        LR 0.000008    Time 0.021736    
2023-01-06 16:05:28,602 - Epoch: [153][   37/   37]    Overall Loss 0.416283    Objective Loss 0.416283    Top1 81.380753    LR 0.000008    Time 0.019955    
2023-01-06 16:05:28,669 - --- validate (epoch=153)-----------
2023-01-06 16:05:28,669 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:28,898 - Epoch: [153][    5/    5]    Loss 0.427126    Top1 83.110687    
2023-01-06 16:05:28,966 - ==> Top1: 83.111    Loss: 0.427

2023-01-06 16:05:28,966 - ==> Confusion:
[[359  70   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 16:05:28,968 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 153]
2023-01-06 16:05:28,968 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:28,973 - 

2023-01-06 16:05:28,973 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:29,333 - Epoch: [154][   10/   37]    Overall Loss 0.418524    Objective Loss 0.418524                                        LR 0.000008    Time 0.035977    
2023-01-06 16:05:29,473 - Epoch: [154][   20/   37]    Overall Loss 0.423350    Objective Loss 0.423350                                        LR 0.000008    Time 0.024971    
2023-01-06 16:05:29,616 - Epoch: [154][   30/   37]    Overall Loss 0.416443    Objective Loss 0.416443                                        LR 0.000008    Time 0.021398    
2023-01-06 16:05:29,703 - Epoch: [154][   37/   37]    Overall Loss 0.416120    Objective Loss 0.416120    Top1 82.635983    LR 0.000008    Time 0.019693    
2023-01-06 16:05:29,771 - --- validate (epoch=154)-----------
2023-01-06 16:05:29,772 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:29,999 - Epoch: [154][    5/    5]    Loss 0.421430    Top1 82.824427    
2023-01-06 16:05:30,066 - ==> Top1: 82.824    Loss: 0.421

2023-01-06 16:05:30,067 - ==> Confusion:
[[352  77   0]
 [103 516   0]
 [  0   0   0]]

2023-01-06 16:05:30,068 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 153]
2023-01-06 16:05:30,068 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:30,072 - 

2023-01-06 16:05:30,073 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:30,418 - Epoch: [155][   10/   37]    Overall Loss 0.424637    Objective Loss 0.424637                                        LR 0.000008    Time 0.034519    
2023-01-06 16:05:30,567 - Epoch: [155][   20/   37]    Overall Loss 0.416342    Objective Loss 0.416342                                        LR 0.000008    Time 0.024648    
2023-01-06 16:05:30,713 - Epoch: [155][   30/   37]    Overall Loss 0.413513    Objective Loss 0.413513                                        LR 0.000008    Time 0.021294    
2023-01-06 16:05:30,806 - Epoch: [155][   37/   37]    Overall Loss 0.415551    Objective Loss 0.415551    Top1 80.962343    LR 0.000008    Time 0.019786    
2023-01-06 16:05:30,887 - --- validate (epoch=155)-----------
2023-01-06 16:05:30,887 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:31,113 - Epoch: [155][    5/    5]    Loss 0.404605    Top1 83.110687    
2023-01-06 16:05:31,178 - ==> Top1: 83.111    Loss: 0.405

2023-01-06 16:05:31,178 - ==> Confusion:
[[354  75   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:05:31,179 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:31,179 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:31,187 - 

2023-01-06 16:05:31,187 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:31,554 - Epoch: [156][   10/   37]    Overall Loss 0.422983    Objective Loss 0.422983                                        LR 0.000008    Time 0.036625    
2023-01-06 16:05:31,713 - Epoch: [156][   20/   37]    Overall Loss 0.423405    Objective Loss 0.423405                                        LR 0.000008    Time 0.026250    
2023-01-06 16:05:31,889 - Epoch: [156][   30/   37]    Overall Loss 0.415845    Objective Loss 0.415845                                        LR 0.000008    Time 0.023355    
2023-01-06 16:05:31,985 - Epoch: [156][   37/   37]    Overall Loss 0.415094    Objective Loss 0.415094    Top1 80.334728    LR 0.000008    Time 0.021509    
2023-01-06 16:05:32,078 - --- validate (epoch=156)-----------
2023-01-06 16:05:32,078 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:32,304 - Epoch: [156][    5/    5]    Loss 0.398120    Top1 81.870229    
2023-01-06 16:05:32,368 - ==> Top1: 81.870    Loss: 0.398

2023-01-06 16:05:32,368 - ==> Confusion:
[[357  72   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 16:05:32,369 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:32,369 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:32,374 - 

2023-01-06 16:05:32,374 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:32,737 - Epoch: [157][   10/   37]    Overall Loss 0.398464    Objective Loss 0.398464                                        LR 0.000008    Time 0.036248    
2023-01-06 16:05:32,898 - Epoch: [157][   20/   37]    Overall Loss 0.408691    Objective Loss 0.408691                                        LR 0.000008    Time 0.026142    
2023-01-06 16:05:33,056 - Epoch: [157][   30/   37]    Overall Loss 0.412010    Objective Loss 0.412010                                        LR 0.000008    Time 0.022665    
2023-01-06 16:05:33,149 - Epoch: [157][   37/   37]    Overall Loss 0.414639    Objective Loss 0.414639    Top1 80.543933    LR 0.000008    Time 0.020890    
2023-01-06 16:05:33,227 - --- validate (epoch=157)-----------
2023-01-06 16:05:33,227 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:33,458 - Epoch: [157][    5/    5]    Loss 0.401858    Top1 81.870229    
2023-01-06 16:05:33,526 - ==> Top1: 81.870    Loss: 0.402

2023-01-06 16:05:33,526 - ==> Confusion:
[[358  71   0]
 [119 500   0]
 [  0   0   0]]

2023-01-06 16:05:33,528 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:33,528 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:33,532 - 

2023-01-06 16:05:33,532 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:33,883 - Epoch: [158][   10/   37]    Overall Loss 0.424679    Objective Loss 0.424679                                        LR 0.000008    Time 0.035018    
2023-01-06 16:05:34,024 - Epoch: [158][   20/   37]    Overall Loss 0.416681    Objective Loss 0.416681                                        LR 0.000008    Time 0.024521    
2023-01-06 16:05:34,180 - Epoch: [158][   30/   37]    Overall Loss 0.413786    Objective Loss 0.413786                                        LR 0.000008    Time 0.021525    
2023-01-06 16:05:34,273 - Epoch: [158][   37/   37]    Overall Loss 0.413770    Objective Loss 0.413770    Top1 79.079498    LR 0.000008    Time 0.019976    
2023-01-06 16:05:34,343 - --- validate (epoch=158)-----------
2023-01-06 16:05:34,343 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:34,571 - Epoch: [158][    5/    5]    Loss 0.444541    Top1 83.015267    
2023-01-06 16:05:34,634 - ==> Top1: 83.015    Loss: 0.445

2023-01-06 16:05:34,635 - ==> Confusion:
[[358  71   0]
 [107 512   0]
 [  0   0   0]]

2023-01-06 16:05:34,636 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:34,636 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:34,640 - 

2023-01-06 16:05:34,641 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:35,028 - Epoch: [159][   10/   37]    Overall Loss 0.413587    Objective Loss 0.413587                                        LR 0.000008    Time 0.038665    
2023-01-06 16:05:35,196 - Epoch: [159][   20/   37]    Overall Loss 0.412165    Objective Loss 0.412165                                        LR 0.000008    Time 0.027727    
2023-01-06 16:05:35,344 - Epoch: [159][   30/   37]    Overall Loss 0.414076    Objective Loss 0.414076                                        LR 0.000008    Time 0.023384    
2023-01-06 16:05:35,433 - Epoch: [159][   37/   37]    Overall Loss 0.414385    Objective Loss 0.414385    Top1 78.451883    LR 0.000008    Time 0.021376    
2023-01-06 16:05:35,502 - --- validate (epoch=159)-----------
2023-01-06 16:05:35,502 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:35,727 - Epoch: [159][    5/    5]    Loss 0.419059    Top1 82.824427    
2023-01-06 16:05:35,792 - ==> Top1: 82.824    Loss: 0.419

2023-01-06 16:05:35,793 - ==> Confusion:
[[352  77   0]
 [103 516   0]
 [  0   0   0]]

2023-01-06 16:05:35,794 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:35,794 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:35,798 - 

2023-01-06 16:05:35,799 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:36,161 - Epoch: [160][   10/   37]    Overall Loss 0.418527    Objective Loss 0.418527                                        LR 0.000008    Time 0.036135    
2023-01-06 16:05:36,329 - Epoch: [160][   20/   37]    Overall Loss 0.416622    Objective Loss 0.416622                                        LR 0.000008    Time 0.026467    
2023-01-06 16:05:36,494 - Epoch: [160][   30/   37]    Overall Loss 0.418496    Objective Loss 0.418496                                        LR 0.000008    Time 0.023127    
2023-01-06 16:05:36,590 - Epoch: [160][   37/   37]    Overall Loss 0.413458    Objective Loss 0.413458    Top1 81.380753    LR 0.000008    Time 0.021345    
2023-01-06 16:05:36,664 - --- validate (epoch=160)-----------
2023-01-06 16:05:36,664 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:37,146 - Epoch: [160][    5/    5]    Loss 0.424469    Top1 81.965649    
2023-01-06 16:05:37,215 - ==> Top1: 81.966    Loss: 0.424

2023-01-06 16:05:37,215 - ==> Confusion:
[[341  88   0]
 [101 518   0]
 [  0   0   0]]

2023-01-06 16:05:37,217 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:37,217 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:37,223 - 

2023-01-06 16:05:37,223 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:37,610 - Epoch: [161][   10/   37]    Overall Loss 0.405248    Objective Loss 0.405248                                        LR 0.000008    Time 0.038600    
2023-01-06 16:05:37,794 - Epoch: [161][   20/   37]    Overall Loss 0.416208    Objective Loss 0.416208                                        LR 0.000008    Time 0.028485    
2023-01-06 16:05:37,974 - Epoch: [161][   30/   37]    Overall Loss 0.415406    Objective Loss 0.415406                                        LR 0.000008    Time 0.024919    
2023-01-06 16:05:38,069 - Epoch: [161][   37/   37]    Overall Loss 0.413585    Objective Loss 0.413585    Top1 80.543933    LR 0.000008    Time 0.022756    
2023-01-06 16:05:38,147 - --- validate (epoch=161)-----------
2023-01-06 16:05:38,147 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:38,377 - Epoch: [161][    5/    5]    Loss 0.425709    Top1 82.538168    
2023-01-06 16:05:38,449 - ==> Top1: 82.538    Loss: 0.426

2023-01-06 16:05:38,450 - ==> Confusion:
[[338  91   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:05:38,451 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:38,451 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:38,456 - 

2023-01-06 16:05:38,456 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:38,816 - Epoch: [162][   10/   37]    Overall Loss 0.414166    Objective Loss 0.414166                                        LR 0.000008    Time 0.035951    
2023-01-06 16:05:38,977 - Epoch: [162][   20/   37]    Overall Loss 0.417034    Objective Loss 0.417034                                        LR 0.000008    Time 0.025999    
2023-01-06 16:05:39,148 - Epoch: [162][   30/   37]    Overall Loss 0.415410    Objective Loss 0.415410                                        LR 0.000008    Time 0.023018    
2023-01-06 16:05:39,244 - Epoch: [162][   37/   37]    Overall Loss 0.414109    Objective Loss 0.414109    Top1 80.753138    LR 0.000008    Time 0.021240    
2023-01-06 16:05:39,318 - --- validate (epoch=162)-----------
2023-01-06 16:05:39,318 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:39,547 - Epoch: [162][    5/    5]    Loss 0.399210    Top1 82.729008    
2023-01-06 16:05:39,612 - ==> Top1: 82.729    Loss: 0.399

2023-01-06 16:05:39,613 - ==> Confusion:
[[340  89   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:05:39,614 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:39,614 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:39,618 - 

2023-01-06 16:05:39,618 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:39,997 - Epoch: [163][   10/   37]    Overall Loss 0.422432    Objective Loss 0.422432                                        LR 0.000008    Time 0.037766    
2023-01-06 16:05:40,165 - Epoch: [163][   20/   37]    Overall Loss 0.414597    Objective Loss 0.414597                                        LR 0.000008    Time 0.027241    
2023-01-06 16:05:40,332 - Epoch: [163][   30/   37]    Overall Loss 0.412869    Objective Loss 0.412869                                        LR 0.000008    Time 0.023670    
2023-01-06 16:05:40,428 - Epoch: [163][   37/   37]    Overall Loss 0.414007    Objective Loss 0.414007    Top1 78.033473    LR 0.000008    Time 0.021771    
2023-01-06 16:05:40,506 - --- validate (epoch=163)-----------
2023-01-06 16:05:40,506 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:40,733 - Epoch: [163][    5/    5]    Loss 0.426618    Top1 82.347328    
2023-01-06 16:05:40,802 - ==> Top1: 82.347    Loss: 0.427

2023-01-06 16:05:40,803 - ==> Confusion:
[[360  69   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:05:40,804 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:40,804 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:40,809 - 

2023-01-06 16:05:40,809 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:41,182 - Epoch: [164][   10/   37]    Overall Loss 0.399708    Objective Loss 0.399708                                        LR 0.000008    Time 0.037237    
2023-01-06 16:05:41,350 - Epoch: [164][   20/   37]    Overall Loss 0.410843    Objective Loss 0.410843                                        LR 0.000008    Time 0.026979    
2023-01-06 16:05:41,514 - Epoch: [164][   30/   37]    Overall Loss 0.412668    Objective Loss 0.412668                                        LR 0.000008    Time 0.023441    
2023-01-06 16:05:41,613 - Epoch: [164][   37/   37]    Overall Loss 0.414286    Objective Loss 0.414286    Top1 80.334728    LR 0.000008    Time 0.021688    
2023-01-06 16:05:41,693 - --- validate (epoch=164)-----------
2023-01-06 16:05:41,693 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:41,922 - Epoch: [164][    5/    5]    Loss 0.398668    Top1 82.156489    
2023-01-06 16:05:41,989 - ==> Top1: 82.156    Loss: 0.399

2023-01-06 16:05:41,989 - ==> Confusion:
[[336  93   0]
 [ 94 525   0]
 [  0   0   0]]

2023-01-06 16:05:41,991 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:41,991 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:41,995 - 

2023-01-06 16:05:41,995 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:42,349 - Epoch: [165][   10/   37]    Overall Loss 0.407441    Objective Loss 0.407441                                        LR 0.000008    Time 0.035260    
2023-01-06 16:05:42,495 - Epoch: [165][   20/   37]    Overall Loss 0.405569    Objective Loss 0.405569                                        LR 0.000008    Time 0.024913    
2023-01-06 16:05:42,641 - Epoch: [165][   30/   37]    Overall Loss 0.412664    Objective Loss 0.412664                                        LR 0.000008    Time 0.021464    
2023-01-06 16:05:42,730 - Epoch: [165][   37/   37]    Overall Loss 0.412691    Objective Loss 0.412691    Top1 80.962343    LR 0.000008    Time 0.019806    
2023-01-06 16:05:42,809 - --- validate (epoch=165)-----------
2023-01-06 16:05:42,810 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:43,034 - Epoch: [165][    5/    5]    Loss 0.400727    Top1 81.297710    
2023-01-06 16:05:43,104 - ==> Top1: 81.298    Loss: 0.401

2023-01-06 16:05:43,104 - ==> Confusion:
[[365  64   0]
 [132 487   0]
 [  0   0   0]]

2023-01-06 16:05:43,105 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:43,105 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:43,110 - 

2023-01-06 16:05:43,110 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:43,472 - Epoch: [166][   10/   37]    Overall Loss 0.419302    Objective Loss 0.419302                                        LR 0.000008    Time 0.036111    
2023-01-06 16:05:43,620 - Epoch: [166][   20/   37]    Overall Loss 0.417820    Objective Loss 0.417820                                        LR 0.000008    Time 0.025438    
2023-01-06 16:05:43,770 - Epoch: [166][   30/   37]    Overall Loss 0.413779    Objective Loss 0.413779                                        LR 0.000008    Time 0.021945    
2023-01-06 16:05:43,861 - Epoch: [166][   37/   37]    Overall Loss 0.413195    Objective Loss 0.413195    Top1 83.054393    LR 0.000008    Time 0.020268    
2023-01-06 16:05:43,933 - --- validate (epoch=166)-----------
2023-01-06 16:05:43,933 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:44,160 - Epoch: [166][    5/    5]    Loss 0.412349    Top1 82.824427    
2023-01-06 16:05:44,234 - ==> Top1: 82.824    Loss: 0.412

2023-01-06 16:05:44,235 - ==> Confusion:
[[341  88   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-06 16:05:44,236 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:44,236 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:44,241 - 

2023-01-06 16:05:44,241 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:44,585 - Epoch: [167][   10/   37]    Overall Loss 0.404456    Objective Loss 0.404456                                        LR 0.000008    Time 0.034370    
2023-01-06 16:05:44,741 - Epoch: [167][   20/   37]    Overall Loss 0.404531    Objective Loss 0.404531                                        LR 0.000008    Time 0.024982    
2023-01-06 16:05:44,910 - Epoch: [167][   30/   37]    Overall Loss 0.408429    Objective Loss 0.408429                                        LR 0.000008    Time 0.022272    
2023-01-06 16:05:45,004 - Epoch: [167][   37/   37]    Overall Loss 0.412657    Objective Loss 0.412657    Top1 78.870293    LR 0.000008    Time 0.020576    
2023-01-06 16:05:45,082 - --- validate (epoch=167)-----------
2023-01-06 16:05:45,083 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:45,318 - Epoch: [167][    5/    5]    Loss 0.407252    Top1 82.061069    
2023-01-06 16:05:45,380 - ==> Top1: 82.061    Loss: 0.407

2023-01-06 16:05:45,380 - ==> Confusion:
[[318 111   0]
 [ 77 542   0]
 [  0   0   0]]

2023-01-06 16:05:45,382 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:45,382 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:45,386 - 

2023-01-06 16:05:45,386 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:45,733 - Epoch: [168][   10/   37]    Overall Loss 0.415483    Objective Loss 0.415483                                        LR 0.000008    Time 0.034560    
2023-01-06 16:05:45,877 - Epoch: [168][   20/   37]    Overall Loss 0.411283    Objective Loss 0.411283                                        LR 0.000008    Time 0.024451    
2023-01-06 16:05:46,042 - Epoch: [168][   30/   37]    Overall Loss 0.413303    Objective Loss 0.413303                                        LR 0.000008    Time 0.021808    
2023-01-06 16:05:46,138 - Epoch: [168][   37/   37]    Overall Loss 0.412701    Objective Loss 0.412701    Top1 82.217573    LR 0.000008    Time 0.020270    
2023-01-06 16:05:46,224 - --- validate (epoch=168)-----------
2023-01-06 16:05:46,224 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:46,457 - Epoch: [168][    5/    5]    Loss 0.392695    Top1 82.633588    
2023-01-06 16:05:46,533 - ==> Top1: 82.634    Loss: 0.393

2023-01-06 16:05:46,533 - ==> Confusion:
[[350  79   0]
 [103 516   0]
 [  0   0   0]]

2023-01-06 16:05:46,534 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:46,534 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:46,539 - 

2023-01-06 16:05:46,539 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:46,883 - Epoch: [169][   10/   37]    Overall Loss 0.415501    Objective Loss 0.415501                                        LR 0.000008    Time 0.034299    
2023-01-06 16:05:47,026 - Epoch: [169][   20/   37]    Overall Loss 0.412458    Objective Loss 0.412458                                        LR 0.000008    Time 0.024282    
2023-01-06 16:05:47,176 - Epoch: [169][   30/   37]    Overall Loss 0.413424    Objective Loss 0.413424                                        LR 0.000008    Time 0.021172    
2023-01-06 16:05:47,263 - Epoch: [169][   37/   37]    Overall Loss 0.410941    Objective Loss 0.410941    Top1 83.472803    LR 0.000008    Time 0.019530    
2023-01-06 16:05:47,335 - --- validate (epoch=169)-----------
2023-01-06 16:05:47,336 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:47,562 - Epoch: [169][    5/    5]    Loss 0.425739    Top1 82.156489    
2023-01-06 16:05:47,635 - ==> Top1: 82.156    Loss: 0.426

2023-01-06 16:05:47,635 - ==> Confusion:
[[347  82   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 16:05:47,636 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:47,636 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:47,641 - 

2023-01-06 16:05:47,641 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:47,989 - Epoch: [170][   10/   37]    Overall Loss 0.418313    Objective Loss 0.418313                                        LR 0.000008    Time 0.034717    
2023-01-06 16:05:48,121 - Epoch: [170][   20/   37]    Overall Loss 0.422462    Objective Loss 0.422462                                        LR 0.000008    Time 0.023958    
2023-01-06 16:05:48,257 - Epoch: [170][   30/   37]    Overall Loss 0.414057    Objective Loss 0.414057                                        LR 0.000008    Time 0.020465    
2023-01-06 16:05:48,341 - Epoch: [170][   37/   37]    Overall Loss 0.411204    Objective Loss 0.411204    Top1 81.589958    LR 0.000008    Time 0.018858    
2023-01-06 16:05:48,423 - --- validate (epoch=170)-----------
2023-01-06 16:05:48,424 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:48,655 - Epoch: [170][    5/    5]    Loss 0.383577    Top1 82.633588    
2023-01-06 16:05:48,716 - ==> Top1: 82.634    Loss: 0.384

2023-01-06 16:05:48,717 - ==> Confusion:
[[333  96   0]
 [ 86 533   0]
 [  0   0   0]]

2023-01-06 16:05:48,718 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:48,718 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:48,723 - 

2023-01-06 16:05:48,723 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:49,074 - Epoch: [171][   10/   37]    Overall Loss 0.397153    Objective Loss 0.397153                                        LR 0.000008    Time 0.035034    
2023-01-06 16:05:49,227 - Epoch: [171][   20/   37]    Overall Loss 0.406528    Objective Loss 0.406528                                        LR 0.000008    Time 0.025139    
2023-01-06 16:05:49,380 - Epoch: [171][   30/   37]    Overall Loss 0.410831    Objective Loss 0.410831                                        LR 0.000008    Time 0.021847    
2023-01-06 16:05:49,465 - Epoch: [171][   37/   37]    Overall Loss 0.409925    Objective Loss 0.409925    Top1 82.426778    LR 0.000008    Time 0.020003    
2023-01-06 16:05:49,541 - --- validate (epoch=171)-----------
2023-01-06 16:05:49,541 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:49,770 - Epoch: [171][    5/    5]    Loss 0.374435    Top1 82.061069    
2023-01-06 16:05:49,848 - ==> Top1: 82.061    Loss: 0.374

2023-01-06 16:05:49,848 - ==> Confusion:
[[358  71   0]
 [117 502   0]
 [  0   0   0]]

2023-01-06 16:05:49,850 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:49,850 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:49,854 - 

2023-01-06 16:05:49,855 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:50,207 - Epoch: [172][   10/   37]    Overall Loss 0.414354    Objective Loss 0.414354                                        LR 0.000008    Time 0.035208    
2023-01-06 16:05:50,355 - Epoch: [172][   20/   37]    Overall Loss 0.410473    Objective Loss 0.410473                                        LR 0.000008    Time 0.024976    
2023-01-06 16:05:50,506 - Epoch: [172][   30/   37]    Overall Loss 0.411071    Objective Loss 0.411071                                        LR 0.000008    Time 0.021650    
2023-01-06 16:05:50,592 - Epoch: [172][   37/   37]    Overall Loss 0.410849    Objective Loss 0.410849    Top1 82.008368    LR 0.000008    Time 0.019879    
2023-01-06 16:05:50,662 - --- validate (epoch=172)-----------
2023-01-06 16:05:50,662 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:50,893 - Epoch: [172][    5/    5]    Loss 0.410776    Top1 82.251908    
2023-01-06 16:05:50,977 - ==> Top1: 82.252    Loss: 0.411

2023-01-06 16:05:50,978 - ==> Confusion:
[[305 124   0]
 [ 62 557   0]
 [  0   0   0]]

2023-01-06 16:05:50,979 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:50,979 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:50,983 - 

2023-01-06 16:05:50,984 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:51,331 - Epoch: [173][   10/   37]    Overall Loss 0.412455    Objective Loss 0.412455                                        LR 0.000008    Time 0.034709    
2023-01-06 16:05:51,472 - Epoch: [173][   20/   37]    Overall Loss 0.412574    Objective Loss 0.412574                                        LR 0.000008    Time 0.024341    
2023-01-06 16:05:51,619 - Epoch: [173][   30/   37]    Overall Loss 0.410917    Objective Loss 0.410917                                        LR 0.000008    Time 0.021119    
2023-01-06 16:05:51,703 - Epoch: [173][   37/   37]    Overall Loss 0.410348    Objective Loss 0.410348    Top1 78.870293    LR 0.000008    Time 0.019406    
2023-01-06 16:05:51,782 - --- validate (epoch=173)-----------
2023-01-06 16:05:51,782 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:52,008 - Epoch: [173][    5/    5]    Loss 0.382420    Top1 82.061069    
2023-01-06 16:05:52,076 - ==> Top1: 82.061    Loss: 0.382

2023-01-06 16:05:52,076 - ==> Confusion:
[[357  72   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:05:52,077 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:52,077 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:52,082 - 

2023-01-06 16:05:52,082 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:52,463 - Epoch: [174][   10/   37]    Overall Loss 0.411306    Objective Loss 0.411306                                        LR 0.000008    Time 0.038032    
2023-01-06 16:05:52,632 - Epoch: [174][   20/   37]    Overall Loss 0.411066    Objective Loss 0.411066                                        LR 0.000008    Time 0.027461    
2023-01-06 16:05:52,805 - Epoch: [174][   30/   37]    Overall Loss 0.410294    Objective Loss 0.410294                                        LR 0.000008    Time 0.024065    
2023-01-06 16:05:52,902 - Epoch: [174][   37/   37]    Overall Loss 0.409567    Objective Loss 0.409567    Top1 82.217573    LR 0.000008    Time 0.022122    
2023-01-06 16:05:52,986 - --- validate (epoch=174)-----------
2023-01-06 16:05:52,986 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:53,215 - Epoch: [174][    5/    5]    Loss 0.444867    Top1 82.061069    
2023-01-06 16:05:53,284 - ==> Top1: 82.061    Loss: 0.445

2023-01-06 16:05:53,284 - ==> Confusion:
[[358  71   0]
 [117 502   0]
 [  0   0   0]]

2023-01-06 16:05:53,285 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:53,286 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:53,290 - 

2023-01-06 16:05:53,290 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:53,655 - Epoch: [175][   10/   37]    Overall Loss 0.409998    Objective Loss 0.409998                                        LR 0.000008    Time 0.036388    
2023-01-06 16:05:53,801 - Epoch: [175][   20/   37]    Overall Loss 0.411687    Objective Loss 0.411687                                        LR 0.000008    Time 0.025475    
2023-01-06 16:05:53,948 - Epoch: [175][   30/   37]    Overall Loss 0.407854    Objective Loss 0.407854                                        LR 0.000008    Time 0.021866    
2023-01-06 16:05:54,032 - Epoch: [175][   37/   37]    Overall Loss 0.410072    Objective Loss 0.410072    Top1 80.334728    LR 0.000008    Time 0.019992    
2023-01-06 16:05:54,101 - --- validate (epoch=175)-----------
2023-01-06 16:05:54,101 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:54,332 - Epoch: [175][    5/    5]    Loss 0.382263    Top1 82.251908    
2023-01-06 16:05:54,398 - ==> Top1: 82.252    Loss: 0.382

2023-01-06 16:05:54,399 - ==> Confusion:
[[348  81   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 16:05:54,400 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:54,400 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:54,406 - 

2023-01-06 16:05:54,406 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:54,907 - Epoch: [176][   10/   37]    Overall Loss 0.408290    Objective Loss 0.408290                                        LR 0.000008    Time 0.050001    
2023-01-06 16:05:55,077 - Epoch: [176][   20/   37]    Overall Loss 0.413323    Objective Loss 0.413323                                        LR 0.000008    Time 0.033512    
2023-01-06 16:05:55,247 - Epoch: [176][   30/   37]    Overall Loss 0.410994    Objective Loss 0.410994                                        LR 0.000008    Time 0.027944    
2023-01-06 16:05:55,342 - Epoch: [176][   37/   37]    Overall Loss 0.410623    Objective Loss 0.410623    Top1 79.916318    LR 0.000008    Time 0.025236    
2023-01-06 16:05:55,412 - --- validate (epoch=176)-----------
2023-01-06 16:05:55,412 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:55,646 - Epoch: [176][    5/    5]    Loss 0.384865    Top1 81.679389    
2023-01-06 16:05:55,706 - ==> Top1: 81.679    Loss: 0.385

2023-01-06 16:05:55,706 - ==> Confusion:
[[358  71   0]
 [121 498   0]
 [  0   0   0]]

2023-01-06 16:05:55,707 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:55,707 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:55,712 - 

2023-01-06 16:05:55,712 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:56,075 - Epoch: [177][   10/   37]    Overall Loss 0.403781    Objective Loss 0.403781                                        LR 0.000008    Time 0.036268    
2023-01-06 16:05:56,238 - Epoch: [177][   20/   37]    Overall Loss 0.409743    Objective Loss 0.409743                                        LR 0.000008    Time 0.026263    
2023-01-06 16:05:56,399 - Epoch: [177][   30/   37]    Overall Loss 0.409281    Objective Loss 0.409281                                        LR 0.000008    Time 0.022853    
2023-01-06 16:05:56,497 - Epoch: [177][   37/   37]    Overall Loss 0.409206    Objective Loss 0.409206    Top1 82.635983    LR 0.000008    Time 0.021161    
2023-01-06 16:05:56,566 - --- validate (epoch=177)-----------
2023-01-06 16:05:56,566 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:56,799 - Epoch: [177][    5/    5]    Loss 0.417237    Top1 82.061069    
2023-01-06 16:05:56,879 - ==> Top1: 82.061    Loss: 0.417

2023-01-06 16:05:56,879 - ==> Confusion:
[[359  70   0]
 [118 501   0]
 [  0   0   0]]

2023-01-06 16:05:56,881 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:56,881 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:56,885 - 

2023-01-06 16:05:56,885 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:57,238 - Epoch: [178][   10/   37]    Overall Loss 0.417155    Objective Loss 0.417155                                        LR 0.000008    Time 0.035202    
2023-01-06 16:05:57,390 - Epoch: [178][   20/   37]    Overall Loss 0.411653    Objective Loss 0.411653                                        LR 0.000008    Time 0.025145    
2023-01-06 16:05:57,543 - Epoch: [178][   30/   37]    Overall Loss 0.411003    Objective Loss 0.411003                                        LR 0.000008    Time 0.021862    
2023-01-06 16:05:57,631 - Epoch: [178][   37/   37]    Overall Loss 0.408709    Objective Loss 0.408709    Top1 80.962343    LR 0.000008    Time 0.020097    
2023-01-06 16:05:57,703 - --- validate (epoch=178)-----------
2023-01-06 16:05:57,704 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:57,935 - Epoch: [178][    5/    5]    Loss 0.383291    Top1 82.538168    
2023-01-06 16:05:58,006 - ==> Top1: 82.538    Loss: 0.383

2023-01-06 16:05:58,006 - ==> Confusion:
[[345  84   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:05:58,008 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:58,008 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:58,012 - 

2023-01-06 16:05:58,012 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:58,360 - Epoch: [179][   10/   37]    Overall Loss 0.399291    Objective Loss 0.399291                                        LR 0.000008    Time 0.034742    
2023-01-06 16:05:58,506 - Epoch: [179][   20/   37]    Overall Loss 0.404926    Objective Loss 0.404926                                        LR 0.000008    Time 0.024651    
2023-01-06 16:05:58,661 - Epoch: [179][   30/   37]    Overall Loss 0.403999    Objective Loss 0.403999                                        LR 0.000008    Time 0.021564    
2023-01-06 16:05:58,744 - Epoch: [179][   37/   37]    Overall Loss 0.408895    Objective Loss 0.408895    Top1 82.217573    LR 0.000008    Time 0.019738    
2023-01-06 16:05:58,814 - --- validate (epoch=179)-----------
2023-01-06 16:05:58,814 - 1048 samples (256 per mini-batch)
2023-01-06 16:05:59,039 - Epoch: [179][    5/    5]    Loss 0.407806    Top1 82.919847    
2023-01-06 16:05:59,112 - ==> Top1: 82.920    Loss: 0.408

2023-01-06 16:05:59,112 - ==> Confusion:
[[354  75   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 16:05:59,114 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:05:59,114 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:05:59,118 - 

2023-01-06 16:05:59,118 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:05:59,490 - Epoch: [180][   10/   37]    Overall Loss 0.417988    Objective Loss 0.417988                                        LR 0.000005    Time 0.037141    
2023-01-06 16:05:59,644 - Epoch: [180][   20/   37]    Overall Loss 0.414353    Objective Loss 0.414353                                        LR 0.000005    Time 0.026198    
2023-01-06 16:05:59,802 - Epoch: [180][   30/   37]    Overall Loss 0.408189    Objective Loss 0.408189                                        LR 0.000005    Time 0.022728    
2023-01-06 16:05:59,895 - Epoch: [180][   37/   37]    Overall Loss 0.409090    Objective Loss 0.409090    Top1 83.472803    LR 0.000005    Time 0.020935    
2023-01-06 16:05:59,970 - --- validate (epoch=180)-----------
2023-01-06 16:05:59,971 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:00,211 - Epoch: [180][    5/    5]    Loss 0.399376    Top1 81.202290    
2023-01-06 16:06:00,274 - ==> Top1: 81.202    Loss: 0.399

2023-01-06 16:06:00,274 - ==> Confusion:
[[367  62   0]
 [135 484   0]
 [  0   0   0]]

2023-01-06 16:06:00,275 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:06:00,275 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:00,280 - 

2023-01-06 16:06:00,280 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:00,647 - Epoch: [181][   10/   37]    Overall Loss 0.418940    Objective Loss 0.418940                                        LR 0.000005    Time 0.036642    
2023-01-06 16:06:00,813 - Epoch: [181][   20/   37]    Overall Loss 0.414542    Objective Loss 0.414542                                        LR 0.000005    Time 0.026591    
2023-01-06 16:06:00,988 - Epoch: [181][   30/   37]    Overall Loss 0.408484    Objective Loss 0.408484                                        LR 0.000005    Time 0.023548    
2023-01-06 16:06:01,084 - Epoch: [181][   37/   37]    Overall Loss 0.408573    Objective Loss 0.408573    Top1 82.635983    LR 0.000005    Time 0.021690    
2023-01-06 16:06:01,150 - --- validate (epoch=181)-----------
2023-01-06 16:06:01,150 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:01,382 - Epoch: [181][    5/    5]    Loss 0.388291    Top1 82.633588    
2023-01-06 16:06:01,444 - ==> Top1: 82.634    Loss: 0.388

2023-01-06 16:06:01,445 - ==> Confusion:
[[343  86   0]
 [ 96 523   0]
 [  0   0   0]]

2023-01-06 16:06:01,446 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:06:01,446 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:01,451 - 

2023-01-06 16:06:01,451 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:01,819 - Epoch: [182][   10/   37]    Overall Loss 0.399887    Objective Loss 0.399887                                        LR 0.000005    Time 0.036704    
2023-01-06 16:06:01,969 - Epoch: [182][   20/   37]    Overall Loss 0.404060    Objective Loss 0.404060                                        LR 0.000005    Time 0.025804    
2023-01-06 16:06:02,121 - Epoch: [182][   30/   37]    Overall Loss 0.406187    Objective Loss 0.406187                                        LR 0.000005    Time 0.022248    
2023-01-06 16:06:02,217 - Epoch: [182][   37/   37]    Overall Loss 0.408291    Objective Loss 0.408291    Top1 79.707113    LR 0.000005    Time 0.020636    
2023-01-06 16:06:02,291 - --- validate (epoch=182)-----------
2023-01-06 16:06:02,292 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:02,526 - Epoch: [182][    5/    5]    Loss 0.412662    Top1 82.729008    
2023-01-06 16:06:02,588 - ==> Top1: 82.729    Loss: 0.413

2023-01-06 16:06:02,588 - ==> Confusion:
[[362  67   0]
 [114 505   0]
 [  0   0   0]]

2023-01-06 16:06:02,589 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:06:02,589 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:02,594 - 

2023-01-06 16:06:02,594 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:02,947 - Epoch: [183][   10/   37]    Overall Loss 0.414967    Objective Loss 0.414967                                        LR 0.000005    Time 0.035216    
2023-01-06 16:06:03,110 - Epoch: [183][   20/   37]    Overall Loss 0.407096    Objective Loss 0.407096                                        LR 0.000005    Time 0.025718    
2023-01-06 16:06:03,274 - Epoch: [183][   30/   37]    Overall Loss 0.408959    Objective Loss 0.408959                                        LR 0.000005    Time 0.022616    
2023-01-06 16:06:03,369 - Epoch: [183][   37/   37]    Overall Loss 0.407973    Objective Loss 0.407973    Top1 83.472803    LR 0.000005    Time 0.020909    
2023-01-06 16:06:03,445 - --- validate (epoch=183)-----------
2023-01-06 16:06:03,445 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:03,670 - Epoch: [183][    5/    5]    Loss 0.421456    Top1 82.347328    
2023-01-06 16:06:03,745 - ==> Top1: 82.347    Loss: 0.421

2023-01-06 16:06:03,745 - ==> Confusion:
[[357  72   0]
 [113 506   0]
 [  0   0   0]]

2023-01-06 16:06:03,747 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 155]
2023-01-06 16:06:03,747 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:03,751 - 

2023-01-06 16:06:03,751 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:04,123 - Epoch: [184][   10/   37]    Overall Loss 0.420187    Objective Loss 0.420187                                        LR 0.000005    Time 0.037087    
2023-01-06 16:06:04,296 - Epoch: [184][   20/   37]    Overall Loss 0.410483    Objective Loss 0.410483                                        LR 0.000005    Time 0.027175    
2023-01-06 16:06:04,463 - Epoch: [184][   30/   37]    Overall Loss 0.411124    Objective Loss 0.411124                                        LR 0.000005    Time 0.023622    
2023-01-06 16:06:04,559 - Epoch: [184][   37/   37]    Overall Loss 0.407526    Objective Loss 0.407526    Top1 81.171548    LR 0.000005    Time 0.021761    
2023-01-06 16:06:04,633 - --- validate (epoch=184)-----------
2023-01-06 16:06:04,633 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:04,865 - Epoch: [184][    5/    5]    Loss 0.395375    Top1 83.110687    
2023-01-06 16:06:04,937 - ==> Top1: 83.111    Loss: 0.395

2023-01-06 16:06:04,938 - ==> Confusion:
[[357  72   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 16:06:04,939 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 184]
2023-01-06 16:06:04,939 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:04,944 - 

2023-01-06 16:06:04,944 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:05,297 - Epoch: [185][   10/   37]    Overall Loss 0.415184    Objective Loss 0.415184                                        LR 0.000005    Time 0.035177    
2023-01-06 16:06:05,439 - Epoch: [185][   20/   37]    Overall Loss 0.411380    Objective Loss 0.411380                                        LR 0.000005    Time 0.024674    
2023-01-06 16:06:05,593 - Epoch: [185][   30/   37]    Overall Loss 0.407769    Objective Loss 0.407769                                        LR 0.000005    Time 0.021589    
2023-01-06 16:06:05,683 - Epoch: [185][   37/   37]    Overall Loss 0.407115    Objective Loss 0.407115    Top1 78.661088    LR 0.000005    Time 0.019912    
2023-01-06 16:06:05,763 - --- validate (epoch=185)-----------
2023-01-06 16:06:05,763 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:05,993 - Epoch: [185][    5/    5]    Loss 0.404393    Top1 82.729008    
2023-01-06 16:06:06,050 - ==> Top1: 82.729    Loss: 0.404

2023-01-06 16:06:06,050 - ==> Confusion:
[[360  69   0]
 [112 507   0]
 [  0   0   0]]

2023-01-06 16:06:06,051 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 184]
2023-01-06 16:06:06,051 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:06,056 - 

2023-01-06 16:06:06,056 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:06,427 - Epoch: [186][   10/   37]    Overall Loss 0.410174    Objective Loss 0.410174                                        LR 0.000005    Time 0.037068    
2023-01-06 16:06:06,585 - Epoch: [186][   20/   37]    Overall Loss 0.413926    Objective Loss 0.413926                                        LR 0.000005    Time 0.026412    
2023-01-06 16:06:06,753 - Epoch: [186][   30/   37]    Overall Loss 0.408998    Objective Loss 0.408998                                        LR 0.000005    Time 0.023169    
2023-01-06 16:06:06,846 - Epoch: [186][   37/   37]    Overall Loss 0.406837    Objective Loss 0.406837    Top1 82.845188    LR 0.000005    Time 0.021295    
2023-01-06 16:06:06,921 - --- validate (epoch=186)-----------
2023-01-06 16:06:06,921 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:07,155 - Epoch: [186][    5/    5]    Loss 0.413436    Top1 83.015267    
2023-01-06 16:06:07,222 - ==> Top1: 83.015    Loss: 0.413

2023-01-06 16:06:07,222 - ==> Confusion:
[[350  79   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:06:07,223 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 184]
2023-01-06 16:06:07,223 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:07,228 - 

2023-01-06 16:06:07,228 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:07,580 - Epoch: [187][   10/   37]    Overall Loss 0.411986    Objective Loss 0.411986                                        LR 0.000005    Time 0.035167    
2023-01-06 16:06:07,724 - Epoch: [187][   20/   37]    Overall Loss 0.404900    Objective Loss 0.404900                                        LR 0.000005    Time 0.024736    
2023-01-06 16:06:07,864 - Epoch: [187][   30/   37]    Overall Loss 0.405475    Objective Loss 0.405475                                        LR 0.000005    Time 0.021155    
2023-01-06 16:06:07,950 - Epoch: [187][   37/   37]    Overall Loss 0.406799    Objective Loss 0.406799    Top1 82.008368    LR 0.000005    Time 0.019463    
2023-01-06 16:06:08,022 - --- validate (epoch=187)-----------
2023-01-06 16:06:08,023 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:08,260 - Epoch: [187][    5/    5]    Loss 0.389448    Top1 83.110687    
2023-01-06 16:06:08,324 - ==> Top1: 83.111    Loss: 0.389

2023-01-06 16:06:08,324 - ==> Confusion:
[[354  75   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:06:08,326 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:08,326 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:08,331 - 

2023-01-06 16:06:08,331 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:08,699 - Epoch: [188][   10/   37]    Overall Loss 0.403823    Objective Loss 0.403823                                        LR 0.000005    Time 0.036740    
2023-01-06 16:06:08,860 - Epoch: [188][   20/   37]    Overall Loss 0.405996    Objective Loss 0.405996                                        LR 0.000005    Time 0.026393    
2023-01-06 16:06:09,021 - Epoch: [188][   30/   37]    Overall Loss 0.406278    Objective Loss 0.406278                                        LR 0.000005    Time 0.022958    
2023-01-06 16:06:09,109 - Epoch: [188][   37/   37]    Overall Loss 0.406860    Objective Loss 0.406860    Top1 79.707113    LR 0.000005    Time 0.020974    
2023-01-06 16:06:09,186 - --- validate (epoch=188)-----------
2023-01-06 16:06:09,186 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:09,415 - Epoch: [188][    5/    5]    Loss 0.408245    Top1 82.919847    
2023-01-06 16:06:09,497 - ==> Top1: 82.920    Loss: 0.408

2023-01-06 16:06:09,497 - ==> Confusion:
[[352  77   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:06:09,499 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:09,499 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:09,503 - 

2023-01-06 16:06:09,503 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:09,888 - Epoch: [189][   10/   37]    Overall Loss 0.412882    Objective Loss 0.412882                                        LR 0.000005    Time 0.038400    
2023-01-06 16:06:10,064 - Epoch: [189][   20/   37]    Overall Loss 0.407831    Objective Loss 0.407831                                        LR 0.000005    Time 0.027963    
2023-01-06 16:06:10,239 - Epoch: [189][   30/   37]    Overall Loss 0.406001    Objective Loss 0.406001                                        LR 0.000005    Time 0.024485    
2023-01-06 16:06:10,337 - Epoch: [189][   37/   37]    Overall Loss 0.407097    Objective Loss 0.407097    Top1 79.916318    LR 0.000005    Time 0.022479    
2023-01-06 16:06:10,415 - --- validate (epoch=189)-----------
2023-01-06 16:06:10,415 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:10,646 - Epoch: [189][    5/    5]    Loss 0.425639    Top1 82.729008    
2023-01-06 16:06:10,711 - ==> Top1: 82.729    Loss: 0.426

2023-01-06 16:06:10,711 - ==> Confusion:
[[336  93   0]
 [ 88 531   0]
 [  0   0   0]]

2023-01-06 16:06:10,713 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:10,713 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:10,717 - 

2023-01-06 16:06:10,717 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:11,075 - Epoch: [190][   10/   37]    Overall Loss 0.410751    Objective Loss 0.410751                                        LR 0.000003    Time 0.035700    
2023-01-06 16:06:11,220 - Epoch: [190][   20/   37]    Overall Loss 0.412166    Objective Loss 0.412166                                        LR 0.000003    Time 0.025053    
2023-01-06 16:06:11,364 - Epoch: [190][   30/   37]    Overall Loss 0.411790    Objective Loss 0.411790                                        LR 0.000003    Time 0.021499    
2023-01-06 16:06:11,452 - Epoch: [190][   37/   37]    Overall Loss 0.405924    Objective Loss 0.405924    Top1 83.891213    LR 0.000003    Time 0.019808    
2023-01-06 16:06:11,530 - --- validate (epoch=190)-----------
2023-01-06 16:06:11,530 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:11,761 - Epoch: [190][    5/    5]    Loss 0.395629    Top1 82.633588    
2023-01-06 16:06:11,836 - ==> Top1: 82.634    Loss: 0.396

2023-01-06 16:06:11,836 - ==> Confusion:
[[352  77   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 16:06:11,838 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:11,838 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:11,842 - 

2023-01-06 16:06:11,842 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:12,321 - Epoch: [191][   10/   37]    Overall Loss 0.414149    Objective Loss 0.414149                                        LR 0.000003    Time 0.047772    
2023-01-06 16:06:12,465 - Epoch: [191][   20/   37]    Overall Loss 0.405301    Objective Loss 0.405301                                        LR 0.000003    Time 0.031076    
2023-01-06 16:06:12,605 - Epoch: [191][   30/   37]    Overall Loss 0.402879    Objective Loss 0.402879                                        LR 0.000003    Time 0.025365    
2023-01-06 16:06:12,688 - Epoch: [191][   37/   37]    Overall Loss 0.406293    Objective Loss 0.406293    Top1 78.451883    LR 0.000003    Time 0.022817    
2023-01-06 16:06:12,767 - --- validate (epoch=191)-----------
2023-01-06 16:06:12,768 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:12,997 - Epoch: [191][    5/    5]    Loss 0.407430    Top1 82.919847    
2023-01-06 16:06:13,057 - ==> Top1: 82.920    Loss: 0.407

2023-01-06 16:06:13,057 - ==> Confusion:
[[349  80   0]
 [ 99 520   0]
 [  0   0   0]]

2023-01-06 16:06:13,059 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:13,059 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:13,063 - 

2023-01-06 16:06:13,063 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:13,419 - Epoch: [192][   10/   37]    Overall Loss 0.405803    Objective Loss 0.405803                                        LR 0.000003    Time 0.035462    
2023-01-06 16:06:13,565 - Epoch: [192][   20/   37]    Overall Loss 0.405597    Objective Loss 0.405597                                        LR 0.000003    Time 0.025018    
2023-01-06 16:06:13,706 - Epoch: [192][   30/   37]    Overall Loss 0.406333    Objective Loss 0.406333                                        LR 0.000003    Time 0.021373    
2023-01-06 16:06:13,792 - Epoch: [192][   37/   37]    Overall Loss 0.405601    Objective Loss 0.405601    Top1 83.472803    LR 0.000003    Time 0.019654    
2023-01-06 16:06:13,871 - --- validate (epoch=192)-----------
2023-01-06 16:06:13,871 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:14,100 - Epoch: [192][    5/    5]    Loss 0.373715    Top1 82.633588    
2023-01-06 16:06:14,178 - ==> Top1: 82.634    Loss: 0.374

2023-01-06 16:06:14,178 - ==> Confusion:
[[352  77   0]
 [105 514   0]
 [  0   0   0]]

2023-01-06 16:06:14,179 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:14,179 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:14,184 - 

2023-01-06 16:06:14,184 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:14,539 - Epoch: [193][   10/   37]    Overall Loss 0.404554    Objective Loss 0.404554                                        LR 0.000003    Time 0.035460    
2023-01-06 16:06:14,710 - Epoch: [193][   20/   37]    Overall Loss 0.407921    Objective Loss 0.407921                                        LR 0.000003    Time 0.026258    
2023-01-06 16:06:14,878 - Epoch: [193][   30/   37]    Overall Loss 0.404225    Objective Loss 0.404225                                        LR 0.000003    Time 0.023103    
2023-01-06 16:06:14,966 - Epoch: [193][   37/   37]    Overall Loss 0.405317    Objective Loss 0.405317    Top1 80.753138    LR 0.000003    Time 0.021099    
2023-01-06 16:06:15,049 - --- validate (epoch=193)-----------
2023-01-06 16:06:15,050 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:15,284 - Epoch: [193][    5/    5]    Loss 0.412278    Top1 83.015267    
2023-01-06 16:06:15,354 - ==> Top1: 83.015    Loss: 0.412

2023-01-06 16:06:15,354 - ==> Confusion:
[[351  78   0]
 [100 519   0]
 [  0   0   0]]

2023-01-06 16:06:15,356 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:15,356 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:15,360 - 

2023-01-06 16:06:15,360 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:15,718 - Epoch: [194][   10/   37]    Overall Loss 0.397397    Objective Loss 0.397397                                        LR 0.000003    Time 0.035751    
2023-01-06 16:06:15,866 - Epoch: [194][   20/   37]    Overall Loss 0.399730    Objective Loss 0.399730                                        LR 0.000003    Time 0.025226    
2023-01-06 16:06:16,017 - Epoch: [194][   30/   37]    Overall Loss 0.401976    Objective Loss 0.401976                                        LR 0.000003    Time 0.021848    
2023-01-06 16:06:16,111 - Epoch: [194][   37/   37]    Overall Loss 0.403960    Objective Loss 0.403960    Top1 80.753138    LR 0.000003    Time 0.020253    
2023-01-06 16:06:16,191 - --- validate (epoch=194)-----------
2023-01-06 16:06:16,191 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:16,428 - Epoch: [194][    5/    5]    Loss 0.418403    Top1 82.442748    
2023-01-06 16:06:16,497 - ==> Top1: 82.443    Loss: 0.418

2023-01-06 16:06:16,498 - ==> Confusion:
[[360  69   0]
 [115 504   0]
 [  0   0   0]]

2023-01-06 16:06:16,499 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 187]
2023-01-06 16:06:16,499 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:16,503 - 

2023-01-06 16:06:16,504 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:16,856 - Epoch: [195][   10/   37]    Overall Loss 0.395339    Objective Loss 0.395339                                        LR 0.000003    Time 0.035146    
2023-01-06 16:06:16,995 - Epoch: [195][   20/   37]    Overall Loss 0.398426    Objective Loss 0.398426                                        LR 0.000003    Time 0.024490    
2023-01-06 16:06:17,145 - Epoch: [195][   30/   37]    Overall Loss 0.406432    Objective Loss 0.406432                                        LR 0.000003    Time 0.021339    
2023-01-06 16:06:17,231 - Epoch: [195][   37/   37]    Overall Loss 0.405144    Objective Loss 0.405144    Top1 79.916318    LR 0.000003    Time 0.019600    
2023-01-06 16:06:17,311 - --- validate (epoch=195)-----------
2023-01-06 16:06:17,311 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:17,543 - Epoch: [195][    5/    5]    Loss 0.384066    Top1 83.110687    
2023-01-06 16:06:17,617 - ==> Top1: 83.111    Loss: 0.384

2023-01-06 16:06:17,617 - ==> Confusion:
[[356  73   0]
 [104 515   0]
 [  0   0   0]]

2023-01-06 16:06:17,619 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 195]
2023-01-06 16:06:17,619 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:17,624 - 

2023-01-06 16:06:17,624 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:17,973 - Epoch: [196][   10/   37]    Overall Loss 0.402958    Objective Loss 0.402958                                        LR 0.000003    Time 0.034874    
2023-01-06 16:06:18,137 - Epoch: [196][   20/   37]    Overall Loss 0.404877    Objective Loss 0.404877                                        LR 0.000003    Time 0.025618    
2023-01-06 16:06:18,307 - Epoch: [196][   30/   37]    Overall Loss 0.402571    Objective Loss 0.402571                                        LR 0.000003    Time 0.022724    
2023-01-06 16:06:18,396 - Epoch: [196][   37/   37]    Overall Loss 0.404641    Objective Loss 0.404641    Top1 80.125523    LR 0.000003    Time 0.020835    
2023-01-06 16:06:18,473 - --- validate (epoch=196)-----------
2023-01-06 16:06:18,473 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:18,702 - Epoch: [196][    5/    5]    Loss 0.389843    Top1 82.442748    
2023-01-06 16:06:18,771 - ==> Top1: 82.443    Loss: 0.390

2023-01-06 16:06:18,771 - ==> Confusion:
[[333  96   0]
 [ 88 531   0]
 [  0   0   0]]

2023-01-06 16:06:18,772 - ==> Best [Top1: 83.111   Sparsity:0.00   Params: 46192 on epoch: 195]
2023-01-06 16:06:18,772 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:18,777 - 

2023-01-06 16:06:18,777 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:19,138 - Epoch: [197][   10/   37]    Overall Loss 0.420965    Objective Loss 0.420965                                        LR 0.000003    Time 0.036064    
2023-01-06 16:06:19,296 - Epoch: [197][   20/   37]    Overall Loss 0.419105    Objective Loss 0.419105                                        LR 0.000003    Time 0.025876    
2023-01-06 16:06:19,459 - Epoch: [197][   30/   37]    Overall Loss 0.407306    Objective Loss 0.407306                                        LR 0.000003    Time 0.022698    
2023-01-06 16:06:19,554 - Epoch: [197][   37/   37]    Overall Loss 0.405132    Objective Loss 0.405132    Top1 81.171548    LR 0.000003    Time 0.020967    
2023-01-06 16:06:19,629 - --- validate (epoch=197)-----------
2023-01-06 16:06:19,630 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:19,865 - Epoch: [197][    5/    5]    Loss 0.388369    Top1 83.206107    
2023-01-06 16:06:19,928 - ==> Top1: 83.206    Loss: 0.388

2023-01-06 16:06:19,928 - ==> Confusion:
[[355  74   0]
 [102 517   0]
 [  0   0   0]]

2023-01-06 16:06:19,930 - ==> Best [Top1: 83.206   Sparsity:0.00   Params: 46192 on epoch: 197]
2023-01-06 16:06:19,930 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:19,935 - 

2023-01-06 16:06:19,935 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:20,301 - Epoch: [198][   10/   37]    Overall Loss 0.403497    Objective Loss 0.403497                                        LR 0.000003    Time 0.036584    
2023-01-06 16:06:20,471 - Epoch: [198][   20/   37]    Overall Loss 0.401937    Objective Loss 0.401937                                        LR 0.000003    Time 0.026685    
2023-01-06 16:06:20,638 - Epoch: [198][   30/   37]    Overall Loss 0.402810    Objective Loss 0.402810                                        LR 0.000003    Time 0.023373    
2023-01-06 16:06:20,733 - Epoch: [198][   37/   37]    Overall Loss 0.405525    Objective Loss 0.405525    Top1 81.589958    LR 0.000003    Time 0.021502    
2023-01-06 16:06:20,823 - --- validate (epoch=198)-----------
2023-01-06 16:06:20,823 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:21,053 - Epoch: [198][    5/    5]    Loss 0.367991    Top1 82.442748    
2023-01-06 16:06:21,123 - ==> Top1: 82.443    Loss: 0.368

2023-01-06 16:06:21,123 - ==> Confusion:
[[361  68   0]
 [116 503   0]
 [  0   0   0]]

2023-01-06 16:06:21,125 - ==> Best [Top1: 83.206   Sparsity:0.00   Params: 46192 on epoch: 197]
2023-01-06 16:06:21,125 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:21,129 - 

2023-01-06 16:06:21,129 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-06 16:06:21,509 - Epoch: [199][   10/   37]    Overall Loss 0.413978    Objective Loss 0.413978                                        LR 0.000003    Time 0.037925    
2023-01-06 16:06:21,681 - Epoch: [199][   20/   37]    Overall Loss 0.407046    Objective Loss 0.407046                                        LR 0.000003    Time 0.027514    
2023-01-06 16:06:21,849 - Epoch: [199][   30/   37]    Overall Loss 0.406993    Objective Loss 0.406993                                        LR 0.000003    Time 0.023926    
2023-01-06 16:06:21,946 - Epoch: [199][   37/   37]    Overall Loss 0.405657    Objective Loss 0.405657    Top1 81.380753    LR 0.000003    Time 0.022035    
2023-01-06 16:06:22,019 - --- validate (epoch=199)-----------
2023-01-06 16:06:22,019 - 1048 samples (256 per mini-batch)
2023-01-06 16:06:22,258 - Epoch: [199][    5/    5]    Loss 0.390055    Top1 82.633588    
2023-01-06 16:06:22,326 - ==> Top1: 82.634    Loss: 0.390

2023-01-06 16:06:22,326 - ==> Confusion:
[[350  79   0]
 [103 516   0]
 [  0   0   0]]

2023-01-06 16:06:22,327 - ==> Best [Top1: 83.206   Sparsity:0.00   Params: 46192 on epoch: 197]
2023-01-06 16:06:22,328 - Saving checkpoint to: logs/2023.01.06-160224/qat_checkpoint.pth.tar
2023-01-06 16:06:22,332 - --- test ---------------------
2023-01-06 16:06:22,332 - 1317 samples (256 per mini-batch)
2023-01-06 16:06:22,567 - Test: [    6/    6]    Loss 0.462487    Top1 78.056188    
2023-01-06 16:06:22,637 - ==> Top1: 78.056    Loss: 0.462

2023-01-06 16:06:22,638 - ==> Confusion:
[[434 127   0]
 [162 594   0]
 [  0   0   0]]

2023-01-06 16:06:22,649 - 
2023-01-06 16:06:22,649 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-160224/2023.01.06-160224.log
