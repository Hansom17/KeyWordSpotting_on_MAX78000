2023-01-06 16:33:27,329 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-163327/2023.01.06-163327.log
2023-01-06 16:33:29,438 - => loading checkpoint qat_best.pth.tar
2023-01-06 16:33:29,440 - => Checkpoint contents:
+----------------------+-------------+----------------+
| Key                  | Type        | Value          |
|----------------------+-------------+----------------|
| arch                 | str         | ai85kws20netv2 |
| compression_sched    | dict        |                |
| epoch                | int         | 198            |
| extras               | dict        |                |
| optimizer_state_dict | dict        |                |
| optimizer_type       | type        | Adam           |
| state_dict           | OrderedDict |                |
+----------------------+-------------+----------------+

2023-01-06 16:33:29,440 - => Checkpoint['extras'] contents:
+--------------+--------+----------+
| Key          | Type   |    Value |
|--------------+--------+----------|
| best_epoch   | int    | 198      |
| best_mAP     | int    |   0      |
| best_top1    | float  |  83.3015 |
| current_mAP  | int    |   0      |
| current_top1 | float  |  83.3015 |
+--------------+--------+----------+

2023-01-06 16:33:29,441 - Loaded compression schedule from checkpoint (epoch 198)
2023-01-06 16:33:29,443 - => loaded 'state_dict' from checkpoint 'qat_best.pth.tar'
2023-01-06 16:33:29,448 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 16:33:29,448 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 16:34:24,938 - Dataset sizes:
	training=62882
	validation=6986
	test=13117
2023-01-06 16:34:24,938 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 16:34:24,941 - 

2023-01-06 16:34:24,941 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:25,846 - Epoch: [0][   10/  246]    Overall Loss 5.735998    Objective Loss 5.735998                                        LR 0.000060    Time 0.090401    
2023-01-06 16:34:26,000 - Epoch: [0][   20/  246]    Overall Loss 4.356670    Objective Loss 4.356670                                        LR 0.000060    Time 0.052899    
2023-01-06 16:34:26,152 - Epoch: [0][   30/  246]    Overall Loss 3.408707    Objective Loss 3.408707                                        LR 0.000060    Time 0.040294    
2023-01-06 16:34:26,285 - Epoch: [0][   40/  246]    Overall Loss 2.780313    Objective Loss 2.780313                                        LR 0.000060    Time 0.033555    
2023-01-06 16:34:26,423 - Epoch: [0][   50/  246]    Overall Loss 2.357011    Objective Loss 2.357011                                        LR 0.000060    Time 0.029590    
2023-01-06 16:34:26,572 - Epoch: [0][   60/  246]    Overall Loss 2.058808    Objective Loss 2.058808                                        LR 0.000060    Time 0.027125    
2023-01-06 16:34:26,723 - Epoch: [0][   70/  246]    Overall Loss 1.843048    Objective Loss 1.843048                                        LR 0.000060    Time 0.025405    
2023-01-06 16:34:26,876 - Epoch: [0][   80/  246]    Overall Loss 1.678708    Objective Loss 1.678708                                        LR 0.000060    Time 0.024139    
2023-01-06 16:34:27,009 - Epoch: [0][   90/  246]    Overall Loss 1.549006    Objective Loss 1.549006                                        LR 0.000060    Time 0.022921    
2023-01-06 16:34:27,153 - Epoch: [0][  100/  246]    Overall Loss 1.446859    Objective Loss 1.446859                                        LR 0.000060    Time 0.022075    
2023-01-06 16:34:27,290 - Epoch: [0][  110/  246]    Overall Loss 1.361810    Objective Loss 1.361810                                        LR 0.000060    Time 0.021302    
2023-01-06 16:34:27,438 - Epoch: [0][  120/  246]    Overall Loss 1.290744    Objective Loss 1.290744                                        LR 0.000060    Time 0.020760    
2023-01-06 16:34:27,574 - Epoch: [0][  130/  246]    Overall Loss 1.233377    Objective Loss 1.233377                                        LR 0.000060    Time 0.020194    
2023-01-06 16:34:27,720 - Epoch: [0][  140/  246]    Overall Loss 1.181097    Objective Loss 1.181097                                        LR 0.000060    Time 0.019797    
2023-01-06 16:34:27,871 - Epoch: [0][  150/  246]    Overall Loss 1.137536    Objective Loss 1.137536                                        LR 0.000060    Time 0.019478    
2023-01-06 16:34:28,025 - Epoch: [0][  160/  246]    Overall Loss 1.099022    Objective Loss 1.099022                                        LR 0.000060    Time 0.019219    
2023-01-06 16:34:28,182 - Epoch: [0][  170/  246]    Overall Loss 1.066133    Objective Loss 1.066133                                        LR 0.000060    Time 0.019009    
2023-01-06 16:34:28,330 - Epoch: [0][  180/  246]    Overall Loss 1.037198    Objective Loss 1.037198                                        LR 0.000060    Time 0.018777    
2023-01-06 16:34:28,473 - Epoch: [0][  190/  246]    Overall Loss 1.009552    Objective Loss 1.009552                                        LR 0.000060    Time 0.018538    
2023-01-06 16:34:28,622 - Epoch: [0][  200/  246]    Overall Loss 0.983737    Objective Loss 0.983737                                        LR 0.000060    Time 0.018341    
2023-01-06 16:34:28,766 - Epoch: [0][  210/  246]    Overall Loss 0.962376    Objective Loss 0.962376                                        LR 0.000060    Time 0.018154    
2023-01-06 16:34:28,899 - Epoch: [0][  220/  246]    Overall Loss 0.941950    Objective Loss 0.941950                                        LR 0.000060    Time 0.017923    
2023-01-06 16:34:29,039 - Epoch: [0][  230/  246]    Overall Loss 0.922522    Objective Loss 0.922522                                        LR 0.000060    Time 0.017750    
2023-01-06 16:34:29,178 - Epoch: [0][  240/  246]    Overall Loss 0.904860    Objective Loss 0.904860                                        LR 0.000060    Time 0.017585    
2023-01-06 16:34:29,240 - Epoch: [0][  246/  246]    Overall Loss 0.894624    Objective Loss 0.894624    Top1 84.449761    LR 0.000060    Time 0.017407    
2023-01-06 16:34:29,368 - --- validate (epoch=0)-----------
2023-01-06 16:34:29,368 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:29,775 - Epoch: [0][   10/   28]    Loss 0.478567    Top1 86.367188    
2023-01-06 16:34:29,875 - Epoch: [0][   20/   28]    Loss 0.507459    Top1 85.097656    
2023-01-06 16:34:29,923 - Epoch: [0][   28/   28]    Loss 0.502602    Top1 85.098769    
2023-01-06 16:34:30,080 - ==> Top1: 85.099    Loss: 0.503

2023-01-06 16:34:30,080 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:34:30,081 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 151104 on epoch: 0]
2023-01-06 16:34:30,081 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:30,087 - 

2023-01-06 16:34:30,087 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:30,566 - Epoch: [1][   10/  246]    Overall Loss 0.509926    Objective Loss 0.509926                                        LR 0.000060    Time 0.047889    
2023-01-06 16:34:30,704 - Epoch: [1][   20/  246]    Overall Loss 0.501990    Objective Loss 0.501990                                        LR 0.000060    Time 0.030814    
2023-01-06 16:34:30,846 - Epoch: [1][   30/  246]    Overall Loss 0.502417    Objective Loss 0.502417                                        LR 0.000060    Time 0.025261    
2023-01-06 16:34:30,986 - Epoch: [1][   40/  246]    Overall Loss 0.504631    Objective Loss 0.504631                                        LR 0.000060    Time 0.022418    
2023-01-06 16:34:31,128 - Epoch: [1][   50/  246]    Overall Loss 0.506556    Objective Loss 0.506556                                        LR 0.000060    Time 0.020741    
2023-01-06 16:34:31,262 - Epoch: [1][   60/  246]    Overall Loss 0.509323    Objective Loss 0.509323                                        LR 0.000060    Time 0.019523    
2023-01-06 16:34:31,395 - Epoch: [1][   70/  246]    Overall Loss 0.507848    Objective Loss 0.507848                                        LR 0.000060    Time 0.018632    
2023-01-06 16:34:31,530 - Epoch: [1][   80/  246]    Overall Loss 0.505627    Objective Loss 0.505627                                        LR 0.000060    Time 0.017985    
2023-01-06 16:34:31,675 - Epoch: [1][   90/  246]    Overall Loss 0.502851    Objective Loss 0.502851                                        LR 0.000060    Time 0.017589    
2023-01-06 16:34:31,819 - Epoch: [1][  100/  246]    Overall Loss 0.502902    Objective Loss 0.502902                                        LR 0.000060    Time 0.017271    
2023-01-06 16:34:31,945 - Epoch: [1][  110/  246]    Overall Loss 0.504104    Objective Loss 0.504104                                        LR 0.000060    Time 0.016837    
2023-01-06 16:34:32,067 - Epoch: [1][  120/  246]    Overall Loss 0.504090    Objective Loss 0.504090                                        LR 0.000060    Time 0.016450    
2023-01-06 16:34:32,187 - Epoch: [1][  130/  246]    Overall Loss 0.505683    Objective Loss 0.505683                                        LR 0.000060    Time 0.016105    
2023-01-06 16:34:32,324 - Epoch: [1][  140/  246]    Overall Loss 0.504968    Objective Loss 0.504968                                        LR 0.000060    Time 0.015930    
2023-01-06 16:34:32,465 - Epoch: [1][  150/  246]    Overall Loss 0.503520    Objective Loss 0.503520                                        LR 0.000060    Time 0.015803    
2023-01-06 16:34:32,607 - Epoch: [1][  160/  246]    Overall Loss 0.503715    Objective Loss 0.503715                                        LR 0.000060    Time 0.015702    
2023-01-06 16:34:32,747 - Epoch: [1][  170/  246]    Overall Loss 0.503033    Objective Loss 0.503033                                        LR 0.000060    Time 0.015599    
2023-01-06 16:34:32,888 - Epoch: [1][  180/  246]    Overall Loss 0.503652    Objective Loss 0.503652                                        LR 0.000060    Time 0.015512    
2023-01-06 16:34:33,032 - Epoch: [1][  190/  246]    Overall Loss 0.502012    Objective Loss 0.502012                                        LR 0.000060    Time 0.015451    
2023-01-06 16:34:33,169 - Epoch: [1][  200/  246]    Overall Loss 0.500071    Objective Loss 0.500071                                        LR 0.000060    Time 0.015362    
2023-01-06 16:34:33,304 - Epoch: [1][  210/  246]    Overall Loss 0.500659    Objective Loss 0.500659                                        LR 0.000060    Time 0.015269    
2023-01-06 16:34:33,439 - Epoch: [1][  220/  246]    Overall Loss 0.500323    Objective Loss 0.500323                                        LR 0.000060    Time 0.015188    
2023-01-06 16:34:33,575 - Epoch: [1][  230/  246]    Overall Loss 0.499612    Objective Loss 0.499612                                        LR 0.000060    Time 0.015118    
2023-01-06 16:34:33,723 - Epoch: [1][  240/  246]    Overall Loss 0.499035    Objective Loss 0.499035                                        LR 0.000060    Time 0.015103    
2023-01-06 16:34:33,785 - Epoch: [1][  246/  246]    Overall Loss 0.498458    Objective Loss 0.498458    Top1 85.645933    LR 0.000060    Time 0.014986    
2023-01-06 16:34:33,910 - --- validate (epoch=1)-----------
2023-01-06 16:34:33,910 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:34,350 - Epoch: [1][   10/   28]    Loss 0.475817    Top1 85.468750    
2023-01-06 16:34:34,452 - Epoch: [1][   20/   28]    Loss 0.484029    Top1 85.019531    
2023-01-06 16:34:34,503 - Epoch: [1][   28/   28]    Loss 0.480780    Top1 85.098769    
2023-01-06 16:34:34,631 - ==> Top1: 85.099    Loss: 0.481

2023-01-06 16:34:34,631 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:34:34,632 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 151104 on epoch: 1]
2023-01-06 16:34:34,632 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:34,639 - 

2023-01-06 16:34:34,639 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:35,263 - Epoch: [2][   10/  246]    Overall Loss 0.459451    Objective Loss 0.459451                                        LR 0.000060    Time 0.062409    
2023-01-06 16:34:35,380 - Epoch: [2][   20/  246]    Overall Loss 0.468075    Objective Loss 0.468075                                        LR 0.000060    Time 0.036994    
2023-01-06 16:34:35,501 - Epoch: [2][   30/  246]    Overall Loss 0.475744    Objective Loss 0.475744                                        LR 0.000060    Time 0.028678    
2023-01-06 16:34:35,618 - Epoch: [2][   40/  246]    Overall Loss 0.482092    Objective Loss 0.482092                                        LR 0.000060    Time 0.024448    
2023-01-06 16:34:35,744 - Epoch: [2][   50/  246]    Overall Loss 0.483884    Objective Loss 0.483884                                        LR 0.000060    Time 0.022025    
2023-01-06 16:34:35,882 - Epoch: [2][   60/  246]    Overall Loss 0.481256    Objective Loss 0.481256                                        LR 0.000060    Time 0.020652    
2023-01-06 16:34:36,019 - Epoch: [2][   70/  246]    Overall Loss 0.481515    Objective Loss 0.481515                                        LR 0.000060    Time 0.019648    
2023-01-06 16:34:36,155 - Epoch: [2][   80/  246]    Overall Loss 0.479734    Objective Loss 0.479734                                        LR 0.000060    Time 0.018887    
2023-01-06 16:34:36,290 - Epoch: [2][   90/  246]    Overall Loss 0.480664    Objective Loss 0.480664                                        LR 0.000060    Time 0.018284    
2023-01-06 16:34:36,427 - Epoch: [2][  100/  246]    Overall Loss 0.478421    Objective Loss 0.478421                                        LR 0.000060    Time 0.017818    
2023-01-06 16:34:36,565 - Epoch: [2][  110/  246]    Overall Loss 0.479432    Objective Loss 0.479432                                        LR 0.000060    Time 0.017453    
2023-01-06 16:34:36,700 - Epoch: [2][  120/  246]    Overall Loss 0.478937    Objective Loss 0.478937                                        LR 0.000060    Time 0.017123    
2023-01-06 16:34:36,834 - Epoch: [2][  130/  246]    Overall Loss 0.477687    Objective Loss 0.477687                                        LR 0.000060    Time 0.016832    
2023-01-06 16:34:36,968 - Epoch: [2][  140/  246]    Overall Loss 0.476704    Objective Loss 0.476704                                        LR 0.000060    Time 0.016583    
2023-01-06 16:34:37,103 - Epoch: [2][  150/  246]    Overall Loss 0.475136    Objective Loss 0.475136                                        LR 0.000060    Time 0.016373    
2023-01-06 16:34:37,238 - Epoch: [2][  160/  246]    Overall Loss 0.474199    Objective Loss 0.474199                                        LR 0.000060    Time 0.016190    
2023-01-06 16:34:37,376 - Epoch: [2][  170/  246]    Overall Loss 0.473385    Objective Loss 0.473385                                        LR 0.000060    Time 0.016047    
2023-01-06 16:34:37,510 - Epoch: [2][  180/  246]    Overall Loss 0.472747    Objective Loss 0.472747                                        LR 0.000060    Time 0.015898    
2023-01-06 16:34:37,645 - Epoch: [2][  190/  246]    Overall Loss 0.472665    Objective Loss 0.472665                                        LR 0.000060    Time 0.015771    
2023-01-06 16:34:37,780 - Epoch: [2][  200/  246]    Overall Loss 0.472487    Objective Loss 0.472487                                        LR 0.000060    Time 0.015651    
2023-01-06 16:34:37,924 - Epoch: [2][  210/  246]    Overall Loss 0.473879    Objective Loss 0.473879                                        LR 0.000060    Time 0.015590    
2023-01-06 16:34:38,077 - Epoch: [2][  220/  246]    Overall Loss 0.473034    Objective Loss 0.473034                                        LR 0.000060    Time 0.015577    
2023-01-06 16:34:38,232 - Epoch: [2][  230/  246]    Overall Loss 0.471930    Objective Loss 0.471930                                        LR 0.000060    Time 0.015571    
2023-01-06 16:34:38,393 - Epoch: [2][  240/  246]    Overall Loss 0.471641    Objective Loss 0.471641                                        LR 0.000060    Time 0.015590    
2023-01-06 16:34:38,454 - Epoch: [2][  246/  246]    Overall Loss 0.470641    Objective Loss 0.470641    Top1 86.124402    LR 0.000060    Time 0.015460    
2023-01-06 16:34:38,590 - --- validate (epoch=2)-----------
2023-01-06 16:34:38,591 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:39,016 - Epoch: [2][   10/   28]    Loss 0.464840    Top1 84.726562    
2023-01-06 16:34:39,121 - Epoch: [2][   20/   28]    Loss 0.457137    Top1 84.941406    
2023-01-06 16:34:39,169 - Epoch: [2][   28/   28]    Loss 0.455822    Top1 85.098769    
2023-01-06 16:34:39,303 - ==> Top1: 85.099    Loss: 0.456

2023-01-06 16:34:39,303 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:34:39,304 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 151104 on epoch: 2]
2023-01-06 16:34:39,304 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:39,313 - 

2023-01-06 16:34:39,313 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:39,829 - Epoch: [3][   10/  246]    Overall Loss 0.450095    Objective Loss 0.450095                                        LR 0.000060    Time 0.051567    
2023-01-06 16:34:39,962 - Epoch: [3][   20/  246]    Overall Loss 0.459862    Objective Loss 0.459862                                        LR 0.000060    Time 0.032371    
2023-01-06 16:34:40,089 - Epoch: [3][   30/  246]    Overall Loss 0.460559    Objective Loss 0.460559                                        LR 0.000060    Time 0.025828    
2023-01-06 16:34:40,226 - Epoch: [3][   40/  246]    Overall Loss 0.458090    Objective Loss 0.458090                                        LR 0.000060    Time 0.022768    
2023-01-06 16:34:40,366 - Epoch: [3][   50/  246]    Overall Loss 0.457260    Objective Loss 0.457260                                        LR 0.000060    Time 0.021025    
2023-01-06 16:34:40,512 - Epoch: [3][   60/  246]    Overall Loss 0.456576    Objective Loss 0.456576                                        LR 0.000060    Time 0.019949    
2023-01-06 16:34:40,682 - Epoch: [3][   70/  246]    Overall Loss 0.455596    Objective Loss 0.455596                                        LR 0.000060    Time 0.019519    
2023-01-06 16:34:40,844 - Epoch: [3][   80/  246]    Overall Loss 0.453788    Objective Loss 0.453788                                        LR 0.000060    Time 0.019091    
2023-01-06 16:34:41,000 - Epoch: [3][   90/  246]    Overall Loss 0.453536    Objective Loss 0.453536                                        LR 0.000060    Time 0.018698    
2023-01-06 16:34:41,133 - Epoch: [3][  100/  246]    Overall Loss 0.452426    Objective Loss 0.452426                                        LR 0.000060    Time 0.018161    
2023-01-06 16:34:41,267 - Epoch: [3][  110/  246]    Overall Loss 0.453671    Objective Loss 0.453671                                        LR 0.000060    Time 0.017721    
2023-01-06 16:34:41,403 - Epoch: [3][  120/  246]    Overall Loss 0.454092    Objective Loss 0.454092                                        LR 0.000060    Time 0.017377    
2023-01-06 16:34:41,537 - Epoch: [3][  130/  246]    Overall Loss 0.453149    Objective Loss 0.453149                                        LR 0.000060    Time 0.017062    
2023-01-06 16:34:41,672 - Epoch: [3][  140/  246]    Overall Loss 0.453526    Objective Loss 0.453526                                        LR 0.000060    Time 0.016794    
2023-01-06 16:34:41,804 - Epoch: [3][  150/  246]    Overall Loss 0.451175    Objective Loss 0.451175                                        LR 0.000060    Time 0.016550    
2023-01-06 16:34:41,941 - Epoch: [3][  160/  246]    Overall Loss 0.451568    Objective Loss 0.451568                                        LR 0.000060    Time 0.016375    
2023-01-06 16:34:42,075 - Epoch: [3][  170/  246]    Overall Loss 0.451123    Objective Loss 0.451123                                        LR 0.000060    Time 0.016197    
2023-01-06 16:34:42,210 - Epoch: [3][  180/  246]    Overall Loss 0.450112    Objective Loss 0.450112                                        LR 0.000060    Time 0.016040    
2023-01-06 16:34:42,341 - Epoch: [3][  190/  246]    Overall Loss 0.451879    Objective Loss 0.451879                                        LR 0.000060    Time 0.015884    
2023-01-06 16:34:42,476 - Epoch: [3][  200/  246]    Overall Loss 0.451716    Objective Loss 0.451716                                        LR 0.000060    Time 0.015765    
2023-01-06 16:34:42,608 - Epoch: [3][  210/  246]    Overall Loss 0.451941    Objective Loss 0.451941                                        LR 0.000060    Time 0.015639    
2023-01-06 16:34:42,740 - Epoch: [3][  220/  246]    Overall Loss 0.451604    Objective Loss 0.451604                                        LR 0.000060    Time 0.015527    
2023-01-06 16:34:42,873 - Epoch: [3][  230/  246]    Overall Loss 0.450822    Objective Loss 0.450822                                        LR 0.000060    Time 0.015429    
2023-01-06 16:34:43,019 - Epoch: [3][  240/  246]    Overall Loss 0.450843    Objective Loss 0.450843                                        LR 0.000060    Time 0.015393    
2023-01-06 16:34:43,082 - Epoch: [3][  246/  246]    Overall Loss 0.450532    Objective Loss 0.450532    Top1 84.449761    LR 0.000060    Time 0.015271    
2023-01-06 16:34:43,217 - --- validate (epoch=3)-----------
2023-01-06 16:34:43,217 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:43,638 - Epoch: [3][   10/   28]    Loss 0.424022    Top1 85.781250    
2023-01-06 16:34:43,736 - Epoch: [3][   20/   28]    Loss 0.436324    Top1 85.273438    
2023-01-06 16:34:43,786 - Epoch: [3][   28/   28]    Loss 0.439255    Top1 85.070140    
2023-01-06 16:34:43,930 - ==> Top1: 85.070    Loss: 0.439

2023-01-06 16:34:43,930 - ==> Confusion:
[[   2    0  437]
 [   0    0  602]
 [   4    0 5941]]

2023-01-06 16:34:43,931 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 151104 on epoch: 2]
2023-01-06 16:34:43,931 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:43,936 - 

2023-01-06 16:34:43,937 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:44,585 - Epoch: [4][   10/  246]    Overall Loss 0.448041    Objective Loss 0.448041                                        LR 0.000060    Time 0.064765    
2023-01-06 16:34:44,733 - Epoch: [4][   20/  246]    Overall Loss 0.438158    Objective Loss 0.438158                                        LR 0.000060    Time 0.039772    
2023-01-06 16:34:44,876 - Epoch: [4][   30/  246]    Overall Loss 0.442059    Objective Loss 0.442059                                        LR 0.000060    Time 0.031257    
2023-01-06 16:34:45,014 - Epoch: [4][   40/  246]    Overall Loss 0.442084    Objective Loss 0.442084                                        LR 0.000060    Time 0.026881    
2023-01-06 16:34:45,152 - Epoch: [4][   50/  246]    Overall Loss 0.441782    Objective Loss 0.441782                                        LR 0.000060    Time 0.024264    
2023-01-06 16:34:45,291 - Epoch: [4][   60/  246]    Overall Loss 0.441121    Objective Loss 0.441121                                        LR 0.000060    Time 0.022538    
2023-01-06 16:34:45,430 - Epoch: [4][   70/  246]    Overall Loss 0.439956    Objective Loss 0.439956                                        LR 0.000060    Time 0.021285    
2023-01-06 16:34:45,564 - Epoch: [4][   80/  246]    Overall Loss 0.441906    Objective Loss 0.441906                                        LR 0.000060    Time 0.020294    
2023-01-06 16:34:45,702 - Epoch: [4][   90/  246]    Overall Loss 0.440598    Objective Loss 0.440598                                        LR 0.000060    Time 0.019557    
2023-01-06 16:34:45,838 - Epoch: [4][  100/  246]    Overall Loss 0.440136    Objective Loss 0.440136                                        LR 0.000060    Time 0.018957    
2023-01-06 16:34:45,978 - Epoch: [4][  110/  246]    Overall Loss 0.440317    Objective Loss 0.440317                                        LR 0.000060    Time 0.018500    
2023-01-06 16:34:46,104 - Epoch: [4][  120/  246]    Overall Loss 0.437581    Objective Loss 0.437581                                        LR 0.000060    Time 0.018008    
2023-01-06 16:34:46,250 - Epoch: [4][  130/  246]    Overall Loss 0.439032    Objective Loss 0.439032                                        LR 0.000060    Time 0.017742    
2023-01-06 16:34:46,388 - Epoch: [4][  140/  246]    Overall Loss 0.439578    Objective Loss 0.439578                                        LR 0.000060    Time 0.017458    
2023-01-06 16:34:46,536 - Epoch: [4][  150/  246]    Overall Loss 0.439970    Objective Loss 0.439970                                        LR 0.000060    Time 0.017280    
2023-01-06 16:34:46,680 - Epoch: [4][  160/  246]    Overall Loss 0.441429    Objective Loss 0.441429                                        LR 0.000060    Time 0.017096    
2023-01-06 16:34:46,825 - Epoch: [4][  170/  246]    Overall Loss 0.440195    Objective Loss 0.440195                                        LR 0.000060    Time 0.016940    
2023-01-06 16:34:46,966 - Epoch: [4][  180/  246]    Overall Loss 0.440685    Objective Loss 0.440685                                        LR 0.000060    Time 0.016782    
2023-01-06 16:34:47,122 - Epoch: [4][  190/  246]    Overall Loss 0.439919    Objective Loss 0.439919                                        LR 0.000060    Time 0.016714    
2023-01-06 16:34:47,276 - Epoch: [4][  200/  246]    Overall Loss 0.438268    Objective Loss 0.438268                                        LR 0.000060    Time 0.016648    
2023-01-06 16:34:47,435 - Epoch: [4][  210/  246]    Overall Loss 0.438426    Objective Loss 0.438426                                        LR 0.000060    Time 0.016611    
2023-01-06 16:34:47,605 - Epoch: [4][  220/  246]    Overall Loss 0.438892    Objective Loss 0.438892                                        LR 0.000060    Time 0.016627    
2023-01-06 16:34:47,769 - Epoch: [4][  230/  246]    Overall Loss 0.439202    Objective Loss 0.439202                                        LR 0.000060    Time 0.016615    
2023-01-06 16:34:47,925 - Epoch: [4][  240/  246]    Overall Loss 0.439674    Objective Loss 0.439674                                        LR 0.000060    Time 0.016572    
2023-01-06 16:34:47,988 - Epoch: [4][  246/  246]    Overall Loss 0.439825    Objective Loss 0.439825    Top1 83.492823    LR 0.000060    Time 0.016421    
2023-01-06 16:34:48,113 - --- validate (epoch=4)-----------
2023-01-06 16:34:48,113 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:48,543 - Epoch: [4][   10/   28]    Loss 0.442234    Top1 84.648438    
2023-01-06 16:34:48,642 - Epoch: [4][   20/   28]    Loss 0.440090    Top1 84.667969    
2023-01-06 16:34:48,694 - Epoch: [4][   28/   28]    Loss 0.429869    Top1 85.012883    
2023-01-06 16:34:48,826 - ==> Top1: 85.013    Loss: 0.430

2023-01-06 16:34:48,826 - ==> Confusion:
[[  29    0  410]
 [   1    0  601]
 [  34    1 5910]]

2023-01-06 16:34:48,827 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 151104 on epoch: 2]
2023-01-06 16:34:48,827 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:48,832 - 

2023-01-06 16:34:48,832 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:49,479 - Epoch: [5][   10/  246]    Overall Loss 0.445574    Objective Loss 0.445574                                        LR 0.000060    Time 0.064588    
2023-01-06 16:34:49,619 - Epoch: [5][   20/  246]    Overall Loss 0.432826    Objective Loss 0.432826                                        LR 0.000060    Time 0.039283    
2023-01-06 16:34:49,779 - Epoch: [5][   30/  246]    Overall Loss 0.431349    Objective Loss 0.431349                                        LR 0.000060    Time 0.031498    
2023-01-06 16:34:49,918 - Epoch: [5][   40/  246]    Overall Loss 0.431087    Objective Loss 0.431087                                        LR 0.000060    Time 0.027057    
2023-01-06 16:34:50,052 - Epoch: [5][   50/  246]    Overall Loss 0.431643    Objective Loss 0.431643                                        LR 0.000060    Time 0.024322    
2023-01-06 16:34:50,199 - Epoch: [5][   60/  246]    Overall Loss 0.432973    Objective Loss 0.432973                                        LR 0.000060    Time 0.022710    
2023-01-06 16:34:50,345 - Epoch: [5][   70/  246]    Overall Loss 0.435184    Objective Loss 0.435184                                        LR 0.000060    Time 0.021546    
2023-01-06 16:34:50,488 - Epoch: [5][   80/  246]    Overall Loss 0.437648    Objective Loss 0.437648                                        LR 0.000060    Time 0.020643    
2023-01-06 16:34:50,633 - Epoch: [5][   90/  246]    Overall Loss 0.436492    Objective Loss 0.436492                                        LR 0.000060    Time 0.019948    
2023-01-06 16:34:50,769 - Epoch: [5][  100/  246]    Overall Loss 0.434363    Objective Loss 0.434363                                        LR 0.000060    Time 0.019310    
2023-01-06 16:34:50,907 - Epoch: [5][  110/  246]    Overall Loss 0.434503    Objective Loss 0.434503                                        LR 0.000060    Time 0.018805    
2023-01-06 16:34:51,046 - Epoch: [5][  120/  246]    Overall Loss 0.433445    Objective Loss 0.433445                                        LR 0.000060    Time 0.018394    
2023-01-06 16:34:51,196 - Epoch: [5][  130/  246]    Overall Loss 0.433337    Objective Loss 0.433337                                        LR 0.000060    Time 0.018131    
2023-01-06 16:34:51,340 - Epoch: [5][  140/  246]    Overall Loss 0.432676    Objective Loss 0.432676                                        LR 0.000060    Time 0.017864    
2023-01-06 16:34:51,496 - Epoch: [5][  150/  246]    Overall Loss 0.431181    Objective Loss 0.431181                                        LR 0.000060    Time 0.017707    
2023-01-06 16:34:51,654 - Epoch: [5][  160/  246]    Overall Loss 0.432445    Objective Loss 0.432445                                        LR 0.000060    Time 0.017585    
2023-01-06 16:34:51,812 - Epoch: [5][  170/  246]    Overall Loss 0.433200    Objective Loss 0.433200                                        LR 0.000060    Time 0.017478    
2023-01-06 16:34:51,966 - Epoch: [5][  180/  246]    Overall Loss 0.432083    Objective Loss 0.432083                                        LR 0.000060    Time 0.017364    
2023-01-06 16:34:52,117 - Epoch: [5][  190/  246]    Overall Loss 0.431486    Objective Loss 0.431486                                        LR 0.000060    Time 0.017242    
2023-01-06 16:34:52,269 - Epoch: [5][  200/  246]    Overall Loss 0.430970    Objective Loss 0.430970                                        LR 0.000060    Time 0.017136    
2023-01-06 16:34:52,419 - Epoch: [5][  210/  246]    Overall Loss 0.431849    Objective Loss 0.431849                                        LR 0.000060    Time 0.017031    
2023-01-06 16:34:52,546 - Epoch: [5][  220/  246]    Overall Loss 0.431423    Objective Loss 0.431423                                        LR 0.000060    Time 0.016835    
2023-01-06 16:34:52,685 - Epoch: [5][  230/  246]    Overall Loss 0.431899    Objective Loss 0.431899                                        LR 0.000060    Time 0.016704    
2023-01-06 16:34:52,842 - Epoch: [5][  240/  246]    Overall Loss 0.431619    Objective Loss 0.431619                                        LR 0.000060    Time 0.016661    
2023-01-06 16:34:52,909 - Epoch: [5][  246/  246]    Overall Loss 0.431563    Objective Loss 0.431563    Top1 82.535885    LR 0.000060    Time 0.016527    
2023-01-06 16:34:53,034 - --- validate (epoch=5)-----------
2023-01-06 16:34:53,035 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:53,471 - Epoch: [5][   10/   28]    Loss 0.430786    Top1 84.531250    
2023-01-06 16:34:53,574 - Epoch: [5][   20/   28]    Loss 0.424584    Top1 84.980469    
2023-01-06 16:34:53,624 - Epoch: [5][   28/   28]    Loss 0.420798    Top1 85.213284    
2023-01-06 16:34:53,774 - ==> Top1: 85.213    Loss: 0.421

2023-01-06 16:34:53,774 - ==> Confusion:
[[  32    1  406]
 [   1    5  596]
 [  25    4 5916]]

2023-01-06 16:34:53,775 - ==> Best [Top1: 85.213   Sparsity:0.00   Params: 151104 on epoch: 5]
2023-01-06 16:34:53,775 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:53,781 - 

2023-01-06 16:34:53,781 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:54,316 - Epoch: [6][   10/  246]    Overall Loss 0.436645    Objective Loss 0.436645                                        LR 0.000060    Time 0.053355    
2023-01-06 16:34:54,463 - Epoch: [6][   20/  246]    Overall Loss 0.444758    Objective Loss 0.444758                                        LR 0.000060    Time 0.034007    
2023-01-06 16:34:54,612 - Epoch: [6][   30/  246]    Overall Loss 0.438731    Objective Loss 0.438731                                        LR 0.000060    Time 0.027622    
2023-01-06 16:34:54,775 - Epoch: [6][   40/  246]    Overall Loss 0.433528    Objective Loss 0.433528                                        LR 0.000060    Time 0.024747    
2023-01-06 16:34:54,917 - Epoch: [6][   50/  246]    Overall Loss 0.426911    Objective Loss 0.426911                                        LR 0.000060    Time 0.022630    
2023-01-06 16:34:55,069 - Epoch: [6][   60/  246]    Overall Loss 0.423642    Objective Loss 0.423642                                        LR 0.000060    Time 0.021391    
2023-01-06 16:34:55,220 - Epoch: [6][   70/  246]    Overall Loss 0.424721    Objective Loss 0.424721                                        LR 0.000060    Time 0.020481    
2023-01-06 16:34:55,381 - Epoch: [6][   80/  246]    Overall Loss 0.425864    Objective Loss 0.425864                                        LR 0.000060    Time 0.019931    
2023-01-06 16:34:55,531 - Epoch: [6][   90/  246]    Overall Loss 0.423298    Objective Loss 0.423298                                        LR 0.000060    Time 0.019376    
2023-01-06 16:34:55,687 - Epoch: [6][  100/  246]    Overall Loss 0.422944    Objective Loss 0.422944                                        LR 0.000060    Time 0.018997    
2023-01-06 16:34:55,835 - Epoch: [6][  110/  246]    Overall Loss 0.422528    Objective Loss 0.422528                                        LR 0.000060    Time 0.018605    
2023-01-06 16:34:55,994 - Epoch: [6][  120/  246]    Overall Loss 0.422991    Objective Loss 0.422991                                        LR 0.000060    Time 0.018362    
2023-01-06 16:34:56,139 - Epoch: [6][  130/  246]    Overall Loss 0.421219    Objective Loss 0.421219                                        LR 0.000060    Time 0.018063    
2023-01-06 16:34:56,295 - Epoch: [6][  140/  246]    Overall Loss 0.422554    Objective Loss 0.422554                                        LR 0.000060    Time 0.017886    
2023-01-06 16:34:56,442 - Epoch: [6][  150/  246]    Overall Loss 0.423173    Objective Loss 0.423173                                        LR 0.000060    Time 0.017672    
2023-01-06 16:34:56,593 - Epoch: [6][  160/  246]    Overall Loss 0.424231    Objective Loss 0.424231                                        LR 0.000060    Time 0.017505    
2023-01-06 16:34:56,740 - Epoch: [6][  170/  246]    Overall Loss 0.423532    Objective Loss 0.423532                                        LR 0.000060    Time 0.017337    
2023-01-06 16:34:56,897 - Epoch: [6][  180/  246]    Overall Loss 0.423920    Objective Loss 0.423920                                        LR 0.000060    Time 0.017242    
2023-01-06 16:34:57,045 - Epoch: [6][  190/  246]    Overall Loss 0.424835    Objective Loss 0.424835                                        LR 0.000060    Time 0.017114    
2023-01-06 16:34:57,196 - Epoch: [6][  200/  246]    Overall Loss 0.424980    Objective Loss 0.424980                                        LR 0.000060    Time 0.017009    
2023-01-06 16:34:57,343 - Epoch: [6][  210/  246]    Overall Loss 0.424975    Objective Loss 0.424975                                        LR 0.000060    Time 0.016899    
2023-01-06 16:34:57,496 - Epoch: [6][  220/  246]    Overall Loss 0.424029    Objective Loss 0.424029                                        LR 0.000060    Time 0.016817    
2023-01-06 16:34:57,641 - Epoch: [6][  230/  246]    Overall Loss 0.424469    Objective Loss 0.424469                                        LR 0.000060    Time 0.016714    
2023-01-06 16:34:57,809 - Epoch: [6][  240/  246]    Overall Loss 0.424015    Objective Loss 0.424015                                        LR 0.000060    Time 0.016709    
2023-01-06 16:34:57,885 - Epoch: [6][  246/  246]    Overall Loss 0.422937    Objective Loss 0.422937    Top1 87.799043    LR 0.000060    Time 0.016608    
2023-01-06 16:34:58,025 - --- validate (epoch=6)-----------
2023-01-06 16:34:58,025 - 6986 samples (256 per mini-batch)
2023-01-06 16:34:58,485 - Epoch: [6][   10/   28]    Loss 0.419453    Top1 84.882812    
2023-01-06 16:34:58,608 - Epoch: [6][   20/   28]    Loss 0.407692    Top1 85.507812    
2023-01-06 16:34:58,661 - Epoch: [6][   28/   28]    Loss 0.415335    Top1 85.141712    
2023-01-06 16:34:58,805 - ==> Top1: 85.142    Loss: 0.415

2023-01-06 16:34:58,805 - ==> Confusion:
[[   8    1  430]
 [   0    4  598]
 [   7    2 5936]]

2023-01-06 16:34:58,806 - ==> Best [Top1: 85.213   Sparsity:0.00   Params: 151104 on epoch: 5]
2023-01-06 16:34:58,806 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:34:58,811 - 

2023-01-06 16:34:58,811 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:34:59,477 - Epoch: [7][   10/  246]    Overall Loss 0.448115    Objective Loss 0.448115                                        LR 0.000060    Time 0.066483    
2023-01-06 16:34:59,631 - Epoch: [7][   20/  246]    Overall Loss 0.421047    Objective Loss 0.421047                                        LR 0.000060    Time 0.040933    
2023-01-06 16:34:59,774 - Epoch: [7][   30/  246]    Overall Loss 0.420836    Objective Loss 0.420836                                        LR 0.000060    Time 0.032046    
2023-01-06 16:34:59,940 - Epoch: [7][   40/  246]    Overall Loss 0.422419    Objective Loss 0.422419                                        LR 0.000060    Time 0.028162    
2023-01-06 16:35:00,101 - Epoch: [7][   50/  246]    Overall Loss 0.419915    Objective Loss 0.419915                                        LR 0.000060    Time 0.025753    
2023-01-06 16:35:00,275 - Epoch: [7][   60/  246]    Overall Loss 0.420599    Objective Loss 0.420599                                        LR 0.000060    Time 0.024345    
2023-01-06 16:35:00,451 - Epoch: [7][   70/  246]    Overall Loss 0.418087    Objective Loss 0.418087                                        LR 0.000060    Time 0.023378    
2023-01-06 16:35:00,645 - Epoch: [7][   80/  246]    Overall Loss 0.416568    Objective Loss 0.416568                                        LR 0.000060    Time 0.022881    
2023-01-06 16:35:00,836 - Epoch: [7][   90/  246]    Overall Loss 0.416835    Objective Loss 0.416835                                        LR 0.000060    Time 0.022446    
2023-01-06 16:35:01,027 - Epoch: [7][  100/  246]    Overall Loss 0.415895    Objective Loss 0.415895                                        LR 0.000060    Time 0.022110    
2023-01-06 16:35:01,215 - Epoch: [7][  110/  246]    Overall Loss 0.415545    Objective Loss 0.415545                                        LR 0.000060    Time 0.021806    
2023-01-06 16:35:01,402 - Epoch: [7][  120/  246]    Overall Loss 0.414603    Objective Loss 0.414603                                        LR 0.000060    Time 0.021543    
2023-01-06 16:35:01,586 - Epoch: [7][  130/  246]    Overall Loss 0.415718    Objective Loss 0.415718                                        LR 0.000060    Time 0.021300    
2023-01-06 16:35:01,773 - Epoch: [7][  140/  246]    Overall Loss 0.415158    Objective Loss 0.415158                                        LR 0.000060    Time 0.021112    
2023-01-06 16:35:01,957 - Epoch: [7][  150/  246]    Overall Loss 0.415763    Objective Loss 0.415763                                        LR 0.000060    Time 0.020928    
2023-01-06 16:35:02,146 - Epoch: [7][  160/  246]    Overall Loss 0.417141    Objective Loss 0.417141                                        LR 0.000060    Time 0.020797    
2023-01-06 16:35:02,314 - Epoch: [7][  170/  246]    Overall Loss 0.417403    Objective Loss 0.417403                                        LR 0.000060    Time 0.020559    
2023-01-06 16:35:02,486 - Epoch: [7][  180/  246]    Overall Loss 0.417320    Objective Loss 0.417320                                        LR 0.000060    Time 0.020370    
2023-01-06 16:35:02,672 - Epoch: [7][  190/  246]    Overall Loss 0.417009    Objective Loss 0.417009                                        LR 0.000060    Time 0.020273    
2023-01-06 16:35:02,860 - Epoch: [7][  200/  246]    Overall Loss 0.415612    Objective Loss 0.415612                                        LR 0.000060    Time 0.020200    
2023-01-06 16:35:03,053 - Epoch: [7][  210/  246]    Overall Loss 0.414867    Objective Loss 0.414867                                        LR 0.000060    Time 0.020154    
2023-01-06 16:35:03,242 - Epoch: [7][  220/  246]    Overall Loss 0.414285    Objective Loss 0.414285                                        LR 0.000060    Time 0.020092    
2023-01-06 16:35:03,425 - Epoch: [7][  230/  246]    Overall Loss 0.415182    Objective Loss 0.415182                                        LR 0.000060    Time 0.020012    
2023-01-06 16:35:03,608 - Epoch: [7][  240/  246]    Overall Loss 0.414688    Objective Loss 0.414688                                        LR 0.000060    Time 0.019942    
2023-01-06 16:35:03,690 - Epoch: [7][  246/  246]    Overall Loss 0.414989    Objective Loss 0.414989    Top1 82.535885    LR 0.000060    Time 0.019788    
2023-01-06 16:35:03,841 - --- validate (epoch=7)-----------
2023-01-06 16:35:03,841 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:04,260 - Epoch: [7][   10/   28]    Loss 0.410257    Top1 85.390625    
2023-01-06 16:35:04,359 - Epoch: [7][   20/   28]    Loss 0.398175    Top1 85.996094    
2023-01-06 16:35:04,411 - Epoch: [7][   28/   28]    Loss 0.404963    Top1 85.699971    
2023-01-06 16:35:04,530 - ==> Top1: 85.700    Loss: 0.405

2023-01-06 16:35:04,531 - ==> Confusion:
[[  81    4  354]
 [   2   19  581]
 [  44   14 5887]]

2023-01-06 16:35:04,531 - ==> Best [Top1: 85.700   Sparsity:0.00   Params: 151104 on epoch: 7]
2023-01-06 16:35:04,532 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:35:04,538 - 

2023-01-06 16:35:04,538 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:05,195 - Epoch: [8][   10/  246]    Overall Loss 0.398039    Objective Loss 0.398039                                        LR 0.000060    Time 0.065660    
2023-01-06 16:35:05,323 - Epoch: [8][   20/  246]    Overall Loss 0.411978    Objective Loss 0.411978                                        LR 0.000060    Time 0.039200    
2023-01-06 16:35:05,450 - Epoch: [8][   30/  246]    Overall Loss 0.408375    Objective Loss 0.408375                                        LR 0.000060    Time 0.030365    
2023-01-06 16:35:05,572 - Epoch: [8][   40/  246]    Overall Loss 0.409211    Objective Loss 0.409211                                        LR 0.000060    Time 0.025814    
2023-01-06 16:35:05,695 - Epoch: [8][   50/  246]    Overall Loss 0.409951    Objective Loss 0.409951                                        LR 0.000060    Time 0.023095    
2023-01-06 16:35:05,818 - Epoch: [8][   60/  246]    Overall Loss 0.408751    Objective Loss 0.408751                                        LR 0.000060    Time 0.021288    
2023-01-06 16:35:05,945 - Epoch: [8][   70/  246]    Overall Loss 0.411400    Objective Loss 0.411400                                        LR 0.000060    Time 0.020064    
2023-01-06 16:35:06,078 - Epoch: [8][   80/  246]    Overall Loss 0.409214    Objective Loss 0.409214                                        LR 0.000060    Time 0.019202    
2023-01-06 16:35:06,210 - Epoch: [8][   90/  246]    Overall Loss 0.409711    Objective Loss 0.409711                                        LR 0.000060    Time 0.018535    
2023-01-06 16:35:06,341 - Epoch: [8][  100/  246]    Overall Loss 0.409878    Objective Loss 0.409878                                        LR 0.000060    Time 0.017988    
2023-01-06 16:35:06,473 - Epoch: [8][  110/  246]    Overall Loss 0.409930    Objective Loss 0.409930                                        LR 0.000060    Time 0.017548    
2023-01-06 16:35:06,609 - Epoch: [8][  120/  246]    Overall Loss 0.410516    Objective Loss 0.410516                                        LR 0.000060    Time 0.017216    
2023-01-06 16:35:06,743 - Epoch: [8][  130/  246]    Overall Loss 0.410369    Objective Loss 0.410369                                        LR 0.000060    Time 0.016917    
2023-01-06 16:35:06,879 - Epoch: [8][  140/  246]    Overall Loss 0.410242    Objective Loss 0.410242                                        LR 0.000060    Time 0.016676    
2023-01-06 16:35:07,013 - Epoch: [8][  150/  246]    Overall Loss 0.409201    Objective Loss 0.409201                                        LR 0.000060    Time 0.016456    
2023-01-06 16:35:07,153 - Epoch: [8][  160/  246]    Overall Loss 0.409621    Objective Loss 0.409621                                        LR 0.000060    Time 0.016302    
2023-01-06 16:35:07,294 - Epoch: [8][  170/  246]    Overall Loss 0.408425    Objective Loss 0.408425                                        LR 0.000060    Time 0.016170    
2023-01-06 16:35:07,435 - Epoch: [8][  180/  246]    Overall Loss 0.407549    Objective Loss 0.407549                                        LR 0.000060    Time 0.016049    
2023-01-06 16:35:07,575 - Epoch: [8][  190/  246]    Overall Loss 0.407750    Objective Loss 0.407750                                        LR 0.000060    Time 0.015941    
2023-01-06 16:35:07,712 - Epoch: [8][  200/  246]    Overall Loss 0.407417    Objective Loss 0.407417                                        LR 0.000060    Time 0.015826    
2023-01-06 16:35:07,852 - Epoch: [8][  210/  246]    Overall Loss 0.408268    Objective Loss 0.408268                                        LR 0.000060    Time 0.015737    
2023-01-06 16:35:07,996 - Epoch: [8][  220/  246]    Overall Loss 0.407732    Objective Loss 0.407732                                        LR 0.000060    Time 0.015673    
2023-01-06 16:35:08,146 - Epoch: [8][  230/  246]    Overall Loss 0.408114    Objective Loss 0.408114                                        LR 0.000060    Time 0.015644    
2023-01-06 16:35:08,303 - Epoch: [8][  240/  246]    Overall Loss 0.408162    Objective Loss 0.408162                                        LR 0.000060    Time 0.015646    
2023-01-06 16:35:08,368 - Epoch: [8][  246/  246]    Overall Loss 0.408378    Objective Loss 0.408378    Top1 86.363636    LR 0.000060    Time 0.015526    
2023-01-06 16:35:08,505 - --- validate (epoch=8)-----------
2023-01-06 16:35:08,507 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:08,931 - Epoch: [8][   10/   28]    Loss 0.408968    Top1 85.273438    
2023-01-06 16:35:09,019 - Epoch: [8][   20/   28]    Loss 0.396685    Top1 85.781250    
2023-01-06 16:35:09,071 - Epoch: [8][   28/   28]    Loss 0.397213    Top1 85.771543    
2023-01-06 16:35:09,207 - ==> Top1: 85.772    Loss: 0.397

2023-01-06 16:35:09,208 - ==> Confusion:
[[  93    2  344]
 [   3   12  587]
 [  48   10 5887]]

2023-01-06 16:35:09,209 - ==> Best [Top1: 85.772   Sparsity:0.00   Params: 151104 on epoch: 8]
2023-01-06 16:35:09,209 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:35:09,220 - 

2023-01-06 16:35:09,220 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:09,742 - Epoch: [9][   10/  246]    Overall Loss 0.404228    Objective Loss 0.404228                                        LR 0.000060    Time 0.052111    
2023-01-06 16:35:09,879 - Epoch: [9][   20/  246]    Overall Loss 0.394866    Objective Loss 0.394866                                        LR 0.000060    Time 0.032873    
2023-01-06 16:35:10,025 - Epoch: [9][   30/  246]    Overall Loss 0.392807    Objective Loss 0.392807                                        LR 0.000060    Time 0.026764    
2023-01-06 16:35:10,168 - Epoch: [9][   40/  246]    Overall Loss 0.394133    Objective Loss 0.394133                                        LR 0.000060    Time 0.023628    
2023-01-06 16:35:10,309 - Epoch: [9][   50/  246]    Overall Loss 0.391835    Objective Loss 0.391835                                        LR 0.000060    Time 0.021727    
2023-01-06 16:35:10,453 - Epoch: [9][   60/  246]    Overall Loss 0.395420    Objective Loss 0.395420                                        LR 0.000060    Time 0.020489    
2023-01-06 16:35:10,597 - Epoch: [9][   70/  246]    Overall Loss 0.399667    Objective Loss 0.399667                                        LR 0.000060    Time 0.019618    
2023-01-06 16:35:10,736 - Epoch: [9][   80/  246]    Overall Loss 0.397224    Objective Loss 0.397224                                        LR 0.000060    Time 0.018893    
2023-01-06 16:35:10,878 - Epoch: [9][   90/  246]    Overall Loss 0.397818    Objective Loss 0.397818                                        LR 0.000060    Time 0.018376    
2023-01-06 16:35:11,022 - Epoch: [9][  100/  246]    Overall Loss 0.397584    Objective Loss 0.397584                                        LR 0.000060    Time 0.017967    
2023-01-06 16:35:11,166 - Epoch: [9][  110/  246]    Overall Loss 0.397866    Objective Loss 0.397866                                        LR 0.000060    Time 0.017644    
2023-01-06 16:35:11,306 - Epoch: [9][  120/  246]    Overall Loss 0.397041    Objective Loss 0.397041                                        LR 0.000060    Time 0.017339    
2023-01-06 16:35:11,442 - Epoch: [9][  130/  246]    Overall Loss 0.395823    Objective Loss 0.395823                                        LR 0.000060    Time 0.017044    
2023-01-06 16:35:11,576 - Epoch: [9][  140/  246]    Overall Loss 0.396720    Objective Loss 0.396720                                        LR 0.000060    Time 0.016785    
2023-01-06 16:35:11,713 - Epoch: [9][  150/  246]    Overall Loss 0.398089    Objective Loss 0.398089                                        LR 0.000060    Time 0.016576    
2023-01-06 16:35:11,854 - Epoch: [9][  160/  246]    Overall Loss 0.398089    Objective Loss 0.398089                                        LR 0.000060    Time 0.016418    
2023-01-06 16:35:11,994 - Epoch: [9][  170/  246]    Overall Loss 0.398748    Objective Loss 0.398748                                        LR 0.000060    Time 0.016273    
2023-01-06 16:35:12,138 - Epoch: [9][  180/  246]    Overall Loss 0.398362    Objective Loss 0.398362                                        LR 0.000060    Time 0.016165    
2023-01-06 16:35:12,282 - Epoch: [9][  190/  246]    Overall Loss 0.399017    Objective Loss 0.399017                                        LR 0.000060    Time 0.016073    
2023-01-06 16:35:12,428 - Epoch: [9][  200/  246]    Overall Loss 0.398939    Objective Loss 0.398939                                        LR 0.000060    Time 0.015993    
2023-01-06 16:35:12,560 - Epoch: [9][  210/  246]    Overall Loss 0.399069    Objective Loss 0.399069                                        LR 0.000060    Time 0.015858    
2023-01-06 16:35:12,677 - Epoch: [9][  220/  246]    Overall Loss 0.399054    Objective Loss 0.399054                                        LR 0.000060    Time 0.015671    
2023-01-06 16:35:12,795 - Epoch: [9][  230/  246]    Overall Loss 0.399112    Objective Loss 0.399112                                        LR 0.000060    Time 0.015501    
2023-01-06 16:35:12,928 - Epoch: [9][  240/  246]    Overall Loss 0.399756    Objective Loss 0.399756                                        LR 0.000060    Time 0.015405    
2023-01-06 16:35:12,992 - Epoch: [9][  246/  246]    Overall Loss 0.399826    Objective Loss 0.399826    Top1 84.688995    LR 0.000060    Time 0.015291    
2023-01-06 16:35:13,134 - --- validate (epoch=9)-----------
2023-01-06 16:35:13,134 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:13,556 - Epoch: [9][   10/   28]    Loss 0.398243    Top1 85.546875    
2023-01-06 16:35:13,660 - Epoch: [9][   20/   28]    Loss 0.386250    Top1 86.210938    
2023-01-06 16:35:13,709 - Epoch: [9][   28/   28]    Loss 0.390973    Top1 85.986258    
2023-01-06 16:35:13,869 - ==> Top1: 85.986    Loss: 0.391

2023-01-06 16:35:13,869 - ==> Confusion:
[[  79    2  358]
 [   2   24  576]
 [  26   15 5904]]

2023-01-06 16:35:13,870 - ==> Best [Top1: 85.986   Sparsity:0.00   Params: 151104 on epoch: 9]
2023-01-06 16:35:13,870 - Saving checkpoint to: logs/2023.01.06-163327/checkpoint.pth.tar
2023-01-06 16:35:13,886 - 

2023-01-06 16:35:13,887 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:14,540 - Epoch: [10][   10/  246]    Overall Loss 0.426589    Objective Loss 0.426589                                        LR 0.000060    Time 0.065309    
2023-01-06 16:35:14,683 - Epoch: [10][   20/  246]    Overall Loss 0.420212    Objective Loss 0.420212                                        LR 0.000060    Time 0.039750    
2023-01-06 16:35:14,825 - Epoch: [10][   30/  246]    Overall Loss 0.410242    Objective Loss 0.410242                                        LR 0.000060    Time 0.031242    
2023-01-06 16:35:14,965 - Epoch: [10][   40/  246]    Overall Loss 0.410697    Objective Loss 0.410697                                        LR 0.000060    Time 0.026917    
2023-01-06 16:35:15,112 - Epoch: [10][   50/  246]    Overall Loss 0.405307    Objective Loss 0.405307                                        LR 0.000060    Time 0.024462    
2023-01-06 16:35:15,243 - Epoch: [10][   60/  246]    Overall Loss 0.399753    Objective Loss 0.399753                                        LR 0.000060    Time 0.022565    
2023-01-06 16:35:15,368 - Epoch: [10][   70/  246]    Overall Loss 0.401671    Objective Loss 0.401671                                        LR 0.000060    Time 0.021118    
2023-01-06 16:35:15,503 - Epoch: [10][   80/  246]    Overall Loss 0.404409    Objective Loss 0.404409                                        LR 0.000060    Time 0.020155    
2023-01-06 16:35:15,651 - Epoch: [10][   90/  246]    Overall Loss 0.402781    Objective Loss 0.402781                                        LR 0.000060    Time 0.019555    
2023-01-06 16:35:15,804 - Epoch: [10][  100/  246]    Overall Loss 0.402460    Objective Loss 0.402460                                        LR 0.000060    Time 0.019120    
2023-01-06 16:35:15,946 - Epoch: [10][  110/  246]    Overall Loss 0.399566    Objective Loss 0.399566                                        LR 0.000060    Time 0.018669    
2023-01-06 16:35:16,089 - Epoch: [10][  120/  246]    Overall Loss 0.397780    Objective Loss 0.397780                                        LR 0.000060    Time 0.018305    
2023-01-06 16:35:16,222 - Epoch: [10][  130/  246]    Overall Loss 0.396822    Objective Loss 0.396822                                        LR 0.000060    Time 0.017909    
2023-01-06 16:35:16,358 - Epoch: [10][  140/  246]    Overall Loss 0.396481    Objective Loss 0.396481                                        LR 0.000060    Time 0.017603    
2023-01-06 16:35:16,487 - Epoch: [10][  150/  246]    Overall Loss 0.395807    Objective Loss 0.395807                                        LR 0.000060    Time 0.017285    
2023-01-06 16:35:16,612 - Epoch: [10][  160/  246]    Overall Loss 0.395083    Objective Loss 0.395083                                        LR 0.000060    Time 0.016981    
2023-01-06 16:35:16,732 - Epoch: [10][  170/  246]    Overall Loss 0.394786    Objective Loss 0.394786                                        LR 0.000060    Time 0.016685    
2023-01-06 16:35:16,857 - Epoch: [10][  180/  246]    Overall Loss 0.393799    Objective Loss 0.393799                                        LR 0.000060    Time 0.016451    
2023-01-06 16:35:16,978 - Epoch: [10][  190/  246]    Overall Loss 0.395194    Objective Loss 0.395194                                        LR 0.000060    Time 0.016223    
2023-01-06 16:35:17,105 - Epoch: [10][  200/  246]    Overall Loss 0.395726    Objective Loss 0.395726                                        LR 0.000060    Time 0.016045    
2023-01-06 16:35:17,256 - Epoch: [10][  210/  246]    Overall Loss 0.394133    Objective Loss 0.394133                                        LR 0.000060    Time 0.015994    
2023-01-06 16:35:17,415 - Epoch: [10][  220/  246]    Overall Loss 0.394067    Objective Loss 0.394067                                        LR 0.000060    Time 0.015988    
2023-01-06 16:35:17,583 - Epoch: [10][  230/  246]    Overall Loss 0.394262    Objective Loss 0.394262                                        LR 0.000060    Time 0.016023    
2023-01-06 16:35:17,739 - Epoch: [10][  240/  246]    Overall Loss 0.394664    Objective Loss 0.394664                                        LR 0.000060    Time 0.016003    
2023-01-06 16:35:17,802 - Epoch: [10][  246/  246]    Overall Loss 0.394405    Objective Loss 0.394405    Top1 87.081340    LR 0.000060    Time 0.015870    
2023-01-06 16:35:17,974 - --- validate (epoch=10)-----------
2023-01-06 16:35:17,975 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:18,425 - Epoch: [10][   10/   28]    Loss 0.384781    Top1 86.093750    
2023-01-06 16:35:18,535 - Epoch: [10][   20/   28]    Loss 0.391358    Top1 85.878906    
2023-01-06 16:35:18,584 - Epoch: [10][   28/   28]    Loss 0.384493    Top1 85.971944    
2023-01-06 16:35:18,714 - ==> Top1: 85.972    Loss: 0.384

2023-01-06 16:35:18,714 - ==> Confusion:
[[  88    0  351]
 [   3   17  582]
 [  33   11 5901]]

2023-01-06 16:35:18,715 - ==> Best [Top1: 85.972   Sparsity:0.00   Params: 151104 on epoch: 10]
2023-01-06 16:35:18,715 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:18,721 - 

2023-01-06 16:35:18,721 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:19,250 - Epoch: [11][   10/  246]    Overall Loss 0.390208    Objective Loss 0.390208                                        LR 0.000060    Time 0.052865    
2023-01-06 16:35:19,408 - Epoch: [11][   20/  246]    Overall Loss 0.395734    Objective Loss 0.395734                                        LR 0.000060    Time 0.034303    
2023-01-06 16:35:19,560 - Epoch: [11][   30/  246]    Overall Loss 0.387783    Objective Loss 0.387783                                        LR 0.000060    Time 0.027937    
2023-01-06 16:35:19,712 - Epoch: [11][   40/  246]    Overall Loss 0.393716    Objective Loss 0.393716                                        LR 0.000060    Time 0.024724    
2023-01-06 16:35:19,862 - Epoch: [11][   50/  246]    Overall Loss 0.392659    Objective Loss 0.392659                                        LR 0.000060    Time 0.022770    
2023-01-06 16:35:20,013 - Epoch: [11][   60/  246]    Overall Loss 0.393086    Objective Loss 0.393086                                        LR 0.000060    Time 0.021498    
2023-01-06 16:35:20,162 - Epoch: [11][   70/  246]    Overall Loss 0.391287    Objective Loss 0.391287                                        LR 0.000060    Time 0.020538    
2023-01-06 16:35:20,312 - Epoch: [11][   80/  246]    Overall Loss 0.391544    Objective Loss 0.391544                                        LR 0.000060    Time 0.019852    
2023-01-06 16:35:20,454 - Epoch: [11][   90/  246]    Overall Loss 0.390756    Objective Loss 0.390756                                        LR 0.000060    Time 0.019217    
2023-01-06 16:35:20,611 - Epoch: [11][  100/  246]    Overall Loss 0.390979    Objective Loss 0.390979                                        LR 0.000060    Time 0.018855    
2023-01-06 16:35:20,756 - Epoch: [11][  110/  246]    Overall Loss 0.389835    Objective Loss 0.389835                                        LR 0.000060    Time 0.018463    
2023-01-06 16:35:20,902 - Epoch: [11][  120/  246]    Overall Loss 0.389079    Objective Loss 0.389079                                        LR 0.000060    Time 0.018134    
2023-01-06 16:35:21,045 - Epoch: [11][  130/  246]    Overall Loss 0.389997    Objective Loss 0.389997                                        LR 0.000060    Time 0.017834    
2023-01-06 16:35:21,193 - Epoch: [11][  140/  246]    Overall Loss 0.389896    Objective Loss 0.389896                                        LR 0.000060    Time 0.017618    
2023-01-06 16:35:21,332 - Epoch: [11][  150/  246]    Overall Loss 0.388843    Objective Loss 0.388843                                        LR 0.000060    Time 0.017365    
2023-01-06 16:35:21,488 - Epoch: [11][  160/  246]    Overall Loss 0.389966    Objective Loss 0.389966                                        LR 0.000060    Time 0.017254    
2023-01-06 16:35:21,647 - Epoch: [11][  170/  246]    Overall Loss 0.390707    Objective Loss 0.390707                                        LR 0.000060    Time 0.017170    
2023-01-06 16:35:21,815 - Epoch: [11][  180/  246]    Overall Loss 0.392639    Objective Loss 0.392639                                        LR 0.000060    Time 0.017147    
2023-01-06 16:35:21,977 - Epoch: [11][  190/  246]    Overall Loss 0.390546    Objective Loss 0.390546                                        LR 0.000060    Time 0.017096    
2023-01-06 16:35:22,141 - Epoch: [11][  200/  246]    Overall Loss 0.391141    Objective Loss 0.391141                                        LR 0.000060    Time 0.017059    
2023-01-06 16:35:22,303 - Epoch: [11][  210/  246]    Overall Loss 0.391075    Objective Loss 0.391075                                        LR 0.000060    Time 0.017016    
2023-01-06 16:35:22,457 - Epoch: [11][  220/  246]    Overall Loss 0.390127    Objective Loss 0.390127                                        LR 0.000060    Time 0.016939    
2023-01-06 16:35:22,604 - Epoch: [11][  230/  246]    Overall Loss 0.390150    Objective Loss 0.390150                                        LR 0.000060    Time 0.016840    
2023-01-06 16:35:22,765 - Epoch: [11][  240/  246]    Overall Loss 0.389456    Objective Loss 0.389456                                        LR 0.000060    Time 0.016811    
2023-01-06 16:35:22,833 - Epoch: [11][  246/  246]    Overall Loss 0.388494    Objective Loss 0.388494    Top1 87.081340    LR 0.000060    Time 0.016676    
2023-01-06 16:35:22,966 - --- validate (epoch=11)-----------
2023-01-06 16:35:22,966 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:23,380 - Epoch: [11][   10/   28]    Loss 0.383002    Top1 86.367188    
2023-01-06 16:35:23,474 - Epoch: [11][   20/   28]    Loss 0.382958    Top1 86.230469    
2023-01-06 16:35:23,525 - Epoch: [11][   28/   28]    Loss 0.383954    Top1 86.186659    
2023-01-06 16:35:23,656 - ==> Top1: 86.187    Loss: 0.384

2023-01-06 16:35:23,656 - ==> Confusion:
[[ 112    4  323]
 [   5   45  552]
 [  52   29 5864]]

2023-01-06 16:35:23,657 - ==> Best [Top1: 86.187   Sparsity:0.00   Params: 151104 on epoch: 11]
2023-01-06 16:35:23,657 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:23,664 - 

2023-01-06 16:35:23,664 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:24,320 - Epoch: [12][   10/  246]    Overall Loss 0.378426    Objective Loss 0.378426                                        LR 0.000060    Time 0.065578    
2023-01-06 16:35:24,473 - Epoch: [12][   20/  246]    Overall Loss 0.381277    Objective Loss 0.381277                                        LR 0.000060    Time 0.040382    
2023-01-06 16:35:24,617 - Epoch: [12][   30/  246]    Overall Loss 0.385550    Objective Loss 0.385550                                        LR 0.000060    Time 0.031719    
2023-01-06 16:35:24,763 - Epoch: [12][   40/  246]    Overall Loss 0.382808    Objective Loss 0.382808                                        LR 0.000060    Time 0.027422    
2023-01-06 16:35:24,918 - Epoch: [12][   50/  246]    Overall Loss 0.384724    Objective Loss 0.384724                                        LR 0.000060    Time 0.025021    
2023-01-06 16:35:25,062 - Epoch: [12][   60/  246]    Overall Loss 0.384651    Objective Loss 0.384651                                        LR 0.000060    Time 0.023250    
2023-01-06 16:35:25,215 - Epoch: [12][   70/  246]    Overall Loss 0.386001    Objective Loss 0.386001                                        LR 0.000060    Time 0.022084    
2023-01-06 16:35:25,356 - Epoch: [12][   80/  246]    Overall Loss 0.389473    Objective Loss 0.389473                                        LR 0.000060    Time 0.021080    
2023-01-06 16:35:25,501 - Epoch: [12][   90/  246]    Overall Loss 0.390377    Objective Loss 0.390377                                        LR 0.000060    Time 0.020335    
2023-01-06 16:35:25,642 - Epoch: [12][  100/  246]    Overall Loss 0.388217    Objective Loss 0.388217                                        LR 0.000060    Time 0.019706    
2023-01-06 16:35:25,784 - Epoch: [12][  110/  246]    Overall Loss 0.390091    Objective Loss 0.390091                                        LR 0.000060    Time 0.019205    
2023-01-06 16:35:25,926 - Epoch: [12][  120/  246]    Overall Loss 0.389229    Objective Loss 0.389229                                        LR 0.000060    Time 0.018782    
2023-01-06 16:35:26,073 - Epoch: [12][  130/  246]    Overall Loss 0.389003    Objective Loss 0.389003                                        LR 0.000060    Time 0.018466    
2023-01-06 16:35:26,211 - Epoch: [12][  140/  246]    Overall Loss 0.388748    Objective Loss 0.388748                                        LR 0.000060    Time 0.018131    
2023-01-06 16:35:26,348 - Epoch: [12][  150/  246]    Overall Loss 0.387826    Objective Loss 0.387826                                        LR 0.000060    Time 0.017830    
2023-01-06 16:35:26,483 - Epoch: [12][  160/  246]    Overall Loss 0.386428    Objective Loss 0.386428                                        LR 0.000060    Time 0.017558    
2023-01-06 16:35:26,619 - Epoch: [12][  170/  246]    Overall Loss 0.386222    Objective Loss 0.386222                                        LR 0.000060    Time 0.017323    
2023-01-06 16:35:26,754 - Epoch: [12][  180/  246]    Overall Loss 0.385345    Objective Loss 0.385345                                        LR 0.000060    Time 0.017110    
2023-01-06 16:35:26,893 - Epoch: [12][  190/  246]    Overall Loss 0.386152    Objective Loss 0.386152                                        LR 0.000060    Time 0.016934    
2023-01-06 16:35:27,026 - Epoch: [12][  200/  246]    Overall Loss 0.386682    Objective Loss 0.386682                                        LR 0.000060    Time 0.016754    
2023-01-06 16:35:27,164 - Epoch: [12][  210/  246]    Overall Loss 0.386832    Objective Loss 0.386832                                        LR 0.000060    Time 0.016611    
2023-01-06 16:35:27,300 - Epoch: [12][  220/  246]    Overall Loss 0.386890    Objective Loss 0.386890                                        LR 0.000060    Time 0.016469    
2023-01-06 16:35:27,435 - Epoch: [12][  230/  246]    Overall Loss 0.386378    Objective Loss 0.386378                                        LR 0.000060    Time 0.016341    
2023-01-06 16:35:27,587 - Epoch: [12][  240/  246]    Overall Loss 0.385734    Objective Loss 0.385734                                        LR 0.000060    Time 0.016289    
2023-01-06 16:35:27,649 - Epoch: [12][  246/  246]    Overall Loss 0.385646    Objective Loss 0.385646    Top1 86.842105    LR 0.000060    Time 0.016146    
2023-01-06 16:35:27,779 - --- validate (epoch=12)-----------
2023-01-06 16:35:27,780 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:28,206 - Epoch: [12][   10/   28]    Loss 0.381460    Top1 85.976562    
2023-01-06 16:35:28,313 - Epoch: [12][   20/   28]    Loss 0.381076    Top1 86.269531    
2023-01-06 16:35:28,364 - Epoch: [12][   28/   28]    Loss 0.378270    Top1 86.401374    
2023-01-06 16:35:28,526 - ==> Top1: 86.401    Loss: 0.378

2023-01-06 16:35:28,526 - ==> Confusion:
[[ 115    3  321]
 [   2   41  559]
 [  44   21 5880]]

2023-01-06 16:35:28,527 - ==> Best [Top1: 86.401   Sparsity:0.00   Params: 151104 on epoch: 12]
2023-01-06 16:35:28,527 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:28,533 - 

2023-01-06 16:35:28,533 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:29,187 - Epoch: [13][   10/  246]    Overall Loss 0.393158    Objective Loss 0.393158                                        LR 0.000060    Time 0.065326    
2023-01-06 16:35:29,339 - Epoch: [13][   20/  246]    Overall Loss 0.391466    Objective Loss 0.391466                                        LR 0.000060    Time 0.040255    
2023-01-06 16:35:29,492 - Epoch: [13][   30/  246]    Overall Loss 0.396469    Objective Loss 0.396469                                        LR 0.000060    Time 0.031913    
2023-01-06 16:35:29,637 - Epoch: [13][   40/  246]    Overall Loss 0.393857    Objective Loss 0.393857                                        LR 0.000060    Time 0.027541    
2023-01-06 16:35:29,786 - Epoch: [13][   50/  246]    Overall Loss 0.391250    Objective Loss 0.391250                                        LR 0.000060    Time 0.025012    
2023-01-06 16:35:29,928 - Epoch: [13][   60/  246]    Overall Loss 0.388450    Objective Loss 0.388450                                        LR 0.000060    Time 0.023192    
2023-01-06 16:35:30,075 - Epoch: [13][   70/  246]    Overall Loss 0.384574    Objective Loss 0.384574                                        LR 0.000060    Time 0.021978    
2023-01-06 16:35:30,219 - Epoch: [13][   80/  246]    Overall Loss 0.385979    Objective Loss 0.385979                                        LR 0.000060    Time 0.021027    
2023-01-06 16:35:30,365 - Epoch: [13][   90/  246]    Overall Loss 0.384150    Objective Loss 0.384150                                        LR 0.000060    Time 0.020310    
2023-01-06 16:35:30,505 - Epoch: [13][  100/  246]    Overall Loss 0.382923    Objective Loss 0.382923                                        LR 0.000060    Time 0.019669    
2023-01-06 16:35:30,652 - Epoch: [13][  110/  246]    Overall Loss 0.379614    Objective Loss 0.379614                                        LR 0.000060    Time 0.019210    
2023-01-06 16:35:30,803 - Epoch: [13][  120/  246]    Overall Loss 0.379462    Objective Loss 0.379462                                        LR 0.000060    Time 0.018870    
2023-01-06 16:35:30,957 - Epoch: [13][  130/  246]    Overall Loss 0.379643    Objective Loss 0.379643                                        LR 0.000060    Time 0.018599    
2023-01-06 16:35:31,108 - Epoch: [13][  140/  246]    Overall Loss 0.380054    Objective Loss 0.380054                                        LR 0.000060    Time 0.018348    
2023-01-06 16:35:31,262 - Epoch: [13][  150/  246]    Overall Loss 0.380051    Objective Loss 0.380051                                        LR 0.000060    Time 0.018149    
2023-01-06 16:35:31,414 - Epoch: [13][  160/  246]    Overall Loss 0.378868    Objective Loss 0.378868                                        LR 0.000060    Time 0.017958    
2023-01-06 16:35:31,567 - Epoch: [13][  170/  246]    Overall Loss 0.379570    Objective Loss 0.379570                                        LR 0.000060    Time 0.017804    
2023-01-06 16:35:31,719 - Epoch: [13][  180/  246]    Overall Loss 0.380500    Objective Loss 0.380500                                        LR 0.000060    Time 0.017658    
2023-01-06 16:35:31,873 - Epoch: [13][  190/  246]    Overall Loss 0.380674    Objective Loss 0.380674                                        LR 0.000060    Time 0.017536    
2023-01-06 16:35:32,021 - Epoch: [13][  200/  246]    Overall Loss 0.381969    Objective Loss 0.381969                                        LR 0.000060    Time 0.017395    
2023-01-06 16:35:32,165 - Epoch: [13][  210/  246]    Overall Loss 0.381478    Objective Loss 0.381478                                        LR 0.000060    Time 0.017254    
2023-01-06 16:35:32,311 - Epoch: [13][  220/  246]    Overall Loss 0.380957    Objective Loss 0.380957                                        LR 0.000060    Time 0.017129    
2023-01-06 16:35:32,450 - Epoch: [13][  230/  246]    Overall Loss 0.380704    Objective Loss 0.380704                                        LR 0.000060    Time 0.016988    
2023-01-06 16:35:32,610 - Epoch: [13][  240/  246]    Overall Loss 0.380210    Objective Loss 0.380210                                        LR 0.000060    Time 0.016945    
2023-01-06 16:35:32,676 - Epoch: [13][  246/  246]    Overall Loss 0.380211    Objective Loss 0.380211    Top1 88.277512    LR 0.000060    Time 0.016799    
2023-01-06 16:35:32,805 - --- validate (epoch=13)-----------
2023-01-06 16:35:32,805 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:33,238 - Epoch: [13][   10/   28]    Loss 0.373799    Top1 86.406250    
2023-01-06 16:35:33,346 - Epoch: [13][   20/   28]    Loss 0.371205    Top1 86.562500    
2023-01-06 16:35:33,395 - Epoch: [13][   28/   28]    Loss 0.375029    Top1 86.372745    
2023-01-06 16:35:33,515 - ==> Top1: 86.373    Loss: 0.375

2023-01-06 16:35:33,516 - ==> Confusion:
[[ 138    3  298]
 [   5   56  541]
 [  80   25 5840]]

2023-01-06 16:35:33,517 - ==> Best [Top1: 86.401   Sparsity:0.00   Params: 151104 on epoch: 12]
2023-01-06 16:35:33,517 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:33,523 - 

2023-01-06 16:35:33,523 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:34,077 - Epoch: [14][   10/  246]    Overall Loss 0.368137    Objective Loss 0.368137                                        LR 0.000060    Time 0.055331    
2023-01-06 16:35:34,235 - Epoch: [14][   20/  246]    Overall Loss 0.383779    Objective Loss 0.383779                                        LR 0.000060    Time 0.035566    
2023-01-06 16:35:34,385 - Epoch: [14][   30/  246]    Overall Loss 0.387030    Objective Loss 0.387030                                        LR 0.000060    Time 0.028682    
2023-01-06 16:35:34,555 - Epoch: [14][   40/  246]    Overall Loss 0.386738    Objective Loss 0.386738                                        LR 0.000060    Time 0.025753    
2023-01-06 16:35:34,717 - Epoch: [14][   50/  246]    Overall Loss 0.384392    Objective Loss 0.384392                                        LR 0.000060    Time 0.023830    
2023-01-06 16:35:34,872 - Epoch: [14][   60/  246]    Overall Loss 0.382127    Objective Loss 0.382127                                        LR 0.000060    Time 0.022442    
2023-01-06 16:35:35,033 - Epoch: [14][   70/  246]    Overall Loss 0.383086    Objective Loss 0.383086                                        LR 0.000060    Time 0.021522    
2023-01-06 16:35:35,195 - Epoch: [14][   80/  246]    Overall Loss 0.383186    Objective Loss 0.383186                                        LR 0.000060    Time 0.020856    
2023-01-06 16:35:35,351 - Epoch: [14][   90/  246]    Overall Loss 0.378746    Objective Loss 0.378746                                        LR 0.000060    Time 0.020260    
2023-01-06 16:35:35,507 - Epoch: [14][  100/  246]    Overall Loss 0.380721    Objective Loss 0.380721                                        LR 0.000060    Time 0.019792    
2023-01-06 16:35:35,641 - Epoch: [14][  110/  246]    Overall Loss 0.380062    Objective Loss 0.380062                                        LR 0.000060    Time 0.019206    
2023-01-06 16:35:35,798 - Epoch: [14][  120/  246]    Overall Loss 0.378173    Objective Loss 0.378173                                        LR 0.000060    Time 0.018910    
2023-01-06 16:35:35,959 - Epoch: [14][  130/  246]    Overall Loss 0.375799    Objective Loss 0.375799                                        LR 0.000060    Time 0.018689    
2023-01-06 16:35:36,109 - Epoch: [14][  140/  246]    Overall Loss 0.375460    Objective Loss 0.375460                                        LR 0.000060    Time 0.018421    
2023-01-06 16:35:36,261 - Epoch: [14][  150/  246]    Overall Loss 0.375902    Objective Loss 0.375902                                        LR 0.000060    Time 0.018208    
2023-01-06 16:35:36,418 - Epoch: [14][  160/  246]    Overall Loss 0.373896    Objective Loss 0.373896                                        LR 0.000060    Time 0.018046    
2023-01-06 16:35:36,571 - Epoch: [14][  170/  246]    Overall Loss 0.373585    Objective Loss 0.373585                                        LR 0.000060    Time 0.017881    
2023-01-06 16:35:36,726 - Epoch: [14][  180/  246]    Overall Loss 0.374207    Objective Loss 0.374207                                        LR 0.000060    Time 0.017748    
2023-01-06 16:35:36,869 - Epoch: [14][  190/  246]    Overall Loss 0.375447    Objective Loss 0.375447                                        LR 0.000060    Time 0.017564    
2023-01-06 16:35:37,003 - Epoch: [14][  200/  246]    Overall Loss 0.375890    Objective Loss 0.375890                                        LR 0.000060    Time 0.017354    
2023-01-06 16:35:37,153 - Epoch: [14][  210/  246]    Overall Loss 0.375378    Objective Loss 0.375378                                        LR 0.000060    Time 0.017239    
2023-01-06 16:35:37,306 - Epoch: [14][  220/  246]    Overall Loss 0.375248    Objective Loss 0.375248                                        LR 0.000060    Time 0.017148    
2023-01-06 16:35:37,449 - Epoch: [14][  230/  246]    Overall Loss 0.375468    Objective Loss 0.375468                                        LR 0.000060    Time 0.017023    
2023-01-06 16:35:37,600 - Epoch: [14][  240/  246]    Overall Loss 0.376250    Objective Loss 0.376250                                        LR 0.000060    Time 0.016944    
2023-01-06 16:35:37,666 - Epoch: [14][  246/  246]    Overall Loss 0.375802    Objective Loss 0.375802    Top1 86.363636    LR 0.000060    Time 0.016796    
2023-01-06 16:35:37,802 - --- validate (epoch=14)-----------
2023-01-06 16:35:37,802 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:38,218 - Epoch: [14][   10/   28]    Loss 0.360803    Top1 86.562500    
2023-01-06 16:35:38,316 - Epoch: [14][   20/   28]    Loss 0.374468    Top1 85.976562    
2023-01-06 16:35:38,365 - Epoch: [14][   28/   28]    Loss 0.372283    Top1 86.215288    
2023-01-06 16:35:38,496 - ==> Top1: 86.215    Loss: 0.372

2023-01-06 16:35:38,496 - ==> Confusion:
[[ 101    2  336]
 [   5   51  546]
 [  44   30 5871]]

2023-01-06 16:35:38,497 - ==> Best [Top1: 86.401   Sparsity:0.00   Params: 151104 on epoch: 12]
2023-01-06 16:35:38,497 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:38,503 - 

2023-01-06 16:35:38,503 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:39,143 - Epoch: [15][   10/  246]    Overall Loss 0.369769    Objective Loss 0.369769                                        LR 0.000060    Time 0.063954    
2023-01-06 16:35:39,263 - Epoch: [15][   20/  246]    Overall Loss 0.372276    Objective Loss 0.372276                                        LR 0.000060    Time 0.037984    
2023-01-06 16:35:39,386 - Epoch: [15][   30/  246]    Overall Loss 0.375634    Objective Loss 0.375634                                        LR 0.000060    Time 0.029381    
2023-01-06 16:35:39,514 - Epoch: [15][   40/  246]    Overall Loss 0.375424    Objective Loss 0.375424                                        LR 0.000060    Time 0.025238    
2023-01-06 16:35:39,651 - Epoch: [15][   50/  246]    Overall Loss 0.378138    Objective Loss 0.378138                                        LR 0.000060    Time 0.022911    
2023-01-06 16:35:39,784 - Epoch: [15][   60/  246]    Overall Loss 0.376430    Objective Loss 0.376430                                        LR 0.000060    Time 0.021305    
2023-01-06 16:35:39,913 - Epoch: [15][   70/  246]    Overall Loss 0.375222    Objective Loss 0.375222                                        LR 0.000060    Time 0.020095    
2023-01-06 16:35:40,042 - Epoch: [15][   80/  246]    Overall Loss 0.372229    Objective Loss 0.372229                                        LR 0.000060    Time 0.019193    
2023-01-06 16:35:40,169 - Epoch: [15][   90/  246]    Overall Loss 0.373237    Objective Loss 0.373237                                        LR 0.000060    Time 0.018472    
2023-01-06 16:35:40,296 - Epoch: [15][  100/  246]    Overall Loss 0.374475    Objective Loss 0.374475                                        LR 0.000060    Time 0.017882    
2023-01-06 16:35:40,426 - Epoch: [15][  110/  246]    Overall Loss 0.374292    Objective Loss 0.374292                                        LR 0.000060    Time 0.017439    
2023-01-06 16:35:40,550 - Epoch: [15][  120/  246]    Overall Loss 0.374696    Objective Loss 0.374696                                        LR 0.000060    Time 0.017013    
2023-01-06 16:35:40,668 - Epoch: [15][  130/  246]    Overall Loss 0.372791    Objective Loss 0.372791                                        LR 0.000060    Time 0.016614    
2023-01-06 16:35:40,787 - Epoch: [15][  140/  246]    Overall Loss 0.372969    Objective Loss 0.372969                                        LR 0.000060    Time 0.016270    
2023-01-06 16:35:40,909 - Epoch: [15][  150/  246]    Overall Loss 0.373553    Objective Loss 0.373553                                        LR 0.000060    Time 0.015996    
2023-01-06 16:35:41,027 - Epoch: [15][  160/  246]    Overall Loss 0.371389    Objective Loss 0.371389                                        LR 0.000060    Time 0.015734    
2023-01-06 16:35:41,145 - Epoch: [15][  170/  246]    Overall Loss 0.371852    Objective Loss 0.371852                                        LR 0.000060    Time 0.015498    
2023-01-06 16:35:41,265 - Epoch: [15][  180/  246]    Overall Loss 0.370719    Objective Loss 0.370719                                        LR 0.000060    Time 0.015302    
2023-01-06 16:35:41,387 - Epoch: [15][  190/  246]    Overall Loss 0.371521    Objective Loss 0.371521                                        LR 0.000060    Time 0.015137    
2023-01-06 16:35:41,509 - Epoch: [15][  200/  246]    Overall Loss 0.371949    Objective Loss 0.371949                                        LR 0.000060    Time 0.014990    
2023-01-06 16:35:41,632 - Epoch: [15][  210/  246]    Overall Loss 0.371633    Objective Loss 0.371633                                        LR 0.000060    Time 0.014861    
2023-01-06 16:35:41,752 - Epoch: [15][  220/  246]    Overall Loss 0.372045    Objective Loss 0.372045                                        LR 0.000060    Time 0.014730    
2023-01-06 16:35:41,881 - Epoch: [15][  230/  246]    Overall Loss 0.372451    Objective Loss 0.372451                                        LR 0.000060    Time 0.014648    
2023-01-06 16:35:42,022 - Epoch: [15][  240/  246]    Overall Loss 0.372021    Objective Loss 0.372021                                        LR 0.000060    Time 0.014624    
2023-01-06 16:35:42,085 - Epoch: [15][  246/  246]    Overall Loss 0.371919    Objective Loss 0.371919    Top1 89.473684    LR 0.000060    Time 0.014522    
2023-01-06 16:35:42,253 - --- validate (epoch=15)-----------
2023-01-06 16:35:42,253 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:42,690 - Epoch: [15][   10/   28]    Loss 0.377108    Top1 86.289062    
2023-01-06 16:35:42,793 - Epoch: [15][   20/   28]    Loss 0.377575    Top1 86.093750    
2023-01-06 16:35:42,844 - Epoch: [15][   28/   28]    Loss 0.372616    Top1 86.430003    
2023-01-06 16:35:42,983 - ==> Top1: 86.430    Loss: 0.373

2023-01-06 16:35:42,983 - ==> Confusion:
[[ 140    0  299]
 [  10   44  548]
 [  68   23 5854]]

2023-01-06 16:35:42,984 - ==> Best [Top1: 86.430   Sparsity:0.00   Params: 151104 on epoch: 15]
2023-01-06 16:35:42,984 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:42,991 - 

2023-01-06 16:35:42,991 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:43,506 - Epoch: [16][   10/  246]    Overall Loss 0.384555    Objective Loss 0.384555                                        LR 0.000060    Time 0.051456    
2023-01-06 16:35:43,631 - Epoch: [16][   20/  246]    Overall Loss 0.375215    Objective Loss 0.375215                                        LR 0.000060    Time 0.031926    
2023-01-06 16:35:43,753 - Epoch: [16][   30/  246]    Overall Loss 0.370553    Objective Loss 0.370553                                        LR 0.000060    Time 0.025367    
2023-01-06 16:35:43,876 - Epoch: [16][   40/  246]    Overall Loss 0.367026    Objective Loss 0.367026                                        LR 0.000060    Time 0.022086    
2023-01-06 16:35:44,007 - Epoch: [16][   50/  246]    Overall Loss 0.370518    Objective Loss 0.370518                                        LR 0.000060    Time 0.020275    
2023-01-06 16:35:44,129 - Epoch: [16][   60/  246]    Overall Loss 0.367001    Objective Loss 0.367001                                        LR 0.000060    Time 0.018926    
2023-01-06 16:35:44,251 - Epoch: [16][   70/  246]    Overall Loss 0.365391    Objective Loss 0.365391                                        LR 0.000060    Time 0.017961    
2023-01-06 16:35:44,371 - Epoch: [16][   80/  246]    Overall Loss 0.363993    Objective Loss 0.363993                                        LR 0.000060    Time 0.017202    
2023-01-06 16:35:44,488 - Epoch: [16][   90/  246]    Overall Loss 0.365182    Objective Loss 0.365182                                        LR 0.000060    Time 0.016597    
2023-01-06 16:35:44,613 - Epoch: [16][  100/  246]    Overall Loss 0.367069    Objective Loss 0.367069                                        LR 0.000060    Time 0.016179    
2023-01-06 16:35:44,732 - Epoch: [16][  110/  246]    Overall Loss 0.367471    Objective Loss 0.367471                                        LR 0.000060    Time 0.015788    
2023-01-06 16:35:44,859 - Epoch: [16][  120/  246]    Overall Loss 0.368306    Objective Loss 0.368306                                        LR 0.000060    Time 0.015528    
2023-01-06 16:35:44,982 - Epoch: [16][  130/  246]    Overall Loss 0.368005    Objective Loss 0.368005                                        LR 0.000060    Time 0.015276    
2023-01-06 16:35:45,116 - Epoch: [16][  140/  246]    Overall Loss 0.368478    Objective Loss 0.368478                                        LR 0.000060    Time 0.015138    
2023-01-06 16:35:45,248 - Epoch: [16][  150/  246]    Overall Loss 0.368418    Objective Loss 0.368418                                        LR 0.000060    Time 0.015007    
2023-01-06 16:35:45,380 - Epoch: [16][  160/  246]    Overall Loss 0.369120    Objective Loss 0.369120                                        LR 0.000060    Time 0.014891    
2023-01-06 16:35:45,512 - Epoch: [16][  170/  246]    Overall Loss 0.369518    Objective Loss 0.369518                                        LR 0.000060    Time 0.014780    
2023-01-06 16:35:45,642 - Epoch: [16][  180/  246]    Overall Loss 0.370012    Objective Loss 0.370012                                        LR 0.000060    Time 0.014668    
2023-01-06 16:35:45,775 - Epoch: [16][  190/  246]    Overall Loss 0.368338    Objective Loss 0.368338                                        LR 0.000060    Time 0.014597    
2023-01-06 16:35:45,904 - Epoch: [16][  200/  246]    Overall Loss 0.368415    Objective Loss 0.368415                                        LR 0.000060    Time 0.014508    
2023-01-06 16:35:46,038 - Epoch: [16][  210/  246]    Overall Loss 0.367619    Objective Loss 0.367619                                        LR 0.000060    Time 0.014454    
2023-01-06 16:35:46,178 - Epoch: [16][  220/  246]    Overall Loss 0.367087    Objective Loss 0.367087                                        LR 0.000060    Time 0.014432    
2023-01-06 16:35:46,314 - Epoch: [16][  230/  246]    Overall Loss 0.367211    Objective Loss 0.367211                                        LR 0.000060    Time 0.014394    
2023-01-06 16:35:46,468 - Epoch: [16][  240/  246]    Overall Loss 0.368096    Objective Loss 0.368096                                        LR 0.000060    Time 0.014425    
2023-01-06 16:35:46,529 - Epoch: [16][  246/  246]    Overall Loss 0.368023    Objective Loss 0.368023    Top1 85.406699    LR 0.000060    Time 0.014323    
2023-01-06 16:35:46,657 - --- validate (epoch=16)-----------
2023-01-06 16:35:46,657 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:47,079 - Epoch: [16][   10/   28]    Loss 0.359760    Top1 87.031250    
2023-01-06 16:35:47,175 - Epoch: [16][   20/   28]    Loss 0.359987    Top1 86.835938    
2023-01-06 16:35:47,225 - Epoch: [16][   28/   28]    Loss 0.360055    Top1 86.830804    
2023-01-06 16:35:47,363 - ==> Top1: 86.831    Loss: 0.360

2023-01-06 16:35:47,364 - ==> Confusion:
[[ 129    2  308]
 [   6   64  532]
 [  49   23 5873]]

2023-01-06 16:35:47,365 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 16:35:47,365 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:47,371 - 

2023-01-06 16:35:47,372 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:48,030 - Epoch: [17][   10/  246]    Overall Loss 0.375673    Objective Loss 0.375673                                        LR 0.000060    Time 0.065826    
2023-01-06 16:35:48,166 - Epoch: [17][   20/  246]    Overall Loss 0.385568    Objective Loss 0.385568                                        LR 0.000060    Time 0.039686    
2023-01-06 16:35:48,303 - Epoch: [17][   30/  246]    Overall Loss 0.395630    Objective Loss 0.395630                                        LR 0.000060    Time 0.030990    
2023-01-06 16:35:48,440 - Epoch: [17][   40/  246]    Overall Loss 0.397446    Objective Loss 0.397446                                        LR 0.000060    Time 0.026669    
2023-01-06 16:35:48,600 - Epoch: [17][   50/  246]    Overall Loss 0.398044    Objective Loss 0.398044                                        LR 0.000060    Time 0.024524    
2023-01-06 16:35:48,756 - Epoch: [17][   60/  246]    Overall Loss 0.398667    Objective Loss 0.398667                                        LR 0.000060    Time 0.023034    
2023-01-06 16:35:48,916 - Epoch: [17][   70/  246]    Overall Loss 0.395849    Objective Loss 0.395849                                        LR 0.000060    Time 0.022024    
2023-01-06 16:35:49,072 - Epoch: [17][   80/  246]    Overall Loss 0.396573    Objective Loss 0.396573                                        LR 0.000060    Time 0.021205    
2023-01-06 16:35:49,226 - Epoch: [17][   90/  246]    Overall Loss 0.394346    Objective Loss 0.394346                                        LR 0.000060    Time 0.020558    
2023-01-06 16:35:49,367 - Epoch: [17][  100/  246]    Overall Loss 0.394897    Objective Loss 0.394897                                        LR 0.000060    Time 0.019910    
2023-01-06 16:35:49,498 - Epoch: [17][  110/  246]    Overall Loss 0.393483    Objective Loss 0.393483                                        LR 0.000060    Time 0.019289    
2023-01-06 16:35:49,637 - Epoch: [17][  120/  246]    Overall Loss 0.392744    Objective Loss 0.392744                                        LR 0.000060    Time 0.018841    
2023-01-06 16:35:49,774 - Epoch: [17][  130/  246]    Overall Loss 0.393690    Objective Loss 0.393690                                        LR 0.000060    Time 0.018442    
2023-01-06 16:35:49,913 - Epoch: [17][  140/  246]    Overall Loss 0.393802    Objective Loss 0.393802                                        LR 0.000060    Time 0.018115    
2023-01-06 16:35:50,048 - Epoch: [17][  150/  246]    Overall Loss 0.394047    Objective Loss 0.394047                                        LR 0.000060    Time 0.017804    
2023-01-06 16:35:50,179 - Epoch: [17][  160/  246]    Overall Loss 0.393646    Objective Loss 0.393646                                        LR 0.000060    Time 0.017505    
2023-01-06 16:35:50,308 - Epoch: [17][  170/  246]    Overall Loss 0.393276    Objective Loss 0.393276                                        LR 0.000060    Time 0.017234    
2023-01-06 16:35:50,439 - Epoch: [17][  180/  246]    Overall Loss 0.391963    Objective Loss 0.391963                                        LR 0.000060    Time 0.017002    
2023-01-06 16:35:50,573 - Epoch: [17][  190/  246]    Overall Loss 0.390476    Objective Loss 0.390476                                        LR 0.000060    Time 0.016808    
2023-01-06 16:35:50,701 - Epoch: [17][  200/  246]    Overall Loss 0.390634    Objective Loss 0.390634                                        LR 0.000060    Time 0.016608    
2023-01-06 16:35:50,829 - Epoch: [17][  210/  246]    Overall Loss 0.390415    Objective Loss 0.390415                                        LR 0.000060    Time 0.016425    
2023-01-06 16:35:50,960 - Epoch: [17][  220/  246]    Overall Loss 0.390615    Objective Loss 0.390615                                        LR 0.000060    Time 0.016273    
2023-01-06 16:35:51,092 - Epoch: [17][  230/  246]    Overall Loss 0.390633    Objective Loss 0.390633                                        LR 0.000060    Time 0.016135    
2023-01-06 16:35:51,240 - Epoch: [17][  240/  246]    Overall Loss 0.389865    Objective Loss 0.389865                                        LR 0.000060    Time 0.016081    
2023-01-06 16:35:51,303 - Epoch: [17][  246/  246]    Overall Loss 0.389551    Objective Loss 0.389551    Top1 86.602871    LR 0.000060    Time 0.015944    
2023-01-06 16:35:51,431 - --- validate (epoch=17)-----------
2023-01-06 16:35:51,431 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:51,866 - Epoch: [17][   10/   28]    Loss 0.376663    Top1 86.445312    
2023-01-06 16:35:51,963 - Epoch: [17][   20/   28]    Loss 0.386708    Top1 85.957031    
2023-01-06 16:35:52,016 - Epoch: [17][   28/   28]    Loss 0.384090    Top1 86.129402    
2023-01-06 16:35:52,164 - ==> Top1: 86.129    Loss: 0.384

2023-01-06 16:35:52,164 - ==> Confusion:
[[  94    0  345]
 [   1   13  588]
 [  31    4 5910]]

2023-01-06 16:35:52,165 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 16:35:52,165 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:52,170 - 

2023-01-06 16:35:52,170 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:52,691 - Epoch: [18][   10/  246]    Overall Loss 0.387461    Objective Loss 0.387461                                        LR 0.000060    Time 0.052041    
2023-01-06 16:35:52,816 - Epoch: [18][   20/  246]    Overall Loss 0.378368    Objective Loss 0.378368                                        LR 0.000060    Time 0.032239    
2023-01-06 16:35:52,944 - Epoch: [18][   30/  246]    Overall Loss 0.376971    Objective Loss 0.376971                                        LR 0.000060    Time 0.025745    
2023-01-06 16:35:53,067 - Epoch: [18][   40/  246]    Overall Loss 0.377750    Objective Loss 0.377750                                        LR 0.000060    Time 0.022379    
2023-01-06 16:35:53,186 - Epoch: [18][   50/  246]    Overall Loss 0.378178    Objective Loss 0.378178                                        LR 0.000060    Time 0.020262    
2023-01-06 16:35:53,304 - Epoch: [18][   60/  246]    Overall Loss 0.377939    Objective Loss 0.377939                                        LR 0.000060    Time 0.018822    
2023-01-06 16:35:53,426 - Epoch: [18][   70/  246]    Overall Loss 0.378541    Objective Loss 0.378541                                        LR 0.000060    Time 0.017876    
2023-01-06 16:35:53,548 - Epoch: [18][   80/  246]    Overall Loss 0.379385    Objective Loss 0.379385                                        LR 0.000060    Time 0.017160    
2023-01-06 16:35:53,686 - Epoch: [18][   90/  246]    Overall Loss 0.380123    Objective Loss 0.380123                                        LR 0.000060    Time 0.016776    
2023-01-06 16:35:53,819 - Epoch: [18][  100/  246]    Overall Loss 0.380768    Objective Loss 0.380768                                        LR 0.000060    Time 0.016425    
2023-01-06 16:35:53,954 - Epoch: [18][  110/  246]    Overall Loss 0.382877    Objective Loss 0.382877                                        LR 0.000060    Time 0.016158    
2023-01-06 16:35:54,087 - Epoch: [18][  120/  246]    Overall Loss 0.382285    Objective Loss 0.382285                                        LR 0.000060    Time 0.015918    
2023-01-06 16:35:54,222 - Epoch: [18][  130/  246]    Overall Loss 0.382469    Objective Loss 0.382469                                        LR 0.000060    Time 0.015723    
2023-01-06 16:35:54,355 - Epoch: [18][  140/  246]    Overall Loss 0.381121    Objective Loss 0.381121                                        LR 0.000060    Time 0.015550    
2023-01-06 16:35:54,489 - Epoch: [18][  150/  246]    Overall Loss 0.380391    Objective Loss 0.380391                                        LR 0.000060    Time 0.015407    
2023-01-06 16:35:54,623 - Epoch: [18][  160/  246]    Overall Loss 0.380988    Objective Loss 0.380988                                        LR 0.000060    Time 0.015275    
2023-01-06 16:35:54,771 - Epoch: [18][  170/  246]    Overall Loss 0.380276    Objective Loss 0.380276                                        LR 0.000060    Time 0.015243    
2023-01-06 16:35:54,914 - Epoch: [18][  180/  246]    Overall Loss 0.379318    Objective Loss 0.379318                                        LR 0.000060    Time 0.015188    
2023-01-06 16:35:55,074 - Epoch: [18][  190/  246]    Overall Loss 0.379400    Objective Loss 0.379400                                        LR 0.000060    Time 0.015229    
2023-01-06 16:35:55,220 - Epoch: [18][  200/  246]    Overall Loss 0.379725    Objective Loss 0.379725                                        LR 0.000060    Time 0.015196    
2023-01-06 16:35:55,367 - Epoch: [18][  210/  246]    Overall Loss 0.379796    Objective Loss 0.379796                                        LR 0.000060    Time 0.015171    
2023-01-06 16:35:55,507 - Epoch: [18][  220/  246]    Overall Loss 0.380292    Objective Loss 0.380292                                        LR 0.000060    Time 0.015117    
2023-01-06 16:35:55,655 - Epoch: [18][  230/  246]    Overall Loss 0.380575    Objective Loss 0.380575                                        LR 0.000060    Time 0.015102    
2023-01-06 16:35:55,811 - Epoch: [18][  240/  246]    Overall Loss 0.379870    Objective Loss 0.379870                                        LR 0.000060    Time 0.015120    
2023-01-06 16:35:55,874 - Epoch: [18][  246/  246]    Overall Loss 0.379679    Objective Loss 0.379679    Top1 88.995215    LR 0.000060    Time 0.015008    
2023-01-06 16:35:56,006 - --- validate (epoch=18)-----------
2023-01-06 16:35:56,006 - 6986 samples (256 per mini-batch)
2023-01-06 16:35:56,455 - Epoch: [18][   10/   28]    Loss 0.375253    Top1 86.015625    
2023-01-06 16:35:56,560 - Epoch: [18][   20/   28]    Loss 0.371518    Top1 86.406250    
2023-01-06 16:35:56,611 - Epoch: [18][   28/   28]    Loss 0.373605    Top1 86.515889    
2023-01-06 16:35:56,738 - ==> Top1: 86.516    Loss: 0.374

2023-01-06 16:35:56,738 - ==> Confusion:
[[ 106    2  331]
 [   3   38  561]
 [  35   10 5900]]

2023-01-06 16:35:56,739 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 16:35:56,739 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:35:56,744 - 

2023-01-06 16:35:56,744 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:35:57,423 - Epoch: [19][   10/  246]    Overall Loss 0.377543    Objective Loss 0.377543                                        LR 0.000060    Time 0.067870    
2023-01-06 16:35:57,585 - Epoch: [19][   20/  246]    Overall Loss 0.383178    Objective Loss 0.383178                                        LR 0.000060    Time 0.041970    
2023-01-06 16:35:57,742 - Epoch: [19][   30/  246]    Overall Loss 0.372663    Objective Loss 0.372663                                        LR 0.000060    Time 0.033211    
2023-01-06 16:35:57,902 - Epoch: [19][   40/  246]    Overall Loss 0.377641    Objective Loss 0.377641                                        LR 0.000060    Time 0.028890    
2023-01-06 16:35:58,058 - Epoch: [19][   50/  246]    Overall Loss 0.375878    Objective Loss 0.375878                                        LR 0.000060    Time 0.026230    
2023-01-06 16:35:58,212 - Epoch: [19][   60/  246]    Overall Loss 0.377784    Objective Loss 0.377784                                        LR 0.000060    Time 0.024423    
2023-01-06 16:35:58,370 - Epoch: [19][   70/  246]    Overall Loss 0.374746    Objective Loss 0.374746                                        LR 0.000060    Time 0.023183    
2023-01-06 16:35:58,535 - Epoch: [19][   80/  246]    Overall Loss 0.375579    Objective Loss 0.375579                                        LR 0.000060    Time 0.022334    
2023-01-06 16:35:58,698 - Epoch: [19][   90/  246]    Overall Loss 0.374187    Objective Loss 0.374187                                        LR 0.000060    Time 0.021658    
2023-01-06 16:35:58,863 - Epoch: [19][  100/  246]    Overall Loss 0.374911    Objective Loss 0.374911                                        LR 0.000060    Time 0.021140    
2023-01-06 16:35:59,024 - Epoch: [19][  110/  246]    Overall Loss 0.376142    Objective Loss 0.376142                                        LR 0.000060    Time 0.020682    
2023-01-06 16:35:59,187 - Epoch: [19][  120/  246]    Overall Loss 0.375512    Objective Loss 0.375512                                        LR 0.000060    Time 0.020309    
2023-01-06 16:35:59,343 - Epoch: [19][  130/  246]    Overall Loss 0.376893    Objective Loss 0.376893                                        LR 0.000060    Time 0.019943    
2023-01-06 16:35:59,501 - Epoch: [19][  140/  246]    Overall Loss 0.376990    Objective Loss 0.376990                                        LR 0.000060    Time 0.019648    
2023-01-06 16:35:59,657 - Epoch: [19][  150/  246]    Overall Loss 0.377345    Objective Loss 0.377345                                        LR 0.000060    Time 0.019377    
2023-01-06 16:35:59,820 - Epoch: [19][  160/  246]    Overall Loss 0.376393    Objective Loss 0.376393                                        LR 0.000060    Time 0.019181    
2023-01-06 16:35:59,985 - Epoch: [19][  170/  246]    Overall Loss 0.375266    Objective Loss 0.375266                                        LR 0.000060    Time 0.019022    
2023-01-06 16:36:00,159 - Epoch: [19][  180/  246]    Overall Loss 0.374302    Objective Loss 0.374302                                        LR 0.000060    Time 0.018926    
2023-01-06 16:36:00,334 - Epoch: [19][  190/  246]    Overall Loss 0.374895    Objective Loss 0.374895                                        LR 0.000060    Time 0.018850    
2023-01-06 16:36:00,508 - Epoch: [19][  200/  246]    Overall Loss 0.374598    Objective Loss 0.374598                                        LR 0.000060    Time 0.018775    
2023-01-06 16:36:00,681 - Epoch: [19][  210/  246]    Overall Loss 0.375206    Objective Loss 0.375206                                        LR 0.000060    Time 0.018704    
2023-01-06 16:36:00,853 - Epoch: [19][  220/  246]    Overall Loss 0.373718    Objective Loss 0.373718                                        LR 0.000060    Time 0.018632    
2023-01-06 16:36:01,018 - Epoch: [19][  230/  246]    Overall Loss 0.373524    Objective Loss 0.373524                                        LR 0.000060    Time 0.018541    
2023-01-06 16:36:01,195 - Epoch: [19][  240/  246]    Overall Loss 0.374263    Objective Loss 0.374263                                        LR 0.000060    Time 0.018501    
2023-01-06 16:36:01,263 - Epoch: [19][  246/  246]    Overall Loss 0.374102    Objective Loss 0.374102    Top1 88.038278    LR 0.000060    Time 0.018328    
2023-01-06 16:36:01,404 - --- validate (epoch=19)-----------
2023-01-06 16:36:01,404 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:01,825 - Epoch: [19][   10/   28]    Loss 0.363592    Top1 87.148438    
2023-01-06 16:36:01,923 - Epoch: [19][   20/   28]    Loss 0.371403    Top1 86.562500    
2023-01-06 16:36:01,976 - Epoch: [19][   28/   28]    Loss 0.368704    Top1 86.544518    
2023-01-06 16:36:02,092 - ==> Top1: 86.545    Loss: 0.369

2023-01-06 16:36:02,092 - ==> Confusion:
[[ 120    2  317]
 [   6   43  553]
 [  46   16 5883]]

2023-01-06 16:36:02,093 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 16:36:02,093 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:02,098 - 

2023-01-06 16:36:02,098 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:02,760 - Epoch: [20][   10/  246]    Overall Loss 0.380558    Objective Loss 0.380558                                        LR 0.000060    Time 0.066155    
2023-01-06 16:36:02,900 - Epoch: [20][   20/  246]    Overall Loss 0.370994    Objective Loss 0.370994                                        LR 0.000060    Time 0.040041    
2023-01-06 16:36:03,041 - Epoch: [20][   30/  246]    Overall Loss 0.379438    Objective Loss 0.379438                                        LR 0.000060    Time 0.031370    
2023-01-06 16:36:03,175 - Epoch: [20][   40/  246]    Overall Loss 0.378354    Objective Loss 0.378354                                        LR 0.000060    Time 0.026882    
2023-01-06 16:36:03,316 - Epoch: [20][   50/  246]    Overall Loss 0.379226    Objective Loss 0.379226                                        LR 0.000060    Time 0.024280    
2023-01-06 16:36:03,454 - Epoch: [20][   60/  246]    Overall Loss 0.378569    Objective Loss 0.378569                                        LR 0.000060    Time 0.022531    
2023-01-06 16:36:03,599 - Epoch: [20][   70/  246]    Overall Loss 0.377593    Objective Loss 0.377593                                        LR 0.000060    Time 0.021343    
2023-01-06 16:36:03,739 - Epoch: [20][   80/  246]    Overall Loss 0.376493    Objective Loss 0.376493                                        LR 0.000060    Time 0.020428    
2023-01-06 16:36:03,879 - Epoch: [20][   90/  246]    Overall Loss 0.374666    Objective Loss 0.374666                                        LR 0.000060    Time 0.019708    
2023-01-06 16:36:04,015 - Epoch: [20][  100/  246]    Overall Loss 0.374637    Objective Loss 0.374637                                        LR 0.000060    Time 0.019096    
2023-01-06 16:36:04,164 - Epoch: [20][  110/  246]    Overall Loss 0.374616    Objective Loss 0.374616                                        LR 0.000060    Time 0.018708    
2023-01-06 16:36:04,312 - Epoch: [20][  120/  246]    Overall Loss 0.373938    Objective Loss 0.373938                                        LR 0.000060    Time 0.018377    
2023-01-06 16:36:04,457 - Epoch: [20][  130/  246]    Overall Loss 0.373424    Objective Loss 0.373424                                        LR 0.000060    Time 0.018073    
2023-01-06 16:36:04,604 - Epoch: [20][  140/  246]    Overall Loss 0.373294    Objective Loss 0.373294                                        LR 0.000060    Time 0.017833    
2023-01-06 16:36:04,749 - Epoch: [20][  150/  246]    Overall Loss 0.371476    Objective Loss 0.371476                                        LR 0.000060    Time 0.017609    
2023-01-06 16:36:04,905 - Epoch: [20][  160/  246]    Overall Loss 0.372072    Objective Loss 0.372072                                        LR 0.000060    Time 0.017475    
2023-01-06 16:36:05,055 - Epoch: [20][  170/  246]    Overall Loss 0.371192    Objective Loss 0.371192                                        LR 0.000060    Time 0.017329    
2023-01-06 16:36:05,205 - Epoch: [20][  180/  246]    Overall Loss 0.370798    Objective Loss 0.370798                                        LR 0.000060    Time 0.017197    
2023-01-06 16:36:05,347 - Epoch: [20][  190/  246]    Overall Loss 0.370376    Objective Loss 0.370376                                        LR 0.000060    Time 0.017040    
2023-01-06 16:36:05,485 - Epoch: [20][  200/  246]    Overall Loss 0.370318    Objective Loss 0.370318                                        LR 0.000060    Time 0.016872    
2023-01-06 16:36:05,621 - Epoch: [20][  210/  246]    Overall Loss 0.370111    Objective Loss 0.370111                                        LR 0.000060    Time 0.016717    
2023-01-06 16:36:05,758 - Epoch: [20][  220/  246]    Overall Loss 0.369869    Objective Loss 0.369869                                        LR 0.000060    Time 0.016577    
2023-01-06 16:36:05,898 - Epoch: [20][  230/  246]    Overall Loss 0.369805    Objective Loss 0.369805                                        LR 0.000060    Time 0.016465    
2023-01-06 16:36:06,049 - Epoch: [20][  240/  246]    Overall Loss 0.368942    Objective Loss 0.368942                                        LR 0.000060    Time 0.016408    
2023-01-06 16:36:06,114 - Epoch: [20][  246/  246]    Overall Loss 0.368732    Objective Loss 0.368732    Top1 87.320574    LR 0.000060    Time 0.016269    
2023-01-06 16:36:06,255 - --- validate (epoch=20)-----------
2023-01-06 16:36:06,255 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:06,700 - Epoch: [20][   10/   28]    Loss 0.366716    Top1 86.289062    
2023-01-06 16:36:06,810 - Epoch: [20][   20/   28]    Loss 0.366052    Top1 86.445312    
2023-01-06 16:36:06,861 - Epoch: [20][   28/   28]    Loss 0.365576    Top1 86.501575    
2023-01-06 16:36:06,997 - ==> Top1: 86.502    Loss: 0.366

2023-01-06 16:36:06,997 - ==> Confusion:
[[  78    3  358]
 [   3   53  546]
 [  14   19 5912]]

2023-01-06 16:36:06,998 - ==> Best [Top1: 86.831   Sparsity:0.00   Params: 151104 on epoch: 16]
2023-01-06 16:36:06,998 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:07,004 - 

2023-01-06 16:36:07,004 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:07,519 - Epoch: [21][   10/  246]    Overall Loss 0.380855    Objective Loss 0.380855                                        LR 0.000060    Time 0.051456    
2023-01-06 16:36:07,648 - Epoch: [21][   20/  246]    Overall Loss 0.370815    Objective Loss 0.370815                                        LR 0.000060    Time 0.032148    
2023-01-06 16:36:07,777 - Epoch: [21][   30/  246]    Overall Loss 0.373300    Objective Loss 0.373300                                        LR 0.000060    Time 0.025744    
2023-01-06 16:36:07,907 - Epoch: [21][   40/  246]    Overall Loss 0.372638    Objective Loss 0.372638                                        LR 0.000060    Time 0.022534    
2023-01-06 16:36:08,042 - Epoch: [21][   50/  246]    Overall Loss 0.372266    Objective Loss 0.372266                                        LR 0.000060    Time 0.020720    
2023-01-06 16:36:08,180 - Epoch: [21][   60/  246]    Overall Loss 0.370403    Objective Loss 0.370403                                        LR 0.000060    Time 0.019557    
2023-01-06 16:36:08,318 - Epoch: [21][   70/  246]    Overall Loss 0.371220    Objective Loss 0.371220                                        LR 0.000060    Time 0.018730    
2023-01-06 16:36:08,456 - Epoch: [21][   80/  246]    Overall Loss 0.368004    Objective Loss 0.368004                                        LR 0.000060    Time 0.018111    
2023-01-06 16:36:08,597 - Epoch: [21][   90/  246]    Overall Loss 0.367113    Objective Loss 0.367113                                        LR 0.000060    Time 0.017654    
2023-01-06 16:36:08,739 - Epoch: [21][  100/  246]    Overall Loss 0.365193    Objective Loss 0.365193                                        LR 0.000060    Time 0.017303    
2023-01-06 16:36:08,880 - Epoch: [21][  110/  246]    Overall Loss 0.362943    Objective Loss 0.362943                                        LR 0.000060    Time 0.017007    
2023-01-06 16:36:09,016 - Epoch: [21][  120/  246]    Overall Loss 0.363727    Objective Loss 0.363727                                        LR 0.000060    Time 0.016720    
2023-01-06 16:36:09,154 - Epoch: [21][  130/  246]    Overall Loss 0.364860    Objective Loss 0.364860                                        LR 0.000060    Time 0.016492    
2023-01-06 16:36:09,292 - Epoch: [21][  140/  246]    Overall Loss 0.363246    Objective Loss 0.363246                                        LR 0.000060    Time 0.016298    
2023-01-06 16:36:09,432 - Epoch: [21][  150/  246]    Overall Loss 0.364877    Objective Loss 0.364877                                        LR 0.000060    Time 0.016142    
2023-01-06 16:36:09,570 - Epoch: [21][  160/  246]    Overall Loss 0.365373    Objective Loss 0.365373                                        LR 0.000060    Time 0.015998    
2023-01-06 16:36:09,707 - Epoch: [21][  170/  246]    Overall Loss 0.366349    Objective Loss 0.366349                                        LR 0.000060    Time 0.015858    
2023-01-06 16:36:09,848 - Epoch: [21][  180/  246]    Overall Loss 0.366182    Objective Loss 0.366182                                        LR 0.000060    Time 0.015757    
2023-01-06 16:36:09,987 - Epoch: [21][  190/  246]    Overall Loss 0.366302    Objective Loss 0.366302                                        LR 0.000060    Time 0.015662    
2023-01-06 16:36:10,125 - Epoch: [21][  200/  246]    Overall Loss 0.366822    Objective Loss 0.366822                                        LR 0.000060    Time 0.015565    
2023-01-06 16:36:10,265 - Epoch: [21][  210/  246]    Overall Loss 0.366694    Objective Loss 0.366694                                        LR 0.000060    Time 0.015488    
2023-01-06 16:36:10,403 - Epoch: [21][  220/  246]    Overall Loss 0.366224    Objective Loss 0.366224                                        LR 0.000060    Time 0.015412    
2023-01-06 16:36:10,537 - Epoch: [21][  230/  246]    Overall Loss 0.365416    Objective Loss 0.365416                                        LR 0.000060    Time 0.015324    
2023-01-06 16:36:10,687 - Epoch: [21][  240/  246]    Overall Loss 0.364512    Objective Loss 0.364512                                        LR 0.000060    Time 0.015308    
2023-01-06 16:36:10,749 - Epoch: [21][  246/  246]    Overall Loss 0.364452    Objective Loss 0.364452    Top1 86.602871    LR 0.000060    Time 0.015183    
2023-01-06 16:36:10,880 - --- validate (epoch=21)-----------
2023-01-06 16:36:10,880 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:11,300 - Epoch: [21][   10/   28]    Loss 0.364974    Top1 86.601562    
2023-01-06 16:36:11,396 - Epoch: [21][   20/   28]    Loss 0.354901    Top1 86.855469    
2023-01-06 16:36:11,444 - Epoch: [21][   28/   28]    Loss 0.353350    Top1 86.959634    
2023-01-06 16:36:11,604 - ==> Top1: 86.960    Loss: 0.353

2023-01-06 16:36:11,604 - ==> Confusion:
[[ 134    2  303]
 [   7   67  528]
 [  47   24 5874]]

2023-01-06 16:36:11,605 - ==> Best [Top1: 86.960   Sparsity:0.00   Params: 151104 on epoch: 21]
2023-01-06 16:36:11,605 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:11,612 - 

2023-01-06 16:36:11,612 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:12,253 - Epoch: [22][   10/  246]    Overall Loss 0.364231    Objective Loss 0.364231                                        LR 0.000060    Time 0.064052    
2023-01-06 16:36:12,371 - Epoch: [22][   20/  246]    Overall Loss 0.353460    Objective Loss 0.353460                                        LR 0.000060    Time 0.037913    
2023-01-06 16:36:12,495 - Epoch: [22][   30/  246]    Overall Loss 0.362062    Objective Loss 0.362062                                        LR 0.000060    Time 0.029397    
2023-01-06 16:36:12,632 - Epoch: [22][   40/  246]    Overall Loss 0.361714    Objective Loss 0.361714                                        LR 0.000060    Time 0.025463    
2023-01-06 16:36:12,774 - Epoch: [22][   50/  246]    Overall Loss 0.360403    Objective Loss 0.360403                                        LR 0.000060    Time 0.023176    
2023-01-06 16:36:12,914 - Epoch: [22][   60/  246]    Overall Loss 0.358867    Objective Loss 0.358867                                        LR 0.000060    Time 0.021632    
2023-01-06 16:36:13,054 - Epoch: [22][   70/  246]    Overall Loss 0.360636    Objective Loss 0.360636                                        LR 0.000060    Time 0.020538    
2023-01-06 16:36:13,192 - Epoch: [22][   80/  246]    Overall Loss 0.357537    Objective Loss 0.357537                                        LR 0.000060    Time 0.019689    
2023-01-06 16:36:13,326 - Epoch: [22][   90/  246]    Overall Loss 0.358951    Objective Loss 0.358951                                        LR 0.000060    Time 0.018992    
2023-01-06 16:36:13,465 - Epoch: [22][  100/  246]    Overall Loss 0.360927    Objective Loss 0.360927                                        LR 0.000060    Time 0.018472    
2023-01-06 16:36:13,603 - Epoch: [22][  110/  246]    Overall Loss 0.360637    Objective Loss 0.360637                                        LR 0.000060    Time 0.018045    
2023-01-06 16:36:13,750 - Epoch: [22][  120/  246]    Overall Loss 0.358381    Objective Loss 0.358381                                        LR 0.000060    Time 0.017763    
2023-01-06 16:36:13,895 - Epoch: [22][  130/  246]    Overall Loss 0.357951    Objective Loss 0.357951                                        LR 0.000060    Time 0.017510    
2023-01-06 16:36:14,041 - Epoch: [22][  140/  246]    Overall Loss 0.359503    Objective Loss 0.359503                                        LR 0.000060    Time 0.017293    
2023-01-06 16:36:14,186 - Epoch: [22][  150/  246]    Overall Loss 0.359825    Objective Loss 0.359825                                        LR 0.000060    Time 0.017109    
2023-01-06 16:36:14,330 - Epoch: [22][  160/  246]    Overall Loss 0.359210    Objective Loss 0.359210                                        LR 0.000060    Time 0.016935    
2023-01-06 16:36:14,473 - Epoch: [22][  170/  246]    Overall Loss 0.359131    Objective Loss 0.359131                                        LR 0.000060    Time 0.016777    
2023-01-06 16:36:14,616 - Epoch: [22][  180/  246]    Overall Loss 0.358405    Objective Loss 0.358405                                        LR 0.000060    Time 0.016637    
2023-01-06 16:36:14,762 - Epoch: [22][  190/  246]    Overall Loss 0.358189    Objective Loss 0.358189                                        LR 0.000060    Time 0.016528    
2023-01-06 16:36:14,905 - Epoch: [22][  200/  246]    Overall Loss 0.358285    Objective Loss 0.358285                                        LR 0.000060    Time 0.016412    
2023-01-06 16:36:15,052 - Epoch: [22][  210/  246]    Overall Loss 0.359668    Objective Loss 0.359668                                        LR 0.000060    Time 0.016330    
2023-01-06 16:36:15,198 - Epoch: [22][  220/  246]    Overall Loss 0.359673    Objective Loss 0.359673                                        LR 0.000060    Time 0.016249    
2023-01-06 16:36:15,336 - Epoch: [22][  230/  246]    Overall Loss 0.358808    Objective Loss 0.358808                                        LR 0.000060    Time 0.016138    
2023-01-06 16:36:15,485 - Epoch: [22][  240/  246]    Overall Loss 0.359319    Objective Loss 0.359319                                        LR 0.000060    Time 0.016088    
2023-01-06 16:36:15,544 - Epoch: [22][  246/  246]    Overall Loss 0.359080    Objective Loss 0.359080    Top1 85.885167    LR 0.000060    Time 0.015933    
2023-01-06 16:36:15,689 - --- validate (epoch=22)-----------
2023-01-06 16:36:15,690 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:16,102 - Epoch: [22][   10/   28]    Loss 0.346926    Top1 87.304688    
2023-01-06 16:36:16,201 - Epoch: [22][   20/   28]    Loss 0.351261    Top1 87.460938    
2023-01-06 16:36:16,251 - Epoch: [22][   28/   28]    Loss 0.351813    Top1 87.074148    
2023-01-06 16:36:16,377 - ==> Top1: 87.074    Loss: 0.352

2023-01-06 16:36:16,378 - ==> Confusion:
[[ 141    3  295]
 [   6   75  521]
 [  48   30 5867]]

2023-01-06 16:36:16,379 - ==> Best [Top1: 87.074   Sparsity:0.00   Params: 151104 on epoch: 22]
2023-01-06 16:36:16,379 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:16,385 - 

2023-01-06 16:36:16,385 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:16,898 - Epoch: [23][   10/  246]    Overall Loss 0.344864    Objective Loss 0.344864                                        LR 0.000060    Time 0.051241    
2023-01-06 16:36:17,031 - Epoch: [23][   20/  246]    Overall Loss 0.357139    Objective Loss 0.357139                                        LR 0.000060    Time 0.032250    
2023-01-06 16:36:17,166 - Epoch: [23][   30/  246]    Overall Loss 0.360843    Objective Loss 0.360843                                        LR 0.000060    Time 0.025950    
2023-01-06 16:36:17,299 - Epoch: [23][   40/  246]    Overall Loss 0.356745    Objective Loss 0.356745                                        LR 0.000060    Time 0.022775    
2023-01-06 16:36:17,433 - Epoch: [23][   50/  246]    Overall Loss 0.354663    Objective Loss 0.354663                                        LR 0.000060    Time 0.020886    
2023-01-06 16:36:17,567 - Epoch: [23][   60/  246]    Overall Loss 0.355631    Objective Loss 0.355631                                        LR 0.000060    Time 0.019636    
2023-01-06 16:36:17,709 - Epoch: [23][   70/  246]    Overall Loss 0.356848    Objective Loss 0.356848                                        LR 0.000060    Time 0.018857    
2023-01-06 16:36:17,852 - Epoch: [23][   80/  246]    Overall Loss 0.358548    Objective Loss 0.358548                                        LR 0.000060    Time 0.018281    
2023-01-06 16:36:17,994 - Epoch: [23][   90/  246]    Overall Loss 0.358104    Objective Loss 0.358104                                        LR 0.000060    Time 0.017823    
2023-01-06 16:36:18,141 - Epoch: [23][  100/  246]    Overall Loss 0.360962    Objective Loss 0.360962                                        LR 0.000060    Time 0.017507    
2023-01-06 16:36:18,301 - Epoch: [23][  110/  246]    Overall Loss 0.358565    Objective Loss 0.358565                                        LR 0.000060    Time 0.017367    
2023-01-06 16:36:18,462 - Epoch: [23][  120/  246]    Overall Loss 0.358463    Objective Loss 0.358463                                        LR 0.000060    Time 0.017262    
2023-01-06 16:36:18,624 - Epoch: [23][  130/  246]    Overall Loss 0.360237    Objective Loss 0.360237                                        LR 0.000060    Time 0.017174    
2023-01-06 16:36:18,785 - Epoch: [23][  140/  246]    Overall Loss 0.358075    Objective Loss 0.358075                                        LR 0.000060    Time 0.017094    
2023-01-06 16:36:18,950 - Epoch: [23][  150/  246]    Overall Loss 0.356157    Objective Loss 0.356157                                        LR 0.000060    Time 0.017051    
2023-01-06 16:36:19,116 - Epoch: [23][  160/  246]    Overall Loss 0.355628    Objective Loss 0.355628                                        LR 0.000060    Time 0.017021    
2023-01-06 16:36:19,277 - Epoch: [23][  170/  246]    Overall Loss 0.355849    Objective Loss 0.355849                                        LR 0.000060    Time 0.016966    
2023-01-06 16:36:19,440 - Epoch: [23][  180/  246]    Overall Loss 0.355704    Objective Loss 0.355704                                        LR 0.000060    Time 0.016927    
2023-01-06 16:36:19,597 - Epoch: [23][  190/  246]    Overall Loss 0.356322    Objective Loss 0.356322                                        LR 0.000060    Time 0.016862    
2023-01-06 16:36:19,761 - Epoch: [23][  200/  246]    Overall Loss 0.356374    Objective Loss 0.356374                                        LR 0.000060    Time 0.016826    
2023-01-06 16:36:19,919 - Epoch: [23][  210/  246]    Overall Loss 0.356709    Objective Loss 0.356709                                        LR 0.000060    Time 0.016779    
2023-01-06 16:36:20,079 - Epoch: [23][  220/  246]    Overall Loss 0.355693    Objective Loss 0.355693                                        LR 0.000060    Time 0.016741    
2023-01-06 16:36:20,219 - Epoch: [23][  230/  246]    Overall Loss 0.355571    Objective Loss 0.355571                                        LR 0.000060    Time 0.016620    
2023-01-06 16:36:20,370 - Epoch: [23][  240/  246]    Overall Loss 0.355230    Objective Loss 0.355230                                        LR 0.000060    Time 0.016554    
2023-01-06 16:36:20,437 - Epoch: [23][  246/  246]    Overall Loss 0.355186    Objective Loss 0.355186    Top1 86.602871    LR 0.000060    Time 0.016420    
2023-01-06 16:36:20,577 - --- validate (epoch=23)-----------
2023-01-06 16:36:20,578 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:21,121 - Epoch: [23][   10/   28]    Loss 0.346899    Top1 87.656250    
2023-01-06 16:36:21,221 - Epoch: [23][   20/   28]    Loss 0.346730    Top1 87.421875    
2023-01-06 16:36:21,270 - Epoch: [23][   28/   28]    Loss 0.350252    Top1 87.317492    
2023-01-06 16:36:21,433 - ==> Top1: 87.317    Loss: 0.350

2023-01-06 16:36:21,433 - ==> Confusion:
[[ 161    2  276]
 [   8   80  514]
 [  63   23 5859]]

2023-01-06 16:36:21,434 - ==> Best [Top1: 87.317   Sparsity:0.00   Params: 151104 on epoch: 23]
2023-01-06 16:36:21,434 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:21,440 - 

2023-01-06 16:36:21,440 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:21,953 - Epoch: [24][   10/  246]    Overall Loss 0.352621    Objective Loss 0.352621                                        LR 0.000060    Time 0.051172    
2023-01-06 16:36:22,077 - Epoch: [24][   20/  246]    Overall Loss 0.352345    Objective Loss 0.352345                                        LR 0.000060    Time 0.031797    
2023-01-06 16:36:22,202 - Epoch: [24][   30/  246]    Overall Loss 0.354736    Objective Loss 0.354736                                        LR 0.000060    Time 0.025347    
2023-01-06 16:36:22,328 - Epoch: [24][   40/  246]    Overall Loss 0.356027    Objective Loss 0.356027                                        LR 0.000060    Time 0.022148    
2023-01-06 16:36:22,455 - Epoch: [24][   50/  246]    Overall Loss 0.355151    Objective Loss 0.355151                                        LR 0.000060    Time 0.020244    
2023-01-06 16:36:22,583 - Epoch: [24][   60/  246]    Overall Loss 0.355788    Objective Loss 0.355788                                        LR 0.000060    Time 0.018993    
2023-01-06 16:36:22,706 - Epoch: [24][   70/  246]    Overall Loss 0.357710    Objective Loss 0.357710                                        LR 0.000060    Time 0.018039    
2023-01-06 16:36:22,832 - Epoch: [24][   80/  246]    Overall Loss 0.357985    Objective Loss 0.357985                                        LR 0.000060    Time 0.017356    
2023-01-06 16:36:22,957 - Epoch: [24][   90/  246]    Overall Loss 0.360963    Objective Loss 0.360963                                        LR 0.000060    Time 0.016804    
2023-01-06 16:36:23,095 - Epoch: [24][  100/  246]    Overall Loss 0.359170    Objective Loss 0.359170                                        LR 0.000060    Time 0.016502    
2023-01-06 16:36:23,238 - Epoch: [24][  110/  246]    Overall Loss 0.356341    Objective Loss 0.356341                                        LR 0.000060    Time 0.016298    
2023-01-06 16:36:23,388 - Epoch: [24][  120/  246]    Overall Loss 0.356790    Objective Loss 0.356790                                        LR 0.000060    Time 0.016190    
2023-01-06 16:36:23,541 - Epoch: [24][  130/  246]    Overall Loss 0.356216    Objective Loss 0.356216                                        LR 0.000060    Time 0.016113    
2023-01-06 16:36:23,680 - Epoch: [24][  140/  246]    Overall Loss 0.355188    Objective Loss 0.355188                                        LR 0.000060    Time 0.015952    
2023-01-06 16:36:23,807 - Epoch: [24][  150/  246]    Overall Loss 0.354773    Objective Loss 0.354773                                        LR 0.000060    Time 0.015737    
2023-01-06 16:36:23,931 - Epoch: [24][  160/  246]    Overall Loss 0.354682    Objective Loss 0.354682                                        LR 0.000060    Time 0.015525    
2023-01-06 16:36:24,058 - Epoch: [24][  170/  246]    Overall Loss 0.353791    Objective Loss 0.353791                                        LR 0.000060    Time 0.015352    
2023-01-06 16:36:24,207 - Epoch: [24][  180/  246]    Overall Loss 0.353994    Objective Loss 0.353994                                        LR 0.000060    Time 0.015330    
2023-01-06 16:36:24,359 - Epoch: [24][  190/  246]    Overall Loss 0.354215    Objective Loss 0.354215                                        LR 0.000060    Time 0.015319    
2023-01-06 16:36:24,506 - Epoch: [24][  200/  246]    Overall Loss 0.353165    Objective Loss 0.353165                                        LR 0.000060    Time 0.015284    
2023-01-06 16:36:24,651 - Epoch: [24][  210/  246]    Overall Loss 0.351905    Objective Loss 0.351905                                        LR 0.000060    Time 0.015248    
2023-01-06 16:36:24,801 - Epoch: [24][  220/  246]    Overall Loss 0.352527    Objective Loss 0.352527                                        LR 0.000060    Time 0.015231    
2023-01-06 16:36:24,945 - Epoch: [24][  230/  246]    Overall Loss 0.352788    Objective Loss 0.352788                                        LR 0.000060    Time 0.015194    
2023-01-06 16:36:25,092 - Epoch: [24][  240/  246]    Overall Loss 0.352765    Objective Loss 0.352765                                        LR 0.000060    Time 0.015171    
2023-01-06 16:36:25,153 - Epoch: [24][  246/  246]    Overall Loss 0.351995    Objective Loss 0.351995    Top1 90.909091    LR 0.000060    Time 0.015047    
2023-01-06 16:36:25,301 - --- validate (epoch=24)-----------
2023-01-06 16:36:25,301 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:25,724 - Epoch: [24][   10/   28]    Loss 0.360414    Top1 86.523438    
2023-01-06 16:36:25,819 - Epoch: [24][   20/   28]    Loss 0.345617    Top1 87.285156    
2023-01-06 16:36:25,870 - Epoch: [24][   28/   28]    Loss 0.343612    Top1 87.288863    
2023-01-06 16:36:26,016 - ==> Top1: 87.289    Loss: 0.344

2023-01-06 16:36:26,016 - ==> Confusion:
[[ 124    3  312]
 [   3   92  507]
 [  39   24 5882]]

2023-01-06 16:36:26,017 - ==> Best [Top1: 87.317   Sparsity:0.00   Params: 151104 on epoch: 23]
2023-01-06 16:36:26,017 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:26,022 - 

2023-01-06 16:36:26,022 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:26,702 - Epoch: [25][   10/  246]    Overall Loss 0.345360    Objective Loss 0.345360                                        LR 0.000060    Time 0.067861    
2023-01-06 16:36:26,857 - Epoch: [25][   20/  246]    Overall Loss 0.358415    Objective Loss 0.358415                                        LR 0.000060    Time 0.041661    
2023-01-06 16:36:27,009 - Epoch: [25][   30/  246]    Overall Loss 0.355390    Objective Loss 0.355390                                        LR 0.000060    Time 0.032849    
2023-01-06 16:36:27,137 - Epoch: [25][   40/  246]    Overall Loss 0.352245    Objective Loss 0.352245                                        LR 0.000060    Time 0.027817    
2023-01-06 16:36:27,266 - Epoch: [25][   50/  246]    Overall Loss 0.351012    Objective Loss 0.351012                                        LR 0.000060    Time 0.024824    
2023-01-06 16:36:27,395 - Epoch: [25][   60/  246]    Overall Loss 0.349575    Objective Loss 0.349575                                        LR 0.000060    Time 0.022825    
2023-01-06 16:36:27,521 - Epoch: [25][   70/  246]    Overall Loss 0.348350    Objective Loss 0.348350                                        LR 0.000060    Time 0.021369    
2023-01-06 16:36:27,652 - Epoch: [25][   80/  246]    Overall Loss 0.351023    Objective Loss 0.351023                                        LR 0.000060    Time 0.020328    
2023-01-06 16:36:27,792 - Epoch: [25][   90/  246]    Overall Loss 0.351815    Objective Loss 0.351815                                        LR 0.000060    Time 0.019618    
2023-01-06 16:36:27,933 - Epoch: [25][  100/  246]    Overall Loss 0.350916    Objective Loss 0.350916                                        LR 0.000060    Time 0.019057    
2023-01-06 16:36:28,071 - Epoch: [25][  110/  246]    Overall Loss 0.350164    Objective Loss 0.350164                                        LR 0.000060    Time 0.018581    
2023-01-06 16:36:28,209 - Epoch: [25][  120/  246]    Overall Loss 0.349928    Objective Loss 0.349928                                        LR 0.000060    Time 0.018181    
2023-01-06 16:36:28,347 - Epoch: [25][  130/  246]    Overall Loss 0.349445    Objective Loss 0.349445                                        LR 0.000060    Time 0.017838    
2023-01-06 16:36:28,486 - Epoch: [25][  140/  246]    Overall Loss 0.349357    Objective Loss 0.349357                                        LR 0.000060    Time 0.017555    
2023-01-06 16:36:28,625 - Epoch: [25][  150/  246]    Overall Loss 0.350167    Objective Loss 0.350167                                        LR 0.000060    Time 0.017306    
2023-01-06 16:36:28,764 - Epoch: [25][  160/  246]    Overall Loss 0.349820    Objective Loss 0.349820                                        LR 0.000060    Time 0.017093    
2023-01-06 16:36:28,902 - Epoch: [25][  170/  246]    Overall Loss 0.350758    Objective Loss 0.350758                                        LR 0.000060    Time 0.016895    
2023-01-06 16:36:29,039 - Epoch: [25][  180/  246]    Overall Loss 0.351551    Objective Loss 0.351551                                        LR 0.000060    Time 0.016716    
2023-01-06 16:36:29,177 - Epoch: [25][  190/  246]    Overall Loss 0.351621    Objective Loss 0.351621                                        LR 0.000060    Time 0.016559    
2023-01-06 16:36:29,315 - Epoch: [25][  200/  246]    Overall Loss 0.350422    Objective Loss 0.350422                                        LR 0.000060    Time 0.016419    
2023-01-06 16:36:29,454 - Epoch: [25][  210/  246]    Overall Loss 0.350239    Objective Loss 0.350239                                        LR 0.000060    Time 0.016298    
2023-01-06 16:36:29,596 - Epoch: [25][  220/  246]    Overall Loss 0.350335    Objective Loss 0.350335                                        LR 0.000060    Time 0.016202    
2023-01-06 16:36:29,735 - Epoch: [25][  230/  246]    Overall Loss 0.348576    Objective Loss 0.348576                                        LR 0.000060    Time 0.016101    
2023-01-06 16:36:29,889 - Epoch: [25][  240/  246]    Overall Loss 0.348969    Objective Loss 0.348969                                        LR 0.000060    Time 0.016069    
2023-01-06 16:36:29,954 - Epoch: [25][  246/  246]    Overall Loss 0.348489    Objective Loss 0.348489    Top1 87.799043    LR 0.000060    Time 0.015938    
2023-01-06 16:36:30,100 - --- validate (epoch=25)-----------
2023-01-06 16:36:30,101 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:30,534 - Epoch: [25][   10/   28]    Loss 0.363939    Top1 86.210938    
2023-01-06 16:36:30,631 - Epoch: [25][   20/   28]    Loss 0.357996    Top1 86.835938    
2023-01-06 16:36:30,681 - Epoch: [25][   28/   28]    Loss 0.346205    Top1 87.031205    
2023-01-06 16:36:30,821 - ==> Top1: 87.031    Loss: 0.346

2023-01-06 16:36:30,821 - ==> Confusion:
[[ 148    2  289]
 [  12   59  531]
 [  57   15 5873]]

2023-01-06 16:36:30,822 - ==> Best [Top1: 87.317   Sparsity:0.00   Params: 151104 on epoch: 23]
2023-01-06 16:36:30,822 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:30,827 - 

2023-01-06 16:36:30,827 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:31,351 - Epoch: [26][   10/  246]    Overall Loss 0.353043    Objective Loss 0.353043                                        LR 0.000060    Time 0.052290    
2023-01-06 16:36:31,494 - Epoch: [26][   20/  246]    Overall Loss 0.341524    Objective Loss 0.341524                                        LR 0.000060    Time 0.033305    
2023-01-06 16:36:31,630 - Epoch: [26][   30/  246]    Overall Loss 0.344273    Objective Loss 0.344273                                        LR 0.000060    Time 0.026724    
2023-01-06 16:36:31,765 - Epoch: [26][   40/  246]    Overall Loss 0.342611    Objective Loss 0.342611                                        LR 0.000060    Time 0.023397    
2023-01-06 16:36:31,900 - Epoch: [26][   50/  246]    Overall Loss 0.344648    Objective Loss 0.344648                                        LR 0.000060    Time 0.021417    
2023-01-06 16:36:32,035 - Epoch: [26][   60/  246]    Overall Loss 0.344184    Objective Loss 0.344184                                        LR 0.000060    Time 0.020079    
2023-01-06 16:36:32,168 - Epoch: [26][   70/  246]    Overall Loss 0.344912    Objective Loss 0.344912                                        LR 0.000060    Time 0.019117    
2023-01-06 16:36:32,304 - Epoch: [26][   80/  246]    Overall Loss 0.345202    Objective Loss 0.345202                                        LR 0.000060    Time 0.018414    
2023-01-06 16:36:32,445 - Epoch: [26][   90/  246]    Overall Loss 0.345247    Objective Loss 0.345247                                        LR 0.000060    Time 0.017931    
2023-01-06 16:36:32,584 - Epoch: [26][  100/  246]    Overall Loss 0.346940    Objective Loss 0.346940                                        LR 0.000060    Time 0.017531    
2023-01-06 16:36:32,725 - Epoch: [26][  110/  246]    Overall Loss 0.346937    Objective Loss 0.346937                                        LR 0.000060    Time 0.017210    
2023-01-06 16:36:32,868 - Epoch: [26][  120/  246]    Overall Loss 0.348515    Objective Loss 0.348515                                        LR 0.000060    Time 0.016969    
2023-01-06 16:36:33,016 - Epoch: [26][  130/  246]    Overall Loss 0.346453    Objective Loss 0.346453                                        LR 0.000060    Time 0.016799    
2023-01-06 16:36:33,175 - Epoch: [26][  140/  246]    Overall Loss 0.346198    Objective Loss 0.346198                                        LR 0.000060    Time 0.016725    
2023-01-06 16:36:33,325 - Epoch: [26][  150/  246]    Overall Loss 0.346115    Objective Loss 0.346115                                        LR 0.000060    Time 0.016609    
2023-01-06 16:36:33,469 - Epoch: [26][  160/  246]    Overall Loss 0.346821    Objective Loss 0.346821                                        LR 0.000060    Time 0.016466    
2023-01-06 16:36:33,614 - Epoch: [26][  170/  246]    Overall Loss 0.345875    Objective Loss 0.345875                                        LR 0.000060    Time 0.016352    
2023-01-06 16:36:33,765 - Epoch: [26][  180/  246]    Overall Loss 0.345850    Objective Loss 0.345850                                        LR 0.000060    Time 0.016280    
2023-01-06 16:36:33,914 - Epoch: [26][  190/  246]    Overall Loss 0.344969    Objective Loss 0.344969                                        LR 0.000060    Time 0.016203    
2023-01-06 16:36:34,056 - Epoch: [26][  200/  246]    Overall Loss 0.346306    Objective Loss 0.346306                                        LR 0.000060    Time 0.016102    
2023-01-06 16:36:34,193 - Epoch: [26][  210/  246]    Overall Loss 0.346128    Objective Loss 0.346128                                        LR 0.000060    Time 0.015983    
2023-01-06 16:36:34,316 - Epoch: [26][  220/  246]    Overall Loss 0.346028    Objective Loss 0.346028                                        LR 0.000060    Time 0.015813    
2023-01-06 16:36:34,441 - Epoch: [26][  230/  246]    Overall Loss 0.345818    Objective Loss 0.345818                                        LR 0.000060    Time 0.015670    
2023-01-06 16:36:34,585 - Epoch: [26][  240/  246]    Overall Loss 0.345769    Objective Loss 0.345769                                        LR 0.000060    Time 0.015615    
2023-01-06 16:36:34,652 - Epoch: [26][  246/  246]    Overall Loss 0.345645    Objective Loss 0.345645    Top1 86.842105    LR 0.000060    Time 0.015504    
2023-01-06 16:36:34,794 - --- validate (epoch=26)-----------
2023-01-06 16:36:34,794 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:35,229 - Epoch: [26][   10/   28]    Loss 0.341299    Top1 87.851562    
2023-01-06 16:36:35,332 - Epoch: [26][   20/   28]    Loss 0.344464    Top1 87.480469    
2023-01-06 16:36:35,383 - Epoch: [26][   28/   28]    Loss 0.341922    Top1 87.474950    
2023-01-06 16:36:35,505 - ==> Top1: 87.475    Loss: 0.342

2023-01-06 16:36:35,505 - ==> Confusion:
[[ 129    3  307]
 [   4  125  473]
 [  39   49 5857]]

2023-01-06 16:36:35,506 - ==> Best [Top1: 87.475   Sparsity:0.00   Params: 151104 on epoch: 26]
2023-01-06 16:36:35,506 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:35,512 - 

2023-01-06 16:36:35,512 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:36,161 - Epoch: [27][   10/  246]    Overall Loss 0.338211    Objective Loss 0.338211                                        LR 0.000060    Time 0.064765    
2023-01-06 16:36:36,293 - Epoch: [27][   20/  246]    Overall Loss 0.336770    Objective Loss 0.336770                                        LR 0.000060    Time 0.038977    
2023-01-06 16:36:36,422 - Epoch: [27][   30/  246]    Overall Loss 0.345281    Objective Loss 0.345281                                        LR 0.000060    Time 0.030276    
2023-01-06 16:36:36,558 - Epoch: [27][   40/  246]    Overall Loss 0.341012    Objective Loss 0.341012                                        LR 0.000060    Time 0.026088    
2023-01-06 16:36:36,688 - Epoch: [27][   50/  246]    Overall Loss 0.344205    Objective Loss 0.344205                                        LR 0.000060    Time 0.023469    
2023-01-06 16:36:36,822 - Epoch: [27][   60/  246]    Overall Loss 0.342093    Objective Loss 0.342093                                        LR 0.000060    Time 0.021781    
2023-01-06 16:36:36,963 - Epoch: [27][   70/  246]    Overall Loss 0.343313    Objective Loss 0.343313                                        LR 0.000060    Time 0.020680    
2023-01-06 16:36:37,110 - Epoch: [27][   80/  246]    Overall Loss 0.343220    Objective Loss 0.343220                                        LR 0.000060    Time 0.019929    
2023-01-06 16:36:37,265 - Epoch: [27][   90/  246]    Overall Loss 0.342035    Objective Loss 0.342035                                        LR 0.000060    Time 0.019433    
2023-01-06 16:36:37,419 - Epoch: [27][  100/  246]    Overall Loss 0.340118    Objective Loss 0.340118                                        LR 0.000060    Time 0.019021    
2023-01-06 16:36:37,574 - Epoch: [27][  110/  246]    Overall Loss 0.341281    Objective Loss 0.341281                                        LR 0.000060    Time 0.018696    
2023-01-06 16:36:37,726 - Epoch: [27][  120/  246]    Overall Loss 0.341891    Objective Loss 0.341891                                        LR 0.000060    Time 0.018402    
2023-01-06 16:36:37,882 - Epoch: [27][  130/  246]    Overall Loss 0.342074    Objective Loss 0.342074                                        LR 0.000060    Time 0.018186    
2023-01-06 16:36:38,045 - Epoch: [27][  140/  246]    Overall Loss 0.341130    Objective Loss 0.341130                                        LR 0.000060    Time 0.018047    
2023-01-06 16:36:38,197 - Epoch: [27][  150/  246]    Overall Loss 0.340809    Objective Loss 0.340809                                        LR 0.000060    Time 0.017857    
2023-01-06 16:36:38,341 - Epoch: [27][  160/  246]    Overall Loss 0.340482    Objective Loss 0.340482                                        LR 0.000060    Time 0.017638    
2023-01-06 16:36:38,477 - Epoch: [27][  170/  246]    Overall Loss 0.340939    Objective Loss 0.340939                                        LR 0.000060    Time 0.017394    
2023-01-06 16:36:38,614 - Epoch: [27][  180/  246]    Overall Loss 0.340770    Objective Loss 0.340770                                        LR 0.000060    Time 0.017191    
2023-01-06 16:36:38,755 - Epoch: [27][  190/  246]    Overall Loss 0.341807    Objective Loss 0.341807                                        LR 0.000060    Time 0.017022    
2023-01-06 16:36:38,897 - Epoch: [27][  200/  246]    Overall Loss 0.340803    Objective Loss 0.340803                                        LR 0.000060    Time 0.016878    
2023-01-06 16:36:39,038 - Epoch: [27][  210/  246]    Overall Loss 0.342471    Objective Loss 0.342471                                        LR 0.000060    Time 0.016745    
2023-01-06 16:36:39,186 - Epoch: [27][  220/  246]    Overall Loss 0.342173    Objective Loss 0.342173                                        LR 0.000060    Time 0.016655    
2023-01-06 16:36:39,336 - Epoch: [27][  230/  246]    Overall Loss 0.342435    Objective Loss 0.342435                                        LR 0.000060    Time 0.016581    
2023-01-06 16:36:39,519 - Epoch: [27][  240/  246]    Overall Loss 0.342711    Objective Loss 0.342711                                        LR 0.000060    Time 0.016653    
2023-01-06 16:36:39,586 - Epoch: [27][  246/  246]    Overall Loss 0.342814    Objective Loss 0.342814    Top1 86.842105    LR 0.000060    Time 0.016518    
2023-01-06 16:36:39,726 - --- validate (epoch=27)-----------
2023-01-06 16:36:39,727 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:40,152 - Epoch: [27][   10/   28]    Loss 0.318977    Top1 88.828125    
2023-01-06 16:36:40,249 - Epoch: [27][   20/   28]    Loss 0.331274    Top1 88.066406    
2023-01-06 16:36:40,298 - Epoch: [27][   28/   28]    Loss 0.332314    Top1 87.861437    
2023-01-06 16:36:40,462 - ==> Top1: 87.861    Loss: 0.332

2023-01-06 16:36:40,462 - ==> Confusion:
[[ 156    4  279]
 [   6  117  479]
 [  49   31 5865]]

2023-01-06 16:36:40,464 - ==> Best [Top1: 87.861   Sparsity:0.00   Params: 151104 on epoch: 27]
2023-01-06 16:36:40,464 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:40,471 - 

2023-01-06 16:36:40,471 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:41,128 - Epoch: [28][   10/  246]    Overall Loss 0.335072    Objective Loss 0.335072                                        LR 0.000060    Time 0.065617    
2023-01-06 16:36:41,264 - Epoch: [28][   20/  246]    Overall Loss 0.325241    Objective Loss 0.325241                                        LR 0.000060    Time 0.039580    
2023-01-06 16:36:41,406 - Epoch: [28][   30/  246]    Overall Loss 0.326849    Objective Loss 0.326849                                        LR 0.000060    Time 0.031135    
2023-01-06 16:36:41,545 - Epoch: [28][   40/  246]    Overall Loss 0.327365    Objective Loss 0.327365                                        LR 0.000060    Time 0.026806    
2023-01-06 16:36:41,681 - Epoch: [28][   50/  246]    Overall Loss 0.329192    Objective Loss 0.329192                                        LR 0.000060    Time 0.024156    
2023-01-06 16:36:41,818 - Epoch: [28][   60/  246]    Overall Loss 0.330710    Objective Loss 0.330710                                        LR 0.000060    Time 0.022416    
2023-01-06 16:36:41,955 - Epoch: [28][   70/  246]    Overall Loss 0.332960    Objective Loss 0.332960                                        LR 0.000060    Time 0.021165    
2023-01-06 16:36:42,096 - Epoch: [28][   80/  246]    Overall Loss 0.332040    Objective Loss 0.332040                                        LR 0.000060    Time 0.020273    
2023-01-06 16:36:42,238 - Epoch: [28][   90/  246]    Overall Loss 0.333605    Objective Loss 0.333605                                        LR 0.000060    Time 0.019596    
2023-01-06 16:36:42,387 - Epoch: [28][  100/  246]    Overall Loss 0.334476    Objective Loss 0.334476                                        LR 0.000060    Time 0.019121    
2023-01-06 16:36:42,533 - Epoch: [28][  110/  246]    Overall Loss 0.333852    Objective Loss 0.333852                                        LR 0.000060    Time 0.018711    
2023-01-06 16:36:42,690 - Epoch: [28][  120/  246]    Overall Loss 0.332138    Objective Loss 0.332138                                        LR 0.000060    Time 0.018451    
2023-01-06 16:36:42,850 - Epoch: [28][  130/  246]    Overall Loss 0.331253    Objective Loss 0.331253                                        LR 0.000060    Time 0.018265    
2023-01-06 16:36:43,002 - Epoch: [28][  140/  246]    Overall Loss 0.333038    Objective Loss 0.333038                                        LR 0.000060    Time 0.018042    
2023-01-06 16:36:43,158 - Epoch: [28][  150/  246]    Overall Loss 0.334198    Objective Loss 0.334198                                        LR 0.000060    Time 0.017872    
2023-01-06 16:36:43,311 - Epoch: [28][  160/  246]    Overall Loss 0.333304    Objective Loss 0.333304                                        LR 0.000060    Time 0.017711    
2023-01-06 16:36:43,447 - Epoch: [28][  170/  246]    Overall Loss 0.334540    Objective Loss 0.334540                                        LR 0.000060    Time 0.017469    
2023-01-06 16:36:43,581 - Epoch: [28][  180/  246]    Overall Loss 0.334966    Objective Loss 0.334966                                        LR 0.000060    Time 0.017241    
2023-01-06 16:36:43,719 - Epoch: [28][  190/  246]    Overall Loss 0.335544    Objective Loss 0.335544                                        LR 0.000060    Time 0.017056    
2023-01-06 16:36:43,852 - Epoch: [28][  200/  246]    Overall Loss 0.336455    Objective Loss 0.336455                                        LR 0.000060    Time 0.016867    
2023-01-06 16:36:43,986 - Epoch: [28][  210/  246]    Overall Loss 0.337450    Objective Loss 0.337450                                        LR 0.000060    Time 0.016702    
2023-01-06 16:36:44,120 - Epoch: [28][  220/  246]    Overall Loss 0.337362    Objective Loss 0.337362                                        LR 0.000060    Time 0.016552    
2023-01-06 16:36:44,253 - Epoch: [28][  230/  246]    Overall Loss 0.338291    Objective Loss 0.338291                                        LR 0.000060    Time 0.016407    
2023-01-06 16:36:44,403 - Epoch: [28][  240/  246]    Overall Loss 0.338852    Objective Loss 0.338852                                        LR 0.000060    Time 0.016347    
2023-01-06 16:36:44,469 - Epoch: [28][  246/  246]    Overall Loss 0.338720    Objective Loss 0.338720    Top1 89.473684    LR 0.000060    Time 0.016215    
2023-01-06 16:36:44,615 - --- validate (epoch=28)-----------
2023-01-06 16:36:44,615 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:45,047 - Epoch: [28][   10/   28]    Loss 0.358303    Top1 86.992188    
2023-01-06 16:36:45,150 - Epoch: [28][   20/   28]    Loss 0.341414    Top1 87.343750    
2023-01-06 16:36:45,198 - Epoch: [28][   28/   28]    Loss 0.338939    Top1 87.503579    
2023-01-06 16:36:45,328 - ==> Top1: 87.504    Loss: 0.339

2023-01-06 16:36:45,328 - ==> Confusion:
[[ 182    3  254]
 [  15  119  468]
 [  89   44 5812]]

2023-01-06 16:36:45,329 - ==> Best [Top1: 87.861   Sparsity:0.00   Params: 151104 on epoch: 27]
2023-01-06 16:36:45,329 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:45,334 - 

2023-01-06 16:36:45,334 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:45,849 - Epoch: [29][   10/  246]    Overall Loss 0.339949    Objective Loss 0.339949                                        LR 0.000060    Time 0.051369    
2023-01-06 16:36:45,981 - Epoch: [29][   20/  246]    Overall Loss 0.334458    Objective Loss 0.334458                                        LR 0.000060    Time 0.032284    
2023-01-06 16:36:46,114 - Epoch: [29][   30/  246]    Overall Loss 0.332411    Objective Loss 0.332411                                        LR 0.000060    Time 0.025955    
2023-01-06 16:36:46,249 - Epoch: [29][   40/  246]    Overall Loss 0.330673    Objective Loss 0.330673                                        LR 0.000060    Time 0.022816    
2023-01-06 16:36:46,380 - Epoch: [29][   50/  246]    Overall Loss 0.333756    Objective Loss 0.333756                                        LR 0.000060    Time 0.020863    
2023-01-06 16:36:46,511 - Epoch: [29][   60/  246]    Overall Loss 0.331076    Objective Loss 0.331076                                        LR 0.000060    Time 0.019565    
2023-01-06 16:36:46,650 - Epoch: [29][   70/  246]    Overall Loss 0.332247    Objective Loss 0.332247                                        LR 0.000060    Time 0.018757    
2023-01-06 16:36:46,799 - Epoch: [29][   80/  246]    Overall Loss 0.333007    Objective Loss 0.333007                                        LR 0.000060    Time 0.018270    
2023-01-06 16:36:46,947 - Epoch: [29][   90/  246]    Overall Loss 0.332255    Objective Loss 0.332255                                        LR 0.000060    Time 0.017877    
2023-01-06 16:36:47,087 - Epoch: [29][  100/  246]    Overall Loss 0.330977    Objective Loss 0.330977                                        LR 0.000060    Time 0.017485    
2023-01-06 16:36:47,231 - Epoch: [29][  110/  246]    Overall Loss 0.331285    Objective Loss 0.331285                                        LR 0.000060    Time 0.017202    
2023-01-06 16:36:47,379 - Epoch: [29][  120/  246]    Overall Loss 0.329614    Objective Loss 0.329614                                        LR 0.000060    Time 0.016998    
2023-01-06 16:36:47,529 - Epoch: [29][  130/  246]    Overall Loss 0.328920    Objective Loss 0.328920                                        LR 0.000060    Time 0.016845    
2023-01-06 16:36:47,677 - Epoch: [29][  140/  246]    Overall Loss 0.329161    Objective Loss 0.329161                                        LR 0.000060    Time 0.016692    
2023-01-06 16:36:47,829 - Epoch: [29][  150/  246]    Overall Loss 0.330218    Objective Loss 0.330218                                        LR 0.000060    Time 0.016588    
2023-01-06 16:36:47,974 - Epoch: [29][  160/  246]    Overall Loss 0.331688    Objective Loss 0.331688                                        LR 0.000060    Time 0.016456    
2023-01-06 16:36:48,098 - Epoch: [29][  170/  246]    Overall Loss 0.331924    Objective Loss 0.331924                                        LR 0.000060    Time 0.016216    
2023-01-06 16:36:48,222 - Epoch: [29][  180/  246]    Overall Loss 0.333165    Objective Loss 0.333165                                        LR 0.000060    Time 0.016005    
2023-01-06 16:36:48,343 - Epoch: [29][  190/  246]    Overall Loss 0.333659    Objective Loss 0.333659                                        LR 0.000060    Time 0.015797    
2023-01-06 16:36:48,463 - Epoch: [29][  200/  246]    Overall Loss 0.334454    Objective Loss 0.334454                                        LR 0.000060    Time 0.015606    
2023-01-06 16:36:48,586 - Epoch: [29][  210/  246]    Overall Loss 0.335207    Objective Loss 0.335207                                        LR 0.000060    Time 0.015443    
2023-01-06 16:36:48,706 - Epoch: [29][  220/  246]    Overall Loss 0.335879    Objective Loss 0.335879                                        LR 0.000060    Time 0.015286    
2023-01-06 16:36:48,826 - Epoch: [29][  230/  246]    Overall Loss 0.336406    Objective Loss 0.336406                                        LR 0.000060    Time 0.015143    
2023-01-06 16:36:48,959 - Epoch: [29][  240/  246]    Overall Loss 0.336930    Objective Loss 0.336930                                        LR 0.000060    Time 0.015064    
2023-01-06 16:36:49,019 - Epoch: [29][  246/  246]    Overall Loss 0.336510    Objective Loss 0.336510    Top1 88.516746    LR 0.000060    Time 0.014938    
2023-01-06 16:36:49,131 - --- validate (epoch=29)-----------
2023-01-06 16:36:49,131 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:49,559 - Epoch: [29][   10/   28]    Loss 0.324636    Top1 87.617188    
2023-01-06 16:36:49,660 - Epoch: [29][   20/   28]    Loss 0.331558    Top1 87.480469    
2023-01-06 16:36:49,711 - Epoch: [29][   28/   28]    Loss 0.336542    Top1 87.374749    
2023-01-06 16:36:49,872 - ==> Top1: 87.375    Loss: 0.337

2023-01-06 16:36:49,872 - ==> Confusion:
[[ 145    4  290]
 [   9   90  503]
 [  51   25 5869]]

2023-01-06 16:36:49,873 - ==> Best [Top1: 87.861   Sparsity:0.00   Params: 151104 on epoch: 27]
2023-01-06 16:36:49,873 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:49,878 - 

2023-01-06 16:36:49,878 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:50,522 - Epoch: [30][   10/  246]    Overall Loss 0.335393    Objective Loss 0.335393                                        LR 0.000060    Time 0.064339    
2023-01-06 16:36:50,642 - Epoch: [30][   20/  246]    Overall Loss 0.337437    Objective Loss 0.337437                                        LR 0.000060    Time 0.038143    
2023-01-06 16:36:50,781 - Epoch: [30][   30/  246]    Overall Loss 0.329701    Objective Loss 0.329701                                        LR 0.000060    Time 0.030032    
2023-01-06 16:36:50,914 - Epoch: [30][   40/  246]    Overall Loss 0.338771    Objective Loss 0.338771                                        LR 0.000060    Time 0.025857    
2023-01-06 16:36:51,049 - Epoch: [30][   50/  246]    Overall Loss 0.341565    Objective Loss 0.341565                                        LR 0.000060    Time 0.023367    
2023-01-06 16:36:51,188 - Epoch: [30][   60/  246]    Overall Loss 0.341535    Objective Loss 0.341535                                        LR 0.000060    Time 0.021784    
2023-01-06 16:36:51,327 - Epoch: [30][   70/  246]    Overall Loss 0.342023    Objective Loss 0.342023                                        LR 0.000060    Time 0.020650    
2023-01-06 16:36:51,465 - Epoch: [30][   80/  246]    Overall Loss 0.342608    Objective Loss 0.342608                                        LR 0.000060    Time 0.019794    
2023-01-06 16:36:51,603 - Epoch: [30][   90/  246]    Overall Loss 0.342192    Objective Loss 0.342192                                        LR 0.000060    Time 0.019126    
2023-01-06 16:36:51,739 - Epoch: [30][  100/  246]    Overall Loss 0.339815    Objective Loss 0.339815                                        LR 0.000060    Time 0.018561    
2023-01-06 16:36:51,868 - Epoch: [30][  110/  246]    Overall Loss 0.339780    Objective Loss 0.339780                                        LR 0.000060    Time 0.018048    
2023-01-06 16:36:51,992 - Epoch: [30][  120/  246]    Overall Loss 0.336840    Objective Loss 0.336840                                        LR 0.000060    Time 0.017569    
2023-01-06 16:36:52,123 - Epoch: [30][  130/  246]    Overall Loss 0.335227    Objective Loss 0.335227                                        LR 0.000060    Time 0.017223    
2023-01-06 16:36:52,254 - Epoch: [30][  140/  246]    Overall Loss 0.334361    Objective Loss 0.334361                                        LR 0.000060    Time 0.016931    
2023-01-06 16:36:52,388 - Epoch: [30][  150/  246]    Overall Loss 0.334929    Objective Loss 0.334929                                        LR 0.000060    Time 0.016676    
2023-01-06 16:36:52,523 - Epoch: [30][  160/  246]    Overall Loss 0.333900    Objective Loss 0.333900                                        LR 0.000060    Time 0.016475    
2023-01-06 16:36:52,655 - Epoch: [30][  170/  246]    Overall Loss 0.333888    Objective Loss 0.333888                                        LR 0.000060    Time 0.016282    
2023-01-06 16:36:52,795 - Epoch: [30][  180/  246]    Overall Loss 0.332834    Objective Loss 0.332834                                        LR 0.000060    Time 0.016143    
2023-01-06 16:36:52,929 - Epoch: [30][  190/  246]    Overall Loss 0.333147    Objective Loss 0.333147                                        LR 0.000060    Time 0.015997    
2023-01-06 16:36:53,073 - Epoch: [30][  200/  246]    Overall Loss 0.333935    Objective Loss 0.333935                                        LR 0.000060    Time 0.015917    
2023-01-06 16:36:53,224 - Epoch: [30][  210/  246]    Overall Loss 0.333979    Objective Loss 0.333979                                        LR 0.000060    Time 0.015876    
2023-01-06 16:36:53,373 - Epoch: [30][  220/  246]    Overall Loss 0.333453    Objective Loss 0.333453                                        LR 0.000060    Time 0.015828    
2023-01-06 16:36:53,524 - Epoch: [30][  230/  246]    Overall Loss 0.333242    Objective Loss 0.333242                                        LR 0.000060    Time 0.015793    
2023-01-06 16:36:53,685 - Epoch: [30][  240/  246]    Overall Loss 0.332467    Objective Loss 0.332467                                        LR 0.000060    Time 0.015805    
2023-01-06 16:36:53,752 - Epoch: [30][  246/  246]    Overall Loss 0.333332    Objective Loss 0.333332    Top1 83.971292    LR 0.000060    Time 0.015693    
2023-01-06 16:36:53,915 - --- validate (epoch=30)-----------
2023-01-06 16:36:53,915 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:54,348 - Epoch: [30][   10/   28]    Loss 0.335516    Top1 87.421875    
2023-01-06 16:36:54,448 - Epoch: [30][   20/   28]    Loss 0.322211    Top1 87.968750    
2023-01-06 16:36:54,498 - Epoch: [30][   28/   28]    Loss 0.323505    Top1 88.133410    
2023-01-06 16:36:54,657 - ==> Top1: 88.133    Loss: 0.324

2023-01-06 16:36:54,657 - ==> Confusion:
[[ 181    4  254]
 [  13  122  467]
 [  58   33 5854]]

2023-01-06 16:36:54,658 - ==> Best [Top1: 88.133   Sparsity:0.00   Params: 151104 on epoch: 30]
2023-01-06 16:36:54,658 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:54,665 - 

2023-01-06 16:36:54,665 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:36:55,182 - Epoch: [31][   10/  246]    Overall Loss 0.341534    Objective Loss 0.341534                                        LR 0.000060    Time 0.051658    
2023-01-06 16:36:55,319 - Epoch: [31][   20/  246]    Overall Loss 0.339659    Objective Loss 0.339659                                        LR 0.000060    Time 0.032637    
2023-01-06 16:36:55,458 - Epoch: [31][   30/  246]    Overall Loss 0.339649    Objective Loss 0.339649                                        LR 0.000060    Time 0.026406    
2023-01-06 16:36:55,605 - Epoch: [31][   40/  246]    Overall Loss 0.336706    Objective Loss 0.336706                                        LR 0.000060    Time 0.023464    
2023-01-06 16:36:55,748 - Epoch: [31][   50/  246]    Overall Loss 0.339198    Objective Loss 0.339198                                        LR 0.000060    Time 0.021625    
2023-01-06 16:36:55,895 - Epoch: [31][   60/  246]    Overall Loss 0.338990    Objective Loss 0.338990                                        LR 0.000060    Time 0.020457    
2023-01-06 16:36:56,040 - Epoch: [31][   70/  246]    Overall Loss 0.336451    Objective Loss 0.336451                                        LR 0.000060    Time 0.019604    
2023-01-06 16:36:56,189 - Epoch: [31][   80/  246]    Overall Loss 0.334724    Objective Loss 0.334724                                        LR 0.000060    Time 0.019009    
2023-01-06 16:36:56,334 - Epoch: [31][   90/  246]    Overall Loss 0.334755    Objective Loss 0.334755                                        LR 0.000060    Time 0.018507    
2023-01-06 16:36:56,478 - Epoch: [31][  100/  246]    Overall Loss 0.333398    Objective Loss 0.333398                                        LR 0.000060    Time 0.018090    
2023-01-06 16:36:56,617 - Epoch: [31][  110/  246]    Overall Loss 0.332230    Objective Loss 0.332230                                        LR 0.000060    Time 0.017707    
2023-01-06 16:36:56,757 - Epoch: [31][  120/  246]    Overall Loss 0.332329    Objective Loss 0.332329                                        LR 0.000060    Time 0.017399    
2023-01-06 16:36:56,898 - Epoch: [31][  130/  246]    Overall Loss 0.330720    Objective Loss 0.330720                                        LR 0.000060    Time 0.017140    
2023-01-06 16:36:57,038 - Epoch: [31][  140/  246]    Overall Loss 0.330224    Objective Loss 0.330224                                        LR 0.000060    Time 0.016910    
2023-01-06 16:36:57,181 - Epoch: [31][  150/  246]    Overall Loss 0.330195    Objective Loss 0.330195                                        LR 0.000060    Time 0.016736    
2023-01-06 16:36:57,324 - Epoch: [31][  160/  246]    Overall Loss 0.330068    Objective Loss 0.330068                                        LR 0.000060    Time 0.016581    
2023-01-06 16:36:57,464 - Epoch: [31][  170/  246]    Overall Loss 0.330555    Objective Loss 0.330555                                        LR 0.000060    Time 0.016431    
2023-01-06 16:36:57,607 - Epoch: [31][  180/  246]    Overall Loss 0.332034    Objective Loss 0.332034                                        LR 0.000060    Time 0.016310    
2023-01-06 16:36:57,747 - Epoch: [31][  190/  246]    Overall Loss 0.331471    Objective Loss 0.331471                                        LR 0.000060    Time 0.016183    
2023-01-06 16:36:57,891 - Epoch: [31][  200/  246]    Overall Loss 0.330800    Objective Loss 0.330800                                        LR 0.000060    Time 0.016094    
2023-01-06 16:36:58,032 - Epoch: [31][  210/  246]    Overall Loss 0.330942    Objective Loss 0.330942                                        LR 0.000060    Time 0.015999    
2023-01-06 16:36:58,175 - Epoch: [31][  220/  246]    Overall Loss 0.330760    Objective Loss 0.330760                                        LR 0.000060    Time 0.015918    
2023-01-06 16:36:58,314 - Epoch: [31][  230/  246]    Overall Loss 0.330563    Objective Loss 0.330563                                        LR 0.000060    Time 0.015830    
2023-01-06 16:36:58,467 - Epoch: [31][  240/  246]    Overall Loss 0.330637    Objective Loss 0.330637                                        LR 0.000060    Time 0.015807    
2023-01-06 16:36:58,532 - Epoch: [31][  246/  246]    Overall Loss 0.330103    Objective Loss 0.330103    Top1 85.885167    LR 0.000060    Time 0.015687    
2023-01-06 16:36:58,688 - --- validate (epoch=31)-----------
2023-01-06 16:36:58,689 - 6986 samples (256 per mini-batch)
2023-01-06 16:36:59,252 - Epoch: [31][   10/   28]    Loss 0.332202    Top1 88.085938    
2023-01-06 16:36:59,356 - Epoch: [31][   20/   28]    Loss 0.331377    Top1 87.949219    
2023-01-06 16:36:59,407 - Epoch: [31][   28/   28]    Loss 0.324296    Top1 88.204981    
2023-01-06 16:36:59,567 - ==> Top1: 88.205    Loss: 0.324

2023-01-06 16:36:59,567 - ==> Confusion:
[[ 165   10  264]
 [   8  166  428]
 [  57   57 5831]]

2023-01-06 16:36:59,568 - ==> Best [Top1: 88.205   Sparsity:0.00   Params: 151104 on epoch: 31]
2023-01-06 16:36:59,568 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:36:59,574 - 

2023-01-06 16:36:59,574 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:00,099 - Epoch: [32][   10/  246]    Overall Loss 0.351230    Objective Loss 0.351230                                        LR 0.000060    Time 0.052433    
2023-01-06 16:37:00,240 - Epoch: [32][   20/  246]    Overall Loss 0.358329    Objective Loss 0.358329                                        LR 0.000060    Time 0.033245    
2023-01-06 16:37:00,386 - Epoch: [32][   30/  246]    Overall Loss 0.344862    Objective Loss 0.344862                                        LR 0.000060    Time 0.027018    
2023-01-06 16:37:00,549 - Epoch: [32][   40/  246]    Overall Loss 0.338261    Objective Loss 0.338261                                        LR 0.000060    Time 0.024315    
2023-01-06 16:37:00,707 - Epoch: [32][   50/  246]    Overall Loss 0.336879    Objective Loss 0.336879                                        LR 0.000060    Time 0.022612    
2023-01-06 16:37:00,869 - Epoch: [32][   60/  246]    Overall Loss 0.333715    Objective Loss 0.333715                                        LR 0.000060    Time 0.021536    
2023-01-06 16:37:01,032 - Epoch: [32][   70/  246]    Overall Loss 0.331491    Objective Loss 0.331491                                        LR 0.000060    Time 0.020777    
2023-01-06 16:37:01,198 - Epoch: [32][   80/  246]    Overall Loss 0.332756    Objective Loss 0.332756                                        LR 0.000060    Time 0.020247    
2023-01-06 16:37:01,358 - Epoch: [32][   90/  246]    Overall Loss 0.333601    Objective Loss 0.333601                                        LR 0.000060    Time 0.019768    
2023-01-06 16:37:01,519 - Epoch: [32][  100/  246]    Overall Loss 0.332396    Objective Loss 0.332396                                        LR 0.000060    Time 0.019400    
2023-01-06 16:37:01,679 - Epoch: [32][  110/  246]    Overall Loss 0.333144    Objective Loss 0.333144                                        LR 0.000060    Time 0.019087    
2023-01-06 16:37:01,833 - Epoch: [32][  120/  246]    Overall Loss 0.332966    Objective Loss 0.332966                                        LR 0.000060    Time 0.018780    
2023-01-06 16:37:01,986 - Epoch: [32][  130/  246]    Overall Loss 0.332208    Objective Loss 0.332208                                        LR 0.000060    Time 0.018507    
2023-01-06 16:37:02,139 - Epoch: [32][  140/  246]    Overall Loss 0.332807    Objective Loss 0.332807                                        LR 0.000060    Time 0.018272    
2023-01-06 16:37:02,292 - Epoch: [32][  150/  246]    Overall Loss 0.333531    Objective Loss 0.333531                                        LR 0.000060    Time 0.018074    
2023-01-06 16:37:02,446 - Epoch: [32][  160/  246]    Overall Loss 0.333255    Objective Loss 0.333255                                        LR 0.000060    Time 0.017903    
2023-01-06 16:37:02,600 - Epoch: [32][  170/  246]    Overall Loss 0.331439    Objective Loss 0.331439                                        LR 0.000060    Time 0.017753    
2023-01-06 16:37:02,752 - Epoch: [32][  180/  246]    Overall Loss 0.331342    Objective Loss 0.331342                                        LR 0.000060    Time 0.017609    
2023-01-06 16:37:02,904 - Epoch: [32][  190/  246]    Overall Loss 0.330164    Objective Loss 0.330164                                        LR 0.000060    Time 0.017480    
2023-01-06 16:37:03,055 - Epoch: [32][  200/  246]    Overall Loss 0.330171    Objective Loss 0.330171                                        LR 0.000060    Time 0.017360    
2023-01-06 16:37:03,208 - Epoch: [32][  210/  246]    Overall Loss 0.330432    Objective Loss 0.330432                                        LR 0.000060    Time 0.017260    
2023-01-06 16:37:03,361 - Epoch: [32][  220/  246]    Overall Loss 0.330030    Objective Loss 0.330030                                        LR 0.000060    Time 0.017169    
2023-01-06 16:37:03,515 - Epoch: [32][  230/  246]    Overall Loss 0.329650    Objective Loss 0.329650                                        LR 0.000060    Time 0.017090    
2023-01-06 16:37:03,686 - Epoch: [32][  240/  246]    Overall Loss 0.329725    Objective Loss 0.329725                                        LR 0.000060    Time 0.017092    
2023-01-06 16:37:03,755 - Epoch: [32][  246/  246]    Overall Loss 0.329274    Objective Loss 0.329274    Top1 88.755981    LR 0.000060    Time 0.016954    
2023-01-06 16:37:03,916 - --- validate (epoch=32)-----------
2023-01-06 16:37:03,916 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:04,339 - Epoch: [32][   10/   28]    Loss 0.318749    Top1 88.242188    
2023-01-06 16:37:04,442 - Epoch: [32][   20/   28]    Loss 0.317974    Top1 88.437500    
2023-01-06 16:37:04,493 - Epoch: [32][   28/   28]    Loss 0.322703    Top1 88.305182    
2023-01-06 16:37:04,615 - ==> Top1: 88.305    Loss: 0.323

2023-01-06 16:37:04,616 - ==> Confusion:
[[ 168    6  265]
 [  10  153  439]
 [  50   47 5848]]

2023-01-06 16:37:04,616 - ==> Best [Top1: 88.305   Sparsity:0.00   Params: 151104 on epoch: 32]
2023-01-06 16:37:04,617 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:04,623 - 

2023-01-06 16:37:04,623 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:05,289 - Epoch: [33][   10/  246]    Overall Loss 0.334812    Objective Loss 0.334812                                        LR 0.000060    Time 0.066526    
2023-01-06 16:37:05,435 - Epoch: [33][   20/  246]    Overall Loss 0.332027    Objective Loss 0.332027                                        LR 0.000060    Time 0.040543    
2023-01-06 16:37:05,579 - Epoch: [33][   30/  246]    Overall Loss 0.330934    Objective Loss 0.330934                                        LR 0.000060    Time 0.031826    
2023-01-06 16:37:05,724 - Epoch: [33][   40/  246]    Overall Loss 0.329971    Objective Loss 0.329971                                        LR 0.000060    Time 0.027482    
2023-01-06 16:37:05,872 - Epoch: [33][   50/  246]    Overall Loss 0.332549    Objective Loss 0.332549                                        LR 0.000060    Time 0.024930    
2023-01-06 16:37:06,022 - Epoch: [33][   60/  246]    Overall Loss 0.332344    Objective Loss 0.332344                                        LR 0.000060    Time 0.023274    
2023-01-06 16:37:06,171 - Epoch: [33][   70/  246]    Overall Loss 0.334492    Objective Loss 0.334492                                        LR 0.000060    Time 0.022073    
2023-01-06 16:37:06,326 - Epoch: [33][   80/  246]    Overall Loss 0.335888    Objective Loss 0.335888                                        LR 0.000060    Time 0.021241    
2023-01-06 16:37:06,477 - Epoch: [33][   90/  246]    Overall Loss 0.334597    Objective Loss 0.334597                                        LR 0.000060    Time 0.020551    
2023-01-06 16:37:06,631 - Epoch: [33][  100/  246]    Overall Loss 0.333806    Objective Loss 0.333806                                        LR 0.000060    Time 0.020036    
2023-01-06 16:37:06,782 - Epoch: [33][  110/  246]    Overall Loss 0.333635    Objective Loss 0.333635                                        LR 0.000060    Time 0.019580    
2023-01-06 16:37:06,937 - Epoch: [33][  120/  246]    Overall Loss 0.333568    Objective Loss 0.333568                                        LR 0.000060    Time 0.019234    
2023-01-06 16:37:07,080 - Epoch: [33][  130/  246]    Overall Loss 0.333743    Objective Loss 0.333743                                        LR 0.000060    Time 0.018857    
2023-01-06 16:37:07,222 - Epoch: [33][  140/  246]    Overall Loss 0.333934    Objective Loss 0.333934                                        LR 0.000060    Time 0.018515    
2023-01-06 16:37:07,361 - Epoch: [33][  150/  246]    Overall Loss 0.333608    Objective Loss 0.333608                                        LR 0.000060    Time 0.018205    
2023-01-06 16:37:07,503 - Epoch: [33][  160/  246]    Overall Loss 0.333455    Objective Loss 0.333455                                        LR 0.000060    Time 0.017957    
2023-01-06 16:37:07,644 - Epoch: [33][  170/  246]    Overall Loss 0.332536    Objective Loss 0.332536                                        LR 0.000060    Time 0.017723    
2023-01-06 16:37:07,783 - Epoch: [33][  180/  246]    Overall Loss 0.331568    Objective Loss 0.331568                                        LR 0.000060    Time 0.017512    
2023-01-06 16:37:07,927 - Epoch: [33][  190/  246]    Overall Loss 0.330197    Objective Loss 0.330197                                        LR 0.000060    Time 0.017344    
2023-01-06 16:37:08,072 - Epoch: [33][  200/  246]    Overall Loss 0.328605    Objective Loss 0.328605                                        LR 0.000060    Time 0.017200    
2023-01-06 16:37:08,208 - Epoch: [33][  210/  246]    Overall Loss 0.327712    Objective Loss 0.327712                                        LR 0.000060    Time 0.017029    
2023-01-06 16:37:08,348 - Epoch: [33][  220/  246]    Overall Loss 0.327349    Objective Loss 0.327349                                        LR 0.000060    Time 0.016890    
2023-01-06 16:37:08,485 - Epoch: [33][  230/  246]    Overall Loss 0.327664    Objective Loss 0.327664                                        LR 0.000060    Time 0.016746    
2023-01-06 16:37:08,641 - Epoch: [33][  240/  246]    Overall Loss 0.325647    Objective Loss 0.325647                                        LR 0.000060    Time 0.016698    
2023-01-06 16:37:08,708 - Epoch: [33][  246/  246]    Overall Loss 0.325372    Objective Loss 0.325372    Top1 89.234450    LR 0.000060    Time 0.016562    
2023-01-06 16:37:08,844 - --- validate (epoch=33)-----------
2023-01-06 16:37:08,844 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:09,274 - Epoch: [33][   10/   28]    Loss 0.342736    Top1 86.992188    
2023-01-06 16:37:09,376 - Epoch: [33][   20/   28]    Loss 0.333399    Top1 87.675781    
2023-01-06 16:37:09,424 - Epoch: [33][   28/   28]    Loss 0.330109    Top1 87.732608    
2023-01-06 16:37:09,583 - ==> Top1: 87.733    Loss: 0.330

2023-01-06 16:37:09,583 - ==> Confusion:
[[ 174    5  260]
 [  20  119  463]
 [  71   38 5836]]

2023-01-06 16:37:09,584 - ==> Best [Top1: 88.305   Sparsity:0.00   Params: 151104 on epoch: 32]
2023-01-06 16:37:09,584 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:09,589 - 

2023-01-06 16:37:09,589 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:10,130 - Epoch: [34][   10/  246]    Overall Loss 0.332445    Objective Loss 0.332445                                        LR 0.000060    Time 0.053970    
2023-01-06 16:37:10,278 - Epoch: [34][   20/  246]    Overall Loss 0.331376    Objective Loss 0.331376                                        LR 0.000060    Time 0.034394    
2023-01-06 16:37:10,428 - Epoch: [34][   30/  246]    Overall Loss 0.334047    Objective Loss 0.334047                                        LR 0.000060    Time 0.027893    
2023-01-06 16:37:10,575 - Epoch: [34][   40/  246]    Overall Loss 0.327057    Objective Loss 0.327057                                        LR 0.000060    Time 0.024582    
2023-01-06 16:37:10,722 - Epoch: [34][   50/  246]    Overall Loss 0.329475    Objective Loss 0.329475                                        LR 0.000060    Time 0.022601    
2023-01-06 16:37:10,870 - Epoch: [34][   60/  246]    Overall Loss 0.329572    Objective Loss 0.329572                                        LR 0.000060    Time 0.021300    
2023-01-06 16:37:11,023 - Epoch: [34][   70/  246]    Overall Loss 0.328556    Objective Loss 0.328556                                        LR 0.000060    Time 0.020433    
2023-01-06 16:37:11,174 - Epoch: [34][   80/  246]    Overall Loss 0.327644    Objective Loss 0.327644                                        LR 0.000060    Time 0.019764    
2023-01-06 16:37:11,328 - Epoch: [34][   90/  246]    Overall Loss 0.327474    Objective Loss 0.327474                                        LR 0.000060    Time 0.019267    
2023-01-06 16:37:11,478 - Epoch: [34][  100/  246]    Overall Loss 0.326686    Objective Loss 0.326686                                        LR 0.000060    Time 0.018836    
2023-01-06 16:37:11,629 - Epoch: [34][  110/  246]    Overall Loss 0.328163    Objective Loss 0.328163                                        LR 0.000060    Time 0.018501    
2023-01-06 16:37:11,777 - Epoch: [34][  120/  246]    Overall Loss 0.326659    Objective Loss 0.326659                                        LR 0.000060    Time 0.018185    
2023-01-06 16:37:11,931 - Epoch: [34][  130/  246]    Overall Loss 0.324878    Objective Loss 0.324878                                        LR 0.000060    Time 0.017964    
2023-01-06 16:37:12,088 - Epoch: [34][  140/  246]    Overall Loss 0.325195    Objective Loss 0.325195                                        LR 0.000060    Time 0.017801    
2023-01-06 16:37:12,239 - Epoch: [34][  150/  246]    Overall Loss 0.325720    Objective Loss 0.325720                                        LR 0.000060    Time 0.017623    
2023-01-06 16:37:12,387 - Epoch: [34][  160/  246]    Overall Loss 0.324853    Objective Loss 0.324853                                        LR 0.000060    Time 0.017439    
2023-01-06 16:37:12,542 - Epoch: [34][  170/  246]    Overall Loss 0.325727    Objective Loss 0.325727                                        LR 0.000060    Time 0.017322    
2023-01-06 16:37:12,694 - Epoch: [34][  180/  246]    Overall Loss 0.324559    Objective Loss 0.324559                                        LR 0.000060    Time 0.017203    
2023-01-06 16:37:12,849 - Epoch: [34][  190/  246]    Overall Loss 0.323972    Objective Loss 0.323972                                        LR 0.000060    Time 0.017113    
2023-01-06 16:37:13,002 - Epoch: [34][  200/  246]    Overall Loss 0.322663    Objective Loss 0.322663                                        LR 0.000060    Time 0.017020    
2023-01-06 16:37:13,154 - Epoch: [34][  210/  246]    Overall Loss 0.323303    Objective Loss 0.323303                                        LR 0.000060    Time 0.016934    
2023-01-06 16:37:13,307 - Epoch: [34][  220/  246]    Overall Loss 0.323488    Objective Loss 0.323488                                        LR 0.000060    Time 0.016855    
2023-01-06 16:37:13,454 - Epoch: [34][  230/  246]    Overall Loss 0.322528    Objective Loss 0.322528                                        LR 0.000060    Time 0.016762    
2023-01-06 16:37:13,614 - Epoch: [34][  240/  246]    Overall Loss 0.321903    Objective Loss 0.321903                                        LR 0.000060    Time 0.016728    
2023-01-06 16:37:13,680 - Epoch: [34][  246/  246]    Overall Loss 0.322153    Objective Loss 0.322153    Top1 86.842105    LR 0.000060    Time 0.016589    
2023-01-06 16:37:13,803 - --- validate (epoch=34)-----------
2023-01-06 16:37:13,804 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:14,242 - Epoch: [34][   10/   28]    Loss 0.323567    Top1 88.007812    
2023-01-06 16:37:14,341 - Epoch: [34][   20/   28]    Loss 0.324297    Top1 88.417969    
2023-01-06 16:37:14,392 - Epoch: [34][   28/   28]    Loss 0.317069    Top1 88.505583    
2023-01-06 16:37:14,530 - ==> Top1: 88.506    Loss: 0.317

2023-01-06 16:37:14,530 - ==> Confusion:
[[ 177    8  254]
 [   9  159  434]
 [  57   41 5847]]

2023-01-06 16:37:14,531 - ==> Best [Top1: 88.506   Sparsity:0.00   Params: 151104 on epoch: 34]
2023-01-06 16:37:14,531 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:14,538 - 

2023-01-06 16:37:14,538 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:15,188 - Epoch: [35][   10/  246]    Overall Loss 0.304394    Objective Loss 0.304394                                        LR 0.000060    Time 0.064911    
2023-01-06 16:37:15,326 - Epoch: [35][   20/  246]    Overall Loss 0.315749    Objective Loss 0.315749                                        LR 0.000060    Time 0.039375    
2023-01-06 16:37:15,467 - Epoch: [35][   30/  246]    Overall Loss 0.317736    Objective Loss 0.317736                                        LR 0.000060    Time 0.030913    
2023-01-06 16:37:15,600 - Epoch: [35][   40/  246]    Overall Loss 0.317725    Objective Loss 0.317725                                        LR 0.000060    Time 0.026507    
2023-01-06 16:37:15,733 - Epoch: [35][   50/  246]    Overall Loss 0.325547    Objective Loss 0.325547                                        LR 0.000060    Time 0.023856    
2023-01-06 16:37:15,870 - Epoch: [35][   60/  246]    Overall Loss 0.329622    Objective Loss 0.329622                                        LR 0.000060    Time 0.022156    
2023-01-06 16:37:16,010 - Epoch: [35][   70/  246]    Overall Loss 0.331008    Objective Loss 0.331008                                        LR 0.000060    Time 0.020982    
2023-01-06 16:37:16,152 - Epoch: [35][   80/  246]    Overall Loss 0.327957    Objective Loss 0.327957                                        LR 0.000060    Time 0.020138    
2023-01-06 16:37:16,296 - Epoch: [35][   90/  246]    Overall Loss 0.327219    Objective Loss 0.327219                                        LR 0.000060    Time 0.019489    
2023-01-06 16:37:16,442 - Epoch: [35][  100/  246]    Overall Loss 0.325008    Objective Loss 0.325008                                        LR 0.000060    Time 0.018997    
2023-01-06 16:37:16,586 - Epoch: [35][  110/  246]    Overall Loss 0.324805    Objective Loss 0.324805                                        LR 0.000060    Time 0.018578    
2023-01-06 16:37:16,731 - Epoch: [35][  120/  246]    Overall Loss 0.324834    Objective Loss 0.324834                                        LR 0.000060    Time 0.018239    
2023-01-06 16:37:16,866 - Epoch: [35][  130/  246]    Overall Loss 0.324258    Objective Loss 0.324258                                        LR 0.000060    Time 0.017868    
2023-01-06 16:37:16,992 - Epoch: [35][  140/  246]    Overall Loss 0.322439    Objective Loss 0.322439                                        LR 0.000060    Time 0.017492    
2023-01-06 16:37:17,124 - Epoch: [35][  150/  246]    Overall Loss 0.321558    Objective Loss 0.321558                                        LR 0.000060    Time 0.017204    
2023-01-06 16:37:17,247 - Epoch: [35][  160/  246]    Overall Loss 0.321943    Objective Loss 0.321943                                        LR 0.000060    Time 0.016897    
2023-01-06 16:37:17,376 - Epoch: [35][  170/  246]    Overall Loss 0.321145    Objective Loss 0.321145                                        LR 0.000060    Time 0.016655    
2023-01-06 16:37:17,495 - Epoch: [35][  180/  246]    Overall Loss 0.320738    Objective Loss 0.320738                                        LR 0.000060    Time 0.016390    
2023-01-06 16:37:17,627 - Epoch: [35][  190/  246]    Overall Loss 0.319542    Objective Loss 0.319542                                        LR 0.000060    Time 0.016219    
2023-01-06 16:37:17,757 - Epoch: [35][  200/  246]    Overall Loss 0.319282    Objective Loss 0.319282                                        LR 0.000060    Time 0.016060    
2023-01-06 16:37:17,891 - Epoch: [35][  210/  246]    Overall Loss 0.320122    Objective Loss 0.320122                                        LR 0.000060    Time 0.015930    
2023-01-06 16:37:18,016 - Epoch: [35][  220/  246]    Overall Loss 0.320026    Objective Loss 0.320026                                        LR 0.000060    Time 0.015775    
2023-01-06 16:37:18,156 - Epoch: [35][  230/  246]    Overall Loss 0.319455    Objective Loss 0.319455                                        LR 0.000060    Time 0.015693    
2023-01-06 16:37:18,305 - Epoch: [35][  240/  246]    Overall Loss 0.320474    Objective Loss 0.320474                                        LR 0.000060    Time 0.015659    
2023-01-06 16:37:18,369 - Epoch: [35][  246/  246]    Overall Loss 0.320572    Objective Loss 0.320572    Top1 85.885167    LR 0.000060    Time 0.015536    
2023-01-06 16:37:18,503 - --- validate (epoch=35)-----------
2023-01-06 16:37:18,503 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:18,930 - Epoch: [35][   10/   28]    Loss 0.310260    Top1 88.632812    
2023-01-06 16:37:19,023 - Epoch: [35][   20/   28]    Loss 0.316059    Top1 88.281250    
2023-01-06 16:37:19,072 - Epoch: [35][   28/   28]    Loss 0.317834    Top1 88.190667    
2023-01-06 16:37:19,218 - ==> Top1: 88.191    Loss: 0.318

2023-01-06 16:37:19,219 - ==> Confusion:
[[ 177    4  258]
 [   7  126  469]
 [  62   25 5858]]

2023-01-06 16:37:19,220 - ==> Best [Top1: 88.506   Sparsity:0.00   Params: 151104 on epoch: 34]
2023-01-06 16:37:19,220 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:19,225 - 

2023-01-06 16:37:19,225 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:19,896 - Epoch: [36][   10/  246]    Overall Loss 0.349058    Objective Loss 0.349058                                        LR 0.000060    Time 0.067089    
2023-01-06 16:37:20,036 - Epoch: [36][   20/  246]    Overall Loss 0.335445    Objective Loss 0.335445                                        LR 0.000060    Time 0.040504    
2023-01-06 16:37:20,177 - Epoch: [36][   30/  246]    Overall Loss 0.331719    Objective Loss 0.331719                                        LR 0.000060    Time 0.031695    
2023-01-06 16:37:20,315 - Epoch: [36][   40/  246]    Overall Loss 0.329204    Objective Loss 0.329204                                        LR 0.000060    Time 0.027204    
2023-01-06 16:37:20,458 - Epoch: [36][   50/  246]    Overall Loss 0.325518    Objective Loss 0.325518                                        LR 0.000060    Time 0.024609    
2023-01-06 16:37:20,594 - Epoch: [36][   60/  246]    Overall Loss 0.322848    Objective Loss 0.322848                                        LR 0.000060    Time 0.022778    
2023-01-06 16:37:20,726 - Epoch: [36][   70/  246]    Overall Loss 0.321909    Objective Loss 0.321909                                        LR 0.000060    Time 0.021397    
2023-01-06 16:37:20,862 - Epoch: [36][   80/  246]    Overall Loss 0.319910    Objective Loss 0.319910                                        LR 0.000060    Time 0.020402    
2023-01-06 16:37:21,004 - Epoch: [36][   90/  246]    Overall Loss 0.321780    Objective Loss 0.321780                                        LR 0.000060    Time 0.019703    
2023-01-06 16:37:21,143 - Epoch: [36][  100/  246]    Overall Loss 0.320844    Objective Loss 0.320844                                        LR 0.000060    Time 0.019128    
2023-01-06 16:37:21,275 - Epoch: [36][  110/  246]    Overall Loss 0.319769    Objective Loss 0.319769                                        LR 0.000060    Time 0.018581    
2023-01-06 16:37:21,404 - Epoch: [36][  120/  246]    Overall Loss 0.317955    Objective Loss 0.317955                                        LR 0.000060    Time 0.018105    
2023-01-06 16:37:21,541 - Epoch: [36][  130/  246]    Overall Loss 0.317622    Objective Loss 0.317622                                        LR 0.000060    Time 0.017763    
2023-01-06 16:37:21,671 - Epoch: [36][  140/  246]    Overall Loss 0.317135    Objective Loss 0.317135                                        LR 0.000060    Time 0.017416    
2023-01-06 16:37:21,806 - Epoch: [36][  150/  246]    Overall Loss 0.318535    Objective Loss 0.318535                                        LR 0.000060    Time 0.017156    
2023-01-06 16:37:21,943 - Epoch: [36][  160/  246]    Overall Loss 0.317772    Objective Loss 0.317772                                        LR 0.000060    Time 0.016936    
2023-01-06 16:37:22,081 - Epoch: [36][  170/  246]    Overall Loss 0.318117    Objective Loss 0.318117                                        LR 0.000060    Time 0.016748    
2023-01-06 16:37:22,212 - Epoch: [36][  180/  246]    Overall Loss 0.319190    Objective Loss 0.319190                                        LR 0.000060    Time 0.016544    
2023-01-06 16:37:22,347 - Epoch: [36][  190/  246]    Overall Loss 0.318967    Objective Loss 0.318967                                        LR 0.000060    Time 0.016384    
2023-01-06 16:37:22,483 - Epoch: [36][  200/  246]    Overall Loss 0.319150    Objective Loss 0.319150                                        LR 0.000060    Time 0.016242    
2023-01-06 16:37:22,616 - Epoch: [36][  210/  246]    Overall Loss 0.318706    Objective Loss 0.318706                                        LR 0.000060    Time 0.016101    
2023-01-06 16:37:22,752 - Epoch: [36][  220/  246]    Overall Loss 0.319329    Objective Loss 0.319329                                        LR 0.000060    Time 0.015985    
2023-01-06 16:37:22,894 - Epoch: [36][  230/  246]    Overall Loss 0.318965    Objective Loss 0.318965                                        LR 0.000060    Time 0.015905    
2023-01-06 16:37:23,043 - Epoch: [36][  240/  246]    Overall Loss 0.319089    Objective Loss 0.319089                                        LR 0.000060    Time 0.015860    
2023-01-06 16:37:23,106 - Epoch: [36][  246/  246]    Overall Loss 0.318292    Objective Loss 0.318292    Top1 88.038278    LR 0.000060    Time 0.015729    
2023-01-06 16:37:23,222 - --- validate (epoch=36)-----------
2023-01-06 16:37:23,223 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:23,675 - Epoch: [36][   10/   28]    Loss 0.324737    Top1 88.203125    
2023-01-06 16:37:23,776 - Epoch: [36][   20/   28]    Loss 0.311815    Top1 88.515625    
2023-01-06 16:37:23,829 - Epoch: [36][   28/   28]    Loss 0.318220    Top1 88.505583    
2023-01-06 16:37:23,970 - ==> Top1: 88.506    Loss: 0.318

2023-01-06 16:37:23,970 - ==> Confusion:
[[ 180    6  253]
 [  11  144  447]
 [  53   33 5859]]

2023-01-06 16:37:23,971 - ==> Best [Top1: 88.506   Sparsity:0.00   Params: 151104 on epoch: 36]
2023-01-06 16:37:23,971 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:23,978 - 

2023-01-06 16:37:23,978 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:24,483 - Epoch: [37][   10/  246]    Overall Loss 0.326712    Objective Loss 0.326712                                        LR 0.000060    Time 0.050460    
2023-01-06 16:37:24,604 - Epoch: [37][   20/  246]    Overall Loss 0.317971    Objective Loss 0.317971                                        LR 0.000060    Time 0.031254    
2023-01-06 16:37:24,723 - Epoch: [37][   30/  246]    Overall Loss 0.319379    Objective Loss 0.319379                                        LR 0.000060    Time 0.024781    
2023-01-06 16:37:24,845 - Epoch: [37][   40/  246]    Overall Loss 0.317365    Objective Loss 0.317365                                        LR 0.000060    Time 0.021627    
2023-01-06 16:37:24,963 - Epoch: [37][   50/  246]    Overall Loss 0.320651    Objective Loss 0.320651                                        LR 0.000060    Time 0.019640    
2023-01-06 16:37:25,087 - Epoch: [37][   60/  246]    Overall Loss 0.319112    Objective Loss 0.319112                                        LR 0.000060    Time 0.018439    
2023-01-06 16:37:25,219 - Epoch: [37][   70/  246]    Overall Loss 0.318638    Objective Loss 0.318638                                        LR 0.000060    Time 0.017685    
2023-01-06 16:37:25,357 - Epoch: [37][   80/  246]    Overall Loss 0.314868    Objective Loss 0.314868                                        LR 0.000060    Time 0.017190    
2023-01-06 16:37:25,501 - Epoch: [37][   90/  246]    Overall Loss 0.316188    Objective Loss 0.316188                                        LR 0.000060    Time 0.016871    
2023-01-06 16:37:25,645 - Epoch: [37][  100/  246]    Overall Loss 0.315414    Objective Loss 0.315414                                        LR 0.000060    Time 0.016600    
2023-01-06 16:37:25,784 - Epoch: [37][  110/  246]    Overall Loss 0.314766    Objective Loss 0.314766                                        LR 0.000060    Time 0.016346    
2023-01-06 16:37:25,933 - Epoch: [37][  120/  246]    Overall Loss 0.314819    Objective Loss 0.314819                                        LR 0.000060    Time 0.016220    
2023-01-06 16:37:26,090 - Epoch: [37][  130/  246]    Overall Loss 0.313713    Objective Loss 0.313713                                        LR 0.000060    Time 0.016179    
2023-01-06 16:37:26,245 - Epoch: [37][  140/  246]    Overall Loss 0.313634    Objective Loss 0.313634                                        LR 0.000060    Time 0.016128    
2023-01-06 16:37:26,397 - Epoch: [37][  150/  246]    Overall Loss 0.315536    Objective Loss 0.315536                                        LR 0.000060    Time 0.016065    
2023-01-06 16:37:26,551 - Epoch: [37][  160/  246]    Overall Loss 0.316202    Objective Loss 0.316202                                        LR 0.000060    Time 0.016022    
2023-01-06 16:37:26,702 - Epoch: [37][  170/  246]    Overall Loss 0.315626    Objective Loss 0.315626                                        LR 0.000060    Time 0.015963    
2023-01-06 16:37:26,857 - Epoch: [37][  180/  246]    Overall Loss 0.316067    Objective Loss 0.316067                                        LR 0.000060    Time 0.015935    
2023-01-06 16:37:27,013 - Epoch: [37][  190/  246]    Overall Loss 0.317417    Objective Loss 0.317417                                        LR 0.000060    Time 0.015914    
2023-01-06 16:37:27,169 - Epoch: [37][  200/  246]    Overall Loss 0.316433    Objective Loss 0.316433                                        LR 0.000060    Time 0.015897    
2023-01-06 16:37:27,325 - Epoch: [37][  210/  246]    Overall Loss 0.315999    Objective Loss 0.315999                                        LR 0.000060    Time 0.015879    
2023-01-06 16:37:27,481 - Epoch: [37][  220/  246]    Overall Loss 0.316375    Objective Loss 0.316375                                        LR 0.000060    Time 0.015866    
2023-01-06 16:37:27,633 - Epoch: [37][  230/  246]    Overall Loss 0.316296    Objective Loss 0.316296                                        LR 0.000060    Time 0.015837    
2023-01-06 16:37:27,798 - Epoch: [37][  240/  246]    Overall Loss 0.316266    Objective Loss 0.316266                                        LR 0.000060    Time 0.015861    
2023-01-06 16:37:27,863 - Epoch: [37][  246/  246]    Overall Loss 0.316652    Objective Loss 0.316652    Top1 87.320574    LR 0.000060    Time 0.015735    
2023-01-06 16:37:28,014 - --- validate (epoch=37)-----------
2023-01-06 16:37:28,014 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:28,441 - Epoch: [37][   10/   28]    Loss 0.327318    Top1 88.398438    
2023-01-06 16:37:28,537 - Epoch: [37][   20/   28]    Loss 0.315095    Top1 89.042969    
2023-01-06 16:37:28,585 - Epoch: [37][   28/   28]    Loss 0.317107    Top1 88.648726    
2023-01-06 16:37:28,742 - ==> Top1: 88.649    Loss: 0.317

2023-01-06 16:37:28,742 - ==> Confusion:
[[ 170    7  262]
 [   9  139  454]
 [  39   22 5884]]

2023-01-06 16:37:28,743 - ==> Best [Top1: 88.649   Sparsity:0.00   Params: 151104 on epoch: 37]
2023-01-06 16:37:28,743 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:28,749 - 

2023-01-06 16:37:28,749 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:29,405 - Epoch: [38][   10/  246]    Overall Loss 0.304393    Objective Loss 0.304393                                        LR 0.000060    Time 0.065470    
2023-01-06 16:37:29,532 - Epoch: [38][   20/  246]    Overall Loss 0.316175    Objective Loss 0.316175                                        LR 0.000060    Time 0.039057    
2023-01-06 16:37:29,664 - Epoch: [38][   30/  246]    Overall Loss 0.316186    Objective Loss 0.316186                                        LR 0.000060    Time 0.030432    
2023-01-06 16:37:29,799 - Epoch: [38][   40/  246]    Overall Loss 0.313577    Objective Loss 0.313577                                        LR 0.000060    Time 0.026198    
2023-01-06 16:37:29,946 - Epoch: [38][   50/  246]    Overall Loss 0.313866    Objective Loss 0.313866                                        LR 0.000060    Time 0.023875    
2023-01-06 16:37:30,080 - Epoch: [38][   60/  246]    Overall Loss 0.315745    Objective Loss 0.315745                                        LR 0.000060    Time 0.022136    
2023-01-06 16:37:30,218 - Epoch: [38][   70/  246]    Overall Loss 0.315895    Objective Loss 0.315895                                        LR 0.000060    Time 0.020928    
2023-01-06 16:37:30,354 - Epoch: [38][   80/  246]    Overall Loss 0.315092    Objective Loss 0.315092                                        LR 0.000060    Time 0.020008    
2023-01-06 16:37:30,492 - Epoch: [38][   90/  246]    Overall Loss 0.315685    Objective Loss 0.315685                                        LR 0.000060    Time 0.019303    
2023-01-06 16:37:30,621 - Epoch: [38][  100/  246]    Overall Loss 0.315225    Objective Loss 0.315225                                        LR 0.000060    Time 0.018656    
2023-01-06 16:37:30,767 - Epoch: [38][  110/  246]    Overall Loss 0.313307    Objective Loss 0.313307                                        LR 0.000060    Time 0.018283    
2023-01-06 16:37:30,907 - Epoch: [38][  120/  246]    Overall Loss 0.313536    Objective Loss 0.313536                                        LR 0.000060    Time 0.017920    
2023-01-06 16:37:31,051 - Epoch: [38][  130/  246]    Overall Loss 0.312045    Objective Loss 0.312045                                        LR 0.000060    Time 0.017652    
2023-01-06 16:37:31,197 - Epoch: [38][  140/  246]    Overall Loss 0.312171    Objective Loss 0.312171                                        LR 0.000060    Time 0.017431    
2023-01-06 16:37:31,341 - Epoch: [38][  150/  246]    Overall Loss 0.311450    Objective Loss 0.311450                                        LR 0.000060    Time 0.017227    
2023-01-06 16:37:31,485 - Epoch: [38][  160/  246]    Overall Loss 0.310764    Objective Loss 0.310764                                        LR 0.000060    Time 0.017048    
2023-01-06 16:37:31,632 - Epoch: [38][  170/  246]    Overall Loss 0.310152    Objective Loss 0.310152                                        LR 0.000060    Time 0.016908    
2023-01-06 16:37:31,774 - Epoch: [38][  180/  246]    Overall Loss 0.311566    Objective Loss 0.311566                                        LR 0.000060    Time 0.016755    
2023-01-06 16:37:31,921 - Epoch: [38][  190/  246]    Overall Loss 0.311336    Objective Loss 0.311336                                        LR 0.000060    Time 0.016643    
2023-01-06 16:37:32,069 - Epoch: [38][  200/  246]    Overall Loss 0.311128    Objective Loss 0.311128                                        LR 0.000060    Time 0.016551    
2023-01-06 16:37:32,202 - Epoch: [38][  210/  246]    Overall Loss 0.310639    Objective Loss 0.310639                                        LR 0.000060    Time 0.016397    
2023-01-06 16:37:32,343 - Epoch: [38][  220/  246]    Overall Loss 0.312096    Objective Loss 0.312096                                        LR 0.000060    Time 0.016291    
2023-01-06 16:37:32,481 - Epoch: [38][  230/  246]    Overall Loss 0.312809    Objective Loss 0.312809                                        LR 0.000060    Time 0.016179    
2023-01-06 16:37:32,637 - Epoch: [38][  240/  246]    Overall Loss 0.313492    Objective Loss 0.313492                                        LR 0.000060    Time 0.016153    
2023-01-06 16:37:32,700 - Epoch: [38][  246/  246]    Overall Loss 0.313493    Objective Loss 0.313493    Top1 88.995215    LR 0.000060    Time 0.016014    
2023-01-06 16:37:32,830 - --- validate (epoch=38)-----------
2023-01-06 16:37:32,830 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:33,270 - Epoch: [38][   10/   28]    Loss 0.305989    Top1 88.906250    
2023-01-06 16:37:33,381 - Epoch: [38][   20/   28]    Loss 0.314813    Top1 88.710938    
2023-01-06 16:37:33,434 - Epoch: [38][   28/   28]    Loss 0.311147    Top1 88.748926    
2023-01-06 16:37:33,558 - ==> Top1: 88.749    Loss: 0.311

2023-01-06 16:37:33,558 - ==> Confusion:
[[ 191    8  240]
 [  11  165  426]
 [  62   39 5844]]

2023-01-06 16:37:33,559 - ==> Best [Top1: 88.749   Sparsity:0.00   Params: 151104 on epoch: 38]
2023-01-06 16:37:33,559 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:33,566 - 

2023-01-06 16:37:33,566 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:34,096 - Epoch: [39][   10/  246]    Overall Loss 0.315988    Objective Loss 0.315988                                        LR 0.000060    Time 0.052934    
2023-01-06 16:37:34,233 - Epoch: [39][   20/  246]    Overall Loss 0.313860    Objective Loss 0.313860                                        LR 0.000060    Time 0.033287    
2023-01-06 16:37:34,374 - Epoch: [39][   30/  246]    Overall Loss 0.317817    Objective Loss 0.317817                                        LR 0.000060    Time 0.026867    
2023-01-06 16:37:34,530 - Epoch: [39][   40/  246]    Overall Loss 0.315603    Objective Loss 0.315603                                        LR 0.000060    Time 0.024034    
2023-01-06 16:37:34,671 - Epoch: [39][   50/  246]    Overall Loss 0.314919    Objective Loss 0.314919                                        LR 0.000060    Time 0.022041    
2023-01-06 16:37:34,822 - Epoch: [39][   60/  246]    Overall Loss 0.317271    Objective Loss 0.317271                                        LR 0.000060    Time 0.020878    
2023-01-06 16:37:34,974 - Epoch: [39][   70/  246]    Overall Loss 0.317221    Objective Loss 0.317221                                        LR 0.000060    Time 0.020068    
2023-01-06 16:37:35,143 - Epoch: [39][   80/  246]    Overall Loss 0.319267    Objective Loss 0.319267                                        LR 0.000060    Time 0.019665    
2023-01-06 16:37:35,311 - Epoch: [39][   90/  246]    Overall Loss 0.319206    Objective Loss 0.319206                                        LR 0.000060    Time 0.019336    
2023-01-06 16:37:35,472 - Epoch: [39][  100/  246]    Overall Loss 0.317565    Objective Loss 0.317565                                        LR 0.000060    Time 0.019008    
2023-01-06 16:37:35,639 - Epoch: [39][  110/  246]    Overall Loss 0.316442    Objective Loss 0.316442                                        LR 0.000060    Time 0.018795    
2023-01-06 16:37:35,804 - Epoch: [39][  120/  246]    Overall Loss 0.316027    Objective Loss 0.316027                                        LR 0.000060    Time 0.018602    
2023-01-06 16:37:35,958 - Epoch: [39][  130/  246]    Overall Loss 0.315867    Objective Loss 0.315867                                        LR 0.000060    Time 0.018355    
2023-01-06 16:37:36,120 - Epoch: [39][  140/  246]    Overall Loss 0.315217    Objective Loss 0.315217                                        LR 0.000060    Time 0.018192    
2023-01-06 16:37:36,273 - Epoch: [39][  150/  246]    Overall Loss 0.314859    Objective Loss 0.314859                                        LR 0.000060    Time 0.018002    
2023-01-06 16:37:36,426 - Epoch: [39][  160/  246]    Overall Loss 0.314186    Objective Loss 0.314186                                        LR 0.000060    Time 0.017828    
2023-01-06 16:37:36,586 - Epoch: [39][  170/  246]    Overall Loss 0.313627    Objective Loss 0.313627                                        LR 0.000060    Time 0.017718    
2023-01-06 16:37:36,746 - Epoch: [39][  180/  246]    Overall Loss 0.314601    Objective Loss 0.314601                                        LR 0.000060    Time 0.017619    
2023-01-06 16:37:36,887 - Epoch: [39][  190/  246]    Overall Loss 0.313000    Objective Loss 0.313000                                        LR 0.000060    Time 0.017433    
2023-01-06 16:37:37,031 - Epoch: [39][  200/  246]    Overall Loss 0.312622    Objective Loss 0.312622                                        LR 0.000060    Time 0.017277    
2023-01-06 16:37:37,204 - Epoch: [39][  210/  246]    Overall Loss 0.313148    Objective Loss 0.313148                                        LR 0.000060    Time 0.017276    
2023-01-06 16:37:37,371 - Epoch: [39][  220/  246]    Overall Loss 0.312999    Objective Loss 0.312999                                        LR 0.000060    Time 0.017248    
2023-01-06 16:37:37,533 - Epoch: [39][  230/  246]    Overall Loss 0.312976    Objective Loss 0.312976                                        LR 0.000060    Time 0.017200    
2023-01-06 16:37:37,709 - Epoch: [39][  240/  246]    Overall Loss 0.313391    Objective Loss 0.313391                                        LR 0.000060    Time 0.017217    
2023-01-06 16:37:37,785 - Epoch: [39][  246/  246]    Overall Loss 0.313090    Objective Loss 0.313090    Top1 88.277512    LR 0.000060    Time 0.017105    
2023-01-06 16:37:37,904 - --- validate (epoch=39)-----------
2023-01-06 16:37:37,905 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:38,338 - Epoch: [39][   10/   28]    Loss 0.307110    Top1 88.515625    
2023-01-06 16:37:38,443 - Epoch: [39][   20/   28]    Loss 0.307115    Top1 88.691406    
2023-01-06 16:37:38,494 - Epoch: [39][   28/   28]    Loss 0.304773    Top1 88.820498    
2023-01-06 16:37:38,626 - ==> Top1: 88.820    Loss: 0.305

2023-01-06 16:37:38,626 - ==> Confusion:
[[ 185    7  247]
 [  11  181  410]
 [  49   57 5839]]

2023-01-06 16:37:38,627 - ==> Best [Top1: 88.820   Sparsity:0.00   Params: 151104 on epoch: 39]
2023-01-06 16:37:38,627 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:38,634 - 

2023-01-06 16:37:38,634 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:39,314 - Epoch: [40][   10/  246]    Overall Loss 0.312975    Objective Loss 0.312975                                        LR 0.000036    Time 0.067984    
2023-01-06 16:37:39,476 - Epoch: [40][   20/  246]    Overall Loss 0.309528    Objective Loss 0.309528                                        LR 0.000036    Time 0.042063    
2023-01-06 16:37:39,632 - Epoch: [40][   30/  246]    Overall Loss 0.297943    Objective Loss 0.297943                                        LR 0.000036    Time 0.033206    
2023-01-06 16:37:39,787 - Epoch: [40][   40/  246]    Overall Loss 0.302127    Objective Loss 0.302127                                        LR 0.000036    Time 0.028771    
2023-01-06 16:37:39,935 - Epoch: [40][   50/  246]    Overall Loss 0.301416    Objective Loss 0.301416                                        LR 0.000036    Time 0.025969    
2023-01-06 16:37:40,083 - Epoch: [40][   60/  246]    Overall Loss 0.298225    Objective Loss 0.298225                                        LR 0.000036    Time 0.024109    
2023-01-06 16:37:40,235 - Epoch: [40][   70/  246]    Overall Loss 0.301647    Objective Loss 0.301647                                        LR 0.000036    Time 0.022822    
2023-01-06 16:37:40,389 - Epoch: [40][   80/  246]    Overall Loss 0.300012    Objective Loss 0.300012                                        LR 0.000036    Time 0.021898    
2023-01-06 16:37:40,534 - Epoch: [40][   90/  246]    Overall Loss 0.302421    Objective Loss 0.302421                                        LR 0.000036    Time 0.021066    
2023-01-06 16:37:40,677 - Epoch: [40][  100/  246]    Overall Loss 0.303378    Objective Loss 0.303378                                        LR 0.000036    Time 0.020384    
2023-01-06 16:37:40,821 - Epoch: [40][  110/  246]    Overall Loss 0.303112    Objective Loss 0.303112                                        LR 0.000036    Time 0.019841    
2023-01-06 16:37:40,969 - Epoch: [40][  120/  246]    Overall Loss 0.304706    Objective Loss 0.304706                                        LR 0.000036    Time 0.019414    
2023-01-06 16:37:41,112 - Epoch: [40][  130/  246]    Overall Loss 0.304951    Objective Loss 0.304951                                        LR 0.000036    Time 0.019020    
2023-01-06 16:37:41,258 - Epoch: [40][  140/  246]    Overall Loss 0.304420    Objective Loss 0.304420                                        LR 0.000036    Time 0.018701    
2023-01-06 16:37:41,399 - Epoch: [40][  150/  246]    Overall Loss 0.305031    Objective Loss 0.305031                                        LR 0.000036    Time 0.018387    
2023-01-06 16:37:41,543 - Epoch: [40][  160/  246]    Overall Loss 0.304853    Objective Loss 0.304853                                        LR 0.000036    Time 0.018138    
2023-01-06 16:37:41,686 - Epoch: [40][  170/  246]    Overall Loss 0.304500    Objective Loss 0.304500                                        LR 0.000036    Time 0.017909    
2023-01-06 16:37:41,830 - Epoch: [40][  180/  246]    Overall Loss 0.305173    Objective Loss 0.305173                                        LR 0.000036    Time 0.017713    
2023-01-06 16:37:41,978 - Epoch: [40][  190/  246]    Overall Loss 0.305498    Objective Loss 0.305498                                        LR 0.000036    Time 0.017556    
2023-01-06 16:37:42,126 - Epoch: [40][  200/  246]    Overall Loss 0.306202    Objective Loss 0.306202                                        LR 0.000036    Time 0.017415    
2023-01-06 16:37:42,261 - Epoch: [40][  210/  246]    Overall Loss 0.305105    Objective Loss 0.305105                                        LR 0.000036    Time 0.017227    
2023-01-06 16:37:42,395 - Epoch: [40][  220/  246]    Overall Loss 0.305836    Objective Loss 0.305836                                        LR 0.000036    Time 0.017052    
2023-01-06 16:37:42,537 - Epoch: [40][  230/  246]    Overall Loss 0.306902    Objective Loss 0.306902                                        LR 0.000036    Time 0.016923    
2023-01-06 16:37:42,690 - Epoch: [40][  240/  246]    Overall Loss 0.307224    Objective Loss 0.307224                                        LR 0.000036    Time 0.016854    
2023-01-06 16:37:42,755 - Epoch: [40][  246/  246]    Overall Loss 0.307272    Objective Loss 0.307272    Top1 88.755981    LR 0.000036    Time 0.016706    
2023-01-06 16:37:42,916 - --- validate (epoch=40)-----------
2023-01-06 16:37:42,916 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:43,353 - Epoch: [40][   10/   28]    Loss 0.294474    Top1 89.140625    
2023-01-06 16:37:43,449 - Epoch: [40][   20/   28]    Loss 0.303693    Top1 88.593750    
2023-01-06 16:37:43,499 - Epoch: [40][   28/   28]    Loss 0.304395    Top1 88.677355    
2023-01-06 16:37:43,638 - ==> Top1: 88.677    Loss: 0.304

2023-01-06 16:37:43,638 - ==> Confusion:
[[ 168    7  264]
 [   8  153  441]
 [  38   33 5874]]

2023-01-06 16:37:43,639 - ==> Best [Top1: 88.820   Sparsity:0.00   Params: 151104 on epoch: 39]
2023-01-06 16:37:43,640 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:43,645 - 

2023-01-06 16:37:43,645 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:44,309 - Epoch: [41][   10/  246]    Overall Loss 0.300979    Objective Loss 0.300979                                        LR 0.000036    Time 0.066332    
2023-01-06 16:37:44,448 - Epoch: [41][   20/  246]    Overall Loss 0.311093    Objective Loss 0.311093                                        LR 0.000036    Time 0.040109    
2023-01-06 16:37:44,590 - Epoch: [41][   30/  246]    Overall Loss 0.306440    Objective Loss 0.306440                                        LR 0.000036    Time 0.031468    
2023-01-06 16:37:44,735 - Epoch: [41][   40/  246]    Overall Loss 0.305476    Objective Loss 0.305476                                        LR 0.000036    Time 0.027199    
2023-01-06 16:37:44,879 - Epoch: [41][   50/  246]    Overall Loss 0.306579    Objective Loss 0.306579                                        LR 0.000036    Time 0.024646    
2023-01-06 16:37:45,020 - Epoch: [41][   60/  246]    Overall Loss 0.306798    Objective Loss 0.306798                                        LR 0.000036    Time 0.022876    
2023-01-06 16:37:45,163 - Epoch: [41][   70/  246]    Overall Loss 0.307852    Objective Loss 0.307852                                        LR 0.000036    Time 0.021645    
2023-01-06 16:37:45,304 - Epoch: [41][   80/  246]    Overall Loss 0.309963    Objective Loss 0.309963                                        LR 0.000036    Time 0.020697    
2023-01-06 16:37:45,447 - Epoch: [41][   90/  246]    Overall Loss 0.312235    Objective Loss 0.312235                                        LR 0.000036    Time 0.019980    
2023-01-06 16:37:45,592 - Epoch: [41][  100/  246]    Overall Loss 0.310149    Objective Loss 0.310149                                        LR 0.000036    Time 0.019426    
2023-01-06 16:37:45,739 - Epoch: [41][  110/  246]    Overall Loss 0.306564    Objective Loss 0.306564                                        LR 0.000036    Time 0.018994    
2023-01-06 16:37:45,886 - Epoch: [41][  120/  246]    Overall Loss 0.305307    Objective Loss 0.305307                                        LR 0.000036    Time 0.018636    
2023-01-06 16:37:46,031 - Epoch: [41][  130/  246]    Overall Loss 0.304954    Objective Loss 0.304954                                        LR 0.000036    Time 0.018317    
2023-01-06 16:37:46,178 - Epoch: [41][  140/  246]    Overall Loss 0.303833    Objective Loss 0.303833                                        LR 0.000036    Time 0.018053    
2023-01-06 16:37:46,323 - Epoch: [41][  150/  246]    Overall Loss 0.304356    Objective Loss 0.304356                                        LR 0.000036    Time 0.017812    
2023-01-06 16:37:46,470 - Epoch: [41][  160/  246]    Overall Loss 0.305179    Objective Loss 0.305179                                        LR 0.000036    Time 0.017616    
2023-01-06 16:37:46,616 - Epoch: [41][  170/  246]    Overall Loss 0.304331    Objective Loss 0.304331                                        LR 0.000036    Time 0.017437    
2023-01-06 16:37:46,763 - Epoch: [41][  180/  246]    Overall Loss 0.305710    Objective Loss 0.305710                                        LR 0.000036    Time 0.017278    
2023-01-06 16:37:46,908 - Epoch: [41][  190/  246]    Overall Loss 0.305935    Objective Loss 0.305935                                        LR 0.000036    Time 0.017131    
2023-01-06 16:37:47,055 - Epoch: [41][  200/  246]    Overall Loss 0.305494    Objective Loss 0.305494                                        LR 0.000036    Time 0.017007    
2023-01-06 16:37:47,196 - Epoch: [41][  210/  246]    Overall Loss 0.305690    Objective Loss 0.305690                                        LR 0.000036    Time 0.016866    
2023-01-06 16:37:47,332 - Epoch: [41][  220/  246]    Overall Loss 0.306275    Objective Loss 0.306275                                        LR 0.000036    Time 0.016719    
2023-01-06 16:37:47,469 - Epoch: [41][  230/  246]    Overall Loss 0.305146    Objective Loss 0.305146                                        LR 0.000036    Time 0.016583    
2023-01-06 16:37:47,619 - Epoch: [41][  240/  246]    Overall Loss 0.305657    Objective Loss 0.305657                                        LR 0.000036    Time 0.016518    
2023-01-06 16:37:47,683 - Epoch: [41][  246/  246]    Overall Loss 0.305147    Objective Loss 0.305147    Top1 90.669856    LR 0.000036    Time 0.016375    
2023-01-06 16:37:47,818 - --- validate (epoch=41)-----------
2023-01-06 16:37:47,818 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:48,237 - Epoch: [41][   10/   28]    Loss 0.300202    Top1 89.218750    
2023-01-06 16:37:48,333 - Epoch: [41][   20/   28]    Loss 0.305291    Top1 88.730469    
2023-01-06 16:37:48,384 - Epoch: [41][   28/   28]    Loss 0.303914    Top1 88.877756    
2023-01-06 16:37:48,516 - ==> Top1: 88.878    Loss: 0.304

2023-01-06 16:37:48,516 - ==> Confusion:
[[ 191    6  242]
 [  10  161  431]
 [  51   37 5857]]

2023-01-06 16:37:48,517 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 151104 on epoch: 41]
2023-01-06 16:37:48,517 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:48,524 - 

2023-01-06 16:37:48,524 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:49,022 - Epoch: [42][   10/  246]    Overall Loss 0.299817    Objective Loss 0.299817                                        LR 0.000036    Time 0.049790    
2023-01-06 16:37:49,159 - Epoch: [42][   20/  246]    Overall Loss 0.300955    Objective Loss 0.300955                                        LR 0.000036    Time 0.031691    
2023-01-06 16:37:49,294 - Epoch: [42][   30/  246]    Overall Loss 0.302131    Objective Loss 0.302131                                        LR 0.000036    Time 0.025644    
2023-01-06 16:37:49,435 - Epoch: [42][   40/  246]    Overall Loss 0.303511    Objective Loss 0.303511                                        LR 0.000036    Time 0.022740    
2023-01-06 16:37:49,565 - Epoch: [42][   50/  246]    Overall Loss 0.305588    Objective Loss 0.305588                                        LR 0.000036    Time 0.020789    
2023-01-06 16:37:49,702 - Epoch: [42][   60/  246]    Overall Loss 0.307938    Objective Loss 0.307938                                        LR 0.000036    Time 0.019595    
2023-01-06 16:37:49,836 - Epoch: [42][   70/  246]    Overall Loss 0.307776    Objective Loss 0.307776                                        LR 0.000036    Time 0.018700    
2023-01-06 16:37:49,978 - Epoch: [42][   80/  246]    Overall Loss 0.305742    Objective Loss 0.305742                                        LR 0.000036    Time 0.018135    
2023-01-06 16:37:50,118 - Epoch: [42][   90/  246]    Overall Loss 0.309199    Objective Loss 0.309199                                        LR 0.000036    Time 0.017677    
2023-01-06 16:37:50,256 - Epoch: [42][  100/  246]    Overall Loss 0.309415    Objective Loss 0.309415                                        LR 0.000036    Time 0.017280    
2023-01-06 16:37:50,393 - Epoch: [42][  110/  246]    Overall Loss 0.307901    Objective Loss 0.307901                                        LR 0.000036    Time 0.016952    
2023-01-06 16:37:50,529 - Epoch: [42][  120/  246]    Overall Loss 0.307428    Objective Loss 0.307428                                        LR 0.000036    Time 0.016676    
2023-01-06 16:37:50,674 - Epoch: [42][  130/  246]    Overall Loss 0.307522    Objective Loss 0.307522                                        LR 0.000036    Time 0.016500    
2023-01-06 16:37:50,811 - Epoch: [42][  140/  246]    Overall Loss 0.307489    Objective Loss 0.307489                                        LR 0.000036    Time 0.016298    
2023-01-06 16:37:50,948 - Epoch: [42][  150/  246]    Overall Loss 0.307597    Objective Loss 0.307597                                        LR 0.000036    Time 0.016122    
2023-01-06 16:37:51,093 - Epoch: [42][  160/  246]    Overall Loss 0.306785    Objective Loss 0.306785                                        LR 0.000036    Time 0.016022    
2023-01-06 16:37:51,238 - Epoch: [42][  170/  246]    Overall Loss 0.305590    Objective Loss 0.305590                                        LR 0.000036    Time 0.015928    
2023-01-06 16:37:51,383 - Epoch: [42][  180/  246]    Overall Loss 0.305634    Objective Loss 0.305634                                        LR 0.000036    Time 0.015848    
2023-01-06 16:37:51,529 - Epoch: [42][  190/  246]    Overall Loss 0.304859    Objective Loss 0.304859                                        LR 0.000036    Time 0.015782    
2023-01-06 16:37:51,664 - Epoch: [42][  200/  246]    Overall Loss 0.305402    Objective Loss 0.305402                                        LR 0.000036    Time 0.015666    
2023-01-06 16:37:51,801 - Epoch: [42][  210/  246]    Overall Loss 0.305992    Objective Loss 0.305992                                        LR 0.000036    Time 0.015569    
2023-01-06 16:37:51,937 - Epoch: [42][  220/  246]    Overall Loss 0.306199    Objective Loss 0.306199                                        LR 0.000036    Time 0.015481    
2023-01-06 16:37:52,073 - Epoch: [42][  230/  246]    Overall Loss 0.305472    Objective Loss 0.305472                                        LR 0.000036    Time 0.015395    
2023-01-06 16:37:52,224 - Epoch: [42][  240/  246]    Overall Loss 0.304468    Objective Loss 0.304468                                        LR 0.000036    Time 0.015383    
2023-01-06 16:37:52,289 - Epoch: [42][  246/  246]    Overall Loss 0.305073    Objective Loss 0.305073    Top1 87.320574    LR 0.000036    Time 0.015272    
2023-01-06 16:37:52,418 - --- validate (epoch=42)-----------
2023-01-06 16:37:52,418 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:52,840 - Epoch: [42][   10/   28]    Loss 0.301472    Top1 89.453125    
2023-01-06 16:37:52,943 - Epoch: [42][   20/   28]    Loss 0.295661    Top1 89.433594    
2023-01-06 16:37:52,992 - Epoch: [42][   28/   28]    Loss 0.302697    Top1 89.235614    
2023-01-06 16:37:53,123 - ==> Top1: 89.236    Loss: 0.303

2023-01-06 16:37:53,123 - ==> Confusion:
[[ 193   14  232]
 [  10  212  380]
 [  44   72 5829]]

2023-01-06 16:37:53,124 - ==> Best [Top1: 89.236   Sparsity:0.00   Params: 151104 on epoch: 42]
2023-01-06 16:37:53,124 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:53,131 - 

2023-01-06 16:37:53,131 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:53,817 - Epoch: [43][   10/  246]    Overall Loss 0.318385    Objective Loss 0.318385                                        LR 0.000036    Time 0.068522    
2023-01-06 16:37:53,960 - Epoch: [43][   20/  246]    Overall Loss 0.297767    Objective Loss 0.297767                                        LR 0.000036    Time 0.041414    
2023-01-06 16:37:54,096 - Epoch: [43][   30/  246]    Overall Loss 0.298407    Objective Loss 0.298407                                        LR 0.000036    Time 0.032134    
2023-01-06 16:37:54,233 - Epoch: [43][   40/  246]    Overall Loss 0.298434    Objective Loss 0.298434                                        LR 0.000036    Time 0.027509    
2023-01-06 16:37:54,368 - Epoch: [43][   50/  246]    Overall Loss 0.298210    Objective Loss 0.298210                                        LR 0.000036    Time 0.024709    
2023-01-06 16:37:54,503 - Epoch: [43][   60/  246]    Overall Loss 0.295372    Objective Loss 0.295372                                        LR 0.000036    Time 0.022833    
2023-01-06 16:37:54,638 - Epoch: [43][   70/  246]    Overall Loss 0.295742    Objective Loss 0.295742                                        LR 0.000036    Time 0.021483    
2023-01-06 16:37:54,772 - Epoch: [43][   80/  246]    Overall Loss 0.296285    Objective Loss 0.296285                                        LR 0.000036    Time 0.020479    
2023-01-06 16:37:54,906 - Epoch: [43][   90/  246]    Overall Loss 0.298347    Objective Loss 0.298347                                        LR 0.000036    Time 0.019682    
2023-01-06 16:37:55,039 - Epoch: [43][  100/  246]    Overall Loss 0.299714    Objective Loss 0.299714                                        LR 0.000036    Time 0.019034    
2023-01-06 16:37:55,171 - Epoch: [43][  110/  246]    Overall Loss 0.300083    Objective Loss 0.300083                                        LR 0.000036    Time 0.018500    
2023-01-06 16:37:55,304 - Epoch: [43][  120/  246]    Overall Loss 0.300435    Objective Loss 0.300435                                        LR 0.000036    Time 0.018065    
2023-01-06 16:37:55,436 - Epoch: [43][  130/  246]    Overall Loss 0.300643    Objective Loss 0.300643                                        LR 0.000036    Time 0.017687    
2023-01-06 16:37:55,570 - Epoch: [43][  140/  246]    Overall Loss 0.300819    Objective Loss 0.300819                                        LR 0.000036    Time 0.017382    
2023-01-06 16:37:55,705 - Epoch: [43][  150/  246]    Overall Loss 0.300642    Objective Loss 0.300642                                        LR 0.000036    Time 0.017119    
2023-01-06 16:37:55,840 - Epoch: [43][  160/  246]    Overall Loss 0.300592    Objective Loss 0.300592                                        LR 0.000036    Time 0.016888    
2023-01-06 16:37:55,972 - Epoch: [43][  170/  246]    Overall Loss 0.301953    Objective Loss 0.301953                                        LR 0.000036    Time 0.016668    
2023-01-06 16:37:56,108 - Epoch: [43][  180/  246]    Overall Loss 0.302990    Objective Loss 0.302990                                        LR 0.000036    Time 0.016497    
2023-01-06 16:37:56,246 - Epoch: [43][  190/  246]    Overall Loss 0.303365    Objective Loss 0.303365                                        LR 0.000036    Time 0.016350    
2023-01-06 16:37:56,381 - Epoch: [43][  200/  246]    Overall Loss 0.303453    Objective Loss 0.303453                                        LR 0.000036    Time 0.016207    
2023-01-06 16:37:56,518 - Epoch: [43][  210/  246]    Overall Loss 0.303713    Objective Loss 0.303713                                        LR 0.000036    Time 0.016087    
2023-01-06 16:37:56,654 - Epoch: [43][  220/  246]    Overall Loss 0.304135    Objective Loss 0.304135                                        LR 0.000036    Time 0.015973    
2023-01-06 16:37:56,789 - Epoch: [43][  230/  246]    Overall Loss 0.305082    Objective Loss 0.305082                                        LR 0.000036    Time 0.015864    
2023-01-06 16:37:56,940 - Epoch: [43][  240/  246]    Overall Loss 0.303765    Objective Loss 0.303765                                        LR 0.000036    Time 0.015831    
2023-01-06 16:37:57,005 - Epoch: [43][  246/  246]    Overall Loss 0.303359    Objective Loss 0.303359    Top1 87.559809    LR 0.000036    Time 0.015707    
2023-01-06 16:37:57,133 - --- validate (epoch=43)-----------
2023-01-06 16:37:57,133 - 6986 samples (256 per mini-batch)
2023-01-06 16:37:57,551 - Epoch: [43][   10/   28]    Loss 0.310112    Top1 88.437500    
2023-01-06 16:37:57,644 - Epoch: [43][   20/   28]    Loss 0.296520    Top1 89.414062    
2023-01-06 16:37:57,694 - Epoch: [43][   28/   28]    Loss 0.303414    Top1 89.292871    
2023-01-06 16:37:57,833 - ==> Top1: 89.293    Loss: 0.303

2023-01-06 16:37:57,833 - ==> Confusion:
[[ 201   13  225]
 [  12  200  390]
 [  57   51 5837]]

2023-01-06 16:37:57,834 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:37:57,834 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:37:57,840 - 

2023-01-06 16:37:57,840 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:37:58,357 - Epoch: [44][   10/  246]    Overall Loss 0.307854    Objective Loss 0.307854                                        LR 0.000036    Time 0.051636    
2023-01-06 16:37:58,489 - Epoch: [44][   20/  246]    Overall Loss 0.307712    Objective Loss 0.307712                                        LR 0.000036    Time 0.032392    
2023-01-06 16:37:58,619 - Epoch: [44][   30/  246]    Overall Loss 0.309327    Objective Loss 0.309327                                        LR 0.000036    Time 0.025906    
2023-01-06 16:37:58,745 - Epoch: [44][   40/  246]    Overall Loss 0.310341    Objective Loss 0.310341                                        LR 0.000036    Time 0.022557    
2023-01-06 16:37:58,875 - Epoch: [44][   50/  246]    Overall Loss 0.310932    Objective Loss 0.310932                                        LR 0.000036    Time 0.020640    
2023-01-06 16:37:58,997 - Epoch: [44][   60/  246]    Overall Loss 0.308459    Objective Loss 0.308459                                        LR 0.000036    Time 0.019232    
2023-01-06 16:37:59,116 - Epoch: [44][   70/  246]    Overall Loss 0.307229    Objective Loss 0.307229                                        LR 0.000036    Time 0.018179    
2023-01-06 16:37:59,237 - Epoch: [44][   80/  246]    Overall Loss 0.306694    Objective Loss 0.306694                                        LR 0.000036    Time 0.017414    
2023-01-06 16:37:59,364 - Epoch: [44][   90/  246]    Overall Loss 0.305888    Objective Loss 0.305888                                        LR 0.000036    Time 0.016889    
2023-01-06 16:37:59,500 - Epoch: [44][  100/  246]    Overall Loss 0.304923    Objective Loss 0.304923                                        LR 0.000036    Time 0.016552    
2023-01-06 16:37:59,640 - Epoch: [44][  110/  246]    Overall Loss 0.303794    Objective Loss 0.303794                                        LR 0.000036    Time 0.016311    
2023-01-06 16:37:59,779 - Epoch: [44][  120/  246]    Overall Loss 0.305384    Objective Loss 0.305384                                        LR 0.000036    Time 0.016109    
2023-01-06 16:37:59,921 - Epoch: [44][  130/  246]    Overall Loss 0.307055    Objective Loss 0.307055                                        LR 0.000036    Time 0.015960    
2023-01-06 16:38:00,062 - Epoch: [44][  140/  246]    Overall Loss 0.305703    Objective Loss 0.305703                                        LR 0.000036    Time 0.015827    
2023-01-06 16:38:00,204 - Epoch: [44][  150/  246]    Overall Loss 0.304510    Objective Loss 0.304510                                        LR 0.000036    Time 0.015711    
2023-01-06 16:38:00,345 - Epoch: [44][  160/  246]    Overall Loss 0.305653    Objective Loss 0.305653                                        LR 0.000036    Time 0.015609    
2023-01-06 16:38:00,484 - Epoch: [44][  170/  246]    Overall Loss 0.305596    Objective Loss 0.305596                                        LR 0.000036    Time 0.015503    
2023-01-06 16:38:00,625 - Epoch: [44][  180/  246]    Overall Loss 0.304939    Objective Loss 0.304939                                        LR 0.000036    Time 0.015424    
2023-01-06 16:38:00,765 - Epoch: [44][  190/  246]    Overall Loss 0.305172    Objective Loss 0.305172                                        LR 0.000036    Time 0.015348    
2023-01-06 16:38:00,905 - Epoch: [44][  200/  246]    Overall Loss 0.304058    Objective Loss 0.304058                                        LR 0.000036    Time 0.015278    
2023-01-06 16:38:01,054 - Epoch: [44][  210/  246]    Overall Loss 0.303731    Objective Loss 0.303731                                        LR 0.000036    Time 0.015255    
2023-01-06 16:38:01,204 - Epoch: [44][  220/  246]    Overall Loss 0.303406    Objective Loss 0.303406                                        LR 0.000036    Time 0.015242    
2023-01-06 16:38:01,358 - Epoch: [44][  230/  246]    Overall Loss 0.302825    Objective Loss 0.302825                                        LR 0.000036    Time 0.015248    
2023-01-06 16:38:01,519 - Epoch: [44][  240/  246]    Overall Loss 0.302714    Objective Loss 0.302714                                        LR 0.000036    Time 0.015283    
2023-01-06 16:38:01,583 - Epoch: [44][  246/  246]    Overall Loss 0.302488    Objective Loss 0.302488    Top1 89.952153    LR 0.000036    Time 0.015167    
2023-01-06 16:38:01,739 - --- validate (epoch=44)-----------
2023-01-06 16:38:01,739 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:02,300 - Epoch: [44][   10/   28]    Loss 0.307746    Top1 88.632812    
2023-01-06 16:38:02,400 - Epoch: [44][   20/   28]    Loss 0.303634    Top1 88.906250    
2023-01-06 16:38:02,452 - Epoch: [44][   28/   28]    Loss 0.297900    Top1 89.135414    
2023-01-06 16:38:02,616 - ==> Top1: 89.135    Loss: 0.298

2023-01-06 16:38:02,616 - ==> Confusion:
[[ 196   12  231]
 [  13  187  402]
 [  53   48 5844]]

2023-01-06 16:38:02,617 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:38:02,617 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:02,622 - 

2023-01-06 16:38:02,622 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:03,140 - Epoch: [45][   10/  246]    Overall Loss 0.297396    Objective Loss 0.297396                                        LR 0.000036    Time 0.051762    
2023-01-06 16:38:03,271 - Epoch: [45][   20/  246]    Overall Loss 0.304147    Objective Loss 0.304147                                        LR 0.000036    Time 0.032372    
2023-01-06 16:38:03,400 - Epoch: [45][   30/  246]    Overall Loss 0.305526    Objective Loss 0.305526                                        LR 0.000036    Time 0.025882    
2023-01-06 16:38:03,523 - Epoch: [45][   40/  246]    Overall Loss 0.303486    Objective Loss 0.303486                                        LR 0.000036    Time 0.022486    
2023-01-06 16:38:03,665 - Epoch: [45][   50/  246]    Overall Loss 0.300451    Objective Loss 0.300451                                        LR 0.000036    Time 0.020811    
2023-01-06 16:38:03,811 - Epoch: [45][   60/  246]    Overall Loss 0.298265    Objective Loss 0.298265                                        LR 0.000036    Time 0.019739    
2023-01-06 16:38:03,954 - Epoch: [45][   70/  246]    Overall Loss 0.300201    Objective Loss 0.300201                                        LR 0.000036    Time 0.018963    
2023-01-06 16:38:04,076 - Epoch: [45][   80/  246]    Overall Loss 0.303940    Objective Loss 0.303940                                        LR 0.000036    Time 0.018110    
2023-01-06 16:38:04,214 - Epoch: [45][   90/  246]    Overall Loss 0.303550    Objective Loss 0.303550                                        LR 0.000036    Time 0.017626    
2023-01-06 16:38:04,356 - Epoch: [45][  100/  246]    Overall Loss 0.301849    Objective Loss 0.301849                                        LR 0.000036    Time 0.017284    
2023-01-06 16:38:04,499 - Epoch: [45][  110/  246]    Overall Loss 0.301770    Objective Loss 0.301770                                        LR 0.000036    Time 0.017012    
2023-01-06 16:38:04,643 - Epoch: [45][  120/  246]    Overall Loss 0.301113    Objective Loss 0.301113                                        LR 0.000036    Time 0.016786    
2023-01-06 16:38:04,789 - Epoch: [45][  130/  246]    Overall Loss 0.301829    Objective Loss 0.301829                                        LR 0.000036    Time 0.016614    
2023-01-06 16:38:04,928 - Epoch: [45][  140/  246]    Overall Loss 0.301781    Objective Loss 0.301781                                        LR 0.000036    Time 0.016421    
2023-01-06 16:38:05,065 - Epoch: [45][  150/  246]    Overall Loss 0.301941    Objective Loss 0.301941                                        LR 0.000036    Time 0.016239    
2023-01-06 16:38:05,204 - Epoch: [45][  160/  246]    Overall Loss 0.300465    Objective Loss 0.300465                                        LR 0.000036    Time 0.016089    
2023-01-06 16:38:05,342 - Epoch: [45][  170/  246]    Overall Loss 0.299678    Objective Loss 0.299678                                        LR 0.000036    Time 0.015954    
2023-01-06 16:38:05,488 - Epoch: [45][  180/  246]    Overall Loss 0.299867    Objective Loss 0.299867                                        LR 0.000036    Time 0.015877    
2023-01-06 16:38:05,634 - Epoch: [45][  190/  246]    Overall Loss 0.299998    Objective Loss 0.299998                                        LR 0.000036    Time 0.015809    
2023-01-06 16:38:05,782 - Epoch: [45][  200/  246]    Overall Loss 0.300670    Objective Loss 0.300670                                        LR 0.000036    Time 0.015753    
2023-01-06 16:38:05,929 - Epoch: [45][  210/  246]    Overall Loss 0.300877    Objective Loss 0.300877                                        LR 0.000036    Time 0.015702    
2023-01-06 16:38:06,068 - Epoch: [45][  220/  246]    Overall Loss 0.300827    Objective Loss 0.300827                                        LR 0.000036    Time 0.015619    
2023-01-06 16:38:06,208 - Epoch: [45][  230/  246]    Overall Loss 0.300644    Objective Loss 0.300644                                        LR 0.000036    Time 0.015541    
2023-01-06 16:38:06,364 - Epoch: [45][  240/  246]    Overall Loss 0.300665    Objective Loss 0.300665                                        LR 0.000036    Time 0.015543    
2023-01-06 16:38:06,428 - Epoch: [45][  246/  246]    Overall Loss 0.300967    Objective Loss 0.300967    Top1 88.038278    LR 0.000036    Time 0.015421    
2023-01-06 16:38:06,557 - --- validate (epoch=45)-----------
2023-01-06 16:38:06,557 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:06,981 - Epoch: [45][   10/   28]    Loss 0.305431    Top1 88.750000    
2023-01-06 16:38:07,079 - Epoch: [45][   20/   28]    Loss 0.308733    Top1 88.808594    
2023-01-06 16:38:07,132 - Epoch: [45][   28/   28]    Loss 0.298485    Top1 89.149728    
2023-01-06 16:38:07,305 - ==> Top1: 89.150    Loss: 0.298

2023-01-06 16:38:07,306 - ==> Confusion:
[[ 198   10  231]
 [  11  182  409]
 [  56   41 5848]]

2023-01-06 16:38:07,307 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:38:07,307 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:07,313 - 

2023-01-06 16:38:07,313 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:07,996 - Epoch: [46][   10/  246]    Overall Loss 0.301931    Objective Loss 0.301931                                        LR 0.000036    Time 0.068185    
2023-01-06 16:38:08,159 - Epoch: [46][   20/  246]    Overall Loss 0.308168    Objective Loss 0.308168                                        LR 0.000036    Time 0.042216    
2023-01-06 16:38:08,310 - Epoch: [46][   30/  246]    Overall Loss 0.300718    Objective Loss 0.300718                                        LR 0.000036    Time 0.033190    
2023-01-06 16:38:08,462 - Epoch: [46][   40/  246]    Overall Loss 0.300029    Objective Loss 0.300029                                        LR 0.000036    Time 0.028684    
2023-01-06 16:38:08,611 - Epoch: [46][   50/  246]    Overall Loss 0.304657    Objective Loss 0.304657                                        LR 0.000036    Time 0.025904    
2023-01-06 16:38:08,762 - Epoch: [46][   60/  246]    Overall Loss 0.303940    Objective Loss 0.303940                                        LR 0.000036    Time 0.024098    
2023-01-06 16:38:08,908 - Epoch: [46][   70/  246]    Overall Loss 0.305811    Objective Loss 0.305811                                        LR 0.000036    Time 0.022743    
2023-01-06 16:38:09,055 - Epoch: [46][   80/  246]    Overall Loss 0.302715    Objective Loss 0.302715                                        LR 0.000036    Time 0.021727    
2023-01-06 16:38:09,204 - Epoch: [46][   90/  246]    Overall Loss 0.303548    Objective Loss 0.303548                                        LR 0.000036    Time 0.020971    
2023-01-06 16:38:09,355 - Epoch: [46][  100/  246]    Overall Loss 0.304618    Objective Loss 0.304618                                        LR 0.000036    Time 0.020377    
2023-01-06 16:38:09,504 - Epoch: [46][  110/  246]    Overall Loss 0.302632    Objective Loss 0.302632                                        LR 0.000036    Time 0.019872    
2023-01-06 16:38:09,656 - Epoch: [46][  120/  246]    Overall Loss 0.303960    Objective Loss 0.303960                                        LR 0.000036    Time 0.019479    
2023-01-06 16:38:09,806 - Epoch: [46][  130/  246]    Overall Loss 0.302886    Objective Loss 0.302886                                        LR 0.000036    Time 0.019135    
2023-01-06 16:38:09,958 - Epoch: [46][  140/  246]    Overall Loss 0.302721    Objective Loss 0.302721                                        LR 0.000036    Time 0.018850    
2023-01-06 16:38:10,110 - Epoch: [46][  150/  246]    Overall Loss 0.304631    Objective Loss 0.304631                                        LR 0.000036    Time 0.018603    
2023-01-06 16:38:10,263 - Epoch: [46][  160/  246]    Overall Loss 0.305102    Objective Loss 0.305102                                        LR 0.000036    Time 0.018395    
2023-01-06 16:38:10,412 - Epoch: [46][  170/  246]    Overall Loss 0.303527    Objective Loss 0.303527                                        LR 0.000036    Time 0.018185    
2023-01-06 16:38:10,566 - Epoch: [46][  180/  246]    Overall Loss 0.303724    Objective Loss 0.303724                                        LR 0.000036    Time 0.018028    
2023-01-06 16:38:10,712 - Epoch: [46][  190/  246]    Overall Loss 0.303322    Objective Loss 0.303322                                        LR 0.000036    Time 0.017843    
2023-01-06 16:38:10,861 - Epoch: [46][  200/  246]    Overall Loss 0.301923    Objective Loss 0.301923                                        LR 0.000036    Time 0.017697    
2023-01-06 16:38:11,012 - Epoch: [46][  210/  246]    Overall Loss 0.300557    Objective Loss 0.300557                                        LR 0.000036    Time 0.017571    
2023-01-06 16:38:11,170 - Epoch: [46][  220/  246]    Overall Loss 0.301021    Objective Loss 0.301021                                        LR 0.000036    Time 0.017490    
2023-01-06 16:38:11,347 - Epoch: [46][  230/  246]    Overall Loss 0.300561    Objective Loss 0.300561                                        LR 0.000036    Time 0.017496    
2023-01-06 16:38:11,540 - Epoch: [46][  240/  246]    Overall Loss 0.300055    Objective Loss 0.300055                                        LR 0.000036    Time 0.017568    
2023-01-06 16:38:11,617 - Epoch: [46][  246/  246]    Overall Loss 0.300220    Objective Loss 0.300220    Top1 85.885167    LR 0.000036    Time 0.017452    
2023-01-06 16:38:11,748 - --- validate (epoch=46)-----------
2023-01-06 16:38:11,749 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:12,178 - Epoch: [46][   10/   28]    Loss 0.297740    Top1 89.023438    
2023-01-06 16:38:12,283 - Epoch: [46][   20/   28]    Loss 0.307312    Top1 88.769531    
2023-01-06 16:38:12,332 - Epoch: [46][   28/   28]    Loss 0.303042    Top1 89.121099    
2023-01-06 16:38:12,475 - ==> Top1: 89.121    Loss: 0.303

2023-01-06 16:38:12,475 - ==> Confusion:
[[ 208    7  224]
 [  12  177  413]
 [  59   45 5841]]

2023-01-06 16:38:12,476 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:38:12,476 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:12,481 - 

2023-01-06 16:38:12,482 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:13,017 - Epoch: [47][   10/  246]    Overall Loss 0.302686    Objective Loss 0.302686                                        LR 0.000036    Time 0.053494    
2023-01-06 16:38:13,165 - Epoch: [47][   20/  246]    Overall Loss 0.297062    Objective Loss 0.297062                                        LR 0.000036    Time 0.034140    
2023-01-06 16:38:13,306 - Epoch: [47][   30/  246]    Overall Loss 0.299078    Objective Loss 0.299078                                        LR 0.000036    Time 0.027434    
2023-01-06 16:38:13,455 - Epoch: [47][   40/  246]    Overall Loss 0.300237    Objective Loss 0.300237                                        LR 0.000036    Time 0.024294    
2023-01-06 16:38:13,602 - Epoch: [47][   50/  246]    Overall Loss 0.304556    Objective Loss 0.304556                                        LR 0.000036    Time 0.022373    
2023-01-06 16:38:13,745 - Epoch: [47][   60/  246]    Overall Loss 0.301553    Objective Loss 0.301553                                        LR 0.000036    Time 0.021020    
2023-01-06 16:38:13,895 - Epoch: [47][   70/  246]    Overall Loss 0.299680    Objective Loss 0.299680                                        LR 0.000036    Time 0.020152    
2023-01-06 16:38:14,025 - Epoch: [47][   80/  246]    Overall Loss 0.300124    Objective Loss 0.300124                                        LR 0.000036    Time 0.019248    
2023-01-06 16:38:14,169 - Epoch: [47][   90/  246]    Overall Loss 0.299679    Objective Loss 0.299679                                        LR 0.000036    Time 0.018711    
2023-01-06 16:38:14,303 - Epoch: [47][  100/  246]    Overall Loss 0.299126    Objective Loss 0.299126                                        LR 0.000036    Time 0.018174    
2023-01-06 16:38:14,444 - Epoch: [47][  110/  246]    Overall Loss 0.298824    Objective Loss 0.298824                                        LR 0.000036    Time 0.017795    
2023-01-06 16:38:14,582 - Epoch: [47][  120/  246]    Overall Loss 0.299403    Objective Loss 0.299403                                        LR 0.000036    Time 0.017465    
2023-01-06 16:38:14,720 - Epoch: [47][  130/  246]    Overall Loss 0.299964    Objective Loss 0.299964                                        LR 0.000036    Time 0.017175    
2023-01-06 16:38:14,860 - Epoch: [47][  140/  246]    Overall Loss 0.300083    Objective Loss 0.300083                                        LR 0.000036    Time 0.016929    
2023-01-06 16:38:15,003 - Epoch: [47][  150/  246]    Overall Loss 0.300550    Objective Loss 0.300550                                        LR 0.000036    Time 0.016740    
2023-01-06 16:38:15,166 - Epoch: [47][  160/  246]    Overall Loss 0.300505    Objective Loss 0.300505                                        LR 0.000036    Time 0.016707    
2023-01-06 16:38:15,331 - Epoch: [47][  170/  246]    Overall Loss 0.300170    Objective Loss 0.300170                                        LR 0.000036    Time 0.016697    
2023-01-06 16:38:15,515 - Epoch: [47][  180/  246]    Overall Loss 0.299557    Objective Loss 0.299557                                        LR 0.000036    Time 0.016786    
2023-01-06 16:38:15,701 - Epoch: [47][  190/  246]    Overall Loss 0.299653    Objective Loss 0.299653                                        LR 0.000036    Time 0.016882    
2023-01-06 16:38:15,888 - Epoch: [47][  200/  246]    Overall Loss 0.299518    Objective Loss 0.299518                                        LR 0.000036    Time 0.016969    
2023-01-06 16:38:16,074 - Epoch: [47][  210/  246]    Overall Loss 0.300611    Objective Loss 0.300611                                        LR 0.000036    Time 0.017043    
2023-01-06 16:38:16,225 - Epoch: [47][  220/  246]    Overall Loss 0.300348    Objective Loss 0.300348                                        LR 0.000036    Time 0.016954    
2023-01-06 16:38:16,357 - Epoch: [47][  230/  246]    Overall Loss 0.300139    Objective Loss 0.300139                                        LR 0.000036    Time 0.016783    
2023-01-06 16:38:16,505 - Epoch: [47][  240/  246]    Overall Loss 0.299266    Objective Loss 0.299266                                        LR 0.000036    Time 0.016698    
2023-01-06 16:38:16,569 - Epoch: [47][  246/  246]    Overall Loss 0.298989    Objective Loss 0.298989    Top1 88.995215    LR 0.000036    Time 0.016552    
2023-01-06 16:38:16,718 - --- validate (epoch=47)-----------
2023-01-06 16:38:16,718 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:17,137 - Epoch: [47][   10/   28]    Loss 0.321786    Top1 88.007812    
2023-01-06 16:38:17,231 - Epoch: [47][   20/   28]    Loss 0.303164    Top1 88.945312    
2023-01-06 16:38:17,279 - Epoch: [47][   28/   28]    Loss 0.297498    Top1 89.278557    
2023-01-06 16:38:17,443 - ==> Top1: 89.279    Loss: 0.297

2023-01-06 16:38:17,444 - ==> Confusion:
[[ 204   11  224]
 [  13  193  396]
 [  57   48 5840]]

2023-01-06 16:38:17,445 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:38:17,445 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:17,450 - 

2023-01-06 16:38:17,450 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:18,107 - Epoch: [48][   10/  246]    Overall Loss 0.298063    Objective Loss 0.298063                                        LR 0.000036    Time 0.065655    
2023-01-06 16:38:18,246 - Epoch: [48][   20/  246]    Overall Loss 0.297431    Objective Loss 0.297431                                        LR 0.000036    Time 0.039777    
2023-01-06 16:38:18,381 - Epoch: [48][   30/  246]    Overall Loss 0.293168    Objective Loss 0.293168                                        LR 0.000036    Time 0.030976    
2023-01-06 16:38:18,499 - Epoch: [48][   40/  246]    Overall Loss 0.294389    Objective Loss 0.294389                                        LR 0.000036    Time 0.026189    
2023-01-06 16:38:18,620 - Epoch: [48][   50/  246]    Overall Loss 0.298138    Objective Loss 0.298138                                        LR 0.000036    Time 0.023357    
2023-01-06 16:38:18,735 - Epoch: [48][   60/  246]    Overall Loss 0.298135    Objective Loss 0.298135                                        LR 0.000036    Time 0.021380    
2023-01-06 16:38:18,852 - Epoch: [48][   70/  246]    Overall Loss 0.299470    Objective Loss 0.299470                                        LR 0.000036    Time 0.019997    
2023-01-06 16:38:18,966 - Epoch: [48][   80/  246]    Overall Loss 0.300630    Objective Loss 0.300630                                        LR 0.000036    Time 0.018916    
2023-01-06 16:38:19,084 - Epoch: [48][   90/  246]    Overall Loss 0.300377    Objective Loss 0.300377                                        LR 0.000036    Time 0.018122    
2023-01-06 16:38:19,202 - Epoch: [48][  100/  246]    Overall Loss 0.298373    Objective Loss 0.298373                                        LR 0.000036    Time 0.017479    
2023-01-06 16:38:19,320 - Epoch: [48][  110/  246]    Overall Loss 0.297513    Objective Loss 0.297513                                        LR 0.000036    Time 0.016966    
2023-01-06 16:38:19,455 - Epoch: [48][  120/  246]    Overall Loss 0.297600    Objective Loss 0.297600                                        LR 0.000036    Time 0.016670    
2023-01-06 16:38:19,588 - Epoch: [48][  130/  246]    Overall Loss 0.299084    Objective Loss 0.299084                                        LR 0.000036    Time 0.016412    
2023-01-06 16:38:19,720 - Epoch: [48][  140/  246]    Overall Loss 0.298597    Objective Loss 0.298597                                        LR 0.000036    Time 0.016176    
2023-01-06 16:38:19,852 - Epoch: [48][  150/  246]    Overall Loss 0.299406    Objective Loss 0.299406                                        LR 0.000036    Time 0.015977    
2023-01-06 16:38:19,984 - Epoch: [48][  160/  246]    Overall Loss 0.298991    Objective Loss 0.298991                                        LR 0.000036    Time 0.015800    
2023-01-06 16:38:20,117 - Epoch: [48][  170/  246]    Overall Loss 0.300351    Objective Loss 0.300351                                        LR 0.000036    Time 0.015653    
2023-01-06 16:38:20,250 - Epoch: [48][  180/  246]    Overall Loss 0.299870    Objective Loss 0.299870                                        LR 0.000036    Time 0.015520    
2023-01-06 16:38:20,382 - Epoch: [48][  190/  246]    Overall Loss 0.299280    Objective Loss 0.299280                                        LR 0.000036    Time 0.015394    
2023-01-06 16:38:20,516 - Epoch: [48][  200/  246]    Overall Loss 0.299198    Objective Loss 0.299198                                        LR 0.000036    Time 0.015291    
2023-01-06 16:38:20,646 - Epoch: [48][  210/  246]    Overall Loss 0.299566    Objective Loss 0.299566                                        LR 0.000036    Time 0.015185    
2023-01-06 16:38:20,777 - Epoch: [48][  220/  246]    Overall Loss 0.298917    Objective Loss 0.298917                                        LR 0.000036    Time 0.015086    
2023-01-06 16:38:20,906 - Epoch: [48][  230/  246]    Overall Loss 0.300418    Objective Loss 0.300418                                        LR 0.000036    Time 0.014988    
2023-01-06 16:38:21,056 - Epoch: [48][  240/  246]    Overall Loss 0.299353    Objective Loss 0.299353                                        LR 0.000036    Time 0.014990    
2023-01-06 16:38:21,119 - Epoch: [48][  246/  246]    Overall Loss 0.298941    Objective Loss 0.298941    Top1 90.669856    LR 0.000036    Time 0.014880    
2023-01-06 16:38:21,252 - --- validate (epoch=48)-----------
2023-01-06 16:38:21,253 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:21,690 - Epoch: [48][   10/   28]    Loss 0.313346    Top1 88.554688    
2023-01-06 16:38:21,788 - Epoch: [48][   20/   28]    Loss 0.303120    Top1 88.945312    
2023-01-06 16:38:21,839 - Epoch: [48][   28/   28]    Loss 0.299324    Top1 89.206985    
2023-01-06 16:38:21,973 - ==> Top1: 89.207    Loss: 0.299

2023-01-06 16:38:21,973 - ==> Confusion:
[[ 190   10  239]
 [  10  203  389]
 [  47   59 5839]]

2023-01-06 16:38:21,974 - ==> Best [Top1: 89.293   Sparsity:0.00   Params: 151104 on epoch: 43]
2023-01-06 16:38:21,974 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:21,979 - 

2023-01-06 16:38:21,979 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:22,636 - Epoch: [49][   10/  246]    Overall Loss 0.287308    Objective Loss 0.287308                                        LR 0.000036    Time 0.065569    
2023-01-06 16:38:22,772 - Epoch: [49][   20/  246]    Overall Loss 0.289343    Objective Loss 0.289343                                        LR 0.000036    Time 0.039560    
2023-01-06 16:38:22,908 - Epoch: [49][   30/  246]    Overall Loss 0.286470    Objective Loss 0.286470                                        LR 0.000036    Time 0.030918    
2023-01-06 16:38:23,043 - Epoch: [49][   40/  246]    Overall Loss 0.288007    Objective Loss 0.288007                                        LR 0.000036    Time 0.026555    
2023-01-06 16:38:23,178 - Epoch: [49][   50/  246]    Overall Loss 0.289852    Objective Loss 0.289852                                        LR 0.000036    Time 0.023921    
2023-01-06 16:38:23,314 - Epoch: [49][   60/  246]    Overall Loss 0.288984    Objective Loss 0.288984                                        LR 0.000036    Time 0.022191    
2023-01-06 16:38:23,446 - Epoch: [49][   70/  246]    Overall Loss 0.292754    Objective Loss 0.292754                                        LR 0.000036    Time 0.020906    
2023-01-06 16:38:23,579 - Epoch: [49][   80/  246]    Overall Loss 0.293187    Objective Loss 0.293187                                        LR 0.000036    Time 0.019946    
2023-01-06 16:38:23,713 - Epoch: [49][   90/  246]    Overall Loss 0.292838    Objective Loss 0.292838                                        LR 0.000036    Time 0.019220    
2023-01-06 16:38:23,849 - Epoch: [49][  100/  246]    Overall Loss 0.291555    Objective Loss 0.291555                                        LR 0.000036    Time 0.018649    
2023-01-06 16:38:23,984 - Epoch: [49][  110/  246]    Overall Loss 0.292618    Objective Loss 0.292618                                        LR 0.000036    Time 0.018175    
2023-01-06 16:38:24,121 - Epoch: [49][  120/  246]    Overall Loss 0.294185    Objective Loss 0.294185                                        LR 0.000036    Time 0.017805    
2023-01-06 16:38:24,257 - Epoch: [49][  130/  246]    Overall Loss 0.293868    Objective Loss 0.293868                                        LR 0.000036    Time 0.017474    
2023-01-06 16:38:24,395 - Epoch: [49][  140/  246]    Overall Loss 0.294366    Objective Loss 0.294366                                        LR 0.000036    Time 0.017211    
2023-01-06 16:38:24,534 - Epoch: [49][  150/  246]    Overall Loss 0.295485    Objective Loss 0.295485                                        LR 0.000036    Time 0.016985    
2023-01-06 16:38:24,669 - Epoch: [49][  160/  246]    Overall Loss 0.296216    Objective Loss 0.296216                                        LR 0.000036    Time 0.016765    
2023-01-06 16:38:24,811 - Epoch: [49][  170/  246]    Overall Loss 0.296654    Objective Loss 0.296654                                        LR 0.000036    Time 0.016616    
2023-01-06 16:38:24,957 - Epoch: [49][  180/  246]    Overall Loss 0.295855    Objective Loss 0.295855                                        LR 0.000036    Time 0.016499    
2023-01-06 16:38:25,110 - Epoch: [49][  190/  246]    Overall Loss 0.295474    Objective Loss 0.295474                                        LR 0.000036    Time 0.016435    
2023-01-06 16:38:25,254 - Epoch: [49][  200/  246]    Overall Loss 0.295807    Objective Loss 0.295807                                        LR 0.000036    Time 0.016330    
2023-01-06 16:38:25,398 - Epoch: [49][  210/  246]    Overall Loss 0.296208    Objective Loss 0.296208                                        LR 0.000036    Time 0.016237    
2023-01-06 16:38:25,544 - Epoch: [49][  220/  246]    Overall Loss 0.296489    Objective Loss 0.296489                                        LR 0.000036    Time 0.016161    
2023-01-06 16:38:25,690 - Epoch: [49][  230/  246]    Overall Loss 0.295779    Objective Loss 0.295779                                        LR 0.000036    Time 0.016088    
2023-01-06 16:38:25,844 - Epoch: [49][  240/  246]    Overall Loss 0.296765    Objective Loss 0.296765                                        LR 0.000036    Time 0.016060    
2023-01-06 16:38:25,907 - Epoch: [49][  246/  246]    Overall Loss 0.296584    Objective Loss 0.296584    Top1 91.626794    LR 0.000036    Time 0.015921    
2023-01-06 16:38:26,050 - --- validate (epoch=49)-----------
2023-01-06 16:38:26,050 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:26,484 - Epoch: [49][   10/   28]    Loss 0.292588    Top1 89.687500    
2023-01-06 16:38:26,592 - Epoch: [49][   20/   28]    Loss 0.296171    Top1 89.824219    
2023-01-06 16:38:26,645 - Epoch: [49][   28/   28]    Loss 0.294550    Top1 89.536215    
2023-01-06 16:38:26,775 - ==> Top1: 89.536    Loss: 0.295

2023-01-06 16:38:26,775 - ==> Confusion:
[[ 192   11  236]
 [  10  202  390]
 [  38   46 5861]]

2023-01-06 16:38:26,776 - ==> Best [Top1: 89.536   Sparsity:0.00   Params: 151104 on epoch: 49]
2023-01-06 16:38:26,776 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:26,783 - 

2023-01-06 16:38:26,783 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:27,306 - Epoch: [50][   10/  246]    Overall Loss 0.270978    Objective Loss 0.270978                                        LR 0.000036    Time 0.052250    
2023-01-06 16:38:27,439 - Epoch: [50][   20/  246]    Overall Loss 0.283011    Objective Loss 0.283011                                        LR 0.000036    Time 0.032776    
2023-01-06 16:38:27,568 - Epoch: [50][   30/  246]    Overall Loss 0.285029    Objective Loss 0.285029                                        LR 0.000036    Time 0.026129    
2023-01-06 16:38:27,700 - Epoch: [50][   40/  246]    Overall Loss 0.289882    Objective Loss 0.289882                                        LR 0.000036    Time 0.022856    
2023-01-06 16:38:27,840 - Epoch: [50][   50/  246]    Overall Loss 0.289304    Objective Loss 0.289304                                        LR 0.000036    Time 0.021066    
2023-01-06 16:38:27,990 - Epoch: [50][   60/  246]    Overall Loss 0.289896    Objective Loss 0.289896                                        LR 0.000036    Time 0.020058    
2023-01-06 16:38:28,144 - Epoch: [50][   70/  246]    Overall Loss 0.289587    Objective Loss 0.289587                                        LR 0.000036    Time 0.019389    
2023-01-06 16:38:28,290 - Epoch: [50][   80/  246]    Overall Loss 0.291176    Objective Loss 0.291176                                        LR 0.000036    Time 0.018774    
2023-01-06 16:38:28,441 - Epoch: [50][   90/  246]    Overall Loss 0.289281    Objective Loss 0.289281                                        LR 0.000036    Time 0.018369    
2023-01-06 16:38:28,588 - Epoch: [50][  100/  246]    Overall Loss 0.290629    Objective Loss 0.290629                                        LR 0.000036    Time 0.017995    
2023-01-06 16:38:28,722 - Epoch: [50][  110/  246]    Overall Loss 0.291804    Objective Loss 0.291804                                        LR 0.000036    Time 0.017569    
2023-01-06 16:38:28,859 - Epoch: [50][  120/  246]    Overall Loss 0.290604    Objective Loss 0.290604                                        LR 0.000036    Time 0.017250    
2023-01-06 16:38:28,996 - Epoch: [50][  130/  246]    Overall Loss 0.290135    Objective Loss 0.290135                                        LR 0.000036    Time 0.016969    
2023-01-06 16:38:29,125 - Epoch: [50][  140/  246]    Overall Loss 0.290734    Objective Loss 0.290734                                        LR 0.000036    Time 0.016678    
2023-01-06 16:38:29,262 - Epoch: [50][  150/  246]    Overall Loss 0.291470    Objective Loss 0.291470                                        LR 0.000036    Time 0.016472    
2023-01-06 16:38:29,409 - Epoch: [50][  160/  246]    Overall Loss 0.291623    Objective Loss 0.291623                                        LR 0.000036    Time 0.016360    
2023-01-06 16:38:29,545 - Epoch: [50][  170/  246]    Overall Loss 0.293172    Objective Loss 0.293172                                        LR 0.000036    Time 0.016199    
2023-01-06 16:38:29,678 - Epoch: [50][  180/  246]    Overall Loss 0.293046    Objective Loss 0.293046                                        LR 0.000036    Time 0.016021    
2023-01-06 16:38:29,808 - Epoch: [50][  190/  246]    Overall Loss 0.294185    Objective Loss 0.294185                                        LR 0.000036    Time 0.015860    
2023-01-06 16:38:29,943 - Epoch: [50][  200/  246]    Overall Loss 0.294629    Objective Loss 0.294629                                        LR 0.000036    Time 0.015741    
2023-01-06 16:38:30,076 - Epoch: [50][  210/  246]    Overall Loss 0.293733    Objective Loss 0.293733                                        LR 0.000036    Time 0.015624    
2023-01-06 16:38:30,210 - Epoch: [50][  220/  246]    Overall Loss 0.294962    Objective Loss 0.294962                                        LR 0.000036    Time 0.015520    
2023-01-06 16:38:30,343 - Epoch: [50][  230/  246]    Overall Loss 0.295932    Objective Loss 0.295932                                        LR 0.000036    Time 0.015410    
2023-01-06 16:38:30,488 - Epoch: [50][  240/  246]    Overall Loss 0.295637    Objective Loss 0.295637                                        LR 0.000036    Time 0.015374    
2023-01-06 16:38:30,547 - Epoch: [50][  246/  246]    Overall Loss 0.295357    Objective Loss 0.295357    Top1 90.909091    LR 0.000036    Time 0.015234    
2023-01-06 16:38:30,677 - --- validate (epoch=50)-----------
2023-01-06 16:38:30,678 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:31,105 - Epoch: [50][   10/   28]    Loss 0.301889    Top1 89.101562    
2023-01-06 16:38:31,220 - Epoch: [50][   20/   28]    Loss 0.293688    Top1 89.296875    
2023-01-06 16:38:31,268 - Epoch: [50][   28/   28]    Loss 0.294265    Top1 89.206985    
2023-01-06 16:38:31,428 - ==> Top1: 89.207    Loss: 0.294

2023-01-06 16:38:31,428 - ==> Confusion:
[[ 192    9  238]
 [  11  184  407]
 [  45   44 5856]]

2023-01-06 16:38:31,429 - ==> Best [Top1: 89.536   Sparsity:0.00   Params: 151104 on epoch: 49]
2023-01-06 16:38:31,429 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:31,434 - 

2023-01-06 16:38:31,434 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:32,075 - Epoch: [51][   10/  246]    Overall Loss 0.274660    Objective Loss 0.274660                                        LR 0.000036    Time 0.064024    
2023-01-06 16:38:32,195 - Epoch: [51][   20/  246]    Overall Loss 0.278537    Objective Loss 0.278537                                        LR 0.000036    Time 0.037974    
2023-01-06 16:38:32,315 - Epoch: [51][   30/  246]    Overall Loss 0.286865    Objective Loss 0.286865                                        LR 0.000036    Time 0.029305    
2023-01-06 16:38:32,447 - Epoch: [51][   40/  246]    Overall Loss 0.292003    Objective Loss 0.292003                                        LR 0.000036    Time 0.025259    
2023-01-06 16:38:32,572 - Epoch: [51][   50/  246]    Overall Loss 0.293809    Objective Loss 0.293809                                        LR 0.000036    Time 0.022704    
2023-01-06 16:38:32,690 - Epoch: [51][   60/  246]    Overall Loss 0.295107    Objective Loss 0.295107                                        LR 0.000036    Time 0.020885    
2023-01-06 16:38:32,812 - Epoch: [51][   70/  246]    Overall Loss 0.295747    Objective Loss 0.295747                                        LR 0.000036    Time 0.019635    
2023-01-06 16:38:32,931 - Epoch: [51][   80/  246]    Overall Loss 0.294631    Objective Loss 0.294631                                        LR 0.000036    Time 0.018674    
2023-01-06 16:38:33,051 - Epoch: [51][   90/  246]    Overall Loss 0.294311    Objective Loss 0.294311                                        LR 0.000036    Time 0.017923    
2023-01-06 16:38:33,172 - Epoch: [51][  100/  246]    Overall Loss 0.296568    Objective Loss 0.296568                                        LR 0.000036    Time 0.017343    
2023-01-06 16:38:33,294 - Epoch: [51][  110/  246]    Overall Loss 0.295975    Objective Loss 0.295975                                        LR 0.000036    Time 0.016866    
2023-01-06 16:38:33,442 - Epoch: [51][  120/  246]    Overall Loss 0.296905    Objective Loss 0.296905                                        LR 0.000036    Time 0.016694    
2023-01-06 16:38:33,592 - Epoch: [51][  130/  246]    Overall Loss 0.296284    Objective Loss 0.296284                                        LR 0.000036    Time 0.016557    
2023-01-06 16:38:33,714 - Epoch: [51][  140/  246]    Overall Loss 0.296660    Objective Loss 0.296660                                        LR 0.000036    Time 0.016245    
2023-01-06 16:38:33,861 - Epoch: [51][  150/  246]    Overall Loss 0.296934    Objective Loss 0.296934                                        LR 0.000036    Time 0.016135    
2023-01-06 16:38:34,011 - Epoch: [51][  160/  246]    Overall Loss 0.297076    Objective Loss 0.297076                                        LR 0.000036    Time 0.016063    
2023-01-06 16:38:34,143 - Epoch: [51][  170/  246]    Overall Loss 0.295918    Objective Loss 0.295918                                        LR 0.000036    Time 0.015895    
2023-01-06 16:38:34,262 - Epoch: [51][  180/  246]    Overall Loss 0.296274    Objective Loss 0.296274                                        LR 0.000036    Time 0.015670    
2023-01-06 16:38:34,381 - Epoch: [51][  190/  246]    Overall Loss 0.295797    Objective Loss 0.295797                                        LR 0.000036    Time 0.015468    
2023-01-06 16:38:34,504 - Epoch: [51][  200/  246]    Overall Loss 0.296099    Objective Loss 0.296099                                        LR 0.000036    Time 0.015310    
2023-01-06 16:38:34,636 - Epoch: [51][  210/  246]    Overall Loss 0.295937    Objective Loss 0.295937                                        LR 0.000036    Time 0.015206    
2023-01-06 16:38:34,762 - Epoch: [51][  220/  246]    Overall Loss 0.294924    Objective Loss 0.294924                                        LR 0.000036    Time 0.015084    
2023-01-06 16:38:34,897 - Epoch: [51][  230/  246]    Overall Loss 0.295027    Objective Loss 0.295027                                        LR 0.000036    Time 0.015012    
2023-01-06 16:38:35,052 - Epoch: [51][  240/  246]    Overall Loss 0.294570    Objective Loss 0.294570                                        LR 0.000036    Time 0.015031    
2023-01-06 16:38:35,116 - Epoch: [51][  246/  246]    Overall Loss 0.294207    Objective Loss 0.294207    Top1 90.430622    LR 0.000036    Time 0.014924    
2023-01-06 16:38:35,247 - --- validate (epoch=51)-----------
2023-01-06 16:38:35,247 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:35,668 - Epoch: [51][   10/   28]    Loss 0.300645    Top1 89.414062    
2023-01-06 16:38:35,763 - Epoch: [51][   20/   28]    Loss 0.301346    Top1 89.550781    
2023-01-06 16:38:35,814 - Epoch: [51][   28/   28]    Loss 0.307868    Top1 89.335814    
2023-01-06 16:38:35,953 - ==> Top1: 89.336    Loss: 0.308

2023-01-06 16:38:35,953 - ==> Confusion:
[[ 229   13  197]
 [  15  229  358]
 [  81   81 5783]]

2023-01-06 16:38:35,954 - ==> Best [Top1: 89.536   Sparsity:0.00   Params: 151104 on epoch: 49]
2023-01-06 16:38:35,954 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:35,959 - 

2023-01-06 16:38:35,959 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:36,473 - Epoch: [52][   10/  246]    Overall Loss 0.293134    Objective Loss 0.293134                                        LR 0.000036    Time 0.051350    
2023-01-06 16:38:36,615 - Epoch: [52][   20/  246]    Overall Loss 0.301869    Objective Loss 0.301869                                        LR 0.000036    Time 0.032726    
2023-01-06 16:38:36,753 - Epoch: [52][   30/  246]    Overall Loss 0.295370    Objective Loss 0.295370                                        LR 0.000036    Time 0.026399    
2023-01-06 16:38:36,895 - Epoch: [52][   40/  246]    Overall Loss 0.295544    Objective Loss 0.295544                                        LR 0.000036    Time 0.023337    
2023-01-06 16:38:37,033 - Epoch: [52][   50/  246]    Overall Loss 0.292912    Objective Loss 0.292912                                        LR 0.000036    Time 0.021436    
2023-01-06 16:38:37,174 - Epoch: [52][   60/  246]    Overall Loss 0.293037    Objective Loss 0.293037                                        LR 0.000036    Time 0.020197    
2023-01-06 16:38:37,303 - Epoch: [52][   70/  246]    Overall Loss 0.293518    Objective Loss 0.293518                                        LR 0.000036    Time 0.019148    
2023-01-06 16:38:37,452 - Epoch: [52][   80/  246]    Overall Loss 0.293869    Objective Loss 0.293869                                        LR 0.000036    Time 0.018614    
2023-01-06 16:38:37,595 - Epoch: [52][   90/  246]    Overall Loss 0.292027    Objective Loss 0.292027                                        LR 0.000036    Time 0.018136    
2023-01-06 16:38:37,739 - Epoch: [52][  100/  246]    Overall Loss 0.293197    Objective Loss 0.293197                                        LR 0.000036    Time 0.017754    
2023-01-06 16:38:37,882 - Epoch: [52][  110/  246]    Overall Loss 0.294557    Objective Loss 0.294557                                        LR 0.000036    Time 0.017440    
2023-01-06 16:38:38,024 - Epoch: [52][  120/  246]    Overall Loss 0.294130    Objective Loss 0.294130                                        LR 0.000036    Time 0.017164    
2023-01-06 16:38:38,165 - Epoch: [52][  130/  246]    Overall Loss 0.294276    Objective Loss 0.294276                                        LR 0.000036    Time 0.016927    
2023-01-06 16:38:38,308 - Epoch: [52][  140/  246]    Overall Loss 0.294644    Objective Loss 0.294644                                        LR 0.000036    Time 0.016731    
2023-01-06 16:38:38,450 - Epoch: [52][  150/  246]    Overall Loss 0.294330    Objective Loss 0.294330                                        LR 0.000036    Time 0.016565    
2023-01-06 16:38:38,593 - Epoch: [52][  160/  246]    Overall Loss 0.295781    Objective Loss 0.295781                                        LR 0.000036    Time 0.016420    
2023-01-06 16:38:38,736 - Epoch: [52][  170/  246]    Overall Loss 0.294893    Objective Loss 0.294893                                        LR 0.000036    Time 0.016290    
2023-01-06 16:38:38,882 - Epoch: [52][  180/  246]    Overall Loss 0.295569    Objective Loss 0.295569                                        LR 0.000036    Time 0.016193    
2023-01-06 16:38:39,032 - Epoch: [52][  190/  246]    Overall Loss 0.296820    Objective Loss 0.296820                                        LR 0.000036    Time 0.016132    
2023-01-06 16:38:39,186 - Epoch: [52][  200/  246]    Overall Loss 0.296432    Objective Loss 0.296432                                        LR 0.000036    Time 0.016090    
2023-01-06 16:38:39,339 - Epoch: [52][  210/  246]    Overall Loss 0.295475    Objective Loss 0.295475                                        LR 0.000036    Time 0.016053    
2023-01-06 16:38:39,490 - Epoch: [52][  220/  246]    Overall Loss 0.295564    Objective Loss 0.295564                                        LR 0.000036    Time 0.016004    
2023-01-06 16:38:39,641 - Epoch: [52][  230/  246]    Overall Loss 0.295003    Objective Loss 0.295003                                        LR 0.000036    Time 0.015963    
2023-01-06 16:38:39,807 - Epoch: [52][  240/  246]    Overall Loss 0.294274    Objective Loss 0.294274                                        LR 0.000036    Time 0.015988    
2023-01-06 16:38:39,873 - Epoch: [52][  246/  246]    Overall Loss 0.293932    Objective Loss 0.293932    Top1 89.473684    LR 0.000036    Time 0.015866    
2023-01-06 16:38:40,005 - --- validate (epoch=52)-----------
2023-01-06 16:38:40,006 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:40,434 - Epoch: [52][   10/   28]    Loss 0.297101    Top1 89.921875    
2023-01-06 16:38:40,542 - Epoch: [52][   20/   28]    Loss 0.292286    Top1 89.785156    
2023-01-06 16:38:40,593 - Epoch: [52][   28/   28]    Loss 0.291991    Top1 89.622101    
2023-01-06 16:38:40,719 - ==> Top1: 89.622    Loss: 0.292

2023-01-06 16:38:40,719 - ==> Confusion:
[[ 195   12  232]
 [  11  224  367]
 [  47   56 5842]]

2023-01-06 16:38:40,720 - ==> Best [Top1: 89.622   Sparsity:0.00   Params: 151104 on epoch: 52]
2023-01-06 16:38:40,721 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:40,727 - 

2023-01-06 16:38:40,727 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:41,374 - Epoch: [53][   10/  246]    Overall Loss 0.292305    Objective Loss 0.292305                                        LR 0.000036    Time 0.064560    
2023-01-06 16:38:41,504 - Epoch: [53][   20/  246]    Overall Loss 0.307701    Objective Loss 0.307701                                        LR 0.000036    Time 0.038781    
2023-01-06 16:38:41,640 - Epoch: [53][   30/  246]    Overall Loss 0.299101    Objective Loss 0.299101                                        LR 0.000036    Time 0.030381    
2023-01-06 16:38:41,779 - Epoch: [53][   40/  246]    Overall Loss 0.304959    Objective Loss 0.304959                                        LR 0.000036    Time 0.026231    
2023-01-06 16:38:41,910 - Epoch: [53][   50/  246]    Overall Loss 0.303578    Objective Loss 0.303578                                        LR 0.000036    Time 0.023605    
2023-01-06 16:38:42,044 - Epoch: [53][   60/  246]    Overall Loss 0.300719    Objective Loss 0.300719                                        LR 0.000036    Time 0.021904    
2023-01-06 16:38:42,175 - Epoch: [53][   70/  246]    Overall Loss 0.301218    Objective Loss 0.301218                                        LR 0.000036    Time 0.020628    
2023-01-06 16:38:42,308 - Epoch: [53][   80/  246]    Overall Loss 0.300303    Objective Loss 0.300303                                        LR 0.000036    Time 0.019714    
2023-01-06 16:38:42,447 - Epoch: [53][   90/  246]    Overall Loss 0.298578    Objective Loss 0.298578                                        LR 0.000036    Time 0.019061    
2023-01-06 16:38:42,585 - Epoch: [53][  100/  246]    Overall Loss 0.296225    Objective Loss 0.296225                                        LR 0.000036    Time 0.018533    
2023-01-06 16:38:42,725 - Epoch: [53][  110/  246]    Overall Loss 0.295842    Objective Loss 0.295842                                        LR 0.000036    Time 0.018112    
2023-01-06 16:38:42,865 - Epoch: [53][  120/  246]    Overall Loss 0.294189    Objective Loss 0.294189                                        LR 0.000036    Time 0.017767    
2023-01-06 16:38:43,003 - Epoch: [53][  130/  246]    Overall Loss 0.292327    Objective Loss 0.292327                                        LR 0.000036    Time 0.017458    
2023-01-06 16:38:43,143 - Epoch: [53][  140/  246]    Overall Loss 0.291696    Objective Loss 0.291696                                        LR 0.000036    Time 0.017213    
2023-01-06 16:38:43,285 - Epoch: [53][  150/  246]    Overall Loss 0.292807    Objective Loss 0.292807                                        LR 0.000036    Time 0.017009    
2023-01-06 16:38:43,427 - Epoch: [53][  160/  246]    Overall Loss 0.293382    Objective Loss 0.293382                                        LR 0.000036    Time 0.016829    
2023-01-06 16:38:43,568 - Epoch: [53][  170/  246]    Overall Loss 0.294464    Objective Loss 0.294464                                        LR 0.000036    Time 0.016669    
2023-01-06 16:38:43,710 - Epoch: [53][  180/  246]    Overall Loss 0.293936    Objective Loss 0.293936                                        LR 0.000036    Time 0.016528    
2023-01-06 16:38:43,849 - Epoch: [53][  190/  246]    Overall Loss 0.294031    Objective Loss 0.294031                                        LR 0.000036    Time 0.016389    
2023-01-06 16:38:43,986 - Epoch: [53][  200/  246]    Overall Loss 0.293464    Objective Loss 0.293464                                        LR 0.000036    Time 0.016250    
2023-01-06 16:38:44,123 - Epoch: [53][  210/  246]    Overall Loss 0.292775    Objective Loss 0.292775                                        LR 0.000036    Time 0.016128    
2023-01-06 16:38:44,261 - Epoch: [53][  220/  246]    Overall Loss 0.292918    Objective Loss 0.292918                                        LR 0.000036    Time 0.016022    
2023-01-06 16:38:44,399 - Epoch: [53][  230/  246]    Overall Loss 0.292603    Objective Loss 0.292603                                        LR 0.000036    Time 0.015922    
2023-01-06 16:38:44,551 - Epoch: [53][  240/  246]    Overall Loss 0.292324    Objective Loss 0.292324                                        LR 0.000036    Time 0.015889    
2023-01-06 16:38:44,617 - Epoch: [53][  246/  246]    Overall Loss 0.292110    Objective Loss 0.292110    Top1 88.516746    LR 0.000036    Time 0.015769    
2023-01-06 16:38:44,756 - --- validate (epoch=53)-----------
2023-01-06 16:38:44,756 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:45,184 - Epoch: [53][   10/   28]    Loss 0.295046    Top1 89.726562    
2023-01-06 16:38:45,288 - Epoch: [53][   20/   28]    Loss 0.293529    Top1 89.531250    
2023-01-06 16:38:45,337 - Epoch: [53][   28/   28]    Loss 0.293125    Top1 89.178357    
2023-01-06 16:38:45,479 - ==> Top1: 89.178    Loss: 0.293

2023-01-06 16:38:45,480 - ==> Confusion:
[[ 203   17  219]
 [  12  204  386]
 [  67   55 5823]]

2023-01-06 16:38:45,481 - ==> Best [Top1: 89.622   Sparsity:0.00   Params: 151104 on epoch: 52]
2023-01-06 16:38:45,481 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:45,487 - 

2023-01-06 16:38:45,487 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:46,137 - Epoch: [54][   10/  246]    Overall Loss 0.295401    Objective Loss 0.295401                                        LR 0.000036    Time 0.064850    
2023-01-06 16:38:46,256 - Epoch: [54][   20/  246]    Overall Loss 0.292893    Objective Loss 0.292893                                        LR 0.000036    Time 0.038362    
2023-01-06 16:38:46,384 - Epoch: [54][   30/  246]    Overall Loss 0.296479    Objective Loss 0.296479                                        LR 0.000036    Time 0.029824    
2023-01-06 16:38:46,523 - Epoch: [54][   40/  246]    Overall Loss 0.302739    Objective Loss 0.302739                                        LR 0.000036    Time 0.025830    
2023-01-06 16:38:46,657 - Epoch: [54][   50/  246]    Overall Loss 0.300166    Objective Loss 0.300166                                        LR 0.000036    Time 0.023350    
2023-01-06 16:38:46,789 - Epoch: [54][   60/  246]    Overall Loss 0.297839    Objective Loss 0.297839                                        LR 0.000036    Time 0.021642    
2023-01-06 16:38:46,915 - Epoch: [54][   70/  246]    Overall Loss 0.296127    Objective Loss 0.296127                                        LR 0.000036    Time 0.020344    
2023-01-06 16:38:47,045 - Epoch: [54][   80/  246]    Overall Loss 0.296294    Objective Loss 0.296294                                        LR 0.000036    Time 0.019418    
2023-01-06 16:38:47,180 - Epoch: [54][   90/  246]    Overall Loss 0.296277    Objective Loss 0.296277                                        LR 0.000036    Time 0.018757    
2023-01-06 16:38:47,317 - Epoch: [54][  100/  246]    Overall Loss 0.295090    Objective Loss 0.295090                                        LR 0.000036    Time 0.018237    
2023-01-06 16:38:47,452 - Epoch: [54][  110/  246]    Overall Loss 0.294385    Objective Loss 0.294385                                        LR 0.000036    Time 0.017803    
2023-01-06 16:38:47,582 - Epoch: [54][  120/  246]    Overall Loss 0.293733    Objective Loss 0.293733                                        LR 0.000036    Time 0.017403    
2023-01-06 16:38:47,709 - Epoch: [54][  130/  246]    Overall Loss 0.292427    Objective Loss 0.292427                                        LR 0.000036    Time 0.017033    
2023-01-06 16:38:47,835 - Epoch: [54][  140/  246]    Overall Loss 0.292413    Objective Loss 0.292413                                        LR 0.000036    Time 0.016716    
2023-01-06 16:38:47,967 - Epoch: [54][  150/  246]    Overall Loss 0.293265    Objective Loss 0.293265                                        LR 0.000036    Time 0.016481    
2023-01-06 16:38:48,097 - Epoch: [54][  160/  246]    Overall Loss 0.293569    Objective Loss 0.293569                                        LR 0.000036    Time 0.016257    
2023-01-06 16:38:48,227 - Epoch: [54][  170/  246]    Overall Loss 0.292718    Objective Loss 0.292718                                        LR 0.000036    Time 0.016064    
2023-01-06 16:38:48,364 - Epoch: [54][  180/  246]    Overall Loss 0.291830    Objective Loss 0.291830                                        LR 0.000036    Time 0.015929    
2023-01-06 16:38:48,503 - Epoch: [54][  190/  246]    Overall Loss 0.291688    Objective Loss 0.291688                                        LR 0.000036    Time 0.015821    
2023-01-06 16:38:48,639 - Epoch: [54][  200/  246]    Overall Loss 0.291807    Objective Loss 0.291807                                        LR 0.000036    Time 0.015708    
2023-01-06 16:38:48,777 - Epoch: [54][  210/  246]    Overall Loss 0.291464    Objective Loss 0.291464                                        LR 0.000036    Time 0.015617    
2023-01-06 16:38:48,914 - Epoch: [54][  220/  246]    Overall Loss 0.291187    Objective Loss 0.291187                                        LR 0.000036    Time 0.015527    
2023-01-06 16:38:49,049 - Epoch: [54][  230/  246]    Overall Loss 0.290985    Objective Loss 0.290985                                        LR 0.000036    Time 0.015437    
2023-01-06 16:38:49,189 - Epoch: [54][  240/  246]    Overall Loss 0.291193    Objective Loss 0.291193                                        LR 0.000036    Time 0.015374    
2023-01-06 16:38:49,249 - Epoch: [54][  246/  246]    Overall Loss 0.290722    Objective Loss 0.290722    Top1 91.866029    LR 0.000036    Time 0.015245    
2023-01-06 16:38:49,378 - --- validate (epoch=54)-----------
2023-01-06 16:38:49,378 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:49,803 - Epoch: [54][   10/   28]    Loss 0.307666    Top1 88.984375    
2023-01-06 16:38:49,897 - Epoch: [54][   20/   28]    Loss 0.299755    Top1 89.179688    
2023-01-06 16:38:49,948 - Epoch: [54][   28/   28]    Loss 0.294341    Top1 89.436015    
2023-01-06 16:38:50,098 - ==> Top1: 89.436    Loss: 0.294

2023-01-06 16:38:50,099 - ==> Confusion:
[[ 209   11  219]
 [  14  185  403]
 [  55   36 5854]]

2023-01-06 16:38:50,100 - ==> Best [Top1: 89.622   Sparsity:0.00   Params: 151104 on epoch: 52]
2023-01-06 16:38:50,100 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:50,109 - 

2023-01-06 16:38:50,109 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:50,618 - Epoch: [55][   10/  246]    Overall Loss 0.266682    Objective Loss 0.266682                                        LR 0.000036    Time 0.050881    
2023-01-06 16:38:50,737 - Epoch: [55][   20/  246]    Overall Loss 0.279001    Objective Loss 0.279001                                        LR 0.000036    Time 0.031327    
2023-01-06 16:38:50,860 - Epoch: [55][   30/  246]    Overall Loss 0.277078    Objective Loss 0.277078                                        LR 0.000036    Time 0.024982    
2023-01-06 16:38:50,995 - Epoch: [55][   40/  246]    Overall Loss 0.282062    Objective Loss 0.282062                                        LR 0.000036    Time 0.022090    
2023-01-06 16:38:51,144 - Epoch: [55][   50/  246]    Overall Loss 0.280203    Objective Loss 0.280203                                        LR 0.000036    Time 0.020648    
2023-01-06 16:38:51,292 - Epoch: [55][   60/  246]    Overall Loss 0.280910    Objective Loss 0.280910                                        LR 0.000036    Time 0.019663    
2023-01-06 16:38:51,439 - Epoch: [55][   70/  246]    Overall Loss 0.280780    Objective Loss 0.280780                                        LR 0.000036    Time 0.018957    
2023-01-06 16:38:51,583 - Epoch: [55][   80/  246]    Overall Loss 0.283975    Objective Loss 0.283975                                        LR 0.000036    Time 0.018375    
2023-01-06 16:38:51,729 - Epoch: [55][   90/  246]    Overall Loss 0.285646    Objective Loss 0.285646                                        LR 0.000036    Time 0.017962    
2023-01-06 16:38:51,895 - Epoch: [55][  100/  246]    Overall Loss 0.286992    Objective Loss 0.286992                                        LR 0.000036    Time 0.017818    
2023-01-06 16:38:52,069 - Epoch: [55][  110/  246]    Overall Loss 0.288805    Objective Loss 0.288805                                        LR 0.000036    Time 0.017777    
2023-01-06 16:38:52,253 - Epoch: [55][  120/  246]    Overall Loss 0.290270    Objective Loss 0.290270                                        LR 0.000036    Time 0.017819    
2023-01-06 16:38:52,435 - Epoch: [55][  130/  246]    Overall Loss 0.289973    Objective Loss 0.289973                                        LR 0.000036    Time 0.017849    
2023-01-06 16:38:52,616 - Epoch: [55][  140/  246]    Overall Loss 0.290493    Objective Loss 0.290493                                        LR 0.000036    Time 0.017863    
2023-01-06 16:38:52,801 - Epoch: [55][  150/  246]    Overall Loss 0.290237    Objective Loss 0.290237                                        LR 0.000036    Time 0.017899    
2023-01-06 16:38:52,985 - Epoch: [55][  160/  246]    Overall Loss 0.290786    Objective Loss 0.290786                                        LR 0.000036    Time 0.017932    
2023-01-06 16:38:53,166 - Epoch: [55][  170/  246]    Overall Loss 0.291200    Objective Loss 0.291200                                        LR 0.000036    Time 0.017939    
2023-01-06 16:38:53,354 - Epoch: [55][  180/  246]    Overall Loss 0.292016    Objective Loss 0.292016                                        LR 0.000036    Time 0.017981    
2023-01-06 16:38:53,537 - Epoch: [55][  190/  246]    Overall Loss 0.291257    Objective Loss 0.291257                                        LR 0.000036    Time 0.017994    
2023-01-06 16:38:53,721 - Epoch: [55][  200/  246]    Overall Loss 0.290454    Objective Loss 0.290454                                        LR 0.000036    Time 0.018014    
2023-01-06 16:38:53,902 - Epoch: [55][  210/  246]    Overall Loss 0.290580    Objective Loss 0.290580                                        LR 0.000036    Time 0.018013    
2023-01-06 16:38:54,071 - Epoch: [55][  220/  246]    Overall Loss 0.289791    Objective Loss 0.289791                                        LR 0.000036    Time 0.017964    
2023-01-06 16:38:54,241 - Epoch: [55][  230/  246]    Overall Loss 0.289619    Objective Loss 0.289619                                        LR 0.000036    Time 0.017920    
2023-01-06 16:38:54,399 - Epoch: [55][  240/  246]    Overall Loss 0.289457    Objective Loss 0.289457                                        LR 0.000036    Time 0.017829    
2023-01-06 16:38:54,469 - Epoch: [55][  246/  246]    Overall Loss 0.290282    Objective Loss 0.290282    Top1 87.559809    LR 0.000036    Time 0.017677    
2023-01-06 16:38:54,619 - --- validate (epoch=55)-----------
2023-01-06 16:38:54,619 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:55,050 - Epoch: [55][   10/   28]    Loss 0.290565    Top1 89.765625    
2023-01-06 16:38:55,157 - Epoch: [55][   20/   28]    Loss 0.283524    Top1 89.863281    
2023-01-06 16:38:55,210 - Epoch: [55][   28/   28]    Loss 0.291146    Top1 89.665044    
2023-01-06 16:38:55,372 - ==> Top1: 89.665    Loss: 0.291

2023-01-06 16:38:55,372 - ==> Confusion:
[[ 213   16  210]
 [  11  217  374]
 [  51   60 5834]]

2023-01-06 16:38:55,373 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 151104 on epoch: 55]
2023-01-06 16:38:55,373 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:38:55,380 - 

2023-01-06 16:38:55,380 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:38:56,038 - Epoch: [56][   10/  246]    Overall Loss 0.292863    Objective Loss 0.292863                                        LR 0.000036    Time 0.065746    
2023-01-06 16:38:56,182 - Epoch: [56][   20/  246]    Overall Loss 0.290214    Objective Loss 0.290214                                        LR 0.000036    Time 0.040045    
2023-01-06 16:38:56,328 - Epoch: [56][   30/  246]    Overall Loss 0.284468    Objective Loss 0.284468                                        LR 0.000036    Time 0.031560    
2023-01-06 16:38:56,480 - Epoch: [56][   40/  246]    Overall Loss 0.290400    Objective Loss 0.290400                                        LR 0.000036    Time 0.027461    
2023-01-06 16:38:56,626 - Epoch: [56][   50/  246]    Overall Loss 0.289525    Objective Loss 0.289525                                        LR 0.000036    Time 0.024865    
2023-01-06 16:38:56,767 - Epoch: [56][   60/  246]    Overall Loss 0.291443    Objective Loss 0.291443                                        LR 0.000036    Time 0.023068    
2023-01-06 16:38:56,907 - Epoch: [56][   70/  246]    Overall Loss 0.291117    Objective Loss 0.291117                                        LR 0.000036    Time 0.021775    
2023-01-06 16:38:57,045 - Epoch: [56][   80/  246]    Overall Loss 0.290909    Objective Loss 0.290909                                        LR 0.000036    Time 0.020769    
2023-01-06 16:38:57,186 - Epoch: [56][   90/  246]    Overall Loss 0.288613    Objective Loss 0.288613                                        LR 0.000036    Time 0.020025    
2023-01-06 16:38:57,325 - Epoch: [56][  100/  246]    Overall Loss 0.290940    Objective Loss 0.290940                                        LR 0.000036    Time 0.019412    
2023-01-06 16:38:57,462 - Epoch: [56][  110/  246]    Overall Loss 0.290547    Objective Loss 0.290547                                        LR 0.000036    Time 0.018890    
2023-01-06 16:38:57,609 - Epoch: [56][  120/  246]    Overall Loss 0.291498    Objective Loss 0.291498                                        LR 0.000036    Time 0.018534    
2023-01-06 16:38:57,750 - Epoch: [56][  130/  246]    Overall Loss 0.290354    Objective Loss 0.290354                                        LR 0.000036    Time 0.018193    
2023-01-06 16:38:57,897 - Epoch: [56][  140/  246]    Overall Loss 0.290071    Objective Loss 0.290071                                        LR 0.000036    Time 0.017939    
2023-01-06 16:38:58,034 - Epoch: [56][  150/  246]    Overall Loss 0.288965    Objective Loss 0.288965                                        LR 0.000036    Time 0.017656    
2023-01-06 16:38:58,172 - Epoch: [56][  160/  246]    Overall Loss 0.290311    Objective Loss 0.290311                                        LR 0.000036    Time 0.017411    
2023-01-06 16:38:58,312 - Epoch: [56][  170/  246]    Overall Loss 0.290653    Objective Loss 0.290653                                        LR 0.000036    Time 0.017204    
2023-01-06 16:38:58,450 - Epoch: [56][  180/  246]    Overall Loss 0.291279    Objective Loss 0.291279                                        LR 0.000036    Time 0.017012    
2023-01-06 16:38:58,588 - Epoch: [56][  190/  246]    Overall Loss 0.291261    Objective Loss 0.291261                                        LR 0.000036    Time 0.016845    
2023-01-06 16:38:58,728 - Epoch: [56][  200/  246]    Overall Loss 0.290638    Objective Loss 0.290638                                        LR 0.000036    Time 0.016696    
2023-01-06 16:38:58,866 - Epoch: [56][  210/  246]    Overall Loss 0.291102    Objective Loss 0.291102                                        LR 0.000036    Time 0.016557    
2023-01-06 16:38:59,005 - Epoch: [56][  220/  246]    Overall Loss 0.290864    Objective Loss 0.290864                                        LR 0.000036    Time 0.016435    
2023-01-06 16:38:59,142 - Epoch: [56][  230/  246]    Overall Loss 0.291007    Objective Loss 0.291007                                        LR 0.000036    Time 0.016315    
2023-01-06 16:38:59,296 - Epoch: [56][  240/  246]    Overall Loss 0.290429    Objective Loss 0.290429                                        LR 0.000036    Time 0.016274    
2023-01-06 16:38:59,358 - Epoch: [56][  246/  246]    Overall Loss 0.290005    Objective Loss 0.290005    Top1 92.344498    LR 0.000036    Time 0.016129    
2023-01-06 16:38:59,493 - --- validate (epoch=56)-----------
2023-01-06 16:38:59,493 - 6986 samples (256 per mini-batch)
2023-01-06 16:38:59,918 - Epoch: [56][   10/   28]    Loss 0.276823    Top1 90.585938    
2023-01-06 16:39:00,012 - Epoch: [56][   20/   28]    Loss 0.281301    Top1 89.843750    
2023-01-06 16:39:00,062 - Epoch: [56][   28/   28]    Loss 0.290322    Top1 89.665044    
2023-01-06 16:39:00,218 - ==> Top1: 89.665    Loss: 0.290

2023-01-06 16:39:00,219 - ==> Confusion:
[[ 217   13  209]
 [  13  225  364]
 [  67   56 5822]]

2023-01-06 16:39:00,220 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 16:39:00,220 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:00,226 - 

2023-01-06 16:39:00,226 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:00,887 - Epoch: [57][   10/  246]    Overall Loss 0.297400    Objective Loss 0.297400                                        LR 0.000036    Time 0.066068    
2023-01-06 16:39:01,033 - Epoch: [57][   20/  246]    Overall Loss 0.292261    Objective Loss 0.292261                                        LR 0.000036    Time 0.040285    
2023-01-06 16:39:01,184 - Epoch: [57][   30/  246]    Overall Loss 0.290694    Objective Loss 0.290694                                        LR 0.000036    Time 0.031881    
2023-01-06 16:39:01,331 - Epoch: [57][   40/  246]    Overall Loss 0.290206    Objective Loss 0.290206                                        LR 0.000036    Time 0.027581    
2023-01-06 16:39:01,475 - Epoch: [57][   50/  246]    Overall Loss 0.287495    Objective Loss 0.287495                                        LR 0.000036    Time 0.024925    
2023-01-06 16:39:01,622 - Epoch: [57][   60/  246]    Overall Loss 0.284751    Objective Loss 0.284751                                        LR 0.000036    Time 0.023217    
2023-01-06 16:39:01,766 - Epoch: [57][   70/  246]    Overall Loss 0.283562    Objective Loss 0.283562                                        LR 0.000036    Time 0.021953    
2023-01-06 16:39:01,915 - Epoch: [57][   80/  246]    Overall Loss 0.285494    Objective Loss 0.285494                                        LR 0.000036    Time 0.021064    
2023-01-06 16:39:02,056 - Epoch: [57][   90/  246]    Overall Loss 0.287737    Objective Loss 0.287737                                        LR 0.000036    Time 0.020288    
2023-01-06 16:39:02,196 - Epoch: [57][  100/  246]    Overall Loss 0.287723    Objective Loss 0.287723                                        LR 0.000036    Time 0.019659    
2023-01-06 16:39:02,335 - Epoch: [57][  110/  246]    Overall Loss 0.286733    Objective Loss 0.286733                                        LR 0.000036    Time 0.019128    
2023-01-06 16:39:02,475 - Epoch: [57][  120/  246]    Overall Loss 0.287239    Objective Loss 0.287239                                        LR 0.000036    Time 0.018697    
2023-01-06 16:39:02,614 - Epoch: [57][  130/  246]    Overall Loss 0.288434    Objective Loss 0.288434                                        LR 0.000036    Time 0.018321    
2023-01-06 16:39:02,751 - Epoch: [57][  140/  246]    Overall Loss 0.288343    Objective Loss 0.288343                                        LR 0.000036    Time 0.017994    
2023-01-06 16:39:02,889 - Epoch: [57][  150/  246]    Overall Loss 0.289113    Objective Loss 0.289113                                        LR 0.000036    Time 0.017710    
2023-01-06 16:39:03,029 - Epoch: [57][  160/  246]    Overall Loss 0.288346    Objective Loss 0.288346                                        LR 0.000036    Time 0.017476    
2023-01-06 16:39:03,169 - Epoch: [57][  170/  246]    Overall Loss 0.290534    Objective Loss 0.290534                                        LR 0.000036    Time 0.017267    
2023-01-06 16:39:03,306 - Epoch: [57][  180/  246]    Overall Loss 0.292027    Objective Loss 0.292027                                        LR 0.000036    Time 0.017067    
2023-01-06 16:39:03,446 - Epoch: [57][  190/  246]    Overall Loss 0.291588    Objective Loss 0.291588                                        LR 0.000036    Time 0.016901    
2023-01-06 16:39:03,587 - Epoch: [57][  200/  246]    Overall Loss 0.290978    Objective Loss 0.290978                                        LR 0.000036    Time 0.016761    
2023-01-06 16:39:03,724 - Epoch: [57][  210/  246]    Overall Loss 0.291017    Objective Loss 0.291017                                        LR 0.000036    Time 0.016614    
2023-01-06 16:39:03,865 - Epoch: [57][  220/  246]    Overall Loss 0.290371    Objective Loss 0.290371                                        LR 0.000036    Time 0.016495    
2023-01-06 16:39:04,012 - Epoch: [57][  230/  246]    Overall Loss 0.289358    Objective Loss 0.289358                                        LR 0.000036    Time 0.016414    
2023-01-06 16:39:04,165 - Epoch: [57][  240/  246]    Overall Loss 0.288577    Objective Loss 0.288577                                        LR 0.000036    Time 0.016369    
2023-01-06 16:39:04,231 - Epoch: [57][  246/  246]    Overall Loss 0.288038    Objective Loss 0.288038    Top1 90.909091    LR 0.000036    Time 0.016235    
2023-01-06 16:39:04,362 - --- validate (epoch=57)-----------
2023-01-06 16:39:04,363 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:04,786 - Epoch: [57][   10/   28]    Loss 0.316030    Top1 88.437500    
2023-01-06 16:39:04,884 - Epoch: [57][   20/   28]    Loss 0.298206    Top1 89.042969    
2023-01-06 16:39:04,935 - Epoch: [57][   28/   28]    Loss 0.290317    Top1 89.564844    
2023-01-06 16:39:05,096 - ==> Top1: 89.565    Loss: 0.290

2023-01-06 16:39:05,097 - ==> Confusion:
[[ 216   10  213]
 [  13  198  391]
 [  56   46 5843]]

2023-01-06 16:39:05,098 - ==> Best [Top1: 89.665   Sparsity:0.00   Params: 151104 on epoch: 56]
2023-01-06 16:39:05,098 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:05,103 - 

2023-01-06 16:39:05,103 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:05,614 - Epoch: [58][   10/  246]    Overall Loss 0.298312    Objective Loss 0.298312                                        LR 0.000036    Time 0.051023    
2023-01-06 16:39:05,763 - Epoch: [58][   20/  246]    Overall Loss 0.294560    Objective Loss 0.294560                                        LR 0.000036    Time 0.032926    
2023-01-06 16:39:05,912 - Epoch: [58][   30/  246]    Overall Loss 0.287127    Objective Loss 0.287127                                        LR 0.000036    Time 0.026909    
2023-01-06 16:39:06,050 - Epoch: [58][   40/  246]    Overall Loss 0.290080    Objective Loss 0.290080                                        LR 0.000036    Time 0.023626    
2023-01-06 16:39:06,188 - Epoch: [58][   50/  246]    Overall Loss 0.285913    Objective Loss 0.285913                                        LR 0.000036    Time 0.021661    
2023-01-06 16:39:06,328 - Epoch: [58][   60/  246]    Overall Loss 0.289774    Objective Loss 0.289774                                        LR 0.000036    Time 0.020379    
2023-01-06 16:39:06,466 - Epoch: [58][   70/  246]    Overall Loss 0.287443    Objective Loss 0.287443                                        LR 0.000036    Time 0.019423    
2023-01-06 16:39:06,603 - Epoch: [58][   80/  246]    Overall Loss 0.290100    Objective Loss 0.290100                                        LR 0.000036    Time 0.018709    
2023-01-06 16:39:06,740 - Epoch: [58][   90/  246]    Overall Loss 0.291165    Objective Loss 0.291165                                        LR 0.000036    Time 0.018144    
2023-01-06 16:39:06,877 - Epoch: [58][  100/  246]    Overall Loss 0.289723    Objective Loss 0.289723                                        LR 0.000036    Time 0.017692    
2023-01-06 16:39:07,013 - Epoch: [58][  110/  246]    Overall Loss 0.289061    Objective Loss 0.289061                                        LR 0.000036    Time 0.017323    
2023-01-06 16:39:07,149 - Epoch: [58][  120/  246]    Overall Loss 0.286443    Objective Loss 0.286443                                        LR 0.000036    Time 0.017008    
2023-01-06 16:39:07,286 - Epoch: [58][  130/  246]    Overall Loss 0.286808    Objective Loss 0.286808                                        LR 0.000036    Time 0.016747    
2023-01-06 16:39:07,422 - Epoch: [58][  140/  246]    Overall Loss 0.285909    Objective Loss 0.285909                                        LR 0.000036    Time 0.016519    
2023-01-06 16:39:07,550 - Epoch: [58][  150/  246]    Overall Loss 0.285244    Objective Loss 0.285244                                        LR 0.000036    Time 0.016272    
2023-01-06 16:39:07,687 - Epoch: [58][  160/  246]    Overall Loss 0.285031    Objective Loss 0.285031                                        LR 0.000036    Time 0.016106    
2023-01-06 16:39:07,842 - Epoch: [58][  170/  246]    Overall Loss 0.284495    Objective Loss 0.284495                                        LR 0.000036    Time 0.016065    
2023-01-06 16:39:07,992 - Epoch: [58][  180/  246]    Overall Loss 0.285760    Objective Loss 0.285760                                        LR 0.000036    Time 0.016003    
2023-01-06 16:39:08,115 - Epoch: [58][  190/  246]    Overall Loss 0.286532    Objective Loss 0.286532                                        LR 0.000036    Time 0.015810    
2023-01-06 16:39:08,234 - Epoch: [58][  200/  246]    Overall Loss 0.286726    Objective Loss 0.286726                                        LR 0.000036    Time 0.015611    
2023-01-06 16:39:08,353 - Epoch: [58][  210/  246]    Overall Loss 0.287467    Objective Loss 0.287467                                        LR 0.000036    Time 0.015433    
2023-01-06 16:39:08,472 - Epoch: [58][  220/  246]    Overall Loss 0.287950    Objective Loss 0.287950                                        LR 0.000036    Time 0.015271    
2023-01-06 16:39:08,592 - Epoch: [58][  230/  246]    Overall Loss 0.288503    Objective Loss 0.288503                                        LR 0.000036    Time 0.015124    
2023-01-06 16:39:08,732 - Epoch: [58][  240/  246]    Overall Loss 0.288338    Objective Loss 0.288338                                        LR 0.000036    Time 0.015077    
2023-01-06 16:39:08,789 - Epoch: [58][  246/  246]    Overall Loss 0.288260    Objective Loss 0.288260    Top1 88.277512    LR 0.000036    Time 0.014942    
2023-01-06 16:39:08,927 - --- validate (epoch=58)-----------
2023-01-06 16:39:08,928 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:09,357 - Epoch: [58][   10/   28]    Loss 0.291186    Top1 89.765625    
2023-01-06 16:39:09,454 - Epoch: [58][   20/   28]    Loss 0.290797    Top1 89.765625    
2023-01-06 16:39:09,502 - Epoch: [58][   28/   28]    Loss 0.295743    Top1 89.707987    
2023-01-06 16:39:09,637 - ==> Top1: 89.708    Loss: 0.296

2023-01-06 16:39:09,637 - ==> Confusion:
[[ 223   14  202]
 [  11  233  358]
 [  68   66 5811]]

2023-01-06 16:39:09,638 - ==> Best [Top1: 89.708   Sparsity:0.00   Params: 151104 on epoch: 58]
2023-01-06 16:39:09,638 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:09,644 - 

2023-01-06 16:39:09,645 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:10,303 - Epoch: [59][   10/  246]    Overall Loss 0.281477    Objective Loss 0.281477                                        LR 0.000036    Time 0.065793    
2023-01-06 16:39:10,424 - Epoch: [59][   20/  246]    Overall Loss 0.285880    Objective Loss 0.285880                                        LR 0.000036    Time 0.038899    
2023-01-06 16:39:10,541 - Epoch: [59][   30/  246]    Overall Loss 0.285632    Objective Loss 0.285632                                        LR 0.000036    Time 0.029824    
2023-01-06 16:39:10,666 - Epoch: [59][   40/  246]    Overall Loss 0.286160    Objective Loss 0.286160                                        LR 0.000036    Time 0.025456    
2023-01-06 16:39:10,796 - Epoch: [59][   50/  246]    Overall Loss 0.283563    Objective Loss 0.283563                                        LR 0.000036    Time 0.022966    
2023-01-06 16:39:10,923 - Epoch: [59][   60/  246]    Overall Loss 0.283636    Objective Loss 0.283636                                        LR 0.000036    Time 0.021244    
2023-01-06 16:39:11,050 - Epoch: [59][   70/  246]    Overall Loss 0.283121    Objective Loss 0.283121                                        LR 0.000036    Time 0.020022    
2023-01-06 16:39:11,178 - Epoch: [59][   80/  246]    Overall Loss 0.281616    Objective Loss 0.281616                                        LR 0.000036    Time 0.019084    
2023-01-06 16:39:11,307 - Epoch: [59][   90/  246]    Overall Loss 0.283282    Objective Loss 0.283282                                        LR 0.000036    Time 0.018398    
2023-01-06 16:39:11,432 - Epoch: [59][  100/  246]    Overall Loss 0.284726    Objective Loss 0.284726                                        LR 0.000036    Time 0.017786    
2023-01-06 16:39:11,559 - Epoch: [59][  110/  246]    Overall Loss 0.287114    Objective Loss 0.287114                                        LR 0.000036    Time 0.017324    
2023-01-06 16:39:11,680 - Epoch: [59][  120/  246]    Overall Loss 0.286013    Objective Loss 0.286013                                        LR 0.000036    Time 0.016881    
2023-01-06 16:39:11,793 - Epoch: [59][  130/  246]    Overall Loss 0.287289    Objective Loss 0.287289                                        LR 0.000036    Time 0.016454    
2023-01-06 16:39:11,909 - Epoch: [59][  140/  246]    Overall Loss 0.287931    Objective Loss 0.287931                                        LR 0.000036    Time 0.016099    
2023-01-06 16:39:12,035 - Epoch: [59][  150/  246]    Overall Loss 0.287719    Objective Loss 0.287719                                        LR 0.000036    Time 0.015868    
2023-01-06 16:39:12,178 - Epoch: [59][  160/  246]    Overall Loss 0.288004    Objective Loss 0.288004                                        LR 0.000036    Time 0.015764    
2023-01-06 16:39:12,323 - Epoch: [59][  170/  246]    Overall Loss 0.287192    Objective Loss 0.287192                                        LR 0.000036    Time 0.015690    
2023-01-06 16:39:12,469 - Epoch: [59][  180/  246]    Overall Loss 0.286438    Objective Loss 0.286438                                        LR 0.000036    Time 0.015623    
2023-01-06 16:39:12,608 - Epoch: [59][  190/  246]    Overall Loss 0.286167    Objective Loss 0.286167                                        LR 0.000036    Time 0.015531    
2023-01-06 16:39:12,749 - Epoch: [59][  200/  246]    Overall Loss 0.286119    Objective Loss 0.286119                                        LR 0.000036    Time 0.015457    
2023-01-06 16:39:12,887 - Epoch: [59][  210/  246]    Overall Loss 0.285651    Objective Loss 0.285651                                        LR 0.000036    Time 0.015378    
2023-01-06 16:39:13,027 - Epoch: [59][  220/  246]    Overall Loss 0.287338    Objective Loss 0.287338                                        LR 0.000036    Time 0.015315    
2023-01-06 16:39:13,166 - Epoch: [59][  230/  246]    Overall Loss 0.286783    Objective Loss 0.286783                                        LR 0.000036    Time 0.015249    
2023-01-06 16:39:13,318 - Epoch: [59][  240/  246]    Overall Loss 0.286786    Objective Loss 0.286786                                        LR 0.000036    Time 0.015242    
2023-01-06 16:39:13,382 - Epoch: [59][  246/  246]    Overall Loss 0.286992    Objective Loss 0.286992    Top1 89.712919    LR 0.000036    Time 0.015127    
2023-01-06 16:39:13,517 - --- validate (epoch=59)-----------
2023-01-06 16:39:13,518 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:13,950 - Epoch: [59][   10/   28]    Loss 0.281600    Top1 90.351562    
2023-01-06 16:39:14,045 - Epoch: [59][   20/   28]    Loss 0.290452    Top1 89.726562    
2023-01-06 16:39:14,095 - Epoch: [59][   28/   28]    Loss 0.286111    Top1 89.750930    
2023-01-06 16:39:14,259 - ==> Top1: 89.751    Loss: 0.286

2023-01-06 16:39:14,260 - ==> Confusion:
[[ 224   13  202]
 [  10  245  347]
 [  65   79 5801]]

2023-01-06 16:39:14,261 - ==> Best [Top1: 89.751   Sparsity:0.00   Params: 151104 on epoch: 59]
2023-01-06 16:39:14,261 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:14,267 - 

2023-01-06 16:39:14,267 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:14,766 - Epoch: [60][   10/  246]    Overall Loss 0.304344    Objective Loss 0.304344                                        LR 0.000036    Time 0.049761    
2023-01-06 16:39:14,887 - Epoch: [60][   20/  246]    Overall Loss 0.285532    Objective Loss 0.285532                                        LR 0.000036    Time 0.030924    
2023-01-06 16:39:15,019 - Epoch: [60][   30/  246]    Overall Loss 0.281466    Objective Loss 0.281466                                        LR 0.000036    Time 0.025012    
2023-01-06 16:39:15,154 - Epoch: [60][   40/  246]    Overall Loss 0.285161    Objective Loss 0.285161                                        LR 0.000036    Time 0.022109    
2023-01-06 16:39:15,285 - Epoch: [60][   50/  246]    Overall Loss 0.286652    Objective Loss 0.286652                                        LR 0.000036    Time 0.020309    
2023-01-06 16:39:15,413 - Epoch: [60][   60/  246]    Overall Loss 0.288465    Objective Loss 0.288465                                        LR 0.000036    Time 0.019045    
2023-01-06 16:39:15,546 - Epoch: [60][   70/  246]    Overall Loss 0.287413    Objective Loss 0.287413                                        LR 0.000036    Time 0.018221    
2023-01-06 16:39:15,680 - Epoch: [60][   80/  246]    Overall Loss 0.287151    Objective Loss 0.287151                                        LR 0.000036    Time 0.017613    
2023-01-06 16:39:15,810 - Epoch: [60][   90/  246]    Overall Loss 0.288487    Objective Loss 0.288487                                        LR 0.000036    Time 0.017095    
2023-01-06 16:39:15,938 - Epoch: [60][  100/  246]    Overall Loss 0.287896    Objective Loss 0.287896                                        LR 0.000036    Time 0.016652    
2023-01-06 16:39:16,059 - Epoch: [60][  110/  246]    Overall Loss 0.286555    Objective Loss 0.286555                                        LR 0.000036    Time 0.016229    
2023-01-06 16:39:16,180 - Epoch: [60][  120/  246]    Overall Loss 0.285170    Objective Loss 0.285170                                        LR 0.000036    Time 0.015886    
2023-01-06 16:39:16,319 - Epoch: [60][  130/  246]    Overall Loss 0.283822    Objective Loss 0.283822                                        LR 0.000036    Time 0.015730    
2023-01-06 16:39:16,454 - Epoch: [60][  140/  246]    Overall Loss 0.284095    Objective Loss 0.284095                                        LR 0.000036    Time 0.015563    
2023-01-06 16:39:16,599 - Epoch: [60][  150/  246]    Overall Loss 0.283957    Objective Loss 0.283957                                        LR 0.000036    Time 0.015490    
2023-01-06 16:39:16,739 - Epoch: [60][  160/  246]    Overall Loss 0.284486    Objective Loss 0.284486                                        LR 0.000036    Time 0.015397    
2023-01-06 16:39:16,879 - Epoch: [60][  170/  246]    Overall Loss 0.283878    Objective Loss 0.283878                                        LR 0.000036    Time 0.015313    
2023-01-06 16:39:17,000 - Epoch: [60][  180/  246]    Overall Loss 0.284551    Objective Loss 0.284551                                        LR 0.000036    Time 0.015131    
2023-01-06 16:39:17,125 - Epoch: [60][  190/  246]    Overall Loss 0.285068    Objective Loss 0.285068                                        LR 0.000036    Time 0.014990    
2023-01-06 16:39:17,256 - Epoch: [60][  200/  246]    Overall Loss 0.285189    Objective Loss 0.285189                                        LR 0.000036    Time 0.014895    
2023-01-06 16:39:17,399 - Epoch: [60][  210/  246]    Overall Loss 0.285007    Objective Loss 0.285007                                        LR 0.000036    Time 0.014862    
2023-01-06 16:39:17,539 - Epoch: [60][  220/  246]    Overall Loss 0.284850    Objective Loss 0.284850                                        LR 0.000036    Time 0.014820    
2023-01-06 16:39:17,680 - Epoch: [60][  230/  246]    Overall Loss 0.284706    Objective Loss 0.284706                                        LR 0.000036    Time 0.014788    
2023-01-06 16:39:17,824 - Epoch: [60][  240/  246]    Overall Loss 0.285745    Objective Loss 0.285745                                        LR 0.000036    Time 0.014771    
2023-01-06 16:39:17,885 - Epoch: [60][  246/  246]    Overall Loss 0.285624    Objective Loss 0.285624    Top1 88.516746    LR 0.000036    Time 0.014656    
2023-01-06 16:39:18,007 - --- validate (epoch=60)-----------
2023-01-06 16:39:18,008 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:18,433 - Epoch: [60][   10/   28]    Loss 0.311374    Top1 88.476562    
2023-01-06 16:39:18,532 - Epoch: [60][   20/   28]    Loss 0.293452    Top1 89.218750    
2023-01-06 16:39:18,580 - Epoch: [60][   28/   28]    Loss 0.301147    Top1 89.249928    
2023-01-06 16:39:18,726 - ==> Top1: 89.250    Loss: 0.301

2023-01-06 16:39:18,726 - ==> Confusion:
[[ 182   10  247]
 [  11  188  403]
 [  39   41 5865]]

2023-01-06 16:39:18,727 - ==> Best [Top1: 89.751   Sparsity:0.00   Params: 151104 on epoch: 59]
2023-01-06 16:39:18,727 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:18,732 - 

2023-01-06 16:39:18,732 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:19,395 - Epoch: [61][   10/  246]    Overall Loss 0.295861    Objective Loss 0.295861                                        LR 0.000036    Time 0.066201    
2023-01-06 16:39:19,539 - Epoch: [61][   20/  246]    Overall Loss 0.291269    Objective Loss 0.291269                                        LR 0.000036    Time 0.040261    
2023-01-06 16:39:19,681 - Epoch: [61][   30/  246]    Overall Loss 0.290434    Objective Loss 0.290434                                        LR 0.000036    Time 0.031559    
2023-01-06 16:39:19,813 - Epoch: [61][   40/  246]    Overall Loss 0.281377    Objective Loss 0.281377                                        LR 0.000036    Time 0.026968    
2023-01-06 16:39:19,944 - Epoch: [61][   50/  246]    Overall Loss 0.285812    Objective Loss 0.285812                                        LR 0.000036    Time 0.024174    
2023-01-06 16:39:20,070 - Epoch: [61][   60/  246]    Overall Loss 0.284923    Objective Loss 0.284923                                        LR 0.000036    Time 0.022247    
2023-01-06 16:39:20,199 - Epoch: [61][   70/  246]    Overall Loss 0.285376    Objective Loss 0.285376                                        LR 0.000036    Time 0.020903    
2023-01-06 16:39:20,325 - Epoch: [61][   80/  246]    Overall Loss 0.284358    Objective Loss 0.284358                                        LR 0.000036    Time 0.019853    
2023-01-06 16:39:20,457 - Epoch: [61][   90/  246]    Overall Loss 0.284879    Objective Loss 0.284879                                        LR 0.000036    Time 0.019107    
2023-01-06 16:39:20,581 - Epoch: [61][  100/  246]    Overall Loss 0.283992    Objective Loss 0.283992                                        LR 0.000036    Time 0.018436    
2023-01-06 16:39:20,709 - Epoch: [61][  110/  246]    Overall Loss 0.284485    Objective Loss 0.284485                                        LR 0.000036    Time 0.017914    
2023-01-06 16:39:20,831 - Epoch: [61][  120/  246]    Overall Loss 0.284956    Objective Loss 0.284956                                        LR 0.000036    Time 0.017435    
2023-01-06 16:39:20,954 - Epoch: [61][  130/  246]    Overall Loss 0.285060    Objective Loss 0.285060                                        LR 0.000036    Time 0.017026    
2023-01-06 16:39:21,075 - Epoch: [61][  140/  246]    Overall Loss 0.285252    Objective Loss 0.285252                                        LR 0.000036    Time 0.016673    
2023-01-06 16:39:21,197 - Epoch: [61][  150/  246]    Overall Loss 0.285033    Objective Loss 0.285033                                        LR 0.000036    Time 0.016376    
2023-01-06 16:39:21,323 - Epoch: [61][  160/  246]    Overall Loss 0.284502    Objective Loss 0.284502                                        LR 0.000036    Time 0.016135    
2023-01-06 16:39:21,446 - Epoch: [61][  170/  246]    Overall Loss 0.284340    Objective Loss 0.284340                                        LR 0.000036    Time 0.015906    
2023-01-06 16:39:21,574 - Epoch: [61][  180/  246]    Overall Loss 0.284966    Objective Loss 0.284966                                        LR 0.000036    Time 0.015735    
2023-01-06 16:39:21,702 - Epoch: [61][  190/  246]    Overall Loss 0.285357    Objective Loss 0.285357                                        LR 0.000036    Time 0.015577    
2023-01-06 16:39:21,835 - Epoch: [61][  200/  246]    Overall Loss 0.285865    Objective Loss 0.285865                                        LR 0.000036    Time 0.015460    
2023-01-06 16:39:21,961 - Epoch: [61][  210/  246]    Overall Loss 0.285133    Objective Loss 0.285133                                        LR 0.000036    Time 0.015320    
2023-01-06 16:39:22,076 - Epoch: [61][  220/  246]    Overall Loss 0.284863    Objective Loss 0.284863                                        LR 0.000036    Time 0.015147    
2023-01-06 16:39:22,189 - Epoch: [61][  230/  246]    Overall Loss 0.285605    Objective Loss 0.285605                                        LR 0.000036    Time 0.014979    
2023-01-06 16:39:22,321 - Epoch: [61][  240/  246]    Overall Loss 0.286102    Objective Loss 0.286102                                        LR 0.000036    Time 0.014902    
2023-01-06 16:39:22,378 - Epoch: [61][  246/  246]    Overall Loss 0.286103    Objective Loss 0.286103    Top1 87.799043    LR 0.000036    Time 0.014771    
2023-01-06 16:39:22,555 - --- validate (epoch=61)-----------
2023-01-06 16:39:22,555 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:22,976 - Epoch: [61][   10/   28]    Loss 0.292441    Top1 89.453125    
2023-01-06 16:39:23,073 - Epoch: [61][   20/   28]    Loss 0.281559    Top1 89.746094    
2023-01-06 16:39:23,123 - Epoch: [61][   28/   28]    Loss 0.293482    Top1 89.393072    
2023-01-06 16:39:23,285 - ==> Top1: 89.393    Loss: 0.293

2023-01-06 16:39:23,286 - ==> Confusion:
[[ 187    8  244]
 [  13  188  401]
 [  36   39 5870]]

2023-01-06 16:39:23,287 - ==> Best [Top1: 89.751   Sparsity:0.00   Params: 151104 on epoch: 59]
2023-01-06 16:39:23,287 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:23,292 - 

2023-01-06 16:39:23,292 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:23,945 - Epoch: [62][   10/  246]    Overall Loss 0.257338    Objective Loss 0.257338                                        LR 0.000036    Time 0.065250    
2023-01-06 16:39:24,066 - Epoch: [62][   20/  246]    Overall Loss 0.269286    Objective Loss 0.269286                                        LR 0.000036    Time 0.038626    
2023-01-06 16:39:24,184 - Epoch: [62][   30/  246]    Overall Loss 0.275699    Objective Loss 0.275699                                        LR 0.000036    Time 0.029700    
2023-01-06 16:39:24,306 - Epoch: [62][   40/  246]    Overall Loss 0.277725    Objective Loss 0.277725                                        LR 0.000036    Time 0.025311    
2023-01-06 16:39:24,436 - Epoch: [62][   50/  246]    Overall Loss 0.280706    Objective Loss 0.280706                                        LR 0.000036    Time 0.022834    
2023-01-06 16:39:24,567 - Epoch: [62][   60/  246]    Overall Loss 0.282397    Objective Loss 0.282397                                        LR 0.000036    Time 0.021202    
2023-01-06 16:39:24,688 - Epoch: [62][   70/  246]    Overall Loss 0.280819    Objective Loss 0.280819                                        LR 0.000036    Time 0.019900    
2023-01-06 16:39:24,813 - Epoch: [62][   80/  246]    Overall Loss 0.284244    Objective Loss 0.284244                                        LR 0.000036    Time 0.018965    
2023-01-06 16:39:24,937 - Epoch: [62][   90/  246]    Overall Loss 0.284708    Objective Loss 0.284708                                        LR 0.000036    Time 0.018237    
2023-01-06 16:39:25,065 - Epoch: [62][  100/  246]    Overall Loss 0.284958    Objective Loss 0.284958                                        LR 0.000036    Time 0.017688    
2023-01-06 16:39:25,203 - Epoch: [62][  110/  246]    Overall Loss 0.283794    Objective Loss 0.283794                                        LR 0.000036    Time 0.017331    
2023-01-06 16:39:25,349 - Epoch: [62][  120/  246]    Overall Loss 0.283922    Objective Loss 0.283922                                        LR 0.000036    Time 0.017092    
2023-01-06 16:39:25,492 - Epoch: [62][  130/  246]    Overall Loss 0.284961    Objective Loss 0.284961                                        LR 0.000036    Time 0.016878    
2023-01-06 16:39:25,637 - Epoch: [62][  140/  246]    Overall Loss 0.286170    Objective Loss 0.286170                                        LR 0.000036    Time 0.016706    
2023-01-06 16:39:25,780 - Epoch: [62][  150/  246]    Overall Loss 0.286959    Objective Loss 0.286959                                        LR 0.000036    Time 0.016540    
2023-01-06 16:39:25,916 - Epoch: [62][  160/  246]    Overall Loss 0.286570    Objective Loss 0.286570                                        LR 0.000036    Time 0.016357    
2023-01-06 16:39:26,046 - Epoch: [62][  170/  246]    Overall Loss 0.286534    Objective Loss 0.286534                                        LR 0.000036    Time 0.016154    
2023-01-06 16:39:26,175 - Epoch: [62][  180/  246]    Overall Loss 0.286386    Objective Loss 0.286386                                        LR 0.000036    Time 0.015973    
2023-01-06 16:39:26,304 - Epoch: [62][  190/  246]    Overall Loss 0.285511    Objective Loss 0.285511                                        LR 0.000036    Time 0.015809    
2023-01-06 16:39:26,427 - Epoch: [62][  200/  246]    Overall Loss 0.285690    Objective Loss 0.285690                                        LR 0.000036    Time 0.015632    
2023-01-06 16:39:26,555 - Epoch: [62][  210/  246]    Overall Loss 0.285687    Objective Loss 0.285687                                        LR 0.000036    Time 0.015492    
2023-01-06 16:39:26,686 - Epoch: [62][  220/  246]    Overall Loss 0.286154    Objective Loss 0.286154                                        LR 0.000036    Time 0.015381    
2023-01-06 16:39:26,812 - Epoch: [62][  230/  246]    Overall Loss 0.286414    Objective Loss 0.286414                                        LR 0.000036    Time 0.015260    
2023-01-06 16:39:26,951 - Epoch: [62][  240/  246]    Overall Loss 0.285610    Objective Loss 0.285610                                        LR 0.000036    Time 0.015204    
2023-01-06 16:39:27,010 - Epoch: [62][  246/  246]    Overall Loss 0.285600    Objective Loss 0.285600    Top1 86.842105    LR 0.000036    Time 0.015072    
2023-01-06 16:39:27,157 - --- validate (epoch=62)-----------
2023-01-06 16:39:27,157 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:27,593 - Epoch: [62][   10/   28]    Loss 0.290790    Top1 89.648438    
2023-01-06 16:39:27,691 - Epoch: [62][   20/   28]    Loss 0.290685    Top1 89.687500    
2023-01-06 16:39:27,741 - Epoch: [62][   28/   28]    Loss 0.285501    Top1 89.908388    
2023-01-06 16:39:27,886 - ==> Top1: 89.908    Loss: 0.286

2023-01-06 16:39:27,886 - ==> Confusion:
[[ 215   11  213]
 [  14  224  364]
 [  47   56 5842]]

2023-01-06 16:39:27,887 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:27,887 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:27,894 - 

2023-01-06 16:39:27,894 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:28,415 - Epoch: [63][   10/  246]    Overall Loss 0.287493    Objective Loss 0.287493                                        LR 0.000036    Time 0.052057    
2023-01-06 16:39:28,542 - Epoch: [63][   20/  246]    Overall Loss 0.282904    Objective Loss 0.282904                                        LR 0.000036    Time 0.032381    
2023-01-06 16:39:28,667 - Epoch: [63][   30/  246]    Overall Loss 0.288350    Objective Loss 0.288350                                        LR 0.000036    Time 0.025733    
2023-01-06 16:39:28,793 - Epoch: [63][   40/  246]    Overall Loss 0.288503    Objective Loss 0.288503                                        LR 0.000036    Time 0.022429    
2023-01-06 16:39:28,916 - Epoch: [63][   50/  246]    Overall Loss 0.288724    Objective Loss 0.288724                                        LR 0.000036    Time 0.020405    
2023-01-06 16:39:29,042 - Epoch: [63][   60/  246]    Overall Loss 0.289211    Objective Loss 0.289211                                        LR 0.000036    Time 0.019097    
2023-01-06 16:39:29,168 - Epoch: [63][   70/  246]    Overall Loss 0.289894    Objective Loss 0.289894                                        LR 0.000036    Time 0.018158    
2023-01-06 16:39:29,294 - Epoch: [63][   80/  246]    Overall Loss 0.289837    Objective Loss 0.289837                                        LR 0.000036    Time 0.017461    
2023-01-06 16:39:29,448 - Epoch: [63][   90/  246]    Overall Loss 0.288706    Objective Loss 0.288706                                        LR 0.000036    Time 0.017220    
2023-01-06 16:39:29,603 - Epoch: [63][  100/  246]    Overall Loss 0.286233    Objective Loss 0.286233                                        LR 0.000036    Time 0.017048    
2023-01-06 16:39:29,756 - Epoch: [63][  110/  246]    Overall Loss 0.284312    Objective Loss 0.284312                                        LR 0.000036    Time 0.016883    
2023-01-06 16:39:29,911 - Epoch: [63][  120/  246]    Overall Loss 0.283445    Objective Loss 0.283445                                        LR 0.000036    Time 0.016768    
2023-01-06 16:39:30,057 - Epoch: [63][  130/  246]    Overall Loss 0.283105    Objective Loss 0.283105                                        LR 0.000036    Time 0.016591    
2023-01-06 16:39:30,204 - Epoch: [63][  140/  246]    Overall Loss 0.285269    Objective Loss 0.285269                                        LR 0.000036    Time 0.016453    
2023-01-06 16:39:30,350 - Epoch: [63][  150/  246]    Overall Loss 0.285586    Objective Loss 0.285586                                        LR 0.000036    Time 0.016329    
2023-01-06 16:39:30,501 - Epoch: [63][  160/  246]    Overall Loss 0.285022    Objective Loss 0.285022                                        LR 0.000036    Time 0.016249    
2023-01-06 16:39:30,649 - Epoch: [63][  170/  246]    Overall Loss 0.285186    Objective Loss 0.285186                                        LR 0.000036    Time 0.016160    
2023-01-06 16:39:30,800 - Epoch: [63][  180/  246]    Overall Loss 0.284231    Objective Loss 0.284231                                        LR 0.000036    Time 0.016099    
2023-01-06 16:39:30,947 - Epoch: [63][  190/  246]    Overall Loss 0.283057    Objective Loss 0.283057                                        LR 0.000036    Time 0.016027    
2023-01-06 16:39:31,098 - Epoch: [63][  200/  246]    Overall Loss 0.283001    Objective Loss 0.283001                                        LR 0.000036    Time 0.015975    
2023-01-06 16:39:31,249 - Epoch: [63][  210/  246]    Overall Loss 0.282885    Objective Loss 0.282885                                        LR 0.000036    Time 0.015933    
2023-01-06 16:39:31,393 - Epoch: [63][  220/  246]    Overall Loss 0.283684    Objective Loss 0.283684                                        LR 0.000036    Time 0.015864    
2023-01-06 16:39:31,543 - Epoch: [63][  230/  246]    Overall Loss 0.283278    Objective Loss 0.283278                                        LR 0.000036    Time 0.015821    
2023-01-06 16:39:31,700 - Epoch: [63][  240/  246]    Overall Loss 0.283151    Objective Loss 0.283151                                        LR 0.000036    Time 0.015818    
2023-01-06 16:39:31,766 - Epoch: [63][  246/  246]    Overall Loss 0.282618    Objective Loss 0.282618    Top1 89.712919    LR 0.000036    Time 0.015698    
2023-01-06 16:39:31,910 - --- validate (epoch=63)-----------
2023-01-06 16:39:31,911 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:32,350 - Epoch: [63][   10/   28]    Loss 0.285706    Top1 89.921875    
2023-01-06 16:39:32,455 - Epoch: [63][   20/   28]    Loss 0.285722    Top1 89.921875    
2023-01-06 16:39:32,507 - Epoch: [63][   28/   28]    Loss 0.285866    Top1 89.636416    
2023-01-06 16:39:32,670 - ==> Top1: 89.636    Loss: 0.286

2023-01-06 16:39:32,671 - ==> Confusion:
[[ 210   13  216]
 [  12  244  346]
 [  54   83 5808]]

2023-01-06 16:39:32,671 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:32,672 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:32,677 - 

2023-01-06 16:39:32,677 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:33,324 - Epoch: [64][   10/  246]    Overall Loss 0.270366    Objective Loss 0.270366                                        LR 0.000036    Time 0.064671    
2023-01-06 16:39:33,452 - Epoch: [64][   20/  246]    Overall Loss 0.271300    Objective Loss 0.271300                                        LR 0.000036    Time 0.038618    
2023-01-06 16:39:33,586 - Epoch: [64][   30/  246]    Overall Loss 0.273371    Objective Loss 0.273371                                        LR 0.000036    Time 0.030125    
2023-01-06 16:39:33,717 - Epoch: [64][   40/  246]    Overall Loss 0.274030    Objective Loss 0.274030                                        LR 0.000036    Time 0.025860    
2023-01-06 16:39:33,863 - Epoch: [64][   50/  246]    Overall Loss 0.278673    Objective Loss 0.278673                                        LR 0.000036    Time 0.023593    
2023-01-06 16:39:34,003 - Epoch: [64][   60/  246]    Overall Loss 0.279296    Objective Loss 0.279296                                        LR 0.000036    Time 0.021994    
2023-01-06 16:39:34,143 - Epoch: [64][   70/  246]    Overall Loss 0.282589    Objective Loss 0.282589                                        LR 0.000036    Time 0.020845    
2023-01-06 16:39:34,286 - Epoch: [64][   80/  246]    Overall Loss 0.284493    Objective Loss 0.284493                                        LR 0.000036    Time 0.020014    
2023-01-06 16:39:34,430 - Epoch: [64][   90/  246]    Overall Loss 0.285228    Objective Loss 0.285228                                        LR 0.000036    Time 0.019390    
2023-01-06 16:39:34,573 - Epoch: [64][  100/  246]    Overall Loss 0.285131    Objective Loss 0.285131                                        LR 0.000036    Time 0.018880    
2023-01-06 16:39:34,719 - Epoch: [64][  110/  246]    Overall Loss 0.284880    Objective Loss 0.284880                                        LR 0.000036    Time 0.018487    
2023-01-06 16:39:34,854 - Epoch: [64][  120/  246]    Overall Loss 0.286049    Objective Loss 0.286049                                        LR 0.000036    Time 0.018061    
2023-01-06 16:39:34,985 - Epoch: [64][  130/  246]    Overall Loss 0.284903    Objective Loss 0.284903                                        LR 0.000036    Time 0.017660    
2023-01-06 16:39:35,130 - Epoch: [64][  140/  246]    Overall Loss 0.284336    Objective Loss 0.284336                                        LR 0.000036    Time 0.017434    
2023-01-06 16:39:35,266 - Epoch: [64][  150/  246]    Overall Loss 0.283574    Objective Loss 0.283574                                        LR 0.000036    Time 0.017173    
2023-01-06 16:39:35,406 - Epoch: [64][  160/  246]    Overall Loss 0.283890    Objective Loss 0.283890                                        LR 0.000036    Time 0.016973    
2023-01-06 16:39:35,545 - Epoch: [64][  170/  246]    Overall Loss 0.284523    Objective Loss 0.284523                                        LR 0.000036    Time 0.016788    
2023-01-06 16:39:35,690 - Epoch: [64][  180/  246]    Overall Loss 0.283240    Objective Loss 0.283240                                        LR 0.000036    Time 0.016660    
2023-01-06 16:39:35,833 - Epoch: [64][  190/  246]    Overall Loss 0.282558    Objective Loss 0.282558                                        LR 0.000036    Time 0.016533    
2023-01-06 16:39:35,980 - Epoch: [64][  200/  246]    Overall Loss 0.283526    Objective Loss 0.283526                                        LR 0.000036    Time 0.016438    
2023-01-06 16:39:36,116 - Epoch: [64][  210/  246]    Overall Loss 0.282188    Objective Loss 0.282188                                        LR 0.000036    Time 0.016301    
2023-01-06 16:39:36,263 - Epoch: [64][  220/  246]    Overall Loss 0.282370    Objective Loss 0.282370                                        LR 0.000036    Time 0.016226    
2023-01-06 16:39:36,402 - Epoch: [64][  230/  246]    Overall Loss 0.282672    Objective Loss 0.282672                                        LR 0.000036    Time 0.016124    
2023-01-06 16:39:36,557 - Epoch: [64][  240/  246]    Overall Loss 0.282938    Objective Loss 0.282938                                        LR 0.000036    Time 0.016095    
2023-01-06 16:39:36,626 - Epoch: [64][  246/  246]    Overall Loss 0.282576    Objective Loss 0.282576    Top1 90.191388    LR 0.000036    Time 0.015982    
2023-01-06 16:39:36,769 - --- validate (epoch=64)-----------
2023-01-06 16:39:36,769 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:37,207 - Epoch: [64][   10/   28]    Loss 0.301971    Top1 88.945312    
2023-01-06 16:39:37,307 - Epoch: [64][   20/   28]    Loss 0.295123    Top1 89.531250    
2023-01-06 16:39:37,360 - Epoch: [64][   28/   28]    Loss 0.288585    Top1 89.650730    
2023-01-06 16:39:37,490 - ==> Top1: 89.651    Loss: 0.289

2023-01-06 16:39:37,491 - ==> Confusion:
[[ 196   12  231]
 [  12  195  395]
 [  37   36 5872]]

2023-01-06 16:39:37,491 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:37,492 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:37,497 - 

2023-01-06 16:39:37,497 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:38,150 - Epoch: [65][   10/  246]    Overall Loss 0.310232    Objective Loss 0.310232                                        LR 0.000036    Time 0.065274    
2023-01-06 16:39:38,275 - Epoch: [65][   20/  246]    Overall Loss 0.295604    Objective Loss 0.295604                                        LR 0.000036    Time 0.038866    
2023-01-06 16:39:38,401 - Epoch: [65][   30/  246]    Overall Loss 0.295092    Objective Loss 0.295092                                        LR 0.000036    Time 0.030095    
2023-01-06 16:39:38,534 - Epoch: [65][   40/  246]    Overall Loss 0.286288    Objective Loss 0.286288                                        LR 0.000036    Time 0.025888    
2023-01-06 16:39:38,655 - Epoch: [65][   50/  246]    Overall Loss 0.283662    Objective Loss 0.283662                                        LR 0.000036    Time 0.023122    
2023-01-06 16:39:38,784 - Epoch: [65][   60/  246]    Overall Loss 0.285430    Objective Loss 0.285430                                        LR 0.000036    Time 0.021419    
2023-01-06 16:39:38,912 - Epoch: [65][   70/  246]    Overall Loss 0.284784    Objective Loss 0.284784                                        LR 0.000036    Time 0.020178    
2023-01-06 16:39:39,040 - Epoch: [65][   80/  246]    Overall Loss 0.286958    Objective Loss 0.286958                                        LR 0.000036    Time 0.019248    
2023-01-06 16:39:39,166 - Epoch: [65][   90/  246]    Overall Loss 0.285125    Objective Loss 0.285125                                        LR 0.000036    Time 0.018505    
2023-01-06 16:39:39,293 - Epoch: [65][  100/  246]    Overall Loss 0.283824    Objective Loss 0.283824                                        LR 0.000036    Time 0.017924    
2023-01-06 16:39:39,419 - Epoch: [65][  110/  246]    Overall Loss 0.283231    Objective Loss 0.283231                                        LR 0.000036    Time 0.017432    
2023-01-06 16:39:39,545 - Epoch: [65][  120/  246]    Overall Loss 0.282553    Objective Loss 0.282553                                        LR 0.000036    Time 0.017028    
2023-01-06 16:39:39,670 - Epoch: [65][  130/  246]    Overall Loss 0.282514    Objective Loss 0.282514                                        LR 0.000036    Time 0.016680    
2023-01-06 16:39:39,797 - Epoch: [65][  140/  246]    Overall Loss 0.282730    Objective Loss 0.282730                                        LR 0.000036    Time 0.016386    
2023-01-06 16:39:39,922 - Epoch: [65][  150/  246]    Overall Loss 0.283418    Objective Loss 0.283418                                        LR 0.000036    Time 0.016125    
2023-01-06 16:39:40,049 - Epoch: [65][  160/  246]    Overall Loss 0.283212    Objective Loss 0.283212                                        LR 0.000036    Time 0.015912    
2023-01-06 16:39:40,175 - Epoch: [65][  170/  246]    Overall Loss 0.283201    Objective Loss 0.283201                                        LR 0.000036    Time 0.015716    
2023-01-06 16:39:40,292 - Epoch: [65][  180/  246]    Overall Loss 0.283605    Objective Loss 0.283605                                        LR 0.000036    Time 0.015490    
2023-01-06 16:39:40,413 - Epoch: [65][  190/  246]    Overall Loss 0.283173    Objective Loss 0.283173                                        LR 0.000036    Time 0.015306    
2023-01-06 16:39:40,532 - Epoch: [65][  200/  246]    Overall Loss 0.282894    Objective Loss 0.282894                                        LR 0.000036    Time 0.015134    
2023-01-06 16:39:40,651 - Epoch: [65][  210/  246]    Overall Loss 0.282544    Objective Loss 0.282544                                        LR 0.000036    Time 0.014982    
2023-01-06 16:39:40,771 - Epoch: [65][  220/  246]    Overall Loss 0.282998    Objective Loss 0.282998                                        LR 0.000036    Time 0.014844    
2023-01-06 16:39:40,889 - Epoch: [65][  230/  246]    Overall Loss 0.283097    Objective Loss 0.283097                                        LR 0.000036    Time 0.014711    
2023-01-06 16:39:41,023 - Epoch: [65][  240/  246]    Overall Loss 0.282777    Objective Loss 0.282777                                        LR 0.000036    Time 0.014653    
2023-01-06 16:39:41,084 - Epoch: [65][  246/  246]    Overall Loss 0.282968    Objective Loss 0.282968    Top1 89.712919    LR 0.000036    Time 0.014542    
2023-01-06 16:39:41,208 - --- validate (epoch=65)-----------
2023-01-06 16:39:41,208 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:41,643 - Epoch: [65][   10/   28]    Loss 0.292207    Top1 89.492188    
2023-01-06 16:39:41,746 - Epoch: [65][   20/   28]    Loss 0.295131    Top1 89.375000    
2023-01-06 16:39:41,795 - Epoch: [65][   28/   28]    Loss 0.289824    Top1 89.550530    
2023-01-06 16:39:41,931 - ==> Top1: 89.551    Loss: 0.290

2023-01-06 16:39:41,931 - ==> Confusion:
[[ 237   18  184]
 [  12  258  332]
 [  90   94 5761]]

2023-01-06 16:39:41,932 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:41,932 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:41,937 - 

2023-01-06 16:39:41,937 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:42,462 - Epoch: [66][   10/  246]    Overall Loss 0.276327    Objective Loss 0.276327                                        LR 0.000036    Time 0.052382    
2023-01-06 16:39:42,584 - Epoch: [66][   20/  246]    Overall Loss 0.277413    Objective Loss 0.277413                                        LR 0.000036    Time 0.032267    
2023-01-06 16:39:42,707 - Epoch: [66][   30/  246]    Overall Loss 0.276466    Objective Loss 0.276466                                        LR 0.000036    Time 0.025557    
2023-01-06 16:39:42,828 - Epoch: [66][   40/  246]    Overall Loss 0.279889    Objective Loss 0.279889                                        LR 0.000036    Time 0.022183    
2023-01-06 16:39:42,960 - Epoch: [66][   50/  246]    Overall Loss 0.278666    Objective Loss 0.278666                                        LR 0.000036    Time 0.020371    
2023-01-06 16:39:43,092 - Epoch: [66][   60/  246]    Overall Loss 0.277997    Objective Loss 0.277997                                        LR 0.000036    Time 0.019172    
2023-01-06 16:39:43,235 - Epoch: [66][   70/  246]    Overall Loss 0.277568    Objective Loss 0.277568                                        LR 0.000036    Time 0.018465    
2023-01-06 16:39:43,387 - Epoch: [66][   80/  246]    Overall Loss 0.279717    Objective Loss 0.279717                                        LR 0.000036    Time 0.018050    
2023-01-06 16:39:43,523 - Epoch: [66][   90/  246]    Overall Loss 0.281939    Objective Loss 0.281939                                        LR 0.000036    Time 0.017555    
2023-01-06 16:39:43,671 - Epoch: [66][  100/  246]    Overall Loss 0.281199    Objective Loss 0.281199                                        LR 0.000036    Time 0.017272    
2023-01-06 16:39:43,812 - Epoch: [66][  110/  246]    Overall Loss 0.279677    Objective Loss 0.279677                                        LR 0.000036    Time 0.016982    
2023-01-06 16:39:43,952 - Epoch: [66][  120/  246]    Overall Loss 0.280886    Objective Loss 0.280886                                        LR 0.000036    Time 0.016730    
2023-01-06 16:39:44,089 - Epoch: [66][  130/  246]    Overall Loss 0.281286    Objective Loss 0.281286                                        LR 0.000036    Time 0.016496    
2023-01-06 16:39:44,229 - Epoch: [66][  140/  246]    Overall Loss 0.281725    Objective Loss 0.281725                                        LR 0.000036    Time 0.016313    
2023-01-06 16:39:44,367 - Epoch: [66][  150/  246]    Overall Loss 0.281892    Objective Loss 0.281892                                        LR 0.000036    Time 0.016140    
2023-01-06 16:39:44,498 - Epoch: [66][  160/  246]    Overall Loss 0.282127    Objective Loss 0.282127                                        LR 0.000036    Time 0.015945    
2023-01-06 16:39:44,631 - Epoch: [66][  170/  246]    Overall Loss 0.281747    Objective Loss 0.281747                                        LR 0.000036    Time 0.015786    
2023-01-06 16:39:44,783 - Epoch: [66][  180/  246]    Overall Loss 0.282347    Objective Loss 0.282347                                        LR 0.000036    Time 0.015751    
2023-01-06 16:39:44,939 - Epoch: [66][  190/  246]    Overall Loss 0.281671    Objective Loss 0.281671                                        LR 0.000036    Time 0.015741    
2023-01-06 16:39:45,089 - Epoch: [66][  200/  246]    Overall Loss 0.281546    Objective Loss 0.281546                                        LR 0.000036    Time 0.015705    
2023-01-06 16:39:45,243 - Epoch: [66][  210/  246]    Overall Loss 0.281882    Objective Loss 0.281882                                        LR 0.000036    Time 0.015685    
2023-01-06 16:39:45,392 - Epoch: [66][  220/  246]    Overall Loss 0.282005    Objective Loss 0.282005                                        LR 0.000036    Time 0.015648    
2023-01-06 16:39:45,525 - Epoch: [66][  230/  246]    Overall Loss 0.281977    Objective Loss 0.281977                                        LR 0.000036    Time 0.015546    
2023-01-06 16:39:45,676 - Epoch: [66][  240/  246]    Overall Loss 0.282463    Objective Loss 0.282463                                        LR 0.000036    Time 0.015525    
2023-01-06 16:39:45,740 - Epoch: [66][  246/  246]    Overall Loss 0.282035    Objective Loss 0.282035    Top1 90.909091    LR 0.000036    Time 0.015405    
2023-01-06 16:39:45,870 - --- validate (epoch=66)-----------
2023-01-06 16:39:45,870 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:46,296 - Epoch: [66][   10/   28]    Loss 0.271230    Top1 90.039062    
2023-01-06 16:39:46,394 - Epoch: [66][   20/   28]    Loss 0.286249    Top1 89.707031    
2023-01-06 16:39:46,445 - Epoch: [66][   28/   28]    Loss 0.288333    Top1 89.679359    
2023-01-06 16:39:46,589 - ==> Top1: 89.679    Loss: 0.288

2023-01-06 16:39:46,589 - ==> Confusion:
[[ 199   11  229]
 [  15  206  381]
 [  39   46 5860]]

2023-01-06 16:39:46,590 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:46,590 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:46,595 - 

2023-01-06 16:39:46,596 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:47,258 - Epoch: [67][   10/  246]    Overall Loss 0.277713    Objective Loss 0.277713                                        LR 0.000036    Time 0.066192    
2023-01-06 16:39:47,400 - Epoch: [67][   20/  246]    Overall Loss 0.278169    Objective Loss 0.278169                                        LR 0.000036    Time 0.040176    
2023-01-06 16:39:47,531 - Epoch: [67][   30/  246]    Overall Loss 0.278729    Objective Loss 0.278729                                        LR 0.000036    Time 0.031120    
2023-01-06 16:39:47,659 - Epoch: [67][   40/  246]    Overall Loss 0.278794    Objective Loss 0.278794                                        LR 0.000036    Time 0.026538    
2023-01-06 16:39:47,782 - Epoch: [67][   50/  246]    Overall Loss 0.278979    Objective Loss 0.278979                                        LR 0.000036    Time 0.023652    
2023-01-06 16:39:47,906 - Epoch: [67][   60/  246]    Overall Loss 0.280281    Objective Loss 0.280281                                        LR 0.000036    Time 0.021772    
2023-01-06 16:39:48,030 - Epoch: [67][   70/  246]    Overall Loss 0.282042    Objective Loss 0.282042                                        LR 0.000036    Time 0.020417    
2023-01-06 16:39:48,175 - Epoch: [67][   80/  246]    Overall Loss 0.279792    Objective Loss 0.279792                                        LR 0.000036    Time 0.019645    
2023-01-06 16:39:48,329 - Epoch: [67][   90/  246]    Overall Loss 0.278704    Objective Loss 0.278704                                        LR 0.000036    Time 0.019169    
2023-01-06 16:39:48,481 - Epoch: [67][  100/  246]    Overall Loss 0.278534    Objective Loss 0.278534                                        LR 0.000036    Time 0.018774    
2023-01-06 16:39:48,635 - Epoch: [67][  110/  246]    Overall Loss 0.279832    Objective Loss 0.279832                                        LR 0.000036    Time 0.018457    
2023-01-06 16:39:48,791 - Epoch: [67][  120/  246]    Overall Loss 0.279422    Objective Loss 0.279422                                        LR 0.000036    Time 0.018219    
2023-01-06 16:39:48,946 - Epoch: [67][  130/  246]    Overall Loss 0.278305    Objective Loss 0.278305                                        LR 0.000036    Time 0.018008    
2023-01-06 16:39:49,106 - Epoch: [67][  140/  246]    Overall Loss 0.278603    Objective Loss 0.278603                                        LR 0.000036    Time 0.017857    
2023-01-06 16:39:49,265 - Epoch: [67][  150/  246]    Overall Loss 0.277592    Objective Loss 0.277592                                        LR 0.000036    Time 0.017728    
2023-01-06 16:39:49,425 - Epoch: [67][  160/  246]    Overall Loss 0.277892    Objective Loss 0.277892                                        LR 0.000036    Time 0.017612    
2023-01-06 16:39:49,580 - Epoch: [67][  170/  246]    Overall Loss 0.277514    Objective Loss 0.277514                                        LR 0.000036    Time 0.017487    
2023-01-06 16:39:49,719 - Epoch: [67][  180/  246]    Overall Loss 0.277724    Objective Loss 0.277724                                        LR 0.000036    Time 0.017286    
2023-01-06 16:39:49,846 - Epoch: [67][  190/  246]    Overall Loss 0.277250    Objective Loss 0.277250                                        LR 0.000036    Time 0.017043    
2023-01-06 16:39:49,976 - Epoch: [67][  200/  246]    Overall Loss 0.277671    Objective Loss 0.277671                                        LR 0.000036    Time 0.016835    
2023-01-06 16:39:50,120 - Epoch: [67][  210/  246]    Overall Loss 0.277730    Objective Loss 0.277730                                        LR 0.000036    Time 0.016716    
2023-01-06 16:39:50,243 - Epoch: [67][  220/  246]    Overall Loss 0.278668    Objective Loss 0.278668                                        LR 0.000036    Time 0.016516    
2023-01-06 16:39:50,364 - Epoch: [67][  230/  246]    Overall Loss 0.279075    Objective Loss 0.279075                                        LR 0.000036    Time 0.016319    
2023-01-06 16:39:50,500 - Epoch: [67][  240/  246]    Overall Loss 0.279807    Objective Loss 0.279807                                        LR 0.000036    Time 0.016206    
2023-01-06 16:39:50,566 - Epoch: [67][  246/  246]    Overall Loss 0.280339    Objective Loss 0.280339    Top1 89.712919    LR 0.000036    Time 0.016076    
2023-01-06 16:39:50,695 - --- validate (epoch=67)-----------
2023-01-06 16:39:50,695 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:51,111 - Epoch: [67][   10/   28]    Loss 0.293333    Top1 89.531250    
2023-01-06 16:39:51,205 - Epoch: [67][   20/   28]    Loss 0.290344    Top1 89.667969    
2023-01-06 16:39:51,254 - Epoch: [67][   28/   28]    Loss 0.284331    Top1 89.851131    
2023-01-06 16:39:51,420 - ==> Top1: 89.851    Loss: 0.284

2023-01-06 16:39:51,421 - ==> Confusion:
[[ 210   10  219]
 [  12  209  381]
 [  48   39 5858]]

2023-01-06 16:39:51,422 - ==> Best [Top1: 89.908   Sparsity:0.00   Params: 151104 on epoch: 62]
2023-01-06 16:39:51,422 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:51,427 - 

2023-01-06 16:39:51,427 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:51,950 - Epoch: [68][   10/  246]    Overall Loss 0.266753    Objective Loss 0.266753                                        LR 0.000036    Time 0.052251    
2023-01-06 16:39:52,083 - Epoch: [68][   20/  246]    Overall Loss 0.267784    Objective Loss 0.267784                                        LR 0.000036    Time 0.032728    
2023-01-06 16:39:52,223 - Epoch: [68][   30/  246]    Overall Loss 0.275423    Objective Loss 0.275423                                        LR 0.000036    Time 0.026469    
2023-01-06 16:39:52,356 - Epoch: [68][   40/  246]    Overall Loss 0.277512    Objective Loss 0.277512                                        LR 0.000036    Time 0.023169    
2023-01-06 16:39:52,503 - Epoch: [68][   50/  246]    Overall Loss 0.277084    Objective Loss 0.277084                                        LR 0.000036    Time 0.021471    
2023-01-06 16:39:52,639 - Epoch: [68][   60/  246]    Overall Loss 0.277890    Objective Loss 0.277890                                        LR 0.000036    Time 0.020152    
2023-01-06 16:39:52,788 - Epoch: [68][   70/  246]    Overall Loss 0.277996    Objective Loss 0.277996                                        LR 0.000036    Time 0.019387    
2023-01-06 16:39:52,922 - Epoch: [68][   80/  246]    Overall Loss 0.278921    Objective Loss 0.278921                                        LR 0.000036    Time 0.018636    
2023-01-06 16:39:53,063 - Epoch: [68][   90/  246]    Overall Loss 0.277900    Objective Loss 0.277900                                        LR 0.000036    Time 0.018127    
2023-01-06 16:39:53,197 - Epoch: [68][  100/  246]    Overall Loss 0.277692    Objective Loss 0.277692                                        LR 0.000036    Time 0.017636    
2023-01-06 16:39:53,326 - Epoch: [68][  110/  246]    Overall Loss 0.277295    Objective Loss 0.277295                                        LR 0.000036    Time 0.017203    
2023-01-06 16:39:53,459 - Epoch: [68][  120/  246]    Overall Loss 0.278023    Objective Loss 0.278023                                        LR 0.000036    Time 0.016869    
2023-01-06 16:39:53,591 - Epoch: [68][  130/  246]    Overall Loss 0.276955    Objective Loss 0.276955                                        LR 0.000036    Time 0.016587    
2023-01-06 16:39:53,717 - Epoch: [68][  140/  246]    Overall Loss 0.277904    Objective Loss 0.277904                                        LR 0.000036    Time 0.016297    
2023-01-06 16:39:53,836 - Epoch: [68][  150/  246]    Overall Loss 0.277553    Objective Loss 0.277553                                        LR 0.000036    Time 0.016000    
2023-01-06 16:39:53,954 - Epoch: [68][  160/  246]    Overall Loss 0.278850    Objective Loss 0.278850                                        LR 0.000036    Time 0.015740    
2023-01-06 16:39:54,077 - Epoch: [68][  170/  246]    Overall Loss 0.279211    Objective Loss 0.279211                                        LR 0.000036    Time 0.015531    
2023-01-06 16:39:54,210 - Epoch: [68][  180/  246]    Overall Loss 0.278937    Objective Loss 0.278937                                        LR 0.000036    Time 0.015410    
2023-01-06 16:39:54,342 - Epoch: [68][  190/  246]    Overall Loss 0.278300    Objective Loss 0.278300                                        LR 0.000036    Time 0.015289    
2023-01-06 16:39:54,461 - Epoch: [68][  200/  246]    Overall Loss 0.277770    Objective Loss 0.277770                                        LR 0.000036    Time 0.015117    
2023-01-06 16:39:54,581 - Epoch: [68][  210/  246]    Overall Loss 0.278753    Objective Loss 0.278753                                        LR 0.000036    Time 0.014965    
2023-01-06 16:39:54,700 - Epoch: [68][  220/  246]    Overall Loss 0.278098    Objective Loss 0.278098                                        LR 0.000036    Time 0.014826    
2023-01-06 16:39:54,821 - Epoch: [68][  230/  246]    Overall Loss 0.278140    Objective Loss 0.278140                                        LR 0.000036    Time 0.014703    
2023-01-06 16:39:54,970 - Epoch: [68][  240/  246]    Overall Loss 0.278201    Objective Loss 0.278201                                        LR 0.000036    Time 0.014712    
2023-01-06 16:39:55,037 - Epoch: [68][  246/  246]    Overall Loss 0.278650    Objective Loss 0.278650    Top1 90.191388    LR 0.000036    Time 0.014626    
2023-01-06 16:39:55,158 - --- validate (epoch=68)-----------
2023-01-06 16:39:55,158 - 6986 samples (256 per mini-batch)
2023-01-06 16:39:55,600 - Epoch: [68][   10/   28]    Loss 0.291062    Top1 89.492188    
2023-01-06 16:39:55,694 - Epoch: [68][   20/   28]    Loss 0.286526    Top1 89.960938    
2023-01-06 16:39:55,744 - Epoch: [68][   28/   28]    Loss 0.284402    Top1 90.008589    
2023-01-06 16:39:55,870 - ==> Top1: 90.009    Loss: 0.284

2023-01-06 16:39:55,871 - ==> Confusion:
[[ 214   14  211]
 [  13  229  360]
 [  43   57 5845]]

2023-01-06 16:39:55,872 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 151104 on epoch: 68]
2023-01-06 16:39:55,872 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:39:55,878 - 

2023-01-06 16:39:55,879 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:39:56,554 - Epoch: [69][   10/  246]    Overall Loss 0.284650    Objective Loss 0.284650                                        LR 0.000036    Time 0.067445    
2023-01-06 16:39:56,701 - Epoch: [69][   20/  246]    Overall Loss 0.288647    Objective Loss 0.288647                                        LR 0.000036    Time 0.041072    
2023-01-06 16:39:56,836 - Epoch: [69][   30/  246]    Overall Loss 0.287039    Objective Loss 0.287039                                        LR 0.000036    Time 0.031875    
2023-01-06 16:39:56,968 - Epoch: [69][   40/  246]    Overall Loss 0.285812    Objective Loss 0.285812                                        LR 0.000036    Time 0.027150    
2023-01-06 16:39:57,102 - Epoch: [69][   50/  246]    Overall Loss 0.284015    Objective Loss 0.284015                                        LR 0.000036    Time 0.024387    
2023-01-06 16:39:57,231 - Epoch: [69][   60/  246]    Overall Loss 0.285164    Objective Loss 0.285164                                        LR 0.000036    Time 0.022458    
2023-01-06 16:39:57,360 - Epoch: [69][   70/  246]    Overall Loss 0.285108    Objective Loss 0.285108                                        LR 0.000036    Time 0.021097    
2023-01-06 16:39:57,485 - Epoch: [69][   80/  246]    Overall Loss 0.283823    Objective Loss 0.283823                                        LR 0.000036    Time 0.020015    
2023-01-06 16:39:57,617 - Epoch: [69][   90/  246]    Overall Loss 0.284461    Objective Loss 0.284461                                        LR 0.000036    Time 0.019249    
2023-01-06 16:39:57,752 - Epoch: [69][  100/  246]    Overall Loss 0.281838    Objective Loss 0.281838                                        LR 0.000036    Time 0.018652    
2023-01-06 16:39:57,898 - Epoch: [69][  110/  246]    Overall Loss 0.281416    Objective Loss 0.281416                                        LR 0.000036    Time 0.018288    
2023-01-06 16:39:58,042 - Epoch: [69][  120/  246]    Overall Loss 0.280843    Objective Loss 0.280843                                        LR 0.000036    Time 0.017956    
2023-01-06 16:39:58,189 - Epoch: [69][  130/  246]    Overall Loss 0.282928    Objective Loss 0.282928                                        LR 0.000036    Time 0.017702    
2023-01-06 16:39:58,331 - Epoch: [69][  140/  246]    Overall Loss 0.281993    Objective Loss 0.281993                                        LR 0.000036    Time 0.017453    
2023-01-06 16:39:58,474 - Epoch: [69][  150/  246]    Overall Loss 0.283108    Objective Loss 0.283108                                        LR 0.000036    Time 0.017236    
2023-01-06 16:39:58,619 - Epoch: [69][  160/  246]    Overall Loss 0.282776    Objective Loss 0.282776                                        LR 0.000036    Time 0.017066    
2023-01-06 16:39:58,760 - Epoch: [69][  170/  246]    Overall Loss 0.281428    Objective Loss 0.281428                                        LR 0.000036    Time 0.016887    
2023-01-06 16:39:58,905 - Epoch: [69][  180/  246]    Overall Loss 0.282221    Objective Loss 0.282221                                        LR 0.000036    Time 0.016750    
2023-01-06 16:39:59,053 - Epoch: [69][  190/  246]    Overall Loss 0.281923    Objective Loss 0.281923                                        LR 0.000036    Time 0.016647    
2023-01-06 16:39:59,200 - Epoch: [69][  200/  246]    Overall Loss 0.281038    Objective Loss 0.281038                                        LR 0.000036    Time 0.016548    
2023-01-06 16:39:59,341 - Epoch: [69][  210/  246]    Overall Loss 0.281206    Objective Loss 0.281206                                        LR 0.000036    Time 0.016428    
2023-01-06 16:39:59,484 - Epoch: [69][  220/  246]    Overall Loss 0.279984    Objective Loss 0.279984                                        LR 0.000036    Time 0.016333    
2023-01-06 16:39:59,628 - Epoch: [69][  230/  246]    Overall Loss 0.279146    Objective Loss 0.279146                                        LR 0.000036    Time 0.016245    
2023-01-06 16:39:59,774 - Epoch: [69][  240/  246]    Overall Loss 0.280200    Objective Loss 0.280200                                        LR 0.000036    Time 0.016176    
2023-01-06 16:39:59,839 - Epoch: [69][  246/  246]    Overall Loss 0.279740    Objective Loss 0.279740    Top1 90.909091    LR 0.000036    Time 0.016045    
2023-01-06 16:39:59,971 - --- validate (epoch=69)-----------
2023-01-06 16:39:59,971 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:00,403 - Epoch: [69][   10/   28]    Loss 0.293610    Top1 89.023438    
2023-01-06 16:40:00,495 - Epoch: [69][   20/   28]    Loss 0.285473    Top1 89.492188    
2023-01-06 16:40:00,546 - Epoch: [69][   28/   28]    Loss 0.287063    Top1 89.550530    
2023-01-06 16:40:00,715 - ==> Top1: 89.551    Loss: 0.287

2023-01-06 16:40:00,715 - ==> Confusion:
[[ 176   13  250]
 [   9  199  394]
 [  28   36 5881]]

2023-01-06 16:40:00,716 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 151104 on epoch: 68]
2023-01-06 16:40:00,716 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:00,721 - 

2023-01-06 16:40:00,722 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:01,241 - Epoch: [70][   10/  246]    Overall Loss 0.275571    Objective Loss 0.275571                                        LR 0.000022    Time 0.051862    
2023-01-06 16:40:01,381 - Epoch: [70][   20/  246]    Overall Loss 0.284870    Objective Loss 0.284870                                        LR 0.000022    Time 0.032911    
2023-01-06 16:40:01,530 - Epoch: [70][   30/  246]    Overall Loss 0.281172    Objective Loss 0.281172                                        LR 0.000022    Time 0.026888    
2023-01-06 16:40:01,679 - Epoch: [70][   40/  246]    Overall Loss 0.279579    Objective Loss 0.279579                                        LR 0.000022    Time 0.023881    
2023-01-06 16:40:01,823 - Epoch: [70][   50/  246]    Overall Loss 0.282304    Objective Loss 0.282304                                        LR 0.000022    Time 0.021953    
2023-01-06 16:40:01,970 - Epoch: [70][   60/  246]    Overall Loss 0.282528    Objective Loss 0.282528                                        LR 0.000022    Time 0.020731    
2023-01-06 16:40:02,110 - Epoch: [70][   70/  246]    Overall Loss 0.283275    Objective Loss 0.283275                                        LR 0.000022    Time 0.019758    
2023-01-06 16:40:02,248 - Epoch: [70][   80/  246]    Overall Loss 0.281120    Objective Loss 0.281120                                        LR 0.000022    Time 0.019012    
2023-01-06 16:40:02,386 - Epoch: [70][   90/  246]    Overall Loss 0.279928    Objective Loss 0.279928                                        LR 0.000022    Time 0.018433    
2023-01-06 16:40:02,525 - Epoch: [70][  100/  246]    Overall Loss 0.279862    Objective Loss 0.279862                                        LR 0.000022    Time 0.017968    
2023-01-06 16:40:02,662 - Epoch: [70][  110/  246]    Overall Loss 0.277968    Objective Loss 0.277968                                        LR 0.000022    Time 0.017581    
2023-01-06 16:40:02,800 - Epoch: [70][  120/  246]    Overall Loss 0.276934    Objective Loss 0.276934                                        LR 0.000022    Time 0.017258    
2023-01-06 16:40:02,935 - Epoch: [70][  130/  246]    Overall Loss 0.277127    Objective Loss 0.277127                                        LR 0.000022    Time 0.016969    
2023-01-06 16:40:03,073 - Epoch: [70][  140/  246]    Overall Loss 0.276654    Objective Loss 0.276654                                        LR 0.000022    Time 0.016743    
2023-01-06 16:40:03,212 - Epoch: [70][  150/  246]    Overall Loss 0.275987    Objective Loss 0.275987                                        LR 0.000022    Time 0.016548    
2023-01-06 16:40:03,349 - Epoch: [70][  160/  246]    Overall Loss 0.275584    Objective Loss 0.275584                                        LR 0.000022    Time 0.016369    
2023-01-06 16:40:03,485 - Epoch: [70][  170/  246]    Overall Loss 0.275386    Objective Loss 0.275386                                        LR 0.000022    Time 0.016204    
2023-01-06 16:40:03,623 - Epoch: [70][  180/  246]    Overall Loss 0.274128    Objective Loss 0.274128                                        LR 0.000022    Time 0.016069    
2023-01-06 16:40:03,755 - Epoch: [70][  190/  246]    Overall Loss 0.273966    Objective Loss 0.273966                                        LR 0.000022    Time 0.015915    
2023-01-06 16:40:03,888 - Epoch: [70][  200/  246]    Overall Loss 0.274120    Objective Loss 0.274120                                        LR 0.000022    Time 0.015781    
2023-01-06 16:40:04,020 - Epoch: [70][  210/  246]    Overall Loss 0.274875    Objective Loss 0.274875                                        LR 0.000022    Time 0.015657    
2023-01-06 16:40:04,152 - Epoch: [70][  220/  246]    Overall Loss 0.274874    Objective Loss 0.274874                                        LR 0.000022    Time 0.015544    
2023-01-06 16:40:04,284 - Epoch: [70][  230/  246]    Overall Loss 0.275309    Objective Loss 0.275309                                        LR 0.000022    Time 0.015441    
2023-01-06 16:40:04,432 - Epoch: [70][  240/  246]    Overall Loss 0.275197    Objective Loss 0.275197                                        LR 0.000022    Time 0.015412    
2023-01-06 16:40:04,498 - Epoch: [70][  246/  246]    Overall Loss 0.274506    Objective Loss 0.274506    Top1 89.473684    LR 0.000022    Time 0.015304    
2023-01-06 16:40:04,629 - --- validate (epoch=70)-----------
2023-01-06 16:40:04,632 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:05,067 - Epoch: [70][   10/   28]    Loss 0.279909    Top1 90.039062    
2023-01-06 16:40:05,177 - Epoch: [70][   20/   28]    Loss 0.292633    Top1 89.609375    
2023-01-06 16:40:05,226 - Epoch: [70][   28/   28]    Loss 0.286820    Top1 89.822502    
2023-01-06 16:40:05,371 - ==> Top1: 89.823    Loss: 0.287

2023-01-06 16:40:05,371 - ==> Confusion:
[[ 213   20  206]
 [  15  247  340]
 [  57   73 5815]]

2023-01-06 16:40:05,372 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 151104 on epoch: 68]
2023-01-06 16:40:05,372 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:05,377 - 

2023-01-06 16:40:05,377 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:06,028 - Epoch: [71][   10/  246]    Overall Loss 0.269162    Objective Loss 0.269162                                        LR 0.000022    Time 0.065053    
2023-01-06 16:40:06,173 - Epoch: [71][   20/  246]    Overall Loss 0.273173    Objective Loss 0.273173                                        LR 0.000022    Time 0.039744    
2023-01-06 16:40:06,325 - Epoch: [71][   30/  246]    Overall Loss 0.273168    Objective Loss 0.273168                                        LR 0.000022    Time 0.031553    
2023-01-06 16:40:06,471 - Epoch: [71][   40/  246]    Overall Loss 0.271692    Objective Loss 0.271692                                        LR 0.000022    Time 0.027283    
2023-01-06 16:40:06,619 - Epoch: [71][   50/  246]    Overall Loss 0.273545    Objective Loss 0.273545                                        LR 0.000022    Time 0.024791    
2023-01-06 16:40:06,766 - Epoch: [71][   60/  246]    Overall Loss 0.274785    Objective Loss 0.274785                                        LR 0.000022    Time 0.023103    
2023-01-06 16:40:06,916 - Epoch: [71][   70/  246]    Overall Loss 0.277206    Objective Loss 0.277206                                        LR 0.000022    Time 0.021934    
2023-01-06 16:40:07,061 - Epoch: [71][   80/  246]    Overall Loss 0.277746    Objective Loss 0.277746                                        LR 0.000022    Time 0.021005    
2023-01-06 16:40:07,214 - Epoch: [71][   90/  246]    Overall Loss 0.277078    Objective Loss 0.277078                                        LR 0.000022    Time 0.020338    
2023-01-06 16:40:07,358 - Epoch: [71][  100/  246]    Overall Loss 0.278024    Objective Loss 0.278024                                        LR 0.000022    Time 0.019747    
2023-01-06 16:40:07,511 - Epoch: [71][  110/  246]    Overall Loss 0.278349    Objective Loss 0.278349                                        LR 0.000022    Time 0.019319    
2023-01-06 16:40:07,659 - Epoch: [71][  120/  246]    Overall Loss 0.278677    Objective Loss 0.278677                                        LR 0.000022    Time 0.018942    
2023-01-06 16:40:07,812 - Epoch: [71][  130/  246]    Overall Loss 0.279016    Objective Loss 0.279016                                        LR 0.000022    Time 0.018659    
2023-01-06 16:40:07,958 - Epoch: [71][  140/  246]    Overall Loss 0.277670    Objective Loss 0.277670                                        LR 0.000022    Time 0.018367    
2023-01-06 16:40:08,113 - Epoch: [71][  150/  246]    Overall Loss 0.277492    Objective Loss 0.277492                                        LR 0.000022    Time 0.018175    
2023-01-06 16:40:08,263 - Epoch: [71][  160/  246]    Overall Loss 0.278280    Objective Loss 0.278280                                        LR 0.000022    Time 0.017968    
2023-01-06 16:40:08,400 - Epoch: [71][  170/  246]    Overall Loss 0.277787    Objective Loss 0.277787                                        LR 0.000022    Time 0.017717    
2023-01-06 16:40:08,527 - Epoch: [71][  180/  246]    Overall Loss 0.277398    Objective Loss 0.277398                                        LR 0.000022    Time 0.017439    
2023-01-06 16:40:08,690 - Epoch: [71][  190/  246]    Overall Loss 0.276561    Objective Loss 0.276561                                        LR 0.000022    Time 0.017372    
2023-01-06 16:40:08,843 - Epoch: [71][  200/  246]    Overall Loss 0.276297    Objective Loss 0.276297                                        LR 0.000022    Time 0.017267    
2023-01-06 16:40:09,004 - Epoch: [71][  210/  246]    Overall Loss 0.276471    Objective Loss 0.276471                                        LR 0.000022    Time 0.017211    
2023-01-06 16:40:09,154 - Epoch: [71][  220/  246]    Overall Loss 0.276550    Objective Loss 0.276550                                        LR 0.000022    Time 0.017107    
2023-01-06 16:40:09,306 - Epoch: [71][  230/  246]    Overall Loss 0.275522    Objective Loss 0.275522                                        LR 0.000022    Time 0.017022    
2023-01-06 16:40:09,463 - Epoch: [71][  240/  246]    Overall Loss 0.275021    Objective Loss 0.275021                                        LR 0.000022    Time 0.016966    
2023-01-06 16:40:09,528 - Epoch: [71][  246/  246]    Overall Loss 0.275197    Objective Loss 0.275197    Top1 89.234450    LR 0.000022    Time 0.016816    
2023-01-06 16:40:09,665 - --- validate (epoch=71)-----------
2023-01-06 16:40:09,666 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:10,112 - Epoch: [71][   10/   28]    Loss 0.294965    Top1 89.335938    
2023-01-06 16:40:10,225 - Epoch: [71][   20/   28]    Loss 0.289045    Top1 89.648438    
2023-01-06 16:40:10,273 - Epoch: [71][   28/   28]    Loss 0.281469    Top1 89.836816    
2023-01-06 16:40:10,411 - ==> Top1: 89.837    Loss: 0.281

2023-01-06 16:40:10,411 - ==> Confusion:
[[ 200   12  227]
 [   9  215  378]
 [  35   49 5861]]

2023-01-06 16:40:10,412 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 151104 on epoch: 68]
2023-01-06 16:40:10,412 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:10,417 - 

2023-01-06 16:40:10,418 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:11,101 - Epoch: [72][   10/  246]    Overall Loss 0.290321    Objective Loss 0.290321                                        LR 0.000022    Time 0.068315    
2023-01-06 16:40:11,250 - Epoch: [72][   20/  246]    Overall Loss 0.301007    Objective Loss 0.301007                                        LR 0.000022    Time 0.041564    
2023-01-06 16:40:11,401 - Epoch: [72][   30/  246]    Overall Loss 0.293461    Objective Loss 0.293461                                        LR 0.000022    Time 0.032730    
2023-01-06 16:40:11,541 - Epoch: [72][   40/  246]    Overall Loss 0.289521    Objective Loss 0.289521                                        LR 0.000022    Time 0.028048    
2023-01-06 16:40:11,680 - Epoch: [72][   50/  246]    Overall Loss 0.283455    Objective Loss 0.283455                                        LR 0.000022    Time 0.025208    
2023-01-06 16:40:11,825 - Epoch: [72][   60/  246]    Overall Loss 0.285332    Objective Loss 0.285332                                        LR 0.000022    Time 0.023411    
2023-01-06 16:40:11,973 - Epoch: [72][   70/  246]    Overall Loss 0.282145    Objective Loss 0.282145                                        LR 0.000022    Time 0.022170    
2023-01-06 16:40:12,117 - Epoch: [72][   80/  246]    Overall Loss 0.280604    Objective Loss 0.280604                                        LR 0.000022    Time 0.021197    
2023-01-06 16:40:12,262 - Epoch: [72][   90/  246]    Overall Loss 0.279009    Objective Loss 0.279009                                        LR 0.000022    Time 0.020454    
2023-01-06 16:40:12,410 - Epoch: [72][  100/  246]    Overall Loss 0.276292    Objective Loss 0.276292                                        LR 0.000022    Time 0.019880    
2023-01-06 16:40:12,560 - Epoch: [72][  110/  246]    Overall Loss 0.274853    Objective Loss 0.274853                                        LR 0.000022    Time 0.019433    
2023-01-06 16:40:12,702 - Epoch: [72][  120/  246]    Overall Loss 0.276651    Objective Loss 0.276651                                        LR 0.000022    Time 0.018998    
2023-01-06 16:40:12,839 - Epoch: [72][  130/  246]    Overall Loss 0.275416    Objective Loss 0.275416                                        LR 0.000022    Time 0.018587    
2023-01-06 16:40:12,975 - Epoch: [72][  140/  246]    Overall Loss 0.275639    Objective Loss 0.275639                                        LR 0.000022    Time 0.018228    
2023-01-06 16:40:13,115 - Epoch: [72][  150/  246]    Overall Loss 0.276226    Objective Loss 0.276226                                        LR 0.000022    Time 0.017941    
2023-01-06 16:40:13,232 - Epoch: [72][  160/  246]    Overall Loss 0.275362    Objective Loss 0.275362                                        LR 0.000022    Time 0.017550    
2023-01-06 16:40:13,351 - Epoch: [72][  170/  246]    Overall Loss 0.275356    Objective Loss 0.275356                                        LR 0.000022    Time 0.017216    
2023-01-06 16:40:13,469 - Epoch: [72][  180/  246]    Overall Loss 0.274597    Objective Loss 0.274597                                        LR 0.000022    Time 0.016915    
2023-01-06 16:40:13,586 - Epoch: [72][  190/  246]    Overall Loss 0.274344    Objective Loss 0.274344                                        LR 0.000022    Time 0.016636    
2023-01-06 16:40:13,704 - Epoch: [72][  200/  246]    Overall Loss 0.274161    Objective Loss 0.274161                                        LR 0.000022    Time 0.016391    
2023-01-06 16:40:13,840 - Epoch: [72][  210/  246]    Overall Loss 0.274085    Objective Loss 0.274085                                        LR 0.000022    Time 0.016257    
2023-01-06 16:40:13,969 - Epoch: [72][  220/  246]    Overall Loss 0.274996    Objective Loss 0.274996                                        LR 0.000022    Time 0.016103    
2023-01-06 16:40:14,096 - Epoch: [72][  230/  246]    Overall Loss 0.274840    Objective Loss 0.274840                                        LR 0.000022    Time 0.015954    
2023-01-06 16:40:14,251 - Epoch: [72][  240/  246]    Overall Loss 0.274731    Objective Loss 0.274731                                        LR 0.000022    Time 0.015935    
2023-01-06 16:40:14,316 - Epoch: [72][  246/  246]    Overall Loss 0.274767    Objective Loss 0.274767    Top1 89.234450    LR 0.000022    Time 0.015810    
2023-01-06 16:40:14,465 - --- validate (epoch=72)-----------
2023-01-06 16:40:14,465 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:14,891 - Epoch: [72][   10/   28]    Loss 0.266658    Top1 90.859375    
2023-01-06 16:40:14,987 - Epoch: [72][   20/   28]    Loss 0.284147    Top1 89.980469    
2023-01-06 16:40:15,038 - Epoch: [72][   28/   28]    Loss 0.277162    Top1 89.994274    
2023-01-06 16:40:15,195 - ==> Top1: 89.994    Loss: 0.277

2023-01-06 16:40:15,196 - ==> Confusion:
[[ 224   12  203]
 [  16  215  371]
 [  55   42 5848]]

2023-01-06 16:40:15,197 - ==> Best [Top1: 90.009   Sparsity:0.00   Params: 151104 on epoch: 68]
2023-01-06 16:40:15,197 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:15,202 - 

2023-01-06 16:40:15,202 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:15,710 - Epoch: [73][   10/  246]    Overall Loss 0.272446    Objective Loss 0.272446                                        LR 0.000022    Time 0.050724    
2023-01-06 16:40:15,848 - Epoch: [73][   20/  246]    Overall Loss 0.276661    Objective Loss 0.276661                                        LR 0.000022    Time 0.032237    
2023-01-06 16:40:15,990 - Epoch: [73][   30/  246]    Overall Loss 0.278638    Objective Loss 0.278638                                        LR 0.000022    Time 0.026241    
2023-01-06 16:40:16,141 - Epoch: [73][   40/  246]    Overall Loss 0.277330    Objective Loss 0.277330                                        LR 0.000022    Time 0.023444    
2023-01-06 16:40:16,292 - Epoch: [73][   50/  246]    Overall Loss 0.278096    Objective Loss 0.278096                                        LR 0.000022    Time 0.021759    
2023-01-06 16:40:16,446 - Epoch: [73][   60/  246]    Overall Loss 0.278596    Objective Loss 0.278596                                        LR 0.000022    Time 0.020695    
2023-01-06 16:40:16,594 - Epoch: [73][   70/  246]    Overall Loss 0.277296    Objective Loss 0.277296                                        LR 0.000022    Time 0.019838    
2023-01-06 16:40:16,743 - Epoch: [73][   80/  246]    Overall Loss 0.279915    Objective Loss 0.279915                                        LR 0.000022    Time 0.019201    
2023-01-06 16:40:16,895 - Epoch: [73][   90/  246]    Overall Loss 0.277929    Objective Loss 0.277929                                        LR 0.000022    Time 0.018743    
2023-01-06 16:40:17,049 - Epoch: [73][  100/  246]    Overall Loss 0.278425    Objective Loss 0.278425                                        LR 0.000022    Time 0.018409    
2023-01-06 16:40:17,191 - Epoch: [73][  110/  246]    Overall Loss 0.279426    Objective Loss 0.279426                                        LR 0.000022    Time 0.018021    
2023-01-06 16:40:17,340 - Epoch: [73][  120/  246]    Overall Loss 0.279128    Objective Loss 0.279128                                        LR 0.000022    Time 0.017758    
2023-01-06 16:40:17,485 - Epoch: [73][  130/  246]    Overall Loss 0.278766    Objective Loss 0.278766                                        LR 0.000022    Time 0.017503    
2023-01-06 16:40:17,630 - Epoch: [73][  140/  246]    Overall Loss 0.278658    Objective Loss 0.278658                                        LR 0.000022    Time 0.017284    
2023-01-06 16:40:17,770 - Epoch: [73][  150/  246]    Overall Loss 0.278644    Objective Loss 0.278644                                        LR 0.000022    Time 0.017065    
2023-01-06 16:40:17,910 - Epoch: [73][  160/  246]    Overall Loss 0.277254    Objective Loss 0.277254                                        LR 0.000022    Time 0.016871    
2023-01-06 16:40:18,052 - Epoch: [73][  170/  246]    Overall Loss 0.276761    Objective Loss 0.276761                                        LR 0.000022    Time 0.016709    
2023-01-06 16:40:18,195 - Epoch: [73][  180/  246]    Overall Loss 0.276511    Objective Loss 0.276511                                        LR 0.000022    Time 0.016573    
2023-01-06 16:40:18,339 - Epoch: [73][  190/  246]    Overall Loss 0.275025    Objective Loss 0.275025                                        LR 0.000022    Time 0.016459    
2023-01-06 16:40:18,482 - Epoch: [73][  200/  246]    Overall Loss 0.275618    Objective Loss 0.275618                                        LR 0.000022    Time 0.016347    
2023-01-06 16:40:18,625 - Epoch: [73][  210/  246]    Overall Loss 0.274822    Objective Loss 0.274822                                        LR 0.000022    Time 0.016248    
2023-01-06 16:40:18,767 - Epoch: [73][  220/  246]    Overall Loss 0.274770    Objective Loss 0.274770                                        LR 0.000022    Time 0.016152    
2023-01-06 16:40:18,913 - Epoch: [73][  230/  246]    Overall Loss 0.274425    Objective Loss 0.274425                                        LR 0.000022    Time 0.016084    
2023-01-06 16:40:19,072 - Epoch: [73][  240/  246]    Overall Loss 0.274256    Objective Loss 0.274256                                        LR 0.000022    Time 0.016076    
2023-01-06 16:40:19,136 - Epoch: [73][  246/  246]    Overall Loss 0.274660    Objective Loss 0.274660    Top1 88.277512    LR 0.000022    Time 0.015940    
2023-01-06 16:40:19,302 - --- validate (epoch=73)-----------
2023-01-06 16:40:19,302 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:19,724 - Epoch: [73][   10/   28]    Loss 0.265347    Top1 90.820312    
2023-01-06 16:40:19,823 - Epoch: [73][   20/   28]    Loss 0.272416    Top1 90.273438    
2023-01-06 16:40:19,877 - Epoch: [73][   28/   28]    Loss 0.277861    Top1 90.166046    
2023-01-06 16:40:20,008 - ==> Top1: 90.166    Loss: 0.278

2023-01-06 16:40:20,008 - ==> Confusion:
[[ 208   15  216]
 [  12  245  345]
 [  42   57 5846]]

2023-01-06 16:40:20,009 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 151104 on epoch: 73]
2023-01-06 16:40:20,009 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:20,016 - 

2023-01-06 16:40:20,016 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:20,668 - Epoch: [74][   10/  246]    Overall Loss 0.269511    Objective Loss 0.269511                                        LR 0.000022    Time 0.065139    
2023-01-06 16:40:20,801 - Epoch: [74][   20/  246]    Overall Loss 0.263884    Objective Loss 0.263884                                        LR 0.000022    Time 0.039237    
2023-01-06 16:40:20,934 - Epoch: [74][   30/  246]    Overall Loss 0.263451    Objective Loss 0.263451                                        LR 0.000022    Time 0.030561    
2023-01-06 16:40:21,064 - Epoch: [74][   40/  246]    Overall Loss 0.267026    Objective Loss 0.267026                                        LR 0.000022    Time 0.026164    
2023-01-06 16:40:21,211 - Epoch: [74][   50/  246]    Overall Loss 0.269564    Objective Loss 0.269564                                        LR 0.000022    Time 0.023856    
2023-01-06 16:40:21,360 - Epoch: [74][   60/  246]    Overall Loss 0.269827    Objective Loss 0.269827                                        LR 0.000022    Time 0.022363    
2023-01-06 16:40:21,509 - Epoch: [74][   70/  246]    Overall Loss 0.268111    Objective Loss 0.268111                                        LR 0.000022    Time 0.021289    
2023-01-06 16:40:21,656 - Epoch: [74][   80/  246]    Overall Loss 0.267511    Objective Loss 0.267511                                        LR 0.000022    Time 0.020466    
2023-01-06 16:40:21,807 - Epoch: [74][   90/  246]    Overall Loss 0.268945    Objective Loss 0.268945                                        LR 0.000022    Time 0.019856    
2023-01-06 16:40:21,951 - Epoch: [74][  100/  246]    Overall Loss 0.268057    Objective Loss 0.268057                                        LR 0.000022    Time 0.019313    
2023-01-06 16:40:22,103 - Epoch: [74][  110/  246]    Overall Loss 0.269232    Objective Loss 0.269232                                        LR 0.000022    Time 0.018929    
2023-01-06 16:40:22,257 - Epoch: [74][  120/  246]    Overall Loss 0.271041    Objective Loss 0.271041                                        LR 0.000022    Time 0.018630    
2023-01-06 16:40:22,432 - Epoch: [74][  130/  246]    Overall Loss 0.270461    Objective Loss 0.270461                                        LR 0.000022    Time 0.018528    
2023-01-06 16:40:22,610 - Epoch: [74][  140/  246]    Overall Loss 0.270567    Objective Loss 0.270567                                        LR 0.000022    Time 0.018476    
2023-01-06 16:40:22,792 - Epoch: [74][  150/  246]    Overall Loss 0.270869    Objective Loss 0.270869                                        LR 0.000022    Time 0.018453    
2023-01-06 16:40:22,975 - Epoch: [74][  160/  246]    Overall Loss 0.270454    Objective Loss 0.270454                                        LR 0.000022    Time 0.018441    
2023-01-06 16:40:23,159 - Epoch: [74][  170/  246]    Overall Loss 0.270536    Objective Loss 0.270536                                        LR 0.000022    Time 0.018438    
2023-01-06 16:40:23,343 - Epoch: [74][  180/  246]    Overall Loss 0.270316    Objective Loss 0.270316                                        LR 0.000022    Time 0.018432    
2023-01-06 16:40:23,527 - Epoch: [74][  190/  246]    Overall Loss 0.271372    Objective Loss 0.271372                                        LR 0.000022    Time 0.018427    
2023-01-06 16:40:23,709 - Epoch: [74][  200/  246]    Overall Loss 0.270982    Objective Loss 0.270982                                        LR 0.000022    Time 0.018412    
2023-01-06 16:40:23,887 - Epoch: [74][  210/  246]    Overall Loss 0.272129    Objective Loss 0.272129                                        LR 0.000022    Time 0.018384    
2023-01-06 16:40:24,066 - Epoch: [74][  220/  246]    Overall Loss 0.273136    Objective Loss 0.273136                                        LR 0.000022    Time 0.018359    
2023-01-06 16:40:24,245 - Epoch: [74][  230/  246]    Overall Loss 0.272769    Objective Loss 0.272769                                        LR 0.000022    Time 0.018336    
2023-01-06 16:40:24,440 - Epoch: [74][  240/  246]    Overall Loss 0.272652    Objective Loss 0.272652                                        LR 0.000022    Time 0.018382    
2023-01-06 16:40:24,521 - Epoch: [74][  246/  246]    Overall Loss 0.272681    Objective Loss 0.272681    Top1 91.866029    LR 0.000022    Time 0.018264    
2023-01-06 16:40:24,661 - --- validate (epoch=74)-----------
2023-01-06 16:40:24,662 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:25,113 - Epoch: [74][   10/   28]    Loss 0.268173    Top1 90.234375    
2023-01-06 16:40:25,214 - Epoch: [74][   20/   28]    Loss 0.273460    Top1 90.156250    
2023-01-06 16:40:25,265 - Epoch: [74][   28/   28]    Loss 0.279461    Top1 89.922703    
2023-01-06 16:40:25,429 - ==> Top1: 89.923    Loss: 0.279

2023-01-06 16:40:25,429 - ==> Confusion:
[[ 205   15  219]
 [  12  237  353]
 [  40   65 5840]]

2023-01-06 16:40:25,430 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 151104 on epoch: 73]
2023-01-06 16:40:25,431 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:25,435 - 

2023-01-06 16:40:25,436 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:25,977 - Epoch: [75][   10/  246]    Overall Loss 0.276623    Objective Loss 0.276623                                        LR 0.000022    Time 0.054036    
2023-01-06 16:40:26,140 - Epoch: [75][   20/  246]    Overall Loss 0.267115    Objective Loss 0.267115                                        LR 0.000022    Time 0.035169    
2023-01-06 16:40:26,299 - Epoch: [75][   30/  246]    Overall Loss 0.271817    Objective Loss 0.271817                                        LR 0.000022    Time 0.028719    
2023-01-06 16:40:26,443 - Epoch: [75][   40/  246]    Overall Loss 0.269759    Objective Loss 0.269759                                        LR 0.000022    Time 0.025131    
2023-01-06 16:40:26,581 - Epoch: [75][   50/  246]    Overall Loss 0.274898    Objective Loss 0.274898                                        LR 0.000022    Time 0.022855    
2023-01-06 16:40:26,719 - Epoch: [75][   60/  246]    Overall Loss 0.275915    Objective Loss 0.275915                                        LR 0.000022    Time 0.021343    
2023-01-06 16:40:26,858 - Epoch: [75][   70/  246]    Overall Loss 0.275609    Objective Loss 0.275609                                        LR 0.000022    Time 0.020269    
2023-01-06 16:40:26,995 - Epoch: [75][   80/  246]    Overall Loss 0.276082    Objective Loss 0.276082                                        LR 0.000022    Time 0.019449    
2023-01-06 16:40:27,133 - Epoch: [75][   90/  246]    Overall Loss 0.275718    Objective Loss 0.275718                                        LR 0.000022    Time 0.018812    
2023-01-06 16:40:27,274 - Epoch: [75][  100/  246]    Overall Loss 0.276678    Objective Loss 0.276678                                        LR 0.000022    Time 0.018336    
2023-01-06 16:40:27,412 - Epoch: [75][  110/  246]    Overall Loss 0.278310    Objective Loss 0.278310                                        LR 0.000022    Time 0.017926    
2023-01-06 16:40:27,555 - Epoch: [75][  120/  246]    Overall Loss 0.278758    Objective Loss 0.278758                                        LR 0.000022    Time 0.017608    
2023-01-06 16:40:27,696 - Epoch: [75][  130/  246]    Overall Loss 0.278103    Objective Loss 0.278103                                        LR 0.000022    Time 0.017329    
2023-01-06 16:40:27,837 - Epoch: [75][  140/  246]    Overall Loss 0.277373    Objective Loss 0.277373                                        LR 0.000022    Time 0.017100    
2023-01-06 16:40:27,977 - Epoch: [75][  150/  246]    Overall Loss 0.275000    Objective Loss 0.275000                                        LR 0.000022    Time 0.016887    
2023-01-06 16:40:28,114 - Epoch: [75][  160/  246]    Overall Loss 0.274994    Objective Loss 0.274994                                        LR 0.000022    Time 0.016686    
2023-01-06 16:40:28,252 - Epoch: [75][  170/  246]    Overall Loss 0.275429    Objective Loss 0.275429                                        LR 0.000022    Time 0.016515    
2023-01-06 16:40:28,390 - Epoch: [75][  180/  246]    Overall Loss 0.275693    Objective Loss 0.275693                                        LR 0.000022    Time 0.016363    
2023-01-06 16:40:28,527 - Epoch: [75][  190/  246]    Overall Loss 0.275523    Objective Loss 0.275523                                        LR 0.000022    Time 0.016222    
2023-01-06 16:40:28,663 - Epoch: [75][  200/  246]    Overall Loss 0.274639    Objective Loss 0.274639                                        LR 0.000022    Time 0.016087    
2023-01-06 16:40:28,800 - Epoch: [75][  210/  246]    Overall Loss 0.274207    Objective Loss 0.274207                                        LR 0.000022    Time 0.015971    
2023-01-06 16:40:28,936 - Epoch: [75][  220/  246]    Overall Loss 0.273093    Objective Loss 0.273093                                        LR 0.000022    Time 0.015863    
2023-01-06 16:40:29,078 - Epoch: [75][  230/  246]    Overall Loss 0.272973    Objective Loss 0.272973                                        LR 0.000022    Time 0.015788    
2023-01-06 16:40:29,237 - Epoch: [75][  240/  246]    Overall Loss 0.272727    Objective Loss 0.272727                                        LR 0.000022    Time 0.015791    
2023-01-06 16:40:29,307 - Epoch: [75][  246/  246]    Overall Loss 0.272554    Objective Loss 0.272554    Top1 89.473684    LR 0.000022    Time 0.015687    
2023-01-06 16:40:29,446 - --- validate (epoch=75)-----------
2023-01-06 16:40:29,446 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:29,871 - Epoch: [75][   10/   28]    Loss 0.292902    Top1 89.296875    
2023-01-06 16:40:29,979 - Epoch: [75][   20/   28]    Loss 0.283413    Top1 89.804688    
2023-01-06 16:40:30,028 - Epoch: [75][   28/   28]    Loss 0.284191    Top1 89.965646    
2023-01-06 16:40:30,187 - ==> Top1: 89.966    Loss: 0.284

2023-01-06 16:40:30,187 - ==> Confusion:
[[ 224   10  205]
 [  13  209  380]
 [  48   45 5852]]

2023-01-06 16:40:30,188 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 151104 on epoch: 73]
2023-01-06 16:40:30,188 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:30,193 - 

2023-01-06 16:40:30,193 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:30,841 - Epoch: [76][   10/  246]    Overall Loss 0.275861    Objective Loss 0.275861                                        LR 0.000022    Time 0.064665    
2023-01-06 16:40:30,978 - Epoch: [76][   20/  246]    Overall Loss 0.278286    Objective Loss 0.278286                                        LR 0.000022    Time 0.039197    
2023-01-06 16:40:31,118 - Epoch: [76][   30/  246]    Overall Loss 0.276903    Objective Loss 0.276903                                        LR 0.000022    Time 0.030786    
2023-01-06 16:40:31,257 - Epoch: [76][   40/  246]    Overall Loss 0.274086    Objective Loss 0.274086                                        LR 0.000022    Time 0.026537    
2023-01-06 16:40:31,395 - Epoch: [76][   50/  246]    Overall Loss 0.273867    Objective Loss 0.273867                                        LR 0.000022    Time 0.023991    
2023-01-06 16:40:31,538 - Epoch: [76][   60/  246]    Overall Loss 0.274488    Objective Loss 0.274488                                        LR 0.000022    Time 0.022372    
2023-01-06 16:40:31,681 - Epoch: [76][   70/  246]    Overall Loss 0.276375    Objective Loss 0.276375                                        LR 0.000022    Time 0.021207    
2023-01-06 16:40:31,826 - Epoch: [76][   80/  246]    Overall Loss 0.276891    Objective Loss 0.276891                                        LR 0.000022    Time 0.020361    
2023-01-06 16:40:31,967 - Epoch: [76][   90/  246]    Overall Loss 0.277115    Objective Loss 0.277115                                        LR 0.000022    Time 0.019646    
2023-01-06 16:40:32,097 - Epoch: [76][  100/  246]    Overall Loss 0.277149    Objective Loss 0.277149                                        LR 0.000022    Time 0.018980    
2023-01-06 16:40:32,238 - Epoch: [76][  110/  246]    Overall Loss 0.276314    Objective Loss 0.276314                                        LR 0.000022    Time 0.018534    
2023-01-06 16:40:32,379 - Epoch: [76][  120/  246]    Overall Loss 0.277623    Objective Loss 0.277623                                        LR 0.000022    Time 0.018157    
2023-01-06 16:40:32,521 - Epoch: [76][  130/  246]    Overall Loss 0.277934    Objective Loss 0.277934                                        LR 0.000022    Time 0.017834    
2023-01-06 16:40:32,664 - Epoch: [76][  140/  246]    Overall Loss 0.276858    Objective Loss 0.276858                                        LR 0.000022    Time 0.017582    
2023-01-06 16:40:32,806 - Epoch: [76][  150/  246]    Overall Loss 0.275586    Objective Loss 0.275586                                        LR 0.000022    Time 0.017353    
2023-01-06 16:40:32,945 - Epoch: [76][  160/  246]    Overall Loss 0.273607    Objective Loss 0.273607                                        LR 0.000022    Time 0.017136    
2023-01-06 16:40:33,086 - Epoch: [76][  170/  246]    Overall Loss 0.272804    Objective Loss 0.272804                                        LR 0.000022    Time 0.016955    
2023-01-06 16:40:33,256 - Epoch: [76][  180/  246]    Overall Loss 0.272958    Objective Loss 0.272958                                        LR 0.000022    Time 0.016944    
2023-01-06 16:40:33,436 - Epoch: [76][  190/  246]    Overall Loss 0.273245    Objective Loss 0.273245                                        LR 0.000022    Time 0.016988    
2023-01-06 16:40:33,623 - Epoch: [76][  200/  246]    Overall Loss 0.273151    Objective Loss 0.273151                                        LR 0.000022    Time 0.017066    
2023-01-06 16:40:33,806 - Epoch: [76][  210/  246]    Overall Loss 0.273114    Objective Loss 0.273114                                        LR 0.000022    Time 0.017126    
2023-01-06 16:40:33,982 - Epoch: [76][  220/  246]    Overall Loss 0.273872    Objective Loss 0.273872                                        LR 0.000022    Time 0.017145    
2023-01-06 16:40:34,163 - Epoch: [76][  230/  246]    Overall Loss 0.273780    Objective Loss 0.273780                                        LR 0.000022    Time 0.017177    
2023-01-06 16:40:34,355 - Epoch: [76][  240/  246]    Overall Loss 0.273386    Objective Loss 0.273386                                        LR 0.000022    Time 0.017263    
2023-01-06 16:40:34,434 - Epoch: [76][  246/  246]    Overall Loss 0.273528    Objective Loss 0.273528    Top1 89.234450    LR 0.000022    Time 0.017161    
2023-01-06 16:40:34,580 - --- validate (epoch=76)-----------
2023-01-06 16:40:34,580 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:35,016 - Epoch: [76][   10/   28]    Loss 0.284809    Top1 89.531250    
2023-01-06 16:40:35,114 - Epoch: [76][   20/   28]    Loss 0.285464    Top1 89.746094    
2023-01-06 16:40:35,168 - Epoch: [76][   28/   28]    Loss 0.276086    Top1 89.879760    
2023-01-06 16:40:35,303 - ==> Top1: 89.880    Loss: 0.276

2023-01-06 16:40:35,304 - ==> Confusion:
[[ 211   12  216]
 [  12  240  350]
 [  48   69 5828]]

2023-01-06 16:40:35,305 - ==> Best [Top1: 90.166   Sparsity:0.00   Params: 151104 on epoch: 73]
2023-01-06 16:40:35,305 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:35,310 - 

2023-01-06 16:40:35,310 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:35,991 - Epoch: [77][   10/  246]    Overall Loss 0.253431    Objective Loss 0.253431                                        LR 0.000022    Time 0.068020    
2023-01-06 16:40:36,143 - Epoch: [77][   20/  246]    Overall Loss 0.259381    Objective Loss 0.259381                                        LR 0.000022    Time 0.041575    
2023-01-06 16:40:36,285 - Epoch: [77][   30/  246]    Overall Loss 0.269462    Objective Loss 0.269462                                        LR 0.000022    Time 0.032456    
2023-01-06 16:40:36,428 - Epoch: [77][   40/  246]    Overall Loss 0.271776    Objective Loss 0.271776                                        LR 0.000022    Time 0.027902    
2023-01-06 16:40:36,575 - Epoch: [77][   50/  246]    Overall Loss 0.271534    Objective Loss 0.271534                                        LR 0.000022    Time 0.025249    
2023-01-06 16:40:36,722 - Epoch: [77][   60/  246]    Overall Loss 0.272398    Objective Loss 0.272398                                        LR 0.000022    Time 0.023477    
2023-01-06 16:40:36,872 - Epoch: [77][   70/  246]    Overall Loss 0.274458    Objective Loss 0.274458                                        LR 0.000022    Time 0.022262    
2023-01-06 16:40:37,017 - Epoch: [77][   80/  246]    Overall Loss 0.272415    Objective Loss 0.272415                                        LR 0.000022    Time 0.021294    
2023-01-06 16:40:37,163 - Epoch: [77][   90/  246]    Overall Loss 0.272963    Objective Loss 0.272963                                        LR 0.000022    Time 0.020543    
2023-01-06 16:40:37,309 - Epoch: [77][  100/  246]    Overall Loss 0.273250    Objective Loss 0.273250                                        LR 0.000022    Time 0.019945    
2023-01-06 16:40:37,454 - Epoch: [77][  110/  246]    Overall Loss 0.272708    Objective Loss 0.272708                                        LR 0.000022    Time 0.019443    
2023-01-06 16:40:37,603 - Epoch: [77][  120/  246]    Overall Loss 0.273125    Objective Loss 0.273125                                        LR 0.000022    Time 0.019058    
2023-01-06 16:40:37,747 - Epoch: [77][  130/  246]    Overall Loss 0.272640    Objective Loss 0.272640                                        LR 0.000022    Time 0.018700    
2023-01-06 16:40:37,894 - Epoch: [77][  140/  246]    Overall Loss 0.271116    Objective Loss 0.271116                                        LR 0.000022    Time 0.018413    
2023-01-06 16:40:38,039 - Epoch: [77][  150/  246]    Overall Loss 0.270804    Objective Loss 0.270804                                        LR 0.000022    Time 0.018148    
2023-01-06 16:40:38,185 - Epoch: [77][  160/  246]    Overall Loss 0.271288    Objective Loss 0.271288                                        LR 0.000022    Time 0.017924    
2023-01-06 16:40:38,332 - Epoch: [77][  170/  246]    Overall Loss 0.271191    Objective Loss 0.271191                                        LR 0.000022    Time 0.017727    
2023-01-06 16:40:38,478 - Epoch: [77][  180/  246]    Overall Loss 0.270890    Objective Loss 0.270890                                        LR 0.000022    Time 0.017555    
2023-01-06 16:40:38,611 - Epoch: [77][  190/  246]    Overall Loss 0.269953    Objective Loss 0.269953                                        LR 0.000022    Time 0.017327    
2023-01-06 16:40:38,734 - Epoch: [77][  200/  246]    Overall Loss 0.269311    Objective Loss 0.269311                                        LR 0.000022    Time 0.017071    
2023-01-06 16:40:38,857 - Epoch: [77][  210/  246]    Overall Loss 0.270022    Objective Loss 0.270022                                        LR 0.000022    Time 0.016842    
2023-01-06 16:40:38,980 - Epoch: [77][  220/  246]    Overall Loss 0.270022    Objective Loss 0.270022                                        LR 0.000022    Time 0.016634    
2023-01-06 16:40:39,102 - Epoch: [77][  230/  246]    Overall Loss 0.270084    Objective Loss 0.270084                                        LR 0.000022    Time 0.016442    
2023-01-06 16:40:39,244 - Epoch: [77][  240/  246]    Overall Loss 0.270866    Objective Loss 0.270866                                        LR 0.000022    Time 0.016346    
2023-01-06 16:40:39,307 - Epoch: [77][  246/  246]    Overall Loss 0.271077    Objective Loss 0.271077    Top1 87.320574    LR 0.000022    Time 0.016203    
2023-01-06 16:40:39,454 - --- validate (epoch=77)-----------
2023-01-06 16:40:39,454 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:39,869 - Epoch: [77][   10/   28]    Loss 0.273685    Top1 90.468750    
2023-01-06 16:40:39,963 - Epoch: [77][   20/   28]    Loss 0.273085    Top1 90.468750    
2023-01-06 16:40:40,013 - Epoch: [77][   28/   28]    Loss 0.277448    Top1 90.251932    
2023-01-06 16:40:40,139 - ==> Top1: 90.252    Loss: 0.277

2023-01-06 16:40:40,140 - ==> Confusion:
[[ 217   14  208]
 [  12  235  355]
 [  46   46 5853]]

2023-01-06 16:40:40,141 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:40:40,141 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:40,147 - 

2023-01-06 16:40:40,147 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:40,686 - Epoch: [78][   10/  246]    Overall Loss 0.264931    Objective Loss 0.264931                                        LR 0.000022    Time 0.053841    
2023-01-06 16:40:40,835 - Epoch: [78][   20/  246]    Overall Loss 0.266337    Objective Loss 0.266337                                        LR 0.000022    Time 0.034325    
2023-01-06 16:40:40,981 - Epoch: [78][   30/  246]    Overall Loss 0.266542    Objective Loss 0.266542                                        LR 0.000022    Time 0.027748    
2023-01-06 16:40:41,129 - Epoch: [78][   40/  246]    Overall Loss 0.264257    Objective Loss 0.264257                                        LR 0.000022    Time 0.024501    
2023-01-06 16:40:41,278 - Epoch: [78][   50/  246]    Overall Loss 0.263139    Objective Loss 0.263139                                        LR 0.000022    Time 0.022572    
2023-01-06 16:40:41,424 - Epoch: [78][   60/  246]    Overall Loss 0.264445    Objective Loss 0.264445                                        LR 0.000022    Time 0.021234    
2023-01-06 16:40:41,579 - Epoch: [78][   70/  246]    Overall Loss 0.263584    Objective Loss 0.263584                                        LR 0.000022    Time 0.020411    
2023-01-06 16:40:41,735 - Epoch: [78][   80/  246]    Overall Loss 0.263955    Objective Loss 0.263955                                        LR 0.000022    Time 0.019793    
2023-01-06 16:40:41,889 - Epoch: [78][   90/  246]    Overall Loss 0.265439    Objective Loss 0.265439                                        LR 0.000022    Time 0.019304    
2023-01-06 16:40:42,050 - Epoch: [78][  100/  246]    Overall Loss 0.266722    Objective Loss 0.266722                                        LR 0.000022    Time 0.018982    
2023-01-06 16:40:42,211 - Epoch: [78][  110/  246]    Overall Loss 0.267494    Objective Loss 0.267494                                        LR 0.000022    Time 0.018710    
2023-01-06 16:40:42,371 - Epoch: [78][  120/  246]    Overall Loss 0.268671    Objective Loss 0.268671                                        LR 0.000022    Time 0.018481    
2023-01-06 16:40:42,532 - Epoch: [78][  130/  246]    Overall Loss 0.270989    Objective Loss 0.270989                                        LR 0.000022    Time 0.018300    
2023-01-06 16:40:42,693 - Epoch: [78][  140/  246]    Overall Loss 0.272899    Objective Loss 0.272899                                        LR 0.000022    Time 0.018140    
2023-01-06 16:40:42,854 - Epoch: [78][  150/  246]    Overall Loss 0.271335    Objective Loss 0.271335                                        LR 0.000022    Time 0.017999    
2023-01-06 16:40:43,014 - Epoch: [78][  160/  246]    Overall Loss 0.270363    Objective Loss 0.270363                                        LR 0.000022    Time 0.017875    
2023-01-06 16:40:43,174 - Epoch: [78][  170/  246]    Overall Loss 0.269420    Objective Loss 0.269420                                        LR 0.000022    Time 0.017762    
2023-01-06 16:40:43,335 - Epoch: [78][  180/  246]    Overall Loss 0.269159    Objective Loss 0.269159                                        LR 0.000022    Time 0.017668    
2023-01-06 16:40:43,496 - Epoch: [78][  190/  246]    Overall Loss 0.268337    Objective Loss 0.268337                                        LR 0.000022    Time 0.017579    
2023-01-06 16:40:43,656 - Epoch: [78][  200/  246]    Overall Loss 0.268938    Objective Loss 0.268938                                        LR 0.000022    Time 0.017499    
2023-01-06 16:40:43,818 - Epoch: [78][  210/  246]    Overall Loss 0.269995    Objective Loss 0.269995                                        LR 0.000022    Time 0.017434    
2023-01-06 16:40:43,966 - Epoch: [78][  220/  246]    Overall Loss 0.270308    Objective Loss 0.270308                                        LR 0.000022    Time 0.017315    
2023-01-06 16:40:44,114 - Epoch: [78][  230/  246]    Overall Loss 0.270152    Objective Loss 0.270152                                        LR 0.000022    Time 0.017204    
2023-01-06 16:40:44,271 - Epoch: [78][  240/  246]    Overall Loss 0.270751    Objective Loss 0.270751                                        LR 0.000022    Time 0.017140    
2023-01-06 16:40:44,331 - Epoch: [78][  246/  246]    Overall Loss 0.270907    Objective Loss 0.270907    Top1 89.952153    LR 0.000022    Time 0.016963    
2023-01-06 16:40:44,465 - --- validate (epoch=78)-----------
2023-01-06 16:40:44,465 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:44,890 - Epoch: [78][   10/   28]    Loss 0.270201    Top1 90.312500    
2023-01-06 16:40:44,989 - Epoch: [78][   20/   28]    Loss 0.271775    Top1 90.156250    
2023-01-06 16:40:45,042 - Epoch: [78][   28/   28]    Loss 0.277175    Top1 89.965646    
2023-01-06 16:40:45,206 - ==> Top1: 89.966    Loss: 0.277

2023-01-06 16:40:45,206 - ==> Confusion:
[[ 212   15  212]
 [  13  238  351]
 [  45   65 5835]]

2023-01-06 16:40:45,207 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:40:45,208 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:45,213 - 

2023-01-06 16:40:45,213 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:45,891 - Epoch: [79][   10/  246]    Overall Loss 0.254537    Objective Loss 0.254537                                        LR 0.000022    Time 0.067751    
2023-01-06 16:40:46,051 - Epoch: [79][   20/  246]    Overall Loss 0.263167    Objective Loss 0.263167                                        LR 0.000022    Time 0.041815    
2023-01-06 16:40:46,204 - Epoch: [79][   30/  246]    Overall Loss 0.267520    Objective Loss 0.267520                                        LR 0.000022    Time 0.032972    
2023-01-06 16:40:46,358 - Epoch: [79][   40/  246]    Overall Loss 0.265827    Objective Loss 0.265827                                        LR 0.000022    Time 0.028559    
2023-01-06 16:40:46,511 - Epoch: [79][   50/  246]    Overall Loss 0.269736    Objective Loss 0.269736                                        LR 0.000022    Time 0.025901    
2023-01-06 16:40:46,658 - Epoch: [79][   60/  246]    Overall Loss 0.266103    Objective Loss 0.266103                                        LR 0.000022    Time 0.024026    
2023-01-06 16:40:46,814 - Epoch: [79][   70/  246]    Overall Loss 0.269315    Objective Loss 0.269315                                        LR 0.000022    Time 0.022821    
2023-01-06 16:40:46,968 - Epoch: [79][   80/  246]    Overall Loss 0.272795    Objective Loss 0.272795                                        LR 0.000022    Time 0.021886    
2023-01-06 16:40:47,121 - Epoch: [79][   90/  246]    Overall Loss 0.271192    Objective Loss 0.271192                                        LR 0.000022    Time 0.021151    
2023-01-06 16:40:47,268 - Epoch: [79][  100/  246]    Overall Loss 0.269302    Objective Loss 0.269302                                        LR 0.000022    Time 0.020493    
2023-01-06 16:40:47,413 - Epoch: [79][  110/  246]    Overall Loss 0.269147    Objective Loss 0.269147                                        LR 0.000022    Time 0.019952    
2023-01-06 16:40:47,553 - Epoch: [79][  120/  246]    Overall Loss 0.270792    Objective Loss 0.270792                                        LR 0.000022    Time 0.019447    
2023-01-06 16:40:47,696 - Epoch: [79][  130/  246]    Overall Loss 0.270721    Objective Loss 0.270721                                        LR 0.000022    Time 0.019046    
2023-01-06 16:40:47,836 - Epoch: [79][  140/  246]    Overall Loss 0.270022    Objective Loss 0.270022                                        LR 0.000022    Time 0.018682    
2023-01-06 16:40:47,967 - Epoch: [79][  150/  246]    Overall Loss 0.270956    Objective Loss 0.270956                                        LR 0.000022    Time 0.018307    
2023-01-06 16:40:48,095 - Epoch: [79][  160/  246]    Overall Loss 0.270708    Objective Loss 0.270708                                        LR 0.000022    Time 0.017962    
2023-01-06 16:40:48,233 - Epoch: [79][  170/  246]    Overall Loss 0.270276    Objective Loss 0.270276                                        LR 0.000022    Time 0.017715    
2023-01-06 16:40:48,361 - Epoch: [79][  180/  246]    Overall Loss 0.270117    Objective Loss 0.270117                                        LR 0.000022    Time 0.017439    
2023-01-06 16:40:48,498 - Epoch: [79][  190/  246]    Overall Loss 0.270154    Objective Loss 0.270154                                        LR 0.000022    Time 0.017240    
2023-01-06 16:40:48,639 - Epoch: [79][  200/  246]    Overall Loss 0.270591    Objective Loss 0.270591                                        LR 0.000022    Time 0.017079    
2023-01-06 16:40:48,794 - Epoch: [79][  210/  246]    Overall Loss 0.270942    Objective Loss 0.270942                                        LR 0.000022    Time 0.017004    
2023-01-06 16:40:48,937 - Epoch: [79][  220/  246]    Overall Loss 0.271179    Objective Loss 0.271179                                        LR 0.000022    Time 0.016872    
2023-01-06 16:40:49,091 - Epoch: [79][  230/  246]    Overall Loss 0.270981    Objective Loss 0.270981                                        LR 0.000022    Time 0.016806    
2023-01-06 16:40:49,231 - Epoch: [79][  240/  246]    Overall Loss 0.270367    Objective Loss 0.270367                                        LR 0.000022    Time 0.016688    
2023-01-06 16:40:49,292 - Epoch: [79][  246/  246]    Overall Loss 0.270469    Objective Loss 0.270469    Top1 92.583732    LR 0.000022    Time 0.016527    
2023-01-06 16:40:49,439 - --- validate (epoch=79)-----------
2023-01-06 16:40:49,439 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:49,862 - Epoch: [79][   10/   28]    Loss 0.291573    Top1 89.414062    
2023-01-06 16:40:49,958 - Epoch: [79][   20/   28]    Loss 0.290841    Top1 89.492188    
2023-01-06 16:40:50,008 - Epoch: [79][   28/   28]    Loss 0.278441    Top1 90.022903    
2023-01-06 16:40:50,166 - ==> Top1: 90.023    Loss: 0.278

2023-01-06 16:40:50,167 - ==> Confusion:
[[ 221   22  196]
 [  11  268  323]
 [  50   95 5800]]

2023-01-06 16:40:50,168 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:40:50,168 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:50,173 - 

2023-01-06 16:40:50,173 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:50,684 - Epoch: [80][   10/  246]    Overall Loss 0.257915    Objective Loss 0.257915                                        LR 0.000022    Time 0.051084    
2023-01-06 16:40:50,816 - Epoch: [80][   20/  246]    Overall Loss 0.268801    Objective Loss 0.268801                                        LR 0.000022    Time 0.032109    
2023-01-06 16:40:50,947 - Epoch: [80][   30/  246]    Overall Loss 0.271312    Objective Loss 0.271312                                        LR 0.000022    Time 0.025744    
2023-01-06 16:40:51,089 - Epoch: [80][   40/  246]    Overall Loss 0.272508    Objective Loss 0.272508                                        LR 0.000022    Time 0.022842    
2023-01-06 16:40:51,227 - Epoch: [80][   50/  246]    Overall Loss 0.276235    Objective Loss 0.276235                                        LR 0.000022    Time 0.021037    
2023-01-06 16:40:51,376 - Epoch: [80][   60/  246]    Overall Loss 0.276267    Objective Loss 0.276267                                        LR 0.000022    Time 0.019982    
2023-01-06 16:40:51,520 - Epoch: [80][   70/  246]    Overall Loss 0.275188    Objective Loss 0.275188                                        LR 0.000022    Time 0.019179    
2023-01-06 16:40:51,668 - Epoch: [80][   80/  246]    Overall Loss 0.274084    Objective Loss 0.274084                                        LR 0.000022    Time 0.018624    
2023-01-06 16:40:51,808 - Epoch: [80][   90/  246]    Overall Loss 0.273250    Objective Loss 0.273250                                        LR 0.000022    Time 0.018102    
2023-01-06 16:40:51,934 - Epoch: [80][  100/  246]    Overall Loss 0.272676    Objective Loss 0.272676                                        LR 0.000022    Time 0.017553    
2023-01-06 16:40:52,065 - Epoch: [80][  110/  246]    Overall Loss 0.273117    Objective Loss 0.273117                                        LR 0.000022    Time 0.017142    
2023-01-06 16:40:52,192 - Epoch: [80][  120/  246]    Overall Loss 0.273091    Objective Loss 0.273091                                        LR 0.000022    Time 0.016770    
2023-01-06 16:40:52,321 - Epoch: [80][  130/  246]    Overall Loss 0.272404    Objective Loss 0.272404                                        LR 0.000022    Time 0.016467    
2023-01-06 16:40:52,451 - Epoch: [80][  140/  246]    Overall Loss 0.271430    Objective Loss 0.271430                                        LR 0.000022    Time 0.016218    
2023-01-06 16:40:52,581 - Epoch: [80][  150/  246]    Overall Loss 0.270793    Objective Loss 0.270793                                        LR 0.000022    Time 0.016001    
2023-01-06 16:40:52,709 - Epoch: [80][  160/  246]    Overall Loss 0.270543    Objective Loss 0.270543                                        LR 0.000022    Time 0.015795    
2023-01-06 16:40:52,836 - Epoch: [80][  170/  246]    Overall Loss 0.270739    Objective Loss 0.270739                                        LR 0.000022    Time 0.015611    
2023-01-06 16:40:52,985 - Epoch: [80][  180/  246]    Overall Loss 0.272254    Objective Loss 0.272254                                        LR 0.000022    Time 0.015570    
2023-01-06 16:40:53,126 - Epoch: [80][  190/  246]    Overall Loss 0.271676    Objective Loss 0.271676                                        LR 0.000022    Time 0.015491    
2023-01-06 16:40:53,264 - Epoch: [80][  200/  246]    Overall Loss 0.271255    Objective Loss 0.271255                                        LR 0.000022    Time 0.015403    
2023-01-06 16:40:53,398 - Epoch: [80][  210/  246]    Overall Loss 0.271485    Objective Loss 0.271485                                        LR 0.000022    Time 0.015304    
2023-01-06 16:40:53,534 - Epoch: [80][  220/  246]    Overall Loss 0.271003    Objective Loss 0.271003                                        LR 0.000022    Time 0.015227    
2023-01-06 16:40:53,673 - Epoch: [80][  230/  246]    Overall Loss 0.270149    Objective Loss 0.270149                                        LR 0.000022    Time 0.015165    
2023-01-06 16:40:53,819 - Epoch: [80][  240/  246]    Overall Loss 0.269694    Objective Loss 0.269694                                        LR 0.000022    Time 0.015143    
2023-01-06 16:40:53,878 - Epoch: [80][  246/  246]    Overall Loss 0.270215    Objective Loss 0.270215    Top1 88.755981    LR 0.000022    Time 0.015011    
2023-01-06 16:40:54,011 - --- validate (epoch=80)-----------
2023-01-06 16:40:54,011 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:54,570 - Epoch: [80][   10/   28]    Loss 0.284370    Top1 89.375000    
2023-01-06 16:40:54,675 - Epoch: [80][   20/   28]    Loss 0.280271    Top1 89.746094    
2023-01-06 16:40:54,725 - Epoch: [80][   28/   28]    Loss 0.275136    Top1 89.994274    
2023-01-06 16:40:54,851 - ==> Top1: 89.994    Loss: 0.275

2023-01-06 16:40:54,851 - ==> Confusion:
[[ 221   18  200]
 [  12  251  339]
 [  51   79 5815]]

2023-01-06 16:40:54,853 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:40:54,853 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:54,858 - 

2023-01-06 16:40:54,858 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:40:55,388 - Epoch: [81][   10/  246]    Overall Loss 0.263987    Objective Loss 0.263987                                        LR 0.000022    Time 0.052891    
2023-01-06 16:40:55,519 - Epoch: [81][   20/  246]    Overall Loss 0.268112    Objective Loss 0.268112                                        LR 0.000022    Time 0.032973    
2023-01-06 16:40:55,655 - Epoch: [81][   30/  246]    Overall Loss 0.269329    Objective Loss 0.269329                                        LR 0.000022    Time 0.026493    
2023-01-06 16:40:55,783 - Epoch: [81][   40/  246]    Overall Loss 0.267153    Objective Loss 0.267153                                        LR 0.000022    Time 0.023061    
2023-01-06 16:40:55,919 - Epoch: [81][   50/  246]    Overall Loss 0.268621    Objective Loss 0.268621                                        LR 0.000022    Time 0.021176    
2023-01-06 16:40:56,057 - Epoch: [81][   60/  246]    Overall Loss 0.270399    Objective Loss 0.270399                                        LR 0.000022    Time 0.019934    
2023-01-06 16:40:56,191 - Epoch: [81][   70/  246]    Overall Loss 0.272712    Objective Loss 0.272712                                        LR 0.000022    Time 0.018996    
2023-01-06 16:40:56,343 - Epoch: [81][   80/  246]    Overall Loss 0.271333    Objective Loss 0.271333                                        LR 0.000022    Time 0.018486    
2023-01-06 16:40:56,488 - Epoch: [81][   90/  246]    Overall Loss 0.272213    Objective Loss 0.272213                                        LR 0.000022    Time 0.018035    
2023-01-06 16:40:56,637 - Epoch: [81][  100/  246]    Overall Loss 0.270593    Objective Loss 0.270593                                        LR 0.000022    Time 0.017727    
2023-01-06 16:40:56,791 - Epoch: [81][  110/  246]    Overall Loss 0.269138    Objective Loss 0.269138                                        LR 0.000022    Time 0.017512    
2023-01-06 16:40:56,942 - Epoch: [81][  120/  246]    Overall Loss 0.270032    Objective Loss 0.270032                                        LR 0.000022    Time 0.017300    
2023-01-06 16:40:57,092 - Epoch: [81][  130/  246]    Overall Loss 0.271608    Objective Loss 0.271608                                        LR 0.000022    Time 0.017126    
2023-01-06 16:40:57,242 - Epoch: [81][  140/  246]    Overall Loss 0.272692    Objective Loss 0.272692                                        LR 0.000022    Time 0.016967    
2023-01-06 16:40:57,392 - Epoch: [81][  150/  246]    Overall Loss 0.272690    Objective Loss 0.272690                                        LR 0.000022    Time 0.016837    
2023-01-06 16:40:57,542 - Epoch: [81][  160/  246]    Overall Loss 0.273223    Objective Loss 0.273223                                        LR 0.000022    Time 0.016717    
2023-01-06 16:40:57,694 - Epoch: [81][  170/  246]    Overall Loss 0.272319    Objective Loss 0.272319                                        LR 0.000022    Time 0.016625    
2023-01-06 16:40:57,839 - Epoch: [81][  180/  246]    Overall Loss 0.272174    Objective Loss 0.272174                                        LR 0.000022    Time 0.016509    
2023-01-06 16:40:57,986 - Epoch: [81][  190/  246]    Overall Loss 0.273005    Objective Loss 0.273005                                        LR 0.000022    Time 0.016410    
2023-01-06 16:40:58,110 - Epoch: [81][  200/  246]    Overall Loss 0.272206    Objective Loss 0.272206                                        LR 0.000022    Time 0.016200    
2023-01-06 16:40:58,252 - Epoch: [81][  210/  246]    Overall Loss 0.271083    Objective Loss 0.271083                                        LR 0.000022    Time 0.016106    
2023-01-06 16:40:58,404 - Epoch: [81][  220/  246]    Overall Loss 0.271976    Objective Loss 0.271976                                        LR 0.000022    Time 0.016060    
2023-01-06 16:40:58,556 - Epoch: [81][  230/  246]    Overall Loss 0.271691    Objective Loss 0.271691                                        LR 0.000022    Time 0.016024    
2023-01-06 16:40:58,718 - Epoch: [81][  240/  246]    Overall Loss 0.271531    Objective Loss 0.271531                                        LR 0.000022    Time 0.016026    
2023-01-06 16:40:58,783 - Epoch: [81][  246/  246]    Overall Loss 0.271080    Objective Loss 0.271080    Top1 90.430622    LR 0.000022    Time 0.015899    
2023-01-06 16:40:58,928 - --- validate (epoch=81)-----------
2023-01-06 16:40:58,928 - 6986 samples (256 per mini-batch)
2023-01-06 16:40:59,359 - Epoch: [81][   10/   28]    Loss 0.298445    Top1 89.687500    
2023-01-06 16:40:59,460 - Epoch: [81][   20/   28]    Loss 0.289344    Top1 89.687500    
2023-01-06 16:40:59,509 - Epoch: [81][   28/   28]    Loss 0.279258    Top1 89.836816    
2023-01-06 16:40:59,660 - ==> Top1: 89.837    Loss: 0.279

2023-01-06 16:40:59,660 - ==> Confusion:
[[ 210   10  219]
 [  16  228  358]
 [  48   59 5838]]

2023-01-06 16:40:59,661 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:40:59,661 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:40:59,666 - 

2023-01-06 16:40:59,666 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:00,332 - Epoch: [82][   10/  246]    Overall Loss 0.273357    Objective Loss 0.273357                                        LR 0.000022    Time 0.066479    
2023-01-06 16:41:00,470 - Epoch: [82][   20/  246]    Overall Loss 0.273629    Objective Loss 0.273629                                        LR 0.000022    Time 0.040157    
2023-01-06 16:41:00,611 - Epoch: [82][   30/  246]    Overall Loss 0.275922    Objective Loss 0.275922                                        LR 0.000022    Time 0.031459    
2023-01-06 16:41:00,757 - Epoch: [82][   40/  246]    Overall Loss 0.278287    Objective Loss 0.278287                                        LR 0.000022    Time 0.027224    
2023-01-06 16:41:00,910 - Epoch: [82][   50/  246]    Overall Loss 0.281528    Objective Loss 0.281528                                        LR 0.000022    Time 0.024834    
2023-01-06 16:41:01,063 - Epoch: [82][   60/  246]    Overall Loss 0.278949    Objective Loss 0.278949                                        LR 0.000022    Time 0.023242    
2023-01-06 16:41:01,213 - Epoch: [82][   70/  246]    Overall Loss 0.277745    Objective Loss 0.277745                                        LR 0.000022    Time 0.022047    
2023-01-06 16:41:01,369 - Epoch: [82][   80/  246]    Overall Loss 0.276255    Objective Loss 0.276255                                        LR 0.000022    Time 0.021238    
2023-01-06 16:41:01,515 - Epoch: [82][   90/  246]    Overall Loss 0.274480    Objective Loss 0.274480                                        LR 0.000022    Time 0.020491    
2023-01-06 16:41:01,662 - Epoch: [82][  100/  246]    Overall Loss 0.273294    Objective Loss 0.273294                                        LR 0.000022    Time 0.019908    
2023-01-06 16:41:01,830 - Epoch: [82][  110/  246]    Overall Loss 0.273045    Objective Loss 0.273045                                        LR 0.000022    Time 0.019621    
2023-01-06 16:41:01,983 - Epoch: [82][  120/  246]    Overall Loss 0.273456    Objective Loss 0.273456                                        LR 0.000022    Time 0.019263    
2023-01-06 16:41:02,129 - Epoch: [82][  130/  246]    Overall Loss 0.273257    Objective Loss 0.273257                                        LR 0.000022    Time 0.018899    
2023-01-06 16:41:02,275 - Epoch: [82][  140/  246]    Overall Loss 0.273514    Objective Loss 0.273514                                        LR 0.000022    Time 0.018591    
2023-01-06 16:41:02,438 - Epoch: [82][  150/  246]    Overall Loss 0.272792    Objective Loss 0.272792                                        LR 0.000022    Time 0.018436    
2023-01-06 16:41:02,590 - Epoch: [82][  160/  246]    Overall Loss 0.271109    Objective Loss 0.271109                                        LR 0.000022    Time 0.018228    
2023-01-06 16:41:02,733 - Epoch: [82][  170/  246]    Overall Loss 0.270210    Objective Loss 0.270210                                        LR 0.000022    Time 0.017995    
2023-01-06 16:41:02,873 - Epoch: [82][  180/  246]    Overall Loss 0.269516    Objective Loss 0.269516                                        LR 0.000022    Time 0.017770    
2023-01-06 16:41:03,010 - Epoch: [82][  190/  246]    Overall Loss 0.270395    Objective Loss 0.270395                                        LR 0.000022    Time 0.017557    
2023-01-06 16:41:03,151 - Epoch: [82][  200/  246]    Overall Loss 0.270876    Objective Loss 0.270876                                        LR 0.000022    Time 0.017378    
2023-01-06 16:41:03,290 - Epoch: [82][  210/  246]    Overall Loss 0.270541    Objective Loss 0.270541                                        LR 0.000022    Time 0.017213    
2023-01-06 16:41:03,435 - Epoch: [82][  220/  246]    Overall Loss 0.269781    Objective Loss 0.269781                                        LR 0.000022    Time 0.017086    
2023-01-06 16:41:03,581 - Epoch: [82][  230/  246]    Overall Loss 0.269728    Objective Loss 0.269728                                        LR 0.000022    Time 0.016977    
2023-01-06 16:41:03,738 - Epoch: [82][  240/  246]    Overall Loss 0.270080    Objective Loss 0.270080                                        LR 0.000022    Time 0.016925    
2023-01-06 16:41:03,803 - Epoch: [82][  246/  246]    Overall Loss 0.269786    Objective Loss 0.269786    Top1 92.822967    LR 0.000022    Time 0.016775    
2023-01-06 16:41:03,948 - --- validate (epoch=82)-----------
2023-01-06 16:41:03,948 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:04,364 - Epoch: [82][   10/   28]    Loss 0.281988    Top1 90.273438    
2023-01-06 16:41:04,459 - Epoch: [82][   20/   28]    Loss 0.279814    Top1 90.078125    
2023-01-06 16:41:04,508 - Epoch: [82][   28/   28]    Loss 0.279176    Top1 90.151732    
2023-01-06 16:41:04,664 - ==> Top1: 90.152    Loss: 0.279

2023-01-06 16:41:04,664 - ==> Confusion:
[[ 214   17  208]
 [  14  254  334]
 [  51   64 5830]]

2023-01-06 16:41:04,665 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:04,665 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:04,670 - 

2023-01-06 16:41:04,670 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:05,201 - Epoch: [83][   10/  246]    Overall Loss 0.250013    Objective Loss 0.250013                                        LR 0.000022    Time 0.053042    
2023-01-06 16:41:05,345 - Epoch: [83][   20/  246]    Overall Loss 0.252987    Objective Loss 0.252987                                        LR 0.000022    Time 0.033675    
2023-01-06 16:41:05,481 - Epoch: [83][   30/  246]    Overall Loss 0.255888    Objective Loss 0.255888                                        LR 0.000022    Time 0.026978    
2023-01-06 16:41:05,616 - Epoch: [83][   40/  246]    Overall Loss 0.264461    Objective Loss 0.264461                                        LR 0.000022    Time 0.023606    
2023-01-06 16:41:05,756 - Epoch: [83][   50/  246]    Overall Loss 0.267067    Objective Loss 0.267067                                        LR 0.000022    Time 0.021670    
2023-01-06 16:41:05,894 - Epoch: [83][   60/  246]    Overall Loss 0.269879    Objective Loss 0.269879                                        LR 0.000022    Time 0.020356    
2023-01-06 16:41:06,032 - Epoch: [83][   70/  246]    Overall Loss 0.267629    Objective Loss 0.267629                                        LR 0.000022    Time 0.019400    
2023-01-06 16:41:06,172 - Epoch: [83][   80/  246]    Overall Loss 0.267869    Objective Loss 0.267869                                        LR 0.000022    Time 0.018719    
2023-01-06 16:41:06,319 - Epoch: [83][   90/  246]    Overall Loss 0.268671    Objective Loss 0.268671                                        LR 0.000022    Time 0.018268    
2023-01-06 16:41:06,462 - Epoch: [83][  100/  246]    Overall Loss 0.271861    Objective Loss 0.271861                                        LR 0.000022    Time 0.017868    
2023-01-06 16:41:06,608 - Epoch: [83][  110/  246]    Overall Loss 0.271190    Objective Loss 0.271190                                        LR 0.000022    Time 0.017567    
2023-01-06 16:41:06,749 - Epoch: [83][  120/  246]    Overall Loss 0.271134    Objective Loss 0.271134                                        LR 0.000022    Time 0.017276    
2023-01-06 16:41:06,894 - Epoch: [83][  130/  246]    Overall Loss 0.270826    Objective Loss 0.270826                                        LR 0.000022    Time 0.017052    
2023-01-06 16:41:07,035 - Epoch: [83][  140/  246]    Overall Loss 0.269414    Objective Loss 0.269414                                        LR 0.000022    Time 0.016835    
2023-01-06 16:41:07,179 - Epoch: [83][  150/  246]    Overall Loss 0.268751    Objective Loss 0.268751                                        LR 0.000022    Time 0.016670    
2023-01-06 16:41:07,317 - Epoch: [83][  160/  246]    Overall Loss 0.269103    Objective Loss 0.269103                                        LR 0.000022    Time 0.016491    
2023-01-06 16:41:07,462 - Epoch: [83][  170/  246]    Overall Loss 0.268396    Objective Loss 0.268396                                        LR 0.000022    Time 0.016362    
2023-01-06 16:41:07,602 - Epoch: [83][  180/  246]    Overall Loss 0.267676    Objective Loss 0.267676                                        LR 0.000022    Time 0.016233    
2023-01-06 16:41:07,746 - Epoch: [83][  190/  246]    Overall Loss 0.269101    Objective Loss 0.269101                                        LR 0.000022    Time 0.016133    
2023-01-06 16:41:07,885 - Epoch: [83][  200/  246]    Overall Loss 0.268713    Objective Loss 0.268713                                        LR 0.000022    Time 0.016019    
2023-01-06 16:41:08,020 - Epoch: [83][  210/  246]    Overall Loss 0.268759    Objective Loss 0.268759                                        LR 0.000022    Time 0.015895    
2023-01-06 16:41:08,157 - Epoch: [83][  220/  246]    Overall Loss 0.269757    Objective Loss 0.269757                                        LR 0.000022    Time 0.015796    
2023-01-06 16:41:08,294 - Epoch: [83][  230/  246]    Overall Loss 0.269765    Objective Loss 0.269765                                        LR 0.000022    Time 0.015704    
2023-01-06 16:41:08,447 - Epoch: [83][  240/  246]    Overall Loss 0.269253    Objective Loss 0.269253                                        LR 0.000022    Time 0.015685    
2023-01-06 16:41:08,514 - Epoch: [83][  246/  246]    Overall Loss 0.269522    Objective Loss 0.269522    Top1 89.234450    LR 0.000022    Time 0.015574    
2023-01-06 16:41:08,644 - --- validate (epoch=83)-----------
2023-01-06 16:41:08,644 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:09,054 - Epoch: [83][   10/   28]    Loss 0.277222    Top1 90.117188    
2023-01-06 16:41:09,143 - Epoch: [83][   20/   28]    Loss 0.278099    Top1 90.000000    
2023-01-06 16:41:09,193 - Epoch: [83][   28/   28]    Loss 0.279904    Top1 89.979960    
2023-01-06 16:41:09,316 - ==> Top1: 89.980    Loss: 0.280

2023-01-06 16:41:09,316 - ==> Confusion:
[[ 234   17  188]
 [  13  261  328]
 [  66   88 5791]]

2023-01-06 16:41:09,317 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:09,318 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:09,323 - 

2023-01-06 16:41:09,323 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:09,978 - Epoch: [84][   10/  246]    Overall Loss 0.262463    Objective Loss 0.262463                                        LR 0.000022    Time 0.065507    
2023-01-06 16:41:10,119 - Epoch: [84][   20/  246]    Overall Loss 0.259448    Objective Loss 0.259448                                        LR 0.000022    Time 0.039783    
2023-01-06 16:41:10,254 - Epoch: [84][   30/  246]    Overall Loss 0.254488    Objective Loss 0.254488                                        LR 0.000022    Time 0.030993    
2023-01-06 16:41:10,399 - Epoch: [84][   40/  246]    Overall Loss 0.261653    Objective Loss 0.261653                                        LR 0.000022    Time 0.026859    
2023-01-06 16:41:10,541 - Epoch: [84][   50/  246]    Overall Loss 0.266245    Objective Loss 0.266245                                        LR 0.000022    Time 0.024316    
2023-01-06 16:41:10,684 - Epoch: [84][   60/  246]    Overall Loss 0.266190    Objective Loss 0.266190                                        LR 0.000022    Time 0.022649    
2023-01-06 16:41:10,825 - Epoch: [84][   70/  246]    Overall Loss 0.268540    Objective Loss 0.268540                                        LR 0.000022    Time 0.021416    
2023-01-06 16:41:10,968 - Epoch: [84][   80/  246]    Overall Loss 0.269884    Objective Loss 0.269884                                        LR 0.000022    Time 0.020526    
2023-01-06 16:41:11,109 - Epoch: [84][   90/  246]    Overall Loss 0.270527    Objective Loss 0.270527                                        LR 0.000022    Time 0.019801    
2023-01-06 16:41:11,252 - Epoch: [84][  100/  246]    Overall Loss 0.269900    Objective Loss 0.269900                                        LR 0.000022    Time 0.019249    
2023-01-06 16:41:11,400 - Epoch: [84][  110/  246]    Overall Loss 0.270118    Objective Loss 0.270118                                        LR 0.000022    Time 0.018840    
2023-01-06 16:41:11,549 - Epoch: [84][  120/  246]    Overall Loss 0.271209    Objective Loss 0.271209                                        LR 0.000022    Time 0.018509    
2023-01-06 16:41:11,700 - Epoch: [84][  130/  246]    Overall Loss 0.269436    Objective Loss 0.269436                                        LR 0.000022    Time 0.018245    
2023-01-06 16:41:11,849 - Epoch: [84][  140/  246]    Overall Loss 0.268655    Objective Loss 0.268655                                        LR 0.000022    Time 0.018000    
2023-01-06 16:41:11,993 - Epoch: [84][  150/  246]    Overall Loss 0.267791    Objective Loss 0.267791                                        LR 0.000022    Time 0.017754    
2023-01-06 16:41:12,134 - Epoch: [84][  160/  246]    Overall Loss 0.267275    Objective Loss 0.267275                                        LR 0.000022    Time 0.017525    
2023-01-06 16:41:12,277 - Epoch: [84][  170/  246]    Overall Loss 0.268210    Objective Loss 0.268210                                        LR 0.000022    Time 0.017331    
2023-01-06 16:41:12,417 - Epoch: [84][  180/  246]    Overall Loss 0.268539    Objective Loss 0.268539                                        LR 0.000022    Time 0.017147    
2023-01-06 16:41:12,557 - Epoch: [84][  190/  246]    Overall Loss 0.269433    Objective Loss 0.269433                                        LR 0.000022    Time 0.016975    
2023-01-06 16:41:12,697 - Epoch: [84][  200/  246]    Overall Loss 0.268728    Objective Loss 0.268728                                        LR 0.000022    Time 0.016826    
2023-01-06 16:41:12,837 - Epoch: [84][  210/  246]    Overall Loss 0.268596    Objective Loss 0.268596                                        LR 0.000022    Time 0.016686    
2023-01-06 16:41:12,974 - Epoch: [84][  220/  246]    Overall Loss 0.269215    Objective Loss 0.269215                                        LR 0.000022    Time 0.016552    
2023-01-06 16:41:13,113 - Epoch: [84][  230/  246]    Overall Loss 0.269388    Objective Loss 0.269388                                        LR 0.000022    Time 0.016432    
2023-01-06 16:41:13,267 - Epoch: [84][  240/  246]    Overall Loss 0.269047    Objective Loss 0.269047                                        LR 0.000022    Time 0.016389    
2023-01-06 16:41:13,331 - Epoch: [84][  246/  246]    Overall Loss 0.269003    Objective Loss 0.269003    Top1 92.105263    LR 0.000022    Time 0.016246    
2023-01-06 16:41:13,470 - --- validate (epoch=84)-----------
2023-01-06 16:41:13,470 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:13,891 - Epoch: [84][   10/   28]    Loss 0.294069    Top1 89.179688    
2023-01-06 16:41:13,983 - Epoch: [84][   20/   28]    Loss 0.275864    Top1 89.960938    
2023-01-06 16:41:14,034 - Epoch: [84][   28/   28]    Loss 0.273647    Top1 90.080160    
2023-01-06 16:41:14,163 - ==> Top1: 90.080    Loss: 0.274

2023-01-06 16:41:14,163 - ==> Confusion:
[[ 210   12  217]
 [  13  238  351]
 [  48   52 5845]]

2023-01-06 16:41:14,164 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:14,164 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:14,169 - 

2023-01-06 16:41:14,169 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:14,825 - Epoch: [85][   10/  246]    Overall Loss 0.241049    Objective Loss 0.241049                                        LR 0.000022    Time 0.065512    
2023-01-06 16:41:14,961 - Epoch: [85][   20/  246]    Overall Loss 0.245931    Objective Loss 0.245931                                        LR 0.000022    Time 0.039555    
2023-01-06 16:41:15,095 - Epoch: [85][   30/  246]    Overall Loss 0.261764    Objective Loss 0.261764                                        LR 0.000022    Time 0.030827    
2023-01-06 16:41:15,239 - Epoch: [85][   40/  246]    Overall Loss 0.259071    Objective Loss 0.259071                                        LR 0.000022    Time 0.026712    
2023-01-06 16:41:15,373 - Epoch: [85][   50/  246]    Overall Loss 0.260777    Objective Loss 0.260777                                        LR 0.000022    Time 0.024036    
2023-01-06 16:41:15,507 - Epoch: [85][   60/  246]    Overall Loss 0.260936    Objective Loss 0.260936                                        LR 0.000022    Time 0.022262    
2023-01-06 16:41:15,644 - Epoch: [85][   70/  246]    Overall Loss 0.262690    Objective Loss 0.262690                                        LR 0.000022    Time 0.021025    
2023-01-06 16:41:15,782 - Epoch: [85][   80/  246]    Overall Loss 0.261078    Objective Loss 0.261078                                        LR 0.000022    Time 0.020120    
2023-01-06 16:41:15,921 - Epoch: [85][   90/  246]    Overall Loss 0.262201    Objective Loss 0.262201                                        LR 0.000022    Time 0.019428    
2023-01-06 16:41:16,055 - Epoch: [85][  100/  246]    Overall Loss 0.262986    Objective Loss 0.262986                                        LR 0.000022    Time 0.018815    
2023-01-06 16:41:16,187 - Epoch: [85][  110/  246]    Overall Loss 0.263314    Objective Loss 0.263314                                        LR 0.000022    Time 0.018293    
2023-01-06 16:41:16,318 - Epoch: [85][  120/  246]    Overall Loss 0.264448    Objective Loss 0.264448                                        LR 0.000022    Time 0.017849    
2023-01-06 16:41:16,450 - Epoch: [85][  130/  246]    Overall Loss 0.266479    Objective Loss 0.266479                                        LR 0.000022    Time 0.017481    
2023-01-06 16:41:16,582 - Epoch: [85][  140/  246]    Overall Loss 0.266779    Objective Loss 0.266779                                        LR 0.000022    Time 0.017174    
2023-01-06 16:41:16,713 - Epoch: [85][  150/  246]    Overall Loss 0.266406    Objective Loss 0.266406                                        LR 0.000022    Time 0.016900    
2023-01-06 16:41:16,844 - Epoch: [85][  160/  246]    Overall Loss 0.267184    Objective Loss 0.267184                                        LR 0.000022    Time 0.016663    
2023-01-06 16:41:16,976 - Epoch: [85][  170/  246]    Overall Loss 0.267108    Objective Loss 0.267108                                        LR 0.000022    Time 0.016456    
2023-01-06 16:41:17,108 - Epoch: [85][  180/  246]    Overall Loss 0.266334    Objective Loss 0.266334                                        LR 0.000022    Time 0.016274    
2023-01-06 16:41:17,245 - Epoch: [85][  190/  246]    Overall Loss 0.265791    Objective Loss 0.265791                                        LR 0.000022    Time 0.016140    
2023-01-06 16:41:17,377 - Epoch: [85][  200/  246]    Overall Loss 0.265960    Objective Loss 0.265960                                        LR 0.000022    Time 0.015987    
2023-01-06 16:41:17,508 - Epoch: [85][  210/  246]    Overall Loss 0.265468    Objective Loss 0.265468                                        LR 0.000022    Time 0.015850    
2023-01-06 16:41:17,640 - Epoch: [85][  220/  246]    Overall Loss 0.266247    Objective Loss 0.266247                                        LR 0.000022    Time 0.015727    
2023-01-06 16:41:17,769 - Epoch: [85][  230/  246]    Overall Loss 0.267126    Objective Loss 0.267126                                        LR 0.000022    Time 0.015606    
2023-01-06 16:41:17,917 - Epoch: [85][  240/  246]    Overall Loss 0.267061    Objective Loss 0.267061                                        LR 0.000022    Time 0.015565    
2023-01-06 16:41:17,985 - Epoch: [85][  246/  246]    Overall Loss 0.267389    Objective Loss 0.267389    Top1 89.234450    LR 0.000022    Time 0.015459    
2023-01-06 16:41:18,110 - --- validate (epoch=85)-----------
2023-01-06 16:41:18,111 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:18,540 - Epoch: [85][   10/   28]    Loss 0.272773    Top1 90.429688    
2023-01-06 16:41:18,644 - Epoch: [85][   20/   28]    Loss 0.272765    Top1 90.332031    
2023-01-06 16:41:18,693 - Epoch: [85][   28/   28]    Loss 0.280449    Top1 90.123103    
2023-01-06 16:41:18,853 - ==> Top1: 90.123    Loss: 0.280

2023-01-06 16:41:18,853 - ==> Confusion:
[[ 228   14  197]
 [  13  232  357]
 [  59   50 5836]]

2023-01-06 16:41:18,854 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:18,855 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:18,860 - 

2023-01-06 16:41:18,860 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:19,369 - Epoch: [86][   10/  246]    Overall Loss 0.288516    Objective Loss 0.288516                                        LR 0.000022    Time 0.050859    
2023-01-06 16:41:19,512 - Epoch: [86][   20/  246]    Overall Loss 0.286038    Objective Loss 0.286038                                        LR 0.000022    Time 0.032583    
2023-01-06 16:41:19,668 - Epoch: [86][   30/  246]    Overall Loss 0.277526    Objective Loss 0.277526                                        LR 0.000022    Time 0.026880    
2023-01-06 16:41:19,822 - Epoch: [86][   40/  246]    Overall Loss 0.272366    Objective Loss 0.272366                                        LR 0.000022    Time 0.024002    
2023-01-06 16:41:19,973 - Epoch: [86][   50/  246]    Overall Loss 0.274207    Objective Loss 0.274207                                        LR 0.000022    Time 0.022225    
2023-01-06 16:41:20,122 - Epoch: [86][   60/  246]    Overall Loss 0.268762    Objective Loss 0.268762                                        LR 0.000022    Time 0.020993    
2023-01-06 16:41:20,274 - Epoch: [86][   70/  246]    Overall Loss 0.268323    Objective Loss 0.268323                                        LR 0.000022    Time 0.020152    
2023-01-06 16:41:20,423 - Epoch: [86][   80/  246]    Overall Loss 0.268745    Objective Loss 0.268745                                        LR 0.000022    Time 0.019493    
2023-01-06 16:41:20,573 - Epoch: [86][   90/  246]    Overall Loss 0.266353    Objective Loss 0.266353                                        LR 0.000022    Time 0.018996    
2023-01-06 16:41:20,715 - Epoch: [86][  100/  246]    Overall Loss 0.267157    Objective Loss 0.267157                                        LR 0.000022    Time 0.018510    
2023-01-06 16:41:20,871 - Epoch: [86][  110/  246]    Overall Loss 0.268525    Objective Loss 0.268525                                        LR 0.000022    Time 0.018238    
2023-01-06 16:41:21,023 - Epoch: [86][  120/  246]    Overall Loss 0.269293    Objective Loss 0.269293                                        LR 0.000022    Time 0.017980    
2023-01-06 16:41:21,169 - Epoch: [86][  130/  246]    Overall Loss 0.269169    Objective Loss 0.269169                                        LR 0.000022    Time 0.017718    
2023-01-06 16:41:21,317 - Epoch: [86][  140/  246]    Overall Loss 0.268331    Objective Loss 0.268331                                        LR 0.000022    Time 0.017510    
2023-01-06 16:41:21,462 - Epoch: [86][  150/  246]    Overall Loss 0.268177    Objective Loss 0.268177                                        LR 0.000022    Time 0.017305    
2023-01-06 16:41:21,612 - Epoch: [86][  160/  246]    Overall Loss 0.268746    Objective Loss 0.268746                                        LR 0.000022    Time 0.017159    
2023-01-06 16:41:21,752 - Epoch: [86][  170/  246]    Overall Loss 0.269771    Objective Loss 0.269771                                        LR 0.000022    Time 0.016970    
2023-01-06 16:41:21,897 - Epoch: [86][  180/  246]    Overall Loss 0.269428    Objective Loss 0.269428                                        LR 0.000022    Time 0.016830    
2023-01-06 16:41:22,045 - Epoch: [86][  190/  246]    Overall Loss 0.268245    Objective Loss 0.268245                                        LR 0.000022    Time 0.016721    
2023-01-06 16:41:22,192 - Epoch: [86][  200/  246]    Overall Loss 0.267425    Objective Loss 0.267425                                        LR 0.000022    Time 0.016618    
2023-01-06 16:41:22,332 - Epoch: [86][  210/  246]    Overall Loss 0.267302    Objective Loss 0.267302                                        LR 0.000022    Time 0.016496    
2023-01-06 16:41:22,476 - Epoch: [86][  220/  246]    Overall Loss 0.266828    Objective Loss 0.266828                                        LR 0.000022    Time 0.016394    
2023-01-06 16:41:22,641 - Epoch: [86][  230/  246]    Overall Loss 0.268043    Objective Loss 0.268043                                        LR 0.000022    Time 0.016399    
2023-01-06 16:41:22,811 - Epoch: [86][  240/  246]    Overall Loss 0.267215    Objective Loss 0.267215                                        LR 0.000022    Time 0.016420    
2023-01-06 16:41:22,882 - Epoch: [86][  246/  246]    Overall Loss 0.267637    Objective Loss 0.267637    Top1 89.234450    LR 0.000022    Time 0.016308    
2023-01-06 16:41:23,018 - --- validate (epoch=86)-----------
2023-01-06 16:41:23,018 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:23,437 - Epoch: [86][   10/   28]    Loss 0.278950    Top1 90.117188    
2023-01-06 16:41:23,531 - Epoch: [86][   20/   28]    Loss 0.281695    Top1 90.097656    
2023-01-06 16:41:23,583 - Epoch: [86][   28/   28]    Loss 0.277942    Top1 90.065846    
2023-01-06 16:41:23,730 - ==> Top1: 90.066    Loss: 0.278

2023-01-06 16:41:23,730 - ==> Confusion:
[[ 211   12  216]
 [  12  225  365]
 [  45   44 5856]]

2023-01-06 16:41:23,731 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:23,731 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:23,737 - 

2023-01-06 16:41:23,737 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:24,400 - Epoch: [87][   10/  246]    Overall Loss 0.272169    Objective Loss 0.272169                                        LR 0.000022    Time 0.066238    
2023-01-06 16:41:24,539 - Epoch: [87][   20/  246]    Overall Loss 0.262175    Objective Loss 0.262175                                        LR 0.000022    Time 0.040081    
2023-01-06 16:41:24,677 - Epoch: [87][   30/  246]    Overall Loss 0.263629    Objective Loss 0.263629                                        LR 0.000022    Time 0.031307    
2023-01-06 16:41:24,808 - Epoch: [87][   40/  246]    Overall Loss 0.266121    Objective Loss 0.266121                                        LR 0.000022    Time 0.026732    
2023-01-06 16:41:24,944 - Epoch: [87][   50/  246]    Overall Loss 0.265550    Objective Loss 0.265550                                        LR 0.000022    Time 0.024095    
2023-01-06 16:41:25,079 - Epoch: [87][   60/  246]    Overall Loss 0.267473    Objective Loss 0.267473                                        LR 0.000022    Time 0.022335    
2023-01-06 16:41:25,216 - Epoch: [87][   70/  246]    Overall Loss 0.266846    Objective Loss 0.266846                                        LR 0.000022    Time 0.021086    
2023-01-06 16:41:25,347 - Epoch: [87][   80/  246]    Overall Loss 0.268123    Objective Loss 0.268123                                        LR 0.000022    Time 0.020085    
2023-01-06 16:41:25,479 - Epoch: [87][   90/  246]    Overall Loss 0.268118    Objective Loss 0.268118                                        LR 0.000022    Time 0.019319    
2023-01-06 16:41:25,614 - Epoch: [87][  100/  246]    Overall Loss 0.265527    Objective Loss 0.265527                                        LR 0.000022    Time 0.018733    
2023-01-06 16:41:25,750 - Epoch: [87][  110/  246]    Overall Loss 0.268452    Objective Loss 0.268452                                        LR 0.000022    Time 0.018262    
2023-01-06 16:41:25,886 - Epoch: [87][  120/  246]    Overall Loss 0.269328    Objective Loss 0.269328                                        LR 0.000022    Time 0.017866    
2023-01-06 16:41:26,020 - Epoch: [87][  130/  246]    Overall Loss 0.267178    Objective Loss 0.267178                                        LR 0.000022    Time 0.017524    
2023-01-06 16:41:26,154 - Epoch: [87][  140/  246]    Overall Loss 0.267248    Objective Loss 0.267248                                        LR 0.000022    Time 0.017227    
2023-01-06 16:41:26,288 - Epoch: [87][  150/  246]    Overall Loss 0.267596    Objective Loss 0.267596                                        LR 0.000022    Time 0.016969    
2023-01-06 16:41:26,421 - Epoch: [87][  160/  246]    Overall Loss 0.267155    Objective Loss 0.267155                                        LR 0.000022    Time 0.016738    
2023-01-06 16:41:26,556 - Epoch: [87][  170/  246]    Overall Loss 0.266836    Objective Loss 0.266836                                        LR 0.000022    Time 0.016544    
2023-01-06 16:41:26,691 - Epoch: [87][  180/  246]    Overall Loss 0.266984    Objective Loss 0.266984                                        LR 0.000022    Time 0.016372    
2023-01-06 16:41:26,828 - Epoch: [87][  190/  246]    Overall Loss 0.267965    Objective Loss 0.267965                                        LR 0.000022    Time 0.016230    
2023-01-06 16:41:26,961 - Epoch: [87][  200/  246]    Overall Loss 0.267711    Objective Loss 0.267711                                        LR 0.000022    Time 0.016082    
2023-01-06 16:41:27,097 - Epoch: [87][  210/  246]    Overall Loss 0.267081    Objective Loss 0.267081                                        LR 0.000022    Time 0.015962    
2023-01-06 16:41:27,231 - Epoch: [87][  220/  246]    Overall Loss 0.267228    Objective Loss 0.267228                                        LR 0.000022    Time 0.015844    
2023-01-06 16:41:27,365 - Epoch: [87][  230/  246]    Overall Loss 0.267180    Objective Loss 0.267180                                        LR 0.000022    Time 0.015739    
2023-01-06 16:41:27,516 - Epoch: [87][  240/  246]    Overall Loss 0.267561    Objective Loss 0.267561                                        LR 0.000022    Time 0.015709    
2023-01-06 16:41:27,580 - Epoch: [87][  246/  246]    Overall Loss 0.266969    Objective Loss 0.266969    Top1 92.344498    LR 0.000022    Time 0.015585    
2023-01-06 16:41:27,716 - --- validate (epoch=87)-----------
2023-01-06 16:41:27,716 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:28,136 - Epoch: [87][   10/   28]    Loss 0.269534    Top1 90.117188    
2023-01-06 16:41:28,229 - Epoch: [87][   20/   28]    Loss 0.269896    Top1 90.312500    
2023-01-06 16:41:28,280 - Epoch: [87][   28/   28]    Loss 0.277615    Top1 90.166046    
2023-01-06 16:41:28,414 - ==> Top1: 90.166    Loss: 0.278

2023-01-06 16:41:28,414 - ==> Confusion:
[[ 217   13  209]
 [  13  228  361]
 [  45   46 5854]]

2023-01-06 16:41:28,415 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:28,415 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:28,421 - 

2023-01-06 16:41:28,421 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:28,936 - Epoch: [88][   10/  246]    Overall Loss 0.275861    Objective Loss 0.275861                                        LR 0.000022    Time 0.051441    
2023-01-06 16:41:29,057 - Epoch: [88][   20/  246]    Overall Loss 0.271668    Objective Loss 0.271668                                        LR 0.000022    Time 0.031765    
2023-01-06 16:41:29,178 - Epoch: [88][   30/  246]    Overall Loss 0.279533    Objective Loss 0.279533                                        LR 0.000022    Time 0.025176    
2023-01-06 16:41:29,301 - Epoch: [88][   40/  246]    Overall Loss 0.275507    Objective Loss 0.275507                                        LR 0.000022    Time 0.021962    
2023-01-06 16:41:29,419 - Epoch: [88][   50/  246]    Overall Loss 0.273772    Objective Loss 0.273772                                        LR 0.000022    Time 0.019921    
2023-01-06 16:41:29,542 - Epoch: [88][   60/  246]    Overall Loss 0.272040    Objective Loss 0.272040                                        LR 0.000022    Time 0.018640    
2023-01-06 16:41:29,658 - Epoch: [88][   70/  246]    Overall Loss 0.268007    Objective Loss 0.268007                                        LR 0.000022    Time 0.017632    
2023-01-06 16:41:29,786 - Epoch: [88][   80/  246]    Overall Loss 0.266725    Objective Loss 0.266725                                        LR 0.000022    Time 0.017019    
2023-01-06 16:41:29,908 - Epoch: [88][   90/  246]    Overall Loss 0.267485    Objective Loss 0.267485                                        LR 0.000022    Time 0.016485    
2023-01-06 16:41:30,033 - Epoch: [88][  100/  246]    Overall Loss 0.266634    Objective Loss 0.266634                                        LR 0.000022    Time 0.016078    
2023-01-06 16:41:30,158 - Epoch: [88][  110/  246]    Overall Loss 0.266424    Objective Loss 0.266424                                        LR 0.000022    Time 0.015750    
2023-01-06 16:41:30,284 - Epoch: [88][  120/  246]    Overall Loss 0.267082    Objective Loss 0.267082                                        LR 0.000022    Time 0.015483    
2023-01-06 16:41:30,406 - Epoch: [88][  130/  246]    Overall Loss 0.265590    Objective Loss 0.265590                                        LR 0.000022    Time 0.015229    
2023-01-06 16:41:30,530 - Epoch: [88][  140/  246]    Overall Loss 0.265529    Objective Loss 0.265529                                        LR 0.000022    Time 0.015023    
2023-01-06 16:41:30,655 - Epoch: [88][  150/  246]    Overall Loss 0.265247    Objective Loss 0.265247                                        LR 0.000022    Time 0.014855    
2023-01-06 16:41:30,779 - Epoch: [88][  160/  246]    Overall Loss 0.265696    Objective Loss 0.265696                                        LR 0.000022    Time 0.014701    
2023-01-06 16:41:30,903 - Epoch: [88][  170/  246]    Overall Loss 0.266293    Objective Loss 0.266293                                        LR 0.000022    Time 0.014563    
2023-01-06 16:41:31,026 - Epoch: [88][  180/  246]    Overall Loss 0.265751    Objective Loss 0.265751                                        LR 0.000022    Time 0.014437    
2023-01-06 16:41:31,148 - Epoch: [88][  190/  246]    Overall Loss 0.266176    Objective Loss 0.266176                                        LR 0.000022    Time 0.014315    
2023-01-06 16:41:31,269 - Epoch: [88][  200/  246]    Overall Loss 0.267489    Objective Loss 0.267489                                        LR 0.000022    Time 0.014205    
2023-01-06 16:41:31,393 - Epoch: [88][  210/  246]    Overall Loss 0.267336    Objective Loss 0.267336                                        LR 0.000022    Time 0.014117    
2023-01-06 16:41:31,518 - Epoch: [88][  220/  246]    Overall Loss 0.267555    Objective Loss 0.267555                                        LR 0.000022    Time 0.014038    
2023-01-06 16:41:31,639 - Epoch: [88][  230/  246]    Overall Loss 0.267501    Objective Loss 0.267501                                        LR 0.000022    Time 0.013954    
2023-01-06 16:41:31,778 - Epoch: [88][  240/  246]    Overall Loss 0.266970    Objective Loss 0.266970                                        LR 0.000022    Time 0.013950    
2023-01-06 16:41:31,839 - Epoch: [88][  246/  246]    Overall Loss 0.266738    Objective Loss 0.266738    Top1 90.669856    LR 0.000022    Time 0.013856    
2023-01-06 16:41:31,969 - --- validate (epoch=88)-----------
2023-01-06 16:41:31,969 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:32,406 - Epoch: [88][   10/   28]    Loss 0.282030    Top1 90.234375    
2023-01-06 16:41:32,502 - Epoch: [88][   20/   28]    Loss 0.270936    Top1 90.546875    
2023-01-06 16:41:32,553 - Epoch: [88][   28/   28]    Loss 0.274393    Top1 90.237618    
2023-01-06 16:41:32,690 - ==> Top1: 90.238    Loss: 0.274

2023-01-06 16:41:32,690 - ==> Confusion:
[[ 243   14  182]
 [  17  263  322]
 [  70   77 5798]]

2023-01-06 16:41:32,692 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:32,692 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:32,699 - 

2023-01-06 16:41:32,699 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:33,362 - Epoch: [89][   10/  246]    Overall Loss 0.281876    Objective Loss 0.281876                                        LR 0.000022    Time 0.066230    
2023-01-06 16:41:33,489 - Epoch: [89][   20/  246]    Overall Loss 0.269327    Objective Loss 0.269327                                        LR 0.000022    Time 0.039438    
2023-01-06 16:41:33,620 - Epoch: [89][   30/  246]    Overall Loss 0.276184    Objective Loss 0.276184                                        LR 0.000022    Time 0.030632    
2023-01-06 16:41:33,752 - Epoch: [89][   40/  246]    Overall Loss 0.271201    Objective Loss 0.271201                                        LR 0.000022    Time 0.026265    
2023-01-06 16:41:33,890 - Epoch: [89][   50/  246]    Overall Loss 0.273115    Objective Loss 0.273115                                        LR 0.000022    Time 0.023760    
2023-01-06 16:41:34,022 - Epoch: [89][   60/  246]    Overall Loss 0.268248    Objective Loss 0.268248                                        LR 0.000022    Time 0.021993    
2023-01-06 16:41:34,143 - Epoch: [89][   70/  246]    Overall Loss 0.265194    Objective Loss 0.265194                                        LR 0.000022    Time 0.020579    
2023-01-06 16:41:34,269 - Epoch: [89][   80/  246]    Overall Loss 0.264285    Objective Loss 0.264285                                        LR 0.000022    Time 0.019574    
2023-01-06 16:41:34,396 - Epoch: [89][   90/  246]    Overall Loss 0.267140    Objective Loss 0.267140                                        LR 0.000022    Time 0.018809    
2023-01-06 16:41:34,529 - Epoch: [89][  100/  246]    Overall Loss 0.266561    Objective Loss 0.266561                                        LR 0.000022    Time 0.018256    
2023-01-06 16:41:34,666 - Epoch: [89][  110/  246]    Overall Loss 0.266207    Objective Loss 0.266207                                        LR 0.000022    Time 0.017831    
2023-01-06 16:41:34,799 - Epoch: [89][  120/  246]    Overall Loss 0.266460    Objective Loss 0.266460                                        LR 0.000022    Time 0.017450    
2023-01-06 16:41:34,930 - Epoch: [89][  130/  246]    Overall Loss 0.267376    Objective Loss 0.267376                                        LR 0.000022    Time 0.017119    
2023-01-06 16:41:35,062 - Epoch: [89][  140/  246]    Overall Loss 0.266977    Objective Loss 0.266977                                        LR 0.000022    Time 0.016832    
2023-01-06 16:41:35,194 - Epoch: [89][  150/  246]    Overall Loss 0.266869    Objective Loss 0.266869                                        LR 0.000022    Time 0.016589    
2023-01-06 16:41:35,329 - Epoch: [89][  160/  246]    Overall Loss 0.266692    Objective Loss 0.266692                                        LR 0.000022    Time 0.016391    
2023-01-06 16:41:35,462 - Epoch: [89][  170/  246]    Overall Loss 0.265391    Objective Loss 0.265391                                        LR 0.000022    Time 0.016205    
2023-01-06 16:41:35,604 - Epoch: [89][  180/  246]    Overall Loss 0.265872    Objective Loss 0.265872                                        LR 0.000022    Time 0.016092    
2023-01-06 16:41:35,749 - Epoch: [89][  190/  246]    Overall Loss 0.267074    Objective Loss 0.267074                                        LR 0.000022    Time 0.015998    
2023-01-06 16:41:35,881 - Epoch: [89][  200/  246]    Overall Loss 0.266787    Objective Loss 0.266787                                        LR 0.000022    Time 0.015856    
2023-01-06 16:41:36,021 - Epoch: [89][  210/  246]    Overall Loss 0.266999    Objective Loss 0.266999                                        LR 0.000022    Time 0.015765    
2023-01-06 16:41:36,170 - Epoch: [89][  220/  246]    Overall Loss 0.267614    Objective Loss 0.267614                                        LR 0.000022    Time 0.015727    
2023-01-06 16:41:36,318 - Epoch: [89][  230/  246]    Overall Loss 0.266963    Objective Loss 0.266963                                        LR 0.000022    Time 0.015686    
2023-01-06 16:41:36,477 - Epoch: [89][  240/  246]    Overall Loss 0.266850    Objective Loss 0.266850                                        LR 0.000022    Time 0.015690    
2023-01-06 16:41:36,544 - Epoch: [89][  246/  246]    Overall Loss 0.266380    Objective Loss 0.266380    Top1 93.062201    LR 0.000022    Time 0.015581    
2023-01-06 16:41:36,681 - --- validate (epoch=89)-----------
2023-01-06 16:41:36,682 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:37,121 - Epoch: [89][   10/   28]    Loss 0.280690    Top1 89.765625    
2023-01-06 16:41:37,222 - Epoch: [89][   20/   28]    Loss 0.283308    Top1 89.667969    
2023-01-06 16:41:37,273 - Epoch: [89][   28/   28]    Loss 0.279335    Top1 89.851131    
2023-01-06 16:41:37,409 - ==> Top1: 89.851    Loss: 0.279

2023-01-06 16:41:37,409 - ==> Confusion:
[[ 230   13  196]
 [  21  224  357]
 [  76   46 5823]]

2023-01-06 16:41:37,410 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:37,410 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:37,415 - 

2023-01-06 16:41:37,415 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:38,070 - Epoch: [90][   10/  246]    Overall Loss 0.268929    Objective Loss 0.268929                                        LR 0.000022    Time 0.065369    
2023-01-06 16:41:38,199 - Epoch: [90][   20/  246]    Overall Loss 0.279064    Objective Loss 0.279064                                        LR 0.000022    Time 0.039158    
2023-01-06 16:41:38,328 - Epoch: [90][   30/  246]    Overall Loss 0.276977    Objective Loss 0.276977                                        LR 0.000022    Time 0.030391    
2023-01-06 16:41:38,452 - Epoch: [90][   40/  246]    Overall Loss 0.269584    Objective Loss 0.269584                                        LR 0.000022    Time 0.025880    
2023-01-06 16:41:38,587 - Epoch: [90][   50/  246]    Overall Loss 0.266873    Objective Loss 0.266873                                        LR 0.000022    Time 0.023393    
2023-01-06 16:41:38,724 - Epoch: [90][   60/  246]    Overall Loss 0.262417    Objective Loss 0.262417                                        LR 0.000022    Time 0.021772    
2023-01-06 16:41:38,867 - Epoch: [90][   70/  246]    Overall Loss 0.264840    Objective Loss 0.264840                                        LR 0.000022    Time 0.020699    
2023-01-06 16:41:39,000 - Epoch: [90][   80/  246]    Overall Loss 0.264470    Objective Loss 0.264470                                        LR 0.000022    Time 0.019771    
2023-01-06 16:41:39,144 - Epoch: [90][   90/  246]    Overall Loss 0.263622    Objective Loss 0.263622                                        LR 0.000022    Time 0.019160    
2023-01-06 16:41:39,268 - Epoch: [90][  100/  246]    Overall Loss 0.265309    Objective Loss 0.265309                                        LR 0.000022    Time 0.018484    
2023-01-06 16:41:39,392 - Epoch: [90][  110/  246]    Overall Loss 0.264668    Objective Loss 0.264668                                        LR 0.000022    Time 0.017932    
2023-01-06 16:41:39,517 - Epoch: [90][  120/  246]    Overall Loss 0.264196    Objective Loss 0.264196                                        LR 0.000022    Time 0.017470    
2023-01-06 16:41:39,643 - Epoch: [90][  130/  246]    Overall Loss 0.264635    Objective Loss 0.264635                                        LR 0.000022    Time 0.017091    
2023-01-06 16:41:39,765 - Epoch: [90][  140/  246]    Overall Loss 0.266690    Objective Loss 0.266690                                        LR 0.000022    Time 0.016743    
2023-01-06 16:41:39,887 - Epoch: [90][  150/  246]    Overall Loss 0.266822    Objective Loss 0.266822                                        LR 0.000022    Time 0.016439    
2023-01-06 16:41:40,010 - Epoch: [90][  160/  246]    Overall Loss 0.267851    Objective Loss 0.267851                                        LR 0.000022    Time 0.016174    
2023-01-06 16:41:40,147 - Epoch: [90][  170/  246]    Overall Loss 0.267908    Objective Loss 0.267908                                        LR 0.000022    Time 0.016029    
2023-01-06 16:41:40,300 - Epoch: [90][  180/  246]    Overall Loss 0.268369    Objective Loss 0.268369                                        LR 0.000022    Time 0.015983    
2023-01-06 16:41:40,453 - Epoch: [90][  190/  246]    Overall Loss 0.267971    Objective Loss 0.267971                                        LR 0.000022    Time 0.015947    
2023-01-06 16:41:40,606 - Epoch: [90][  200/  246]    Overall Loss 0.267517    Objective Loss 0.267517                                        LR 0.000022    Time 0.015915    
2023-01-06 16:41:40,758 - Epoch: [90][  210/  246]    Overall Loss 0.267978    Objective Loss 0.267978                                        LR 0.000022    Time 0.015879    
2023-01-06 16:41:40,910 - Epoch: [90][  220/  246]    Overall Loss 0.268634    Objective Loss 0.268634                                        LR 0.000022    Time 0.015846    
2023-01-06 16:41:41,063 - Epoch: [90][  230/  246]    Overall Loss 0.268214    Objective Loss 0.268214                                        LR 0.000022    Time 0.015820    
2023-01-06 16:41:41,217 - Epoch: [90][  240/  246]    Overall Loss 0.268796    Objective Loss 0.268796                                        LR 0.000022    Time 0.015801    
2023-01-06 16:41:41,278 - Epoch: [90][  246/  246]    Overall Loss 0.268108    Objective Loss 0.268108    Top1 91.866029    LR 0.000022    Time 0.015664    
2023-01-06 16:41:41,414 - --- validate (epoch=90)-----------
2023-01-06 16:41:41,415 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:41,837 - Epoch: [90][   10/   28]    Loss 0.289250    Top1 89.687500    
2023-01-06 16:41:41,934 - Epoch: [90][   20/   28]    Loss 0.271796    Top1 90.351562    
2023-01-06 16:41:41,986 - Epoch: [90][   28/   28]    Loss 0.275061    Top1 90.137418    
2023-01-06 16:41:42,117 - ==> Top1: 90.137    Loss: 0.275

2023-01-06 16:41:42,118 - ==> Confusion:
[[ 209   12  218]
 [  12  226  364]
 [  41   42 5862]]

2023-01-06 16:41:42,119 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:42,119 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:42,126 - 

2023-01-06 16:41:42,126 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:42,641 - Epoch: [91][   10/  246]    Overall Loss 0.266199    Objective Loss 0.266199                                        LR 0.000022    Time 0.051418    
2023-01-06 16:41:42,768 - Epoch: [91][   20/  246]    Overall Loss 0.264094    Objective Loss 0.264094                                        LR 0.000022    Time 0.032064    
2023-01-06 16:41:42,898 - Epoch: [91][   30/  246]    Overall Loss 0.263793    Objective Loss 0.263793                                        LR 0.000022    Time 0.025699    
2023-01-06 16:41:43,036 - Epoch: [91][   40/  246]    Overall Loss 0.264509    Objective Loss 0.264509                                        LR 0.000022    Time 0.022697    
2023-01-06 16:41:43,170 - Epoch: [91][   50/  246]    Overall Loss 0.263159    Objective Loss 0.263159                                        LR 0.000022    Time 0.020830    
2023-01-06 16:41:43,303 - Epoch: [91][   60/  246]    Overall Loss 0.265930    Objective Loss 0.265930                                        LR 0.000022    Time 0.019573    
2023-01-06 16:41:43,437 - Epoch: [91][   70/  246]    Overall Loss 0.261769    Objective Loss 0.261769                                        LR 0.000022    Time 0.018687    
2023-01-06 16:41:43,571 - Epoch: [91][   80/  246]    Overall Loss 0.260052    Objective Loss 0.260052                                        LR 0.000022    Time 0.018023    
2023-01-06 16:41:43,706 - Epoch: [91][   90/  246]    Overall Loss 0.258981    Objective Loss 0.258981                                        LR 0.000022    Time 0.017515    
2023-01-06 16:41:43,848 - Epoch: [91][  100/  246]    Overall Loss 0.259847    Objective Loss 0.259847                                        LR 0.000022    Time 0.017180    
2023-01-06 16:41:43,990 - Epoch: [91][  110/  246]    Overall Loss 0.261623    Objective Loss 0.261623                                        LR 0.000022    Time 0.016905    
2023-01-06 16:41:44,134 - Epoch: [91][  120/  246]    Overall Loss 0.261150    Objective Loss 0.261150                                        LR 0.000022    Time 0.016690    
2023-01-06 16:41:44,273 - Epoch: [91][  130/  246]    Overall Loss 0.260840    Objective Loss 0.260840                                        LR 0.000022    Time 0.016475    
2023-01-06 16:41:44,410 - Epoch: [91][  140/  246]    Overall Loss 0.259937    Objective Loss 0.259937                                        LR 0.000022    Time 0.016272    
2023-01-06 16:41:44,547 - Epoch: [91][  150/  246]    Overall Loss 0.260077    Objective Loss 0.260077                                        LR 0.000022    Time 0.016099    
2023-01-06 16:41:44,683 - Epoch: [91][  160/  246]    Overall Loss 0.260505    Objective Loss 0.260505                                        LR 0.000022    Time 0.015940    
2023-01-06 16:41:44,821 - Epoch: [91][  170/  246]    Overall Loss 0.262053    Objective Loss 0.262053                                        LR 0.000022    Time 0.015808    
2023-01-06 16:41:44,959 - Epoch: [91][  180/  246]    Overall Loss 0.262533    Objective Loss 0.262533                                        LR 0.000022    Time 0.015696    
2023-01-06 16:41:45,096 - Epoch: [91][  190/  246]    Overall Loss 0.263428    Objective Loss 0.263428                                        LR 0.000022    Time 0.015589    
2023-01-06 16:41:45,231 - Epoch: [91][  200/  246]    Overall Loss 0.264079    Objective Loss 0.264079                                        LR 0.000022    Time 0.015483    
2023-01-06 16:41:45,370 - Epoch: [91][  210/  246]    Overall Loss 0.264455    Objective Loss 0.264455                                        LR 0.000022    Time 0.015405    
2023-01-06 16:41:45,508 - Epoch: [91][  220/  246]    Overall Loss 0.264787    Objective Loss 0.264787                                        LR 0.000022    Time 0.015333    
2023-01-06 16:41:45,634 - Epoch: [91][  230/  246]    Overall Loss 0.264515    Objective Loss 0.264515                                        LR 0.000022    Time 0.015211    
2023-01-06 16:41:45,772 - Epoch: [91][  240/  246]    Overall Loss 0.264920    Objective Loss 0.264920                                        LR 0.000022    Time 0.015152    
2023-01-06 16:41:45,829 - Epoch: [91][  246/  246]    Overall Loss 0.265209    Objective Loss 0.265209    Top1 90.191388    LR 0.000022    Time 0.015014    
2023-01-06 16:41:45,967 - --- validate (epoch=91)-----------
2023-01-06 16:41:45,967 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:46,404 - Epoch: [91][   10/   28]    Loss 0.273337    Top1 90.468750    
2023-01-06 16:41:46,524 - Epoch: [91][   20/   28]    Loss 0.269251    Top1 90.566406    
2023-01-06 16:41:46,572 - Epoch: [91][   28/   28]    Loss 0.276987    Top1 90.137418    
2023-01-06 16:41:46,737 - ==> Top1: 90.137    Loss: 0.277

2023-01-06 16:41:46,737 - ==> Confusion:
[[ 211   16  212]
 [  11  275  316]
 [  40   94 5811]]

2023-01-06 16:41:46,738 - ==> Best [Top1: 90.252   Sparsity:0.00   Params: 151104 on epoch: 77]
2023-01-06 16:41:46,738 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:46,743 - 

2023-01-06 16:41:46,743 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:47,378 - Epoch: [92][   10/  246]    Overall Loss 0.265382    Objective Loss 0.265382                                        LR 0.000022    Time 0.063431    
2023-01-06 16:41:47,504 - Epoch: [92][   20/  246]    Overall Loss 0.268880    Objective Loss 0.268880                                        LR 0.000022    Time 0.037996    
2023-01-06 16:41:47,639 - Epoch: [92][   30/  246]    Overall Loss 0.263796    Objective Loss 0.263796                                        LR 0.000022    Time 0.029792    
2023-01-06 16:41:47,764 - Epoch: [92][   40/  246]    Overall Loss 0.265597    Objective Loss 0.265597                                        LR 0.000022    Time 0.025482    
2023-01-06 16:41:47,903 - Epoch: [92][   50/  246]    Overall Loss 0.270049    Objective Loss 0.270049                                        LR 0.000022    Time 0.023137    
2023-01-06 16:41:48,045 - Epoch: [92][   60/  246]    Overall Loss 0.271059    Objective Loss 0.271059                                        LR 0.000022    Time 0.021648    
2023-01-06 16:41:48,189 - Epoch: [92][   70/  246]    Overall Loss 0.269484    Objective Loss 0.269484                                        LR 0.000022    Time 0.020605    
2023-01-06 16:41:48,331 - Epoch: [92][   80/  246]    Overall Loss 0.270189    Objective Loss 0.270189                                        LR 0.000022    Time 0.019800    
2023-01-06 16:41:48,474 - Epoch: [92][   90/  246]    Overall Loss 0.269620    Objective Loss 0.269620                                        LR 0.000022    Time 0.019184    
2023-01-06 16:41:48,617 - Epoch: [92][  100/  246]    Overall Loss 0.268612    Objective Loss 0.268612                                        LR 0.000022    Time 0.018691    
2023-01-06 16:41:48,766 - Epoch: [92][  110/  246]    Overall Loss 0.269136    Objective Loss 0.269136                                        LR 0.000022    Time 0.018344    
2023-01-06 16:41:48,915 - Epoch: [92][  120/  246]    Overall Loss 0.268711    Objective Loss 0.268711                                        LR 0.000022    Time 0.018051    
2023-01-06 16:41:49,068 - Epoch: [92][  130/  246]    Overall Loss 0.269199    Objective Loss 0.269199                                        LR 0.000022    Time 0.017834    
2023-01-06 16:41:49,214 - Epoch: [92][  140/  246]    Overall Loss 0.269194    Objective Loss 0.269194                                        LR 0.000022    Time 0.017597    
2023-01-06 16:41:49,359 - Epoch: [92][  150/  246]    Overall Loss 0.268751    Objective Loss 0.268751                                        LR 0.000022    Time 0.017391    
2023-01-06 16:41:49,506 - Epoch: [92][  160/  246]    Overall Loss 0.268264    Objective Loss 0.268264                                        LR 0.000022    Time 0.017217    
2023-01-06 16:41:49,652 - Epoch: [92][  170/  246]    Overall Loss 0.269108    Objective Loss 0.269108                                        LR 0.000022    Time 0.017063    
2023-01-06 16:41:49,789 - Epoch: [92][  180/  246]    Overall Loss 0.269374    Objective Loss 0.269374                                        LR 0.000022    Time 0.016873    
2023-01-06 16:41:49,926 - Epoch: [92][  190/  246]    Overall Loss 0.269473    Objective Loss 0.269473                                        LR 0.000022    Time 0.016705    
2023-01-06 16:41:50,052 - Epoch: [92][  200/  246]    Overall Loss 0.267117    Objective Loss 0.267117                                        LR 0.000022    Time 0.016496    
2023-01-06 16:41:50,195 - Epoch: [92][  210/  246]    Overall Loss 0.266856    Objective Loss 0.266856                                        LR 0.000022    Time 0.016392    
2023-01-06 16:41:50,342 - Epoch: [92][  220/  246]    Overall Loss 0.266851    Objective Loss 0.266851                                        LR 0.000022    Time 0.016312    
2023-01-06 16:41:50,489 - Epoch: [92][  230/  246]    Overall Loss 0.266865    Objective Loss 0.266865                                        LR 0.000022    Time 0.016240    
2023-01-06 16:41:50,647 - Epoch: [92][  240/  246]    Overall Loss 0.266346    Objective Loss 0.266346                                        LR 0.000022    Time 0.016221    
2023-01-06 16:41:50,714 - Epoch: [92][  246/  246]    Overall Loss 0.266397    Objective Loss 0.266397    Top1 90.909091    LR 0.000022    Time 0.016095    
2023-01-06 16:41:50,855 - --- validate (epoch=92)-----------
2023-01-06 16:41:50,855 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:51,287 - Epoch: [92][   10/   28]    Loss 0.261313    Top1 90.976562    
2023-01-06 16:41:51,385 - Epoch: [92][   20/   28]    Loss 0.271419    Top1 90.253906    
2023-01-06 16:41:51,436 - Epoch: [92][   28/   28]    Loss 0.270324    Top1 90.280561    
2023-01-06 16:41:51,573 - ==> Top1: 90.281    Loss: 0.270

2023-01-06 16:41:51,574 - ==> Confusion:
[[ 225   14  200]
 [  16  243  343]
 [  55   51 5839]]

2023-01-06 16:41:51,575 - ==> Best [Top1: 90.281   Sparsity:0.00   Params: 151104 on epoch: 92]
2023-01-06 16:41:51,575 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:51,581 - 

2023-01-06 16:41:51,581 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:52,108 - Epoch: [93][   10/  246]    Overall Loss 0.271294    Objective Loss 0.271294                                        LR 0.000022    Time 0.052658    
2023-01-06 16:41:52,258 - Epoch: [93][   20/  246]    Overall Loss 0.272941    Objective Loss 0.272941                                        LR 0.000022    Time 0.033798    
2023-01-06 16:41:52,407 - Epoch: [93][   30/  246]    Overall Loss 0.270666    Objective Loss 0.270666                                        LR 0.000022    Time 0.027487    
2023-01-06 16:41:52,555 - Epoch: [93][   40/  246]    Overall Loss 0.273785    Objective Loss 0.273785                                        LR 0.000022    Time 0.024287    
2023-01-06 16:41:52,703 - Epoch: [93][   50/  246]    Overall Loss 0.268700    Objective Loss 0.268700                                        LR 0.000022    Time 0.022398    
2023-01-06 16:41:52,852 - Epoch: [93][   60/  246]    Overall Loss 0.267936    Objective Loss 0.267936                                        LR 0.000022    Time 0.021127    
2023-01-06 16:41:52,996 - Epoch: [93][   70/  246]    Overall Loss 0.267501    Objective Loss 0.267501                                        LR 0.000022    Time 0.020170    
2023-01-06 16:41:53,133 - Epoch: [93][   80/  246]    Overall Loss 0.264452    Objective Loss 0.264452                                        LR 0.000022    Time 0.019351    
2023-01-06 16:41:53,271 - Epoch: [93][   90/  246]    Overall Loss 0.264447    Objective Loss 0.264447                                        LR 0.000022    Time 0.018728    
2023-01-06 16:41:53,414 - Epoch: [93][  100/  246]    Overall Loss 0.265486    Objective Loss 0.265486                                        LR 0.000022    Time 0.018280    
2023-01-06 16:41:53,554 - Epoch: [93][  110/  246]    Overall Loss 0.264728    Objective Loss 0.264728                                        LR 0.000022    Time 0.017878    
2023-01-06 16:41:53,698 - Epoch: [93][  120/  246]    Overall Loss 0.263597    Objective Loss 0.263597                                        LR 0.000022    Time 0.017568    
2023-01-06 16:41:53,837 - Epoch: [93][  130/  246]    Overall Loss 0.264360    Objective Loss 0.264360                                        LR 0.000022    Time 0.017278    
2023-01-06 16:41:53,976 - Epoch: [93][  140/  246]    Overall Loss 0.265110    Objective Loss 0.265110                                        LR 0.000022    Time 0.017019    
2023-01-06 16:41:54,106 - Epoch: [93][  150/  246]    Overall Loss 0.264917    Objective Loss 0.264917                                        LR 0.000022    Time 0.016749    
2023-01-06 16:41:54,239 - Epoch: [93][  160/  246]    Overall Loss 0.265674    Objective Loss 0.265674                                        LR 0.000022    Time 0.016531    
2023-01-06 16:41:54,371 - Epoch: [93][  170/  246]    Overall Loss 0.266377    Objective Loss 0.266377                                        LR 0.000022    Time 0.016332    
2023-01-06 16:41:54,505 - Epoch: [93][  180/  246]    Overall Loss 0.267108    Objective Loss 0.267108                                        LR 0.000022    Time 0.016169    
2023-01-06 16:41:54,640 - Epoch: [93][  190/  246]    Overall Loss 0.267660    Objective Loss 0.267660                                        LR 0.000022    Time 0.016023    
2023-01-06 16:41:54,774 - Epoch: [93][  200/  246]    Overall Loss 0.267082    Objective Loss 0.267082                                        LR 0.000022    Time 0.015884    
2023-01-06 16:41:54,926 - Epoch: [93][  210/  246]    Overall Loss 0.267156    Objective Loss 0.267156                                        LR 0.000022    Time 0.015851    
2023-01-06 16:41:55,074 - Epoch: [93][  220/  246]    Overall Loss 0.265966    Objective Loss 0.265966                                        LR 0.000022    Time 0.015791    
2023-01-06 16:41:55,220 - Epoch: [93][  230/  246]    Overall Loss 0.265864    Objective Loss 0.265864                                        LR 0.000022    Time 0.015736    
2023-01-06 16:41:55,373 - Epoch: [93][  240/  246]    Overall Loss 0.265689    Objective Loss 0.265689                                        LR 0.000022    Time 0.015715    
2023-01-06 16:41:55,436 - Epoch: [93][  246/  246]    Overall Loss 0.265402    Objective Loss 0.265402    Top1 91.148325    LR 0.000022    Time 0.015589    
2023-01-06 16:41:55,557 - --- validate (epoch=93)-----------
2023-01-06 16:41:55,558 - 6986 samples (256 per mini-batch)
2023-01-06 16:41:56,123 - Epoch: [93][   10/   28]    Loss 0.284156    Top1 89.414062    
2023-01-06 16:41:56,220 - Epoch: [93][   20/   28]    Loss 0.274002    Top1 90.019531    
2023-01-06 16:41:56,269 - Epoch: [93][   28/   28]    Loss 0.274189    Top1 90.008589    
2023-01-06 16:41:56,431 - ==> Top1: 90.009    Loss: 0.274

2023-01-06 16:41:56,432 - ==> Confusion:
[[ 203   15  221]
 [  12  245  345]
 [  38   67 5840]]

2023-01-06 16:41:56,433 - ==> Best [Top1: 90.281   Sparsity:0.00   Params: 151104 on epoch: 92]
2023-01-06 16:41:56,433 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:41:56,438 - 

2023-01-06 16:41:56,438 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:41:56,957 - Epoch: [94][   10/  246]    Overall Loss 0.260488    Objective Loss 0.260488                                        LR 0.000022    Time 0.051845    
2023-01-06 16:41:57,091 - Epoch: [94][   20/  246]    Overall Loss 0.271989    Objective Loss 0.271989                                        LR 0.000022    Time 0.032576    
2023-01-06 16:41:57,223 - Epoch: [94][   30/  246]    Overall Loss 0.265298    Objective Loss 0.265298                                        LR 0.000022    Time 0.026128    
2023-01-06 16:41:57,370 - Epoch: [94][   40/  246]    Overall Loss 0.266741    Objective Loss 0.266741                                        LR 0.000022    Time 0.023257    
2023-01-06 16:41:57,514 - Epoch: [94][   50/  246]    Overall Loss 0.263352    Objective Loss 0.263352                                        LR 0.000022    Time 0.021472    
2023-01-06 16:41:57,657 - Epoch: [94][   60/  246]    Overall Loss 0.262335    Objective Loss 0.262335                                        LR 0.000022    Time 0.020271    
2023-01-06 16:41:57,801 - Epoch: [94][   70/  246]    Overall Loss 0.260660    Objective Loss 0.260660                                        LR 0.000022    Time 0.019423    
2023-01-06 16:41:57,949 - Epoch: [94][   80/  246]    Overall Loss 0.260860    Objective Loss 0.260860                                        LR 0.000022    Time 0.018835    
2023-01-06 16:41:58,095 - Epoch: [94][   90/  246]    Overall Loss 0.260448    Objective Loss 0.260448                                        LR 0.000022    Time 0.018357    
2023-01-06 16:41:58,239 - Epoch: [94][  100/  246]    Overall Loss 0.260378    Objective Loss 0.260378                                        LR 0.000022    Time 0.017964    
2023-01-06 16:41:58,384 - Epoch: [94][  110/  246]    Overall Loss 0.261606    Objective Loss 0.261606                                        LR 0.000022    Time 0.017642    
2023-01-06 16:41:58,527 - Epoch: [94][  120/  246]    Overall Loss 0.261303    Objective Loss 0.261303                                        LR 0.000022    Time 0.017362    
2023-01-06 16:41:58,671 - Epoch: [94][  130/  246]    Overall Loss 0.262703    Objective Loss 0.262703                                        LR 0.000022    Time 0.017129    
2023-01-06 16:41:58,814 - Epoch: [94][  140/  246]    Overall Loss 0.263961    Objective Loss 0.263961                                        LR 0.000022    Time 0.016924    
2023-01-06 16:41:58,960 - Epoch: [94][  150/  246]    Overall Loss 0.263276    Objective Loss 0.263276                                        LR 0.000022    Time 0.016768    
2023-01-06 16:41:59,110 - Epoch: [94][  160/  246]    Overall Loss 0.264126    Objective Loss 0.264126                                        LR 0.000022    Time 0.016653    
2023-01-06 16:41:59,266 - Epoch: [94][  170/  246]    Overall Loss 0.264023    Objective Loss 0.264023                                        LR 0.000022    Time 0.016586    
2023-01-06 16:41:59,414 - Epoch: [94][  180/  246]    Overall Loss 0.265138    Objective Loss 0.265138                                        LR 0.000022    Time 0.016488    
2023-01-06 16:41:59,566 - Epoch: [94][  190/  246]    Overall Loss 0.264866    Objective Loss 0.264866                                        LR 0.000022    Time 0.016417    
2023-01-06 16:41:59,715 - Epoch: [94][  200/  246]    Overall Loss 0.265248    Objective Loss 0.265248                                        LR 0.000022    Time 0.016335    
2023-01-06 16:41:59,866 - Epoch: [94][  210/  246]    Overall Loss 0.264987    Objective Loss 0.264987                                        LR 0.000022    Time 0.016275    
2023-01-06 16:42:00,010 - Epoch: [94][  220/  246]    Overall Loss 0.265009    Objective Loss 0.265009                                        LR 0.000022    Time 0.016189    
2023-01-06 16:42:00,162 - Epoch: [94][  230/  246]    Overall Loss 0.264555    Objective Loss 0.264555                                        LR 0.000022    Time 0.016143    
2023-01-06 16:42:00,322 - Epoch: [94][  240/  246]    Overall Loss 0.264849    Objective Loss 0.264849                                        LR 0.000022    Time 0.016138    
2023-01-06 16:42:00,388 - Epoch: [94][  246/  246]    Overall Loss 0.264924    Objective Loss 0.264924    Top1 88.038278    LR 0.000022    Time 0.016009    
2023-01-06 16:42:00,537 - --- validate (epoch=94)-----------
2023-01-06 16:42:00,538 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:00,963 - Epoch: [94][   10/   28]    Loss 0.284458    Top1 90.156250    
2023-01-06 16:42:01,062 - Epoch: [94][   20/   28]    Loss 0.276504    Top1 90.253906    
2023-01-06 16:42:01,113 - Epoch: [94][   28/   28]    Loss 0.270464    Top1 90.266247    
2023-01-06 16:42:01,263 - ==> Top1: 90.266    Loss: 0.270

2023-01-06 16:42:01,263 - ==> Confusion:
[[ 222   14  203]
 [  14  256  332]
 [  50   67 5828]]

2023-01-06 16:42:01,264 - ==> Best [Top1: 90.281   Sparsity:0.00   Params: 151104 on epoch: 92]
2023-01-06 16:42:01,264 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:01,269 - 

2023-01-06 16:42:01,270 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:01,932 - Epoch: [95][   10/  246]    Overall Loss 0.260078    Objective Loss 0.260078                                        LR 0.000022    Time 0.066217    
2023-01-06 16:42:02,073 - Epoch: [95][   20/  246]    Overall Loss 0.262818    Objective Loss 0.262818                                        LR 0.000022    Time 0.040131    
2023-01-06 16:42:02,225 - Epoch: [95][   30/  246]    Overall Loss 0.265484    Objective Loss 0.265484                                        LR 0.000022    Time 0.031787    
2023-01-06 16:42:02,373 - Epoch: [95][   40/  246]    Overall Loss 0.266575    Objective Loss 0.266575                                        LR 0.000022    Time 0.027539    
2023-01-06 16:42:02,521 - Epoch: [95][   50/  246]    Overall Loss 0.265612    Objective Loss 0.265612                                        LR 0.000022    Time 0.024984    
2023-01-06 16:42:02,668 - Epoch: [95][   60/  246]    Overall Loss 0.264040    Objective Loss 0.264040                                        LR 0.000022    Time 0.023262    
2023-01-06 16:42:02,821 - Epoch: [95][   70/  246]    Overall Loss 0.263016    Objective Loss 0.263016                                        LR 0.000022    Time 0.022120    
2023-01-06 16:42:02,972 - Epoch: [95][   80/  246]    Overall Loss 0.266152    Objective Loss 0.266152                                        LR 0.000022    Time 0.021239    
2023-01-06 16:42:03,125 - Epoch: [95][   90/  246]    Overall Loss 0.264613    Objective Loss 0.264613                                        LR 0.000022    Time 0.020568    
2023-01-06 16:42:03,275 - Epoch: [95][  100/  246]    Overall Loss 0.264623    Objective Loss 0.264623                                        LR 0.000022    Time 0.020014    
2023-01-06 16:42:03,429 - Epoch: [95][  110/  246]    Overall Loss 0.263007    Objective Loss 0.263007                                        LR 0.000022    Time 0.019577    
2023-01-06 16:42:03,578 - Epoch: [95][  120/  246]    Overall Loss 0.260957    Objective Loss 0.260957                                        LR 0.000022    Time 0.019189    
2023-01-06 16:42:03,727 - Epoch: [95][  130/  246]    Overall Loss 0.261013    Objective Loss 0.261013                                        LR 0.000022    Time 0.018850    
2023-01-06 16:42:03,877 - Epoch: [95][  140/  246]    Overall Loss 0.260586    Objective Loss 0.260586                                        LR 0.000022    Time 0.018575    
2023-01-06 16:42:04,024 - Epoch: [95][  150/  246]    Overall Loss 0.260822    Objective Loss 0.260822                                        LR 0.000022    Time 0.018312    
2023-01-06 16:42:04,164 - Epoch: [95][  160/  246]    Overall Loss 0.260476    Objective Loss 0.260476                                        LR 0.000022    Time 0.018044    
2023-01-06 16:42:04,305 - Epoch: [95][  170/  246]    Overall Loss 0.260817    Objective Loss 0.260817                                        LR 0.000022    Time 0.017812    
2023-01-06 16:42:04,445 - Epoch: [95][  180/  246]    Overall Loss 0.260621    Objective Loss 0.260621                                        LR 0.000022    Time 0.017590    
2023-01-06 16:42:04,586 - Epoch: [95][  190/  246]    Overall Loss 0.260927    Objective Loss 0.260927                                        LR 0.000022    Time 0.017404    
2023-01-06 16:42:04,725 - Epoch: [95][  200/  246]    Overall Loss 0.261260    Objective Loss 0.261260                                        LR 0.000022    Time 0.017223    
2023-01-06 16:42:04,866 - Epoch: [95][  210/  246]    Overall Loss 0.262276    Objective Loss 0.262276                                        LR 0.000022    Time 0.017071    
2023-01-06 16:42:05,005 - Epoch: [95][  220/  246]    Overall Loss 0.262053    Objective Loss 0.262053                                        LR 0.000022    Time 0.016922    
2023-01-06 16:42:05,140 - Epoch: [95][  230/  246]    Overall Loss 0.262957    Objective Loss 0.262957                                        LR 0.000022    Time 0.016774    
2023-01-06 16:42:05,299 - Epoch: [95][  240/  246]    Overall Loss 0.263771    Objective Loss 0.263771                                        LR 0.000022    Time 0.016735    
2023-01-06 16:42:05,366 - Epoch: [95][  246/  246]    Overall Loss 0.263855    Objective Loss 0.263855    Top1 88.755981    LR 0.000022    Time 0.016596    
2023-01-06 16:42:05,494 - --- validate (epoch=95)-----------
2023-01-06 16:42:05,494 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:05,927 - Epoch: [95][   10/   28]    Loss 0.248752    Top1 91.640625    
2023-01-06 16:42:06,023 - Epoch: [95][   20/   28]    Loss 0.262265    Top1 90.644531    
2023-01-06 16:42:06,071 - Epoch: [95][   28/   28]    Loss 0.269157    Top1 90.180361    
2023-01-06 16:42:06,221 - ==> Top1: 90.180    Loss: 0.269

2023-01-06 16:42:06,222 - ==> Confusion:
[[ 218   13  208]
 [  17  240  345]
 [  48   55 5842]]

2023-01-06 16:42:06,223 - ==> Best [Top1: 90.281   Sparsity:0.00   Params: 151104 on epoch: 92]
2023-01-06 16:42:06,223 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:06,228 - 

2023-01-06 16:42:06,228 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:06,765 - Epoch: [96][   10/  246]    Overall Loss 0.228325    Objective Loss 0.228325                                        LR 0.000022    Time 0.053577    
2023-01-06 16:42:06,906 - Epoch: [96][   20/  246]    Overall Loss 0.252998    Objective Loss 0.252998                                        LR 0.000022    Time 0.033852    
2023-01-06 16:42:07,045 - Epoch: [96][   30/  246]    Overall Loss 0.252741    Objective Loss 0.252741                                        LR 0.000022    Time 0.027151    
2023-01-06 16:42:07,182 - Epoch: [96][   40/  246]    Overall Loss 0.250549    Objective Loss 0.250549                                        LR 0.000022    Time 0.023768    
2023-01-06 16:42:07,322 - Epoch: [96][   50/  246]    Overall Loss 0.253491    Objective Loss 0.253491                                        LR 0.000022    Time 0.021794    
2023-01-06 16:42:07,462 - Epoch: [96][   60/  246]    Overall Loss 0.254190    Objective Loss 0.254190                                        LR 0.000022    Time 0.020475    
2023-01-06 16:42:07,600 - Epoch: [96][   70/  246]    Overall Loss 0.257124    Objective Loss 0.257124                                        LR 0.000022    Time 0.019523    
2023-01-06 16:42:07,737 - Epoch: [96][   80/  246]    Overall Loss 0.259013    Objective Loss 0.259013                                        LR 0.000022    Time 0.018792    
2023-01-06 16:42:07,876 - Epoch: [96][   90/  246]    Overall Loss 0.259258    Objective Loss 0.259258                                        LR 0.000022    Time 0.018239    
2023-01-06 16:42:08,013 - Epoch: [96][  100/  246]    Overall Loss 0.263535    Objective Loss 0.263535                                        LR 0.000022    Time 0.017785    
2023-01-06 16:42:08,160 - Epoch: [96][  110/  246]    Overall Loss 0.263123    Objective Loss 0.263123                                        LR 0.000022    Time 0.017503    
2023-01-06 16:42:08,308 - Epoch: [96][  120/  246]    Overall Loss 0.264765    Objective Loss 0.264765                                        LR 0.000022    Time 0.017270    
2023-01-06 16:42:08,450 - Epoch: [96][  130/  246]    Overall Loss 0.264667    Objective Loss 0.264667                                        LR 0.000022    Time 0.017034    
2023-01-06 16:42:08,588 - Epoch: [96][  140/  246]    Overall Loss 0.264413    Objective Loss 0.264413                                        LR 0.000022    Time 0.016797    
2023-01-06 16:42:08,723 - Epoch: [96][  150/  246]    Overall Loss 0.264383    Objective Loss 0.264383                                        LR 0.000022    Time 0.016578    
2023-01-06 16:42:08,863 - Epoch: [96][  160/  246]    Overall Loss 0.263758    Objective Loss 0.263758                                        LR 0.000022    Time 0.016412    
2023-01-06 16:42:08,996 - Epoch: [96][  170/  246]    Overall Loss 0.265107    Objective Loss 0.265107                                        LR 0.000022    Time 0.016229    
2023-01-06 16:42:09,132 - Epoch: [96][  180/  246]    Overall Loss 0.264353    Objective Loss 0.264353                                        LR 0.000022    Time 0.016076    
2023-01-06 16:42:09,267 - Epoch: [96][  190/  246]    Overall Loss 0.265468    Objective Loss 0.265468                                        LR 0.000022    Time 0.015937    
2023-01-06 16:42:09,408 - Epoch: [96][  200/  246]    Overall Loss 0.265330    Objective Loss 0.265330                                        LR 0.000022    Time 0.015844    
2023-01-06 16:42:09,550 - Epoch: [96][  210/  246]    Overall Loss 0.265204    Objective Loss 0.265204                                        LR 0.000022    Time 0.015763    
2023-01-06 16:42:09,691 - Epoch: [96][  220/  246]    Overall Loss 0.264814    Objective Loss 0.264814                                        LR 0.000022    Time 0.015685    
2023-01-06 16:42:09,835 - Epoch: [96][  230/  246]    Overall Loss 0.265019    Objective Loss 0.265019                                        LR 0.000022    Time 0.015628    
2023-01-06 16:42:09,990 - Epoch: [96][  240/  246]    Overall Loss 0.264631    Objective Loss 0.264631                                        LR 0.000022    Time 0.015621    
2023-01-06 16:42:10,057 - Epoch: [96][  246/  246]    Overall Loss 0.264725    Objective Loss 0.264725    Top1 91.387560    LR 0.000022    Time 0.015514    
2023-01-06 16:42:10,191 - --- validate (epoch=96)-----------
2023-01-06 16:42:10,191 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:10,615 - Epoch: [96][   10/   28]    Loss 0.258207    Top1 91.289062    
2023-01-06 16:42:10,712 - Epoch: [96][   20/   28]    Loss 0.267648    Top1 90.585938    
2023-01-06 16:42:10,762 - Epoch: [96][   28/   28]    Loss 0.272031    Top1 90.309190    
2023-01-06 16:42:10,892 - ==> Top1: 90.309    Loss: 0.272

2023-01-06 16:42:10,892 - ==> Confusion:
[[ 211   17  211]
 [  13  252  337]
 [  41   58 5846]]

2023-01-06 16:42:10,893 - ==> Best [Top1: 90.309   Sparsity:0.00   Params: 151104 on epoch: 96]
2023-01-06 16:42:10,893 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:10,900 - 

2023-01-06 16:42:10,900 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:11,550 - Epoch: [97][   10/  246]    Overall Loss 0.262546    Objective Loss 0.262546                                        LR 0.000022    Time 0.064997    
2023-01-06 16:42:11,688 - Epoch: [97][   20/  246]    Overall Loss 0.265712    Objective Loss 0.265712                                        LR 0.000022    Time 0.039350    
2023-01-06 16:42:11,819 - Epoch: [97][   30/  246]    Overall Loss 0.265682    Objective Loss 0.265682                                        LR 0.000022    Time 0.030598    
2023-01-06 16:42:11,958 - Epoch: [97][   40/  246]    Overall Loss 0.266005    Objective Loss 0.266005                                        LR 0.000022    Time 0.026406    
2023-01-06 16:42:12,097 - Epoch: [97][   50/  246]    Overall Loss 0.260189    Objective Loss 0.260189                                        LR 0.000022    Time 0.023901    
2023-01-06 16:42:12,236 - Epoch: [97][   60/  246]    Overall Loss 0.260073    Objective Loss 0.260073                                        LR 0.000022    Time 0.022222    
2023-01-06 16:42:12,374 - Epoch: [97][   70/  246]    Overall Loss 0.263654    Objective Loss 0.263654                                        LR 0.000022    Time 0.021022    
2023-01-06 16:42:12,511 - Epoch: [97][   80/  246]    Overall Loss 0.261492    Objective Loss 0.261492                                        LR 0.000022    Time 0.020103    
2023-01-06 16:42:12,651 - Epoch: [97][   90/  246]    Overall Loss 0.265089    Objective Loss 0.265089                                        LR 0.000022    Time 0.019417    
2023-01-06 16:42:12,788 - Epoch: [97][  100/  246]    Overall Loss 0.263092    Objective Loss 0.263092                                        LR 0.000022    Time 0.018846    
2023-01-06 16:42:12,928 - Epoch: [97][  110/  246]    Overall Loss 0.263599    Objective Loss 0.263599                                        LR 0.000022    Time 0.018400    
2023-01-06 16:42:13,072 - Epoch: [97][  120/  246]    Overall Loss 0.262410    Objective Loss 0.262410                                        LR 0.000022    Time 0.018063    
2023-01-06 16:42:13,204 - Epoch: [97][  130/  246]    Overall Loss 0.262571    Objective Loss 0.262571                                        LR 0.000022    Time 0.017686    
2023-01-06 16:42:13,334 - Epoch: [97][  140/  246]    Overall Loss 0.263408    Objective Loss 0.263408                                        LR 0.000022    Time 0.017346    
2023-01-06 16:42:13,467 - Epoch: [97][  150/  246]    Overall Loss 0.262305    Objective Loss 0.262305                                        LR 0.000022    Time 0.017078    
2023-01-06 16:42:13,601 - Epoch: [97][  160/  246]    Overall Loss 0.262994    Objective Loss 0.262994                                        LR 0.000022    Time 0.016844    
2023-01-06 16:42:13,735 - Epoch: [97][  170/  246]    Overall Loss 0.262910    Objective Loss 0.262910                                        LR 0.000022    Time 0.016642    
2023-01-06 16:42:13,868 - Epoch: [97][  180/  246]    Overall Loss 0.262753    Objective Loss 0.262753                                        LR 0.000022    Time 0.016450    
2023-01-06 16:42:13,999 - Epoch: [97][  190/  246]    Overall Loss 0.262794    Objective Loss 0.262794                                        LR 0.000022    Time 0.016277    
2023-01-06 16:42:14,132 - Epoch: [97][  200/  246]    Overall Loss 0.262989    Objective Loss 0.262989                                        LR 0.000022    Time 0.016125    
2023-01-06 16:42:14,265 - Epoch: [97][  210/  246]    Overall Loss 0.263782    Objective Loss 0.263782                                        LR 0.000022    Time 0.015988    
2023-01-06 16:42:14,398 - Epoch: [97][  220/  246]    Overall Loss 0.263537    Objective Loss 0.263537                                        LR 0.000022    Time 0.015864    
2023-01-06 16:42:14,530 - Epoch: [97][  230/  246]    Overall Loss 0.263735    Objective Loss 0.263735                                        LR 0.000022    Time 0.015749    
2023-01-06 16:42:14,675 - Epoch: [97][  240/  246]    Overall Loss 0.263544    Objective Loss 0.263544                                        LR 0.000022    Time 0.015695    
2023-01-06 16:42:14,736 - Epoch: [97][  246/  246]    Overall Loss 0.263379    Objective Loss 0.263379    Top1 90.191388    LR 0.000022    Time 0.015558    
2023-01-06 16:42:14,891 - --- validate (epoch=97)-----------
2023-01-06 16:42:14,891 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:15,321 - Epoch: [97][   10/   28]    Loss 0.279501    Top1 90.078125    
2023-01-06 16:42:15,416 - Epoch: [97][   20/   28]    Loss 0.285382    Top1 89.687500    
2023-01-06 16:42:15,466 - Epoch: [97][   28/   28]    Loss 0.269548    Top1 90.337818    
2023-01-06 16:42:15,631 - ==> Top1: 90.338    Loss: 0.270

2023-01-06 16:42:15,631 - ==> Confusion:
[[ 234   17  188]
 [  13  271  318]
 [  62   77 5806]]

2023-01-06 16:42:15,632 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 151104 on epoch: 97]
2023-01-06 16:42:15,632 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:15,638 - 

2023-01-06 16:42:15,638 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:16,290 - Epoch: [98][   10/  246]    Overall Loss 0.255851    Objective Loss 0.255851                                        LR 0.000022    Time 0.065086    
2023-01-06 16:42:16,413 - Epoch: [98][   20/  246]    Overall Loss 0.264490    Objective Loss 0.264490                                        LR 0.000022    Time 0.038706    
2023-01-06 16:42:16,536 - Epoch: [98][   30/  246]    Overall Loss 0.266417    Objective Loss 0.266417                                        LR 0.000022    Time 0.029870    
2023-01-06 16:42:16,665 - Epoch: [98][   40/  246]    Overall Loss 0.263787    Objective Loss 0.263787                                        LR 0.000022    Time 0.025632    
2023-01-06 16:42:16,799 - Epoch: [98][   50/  246]    Overall Loss 0.264338    Objective Loss 0.264338                                        LR 0.000022    Time 0.023171    
2023-01-06 16:42:16,937 - Epoch: [98][   60/  246]    Overall Loss 0.266243    Objective Loss 0.266243                                        LR 0.000022    Time 0.021611    
2023-01-06 16:42:17,074 - Epoch: [98][   70/  246]    Overall Loss 0.266524    Objective Loss 0.266524                                        LR 0.000022    Time 0.020464    
2023-01-06 16:42:17,206 - Epoch: [98][   80/  246]    Overall Loss 0.264940    Objective Loss 0.264940                                        LR 0.000022    Time 0.019559    
2023-01-06 16:42:17,341 - Epoch: [98][   90/  246]    Overall Loss 0.262744    Objective Loss 0.262744                                        LR 0.000022    Time 0.018881    
2023-01-06 16:42:17,483 - Epoch: [98][  100/  246]    Overall Loss 0.262816    Objective Loss 0.262816                                        LR 0.000022    Time 0.018408    
2023-01-06 16:42:17,618 - Epoch: [98][  110/  246]    Overall Loss 0.262605    Objective Loss 0.262605                                        LR 0.000022    Time 0.017956    
2023-01-06 16:42:17,750 - Epoch: [98][  120/  246]    Overall Loss 0.262487    Objective Loss 0.262487                                        LR 0.000022    Time 0.017563    
2023-01-06 16:42:17,887 - Epoch: [98][  130/  246]    Overall Loss 0.263403    Objective Loss 0.263403                                        LR 0.000022    Time 0.017258    
2023-01-06 16:42:18,020 - Epoch: [98][  140/  246]    Overall Loss 0.262163    Objective Loss 0.262163                                        LR 0.000022    Time 0.016976    
2023-01-06 16:42:18,157 - Epoch: [98][  150/  246]    Overall Loss 0.263089    Objective Loss 0.263089                                        LR 0.000022    Time 0.016752    
2023-01-06 16:42:18,300 - Epoch: [98][  160/  246]    Overall Loss 0.263292    Objective Loss 0.263292                                        LR 0.000022    Time 0.016599    
2023-01-06 16:42:18,438 - Epoch: [98][  170/  246]    Overall Loss 0.264552    Objective Loss 0.264552                                        LR 0.000022    Time 0.016431    
2023-01-06 16:42:18,573 - Epoch: [98][  180/  246]    Overall Loss 0.264269    Objective Loss 0.264269                                        LR 0.000022    Time 0.016263    
2023-01-06 16:42:18,709 - Epoch: [98][  190/  246]    Overall Loss 0.264133    Objective Loss 0.264133                                        LR 0.000022    Time 0.016122    
2023-01-06 16:42:18,854 - Epoch: [98][  200/  246]    Overall Loss 0.264142    Objective Loss 0.264142                                        LR 0.000022    Time 0.016038    
2023-01-06 16:42:18,995 - Epoch: [98][  210/  246]    Overall Loss 0.263437    Objective Loss 0.263437                                        LR 0.000022    Time 0.015944    
2023-01-06 16:42:19,142 - Epoch: [98][  220/  246]    Overall Loss 0.262890    Objective Loss 0.262890                                        LR 0.000022    Time 0.015872    
2023-01-06 16:42:19,281 - Epoch: [98][  230/  246]    Overall Loss 0.262481    Objective Loss 0.262481                                        LR 0.000022    Time 0.015784    
2023-01-06 16:42:19,430 - Epoch: [98][  240/  246]    Overall Loss 0.263498    Objective Loss 0.263498                                        LR 0.000022    Time 0.015745    
2023-01-06 16:42:19,496 - Epoch: [98][  246/  246]    Overall Loss 0.262889    Objective Loss 0.262889    Top1 92.822967    LR 0.000022    Time 0.015632    
2023-01-06 16:42:19,636 - --- validate (epoch=98)-----------
2023-01-06 16:42:19,636 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:20,071 - Epoch: [98][   10/   28]    Loss 0.289599    Top1 89.335938    
2023-01-06 16:42:20,174 - Epoch: [98][   20/   28]    Loss 0.270640    Top1 90.156250    
2023-01-06 16:42:20,223 - Epoch: [98][   28/   28]    Loss 0.273516    Top1 90.180361    
2023-01-06 16:42:20,389 - ==> Top1: 90.180    Loss: 0.274

2023-01-06 16:42:20,389 - ==> Confusion:
[[ 263   15  161]
 [  21  260  321]
 [  98   70 5777]]

2023-01-06 16:42:20,390 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 151104 on epoch: 97]
2023-01-06 16:42:20,390 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:20,395 - 

2023-01-06 16:42:20,395 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:20,907 - Epoch: [99][   10/  246]    Overall Loss 0.293186    Objective Loss 0.293186                                        LR 0.000022    Time 0.051156    
2023-01-06 16:42:21,039 - Epoch: [99][   20/  246]    Overall Loss 0.275525    Objective Loss 0.275525                                        LR 0.000022    Time 0.032152    
2023-01-06 16:42:21,169 - Epoch: [99][   30/  246]    Overall Loss 0.278844    Objective Loss 0.278844                                        LR 0.000022    Time 0.025735    
2023-01-06 16:42:21,299 - Epoch: [99][   40/  246]    Overall Loss 0.278326    Objective Loss 0.278326                                        LR 0.000022    Time 0.022540    
2023-01-06 16:42:21,444 - Epoch: [99][   50/  246]    Overall Loss 0.271428    Objective Loss 0.271428                                        LR 0.000022    Time 0.020920    
2023-01-06 16:42:21,587 - Epoch: [99][   60/  246]    Overall Loss 0.270507    Objective Loss 0.270507                                        LR 0.000022    Time 0.019819    
2023-01-06 16:42:21,731 - Epoch: [99][   70/  246]    Overall Loss 0.267054    Objective Loss 0.267054                                        LR 0.000022    Time 0.019027    
2023-01-06 16:42:21,875 - Epoch: [99][   80/  246]    Overall Loss 0.264809    Objective Loss 0.264809                                        LR 0.000022    Time 0.018453    
2023-01-06 16:42:22,022 - Epoch: [99][   90/  246]    Overall Loss 0.265943    Objective Loss 0.265943                                        LR 0.000022    Time 0.018022    
2023-01-06 16:42:22,162 - Epoch: [99][  100/  246]    Overall Loss 0.266082    Objective Loss 0.266082                                        LR 0.000022    Time 0.017621    
2023-01-06 16:42:22,309 - Epoch: [99][  110/  246]    Overall Loss 0.265199    Objective Loss 0.265199                                        LR 0.000022    Time 0.017354    
2023-01-06 16:42:22,456 - Epoch: [99][  120/  246]    Overall Loss 0.264788    Objective Loss 0.264788                                        LR 0.000022    Time 0.017124    
2023-01-06 16:42:22,599 - Epoch: [99][  130/  246]    Overall Loss 0.264234    Objective Loss 0.264234                                        LR 0.000022    Time 0.016905    
2023-01-06 16:42:22,727 - Epoch: [99][  140/  246]    Overall Loss 0.263661    Objective Loss 0.263661                                        LR 0.000022    Time 0.016613    
2023-01-06 16:42:22,859 - Epoch: [99][  150/  246]    Overall Loss 0.263543    Objective Loss 0.263543                                        LR 0.000022    Time 0.016381    
2023-01-06 16:42:22,988 - Epoch: [99][  160/  246]    Overall Loss 0.262517    Objective Loss 0.262517                                        LR 0.000022    Time 0.016163    
2023-01-06 16:42:23,127 - Epoch: [99][  170/  246]    Overall Loss 0.261871    Objective Loss 0.261871                                        LR 0.000022    Time 0.016022    
2023-01-06 16:42:23,275 - Epoch: [99][  180/  246]    Overall Loss 0.262335    Objective Loss 0.262335                                        LR 0.000022    Time 0.015957    
2023-01-06 16:42:23,419 - Epoch: [99][  190/  246]    Overall Loss 0.261593    Objective Loss 0.261593                                        LR 0.000022    Time 0.015869    
2023-01-06 16:42:23,567 - Epoch: [99][  200/  246]    Overall Loss 0.262220    Objective Loss 0.262220                                        LR 0.000022    Time 0.015813    
2023-01-06 16:42:23,710 - Epoch: [99][  210/  246]    Overall Loss 0.261594    Objective Loss 0.261594                                        LR 0.000022    Time 0.015742    
2023-01-06 16:42:23,859 - Epoch: [99][  220/  246]    Overall Loss 0.261695    Objective Loss 0.261695                                        LR 0.000022    Time 0.015700    
2023-01-06 16:42:24,003 - Epoch: [99][  230/  246]    Overall Loss 0.262476    Objective Loss 0.262476                                        LR 0.000022    Time 0.015641    
2023-01-06 16:42:24,166 - Epoch: [99][  240/  246]    Overall Loss 0.262554    Objective Loss 0.262554                                        LR 0.000022    Time 0.015668    
2023-01-06 16:42:24,233 - Epoch: [99][  246/  246]    Overall Loss 0.262018    Objective Loss 0.262018    Top1 91.866029    LR 0.000022    Time 0.015558    
2023-01-06 16:42:24,377 - --- validate (epoch=99)-----------
2023-01-06 16:42:24,377 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:24,797 - Epoch: [99][   10/   28]    Loss 0.259448    Top1 90.507812    
2023-01-06 16:42:24,906 - Epoch: [99][   20/   28]    Loss 0.267369    Top1 90.097656    
2023-01-06 16:42:24,955 - Epoch: [99][   28/   28]    Loss 0.275420    Top1 90.137418    
2023-01-06 16:42:25,116 - ==> Top1: 90.137    Loss: 0.275

2023-01-06 16:42:25,117 - ==> Confusion:
[[ 220   13  206]
 [  16  248  338]
 [  49   67 5829]]

2023-01-06 16:42:25,118 - ==> Best [Top1: 90.338   Sparsity:0.00   Params: 151104 on epoch: 97]
2023-01-06 16:42:25,118 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:25,123 - 

2023-01-06 16:42:25,123 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:25,785 - Epoch: [100][   10/  246]    Overall Loss 0.276260    Objective Loss 0.276260                                        LR 0.000013    Time 0.066135    
2023-01-06 16:42:25,928 - Epoch: [100][   20/  246]    Overall Loss 0.266187    Objective Loss 0.266187                                        LR 0.000013    Time 0.040200    
2023-01-06 16:42:26,082 - Epoch: [100][   30/  246]    Overall Loss 0.263416    Objective Loss 0.263416                                        LR 0.000013    Time 0.031920    
2023-01-06 16:42:26,221 - Epoch: [100][   40/  246]    Overall Loss 0.258694    Objective Loss 0.258694                                        LR 0.000013    Time 0.027387    
2023-01-06 16:42:26,360 - Epoch: [100][   50/  246]    Overall Loss 0.258778    Objective Loss 0.258778                                        LR 0.000013    Time 0.024690    
2023-01-06 16:42:26,509 - Epoch: [100][   60/  246]    Overall Loss 0.257500    Objective Loss 0.257500                                        LR 0.000013    Time 0.023048    
2023-01-06 16:42:26,653 - Epoch: [100][   70/  246]    Overall Loss 0.257424    Objective Loss 0.257424                                        LR 0.000013    Time 0.021805    
2023-01-06 16:42:26,798 - Epoch: [100][   80/  246]    Overall Loss 0.258852    Objective Loss 0.258852                                        LR 0.000013    Time 0.020893    
2023-01-06 16:42:26,940 - Epoch: [100][   90/  246]    Overall Loss 0.259844    Objective Loss 0.259844                                        LR 0.000013    Time 0.020149    
2023-01-06 16:42:27,083 - Epoch: [100][  100/  246]    Overall Loss 0.260172    Objective Loss 0.260172                                        LR 0.000013    Time 0.019558    
2023-01-06 16:42:27,222 - Epoch: [100][  110/  246]    Overall Loss 0.261450    Objective Loss 0.261450                                        LR 0.000013    Time 0.019036    
2023-01-06 16:42:27,371 - Epoch: [100][  120/  246]    Overall Loss 0.260548    Objective Loss 0.260548                                        LR 0.000013    Time 0.018687    
2023-01-06 16:42:27,520 - Epoch: [100][  130/  246]    Overall Loss 0.259673    Objective Loss 0.259673                                        LR 0.000013    Time 0.018390    
2023-01-06 16:42:27,671 - Epoch: [100][  140/  246]    Overall Loss 0.260295    Objective Loss 0.260295                                        LR 0.000013    Time 0.018155    
2023-01-06 16:42:27,819 - Epoch: [100][  150/  246]    Overall Loss 0.259835    Objective Loss 0.259835                                        LR 0.000013    Time 0.017930    
2023-01-06 16:42:27,971 - Epoch: [100][  160/  246]    Overall Loss 0.259999    Objective Loss 0.259999                                        LR 0.000013    Time 0.017756    
2023-01-06 16:42:28,122 - Epoch: [100][  170/  246]    Overall Loss 0.259023    Objective Loss 0.259023                                        LR 0.000013    Time 0.017594    
2023-01-06 16:42:28,262 - Epoch: [100][  180/  246]    Overall Loss 0.259160    Objective Loss 0.259160                                        LR 0.000013    Time 0.017393    
2023-01-06 16:42:28,407 - Epoch: [100][  190/  246]    Overall Loss 0.260258    Objective Loss 0.260258                                        LR 0.000013    Time 0.017238    
2023-01-06 16:42:28,548 - Epoch: [100][  200/  246]    Overall Loss 0.259816    Objective Loss 0.259816                                        LR 0.000013    Time 0.017077    
2023-01-06 16:42:28,692 - Epoch: [100][  210/  246]    Overall Loss 0.259662    Objective Loss 0.259662                                        LR 0.000013    Time 0.016949    
2023-01-06 16:42:28,831 - Epoch: [100][  220/  246]    Overall Loss 0.259566    Objective Loss 0.259566                                        LR 0.000013    Time 0.016808    
2023-01-06 16:42:28,970 - Epoch: [100][  230/  246]    Overall Loss 0.259873    Objective Loss 0.259873                                        LR 0.000013    Time 0.016678    
2023-01-06 16:42:29,129 - Epoch: [100][  240/  246]    Overall Loss 0.260107    Objective Loss 0.260107                                        LR 0.000013    Time 0.016647    
2023-01-06 16:42:29,187 - Epoch: [100][  246/  246]    Overall Loss 0.259829    Objective Loss 0.259829    Top1 91.148325    LR 0.000013    Time 0.016473    
2023-01-06 16:42:29,318 - --- validate (epoch=100)-----------
2023-01-06 16:42:29,319 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:29,754 - Epoch: [100][   10/   28]    Loss 0.256496    Top1 91.171875    
2023-01-06 16:42:29,852 - Epoch: [100][   20/   28]    Loss 0.276886    Top1 90.195312    
2023-01-06 16:42:29,901 - Epoch: [100][   28/   28]    Loss 0.273102    Top1 90.352133    
2023-01-06 16:42:30,065 - ==> Top1: 90.352    Loss: 0.273

2023-01-06 16:42:30,065 - ==> Confusion:
[[ 234   15  190]
 [  16  262  324]
 [  55   74 5816]]

2023-01-06 16:42:30,066 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 151104 on epoch: 100]
2023-01-06 16:42:30,066 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:30,073 - 

2023-01-06 16:42:30,073 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:30,731 - Epoch: [101][   10/  246]    Overall Loss 0.251387    Objective Loss 0.251387                                        LR 0.000013    Time 0.065748    
2023-01-06 16:42:30,859 - Epoch: [101][   20/  246]    Overall Loss 0.253387    Objective Loss 0.253387                                        LR 0.000013    Time 0.039235    
2023-01-06 16:42:31,001 - Epoch: [101][   30/  246]    Overall Loss 0.252339    Objective Loss 0.252339                                        LR 0.000013    Time 0.030871    
2023-01-06 16:42:31,139 - Epoch: [101][   40/  246]    Overall Loss 0.255142    Objective Loss 0.255142                                        LR 0.000013    Time 0.026592    
2023-01-06 16:42:31,285 - Epoch: [101][   50/  246]    Overall Loss 0.258296    Objective Loss 0.258296                                        LR 0.000013    Time 0.024199    
2023-01-06 16:42:31,426 - Epoch: [101][   60/  246]    Overall Loss 0.257642    Objective Loss 0.257642                                        LR 0.000013    Time 0.022509    
2023-01-06 16:42:31,566 - Epoch: [101][   70/  246]    Overall Loss 0.260295    Objective Loss 0.260295                                        LR 0.000013    Time 0.021281    
2023-01-06 16:42:31,706 - Epoch: [101][   80/  246]    Overall Loss 0.256895    Objective Loss 0.256895                                        LR 0.000013    Time 0.020369    
2023-01-06 16:42:31,839 - Epoch: [101][   90/  246]    Overall Loss 0.257645    Objective Loss 0.257645                                        LR 0.000013    Time 0.019565    
2023-01-06 16:42:31,968 - Epoch: [101][  100/  246]    Overall Loss 0.259480    Objective Loss 0.259480                                        LR 0.000013    Time 0.018892    
2023-01-06 16:42:32,103 - Epoch: [101][  110/  246]    Overall Loss 0.257837    Objective Loss 0.257837                                        LR 0.000013    Time 0.018399    
2023-01-06 16:42:32,239 - Epoch: [101][  120/  246]    Overall Loss 0.258089    Objective Loss 0.258089                                        LR 0.000013    Time 0.017991    
2023-01-06 16:42:32,374 - Epoch: [101][  130/  246]    Overall Loss 0.257634    Objective Loss 0.257634                                        LR 0.000013    Time 0.017640    
2023-01-06 16:42:32,509 - Epoch: [101][  140/  246]    Overall Loss 0.257322    Objective Loss 0.257322                                        LR 0.000013    Time 0.017343    
2023-01-06 16:42:32,645 - Epoch: [101][  150/  246]    Overall Loss 0.256910    Objective Loss 0.256910                                        LR 0.000013    Time 0.017090    
2023-01-06 16:42:32,778 - Epoch: [101][  160/  246]    Overall Loss 0.257061    Objective Loss 0.257061                                        LR 0.000013    Time 0.016850    
2023-01-06 16:42:32,914 - Epoch: [101][  170/  246]    Overall Loss 0.257695    Objective Loss 0.257695                                        LR 0.000013    Time 0.016659    
2023-01-06 16:42:33,058 - Epoch: [101][  180/  246]    Overall Loss 0.258245    Objective Loss 0.258245                                        LR 0.000013    Time 0.016528    
2023-01-06 16:42:33,212 - Epoch: [101][  190/  246]    Overall Loss 0.259146    Objective Loss 0.259146                                        LR 0.000013    Time 0.016466    
2023-01-06 16:42:33,364 - Epoch: [101][  200/  246]    Overall Loss 0.259028    Objective Loss 0.259028                                        LR 0.000013    Time 0.016403    
2023-01-06 16:42:33,518 - Epoch: [101][  210/  246]    Overall Loss 0.260090    Objective Loss 0.260090                                        LR 0.000013    Time 0.016351    
2023-01-06 16:42:33,668 - Epoch: [101][  220/  246]    Overall Loss 0.260494    Objective Loss 0.260494                                        LR 0.000013    Time 0.016291    
2023-01-06 16:42:33,819 - Epoch: [101][  230/  246]    Overall Loss 0.260758    Objective Loss 0.260758                                        LR 0.000013    Time 0.016236    
2023-01-06 16:42:33,981 - Epoch: [101][  240/  246]    Overall Loss 0.259986    Objective Loss 0.259986                                        LR 0.000013    Time 0.016233    
2023-01-06 16:42:34,052 - Epoch: [101][  246/  246]    Overall Loss 0.259780    Objective Loss 0.259780    Top1 91.387560    LR 0.000013    Time 0.016125    
2023-01-06 16:42:34,190 - --- validate (epoch=101)-----------
2023-01-06 16:42:34,190 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:34,611 - Epoch: [101][   10/   28]    Loss 0.279083    Top1 89.765625    
2023-01-06 16:42:34,708 - Epoch: [101][   20/   28]    Loss 0.281550    Top1 89.609375    
2023-01-06 16:42:34,757 - Epoch: [101][   28/   28]    Loss 0.274759    Top1 89.851131    
2023-01-06 16:42:34,918 - ==> Top1: 89.851    Loss: 0.275

2023-01-06 16:42:34,919 - ==> Confusion:
[[ 195   12  232]
 [  12  229  361]
 [  32   60 5853]]

2023-01-06 16:42:34,920 - ==> Best [Top1: 90.352   Sparsity:0.00   Params: 151104 on epoch: 100]
2023-01-06 16:42:34,920 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:34,925 - 

2023-01-06 16:42:34,925 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:35,442 - Epoch: [102][   10/  246]    Overall Loss 0.235598    Objective Loss 0.235598                                        LR 0.000013    Time 0.051630    
2023-01-06 16:42:35,571 - Epoch: [102][   20/  246]    Overall Loss 0.245848    Objective Loss 0.245848                                        LR 0.000013    Time 0.032229    
2023-01-06 16:42:35,691 - Epoch: [102][   30/  246]    Overall Loss 0.247563    Objective Loss 0.247563                                        LR 0.000013    Time 0.025491    
2023-01-06 16:42:35,815 - Epoch: [102][   40/  246]    Overall Loss 0.251620    Objective Loss 0.251620                                        LR 0.000013    Time 0.022161    
2023-01-06 16:42:35,936 - Epoch: [102][   50/  246]    Overall Loss 0.251730    Objective Loss 0.251730                                        LR 0.000013    Time 0.020142    
2023-01-06 16:42:36,057 - Epoch: [102][   60/  246]    Overall Loss 0.255075    Objective Loss 0.255075                                        LR 0.000013    Time 0.018809    
2023-01-06 16:42:36,182 - Epoch: [102][   70/  246]    Overall Loss 0.257950    Objective Loss 0.257950                                        LR 0.000013    Time 0.017896    
2023-01-06 16:42:36,306 - Epoch: [102][   80/  246]    Overall Loss 0.258881    Objective Loss 0.258881                                        LR 0.000013    Time 0.017182    
2023-01-06 16:42:36,433 - Epoch: [102][   90/  246]    Overall Loss 0.258315    Objective Loss 0.258315                                        LR 0.000013    Time 0.016679    
2023-01-06 16:42:36,557 - Epoch: [102][  100/  246]    Overall Loss 0.259561    Objective Loss 0.259561                                        LR 0.000013    Time 0.016251    
2023-01-06 16:42:36,687 - Epoch: [102][  110/  246]    Overall Loss 0.259242    Objective Loss 0.259242                                        LR 0.000013    Time 0.015951    
2023-01-06 16:42:36,808 - Epoch: [102][  120/  246]    Overall Loss 0.257745    Objective Loss 0.257745                                        LR 0.000013    Time 0.015630    
2023-01-06 16:42:36,936 - Epoch: [102][  130/  246]    Overall Loss 0.258657    Objective Loss 0.258657                                        LR 0.000013    Time 0.015403    
2023-01-06 16:42:37,070 - Epoch: [102][  140/  246]    Overall Loss 0.256647    Objective Loss 0.256647                                        LR 0.000013    Time 0.015258    
2023-01-06 16:42:37,213 - Epoch: [102][  150/  246]    Overall Loss 0.255565    Objective Loss 0.255565                                        LR 0.000013    Time 0.015191    
2023-01-06 16:42:37,354 - Epoch: [102][  160/  246]    Overall Loss 0.255482    Objective Loss 0.255482                                        LR 0.000013    Time 0.015124    
2023-01-06 16:42:37,489 - Epoch: [102][  170/  246]    Overall Loss 0.256954    Objective Loss 0.256954                                        LR 0.000013    Time 0.015022    
2023-01-06 16:42:37,615 - Epoch: [102][  180/  246]    Overall Loss 0.257762    Objective Loss 0.257762                                        LR 0.000013    Time 0.014876    
2023-01-06 16:42:37,743 - Epoch: [102][  190/  246]    Overall Loss 0.258102    Objective Loss 0.258102                                        LR 0.000013    Time 0.014764    
2023-01-06 16:42:37,880 - Epoch: [102][  200/  246]    Overall Loss 0.259148    Objective Loss 0.259148                                        LR 0.000013    Time 0.014701    
2023-01-06 16:42:38,037 - Epoch: [102][  210/  246]    Overall Loss 0.259668    Objective Loss 0.259668                                        LR 0.000013    Time 0.014743    
2023-01-06 16:42:38,188 - Epoch: [102][  220/  246]    Overall Loss 0.259387    Objective Loss 0.259387                                        LR 0.000013    Time 0.014761    
2023-01-06 16:42:38,342 - Epoch: [102][  230/  246]    Overall Loss 0.259740    Objective Loss 0.259740                                        LR 0.000013    Time 0.014783    
2023-01-06 16:42:38,505 - Epoch: [102][  240/  246]    Overall Loss 0.259856    Objective Loss 0.259856                                        LR 0.000013    Time 0.014847    
2023-01-06 16:42:38,569 - Epoch: [102][  246/  246]    Overall Loss 0.259728    Objective Loss 0.259728    Top1 89.234450    LR 0.000013    Time 0.014745    
2023-01-06 16:42:38,702 - --- validate (epoch=102)-----------
2023-01-06 16:42:38,703 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:39,131 - Epoch: [102][   10/   28]    Loss 0.259477    Top1 91.132812    
2023-01-06 16:42:39,226 - Epoch: [102][   20/   28]    Loss 0.272000    Top1 90.312500    
2023-01-06 16:42:39,277 - Epoch: [102][   28/   28]    Loss 0.272110    Top1 90.423705    
2023-01-06 16:42:39,413 - ==> Top1: 90.424    Loss: 0.272

2023-01-06 16:42:39,413 - ==> Confusion:
[[ 239   14  186]
 [  17  248  337]
 [  56   59 5830]]

2023-01-06 16:42:39,414 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:42:39,414 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:39,420 - 

2023-01-06 16:42:39,420 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:40,111 - Epoch: [103][   10/  246]    Overall Loss 0.245911    Objective Loss 0.245911                                        LR 0.000013    Time 0.069025    
2023-01-06 16:42:40,275 - Epoch: [103][   20/  246]    Overall Loss 0.248515    Objective Loss 0.248515                                        LR 0.000013    Time 0.042647    
2023-01-06 16:42:40,439 - Epoch: [103][   30/  246]    Overall Loss 0.253188    Objective Loss 0.253188                                        LR 0.000013    Time 0.033852    
2023-01-06 16:42:40,604 - Epoch: [103][   40/  246]    Overall Loss 0.253596    Objective Loss 0.253596                                        LR 0.000013    Time 0.029525    
2023-01-06 16:42:40,761 - Epoch: [103][   50/  246]    Overall Loss 0.251626    Objective Loss 0.251626                                        LR 0.000013    Time 0.026732    
2023-01-06 16:42:40,905 - Epoch: [103][   60/  246]    Overall Loss 0.250842    Objective Loss 0.250842                                        LR 0.000013    Time 0.024681    
2023-01-06 16:42:41,057 - Epoch: [103][   70/  246]    Overall Loss 0.251637    Objective Loss 0.251637                                        LR 0.000013    Time 0.023312    
2023-01-06 16:42:41,211 - Epoch: [103][   80/  246]    Overall Loss 0.252700    Objective Loss 0.252700                                        LR 0.000013    Time 0.022323    
2023-01-06 16:42:41,366 - Epoch: [103][   90/  246]    Overall Loss 0.251781    Objective Loss 0.251781                                        LR 0.000013    Time 0.021563    
2023-01-06 16:42:41,521 - Epoch: [103][  100/  246]    Overall Loss 0.251455    Objective Loss 0.251455                                        LR 0.000013    Time 0.020952    
2023-01-06 16:42:41,673 - Epoch: [103][  110/  246]    Overall Loss 0.254074    Objective Loss 0.254074                                        LR 0.000013    Time 0.020422    
2023-01-06 16:42:41,827 - Epoch: [103][  120/  246]    Overall Loss 0.254375    Objective Loss 0.254375                                        LR 0.000013    Time 0.020004    
2023-01-06 16:42:41,983 - Epoch: [103][  130/  246]    Overall Loss 0.255056    Objective Loss 0.255056                                        LR 0.000013    Time 0.019662    
2023-01-06 16:42:42,137 - Epoch: [103][  140/  246]    Overall Loss 0.255389    Objective Loss 0.255389                                        LR 0.000013    Time 0.019350    
2023-01-06 16:42:42,292 - Epoch: [103][  150/  246]    Overall Loss 0.254691    Objective Loss 0.254691                                        LR 0.000013    Time 0.019093    
2023-01-06 16:42:42,447 - Epoch: [103][  160/  246]    Overall Loss 0.256210    Objective Loss 0.256210                                        LR 0.000013    Time 0.018864    
2023-01-06 16:42:42,601 - Epoch: [103][  170/  246]    Overall Loss 0.256376    Objective Loss 0.256376                                        LR 0.000013    Time 0.018662    
2023-01-06 16:42:42,755 - Epoch: [103][  180/  246]    Overall Loss 0.257099    Objective Loss 0.257099                                        LR 0.000013    Time 0.018479    
2023-01-06 16:42:42,905 - Epoch: [103][  190/  246]    Overall Loss 0.256843    Objective Loss 0.256843                                        LR 0.000013    Time 0.018291    
2023-01-06 16:42:43,060 - Epoch: [103][  200/  246]    Overall Loss 0.257044    Objective Loss 0.257044                                        LR 0.000013    Time 0.018148    
2023-01-06 16:42:43,206 - Epoch: [103][  210/  246]    Overall Loss 0.257000    Objective Loss 0.257000                                        LR 0.000013    Time 0.017981    
2023-01-06 16:42:43,348 - Epoch: [103][  220/  246]    Overall Loss 0.257576    Objective Loss 0.257576                                        LR 0.000013    Time 0.017807    
2023-01-06 16:42:43,503 - Epoch: [103][  230/  246]    Overall Loss 0.258261    Objective Loss 0.258261                                        LR 0.000013    Time 0.017704    
2023-01-06 16:42:43,675 - Epoch: [103][  240/  246]    Overall Loss 0.258426    Objective Loss 0.258426                                        LR 0.000013    Time 0.017680    
2023-01-06 16:42:43,740 - Epoch: [103][  246/  246]    Overall Loss 0.258693    Objective Loss 0.258693    Top1 90.909091    LR 0.000013    Time 0.017514    
2023-01-06 16:42:43,876 - --- validate (epoch=103)-----------
2023-01-06 16:42:43,876 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:44,306 - Epoch: [103][   10/   28]    Loss 0.285689    Top1 89.375000    
2023-01-06 16:42:44,402 - Epoch: [103][   20/   28]    Loss 0.273607    Top1 90.175781    
2023-01-06 16:42:44,453 - Epoch: [103][   28/   28]    Loss 0.273512    Top1 90.137418    
2023-01-06 16:42:44,616 - ==> Top1: 90.137    Loss: 0.274

2023-01-06 16:42:44,617 - ==> Confusion:
[[ 206   13  220]
 [  14  245  343]
 [  40   59 5846]]

2023-01-06 16:42:44,618 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:42:44,618 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:44,623 - 

2023-01-06 16:42:44,623 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:45,142 - Epoch: [104][   10/  246]    Overall Loss 0.263889    Objective Loss 0.263889                                        LR 0.000013    Time 0.051775    
2023-01-06 16:42:45,287 - Epoch: [104][   20/  246]    Overall Loss 0.268712    Objective Loss 0.268712                                        LR 0.000013    Time 0.033118    
2023-01-06 16:42:45,431 - Epoch: [104][   30/  246]    Overall Loss 0.271368    Objective Loss 0.271368                                        LR 0.000013    Time 0.026885    
2023-01-06 16:42:45,582 - Epoch: [104][   40/  246]    Overall Loss 0.267512    Objective Loss 0.267512                                        LR 0.000013    Time 0.023909    
2023-01-06 16:42:45,735 - Epoch: [104][   50/  246]    Overall Loss 0.262650    Objective Loss 0.262650                                        LR 0.000013    Time 0.022193    
2023-01-06 16:42:45,889 - Epoch: [104][   60/  246]    Overall Loss 0.261942    Objective Loss 0.261942                                        LR 0.000013    Time 0.021051    
2023-01-06 16:42:46,035 - Epoch: [104][   70/  246]    Overall Loss 0.262470    Objective Loss 0.262470                                        LR 0.000013    Time 0.020120    
2023-01-06 16:42:46,181 - Epoch: [104][   80/  246]    Overall Loss 0.262628    Objective Loss 0.262628                                        LR 0.000013    Time 0.019428    
2023-01-06 16:42:46,312 - Epoch: [104][   90/  246]    Overall Loss 0.263099    Objective Loss 0.263099                                        LR 0.000013    Time 0.018720    
2023-01-06 16:42:46,438 - Epoch: [104][  100/  246]    Overall Loss 0.261554    Objective Loss 0.261554                                        LR 0.000013    Time 0.018106    
2023-01-06 16:42:46,564 - Epoch: [104][  110/  246]    Overall Loss 0.262446    Objective Loss 0.262446                                        LR 0.000013    Time 0.017599    
2023-01-06 16:42:46,690 - Epoch: [104][  120/  246]    Overall Loss 0.262388    Objective Loss 0.262388                                        LR 0.000013    Time 0.017182    
2023-01-06 16:42:46,816 - Epoch: [104][  130/  246]    Overall Loss 0.261566    Objective Loss 0.261566                                        LR 0.000013    Time 0.016825    
2023-01-06 16:42:46,941 - Epoch: [104][  140/  246]    Overall Loss 0.262059    Objective Loss 0.262059                                        LR 0.000013    Time 0.016513    
2023-01-06 16:42:47,065 - Epoch: [104][  150/  246]    Overall Loss 0.261971    Objective Loss 0.261971                                        LR 0.000013    Time 0.016234    
2023-01-06 16:42:47,191 - Epoch: [104][  160/  246]    Overall Loss 0.261275    Objective Loss 0.261275                                        LR 0.000013    Time 0.016003    
2023-01-06 16:42:47,321 - Epoch: [104][  170/  246]    Overall Loss 0.261569    Objective Loss 0.261569                                        LR 0.000013    Time 0.015823    
2023-01-06 16:42:47,453 - Epoch: [104][  180/  246]    Overall Loss 0.261543    Objective Loss 0.261543                                        LR 0.000013    Time 0.015675    
2023-01-06 16:42:47,592 - Epoch: [104][  190/  246]    Overall Loss 0.260856    Objective Loss 0.260856                                        LR 0.000013    Time 0.015583    
2023-01-06 16:42:47,736 - Epoch: [104][  200/  246]    Overall Loss 0.260811    Objective Loss 0.260811                                        LR 0.000013    Time 0.015523    
2023-01-06 16:42:47,879 - Epoch: [104][  210/  246]    Overall Loss 0.259852    Objective Loss 0.259852                                        LR 0.000013    Time 0.015459    
2023-01-06 16:42:48,020 - Epoch: [104][  220/  246]    Overall Loss 0.259898    Objective Loss 0.259898                                        LR 0.000013    Time 0.015395    
2023-01-06 16:42:48,162 - Epoch: [104][  230/  246]    Overall Loss 0.259661    Objective Loss 0.259661                                        LR 0.000013    Time 0.015345    
2023-01-06 16:42:48,335 - Epoch: [104][  240/  246]    Overall Loss 0.259238    Objective Loss 0.259238                                        LR 0.000013    Time 0.015421    
2023-01-06 16:42:48,400 - Epoch: [104][  246/  246]    Overall Loss 0.258951    Objective Loss 0.258951    Top1 90.430622    LR 0.000013    Time 0.015309    
2023-01-06 16:42:48,530 - --- validate (epoch=104)-----------
2023-01-06 16:42:48,531 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:48,972 - Epoch: [104][   10/   28]    Loss 0.297982    Top1 89.648438    
2023-01-06 16:42:49,077 - Epoch: [104][   20/   28]    Loss 0.271059    Top1 90.214844    
2023-01-06 16:42:49,125 - Epoch: [104][   28/   28]    Loss 0.271396    Top1 90.237618    
2023-01-06 16:42:49,284 - ==> Top1: 90.238    Loss: 0.271

2023-01-06 16:42:49,285 - ==> Confusion:
[[ 217   14  208]
 [  14  249  339]
 [  43   64 5838]]

2023-01-06 16:42:49,286 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:42:49,286 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:49,291 - 

2023-01-06 16:42:49,291 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:49,967 - Epoch: [105][   10/  246]    Overall Loss 0.262686    Objective Loss 0.262686                                        LR 0.000013    Time 0.067474    
2023-01-06 16:42:50,106 - Epoch: [105][   20/  246]    Overall Loss 0.259320    Objective Loss 0.259320                                        LR 0.000013    Time 0.040671    
2023-01-06 16:42:50,235 - Epoch: [105][   30/  246]    Overall Loss 0.254561    Objective Loss 0.254561                                        LR 0.000013    Time 0.031404    
2023-01-06 16:42:50,356 - Epoch: [105][   40/  246]    Overall Loss 0.248899    Objective Loss 0.248899                                        LR 0.000013    Time 0.026538    
2023-01-06 16:42:50,481 - Epoch: [105][   50/  246]    Overall Loss 0.254081    Objective Loss 0.254081                                        LR 0.000013    Time 0.023710    
2023-01-06 16:42:50,604 - Epoch: [105][   60/  246]    Overall Loss 0.259518    Objective Loss 0.259518                                        LR 0.000013    Time 0.021797    
2023-01-06 16:42:50,732 - Epoch: [105][   70/  246]    Overall Loss 0.259193    Objective Loss 0.259193                                        LR 0.000013    Time 0.020515    
2023-01-06 16:42:50,865 - Epoch: [105][   80/  246]    Overall Loss 0.257968    Objective Loss 0.257968                                        LR 0.000013    Time 0.019603    
2023-01-06 16:42:50,996 - Epoch: [105][   90/  246]    Overall Loss 0.256618    Objective Loss 0.256618                                        LR 0.000013    Time 0.018873    
2023-01-06 16:42:51,119 - Epoch: [105][  100/  246]    Overall Loss 0.258271    Objective Loss 0.258271                                        LR 0.000013    Time 0.018211    
2023-01-06 16:42:51,242 - Epoch: [105][  110/  246]    Overall Loss 0.255475    Objective Loss 0.255475                                        LR 0.000013    Time 0.017673    
2023-01-06 16:42:51,365 - Epoch: [105][  120/  246]    Overall Loss 0.255694    Objective Loss 0.255694                                        LR 0.000013    Time 0.017219    
2023-01-06 16:42:51,487 - Epoch: [105][  130/  246]    Overall Loss 0.256520    Objective Loss 0.256520                                        LR 0.000013    Time 0.016817    
2023-01-06 16:42:51,610 - Epoch: [105][  140/  246]    Overall Loss 0.258495    Objective Loss 0.258495                                        LR 0.000013    Time 0.016493    
2023-01-06 16:42:51,730 - Epoch: [105][  150/  246]    Overall Loss 0.258713    Objective Loss 0.258713                                        LR 0.000013    Time 0.016192    
2023-01-06 16:42:51,854 - Epoch: [105][  160/  246]    Overall Loss 0.258936    Objective Loss 0.258936                                        LR 0.000013    Time 0.015951    
2023-01-06 16:42:51,979 - Epoch: [105][  170/  246]    Overall Loss 0.259798    Objective Loss 0.259798                                        LR 0.000013    Time 0.015744    
2023-01-06 16:42:52,100 - Epoch: [105][  180/  246]    Overall Loss 0.260210    Objective Loss 0.260210                                        LR 0.000013    Time 0.015540    
2023-01-06 16:42:52,218 - Epoch: [105][  190/  246]    Overall Loss 0.259964    Objective Loss 0.259964                                        LR 0.000013    Time 0.015338    
2023-01-06 16:42:52,341 - Epoch: [105][  200/  246]    Overall Loss 0.259123    Objective Loss 0.259123                                        LR 0.000013    Time 0.015175    
2023-01-06 16:42:52,464 - Epoch: [105][  210/  246]    Overall Loss 0.259479    Objective Loss 0.259479                                        LR 0.000013    Time 0.015030    
2023-01-06 16:42:52,591 - Epoch: [105][  220/  246]    Overall Loss 0.259915    Objective Loss 0.259915                                        LR 0.000013    Time 0.014923    
2023-01-06 16:42:52,719 - Epoch: [105][  230/  246]    Overall Loss 0.259743    Objective Loss 0.259743                                        LR 0.000013    Time 0.014830    
2023-01-06 16:42:52,857 - Epoch: [105][  240/  246]    Overall Loss 0.259789    Objective Loss 0.259789                                        LR 0.000013    Time 0.014784    
2023-01-06 16:42:52,918 - Epoch: [105][  246/  246]    Overall Loss 0.259785    Objective Loss 0.259785    Top1 89.234450    LR 0.000013    Time 0.014671    
2023-01-06 16:42:53,046 - --- validate (epoch=105)-----------
2023-01-06 16:42:53,046 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:53,466 - Epoch: [105][   10/   28]    Loss 0.273423    Top1 90.195312    
2023-01-06 16:42:53,566 - Epoch: [105][   20/   28]    Loss 0.277389    Top1 90.039062    
2023-01-06 16:42:53,616 - Epoch: [105][   28/   28]    Loss 0.272153    Top1 90.208989    
2023-01-06 16:42:53,772 - ==> Top1: 90.209    Loss: 0.272

2023-01-06 16:42:53,772 - ==> Confusion:
[[ 230   14  195]
 [  13  225  364]
 [  54   44 5847]]

2023-01-06 16:42:53,774 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:42:53,774 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:53,779 - 

2023-01-06 16:42:53,779 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:54,320 - Epoch: [106][   10/  246]    Overall Loss 0.243398    Objective Loss 0.243398                                        LR 0.000013    Time 0.054072    
2023-01-06 16:42:54,457 - Epoch: [106][   20/  246]    Overall Loss 0.260800    Objective Loss 0.260800                                        LR 0.000013    Time 0.033838    
2023-01-06 16:42:54,603 - Epoch: [106][   30/  246]    Overall Loss 0.257673    Objective Loss 0.257673                                        LR 0.000013    Time 0.027415    
2023-01-06 16:42:54,759 - Epoch: [106][   40/  246]    Overall Loss 0.258461    Objective Loss 0.258461                                        LR 0.000013    Time 0.024439    
2023-01-06 16:42:54,911 - Epoch: [106][   50/  246]    Overall Loss 0.256150    Objective Loss 0.256150                                        LR 0.000013    Time 0.022582    
2023-01-06 16:42:55,065 - Epoch: [106][   60/  246]    Overall Loss 0.253525    Objective Loss 0.253525                                        LR 0.000013    Time 0.021375    
2023-01-06 16:42:55,217 - Epoch: [106][   70/  246]    Overall Loss 0.252610    Objective Loss 0.252610                                        LR 0.000013    Time 0.020496    
2023-01-06 16:42:55,369 - Epoch: [106][   80/  246]    Overall Loss 0.253000    Objective Loss 0.253000                                        LR 0.000013    Time 0.019830    
2023-01-06 16:42:55,521 - Epoch: [106][   90/  246]    Overall Loss 0.253884    Objective Loss 0.253884                                        LR 0.000013    Time 0.019300    
2023-01-06 16:42:55,669 - Epoch: [106][  100/  246]    Overall Loss 0.254292    Objective Loss 0.254292                                        LR 0.000013    Time 0.018852    
2023-01-06 16:42:55,818 - Epoch: [106][  110/  246]    Overall Loss 0.253297    Objective Loss 0.253297                                        LR 0.000013    Time 0.018491    
2023-01-06 16:42:55,960 - Epoch: [106][  120/  246]    Overall Loss 0.252797    Objective Loss 0.252797                                        LR 0.000013    Time 0.018127    
2023-01-06 16:42:56,092 - Epoch: [106][  130/  246]    Overall Loss 0.253814    Objective Loss 0.253814                                        LR 0.000013    Time 0.017734    
2023-01-06 16:42:56,227 - Epoch: [106][  140/  246]    Overall Loss 0.254324    Objective Loss 0.254324                                        LR 0.000013    Time 0.017425    
2023-01-06 16:42:56,352 - Epoch: [106][  150/  246]    Overall Loss 0.255629    Objective Loss 0.255629                                        LR 0.000013    Time 0.017098    
2023-01-06 16:42:56,477 - Epoch: [106][  160/  246]    Overall Loss 0.255867    Objective Loss 0.255867                                        LR 0.000013    Time 0.016803    
2023-01-06 16:42:56,603 - Epoch: [106][  170/  246]    Overall Loss 0.257436    Objective Loss 0.257436                                        LR 0.000013    Time 0.016558    
2023-01-06 16:42:56,729 - Epoch: [106][  180/  246]    Overall Loss 0.258667    Objective Loss 0.258667                                        LR 0.000013    Time 0.016332    
2023-01-06 16:42:56,854 - Epoch: [106][  190/  246]    Overall Loss 0.258910    Objective Loss 0.258910                                        LR 0.000013    Time 0.016131    
2023-01-06 16:42:56,974 - Epoch: [106][  200/  246]    Overall Loss 0.259160    Objective Loss 0.259160                                        LR 0.000013    Time 0.015919    
2023-01-06 16:42:57,098 - Epoch: [106][  210/  246]    Overall Loss 0.260041    Objective Loss 0.260041                                        LR 0.000013    Time 0.015753    
2023-01-06 16:42:57,222 - Epoch: [106][  220/  246]    Overall Loss 0.259601    Objective Loss 0.259601                                        LR 0.000013    Time 0.015599    
2023-01-06 16:42:57,348 - Epoch: [106][  230/  246]    Overall Loss 0.260016    Objective Loss 0.260016                                        LR 0.000013    Time 0.015468    
2023-01-06 16:42:57,489 - Epoch: [106][  240/  246]    Overall Loss 0.259123    Objective Loss 0.259123                                        LR 0.000013    Time 0.015408    
2023-01-06 16:42:57,552 - Epoch: [106][  246/  246]    Overall Loss 0.258761    Objective Loss 0.258761    Top1 93.062201    LR 0.000013    Time 0.015287    
2023-01-06 16:42:57,690 - --- validate (epoch=106)-----------
2023-01-06 16:42:57,691 - 6986 samples (256 per mini-batch)
2023-01-06 16:42:58,229 - Epoch: [106][   10/   28]    Loss 0.268380    Top1 90.273438    
2023-01-06 16:42:58,324 - Epoch: [106][   20/   28]    Loss 0.269909    Top1 90.292969    
2023-01-06 16:42:58,375 - Epoch: [106][   28/   28]    Loss 0.270606    Top1 90.180361    
2023-01-06 16:42:58,536 - ==> Top1: 90.180    Loss: 0.271

2023-01-06 16:42:58,536 - ==> Confusion:
[[ 224   15  200]
 [  14  233  355]
 [  49   53 5843]]

2023-01-06 16:42:58,537 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:42:58,537 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:42:58,542 - 

2023-01-06 16:42:58,542 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:42:59,068 - Epoch: [107][   10/  246]    Overall Loss 0.261300    Objective Loss 0.261300                                        LR 0.000013    Time 0.052482    
2023-01-06 16:42:59,204 - Epoch: [107][   20/  246]    Overall Loss 0.266602    Objective Loss 0.266602                                        LR 0.000013    Time 0.033012    
2023-01-06 16:42:59,343 - Epoch: [107][   30/  246]    Overall Loss 0.263421    Objective Loss 0.263421                                        LR 0.000013    Time 0.026644    
2023-01-06 16:42:59,472 - Epoch: [107][   40/  246]    Overall Loss 0.260913    Objective Loss 0.260913                                        LR 0.000013    Time 0.023203    
2023-01-06 16:42:59,606 - Epoch: [107][   50/  246]    Overall Loss 0.257880    Objective Loss 0.257880                                        LR 0.000013    Time 0.021228    
2023-01-06 16:42:59,739 - Epoch: [107][   60/  246]    Overall Loss 0.261369    Objective Loss 0.261369                                        LR 0.000013    Time 0.019904    
2023-01-06 16:42:59,876 - Epoch: [107][   70/  246]    Overall Loss 0.260251    Objective Loss 0.260251                                        LR 0.000013    Time 0.019004    
2023-01-06 16:43:00,016 - Epoch: [107][   80/  246]    Overall Loss 0.259081    Objective Loss 0.259081                                        LR 0.000013    Time 0.018379    
2023-01-06 16:43:00,149 - Epoch: [107][   90/  246]    Overall Loss 0.259064    Objective Loss 0.259064                                        LR 0.000013    Time 0.017807    
2023-01-06 16:43:00,281 - Epoch: [107][  100/  246]    Overall Loss 0.258846    Objective Loss 0.258846                                        LR 0.000013    Time 0.017339    
2023-01-06 16:43:00,417 - Epoch: [107][  110/  246]    Overall Loss 0.260903    Objective Loss 0.260903                                        LR 0.000013    Time 0.016997    
2023-01-06 16:43:00,555 - Epoch: [107][  120/  246]    Overall Loss 0.261795    Objective Loss 0.261795                                        LR 0.000013    Time 0.016727    
2023-01-06 16:43:00,689 - Epoch: [107][  130/  246]    Overall Loss 0.261766    Objective Loss 0.261766                                        LR 0.000013    Time 0.016463    
2023-01-06 16:43:00,835 - Epoch: [107][  140/  246]    Overall Loss 0.261317    Objective Loss 0.261317                                        LR 0.000013    Time 0.016329    
2023-01-06 16:43:00,986 - Epoch: [107][  150/  246]    Overall Loss 0.260867    Objective Loss 0.260867                                        LR 0.000013    Time 0.016246    
2023-01-06 16:43:01,137 - Epoch: [107][  160/  246]    Overall Loss 0.259371    Objective Loss 0.259371                                        LR 0.000013    Time 0.016173    
2023-01-06 16:43:01,283 - Epoch: [107][  170/  246]    Overall Loss 0.259421    Objective Loss 0.259421                                        LR 0.000013    Time 0.016078    
2023-01-06 16:43:01,434 - Epoch: [107][  180/  246]    Overall Loss 0.258595    Objective Loss 0.258595                                        LR 0.000013    Time 0.016023    
2023-01-06 16:43:01,587 - Epoch: [107][  190/  246]    Overall Loss 0.259011    Objective Loss 0.259011                                        LR 0.000013    Time 0.015980    
2023-01-06 16:43:01,736 - Epoch: [107][  200/  246]    Overall Loss 0.258649    Objective Loss 0.258649                                        LR 0.000013    Time 0.015922    
2023-01-06 16:43:01,888 - Epoch: [107][  210/  246]    Overall Loss 0.259025    Objective Loss 0.259025                                        LR 0.000013    Time 0.015886    
2023-01-06 16:43:02,037 - Epoch: [107][  220/  246]    Overall Loss 0.259209    Objective Loss 0.259209                                        LR 0.000013    Time 0.015842    
2023-01-06 16:43:02,176 - Epoch: [107][  230/  246]    Overall Loss 0.258485    Objective Loss 0.258485                                        LR 0.000013    Time 0.015754    
2023-01-06 16:43:02,331 - Epoch: [107][  240/  246]    Overall Loss 0.258462    Objective Loss 0.258462                                        LR 0.000013    Time 0.015742    
2023-01-06 16:43:02,397 - Epoch: [107][  246/  246]    Overall Loss 0.258112    Objective Loss 0.258112    Top1 92.344498    LR 0.000013    Time 0.015626    
2023-01-06 16:43:02,521 - --- validate (epoch=107)-----------
2023-01-06 16:43:02,521 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:02,945 - Epoch: [107][   10/   28]    Loss 0.256964    Top1 90.898438    
2023-01-06 16:43:03,039 - Epoch: [107][   20/   28]    Loss 0.262354    Top1 90.683594    
2023-01-06 16:43:03,090 - Epoch: [107][   28/   28]    Loss 0.271803    Top1 90.309190    
2023-01-06 16:43:03,228 - ==> Top1: 90.309    Loss: 0.272

2023-01-06 16:43:03,229 - ==> Confusion:
[[ 228   13  198]
 [  16  239  347]
 [  46   57 5842]]

2023-01-06 16:43:03,230 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:43:03,230 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:03,235 - 

2023-01-06 16:43:03,235 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:03,887 - Epoch: [108][   10/  246]    Overall Loss 0.235729    Objective Loss 0.235729                                        LR 0.000013    Time 0.065160    
2023-01-06 16:43:04,027 - Epoch: [108][   20/  246]    Overall Loss 0.252971    Objective Loss 0.252971                                        LR 0.000013    Time 0.039574    
2023-01-06 16:43:04,158 - Epoch: [108][   30/  246]    Overall Loss 0.249664    Objective Loss 0.249664                                        LR 0.000013    Time 0.030725    
2023-01-06 16:43:04,294 - Epoch: [108][   40/  246]    Overall Loss 0.253486    Objective Loss 0.253486                                        LR 0.000013    Time 0.026438    
2023-01-06 16:43:04,428 - Epoch: [108][   50/  246]    Overall Loss 0.255011    Objective Loss 0.255011                                        LR 0.000013    Time 0.023802    
2023-01-06 16:43:04,568 - Epoch: [108][   60/  246]    Overall Loss 0.252226    Objective Loss 0.252226                                        LR 0.000013    Time 0.022154    
2023-01-06 16:43:04,700 - Epoch: [108][   70/  246]    Overall Loss 0.251875    Objective Loss 0.251875                                        LR 0.000013    Time 0.020868    
2023-01-06 16:43:04,839 - Epoch: [108][   80/  246]    Overall Loss 0.251954    Objective Loss 0.251954                                        LR 0.000013    Time 0.019992    
2023-01-06 16:43:04,973 - Epoch: [108][   90/  246]    Overall Loss 0.251729    Objective Loss 0.251729                                        LR 0.000013    Time 0.019263    
2023-01-06 16:43:05,102 - Epoch: [108][  100/  246]    Overall Loss 0.253383    Objective Loss 0.253383                                        LR 0.000013    Time 0.018615    
2023-01-06 16:43:05,229 - Epoch: [108][  110/  246]    Overall Loss 0.252810    Objective Loss 0.252810                                        LR 0.000013    Time 0.018078    
2023-01-06 16:43:05,361 - Epoch: [108][  120/  246]    Overall Loss 0.252205    Objective Loss 0.252205                                        LR 0.000013    Time 0.017666    
2023-01-06 16:43:05,491 - Epoch: [108][  130/  246]    Overall Loss 0.252759    Objective Loss 0.252759                                        LR 0.000013    Time 0.017301    
2023-01-06 16:43:05,623 - Epoch: [108][  140/  246]    Overall Loss 0.252499    Objective Loss 0.252499                                        LR 0.000013    Time 0.017005    
2023-01-06 16:43:05,754 - Epoch: [108][  150/  246]    Overall Loss 0.253288    Objective Loss 0.253288                                        LR 0.000013    Time 0.016742    
2023-01-06 16:43:05,885 - Epoch: [108][  160/  246]    Overall Loss 0.253880    Objective Loss 0.253880                                        LR 0.000013    Time 0.016512    
2023-01-06 16:43:06,017 - Epoch: [108][  170/  246]    Overall Loss 0.253700    Objective Loss 0.253700                                        LR 0.000013    Time 0.016310    
2023-01-06 16:43:06,148 - Epoch: [108][  180/  246]    Overall Loss 0.253833    Objective Loss 0.253833                                        LR 0.000013    Time 0.016131    
2023-01-06 16:43:06,277 - Epoch: [108][  190/  246]    Overall Loss 0.253971    Objective Loss 0.253971                                        LR 0.000013    Time 0.015956    
2023-01-06 16:43:06,405 - Epoch: [108][  200/  246]    Overall Loss 0.254318    Objective Loss 0.254318                                        LR 0.000013    Time 0.015791    
2023-01-06 16:43:06,539 - Epoch: [108][  210/  246]    Overall Loss 0.255178    Objective Loss 0.255178                                        LR 0.000013    Time 0.015673    
2023-01-06 16:43:06,662 - Epoch: [108][  220/  246]    Overall Loss 0.256183    Objective Loss 0.256183                                        LR 0.000013    Time 0.015520    
2023-01-06 16:43:06,795 - Epoch: [108][  230/  246]    Overall Loss 0.256634    Objective Loss 0.256634                                        LR 0.000013    Time 0.015422    
2023-01-06 16:43:06,943 - Epoch: [108][  240/  246]    Overall Loss 0.257472    Objective Loss 0.257472                                        LR 0.000013    Time 0.015393    
2023-01-06 16:43:07,004 - Epoch: [108][  246/  246]    Overall Loss 0.257698    Objective Loss 0.257698    Top1 90.430622    LR 0.000013    Time 0.015265    
2023-01-06 16:43:07,158 - --- validate (epoch=108)-----------
2023-01-06 16:43:07,158 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:07,594 - Epoch: [108][   10/   28]    Loss 0.285438    Top1 89.921875    
2023-01-06 16:43:07,698 - Epoch: [108][   20/   28]    Loss 0.283975    Top1 89.746094    
2023-01-06 16:43:07,747 - Epoch: [108][   28/   28]    Loss 0.272922    Top1 90.051532    
2023-01-06 16:43:07,881 - ==> Top1: 90.052    Loss: 0.273

2023-01-06 16:43:07,881 - ==> Confusion:
[[ 237   15  187]
 [  18  255  329]
 [  72   74 5799]]

2023-01-06 16:43:07,882 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:43:07,882 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:07,887 - 

2023-01-06 16:43:07,888 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:08,421 - Epoch: [109][   10/  246]    Overall Loss 0.277221    Objective Loss 0.277221                                        LR 0.000013    Time 0.053272    
2023-01-06 16:43:08,553 - Epoch: [109][   20/  246]    Overall Loss 0.267568    Objective Loss 0.267568                                        LR 0.000013    Time 0.033229    
2023-01-06 16:43:08,694 - Epoch: [109][   30/  246]    Overall Loss 0.263566    Objective Loss 0.263566                                        LR 0.000013    Time 0.026825    
2023-01-06 16:43:08,839 - Epoch: [109][   40/  246]    Overall Loss 0.259837    Objective Loss 0.259837                                        LR 0.000013    Time 0.023743    
2023-01-06 16:43:08,984 - Epoch: [109][   50/  246]    Overall Loss 0.258747    Objective Loss 0.258747                                        LR 0.000013    Time 0.021874    
2023-01-06 16:43:09,128 - Epoch: [109][   60/  246]    Overall Loss 0.259238    Objective Loss 0.259238                                        LR 0.000013    Time 0.020628    
2023-01-06 16:43:09,279 - Epoch: [109][   70/  246]    Overall Loss 0.258027    Objective Loss 0.258027                                        LR 0.000013    Time 0.019827    
2023-01-06 16:43:09,421 - Epoch: [109][   80/  246]    Overall Loss 0.262350    Objective Loss 0.262350                                        LR 0.000013    Time 0.019127    
2023-01-06 16:43:09,569 - Epoch: [109][   90/  246]    Overall Loss 0.262163    Objective Loss 0.262163                                        LR 0.000013    Time 0.018638    
2023-01-06 16:43:09,718 - Epoch: [109][  100/  246]    Overall Loss 0.261245    Objective Loss 0.261245                                        LR 0.000013    Time 0.018264    
2023-01-06 16:43:09,868 - Epoch: [109][  110/  246]    Overall Loss 0.260507    Objective Loss 0.260507                                        LR 0.000013    Time 0.017957    
2023-01-06 16:43:10,004 - Epoch: [109][  120/  246]    Overall Loss 0.259999    Objective Loss 0.259999                                        LR 0.000013    Time 0.017592    
2023-01-06 16:43:10,144 - Epoch: [109][  130/  246]    Overall Loss 0.259269    Objective Loss 0.259269                                        LR 0.000013    Time 0.017312    
2023-01-06 16:43:10,284 - Epoch: [109][  140/  246]    Overall Loss 0.258659    Objective Loss 0.258659                                        LR 0.000013    Time 0.017072    
2023-01-06 16:43:10,424 - Epoch: [109][  150/  246]    Overall Loss 0.258879    Objective Loss 0.258879                                        LR 0.000013    Time 0.016865    
2023-01-06 16:43:10,560 - Epoch: [109][  160/  246]    Overall Loss 0.258969    Objective Loss 0.258969                                        LR 0.000013    Time 0.016662    
2023-01-06 16:43:10,697 - Epoch: [109][  170/  246]    Overall Loss 0.258447    Objective Loss 0.258447                                        LR 0.000013    Time 0.016475    
2023-01-06 16:43:10,828 - Epoch: [109][  180/  246]    Overall Loss 0.258854    Objective Loss 0.258854                                        LR 0.000013    Time 0.016285    
2023-01-06 16:43:10,967 - Epoch: [109][  190/  246]    Overall Loss 0.257639    Objective Loss 0.257639                                        LR 0.000013    Time 0.016156    
2023-01-06 16:43:11,100 - Epoch: [109][  200/  246]    Overall Loss 0.257291    Objective Loss 0.257291                                        LR 0.000013    Time 0.016014    
2023-01-06 16:43:11,247 - Epoch: [109][  210/  246]    Overall Loss 0.256932    Objective Loss 0.256932                                        LR 0.000013    Time 0.015951    
2023-01-06 16:43:11,395 - Epoch: [109][  220/  246]    Overall Loss 0.256437    Objective Loss 0.256437                                        LR 0.000013    Time 0.015894    
2023-01-06 16:43:11,549 - Epoch: [109][  230/  246]    Overall Loss 0.256383    Objective Loss 0.256383                                        LR 0.000013    Time 0.015873    
2023-01-06 16:43:11,720 - Epoch: [109][  240/  246]    Overall Loss 0.256797    Objective Loss 0.256797                                        LR 0.000013    Time 0.015924    
2023-01-06 16:43:11,786 - Epoch: [109][  246/  246]    Overall Loss 0.257163    Objective Loss 0.257163    Top1 89.712919    LR 0.000013    Time 0.015802    
2023-01-06 16:43:11,929 - --- validate (epoch=109)-----------
2023-01-06 16:43:11,929 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:12,353 - Epoch: [109][   10/   28]    Loss 0.274887    Top1 89.843750    
2023-01-06 16:43:12,449 - Epoch: [109][   20/   28]    Loss 0.265879    Top1 90.136719    
2023-01-06 16:43:12,499 - Epoch: [109][   28/   28]    Loss 0.269709    Top1 90.251932    
2023-01-06 16:43:12,640 - ==> Top1: 90.252    Loss: 0.270

2023-01-06 16:43:12,640 - ==> Confusion:
[[ 238   14  187]
 [  16  259  327]
 [  64   73 5808]]

2023-01-06 16:43:12,641 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:43:12,641 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:12,646 - 

2023-01-06 16:43:12,646 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:13,303 - Epoch: [110][   10/  246]    Overall Loss 0.264082    Objective Loss 0.264082                                        LR 0.000013    Time 0.065613    
2023-01-06 16:43:13,441 - Epoch: [110][   20/  246]    Overall Loss 0.270979    Objective Loss 0.270979                                        LR 0.000013    Time 0.039675    
2023-01-06 16:43:13,580 - Epoch: [110][   30/  246]    Overall Loss 0.268360    Objective Loss 0.268360                                        LR 0.000013    Time 0.031068    
2023-01-06 16:43:13,707 - Epoch: [110][   40/  246]    Overall Loss 0.270177    Objective Loss 0.270177                                        LR 0.000013    Time 0.026454    
2023-01-06 16:43:13,841 - Epoch: [110][   50/  246]    Overall Loss 0.270931    Objective Loss 0.270931                                        LR 0.000013    Time 0.023838    
2023-01-06 16:43:13,975 - Epoch: [110][   60/  246]    Overall Loss 0.270536    Objective Loss 0.270536                                        LR 0.000013    Time 0.022101    
2023-01-06 16:43:14,097 - Epoch: [110][   70/  246]    Overall Loss 0.269555    Objective Loss 0.269555                                        LR 0.000013    Time 0.020672    
2023-01-06 16:43:14,241 - Epoch: [110][   80/  246]    Overall Loss 0.268329    Objective Loss 0.268329                                        LR 0.000013    Time 0.019891    
2023-01-06 16:43:14,391 - Epoch: [110][   90/  246]    Overall Loss 0.264924    Objective Loss 0.264924                                        LR 0.000013    Time 0.019343    
2023-01-06 16:43:14,538 - Epoch: [110][  100/  246]    Overall Loss 0.264330    Objective Loss 0.264330                                        LR 0.000013    Time 0.018870    
2023-01-06 16:43:14,680 - Epoch: [110][  110/  246]    Overall Loss 0.261502    Objective Loss 0.261502                                        LR 0.000013    Time 0.018445    
2023-01-06 16:43:14,828 - Epoch: [110][  120/  246]    Overall Loss 0.260927    Objective Loss 0.260927                                        LR 0.000013    Time 0.018132    
2023-01-06 16:43:14,968 - Epoch: [110][  130/  246]    Overall Loss 0.259187    Objective Loss 0.259187                                        LR 0.000013    Time 0.017810    
2023-01-06 16:43:15,102 - Epoch: [110][  140/  246]    Overall Loss 0.257845    Objective Loss 0.257845                                        LR 0.000013    Time 0.017488    
2023-01-06 16:43:15,223 - Epoch: [110][  150/  246]    Overall Loss 0.259102    Objective Loss 0.259102                                        LR 0.000013    Time 0.017125    
2023-01-06 16:43:15,344 - Epoch: [110][  160/  246]    Overall Loss 0.257992    Objective Loss 0.257992                                        LR 0.000013    Time 0.016807    
2023-01-06 16:43:15,462 - Epoch: [110][  170/  246]    Overall Loss 0.257153    Objective Loss 0.257153                                        LR 0.000013    Time 0.016513    
2023-01-06 16:43:15,586 - Epoch: [110][  180/  246]    Overall Loss 0.256961    Objective Loss 0.256961                                        LR 0.000013    Time 0.016280    
2023-01-06 16:43:15,707 - Epoch: [110][  190/  246]    Overall Loss 0.255828    Objective Loss 0.255828                                        LR 0.000013    Time 0.016059    
2023-01-06 16:43:15,838 - Epoch: [110][  200/  246]    Overall Loss 0.256997    Objective Loss 0.256997                                        LR 0.000013    Time 0.015912    
2023-01-06 16:43:15,983 - Epoch: [110][  210/  246]    Overall Loss 0.257179    Objective Loss 0.257179                                        LR 0.000013    Time 0.015843    
2023-01-06 16:43:16,111 - Epoch: [110][  220/  246]    Overall Loss 0.257250    Objective Loss 0.257250                                        LR 0.000013    Time 0.015699    
2023-01-06 16:43:16,233 - Epoch: [110][  230/  246]    Overall Loss 0.257027    Objective Loss 0.257027                                        LR 0.000013    Time 0.015546    
2023-01-06 16:43:16,388 - Epoch: [110][  240/  246]    Overall Loss 0.257359    Objective Loss 0.257359                                        LR 0.000013    Time 0.015544    
2023-01-06 16:43:16,452 - Epoch: [110][  246/  246]    Overall Loss 0.257537    Objective Loss 0.257537    Top1 92.822967    LR 0.000013    Time 0.015423    
2023-01-06 16:43:16,593 - --- validate (epoch=110)-----------
2023-01-06 16:43:16,593 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:17,017 - Epoch: [110][   10/   28]    Loss 0.259993    Top1 91.093750    
2023-01-06 16:43:17,113 - Epoch: [110][   20/   28]    Loss 0.258660    Top1 90.761719    
2023-01-06 16:43:17,164 - Epoch: [110][   28/   28]    Loss 0.270177    Top1 90.194675    
2023-01-06 16:43:17,291 - ==> Top1: 90.195    Loss: 0.270

2023-01-06 16:43:17,291 - ==> Confusion:
[[ 223   14  202]
 [  13  262  327]
 [  48   81 5816]]

2023-01-06 16:43:17,292 - ==> Best [Top1: 90.424   Sparsity:0.00   Params: 151104 on epoch: 102]
2023-01-06 16:43:17,292 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:17,297 - 

2023-01-06 16:43:17,298 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:17,947 - Epoch: [111][   10/  246]    Overall Loss 0.247005    Objective Loss 0.247005                                        LR 0.000013    Time 0.064930    
2023-01-06 16:43:18,070 - Epoch: [111][   20/  246]    Overall Loss 0.257183    Objective Loss 0.257183                                        LR 0.000013    Time 0.038533    
2023-01-06 16:43:18,197 - Epoch: [111][   30/  246]    Overall Loss 0.256570    Objective Loss 0.256570                                        LR 0.000013    Time 0.029894    
2023-01-06 16:43:18,334 - Epoch: [111][   40/  246]    Overall Loss 0.258036    Objective Loss 0.258036                                        LR 0.000013    Time 0.025851    
2023-01-06 16:43:18,458 - Epoch: [111][   50/  246]    Overall Loss 0.258224    Objective Loss 0.258224                                        LR 0.000013    Time 0.023143    
2023-01-06 16:43:18,573 - Epoch: [111][   60/  246]    Overall Loss 0.258366    Objective Loss 0.258366                                        LR 0.000013    Time 0.021195    
2023-01-06 16:43:18,688 - Epoch: [111][   70/  246]    Overall Loss 0.256205    Objective Loss 0.256205                                        LR 0.000013    Time 0.019812    
2023-01-06 16:43:18,826 - Epoch: [111][   80/  246]    Overall Loss 0.256254    Objective Loss 0.256254                                        LR 0.000013    Time 0.019050    
2023-01-06 16:43:18,959 - Epoch: [111][   90/  246]    Overall Loss 0.255681    Objective Loss 0.255681                                        LR 0.000013    Time 0.018408    
2023-01-06 16:43:19,093 - Epoch: [111][  100/  246]    Overall Loss 0.256648    Objective Loss 0.256648                                        LR 0.000013    Time 0.017905    
2023-01-06 16:43:19,226 - Epoch: [111][  110/  246]    Overall Loss 0.256509    Objective Loss 0.256509                                        LR 0.000013    Time 0.017486    
2023-01-06 16:43:19,363 - Epoch: [111][  120/  246]    Overall Loss 0.255557    Objective Loss 0.255557                                        LR 0.000013    Time 0.017161    
2023-01-06 16:43:19,494 - Epoch: [111][  130/  246]    Overall Loss 0.256257    Objective Loss 0.256257                                        LR 0.000013    Time 0.016851    
2023-01-06 16:43:19,627 - Epoch: [111][  140/  246]    Overall Loss 0.256441    Objective Loss 0.256441                                        LR 0.000013    Time 0.016591    
2023-01-06 16:43:19,764 - Epoch: [111][  150/  246]    Overall Loss 0.256960    Objective Loss 0.256960                                        LR 0.000013    Time 0.016395    
2023-01-06 16:43:19,901 - Epoch: [111][  160/  246]    Overall Loss 0.256810    Objective Loss 0.256810                                        LR 0.000013    Time 0.016228    
2023-01-06 16:43:20,036 - Epoch: [111][  170/  246]    Overall Loss 0.256144    Objective Loss 0.256144                                        LR 0.000013    Time 0.016064    
2023-01-06 16:43:20,170 - Epoch: [111][  180/  246]    Overall Loss 0.257224    Objective Loss 0.257224                                        LR 0.000013    Time 0.015914    
2023-01-06 16:43:20,309 - Epoch: [111][  190/  246]    Overall Loss 0.258371    Objective Loss 0.258371                                        LR 0.000013    Time 0.015804    
2023-01-06 16:43:20,443 - Epoch: [111][  200/  246]    Overall Loss 0.258240    Objective Loss 0.258240                                        LR 0.000013    Time 0.015686    
2023-01-06 16:43:20,578 - Epoch: [111][  210/  246]    Overall Loss 0.257614    Objective Loss 0.257614                                        LR 0.000013    Time 0.015578    
2023-01-06 16:43:20,713 - Epoch: [111][  220/  246]    Overall Loss 0.256728    Objective Loss 0.256728                                        LR 0.000013    Time 0.015484    
2023-01-06 16:43:20,849 - Epoch: [111][  230/  246]    Overall Loss 0.256569    Objective Loss 0.256569                                        LR 0.000013    Time 0.015396    
2023-01-06 16:43:21,001 - Epoch: [111][  240/  246]    Overall Loss 0.257459    Objective Loss 0.257459                                        LR 0.000013    Time 0.015389    
2023-01-06 16:43:21,062 - Epoch: [111][  246/  246]    Overall Loss 0.257861    Objective Loss 0.257861    Top1 90.430622    LR 0.000013    Time 0.015261    
2023-01-06 16:43:21,191 - --- validate (epoch=111)-----------
2023-01-06 16:43:21,191 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:21,620 - Epoch: [111][   10/   28]    Loss 0.275899    Top1 89.960938    
2023-01-06 16:43:21,720 - Epoch: [111][   20/   28]    Loss 0.260912    Top1 90.546875    
2023-01-06 16:43:21,768 - Epoch: [111][   28/   28]    Loss 0.271186    Top1 90.480962    
2023-01-06 16:43:21,903 - ==> Top1: 90.481    Loss: 0.271

2023-01-06 16:43:21,903 - ==> Confusion:
[[ 240   13  186]
 [  17  246  339]
 [  57   53 5835]]

2023-01-06 16:43:21,904 - ==> Best [Top1: 90.481   Sparsity:0.00   Params: 151104 on epoch: 111]
2023-01-06 16:43:21,904 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:21,911 - 

2023-01-06 16:43:21,911 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:22,421 - Epoch: [112][   10/  246]    Overall Loss 0.263265    Objective Loss 0.263265                                        LR 0.000013    Time 0.050944    
2023-01-06 16:43:22,550 - Epoch: [112][   20/  246]    Overall Loss 0.255816    Objective Loss 0.255816                                        LR 0.000013    Time 0.031898    
2023-01-06 16:43:22,679 - Epoch: [112][   30/  246]    Overall Loss 0.262953    Objective Loss 0.262953                                        LR 0.000013    Time 0.025567    
2023-01-06 16:43:22,808 - Epoch: [112][   40/  246]    Overall Loss 0.261954    Objective Loss 0.261954                                        LR 0.000013    Time 0.022371    
2023-01-06 16:43:22,936 - Epoch: [112][   50/  246]    Overall Loss 0.261645    Objective Loss 0.261645                                        LR 0.000013    Time 0.020454    
2023-01-06 16:43:23,053 - Epoch: [112][   60/  246]    Overall Loss 0.260280    Objective Loss 0.260280                                        LR 0.000013    Time 0.018994    
2023-01-06 16:43:23,176 - Epoch: [112][   70/  246]    Overall Loss 0.260776    Objective Loss 0.260776                                        LR 0.000013    Time 0.018029    
2023-01-06 16:43:23,297 - Epoch: [112][   80/  246]    Overall Loss 0.260859    Objective Loss 0.260859                                        LR 0.000013    Time 0.017279    
2023-01-06 16:43:23,420 - Epoch: [112][   90/  246]    Overall Loss 0.260237    Objective Loss 0.260237                                        LR 0.000013    Time 0.016722    
2023-01-06 16:43:23,539 - Epoch: [112][  100/  246]    Overall Loss 0.260535    Objective Loss 0.260535                                        LR 0.000013    Time 0.016237    
2023-01-06 16:43:23,672 - Epoch: [112][  110/  246]    Overall Loss 0.260119    Objective Loss 0.260119                                        LR 0.000013    Time 0.015968    
2023-01-06 16:43:23,806 - Epoch: [112][  120/  246]    Overall Loss 0.259601    Objective Loss 0.259601                                        LR 0.000013    Time 0.015752    
2023-01-06 16:43:23,952 - Epoch: [112][  130/  246]    Overall Loss 0.259331    Objective Loss 0.259331                                        LR 0.000013    Time 0.015661    
2023-01-06 16:43:24,107 - Epoch: [112][  140/  246]    Overall Loss 0.259785    Objective Loss 0.259785                                        LR 0.000013    Time 0.015644    
2023-01-06 16:43:24,260 - Epoch: [112][  150/  246]    Overall Loss 0.259713    Objective Loss 0.259713                                        LR 0.000013    Time 0.015617    
2023-01-06 16:43:24,415 - Epoch: [112][  160/  246]    Overall Loss 0.258695    Objective Loss 0.258695                                        LR 0.000013    Time 0.015608    
2023-01-06 16:43:24,567 - Epoch: [112][  170/  246]    Overall Loss 0.257700    Objective Loss 0.257700                                        LR 0.000013    Time 0.015580    
2023-01-06 16:43:24,722 - Epoch: [112][  180/  246]    Overall Loss 0.257755    Objective Loss 0.257755                                        LR 0.000013    Time 0.015578    
2023-01-06 16:43:24,875 - Epoch: [112][  190/  246]    Overall Loss 0.257256    Objective Loss 0.257256                                        LR 0.000013    Time 0.015560    
2023-01-06 16:43:25,021 - Epoch: [112][  200/  246]    Overall Loss 0.258424    Objective Loss 0.258424                                        LR 0.000013    Time 0.015508    
2023-01-06 16:43:25,163 - Epoch: [112][  210/  246]    Overall Loss 0.257736    Objective Loss 0.257736                                        LR 0.000013    Time 0.015443    
2023-01-06 16:43:25,308 - Epoch: [112][  220/  246]    Overall Loss 0.257772    Objective Loss 0.257772                                        LR 0.000013    Time 0.015399    
2023-01-06 16:43:25,451 - Epoch: [112][  230/  246]    Overall Loss 0.257255    Objective Loss 0.257255                                        LR 0.000013    Time 0.015350    
2023-01-06 16:43:25,607 - Epoch: [112][  240/  246]    Overall Loss 0.257271    Objective Loss 0.257271                                        LR 0.000013    Time 0.015359    
2023-01-06 16:43:25,671 - Epoch: [112][  246/  246]    Overall Loss 0.257198    Objective Loss 0.257198    Top1 89.473684    LR 0.000013    Time 0.015243    
2023-01-06 16:43:25,804 - --- validate (epoch=112)-----------
2023-01-06 16:43:25,804 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:26,228 - Epoch: [112][   10/   28]    Loss 0.262894    Top1 90.585938    
2023-01-06 16:43:26,337 - Epoch: [112][   20/   28]    Loss 0.262347    Top1 90.683594    
2023-01-06 16:43:26,389 - Epoch: [112][   28/   28]    Loss 0.266859    Top1 90.323504    
2023-01-06 16:43:26,531 - ==> Top1: 90.324    Loss: 0.267

2023-01-06 16:43:26,532 - ==> Confusion:
[[ 226   15  198]
 [  17  243  342]
 [  45   59 5841]]

2023-01-06 16:43:26,533 - ==> Best [Top1: 90.481   Sparsity:0.00   Params: 151104 on epoch: 111]
2023-01-06 16:43:26,533 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:26,538 - 

2023-01-06 16:43:26,538 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:27,197 - Epoch: [113][   10/  246]    Overall Loss 0.226976    Objective Loss 0.226976                                        LR 0.000013    Time 0.065890    
2023-01-06 16:43:27,329 - Epoch: [113][   20/  246]    Overall Loss 0.244210    Objective Loss 0.244210                                        LR 0.000013    Time 0.039527    
2023-01-06 16:43:27,481 - Epoch: [113][   30/  246]    Overall Loss 0.252206    Objective Loss 0.252206                                        LR 0.000013    Time 0.031336    
2023-01-06 16:43:27,622 - Epoch: [113][   40/  246]    Overall Loss 0.253709    Objective Loss 0.253709                                        LR 0.000013    Time 0.027007    
2023-01-06 16:43:27,774 - Epoch: [113][   50/  246]    Overall Loss 0.255808    Objective Loss 0.255808                                        LR 0.000013    Time 0.024657    
2023-01-06 16:43:27,911 - Epoch: [113][   60/  246]    Overall Loss 0.255402    Objective Loss 0.255402                                        LR 0.000013    Time 0.022820    
2023-01-06 16:43:28,054 - Epoch: [113][   70/  246]    Overall Loss 0.252786    Objective Loss 0.252786                                        LR 0.000013    Time 0.021598    
2023-01-06 16:43:28,191 - Epoch: [113][   80/  246]    Overall Loss 0.254383    Objective Loss 0.254383                                        LR 0.000013    Time 0.020597    
2023-01-06 16:43:28,336 - Epoch: [113][   90/  246]    Overall Loss 0.255891    Objective Loss 0.255891                                        LR 0.000013    Time 0.019918    
2023-01-06 16:43:28,485 - Epoch: [113][  100/  246]    Overall Loss 0.255069    Objective Loss 0.255069                                        LR 0.000013    Time 0.019415    
2023-01-06 16:43:28,643 - Epoch: [113][  110/  246]    Overall Loss 0.256523    Objective Loss 0.256523                                        LR 0.000013    Time 0.019077    
2023-01-06 16:43:28,795 - Epoch: [113][  120/  246]    Overall Loss 0.255607    Objective Loss 0.255607                                        LR 0.000013    Time 0.018753    
2023-01-06 16:43:28,952 - Epoch: [113][  130/  246]    Overall Loss 0.255404    Objective Loss 0.255404                                        LR 0.000013    Time 0.018517    
2023-01-06 16:43:29,104 - Epoch: [113][  140/  246]    Overall Loss 0.256256    Objective Loss 0.256256                                        LR 0.000013    Time 0.018278    
2023-01-06 16:43:29,250 - Epoch: [113][  150/  246]    Overall Loss 0.257198    Objective Loss 0.257198                                        LR 0.000013    Time 0.018023    
2023-01-06 16:43:29,370 - Epoch: [113][  160/  246]    Overall Loss 0.256874    Objective Loss 0.256874                                        LR 0.000013    Time 0.017648    
2023-01-06 16:43:29,497 - Epoch: [113][  170/  246]    Overall Loss 0.257326    Objective Loss 0.257326                                        LR 0.000013    Time 0.017357    
2023-01-06 16:43:29,620 - Epoch: [113][  180/  246]    Overall Loss 0.257292    Objective Loss 0.257292                                        LR 0.000013    Time 0.017071    
2023-01-06 16:43:29,746 - Epoch: [113][  190/  246]    Overall Loss 0.256498    Objective Loss 0.256498                                        LR 0.000013    Time 0.016832    
2023-01-06 16:43:29,867 - Epoch: [113][  200/  246]    Overall Loss 0.256860    Objective Loss 0.256860                                        LR 0.000013    Time 0.016593    
2023-01-06 16:43:30,000 - Epoch: [113][  210/  246]    Overall Loss 0.257016    Objective Loss 0.257016                                        LR 0.000013    Time 0.016436    
2023-01-06 16:43:30,153 - Epoch: [113][  220/  246]    Overall Loss 0.257031    Objective Loss 0.257031                                        LR 0.000013    Time 0.016380    
2023-01-06 16:43:30,308 - Epoch: [113][  230/  246]    Overall Loss 0.257376    Objective Loss 0.257376                                        LR 0.000013    Time 0.016343    
2023-01-06 16:43:30,474 - Epoch: [113][  240/  246]    Overall Loss 0.257682    Objective Loss 0.257682                                        LR 0.000013    Time 0.016349    
2023-01-06 16:43:30,537 - Epoch: [113][  246/  246]    Overall Loss 0.256929    Objective Loss 0.256929    Top1 93.779904    LR 0.000013    Time 0.016208    
2023-01-06 16:43:30,679 - --- validate (epoch=113)-----------
2023-01-06 16:43:30,679 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:31,098 - Epoch: [113][   10/   28]    Loss 0.287890    Top1 89.453125    
2023-01-06 16:43:31,195 - Epoch: [113][   20/   28]    Loss 0.265158    Top1 90.625000    
2023-01-06 16:43:31,247 - Epoch: [113][   28/   28]    Loss 0.264649    Top1 90.552534    
2023-01-06 16:43:31,379 - ==> Top1: 90.553    Loss: 0.265

2023-01-06 16:43:31,379 - ==> Confusion:
[[ 236   15  188]
 [  14  269  319]
 [  52   72 5821]]

2023-01-06 16:43:31,380 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:31,380 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:31,386 - 

2023-01-06 16:43:31,387 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:31,905 - Epoch: [114][   10/  246]    Overall Loss 0.265774    Objective Loss 0.265774                                        LR 0.000013    Time 0.051827    
2023-01-06 16:43:32,040 - Epoch: [114][   20/  246]    Overall Loss 0.256906    Objective Loss 0.256906                                        LR 0.000013    Time 0.032615    
2023-01-06 16:43:32,182 - Epoch: [114][   30/  246]    Overall Loss 0.258114    Objective Loss 0.258114                                        LR 0.000013    Time 0.026462    
2023-01-06 16:43:32,328 - Epoch: [114][   40/  246]    Overall Loss 0.260294    Objective Loss 0.260294                                        LR 0.000013    Time 0.023464    
2023-01-06 16:43:32,473 - Epoch: [114][   50/  246]    Overall Loss 0.258434    Objective Loss 0.258434                                        LR 0.000013    Time 0.021647    
2023-01-06 16:43:32,617 - Epoch: [114][   60/  246]    Overall Loss 0.258934    Objective Loss 0.258934                                        LR 0.000013    Time 0.020438    
2023-01-06 16:43:32,761 - Epoch: [114][   70/  246]    Overall Loss 0.256099    Objective Loss 0.256099                                        LR 0.000013    Time 0.019577    
2023-01-06 16:43:32,904 - Epoch: [114][   80/  246]    Overall Loss 0.254091    Objective Loss 0.254091                                        LR 0.000013    Time 0.018901    
2023-01-06 16:43:33,051 - Epoch: [114][   90/  246]    Overall Loss 0.254524    Objective Loss 0.254524                                        LR 0.000013    Time 0.018434    
2023-01-06 16:43:33,200 - Epoch: [114][  100/  246]    Overall Loss 0.255418    Objective Loss 0.255418                                        LR 0.000013    Time 0.018082    
2023-01-06 16:43:33,350 - Epoch: [114][  110/  246]    Overall Loss 0.255441    Objective Loss 0.255441                                        LR 0.000013    Time 0.017790    
2023-01-06 16:43:33,492 - Epoch: [114][  120/  246]    Overall Loss 0.255757    Objective Loss 0.255757                                        LR 0.000013    Time 0.017491    
2023-01-06 16:43:33,632 - Epoch: [114][  130/  246]    Overall Loss 0.256433    Objective Loss 0.256433                                        LR 0.000013    Time 0.017220    
2023-01-06 16:43:33,773 - Epoch: [114][  140/  246]    Overall Loss 0.258074    Objective Loss 0.258074                                        LR 0.000013    Time 0.016991    
2023-01-06 16:43:33,914 - Epoch: [114][  150/  246]    Overall Loss 0.257469    Objective Loss 0.257469                                        LR 0.000013    Time 0.016799    
2023-01-06 16:43:34,058 - Epoch: [114][  160/  246]    Overall Loss 0.257194    Objective Loss 0.257194                                        LR 0.000013    Time 0.016647    
2023-01-06 16:43:34,215 - Epoch: [114][  170/  246]    Overall Loss 0.257068    Objective Loss 0.257068                                        LR 0.000013    Time 0.016591    
2023-01-06 16:43:34,351 - Epoch: [114][  180/  246]    Overall Loss 0.255539    Objective Loss 0.255539                                        LR 0.000013    Time 0.016420    
2023-01-06 16:43:34,477 - Epoch: [114][  190/  246]    Overall Loss 0.255093    Objective Loss 0.255093                                        LR 0.000013    Time 0.016217    
2023-01-06 16:43:34,602 - Epoch: [114][  200/  246]    Overall Loss 0.255440    Objective Loss 0.255440                                        LR 0.000013    Time 0.016032    
2023-01-06 16:43:34,731 - Epoch: [114][  210/  246]    Overall Loss 0.256160    Objective Loss 0.256160                                        LR 0.000013    Time 0.015879    
2023-01-06 16:43:34,859 - Epoch: [114][  220/  246]    Overall Loss 0.256508    Objective Loss 0.256508                                        LR 0.000013    Time 0.015738    
2023-01-06 16:43:34,983 - Epoch: [114][  230/  246]    Overall Loss 0.256839    Objective Loss 0.256839                                        LR 0.000013    Time 0.015594    
2023-01-06 16:43:35,120 - Epoch: [114][  240/  246]    Overall Loss 0.256094    Objective Loss 0.256094                                        LR 0.000013    Time 0.015511    
2023-01-06 16:43:35,181 - Epoch: [114][  246/  246]    Overall Loss 0.256878    Objective Loss 0.256878    Top1 88.038278    LR 0.000013    Time 0.015381    
2023-01-06 16:43:35,306 - --- validate (epoch=114)-----------
2023-01-06 16:43:35,306 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:35,732 - Epoch: [114][   10/   28]    Loss 0.269428    Top1 90.781250    
2023-01-06 16:43:35,832 - Epoch: [114][   20/   28]    Loss 0.269232    Top1 90.488281    
2023-01-06 16:43:35,882 - Epoch: [114][   28/   28]    Loss 0.268309    Top1 90.438019    
2023-01-06 16:43:36,005 - ==> Top1: 90.438    Loss: 0.268

2023-01-06 16:43:36,005 - ==> Confusion:
[[ 236   15  188]
 [  16  260  326]
 [  60   63 5822]]

2023-01-06 16:43:36,006 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:36,006 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:36,011 - 

2023-01-06 16:43:36,012 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:36,667 - Epoch: [115][   10/  246]    Overall Loss 0.237888    Objective Loss 0.237888                                        LR 0.000013    Time 0.065446    
2023-01-06 16:43:36,812 - Epoch: [115][   20/  246]    Overall Loss 0.245945    Objective Loss 0.245945                                        LR 0.000013    Time 0.039991    
2023-01-06 16:43:36,958 - Epoch: [115][   30/  246]    Overall Loss 0.247890    Objective Loss 0.247890                                        LR 0.000013    Time 0.031510    
2023-01-06 16:43:37,104 - Epoch: [115][   40/  246]    Overall Loss 0.251283    Objective Loss 0.251283                                        LR 0.000013    Time 0.027275    
2023-01-06 16:43:37,243 - Epoch: [115][   50/  246]    Overall Loss 0.251400    Objective Loss 0.251400                                        LR 0.000013    Time 0.024589    
2023-01-06 16:43:37,391 - Epoch: [115][   60/  246]    Overall Loss 0.251547    Objective Loss 0.251547                                        LR 0.000013    Time 0.022945    
2023-01-06 16:43:37,537 - Epoch: [115][   70/  246]    Overall Loss 0.252602    Objective Loss 0.252602                                        LR 0.000013    Time 0.021758    
2023-01-06 16:43:37,673 - Epoch: [115][   80/  246]    Overall Loss 0.252805    Objective Loss 0.252805                                        LR 0.000013    Time 0.020733    
2023-01-06 16:43:37,818 - Epoch: [115][   90/  246]    Overall Loss 0.251872    Objective Loss 0.251872                                        LR 0.000013    Time 0.020007    
2023-01-06 16:43:37,954 - Epoch: [115][  100/  246]    Overall Loss 0.251631    Objective Loss 0.251631                                        LR 0.000013    Time 0.019367    
2023-01-06 16:43:38,084 - Epoch: [115][  110/  246]    Overall Loss 0.250808    Objective Loss 0.250808                                        LR 0.000013    Time 0.018783    
2023-01-06 16:43:38,210 - Epoch: [115][  120/  246]    Overall Loss 0.252116    Objective Loss 0.252116                                        LR 0.000013    Time 0.018270    
2023-01-06 16:43:38,362 - Epoch: [115][  130/  246]    Overall Loss 0.251704    Objective Loss 0.251704                                        LR 0.000013    Time 0.018029    
2023-01-06 16:43:38,523 - Epoch: [115][  140/  246]    Overall Loss 0.251810    Objective Loss 0.251810                                        LR 0.000013    Time 0.017886    
2023-01-06 16:43:38,681 - Epoch: [115][  150/  246]    Overall Loss 0.251096    Objective Loss 0.251096                                        LR 0.000013    Time 0.017747    
2023-01-06 16:43:38,840 - Epoch: [115][  160/  246]    Overall Loss 0.252316    Objective Loss 0.252316                                        LR 0.000013    Time 0.017630    
2023-01-06 16:43:39,004 - Epoch: [115][  170/  246]    Overall Loss 0.253118    Objective Loss 0.253118                                        LR 0.000013    Time 0.017551    
2023-01-06 16:43:39,164 - Epoch: [115][  180/  246]    Overall Loss 0.253423    Objective Loss 0.253423                                        LR 0.000013    Time 0.017464    
2023-01-06 16:43:39,325 - Epoch: [115][  190/  246]    Overall Loss 0.253793    Objective Loss 0.253793                                        LR 0.000013    Time 0.017389    
2023-01-06 16:43:39,488 - Epoch: [115][  200/  246]    Overall Loss 0.254270    Objective Loss 0.254270                                        LR 0.000013    Time 0.017335    
2023-01-06 16:43:39,650 - Epoch: [115][  210/  246]    Overall Loss 0.255202    Objective Loss 0.255202                                        LR 0.000013    Time 0.017279    
2023-01-06 16:43:39,809 - Epoch: [115][  220/  246]    Overall Loss 0.256082    Objective Loss 0.256082                                        LR 0.000013    Time 0.017217    
2023-01-06 16:43:39,969 - Epoch: [115][  230/  246]    Overall Loss 0.256426    Objective Loss 0.256426                                        LR 0.000013    Time 0.017159    
2023-01-06 16:43:40,140 - Epoch: [115][  240/  246]    Overall Loss 0.256306    Objective Loss 0.256306                                        LR 0.000013    Time 0.017157    
2023-01-06 16:43:40,205 - Epoch: [115][  246/  246]    Overall Loss 0.256391    Objective Loss 0.256391    Top1 92.105263    LR 0.000013    Time 0.017002    
2023-01-06 16:43:40,383 - --- validate (epoch=115)-----------
2023-01-06 16:43:40,383 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:40,829 - Epoch: [115][   10/   28]    Loss 0.270075    Top1 90.507812    
2023-01-06 16:43:40,934 - Epoch: [115][   20/   28]    Loss 0.263522    Top1 90.859375    
2023-01-06 16:43:40,985 - Epoch: [115][   28/   28]    Loss 0.273372    Top1 90.438019    
2023-01-06 16:43:41,120 - ==> Top1: 90.438    Loss: 0.273

2023-01-06 16:43:41,121 - ==> Confusion:
[[ 237   15  187]
 [  15  256  331]
 [  55   65 5825]]

2023-01-06 16:43:41,122 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:41,122 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:41,127 - 

2023-01-06 16:43:41,127 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:41,805 - Epoch: [116][   10/  246]    Overall Loss 0.263625    Objective Loss 0.263625                                        LR 0.000013    Time 0.067755    
2023-01-06 16:43:41,947 - Epoch: [116][   20/  246]    Overall Loss 0.264155    Objective Loss 0.264155                                        LR 0.000013    Time 0.040963    
2023-01-06 16:43:42,073 - Epoch: [116][   30/  246]    Overall Loss 0.263689    Objective Loss 0.263689                                        LR 0.000013    Time 0.031505    
2023-01-06 16:43:42,196 - Epoch: [116][   40/  246]    Overall Loss 0.262806    Objective Loss 0.262806                                        LR 0.000013    Time 0.026694    
2023-01-06 16:43:42,319 - Epoch: [116][   50/  246]    Overall Loss 0.260947    Objective Loss 0.260947                                        LR 0.000013    Time 0.023792    
2023-01-06 16:43:42,443 - Epoch: [116][   60/  246]    Overall Loss 0.256661    Objective Loss 0.256661                                        LR 0.000013    Time 0.021898    
2023-01-06 16:43:42,566 - Epoch: [116][   70/  246]    Overall Loss 0.261020    Objective Loss 0.261020                                        LR 0.000013    Time 0.020512    
2023-01-06 16:43:42,696 - Epoch: [116][   80/  246]    Overall Loss 0.259888    Objective Loss 0.259888                                        LR 0.000013    Time 0.019568    
2023-01-06 16:43:42,833 - Epoch: [116][   90/  246]    Overall Loss 0.258625    Objective Loss 0.258625                                        LR 0.000013    Time 0.018912    
2023-01-06 16:43:42,966 - Epoch: [116][  100/  246]    Overall Loss 0.259541    Objective Loss 0.259541                                        LR 0.000013    Time 0.018345    
2023-01-06 16:43:43,093 - Epoch: [116][  110/  246]    Overall Loss 0.255906    Objective Loss 0.255906                                        LR 0.000013    Time 0.017834    
2023-01-06 16:43:43,227 - Epoch: [116][  120/  246]    Overall Loss 0.256069    Objective Loss 0.256069                                        LR 0.000013    Time 0.017453    
2023-01-06 16:43:43,364 - Epoch: [116][  130/  246]    Overall Loss 0.256314    Objective Loss 0.256314                                        LR 0.000013    Time 0.017161    
2023-01-06 16:43:43,502 - Epoch: [116][  140/  246]    Overall Loss 0.255980    Objective Loss 0.255980                                        LR 0.000013    Time 0.016922    
2023-01-06 16:43:43,643 - Epoch: [116][  150/  246]    Overall Loss 0.256159    Objective Loss 0.256159                                        LR 0.000013    Time 0.016731    
2023-01-06 16:43:43,786 - Epoch: [116][  160/  246]    Overall Loss 0.256382    Objective Loss 0.256382                                        LR 0.000013    Time 0.016573    
2023-01-06 16:43:43,924 - Epoch: [116][  170/  246]    Overall Loss 0.256359    Objective Loss 0.256359                                        LR 0.000013    Time 0.016407    
2023-01-06 16:43:44,062 - Epoch: [116][  180/  246]    Overall Loss 0.256132    Objective Loss 0.256132                                        LR 0.000013    Time 0.016262    
2023-01-06 16:43:44,204 - Epoch: [116][  190/  246]    Overall Loss 0.255774    Objective Loss 0.255774                                        LR 0.000013    Time 0.016145    
2023-01-06 16:43:44,346 - Epoch: [116][  200/  246]    Overall Loss 0.255587    Objective Loss 0.255587                                        LR 0.000013    Time 0.016044    
2023-01-06 16:43:44,487 - Epoch: [116][  210/  246]    Overall Loss 0.255585    Objective Loss 0.255585                                        LR 0.000013    Time 0.015950    
2023-01-06 16:43:44,627 - Epoch: [116][  220/  246]    Overall Loss 0.256626    Objective Loss 0.256626                                        LR 0.000013    Time 0.015858    
2023-01-06 16:43:44,757 - Epoch: [116][  230/  246]    Overall Loss 0.257080    Objective Loss 0.257080                                        LR 0.000013    Time 0.015732    
2023-01-06 16:43:44,904 - Epoch: [116][  240/  246]    Overall Loss 0.256837    Objective Loss 0.256837                                        LR 0.000013    Time 0.015690    
2023-01-06 16:43:44,963 - Epoch: [116][  246/  246]    Overall Loss 0.256444    Objective Loss 0.256444    Top1 92.105263    LR 0.000013    Time 0.015546    
2023-01-06 16:43:45,108 - --- validate (epoch=116)-----------
2023-01-06 16:43:45,108 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:45,534 - Epoch: [116][   10/   28]    Loss 0.274151    Top1 90.351562    
2023-01-06 16:43:45,625 - Epoch: [116][   20/   28]    Loss 0.276104    Top1 90.097656    
2023-01-06 16:43:45,676 - Epoch: [116][   28/   28]    Loss 0.273647    Top1 90.108789    
2023-01-06 16:43:45,837 - ==> Top1: 90.109    Loss: 0.274

2023-01-06 16:43:45,837 - ==> Confusion:
[[ 213   14  212]
 [  10  223  369]
 [  43   43 5859]]

2023-01-06 16:43:45,838 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:45,838 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:45,843 - 

2023-01-06 16:43:45,843 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:46,360 - Epoch: [117][   10/  246]    Overall Loss 0.246902    Objective Loss 0.246902                                        LR 0.000013    Time 0.051579    
2023-01-06 16:43:46,490 - Epoch: [117][   20/  246]    Overall Loss 0.246718    Objective Loss 0.246718                                        LR 0.000013    Time 0.032278    
2023-01-06 16:43:46,628 - Epoch: [117][   30/  246]    Overall Loss 0.251933    Objective Loss 0.251933                                        LR 0.000013    Time 0.026069    
2023-01-06 16:43:46,763 - Epoch: [117][   40/  246]    Overall Loss 0.255398    Objective Loss 0.255398                                        LR 0.000013    Time 0.022903    
2023-01-06 16:43:46,890 - Epoch: [117][   50/  246]    Overall Loss 0.259459    Objective Loss 0.259459                                        LR 0.000013    Time 0.020863    
2023-01-06 16:43:47,018 - Epoch: [117][   60/  246]    Overall Loss 0.259775    Objective Loss 0.259775                                        LR 0.000013    Time 0.019503    
2023-01-06 16:43:47,139 - Epoch: [117][   70/  246]    Overall Loss 0.261727    Objective Loss 0.261727                                        LR 0.000013    Time 0.018421    
2023-01-06 16:43:47,260 - Epoch: [117][   80/  246]    Overall Loss 0.261019    Objective Loss 0.261019                                        LR 0.000013    Time 0.017620    
2023-01-06 16:43:47,383 - Epoch: [117][   90/  246]    Overall Loss 0.260250    Objective Loss 0.260250                                        LR 0.000013    Time 0.017025    
2023-01-06 16:43:47,502 - Epoch: [117][  100/  246]    Overall Loss 0.261020    Objective Loss 0.261020                                        LR 0.000013    Time 0.016507    
2023-01-06 16:43:47,623 - Epoch: [117][  110/  246]    Overall Loss 0.258851    Objective Loss 0.258851                                        LR 0.000013    Time 0.016107    
2023-01-06 16:43:47,744 - Epoch: [117][  120/  246]    Overall Loss 0.259141    Objective Loss 0.259141                                        LR 0.000013    Time 0.015766    
2023-01-06 16:43:47,866 - Epoch: [117][  130/  246]    Overall Loss 0.258717    Objective Loss 0.258717                                        LR 0.000013    Time 0.015485    
2023-01-06 16:43:47,991 - Epoch: [117][  140/  246]    Overall Loss 0.258291    Objective Loss 0.258291                                        LR 0.000013    Time 0.015274    
2023-01-06 16:43:48,115 - Epoch: [117][  150/  246]    Overall Loss 0.259015    Objective Loss 0.259015                                        LR 0.000013    Time 0.015074    
2023-01-06 16:43:48,237 - Epoch: [117][  160/  246]    Overall Loss 0.258256    Objective Loss 0.258256                                        LR 0.000013    Time 0.014895    
2023-01-06 16:43:48,360 - Epoch: [117][  170/  246]    Overall Loss 0.258231    Objective Loss 0.258231                                        LR 0.000013    Time 0.014741    
2023-01-06 16:43:48,482 - Epoch: [117][  180/  246]    Overall Loss 0.258202    Objective Loss 0.258202                                        LR 0.000013    Time 0.014598    
2023-01-06 16:43:48,605 - Epoch: [117][  190/  246]    Overall Loss 0.257789    Objective Loss 0.257789                                        LR 0.000013    Time 0.014471    
2023-01-06 16:43:48,745 - Epoch: [117][  200/  246]    Overall Loss 0.258653    Objective Loss 0.258653                                        LR 0.000013    Time 0.014445    
2023-01-06 16:43:48,878 - Epoch: [117][  210/  246]    Overall Loss 0.258418    Objective Loss 0.258418                                        LR 0.000013    Time 0.014391    
2023-01-06 16:43:49,009 - Epoch: [117][  220/  246]    Overall Loss 0.258199    Objective Loss 0.258199                                        LR 0.000013    Time 0.014331    
2023-01-06 16:43:49,136 - Epoch: [117][  230/  246]    Overall Loss 0.258007    Objective Loss 0.258007                                        LR 0.000013    Time 0.014258    
2023-01-06 16:43:49,285 - Epoch: [117][  240/  246]    Overall Loss 0.257255    Objective Loss 0.257255                                        LR 0.000013    Time 0.014283    
2023-01-06 16:43:49,351 - Epoch: [117][  246/  246]    Overall Loss 0.256591    Objective Loss 0.256591    Top1 90.909091    LR 0.000013    Time 0.014202    
2023-01-06 16:43:49,501 - --- validate (epoch=117)-----------
2023-01-06 16:43:49,501 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:49,935 - Epoch: [117][   10/   28]    Loss 0.282986    Top1 90.078125    
2023-01-06 16:43:50,034 - Epoch: [117][   20/   28]    Loss 0.279451    Top1 89.902344    
2023-01-06 16:43:50,098 - Epoch: [117][   28/   28]    Loss 0.268542    Top1 90.337818    
2023-01-06 16:43:50,225 - ==> Top1: 90.338    Loss: 0.269

2023-01-06 16:43:50,226 - ==> Confusion:
[[ 228   14  197]
 [  16  254  332]
 [  49   67 5829]]

2023-01-06 16:43:50,227 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:50,227 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:50,232 - 

2023-01-06 16:43:50,232 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:50,906 - Epoch: [118][   10/  246]    Overall Loss 0.263008    Objective Loss 0.263008                                        LR 0.000013    Time 0.067335    
2023-01-06 16:43:51,056 - Epoch: [118][   20/  246]    Overall Loss 0.258111    Objective Loss 0.258111                                        LR 0.000013    Time 0.041143    
2023-01-06 16:43:51,197 - Epoch: [118][   30/  246]    Overall Loss 0.259061    Objective Loss 0.259061                                        LR 0.000013    Time 0.032105    
2023-01-06 16:43:51,333 - Epoch: [118][   40/  246]    Overall Loss 0.259433    Objective Loss 0.259433                                        LR 0.000013    Time 0.027471    
2023-01-06 16:43:51,480 - Epoch: [118][   50/  246]    Overall Loss 0.256354    Objective Loss 0.256354                                        LR 0.000013    Time 0.024907    
2023-01-06 16:43:51,626 - Epoch: [118][   60/  246]    Overall Loss 0.257814    Objective Loss 0.257814                                        LR 0.000013    Time 0.023198    
2023-01-06 16:43:51,770 - Epoch: [118][   70/  246]    Overall Loss 0.255824    Objective Loss 0.255824                                        LR 0.000013    Time 0.021933    
2023-01-06 16:43:51,910 - Epoch: [118][   80/  246]    Overall Loss 0.256696    Objective Loss 0.256696                                        LR 0.000013    Time 0.020932    
2023-01-06 16:43:52,047 - Epoch: [118][   90/  246]    Overall Loss 0.256056    Objective Loss 0.256056                                        LR 0.000013    Time 0.020121    
2023-01-06 16:43:52,185 - Epoch: [118][  100/  246]    Overall Loss 0.256540    Objective Loss 0.256540                                        LR 0.000013    Time 0.019485    
2023-01-06 16:43:52,320 - Epoch: [118][  110/  246]    Overall Loss 0.256467    Objective Loss 0.256467                                        LR 0.000013    Time 0.018940    
2023-01-06 16:43:52,456 - Epoch: [118][  120/  246]    Overall Loss 0.257478    Objective Loss 0.257478                                        LR 0.000013    Time 0.018491    
2023-01-06 16:43:52,592 - Epoch: [118][  130/  246]    Overall Loss 0.256320    Objective Loss 0.256320                                        LR 0.000013    Time 0.018110    
2023-01-06 16:43:52,727 - Epoch: [118][  140/  246]    Overall Loss 0.255815    Objective Loss 0.255815                                        LR 0.000013    Time 0.017781    
2023-01-06 16:43:52,862 - Epoch: [118][  150/  246]    Overall Loss 0.255798    Objective Loss 0.255798                                        LR 0.000013    Time 0.017493    
2023-01-06 16:43:52,995 - Epoch: [118][  160/  246]    Overall Loss 0.256976    Objective Loss 0.256976                                        LR 0.000013    Time 0.017229    
2023-01-06 16:43:53,133 - Epoch: [118][  170/  246]    Overall Loss 0.257178    Objective Loss 0.257178                                        LR 0.000013    Time 0.017023    
2023-01-06 16:43:53,269 - Epoch: [118][  180/  246]    Overall Loss 0.256920    Objective Loss 0.256920                                        LR 0.000013    Time 0.016833    
2023-01-06 16:43:53,406 - Epoch: [118][  190/  246]    Overall Loss 0.256723    Objective Loss 0.256723                                        LR 0.000013    Time 0.016662    
2023-01-06 16:43:53,539 - Epoch: [118][  200/  246]    Overall Loss 0.256483    Objective Loss 0.256483                                        LR 0.000013    Time 0.016496    
2023-01-06 16:43:53,676 - Epoch: [118][  210/  246]    Overall Loss 0.256707    Objective Loss 0.256707                                        LR 0.000013    Time 0.016359    
2023-01-06 16:43:53,811 - Epoch: [118][  220/  246]    Overall Loss 0.257001    Objective Loss 0.257001                                        LR 0.000013    Time 0.016226    
2023-01-06 16:43:53,944 - Epoch: [118][  230/  246]    Overall Loss 0.256260    Objective Loss 0.256260                                        LR 0.000013    Time 0.016098    
2023-01-06 16:43:54,097 - Epoch: [118][  240/  246]    Overall Loss 0.256405    Objective Loss 0.256405                                        LR 0.000013    Time 0.016063    
2023-01-06 16:43:54,162 - Epoch: [118][  246/  246]    Overall Loss 0.256342    Objective Loss 0.256342    Top1 89.712919    LR 0.000013    Time 0.015936    
2023-01-06 16:43:54,310 - --- validate (epoch=118)-----------
2023-01-06 16:43:54,310 - 6986 samples (256 per mini-batch)
2023-01-06 16:43:54,742 - Epoch: [118][   10/   28]    Loss 0.282463    Top1 89.765625    
2023-01-06 16:43:54,845 - Epoch: [118][   20/   28]    Loss 0.281587    Top1 89.746094    
2023-01-06 16:43:54,895 - Epoch: [118][   28/   28]    Loss 0.276081    Top1 89.979960    
2023-01-06 16:43:55,058 - ==> Top1: 89.980    Loss: 0.276

2023-01-06 16:43:55,059 - ==> Confusion:
[[ 223   16  200]
 [  16  255  331]
 [  62   75 5808]]

2023-01-06 16:43:55,060 - ==> Best [Top1: 90.553   Sparsity:0.00   Params: 151104 on epoch: 113]
2023-01-06 16:43:55,060 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:43:55,065 - 

2023-01-06 16:43:55,065 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:43:55,592 - Epoch: [119][   10/  246]    Overall Loss 0.246969    Objective Loss 0.246969                                        LR 0.000013    Time 0.052619    
2023-01-06 16:43:55,737 - Epoch: [119][   20/  246]    Overall Loss 0.248541    Objective Loss 0.248541                                        LR 0.000013    Time 0.033536    
2023-01-06 16:43:55,878 - Epoch: [119][   30/  246]    Overall Loss 0.250021    Objective Loss 0.250021                                        LR 0.000013    Time 0.027062    
2023-01-06 16:43:56,026 - Epoch: [119][   40/  246]    Overall Loss 0.249563    Objective Loss 0.249563                                        LR 0.000013    Time 0.023971    
2023-01-06 16:43:56,154 - Epoch: [119][   50/  246]    Overall Loss 0.251489    Objective Loss 0.251489                                        LR 0.000013    Time 0.021738    
2023-01-06 16:43:56,302 - Epoch: [119][   60/  246]    Overall Loss 0.253193    Objective Loss 0.253193                                        LR 0.000013    Time 0.020562    
2023-01-06 16:43:56,445 - Epoch: [119][   70/  246]    Overall Loss 0.252058    Objective Loss 0.252058                                        LR 0.000013    Time 0.019666    
2023-01-06 16:43:56,589 - Epoch: [119][   80/  246]    Overall Loss 0.255475    Objective Loss 0.255475                                        LR 0.000013    Time 0.019006    
2023-01-06 16:43:56,730 - Epoch: [119][   90/  246]    Overall Loss 0.255286    Objective Loss 0.255286                                        LR 0.000013    Time 0.018451    
2023-01-06 16:43:56,871 - Epoch: [119][  100/  246]    Overall Loss 0.254353    Objective Loss 0.254353                                        LR 0.000013    Time 0.018018    
2023-01-06 16:43:57,004 - Epoch: [119][  110/  246]    Overall Loss 0.254004    Objective Loss 0.254004                                        LR 0.000013    Time 0.017589    
2023-01-06 16:43:57,146 - Epoch: [119][  120/  246]    Overall Loss 0.252907    Objective Loss 0.252907                                        LR 0.000013    Time 0.017304    
2023-01-06 16:43:57,283 - Epoch: [119][  130/  246]    Overall Loss 0.253925    Objective Loss 0.253925                                        LR 0.000013    Time 0.017020    
2023-01-06 16:43:57,435 - Epoch: [119][  140/  246]    Overall Loss 0.253786    Objective Loss 0.253786                                        LR 0.000013    Time 0.016889    
2023-01-06 16:43:57,602 - Epoch: [119][  150/  246]    Overall Loss 0.254075    Objective Loss 0.254075                                        LR 0.000013    Time 0.016873    
2023-01-06 16:43:57,783 - Epoch: [119][  160/  246]    Overall Loss 0.253881    Objective Loss 0.253881                                        LR 0.000013    Time 0.016947    
2023-01-06 16:43:57,960 - Epoch: [119][  170/  246]    Overall Loss 0.254529    Objective Loss 0.254529                                        LR 0.000013    Time 0.016991    
2023-01-06 16:43:58,143 - Epoch: [119][  180/  246]    Overall Loss 0.253458    Objective Loss 0.253458                                        LR 0.000013    Time 0.017061    
2023-01-06 16:43:58,324 - Epoch: [119][  190/  246]    Overall Loss 0.254119    Objective Loss 0.254119                                        LR 0.000013    Time 0.017113    
2023-01-06 16:43:58,506 - Epoch: [119][  200/  246]    Overall Loss 0.254323    Objective Loss 0.254323                                        LR 0.000013    Time 0.017163    
2023-01-06 16:43:58,685 - Epoch: [119][  210/  246]    Overall Loss 0.255697    Objective Loss 0.255697                                        LR 0.000013    Time 0.017196    
2023-01-06 16:43:58,869 - Epoch: [119][  220/  246]    Overall Loss 0.256191    Objective Loss 0.256191                                        LR 0.000013    Time 0.017249    
2023-01-06 16:43:59,050 - Epoch: [119][  230/  246]    Overall Loss 0.255802    Objective Loss 0.255802                                        LR 0.000013    Time 0.017283    
2023-01-06 16:43:59,249 - Epoch: [119][  240/  246]    Overall Loss 0.255412    Objective Loss 0.255412                                        LR 0.000013    Time 0.017390    
2023-01-06 16:43:59,322 - Epoch: [119][  246/  246]    Overall Loss 0.255317    Objective Loss 0.255317    Top1 91.626794    LR 0.000013    Time 0.017262    
2023-01-06 16:43:59,443 - --- validate (epoch=119)-----------
2023-01-06 16:43:59,443 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:00,001 - Epoch: [119][   10/   28]    Loss 0.258195    Top1 91.093750    
2023-01-06 16:44:00,103 - Epoch: [119][   20/   28]    Loss 0.259885    Top1 90.917969    
2023-01-06 16:44:00,154 - Epoch: [119][   28/   28]    Loss 0.268978    Top1 90.595477    
2023-01-06 16:44:00,311 - ==> Top1: 90.595    Loss: 0.269

2023-01-06 16:44:00,311 - ==> Confusion:
[[ 250   16  173]
 [  17  276  309]
 [  65   77 5803]]

2023-01-06 16:44:00,312 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:00,312 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:00,319 - 

2023-01-06 16:44:00,319 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:00,852 - Epoch: [120][   10/  246]    Overall Loss 0.251194    Objective Loss 0.251194                                        LR 0.000013    Time 0.053293    
2023-01-06 16:44:01,005 - Epoch: [120][   20/  246]    Overall Loss 0.256226    Objective Loss 0.256226                                        LR 0.000013    Time 0.034246    
2023-01-06 16:44:01,161 - Epoch: [120][   30/  246]    Overall Loss 0.257980    Objective Loss 0.257980                                        LR 0.000013    Time 0.028033    
2023-01-06 16:44:01,310 - Epoch: [120][   40/  246]    Overall Loss 0.252856    Objective Loss 0.252856                                        LR 0.000013    Time 0.024729    
2023-01-06 16:44:01,461 - Epoch: [120][   50/  246]    Overall Loss 0.256060    Objective Loss 0.256060                                        LR 0.000013    Time 0.022803    
2023-01-06 16:44:01,607 - Epoch: [120][   60/  246]    Overall Loss 0.257022    Objective Loss 0.257022                                        LR 0.000013    Time 0.021420    
2023-01-06 16:44:01,756 - Epoch: [120][   70/  246]    Overall Loss 0.255188    Objective Loss 0.255188                                        LR 0.000013    Time 0.020485    
2023-01-06 16:44:01,900 - Epoch: [120][   80/  246]    Overall Loss 0.256317    Objective Loss 0.256317                                        LR 0.000013    Time 0.019715    
2023-01-06 16:44:02,041 - Epoch: [120][   90/  246]    Overall Loss 0.257529    Objective Loss 0.257529                                        LR 0.000013    Time 0.019091    
2023-01-06 16:44:02,184 - Epoch: [120][  100/  246]    Overall Loss 0.256010    Objective Loss 0.256010                                        LR 0.000013    Time 0.018605    
2023-01-06 16:44:02,328 - Epoch: [120][  110/  246]    Overall Loss 0.257262    Objective Loss 0.257262                                        LR 0.000013    Time 0.018221    
2023-01-06 16:44:02,471 - Epoch: [120][  120/  246]    Overall Loss 0.256728    Objective Loss 0.256728                                        LR 0.000013    Time 0.017886    
2023-01-06 16:44:02,612 - Epoch: [120][  130/  246]    Overall Loss 0.256066    Objective Loss 0.256066                                        LR 0.000013    Time 0.017590    
2023-01-06 16:44:02,752 - Epoch: [120][  140/  246]    Overall Loss 0.256071    Objective Loss 0.256071                                        LR 0.000013    Time 0.017332    
2023-01-06 16:44:02,886 - Epoch: [120][  150/  246]    Overall Loss 0.256447    Objective Loss 0.256447                                        LR 0.000013    Time 0.017067    
2023-01-06 16:44:03,020 - Epoch: [120][  160/  246]    Overall Loss 0.256314    Objective Loss 0.256314                                        LR 0.000013    Time 0.016834    
2023-01-06 16:44:03,157 - Epoch: [120][  170/  246]    Overall Loss 0.256977    Objective Loss 0.256977                                        LR 0.000013    Time 0.016649    
2023-01-06 16:44:03,294 - Epoch: [120][  180/  246]    Overall Loss 0.257331    Objective Loss 0.257331                                        LR 0.000013    Time 0.016482    
2023-01-06 16:44:03,427 - Epoch: [120][  190/  246]    Overall Loss 0.256726    Objective Loss 0.256726                                        LR 0.000013    Time 0.016311    
2023-01-06 16:44:03,564 - Epoch: [120][  200/  246]    Overall Loss 0.256099    Objective Loss 0.256099                                        LR 0.000013    Time 0.016180    
2023-01-06 16:44:03,699 - Epoch: [120][  210/  246]    Overall Loss 0.256190    Objective Loss 0.256190                                        LR 0.000013    Time 0.016048    
2023-01-06 16:44:03,835 - Epoch: [120][  220/  246]    Overall Loss 0.257285    Objective Loss 0.257285                                        LR 0.000013    Time 0.015937    
2023-01-06 16:44:03,970 - Epoch: [120][  230/  246]    Overall Loss 0.256789    Objective Loss 0.256789                                        LR 0.000013    Time 0.015826    
2023-01-06 16:44:04,115 - Epoch: [120][  240/  246]    Overall Loss 0.255872    Objective Loss 0.255872                                        LR 0.000013    Time 0.015770    
2023-01-06 16:44:04,181 - Epoch: [120][  246/  246]    Overall Loss 0.255580    Objective Loss 0.255580    Top1 92.105263    LR 0.000013    Time 0.015653    
2023-01-06 16:44:04,358 - --- validate (epoch=120)-----------
2023-01-06 16:44:04,358 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:04,796 - Epoch: [120][   10/   28]    Loss 0.260189    Top1 90.585938    
2023-01-06 16:44:04,889 - Epoch: [120][   20/   28]    Loss 0.271510    Top1 90.175781    
2023-01-06 16:44:04,938 - Epoch: [120][   28/   28]    Loss 0.265206    Top1 90.323504    
2023-01-06 16:44:05,073 - ==> Top1: 90.324    Loss: 0.265

2023-01-06 16:44:05,074 - ==> Confusion:
[[ 230   16  193]
 [  15  271  316]
 [  53   83 5809]]

2023-01-06 16:44:05,075 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:05,075 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:05,080 - 

2023-01-06 16:44:05,080 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:05,760 - Epoch: [121][   10/  246]    Overall Loss 0.238831    Objective Loss 0.238831                                        LR 0.000013    Time 0.067912    
2023-01-06 16:44:05,906 - Epoch: [121][   20/  246]    Overall Loss 0.252360    Objective Loss 0.252360                                        LR 0.000013    Time 0.041277    
2023-01-06 16:44:06,064 - Epoch: [121][   30/  246]    Overall Loss 0.259773    Objective Loss 0.259773                                        LR 0.000013    Time 0.032758    
2023-01-06 16:44:06,239 - Epoch: [121][   40/  246]    Overall Loss 0.251429    Objective Loss 0.251429                                        LR 0.000013    Time 0.028921    
2023-01-06 16:44:06,419 - Epoch: [121][   50/  246]    Overall Loss 0.251881    Objective Loss 0.251881                                        LR 0.000013    Time 0.026728    
2023-01-06 16:44:06,594 - Epoch: [121][   60/  246]    Overall Loss 0.252853    Objective Loss 0.252853                                        LR 0.000013    Time 0.025188    
2023-01-06 16:44:06,775 - Epoch: [121][   70/  246]    Overall Loss 0.252134    Objective Loss 0.252134                                        LR 0.000013    Time 0.024175    
2023-01-06 16:44:06,952 - Epoch: [121][   80/  246]    Overall Loss 0.252002    Objective Loss 0.252002                                        LR 0.000013    Time 0.023359    
2023-01-06 16:44:07,134 - Epoch: [121][   90/  246]    Overall Loss 0.252972    Objective Loss 0.252972                                        LR 0.000013    Time 0.022777    
2023-01-06 16:44:07,311 - Epoch: [121][  100/  246]    Overall Loss 0.253846    Objective Loss 0.253846                                        LR 0.000013    Time 0.022263    
2023-01-06 16:44:07,493 - Epoch: [121][  110/  246]    Overall Loss 0.254205    Objective Loss 0.254205                                        LR 0.000013    Time 0.021893    
2023-01-06 16:44:07,668 - Epoch: [121][  120/  246]    Overall Loss 0.253983    Objective Loss 0.253983                                        LR 0.000013    Time 0.021519    
2023-01-06 16:44:07,849 - Epoch: [121][  130/  246]    Overall Loss 0.255583    Objective Loss 0.255583                                        LR 0.000013    Time 0.021252    
2023-01-06 16:44:08,026 - Epoch: [121][  140/  246]    Overall Loss 0.255824    Objective Loss 0.255824                                        LR 0.000013    Time 0.020994    
2023-01-06 16:44:08,204 - Epoch: [121][  150/  246]    Overall Loss 0.256672    Objective Loss 0.256672                                        LR 0.000013    Time 0.020779    
2023-01-06 16:44:08,379 - Epoch: [121][  160/  246]    Overall Loss 0.256453    Objective Loss 0.256453                                        LR 0.000013    Time 0.020576    
2023-01-06 16:44:08,562 - Epoch: [121][  170/  246]    Overall Loss 0.255840    Objective Loss 0.255840                                        LR 0.000013    Time 0.020435    
2023-01-06 16:44:08,740 - Epoch: [121][  180/  246]    Overall Loss 0.255633    Objective Loss 0.255633                                        LR 0.000013    Time 0.020286    
2023-01-06 16:44:08,921 - Epoch: [121][  190/  246]    Overall Loss 0.255027    Objective Loss 0.255027                                        LR 0.000013    Time 0.020160    
2023-01-06 16:44:09,100 - Epoch: [121][  200/  246]    Overall Loss 0.255226    Objective Loss 0.255226                                        LR 0.000013    Time 0.020047    
2023-01-06 16:44:09,280 - Epoch: [121][  210/  246]    Overall Loss 0.254525    Objective Loss 0.254525                                        LR 0.000013    Time 0.019944    
2023-01-06 16:44:09,454 - Epoch: [121][  220/  246]    Overall Loss 0.254912    Objective Loss 0.254912                                        LR 0.000013    Time 0.019827    
2023-01-06 16:44:09,633 - Epoch: [121][  230/  246]    Overall Loss 0.254677    Objective Loss 0.254677                                        LR 0.000013    Time 0.019742    
2023-01-06 16:44:09,821 - Epoch: [121][  240/  246]    Overall Loss 0.254592    Objective Loss 0.254592                                        LR 0.000013    Time 0.019703    
2023-01-06 16:44:09,901 - Epoch: [121][  246/  246]    Overall Loss 0.254794    Objective Loss 0.254794    Top1 90.191388    LR 0.000013    Time 0.019546    
2023-01-06 16:44:10,045 - --- validate (epoch=121)-----------
2023-01-06 16:44:10,046 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:10,486 - Epoch: [121][   10/   28]    Loss 0.241006    Top1 91.406250    
2023-01-06 16:44:10,588 - Epoch: [121][   20/   28]    Loss 0.265734    Top1 90.722656    
2023-01-06 16:44:10,641 - Epoch: [121][   28/   28]    Loss 0.265946    Top1 90.566848    
2023-01-06 16:44:10,806 - ==> Top1: 90.567    Loss: 0.266

2023-01-06 16:44:10,807 - ==> Confusion:
[[ 250   13  176]
 [  17  272  313]
 [  66   74 5805]]

2023-01-06 16:44:10,808 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:10,809 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:10,814 - 

2023-01-06 16:44:10,814 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:11,341 - Epoch: [122][   10/  246]    Overall Loss 0.273102    Objective Loss 0.273102                                        LR 0.000013    Time 0.052724    
2023-01-06 16:44:11,470 - Epoch: [122][   20/  246]    Overall Loss 0.264690    Objective Loss 0.264690                                        LR 0.000013    Time 0.032776    
2023-01-06 16:44:11,593 - Epoch: [122][   30/  246]    Overall Loss 0.262283    Objective Loss 0.262283                                        LR 0.000013    Time 0.025927    
2023-01-06 16:44:11,730 - Epoch: [122][   40/  246]    Overall Loss 0.259431    Objective Loss 0.259431                                        LR 0.000013    Time 0.022867    
2023-01-06 16:44:11,871 - Epoch: [122][   50/  246]    Overall Loss 0.254341    Objective Loss 0.254341                                        LR 0.000013    Time 0.021105    
2023-01-06 16:44:12,012 - Epoch: [122][   60/  246]    Overall Loss 0.248524    Objective Loss 0.248524                                        LR 0.000013    Time 0.019930    
2023-01-06 16:44:12,149 - Epoch: [122][   70/  246]    Overall Loss 0.245939    Objective Loss 0.245939                                        LR 0.000013    Time 0.019027    
2023-01-06 16:44:12,285 - Epoch: [122][   80/  246]    Overall Loss 0.246713    Objective Loss 0.246713                                        LR 0.000013    Time 0.018349    
2023-01-06 16:44:12,420 - Epoch: [122][   90/  246]    Overall Loss 0.247809    Objective Loss 0.247809                                        LR 0.000013    Time 0.017805    
2023-01-06 16:44:12,558 - Epoch: [122][  100/  246]    Overall Loss 0.248872    Objective Loss 0.248872                                        LR 0.000013    Time 0.017399    
2023-01-06 16:44:12,698 - Epoch: [122][  110/  246]    Overall Loss 0.250856    Objective Loss 0.250856                                        LR 0.000013    Time 0.017090    
2023-01-06 16:44:12,843 - Epoch: [122][  120/  246]    Overall Loss 0.250709    Objective Loss 0.250709                                        LR 0.000013    Time 0.016869    
2023-01-06 16:44:12,993 - Epoch: [122][  130/  246]    Overall Loss 0.251397    Objective Loss 0.251397                                        LR 0.000013    Time 0.016717    
2023-01-06 16:44:13,139 - Epoch: [122][  140/  246]    Overall Loss 0.250315    Objective Loss 0.250315                                        LR 0.000013    Time 0.016566    
2023-01-06 16:44:13,290 - Epoch: [122][  150/  246]    Overall Loss 0.249839    Objective Loss 0.249839                                        LR 0.000013    Time 0.016460    
2023-01-06 16:44:13,438 - Epoch: [122][  160/  246]    Overall Loss 0.249486    Objective Loss 0.249486                                        LR 0.000013    Time 0.016354    
2023-01-06 16:44:13,584 - Epoch: [122][  170/  246]    Overall Loss 0.249725    Objective Loss 0.249725                                        LR 0.000013    Time 0.016251    
2023-01-06 16:44:13,732 - Epoch: [122][  180/  246]    Overall Loss 0.250609    Objective Loss 0.250609                                        LR 0.000013    Time 0.016154    
2023-01-06 16:44:13,882 - Epoch: [122][  190/  246]    Overall Loss 0.252515    Objective Loss 0.252515                                        LR 0.000013    Time 0.016088    
2023-01-06 16:44:14,031 - Epoch: [122][  200/  246]    Overall Loss 0.252973    Objective Loss 0.252973                                        LR 0.000013    Time 0.016026    
2023-01-06 16:44:14,177 - Epoch: [122][  210/  246]    Overall Loss 0.252796    Objective Loss 0.252796                                        LR 0.000013    Time 0.015958    
2023-01-06 16:44:14,325 - Epoch: [122][  220/  246]    Overall Loss 0.253072    Objective Loss 0.253072                                        LR 0.000013    Time 0.015901    
2023-01-06 16:44:14,474 - Epoch: [122][  230/  246]    Overall Loss 0.253800    Objective Loss 0.253800                                        LR 0.000013    Time 0.015855    
2023-01-06 16:44:14,639 - Epoch: [122][  240/  246]    Overall Loss 0.254454    Objective Loss 0.254454                                        LR 0.000013    Time 0.015881    
2023-01-06 16:44:14,706 - Epoch: [122][  246/  246]    Overall Loss 0.255273    Objective Loss 0.255273    Top1 89.952153    LR 0.000013    Time 0.015767    
2023-01-06 16:44:14,833 - --- validate (epoch=122)-----------
2023-01-06 16:44:14,833 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:15,275 - Epoch: [122][   10/   28]    Loss 0.259835    Top1 90.546875    
2023-01-06 16:44:15,383 - Epoch: [122][   20/   28]    Loss 0.264282    Top1 90.468750    
2023-01-06 16:44:15,435 - Epoch: [122][   28/   28]    Loss 0.266909    Top1 90.480962    
2023-01-06 16:44:15,595 - ==> Top1: 90.481    Loss: 0.267

2023-01-06 16:44:15,595 - ==> Confusion:
[[ 233   14  192]
 [  15  262  325]
 [  51   68 5826]]

2023-01-06 16:44:15,596 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:15,597 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:15,602 - 

2023-01-06 16:44:15,602 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:16,254 - Epoch: [123][   10/  246]    Overall Loss 0.264182    Objective Loss 0.264182                                        LR 0.000013    Time 0.065159    
2023-01-06 16:44:16,381 - Epoch: [123][   20/  246]    Overall Loss 0.259711    Objective Loss 0.259711                                        LR 0.000013    Time 0.038907    
2023-01-06 16:44:16,507 - Epoch: [123][   30/  246]    Overall Loss 0.255234    Objective Loss 0.255234                                        LR 0.000013    Time 0.030138    
2023-01-06 16:44:16,644 - Epoch: [123][   40/  246]    Overall Loss 0.255782    Objective Loss 0.255782                                        LR 0.000013    Time 0.026017    
2023-01-06 16:44:16,787 - Epoch: [123][   50/  246]    Overall Loss 0.254245    Objective Loss 0.254245                                        LR 0.000013    Time 0.023652    
2023-01-06 16:44:16,939 - Epoch: [123][   60/  246]    Overall Loss 0.252081    Objective Loss 0.252081                                        LR 0.000013    Time 0.022234    
2023-01-06 16:44:17,090 - Epoch: [123][   70/  246]    Overall Loss 0.253392    Objective Loss 0.253392                                        LR 0.000013    Time 0.021220    
2023-01-06 16:44:17,249 - Epoch: [123][   80/  246]    Overall Loss 0.255089    Objective Loss 0.255089                                        LR 0.000013    Time 0.020528    
2023-01-06 16:44:17,402 - Epoch: [123][   90/  246]    Overall Loss 0.255795    Objective Loss 0.255795                                        LR 0.000013    Time 0.019942    
2023-01-06 16:44:17,560 - Epoch: [123][  100/  246]    Overall Loss 0.254721    Objective Loss 0.254721                                        LR 0.000013    Time 0.019522    
2023-01-06 16:44:17,709 - Epoch: [123][  110/  246]    Overall Loss 0.253565    Objective Loss 0.253565                                        LR 0.000013    Time 0.019099    
2023-01-06 16:44:17,860 - Epoch: [123][  120/  246]    Overall Loss 0.254444    Objective Loss 0.254444                                        LR 0.000013    Time 0.018757    
2023-01-06 16:44:18,002 - Epoch: [123][  130/  246]    Overall Loss 0.252671    Objective Loss 0.252671                                        LR 0.000013    Time 0.018408    
2023-01-06 16:44:18,142 - Epoch: [123][  140/  246]    Overall Loss 0.253316    Objective Loss 0.253316                                        LR 0.000013    Time 0.018087    
2023-01-06 16:44:18,286 - Epoch: [123][  150/  246]    Overall Loss 0.253041    Objective Loss 0.253041                                        LR 0.000013    Time 0.017840    
2023-01-06 16:44:18,424 - Epoch: [123][  160/  246]    Overall Loss 0.252975    Objective Loss 0.252975                                        LR 0.000013    Time 0.017587    
2023-01-06 16:44:18,568 - Epoch: [123][  170/  246]    Overall Loss 0.253629    Objective Loss 0.253629                                        LR 0.000013    Time 0.017386    
2023-01-06 16:44:18,704 - Epoch: [123][  180/  246]    Overall Loss 0.253032    Objective Loss 0.253032                                        LR 0.000013    Time 0.017174    
2023-01-06 16:44:18,848 - Epoch: [123][  190/  246]    Overall Loss 0.253065    Objective Loss 0.253065                                        LR 0.000013    Time 0.017028    
2023-01-06 16:44:18,987 - Epoch: [123][  200/  246]    Overall Loss 0.253424    Objective Loss 0.253424                                        LR 0.000013    Time 0.016868    
2023-01-06 16:44:19,131 - Epoch: [123][  210/  246]    Overall Loss 0.254085    Objective Loss 0.254085                                        LR 0.000013    Time 0.016750    
2023-01-06 16:44:19,270 - Epoch: [123][  220/  246]    Overall Loss 0.253896    Objective Loss 0.253896                                        LR 0.000013    Time 0.016618    
2023-01-06 16:44:19,414 - Epoch: [123][  230/  246]    Overall Loss 0.253958    Objective Loss 0.253958                                        LR 0.000013    Time 0.016520    
2023-01-06 16:44:19,567 - Epoch: [123][  240/  246]    Overall Loss 0.254601    Objective Loss 0.254601                                        LR 0.000013    Time 0.016464    
2023-01-06 16:44:19,631 - Epoch: [123][  246/  246]    Overall Loss 0.254640    Objective Loss 0.254640    Top1 90.191388    LR 0.000013    Time 0.016323    
2023-01-06 16:44:19,767 - --- validate (epoch=123)-----------
2023-01-06 16:44:19,768 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:20,189 - Epoch: [123][   10/   28]    Loss 0.264839    Top1 90.000000    
2023-01-06 16:44:20,293 - Epoch: [123][   20/   28]    Loss 0.274902    Top1 89.843750    
2023-01-06 16:44:20,342 - Epoch: [123][   28/   28]    Loss 0.268141    Top1 90.237618    
2023-01-06 16:44:20,482 - ==> Top1: 90.238    Loss: 0.268

2023-01-06 16:44:20,482 - ==> Confusion:
[[ 210   14  215]
 [  15  263  324]
 [  39   75 5831]]

2023-01-06 16:44:20,484 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:20,484 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:20,490 - 

2023-01-06 16:44:20,490 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:21,145 - Epoch: [124][   10/  246]    Overall Loss 0.259314    Objective Loss 0.259314                                        LR 0.000013    Time 0.065470    
2023-01-06 16:44:21,269 - Epoch: [124][   20/  246]    Overall Loss 0.249369    Objective Loss 0.249369                                        LR 0.000013    Time 0.038935    
2023-01-06 16:44:21,402 - Epoch: [124][   30/  246]    Overall Loss 0.250784    Objective Loss 0.250784                                        LR 0.000013    Time 0.030353    
2023-01-06 16:44:21,533 - Epoch: [124][   40/  246]    Overall Loss 0.251114    Objective Loss 0.251114                                        LR 0.000013    Time 0.026027    
2023-01-06 16:44:21,677 - Epoch: [124][   50/  246]    Overall Loss 0.252909    Objective Loss 0.252909                                        LR 0.000013    Time 0.023706    
2023-01-06 16:44:21,828 - Epoch: [124][   60/  246]    Overall Loss 0.254905    Objective Loss 0.254905                                        LR 0.000013    Time 0.022265    
2023-01-06 16:44:21,980 - Epoch: [124][   70/  246]    Overall Loss 0.253674    Objective Loss 0.253674                                        LR 0.000013    Time 0.021247    
2023-01-06 16:44:22,129 - Epoch: [124][   80/  246]    Overall Loss 0.251638    Objective Loss 0.251638                                        LR 0.000013    Time 0.020446    
2023-01-06 16:44:22,280 - Epoch: [124][   90/  246]    Overall Loss 0.250599    Objective Loss 0.250599                                        LR 0.000013    Time 0.019846    
2023-01-06 16:44:22,430 - Epoch: [124][  100/  246]    Overall Loss 0.251516    Objective Loss 0.251516                                        LR 0.000013    Time 0.019363    
2023-01-06 16:44:22,585 - Epoch: [124][  110/  246]    Overall Loss 0.253167    Objective Loss 0.253167                                        LR 0.000013    Time 0.019005    
2023-01-06 16:44:22,734 - Epoch: [124][  120/  246]    Overall Loss 0.252481    Objective Loss 0.252481                                        LR 0.000013    Time 0.018658    
2023-01-06 16:44:22,890 - Epoch: [124][  130/  246]    Overall Loss 0.252569    Objective Loss 0.252569                                        LR 0.000013    Time 0.018420    
2023-01-06 16:44:23,042 - Epoch: [124][  140/  246]    Overall Loss 0.253363    Objective Loss 0.253363                                        LR 0.000013    Time 0.018190    
2023-01-06 16:44:23,190 - Epoch: [124][  150/  246]    Overall Loss 0.253805    Objective Loss 0.253805                                        LR 0.000013    Time 0.017960    
2023-01-06 16:44:23,333 - Epoch: [124][  160/  246]    Overall Loss 0.253390    Objective Loss 0.253390                                        LR 0.000013    Time 0.017726    
2023-01-06 16:44:23,476 - Epoch: [124][  170/  246]    Overall Loss 0.254362    Objective Loss 0.254362                                        LR 0.000013    Time 0.017524    
2023-01-06 16:44:23,614 - Epoch: [124][  180/  246]    Overall Loss 0.254729    Objective Loss 0.254729                                        LR 0.000013    Time 0.017313    
2023-01-06 16:44:23,754 - Epoch: [124][  190/  246]    Overall Loss 0.255349    Objective Loss 0.255349                                        LR 0.000013    Time 0.017135    
2023-01-06 16:44:23,890 - Epoch: [124][  200/  246]    Overall Loss 0.255568    Objective Loss 0.255568                                        LR 0.000013    Time 0.016958    
2023-01-06 16:44:24,030 - Epoch: [124][  210/  246]    Overall Loss 0.255686    Objective Loss 0.255686                                        LR 0.000013    Time 0.016818    
2023-01-06 16:44:24,166 - Epoch: [124][  220/  246]    Overall Loss 0.255275    Objective Loss 0.255275                                        LR 0.000013    Time 0.016667    
2023-01-06 16:44:24,305 - Epoch: [124][  230/  246]    Overall Loss 0.255396    Objective Loss 0.255396                                        LR 0.000013    Time 0.016544    
2023-01-06 16:44:24,453 - Epoch: [124][  240/  246]    Overall Loss 0.254624    Objective Loss 0.254624                                        LR 0.000013    Time 0.016472    
2023-01-06 16:44:24,520 - Epoch: [124][  246/  246]    Overall Loss 0.254300    Objective Loss 0.254300    Top1 91.148325    LR 0.000013    Time 0.016343    
2023-01-06 16:44:24,669 - --- validate (epoch=124)-----------
2023-01-06 16:44:24,669 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:25,095 - Epoch: [124][   10/   28]    Loss 0.269065    Top1 90.156250    
2023-01-06 16:44:25,199 - Epoch: [124][   20/   28]    Loss 0.267964    Top1 90.371094    
2023-01-06 16:44:25,251 - Epoch: [124][   28/   28]    Loss 0.269694    Top1 90.323504    
2023-01-06 16:44:25,416 - ==> Top1: 90.324    Loss: 0.270

2023-01-06 16:44:25,416 - ==> Confusion:
[[ 219   12  208]
 [  14  238  350]
 [  40   52 5853]]

2023-01-06 16:44:25,417 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:25,417 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:25,422 - 

2023-01-06 16:44:25,423 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:25,948 - Epoch: [125][   10/  246]    Overall Loss 0.261445    Objective Loss 0.261445                                        LR 0.000013    Time 0.052528    
2023-01-06 16:44:26,079 - Epoch: [125][   20/  246]    Overall Loss 0.268646    Objective Loss 0.268646                                        LR 0.000013    Time 0.032784    
2023-01-06 16:44:26,215 - Epoch: [125][   30/  246]    Overall Loss 0.267432    Objective Loss 0.267432                                        LR 0.000013    Time 0.026352    
2023-01-06 16:44:26,353 - Epoch: [125][   40/  246]    Overall Loss 0.267758    Objective Loss 0.267758                                        LR 0.000013    Time 0.023214    
2023-01-06 16:44:26,495 - Epoch: [125][   50/  246]    Overall Loss 0.268933    Objective Loss 0.268933                                        LR 0.000013    Time 0.021399    
2023-01-06 16:44:26,630 - Epoch: [125][   60/  246]    Overall Loss 0.267977    Objective Loss 0.267977                                        LR 0.000013    Time 0.020082    
2023-01-06 16:44:26,768 - Epoch: [125][   70/  246]    Overall Loss 0.264408    Objective Loss 0.264408                                        LR 0.000013    Time 0.019145    
2023-01-06 16:44:26,908 - Epoch: [125][   80/  246]    Overall Loss 0.267160    Objective Loss 0.267160                                        LR 0.000013    Time 0.018477    
2023-01-06 16:44:27,048 - Epoch: [125][   90/  246]    Overall Loss 0.264374    Objective Loss 0.264374                                        LR 0.000013    Time 0.017967    
2023-01-06 16:44:27,190 - Epoch: [125][  100/  246]    Overall Loss 0.263712    Objective Loss 0.263712                                        LR 0.000013    Time 0.017589    
2023-01-06 16:44:27,331 - Epoch: [125][  110/  246]    Overall Loss 0.263866    Objective Loss 0.263866                                        LR 0.000013    Time 0.017269    
2023-01-06 16:44:27,466 - Epoch: [125][  120/  246]    Overall Loss 0.263031    Objective Loss 0.263031                                        LR 0.000013    Time 0.016954    
2023-01-06 16:44:27,604 - Epoch: [125][  130/  246]    Overall Loss 0.259617    Objective Loss 0.259617                                        LR 0.000013    Time 0.016709    
2023-01-06 16:44:27,742 - Epoch: [125][  140/  246]    Overall Loss 0.260963    Objective Loss 0.260963                                        LR 0.000013    Time 0.016495    
2023-01-06 16:44:27,879 - Epoch: [125][  150/  246]    Overall Loss 0.259422    Objective Loss 0.259422                                        LR 0.000013    Time 0.016296    
2023-01-06 16:44:28,016 - Epoch: [125][  160/  246]    Overall Loss 0.258892    Objective Loss 0.258892                                        LR 0.000013    Time 0.016131    
2023-01-06 16:44:28,153 - Epoch: [125][  170/  246]    Overall Loss 0.258446    Objective Loss 0.258446                                        LR 0.000013    Time 0.015984    
2023-01-06 16:44:28,291 - Epoch: [125][  180/  246]    Overall Loss 0.258680    Objective Loss 0.258680                                        LR 0.000013    Time 0.015860    
2023-01-06 16:44:28,429 - Epoch: [125][  190/  246]    Overall Loss 0.255471    Objective Loss 0.255471                                        LR 0.000013    Time 0.015753    
2023-01-06 16:44:28,567 - Epoch: [125][  200/  246]    Overall Loss 0.255390    Objective Loss 0.255390                                        LR 0.000013    Time 0.015653    
2023-01-06 16:44:28,707 - Epoch: [125][  210/  246]    Overall Loss 0.254218    Objective Loss 0.254218                                        LR 0.000013    Time 0.015560    
2023-01-06 16:44:28,851 - Epoch: [125][  220/  246]    Overall Loss 0.254359    Objective Loss 0.254359                                        LR 0.000013    Time 0.015500    
2023-01-06 16:44:28,990 - Epoch: [125][  230/  246]    Overall Loss 0.254717    Objective Loss 0.254717                                        LR 0.000013    Time 0.015428    
2023-01-06 16:44:29,147 - Epoch: [125][  240/  246]    Overall Loss 0.254586    Objective Loss 0.254586                                        LR 0.000013    Time 0.015436    
2023-01-06 16:44:29,212 - Epoch: [125][  246/  246]    Overall Loss 0.253711    Objective Loss 0.253711    Top1 92.344498    LR 0.000013    Time 0.015324    
2023-01-06 16:44:29,370 - --- validate (epoch=125)-----------
2023-01-06 16:44:29,370 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:29,804 - Epoch: [125][   10/   28]    Loss 0.268712    Top1 90.820312    
2023-01-06 16:44:29,905 - Epoch: [125][   20/   28]    Loss 0.267876    Top1 90.371094    
2023-01-06 16:44:29,953 - Epoch: [125][   28/   28]    Loss 0.266193    Top1 90.395076    
2023-01-06 16:44:30,091 - ==> Top1: 90.395    Loss: 0.266

2023-01-06 16:44:30,091 - ==> Confusion:
[[ 223   15  201]
 [  13  263  326]
 [  45   71 5829]]

2023-01-06 16:44:30,093 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:30,093 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:30,098 - 

2023-01-06 16:44:30,098 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:30,766 - Epoch: [126][   10/  246]    Overall Loss 0.260185    Objective Loss 0.260185                                        LR 0.000013    Time 0.066700    
2023-01-06 16:44:30,914 - Epoch: [126][   20/  246]    Overall Loss 0.257304    Objective Loss 0.257304                                        LR 0.000013    Time 0.040748    
2023-01-06 16:44:31,066 - Epoch: [126][   30/  246]    Overall Loss 0.251283    Objective Loss 0.251283                                        LR 0.000013    Time 0.032215    
2023-01-06 16:44:31,211 - Epoch: [126][   40/  246]    Overall Loss 0.251126    Objective Loss 0.251126                                        LR 0.000013    Time 0.027785    
2023-01-06 16:44:31,358 - Epoch: [126][   50/  246]    Overall Loss 0.248297    Objective Loss 0.248297                                        LR 0.000013    Time 0.025147    
2023-01-06 16:44:31,503 - Epoch: [126][   60/  246]    Overall Loss 0.249207    Objective Loss 0.249207                                        LR 0.000013    Time 0.023378    
2023-01-06 16:44:31,648 - Epoch: [126][   70/  246]    Overall Loss 0.250312    Objective Loss 0.250312                                        LR 0.000013    Time 0.022097    
2023-01-06 16:44:31,794 - Epoch: [126][   80/  246]    Overall Loss 0.251323    Objective Loss 0.251323                                        LR 0.000013    Time 0.021155    
2023-01-06 16:44:31,933 - Epoch: [126][   90/  246]    Overall Loss 0.251861    Objective Loss 0.251861                                        LR 0.000013    Time 0.020344    
2023-01-06 16:44:32,072 - Epoch: [126][  100/  246]    Overall Loss 0.250817    Objective Loss 0.250817                                        LR 0.000013    Time 0.019697    
2023-01-06 16:44:32,205 - Epoch: [126][  110/  246]    Overall Loss 0.251808    Objective Loss 0.251808                                        LR 0.000013    Time 0.019112    
2023-01-06 16:44:32,338 - Epoch: [126][  120/  246]    Overall Loss 0.251822    Objective Loss 0.251822                                        LR 0.000013    Time 0.018624    
2023-01-06 16:44:32,484 - Epoch: [126][  130/  246]    Overall Loss 0.252373    Objective Loss 0.252373                                        LR 0.000013    Time 0.018305    
2023-01-06 16:44:32,608 - Epoch: [126][  140/  246]    Overall Loss 0.252053    Objective Loss 0.252053                                        LR 0.000013    Time 0.017877    
2023-01-06 16:44:32,730 - Epoch: [126][  150/  246]    Overall Loss 0.252826    Objective Loss 0.252826                                        LR 0.000013    Time 0.017496    
2023-01-06 16:44:32,847 - Epoch: [126][  160/  246]    Overall Loss 0.253668    Objective Loss 0.253668                                        LR 0.000013    Time 0.017132    
2023-01-06 16:44:32,968 - Epoch: [126][  170/  246]    Overall Loss 0.253191    Objective Loss 0.253191                                        LR 0.000013    Time 0.016835    
2023-01-06 16:44:33,100 - Epoch: [126][  180/  246]    Overall Loss 0.253378    Objective Loss 0.253378                                        LR 0.000013    Time 0.016631    
2023-01-06 16:44:33,233 - Epoch: [126][  190/  246]    Overall Loss 0.253419    Objective Loss 0.253419                                        LR 0.000013    Time 0.016450    
2023-01-06 16:44:33,360 - Epoch: [126][  200/  246]    Overall Loss 0.253675    Objective Loss 0.253675                                        LR 0.000013    Time 0.016262    
2023-01-06 16:44:33,502 - Epoch: [126][  210/  246]    Overall Loss 0.252994    Objective Loss 0.252994                                        LR 0.000013    Time 0.016162    
2023-01-06 16:44:33,647 - Epoch: [126][  220/  246]    Overall Loss 0.253581    Objective Loss 0.253581                                        LR 0.000013    Time 0.016084    
2023-01-06 16:44:33,800 - Epoch: [126][  230/  246]    Overall Loss 0.252942    Objective Loss 0.252942                                        LR 0.000013    Time 0.016047    
2023-01-06 16:44:33,959 - Epoch: [126][  240/  246]    Overall Loss 0.252915    Objective Loss 0.252915                                        LR 0.000013    Time 0.016039    
2023-01-06 16:44:34,023 - Epoch: [126][  246/  246]    Overall Loss 0.253795    Objective Loss 0.253795    Top1 90.191388    LR 0.000013    Time 0.015910    
2023-01-06 16:44:34,161 - --- validate (epoch=126)-----------
2023-01-06 16:44:34,162 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:34,587 - Epoch: [126][   10/   28]    Loss 0.263047    Top1 90.703125    
2023-01-06 16:44:34,684 - Epoch: [126][   20/   28]    Loss 0.263860    Top1 90.585938    
2023-01-06 16:44:34,735 - Epoch: [126][   28/   28]    Loss 0.265234    Top1 90.495276    
2023-01-06 16:44:34,899 - ==> Top1: 90.495    Loss: 0.265

2023-01-06 16:44:34,899 - ==> Confusion:
[[ 231   14  194]
 [  12  274  316]
 [  49   79 5817]]

2023-01-06 16:44:34,900 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:34,900 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:34,905 - 

2023-01-06 16:44:34,905 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:35,431 - Epoch: [127][   10/  246]    Overall Loss 0.285718    Objective Loss 0.285718                                        LR 0.000013    Time 0.052493    
2023-01-06 16:44:35,555 - Epoch: [127][   20/  246]    Overall Loss 0.272347    Objective Loss 0.272347                                        LR 0.000013    Time 0.032440    
2023-01-06 16:44:35,695 - Epoch: [127][   30/  246]    Overall Loss 0.265843    Objective Loss 0.265843                                        LR 0.000013    Time 0.026262    
2023-01-06 16:44:35,841 - Epoch: [127][   40/  246]    Overall Loss 0.264509    Objective Loss 0.264509                                        LR 0.000013    Time 0.023341    
2023-01-06 16:44:35,983 - Epoch: [127][   50/  246]    Overall Loss 0.266545    Objective Loss 0.266545                                        LR 0.000013    Time 0.021510    
2023-01-06 16:44:36,124 - Epoch: [127][   60/  246]    Overall Loss 0.266254    Objective Loss 0.266254                                        LR 0.000013    Time 0.020265    
2023-01-06 16:44:36,263 - Epoch: [127][   70/  246]    Overall Loss 0.264011    Objective Loss 0.264011                                        LR 0.000013    Time 0.019350    
2023-01-06 16:44:36,402 - Epoch: [127][   80/  246]    Overall Loss 0.264820    Objective Loss 0.264820                                        LR 0.000013    Time 0.018663    
2023-01-06 16:44:36,533 - Epoch: [127][   90/  246]    Overall Loss 0.263551    Objective Loss 0.263551                                        LR 0.000013    Time 0.018044    
2023-01-06 16:44:36,659 - Epoch: [127][  100/  246]    Overall Loss 0.261058    Objective Loss 0.261058                                        LR 0.000013    Time 0.017488    
2023-01-06 16:44:36,797 - Epoch: [127][  110/  246]    Overall Loss 0.260103    Objective Loss 0.260103                                        LR 0.000013    Time 0.017154    
2023-01-06 16:44:36,930 - Epoch: [127][  120/  246]    Overall Loss 0.258397    Objective Loss 0.258397                                        LR 0.000013    Time 0.016827    
2023-01-06 16:44:37,066 - Epoch: [127][  130/  246]    Overall Loss 0.257048    Objective Loss 0.257048                                        LR 0.000013    Time 0.016576    
2023-01-06 16:44:37,201 - Epoch: [127][  140/  246]    Overall Loss 0.257053    Objective Loss 0.257053                                        LR 0.000013    Time 0.016354    
2023-01-06 16:44:37,336 - Epoch: [127][  150/  246]    Overall Loss 0.256282    Objective Loss 0.256282                                        LR 0.000013    Time 0.016156    
2023-01-06 16:44:37,470 - Epoch: [127][  160/  246]    Overall Loss 0.255234    Objective Loss 0.255234                                        LR 0.000013    Time 0.015986    
2023-01-06 16:44:37,605 - Epoch: [127][  170/  246]    Overall Loss 0.255052    Objective Loss 0.255052                                        LR 0.000013    Time 0.015837    
2023-01-06 16:44:37,740 - Epoch: [127][  180/  246]    Overall Loss 0.255023    Objective Loss 0.255023                                        LR 0.000013    Time 0.015705    
2023-01-06 16:44:37,875 - Epoch: [127][  190/  246]    Overall Loss 0.254615    Objective Loss 0.254615                                        LR 0.000013    Time 0.015586    
2023-01-06 16:44:38,001 - Epoch: [127][  200/  246]    Overall Loss 0.254872    Objective Loss 0.254872                                        LR 0.000013    Time 0.015436    
2023-01-06 16:44:38,129 - Epoch: [127][  210/  246]    Overall Loss 0.253935    Objective Loss 0.253935                                        LR 0.000013    Time 0.015308    
2023-01-06 16:44:38,256 - Epoch: [127][  220/  246]    Overall Loss 0.253110    Objective Loss 0.253110                                        LR 0.000013    Time 0.015187    
2023-01-06 16:44:38,386 - Epoch: [127][  230/  246]    Overall Loss 0.253216    Objective Loss 0.253216                                        LR 0.000013    Time 0.015088    
2023-01-06 16:44:38,526 - Epoch: [127][  240/  246]    Overall Loss 0.253670    Objective Loss 0.253670                                        LR 0.000013    Time 0.015040    
2023-01-06 16:44:38,583 - Epoch: [127][  246/  246]    Overall Loss 0.253569    Objective Loss 0.253569    Top1 91.148325    LR 0.000013    Time 0.014906    
2023-01-06 16:44:38,721 - --- validate (epoch=127)-----------
2023-01-06 16:44:38,721 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:39,151 - Epoch: [127][   10/   28]    Loss 0.255874    Top1 90.507812    
2023-01-06 16:44:39,254 - Epoch: [127][   20/   28]    Loss 0.261904    Top1 90.664062    
2023-01-06 16:44:39,304 - Epoch: [127][   28/   28]    Loss 0.266743    Top1 90.395076    
2023-01-06 16:44:39,442 - ==> Top1: 90.395    Loss: 0.267

2023-01-06 16:44:39,442 - ==> Confusion:
[[ 225   14  200]
 [  14  254  334]
 [  50   59 5836]]

2023-01-06 16:44:39,443 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:39,443 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:39,449 - 

2023-01-06 16:44:39,449 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:40,088 - Epoch: [128][   10/  246]    Overall Loss 0.240534    Objective Loss 0.240534                                        LR 0.000013    Time 0.063827    
2023-01-06 16:44:40,223 - Epoch: [128][   20/  246]    Overall Loss 0.237256    Objective Loss 0.237256                                        LR 0.000013    Time 0.038650    
2023-01-06 16:44:40,341 - Epoch: [128][   30/  246]    Overall Loss 0.243525    Objective Loss 0.243525                                        LR 0.000013    Time 0.029677    
2023-01-06 16:44:40,462 - Epoch: [128][   40/  246]    Overall Loss 0.243513    Objective Loss 0.243513                                        LR 0.000013    Time 0.025274    
2023-01-06 16:44:40,584 - Epoch: [128][   50/  246]    Overall Loss 0.248710    Objective Loss 0.248710                                        LR 0.000013    Time 0.022647    
2023-01-06 16:44:40,706 - Epoch: [128][   60/  246]    Overall Loss 0.248450    Objective Loss 0.248450                                        LR 0.000013    Time 0.020880    
2023-01-06 16:44:40,831 - Epoch: [128][   70/  246]    Overall Loss 0.248619    Objective Loss 0.248619                                        LR 0.000013    Time 0.019688    
2023-01-06 16:44:40,956 - Epoch: [128][   80/  246]    Overall Loss 0.247371    Objective Loss 0.247371                                        LR 0.000013    Time 0.018770    
2023-01-06 16:44:41,081 - Epoch: [128][   90/  246]    Overall Loss 0.249075    Objective Loss 0.249075                                        LR 0.000013    Time 0.018065    
2023-01-06 16:44:41,221 - Epoch: [128][  100/  246]    Overall Loss 0.249432    Objective Loss 0.249432                                        LR 0.000013    Time 0.017651    
2023-01-06 16:44:41,364 - Epoch: [128][  110/  246]    Overall Loss 0.248401    Objective Loss 0.248401                                        LR 0.000013    Time 0.017346    
2023-01-06 16:44:41,507 - Epoch: [128][  120/  246]    Overall Loss 0.249235    Objective Loss 0.249235                                        LR 0.000013    Time 0.017092    
2023-01-06 16:44:41,652 - Epoch: [128][  130/  246]    Overall Loss 0.250116    Objective Loss 0.250116                                        LR 0.000013    Time 0.016883    
2023-01-06 16:44:41,800 - Epoch: [128][  140/  246]    Overall Loss 0.250412    Objective Loss 0.250412                                        LR 0.000013    Time 0.016734    
2023-01-06 16:44:41,945 - Epoch: [128][  150/  246]    Overall Loss 0.250862    Objective Loss 0.250862                                        LR 0.000013    Time 0.016585    
2023-01-06 16:44:42,093 - Epoch: [128][  160/  246]    Overall Loss 0.250254    Objective Loss 0.250254                                        LR 0.000013    Time 0.016472    
2023-01-06 16:44:42,237 - Epoch: [128][  170/  246]    Overall Loss 0.251310    Objective Loss 0.251310                                        LR 0.000013    Time 0.016348    
2023-01-06 16:44:42,386 - Epoch: [128][  180/  246]    Overall Loss 0.250960    Objective Loss 0.250960                                        LR 0.000013    Time 0.016263    
2023-01-06 16:44:42,533 - Epoch: [128][  190/  246]    Overall Loss 0.251684    Objective Loss 0.251684                                        LR 0.000013    Time 0.016179    
2023-01-06 16:44:42,682 - Epoch: [128][  200/  246]    Overall Loss 0.251929    Objective Loss 0.251929                                        LR 0.000013    Time 0.016111    
2023-01-06 16:44:42,826 - Epoch: [128][  210/  246]    Overall Loss 0.252209    Objective Loss 0.252209                                        LR 0.000013    Time 0.016028    
2023-01-06 16:44:42,974 - Epoch: [128][  220/  246]    Overall Loss 0.252577    Objective Loss 0.252577                                        LR 0.000013    Time 0.015970    
2023-01-06 16:44:43,119 - Epoch: [128][  230/  246]    Overall Loss 0.253300    Objective Loss 0.253300                                        LR 0.000013    Time 0.015906    
2023-01-06 16:44:43,276 - Epoch: [128][  240/  246]    Overall Loss 0.253515    Objective Loss 0.253515                                        LR 0.000013    Time 0.015897    
2023-01-06 16:44:43,342 - Epoch: [128][  246/  246]    Overall Loss 0.253487    Objective Loss 0.253487    Top1 91.626794    LR 0.000013    Time 0.015776    
2023-01-06 16:44:43,476 - --- validate (epoch=128)-----------
2023-01-06 16:44:43,476 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:43,910 - Epoch: [128][   10/   28]    Loss 0.265985    Top1 90.117188    
2023-01-06 16:44:44,005 - Epoch: [128][   20/   28]    Loss 0.259933    Top1 90.664062    
2023-01-06 16:44:44,059 - Epoch: [128][   28/   28]    Loss 0.267084    Top1 90.452333    
2023-01-06 16:44:44,200 - ==> Top1: 90.452    Loss: 0.267

2023-01-06 16:44:44,200 - ==> Confusion:
[[ 218   16  205]
 [  12  254  336]
 [  37   61 5847]]

2023-01-06 16:44:44,201 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:44,201 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:44,206 - 

2023-01-06 16:44:44,206 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:44,854 - Epoch: [129][   10/  246]    Overall Loss 0.245742    Objective Loss 0.245742                                        LR 0.000013    Time 0.064715    
2023-01-06 16:44:44,978 - Epoch: [129][   20/  246]    Overall Loss 0.254996    Objective Loss 0.254996                                        LR 0.000013    Time 0.038536    
2023-01-06 16:44:45,095 - Epoch: [129][   30/  246]    Overall Loss 0.246578    Objective Loss 0.246578                                        LR 0.000013    Time 0.029588    
2023-01-06 16:44:45,215 - Epoch: [129][   40/  246]    Overall Loss 0.247671    Objective Loss 0.247671                                        LR 0.000013    Time 0.025174    
2023-01-06 16:44:45,340 - Epoch: [129][   50/  246]    Overall Loss 0.246505    Objective Loss 0.246505                                        LR 0.000013    Time 0.022626    
2023-01-06 16:44:45,467 - Epoch: [129][   60/  246]    Overall Loss 0.249269    Objective Loss 0.249269                                        LR 0.000013    Time 0.020959    
2023-01-06 16:44:45,588 - Epoch: [129][   70/  246]    Overall Loss 0.250470    Objective Loss 0.250470                                        LR 0.000013    Time 0.019693    
2023-01-06 16:44:45,707 - Epoch: [129][   80/  246]    Overall Loss 0.248131    Objective Loss 0.248131                                        LR 0.000013    Time 0.018711    
2023-01-06 16:44:45,836 - Epoch: [129][   90/  246]    Overall Loss 0.248776    Objective Loss 0.248776                                        LR 0.000013    Time 0.018062    
2023-01-06 16:44:45,966 - Epoch: [129][  100/  246]    Overall Loss 0.248307    Objective Loss 0.248307                                        LR 0.000013    Time 0.017552    
2023-01-06 16:44:46,104 - Epoch: [129][  110/  246]    Overall Loss 0.248748    Objective Loss 0.248748                                        LR 0.000013    Time 0.017204    
2023-01-06 16:44:46,240 - Epoch: [129][  120/  246]    Overall Loss 0.249034    Objective Loss 0.249034                                        LR 0.000013    Time 0.016906    
2023-01-06 16:44:46,379 - Epoch: [129][  130/  246]    Overall Loss 0.250175    Objective Loss 0.250175                                        LR 0.000013    Time 0.016668    
2023-01-06 16:44:46,536 - Epoch: [129][  140/  246]    Overall Loss 0.253119    Objective Loss 0.253119                                        LR 0.000013    Time 0.016593    
2023-01-06 16:44:46,701 - Epoch: [129][  150/  246]    Overall Loss 0.252254    Objective Loss 0.252254                                        LR 0.000013    Time 0.016586    
2023-01-06 16:44:46,867 - Epoch: [129][  160/  246]    Overall Loss 0.251648    Objective Loss 0.251648                                        LR 0.000013    Time 0.016584    
2023-01-06 16:44:47,026 - Epoch: [129][  170/  246]    Overall Loss 0.252051    Objective Loss 0.252051                                        LR 0.000013    Time 0.016540    
2023-01-06 16:44:47,194 - Epoch: [129][  180/  246]    Overall Loss 0.251462    Objective Loss 0.251462                                        LR 0.000013    Time 0.016556    
2023-01-06 16:44:47,358 - Epoch: [129][  190/  246]    Overall Loss 0.252519    Objective Loss 0.252519                                        LR 0.000013    Time 0.016542    
2023-01-06 16:44:47,537 - Epoch: [129][  200/  246]    Overall Loss 0.251918    Objective Loss 0.251918                                        LR 0.000013    Time 0.016610    
2023-01-06 16:44:47,715 - Epoch: [129][  210/  246]    Overall Loss 0.252456    Objective Loss 0.252456                                        LR 0.000013    Time 0.016663    
2023-01-06 16:44:47,889 - Epoch: [129][  220/  246]    Overall Loss 0.252698    Objective Loss 0.252698                                        LR 0.000013    Time 0.016693    
2023-01-06 16:44:48,063 - Epoch: [129][  230/  246]    Overall Loss 0.253823    Objective Loss 0.253823                                        LR 0.000013    Time 0.016724    
2023-01-06 16:44:48,254 - Epoch: [129][  240/  246]    Overall Loss 0.253663    Objective Loss 0.253663                                        LR 0.000013    Time 0.016823    
2023-01-06 16:44:48,331 - Epoch: [129][  246/  246]    Overall Loss 0.253221    Objective Loss 0.253221    Top1 92.583732    LR 0.000013    Time 0.016723    
2023-01-06 16:44:48,487 - --- validate (epoch=129)-----------
2023-01-06 16:44:48,487 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:48,924 - Epoch: [129][   10/   28]    Loss 0.263712    Top1 90.546875    
2023-01-06 16:44:49,023 - Epoch: [129][   20/   28]    Loss 0.266953    Top1 90.292969    
2023-01-06 16:44:49,075 - Epoch: [129][   28/   28]    Loss 0.265316    Top1 90.409390    
2023-01-06 16:44:49,206 - ==> Top1: 90.409    Loss: 0.265

2023-01-06 16:44:49,207 - ==> Confusion:
[[ 234   14  191]
 [  16  242  344]
 [  53   52 5840]]

2023-01-06 16:44:49,208 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:49,208 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:49,213 - 

2023-01-06 16:44:49,213 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:49,718 - Epoch: [130][   10/  246]    Overall Loss 0.230664    Objective Loss 0.230664                                        LR 0.000013    Time 0.050441    
2023-01-06 16:44:49,844 - Epoch: [130][   20/  246]    Overall Loss 0.239554    Objective Loss 0.239554                                        LR 0.000013    Time 0.031482    
2023-01-06 16:44:49,965 - Epoch: [130][   30/  246]    Overall Loss 0.244805    Objective Loss 0.244805                                        LR 0.000013    Time 0.025024    
2023-01-06 16:44:50,086 - Epoch: [130][   40/  246]    Overall Loss 0.247437    Objective Loss 0.247437                                        LR 0.000013    Time 0.021784    
2023-01-06 16:44:50,206 - Epoch: [130][   50/  246]    Overall Loss 0.249721    Objective Loss 0.249721                                        LR 0.000013    Time 0.019811    
2023-01-06 16:44:50,328 - Epoch: [130][   60/  246]    Overall Loss 0.254864    Objective Loss 0.254864                                        LR 0.000013    Time 0.018547    
2023-01-06 16:44:50,451 - Epoch: [130][   70/  246]    Overall Loss 0.255964    Objective Loss 0.255964                                        LR 0.000013    Time 0.017638    
2023-01-06 16:44:50,582 - Epoch: [130][   80/  246]    Overall Loss 0.255447    Objective Loss 0.255447                                        LR 0.000013    Time 0.017070    
2023-01-06 16:44:50,727 - Epoch: [130][   90/  246]    Overall Loss 0.257827    Objective Loss 0.257827                                        LR 0.000013    Time 0.016782    
2023-01-06 16:44:50,877 - Epoch: [130][  100/  246]    Overall Loss 0.257111    Objective Loss 0.257111                                        LR 0.000013    Time 0.016600    
2023-01-06 16:44:51,025 - Epoch: [130][  110/  246]    Overall Loss 0.256647    Objective Loss 0.256647                                        LR 0.000013    Time 0.016437    
2023-01-06 16:44:51,178 - Epoch: [130][  120/  246]    Overall Loss 0.257173    Objective Loss 0.257173                                        LR 0.000013    Time 0.016335    
2023-01-06 16:44:51,331 - Epoch: [130][  130/  246]    Overall Loss 0.255001    Objective Loss 0.255001                                        LR 0.000013    Time 0.016251    
2023-01-06 16:44:51,483 - Epoch: [130][  140/  246]    Overall Loss 0.254526    Objective Loss 0.254526                                        LR 0.000013    Time 0.016175    
2023-01-06 16:44:51,633 - Epoch: [130][  150/  246]    Overall Loss 0.254826    Objective Loss 0.254826                                        LR 0.000013    Time 0.016092    
2023-01-06 16:44:51,785 - Epoch: [130][  160/  246]    Overall Loss 0.254855    Objective Loss 0.254855                                        LR 0.000013    Time 0.016037    
2023-01-06 16:44:51,931 - Epoch: [130][  170/  246]    Overall Loss 0.254124    Objective Loss 0.254124                                        LR 0.000013    Time 0.015949    
2023-01-06 16:44:52,081 - Epoch: [130][  180/  246]    Overall Loss 0.254495    Objective Loss 0.254495                                        LR 0.000013    Time 0.015893    
2023-01-06 16:44:52,230 - Epoch: [130][  190/  246]    Overall Loss 0.254515    Objective Loss 0.254515                                        LR 0.000013    Time 0.015838    
2023-01-06 16:44:52,383 - Epoch: [130][  200/  246]    Overall Loss 0.254332    Objective Loss 0.254332                                        LR 0.000013    Time 0.015810    
2023-01-06 16:44:52,535 - Epoch: [130][  210/  246]    Overall Loss 0.254185    Objective Loss 0.254185                                        LR 0.000013    Time 0.015781    
2023-01-06 16:44:52,689 - Epoch: [130][  220/  246]    Overall Loss 0.253848    Objective Loss 0.253848                                        LR 0.000013    Time 0.015762    
2023-01-06 16:44:52,841 - Epoch: [130][  230/  246]    Overall Loss 0.253768    Objective Loss 0.253768                                        LR 0.000013    Time 0.015734    
2023-01-06 16:44:53,003 - Epoch: [130][  240/  246]    Overall Loss 0.253468    Objective Loss 0.253468                                        LR 0.000013    Time 0.015754    
2023-01-06 16:44:53,067 - Epoch: [130][  246/  246]    Overall Loss 0.252719    Objective Loss 0.252719    Top1 93.779904    LR 0.000013    Time 0.015629    
2023-01-06 16:44:53,193 - --- validate (epoch=130)-----------
2023-01-06 16:44:53,193 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:53,626 - Epoch: [130][   10/   28]    Loss 0.268385    Top1 90.859375    
2023-01-06 16:44:53,728 - Epoch: [130][   20/   28]    Loss 0.267794    Top1 90.664062    
2023-01-06 16:44:53,779 - Epoch: [130][   28/   28]    Loss 0.267925    Top1 90.523905    
2023-01-06 16:44:53,912 - ==> Top1: 90.524    Loss: 0.268

2023-01-06 16:44:53,912 - ==> Confusion:
[[ 235   14  190]
 [  13  270  319]
 [  51   75 5819]]

2023-01-06 16:44:53,913 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:53,913 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:53,918 - 

2023-01-06 16:44:53,918 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:44:54,582 - Epoch: [131][   10/  246]    Overall Loss 0.245084    Objective Loss 0.245084                                        LR 0.000013    Time 0.066264    
2023-01-06 16:44:54,719 - Epoch: [131][   20/  246]    Overall Loss 0.256599    Objective Loss 0.256599                                        LR 0.000013    Time 0.039974    
2023-01-06 16:44:54,863 - Epoch: [131][   30/  246]    Overall Loss 0.250101    Objective Loss 0.250101                                        LR 0.000013    Time 0.031440    
2023-01-06 16:44:55,008 - Epoch: [131][   40/  246]    Overall Loss 0.250864    Objective Loss 0.250864                                        LR 0.000013    Time 0.027174    
2023-01-06 16:44:55,151 - Epoch: [131][   50/  246]    Overall Loss 0.254485    Objective Loss 0.254485                                        LR 0.000013    Time 0.024587    
2023-01-06 16:44:55,296 - Epoch: [131][   60/  246]    Overall Loss 0.249713    Objective Loss 0.249713                                        LR 0.000013    Time 0.022902    
2023-01-06 16:44:55,455 - Epoch: [131][   70/  246]    Overall Loss 0.248074    Objective Loss 0.248074                                        LR 0.000013    Time 0.021885    
2023-01-06 16:44:55,623 - Epoch: [131][   80/  246]    Overall Loss 0.247018    Objective Loss 0.247018                                        LR 0.000013    Time 0.021254    
2023-01-06 16:44:55,801 - Epoch: [131][   90/  246]    Overall Loss 0.249366    Objective Loss 0.249366                                        LR 0.000013    Time 0.020848    
2023-01-06 16:44:55,979 - Epoch: [131][  100/  246]    Overall Loss 0.249570    Objective Loss 0.249570                                        LR 0.000013    Time 0.020539    
2023-01-06 16:44:56,159 - Epoch: [131][  110/  246]    Overall Loss 0.250205    Objective Loss 0.250205                                        LR 0.000013    Time 0.020301    
2023-01-06 16:44:56,342 - Epoch: [131][  120/  246]    Overall Loss 0.250056    Objective Loss 0.250056                                        LR 0.000013    Time 0.020128    
2023-01-06 16:44:56,519 - Epoch: [131][  130/  246]    Overall Loss 0.251019    Objective Loss 0.251019                                        LR 0.000013    Time 0.019937    
2023-01-06 16:44:56,700 - Epoch: [131][  140/  246]    Overall Loss 0.251795    Objective Loss 0.251795                                        LR 0.000013    Time 0.019807    
2023-01-06 16:44:56,875 - Epoch: [131][  150/  246]    Overall Loss 0.252706    Objective Loss 0.252706                                        LR 0.000013    Time 0.019651    
2023-01-06 16:44:57,056 - Epoch: [131][  160/  246]    Overall Loss 0.253454    Objective Loss 0.253454                                        LR 0.000013    Time 0.019550    
2023-01-06 16:44:57,235 - Epoch: [131][  170/  246]    Overall Loss 0.253190    Objective Loss 0.253190                                        LR 0.000013    Time 0.019437    
2023-01-06 16:44:57,418 - Epoch: [131][  180/  246]    Overall Loss 0.253370    Objective Loss 0.253370                                        LR 0.000013    Time 0.019374    
2023-01-06 16:44:57,595 - Epoch: [131][  190/  246]    Overall Loss 0.252947    Objective Loss 0.252947                                        LR 0.000013    Time 0.019281    
2023-01-06 16:44:57,778 - Epoch: [131][  200/  246]    Overall Loss 0.253026    Objective Loss 0.253026                                        LR 0.000013    Time 0.019233    
2023-01-06 16:44:57,958 - Epoch: [131][  210/  246]    Overall Loss 0.253129    Objective Loss 0.253129                                        LR 0.000013    Time 0.019169    
2023-01-06 16:44:58,141 - Epoch: [131][  220/  246]    Overall Loss 0.254194    Objective Loss 0.254194                                        LR 0.000013    Time 0.019128    
2023-01-06 16:44:58,319 - Epoch: [131][  230/  246]    Overall Loss 0.254266    Objective Loss 0.254266                                        LR 0.000013    Time 0.019070    
2023-01-06 16:44:58,515 - Epoch: [131][  240/  246]    Overall Loss 0.253399    Objective Loss 0.253399                                        LR 0.000013    Time 0.019091    
2023-01-06 16:44:58,595 - Epoch: [131][  246/  246]    Overall Loss 0.252829    Objective Loss 0.252829    Top1 90.669856    LR 0.000013    Time 0.018947    
2023-01-06 16:44:58,730 - --- validate (epoch=131)-----------
2023-01-06 16:44:58,730 - 6986 samples (256 per mini-batch)
2023-01-06 16:44:59,160 - Epoch: [131][   10/   28]    Loss 0.284621    Top1 89.882812    
2023-01-06 16:44:59,270 - Epoch: [131][   20/   28]    Loss 0.264442    Top1 90.527344    
2023-01-06 16:44:59,322 - Epoch: [131][   28/   28]    Loss 0.262716    Top1 90.438019    
2023-01-06 16:44:59,464 - ==> Top1: 90.438    Loss: 0.263

2023-01-06 16:44:59,464 - ==> Confusion:
[[ 237   15  187]
 [  14  247  341]
 [  54   57 5834]]

2023-01-06 16:44:59,465 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:44:59,465 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:44:59,470 - 

2023-01-06 16:44:59,470 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:00,008 - Epoch: [132][   10/  246]    Overall Loss 0.269649    Objective Loss 0.269649                                        LR 0.000013    Time 0.053770    
2023-01-06 16:45:00,157 - Epoch: [132][   20/  246]    Overall Loss 0.254649    Objective Loss 0.254649                                        LR 0.000013    Time 0.034276    
2023-01-06 16:45:00,304 - Epoch: [132][   30/  246]    Overall Loss 0.258966    Objective Loss 0.258966                                        LR 0.000013    Time 0.027753    
2023-01-06 16:45:00,447 - Epoch: [132][   40/  246]    Overall Loss 0.257252    Objective Loss 0.257252                                        LR 0.000013    Time 0.024372    
2023-01-06 16:45:00,590 - Epoch: [132][   50/  246]    Overall Loss 0.253932    Objective Loss 0.253932                                        LR 0.000013    Time 0.022346    
2023-01-06 16:45:00,732 - Epoch: [132][   60/  246]    Overall Loss 0.252750    Objective Loss 0.252750                                        LR 0.000013    Time 0.020991    
2023-01-06 16:45:00,874 - Epoch: [132][   70/  246]    Overall Loss 0.255785    Objective Loss 0.255785                                        LR 0.000013    Time 0.020013    
2023-01-06 16:45:01,016 - Epoch: [132][   80/  246]    Overall Loss 0.256397    Objective Loss 0.256397                                        LR 0.000013    Time 0.019278    
2023-01-06 16:45:01,155 - Epoch: [132][   90/  246]    Overall Loss 0.254977    Objective Loss 0.254977                                        LR 0.000013    Time 0.018681    
2023-01-06 16:45:01,293 - Epoch: [132][  100/  246]    Overall Loss 0.254444    Objective Loss 0.254444                                        LR 0.000013    Time 0.018189    
2023-01-06 16:45:01,435 - Epoch: [132][  110/  246]    Overall Loss 0.254793    Objective Loss 0.254793                                        LR 0.000013    Time 0.017822    
2023-01-06 16:45:01,573 - Epoch: [132][  120/  246]    Overall Loss 0.256382    Objective Loss 0.256382                                        LR 0.000013    Time 0.017483    
2023-01-06 16:45:01,713 - Epoch: [132][  130/  246]    Overall Loss 0.255205    Objective Loss 0.255205                                        LR 0.000013    Time 0.017211    
2023-01-06 16:45:01,864 - Epoch: [132][  140/  246]    Overall Loss 0.255245    Objective Loss 0.255245                                        LR 0.000013    Time 0.017061    
2023-01-06 16:45:02,016 - Epoch: [132][  150/  246]    Overall Loss 0.253752    Objective Loss 0.253752                                        LR 0.000013    Time 0.016937    
2023-01-06 16:45:02,171 - Epoch: [132][  160/  246]    Overall Loss 0.252681    Objective Loss 0.252681                                        LR 0.000013    Time 0.016839    
2023-01-06 16:45:02,332 - Epoch: [132][  170/  246]    Overall Loss 0.252802    Objective Loss 0.252802                                        LR 0.000013    Time 0.016794    
2023-01-06 16:45:02,494 - Epoch: [132][  180/  246]    Overall Loss 0.253211    Objective Loss 0.253211                                        LR 0.000013    Time 0.016762    
2023-01-06 16:45:02,654 - Epoch: [132][  190/  246]    Overall Loss 0.253435    Objective Loss 0.253435                                        LR 0.000013    Time 0.016716    
2023-01-06 16:45:02,815 - Epoch: [132][  200/  246]    Overall Loss 0.254148    Objective Loss 0.254148                                        LR 0.000013    Time 0.016686    
2023-01-06 16:45:02,954 - Epoch: [132][  210/  246]    Overall Loss 0.254149    Objective Loss 0.254149                                        LR 0.000013    Time 0.016547    
2023-01-06 16:45:03,096 - Epoch: [132][  220/  246]    Overall Loss 0.254325    Objective Loss 0.254325                                        LR 0.000013    Time 0.016438    
2023-01-06 16:45:03,239 - Epoch: [132][  230/  246]    Overall Loss 0.254039    Objective Loss 0.254039                                        LR 0.000013    Time 0.016346    
2023-01-06 16:45:03,394 - Epoch: [132][  240/  246]    Overall Loss 0.253726    Objective Loss 0.253726                                        LR 0.000013    Time 0.016309    
2023-01-06 16:45:03,461 - Epoch: [132][  246/  246]    Overall Loss 0.253351    Objective Loss 0.253351    Top1 92.105263    LR 0.000013    Time 0.016181    
2023-01-06 16:45:03,600 - --- validate (epoch=132)-----------
2023-01-06 16:45:03,601 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:04,152 - Epoch: [132][   10/   28]    Loss 0.262360    Top1 91.093750    
2023-01-06 16:45:04,251 - Epoch: [132][   20/   28]    Loss 0.266551    Top1 90.703125    
2023-01-06 16:45:04,301 - Epoch: [132][   28/   28]    Loss 0.267122    Top1 90.466648    
2023-01-06 16:45:04,464 - ==> Top1: 90.467    Loss: 0.267

2023-01-06 16:45:04,464 - ==> Confusion:
[[ 226   14  199]
 [  13  266  323]
 [  46   71 5828]]

2023-01-06 16:45:04,466 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:04,466 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:04,471 - 

2023-01-06 16:45:04,471 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:04,984 - Epoch: [133][   10/  246]    Overall Loss 0.251402    Objective Loss 0.251402                                        LR 0.000013    Time 0.051288    
2023-01-06 16:45:05,119 - Epoch: [133][   20/  246]    Overall Loss 0.244705    Objective Loss 0.244705                                        LR 0.000013    Time 0.032338    
2023-01-06 16:45:05,252 - Epoch: [133][   30/  246]    Overall Loss 0.248696    Objective Loss 0.248696                                        LR 0.000013    Time 0.025980    
2023-01-06 16:45:05,388 - Epoch: [133][   40/  246]    Overall Loss 0.249266    Objective Loss 0.249266                                        LR 0.000013    Time 0.022869    
2023-01-06 16:45:05,518 - Epoch: [133][   50/  246]    Overall Loss 0.245953    Objective Loss 0.245953                                        LR 0.000013    Time 0.020904    
2023-01-06 16:45:05,665 - Epoch: [133][   60/  246]    Overall Loss 0.246955    Objective Loss 0.246955                                        LR 0.000013    Time 0.019861    
2023-01-06 16:45:05,812 - Epoch: [133][   70/  246]    Overall Loss 0.247809    Objective Loss 0.247809                                        LR 0.000013    Time 0.019112    
2023-01-06 16:45:05,962 - Epoch: [133][   80/  246]    Overall Loss 0.248879    Objective Loss 0.248879                                        LR 0.000013    Time 0.018595    
2023-01-06 16:45:06,117 - Epoch: [133][   90/  246]    Overall Loss 0.249961    Objective Loss 0.249961                                        LR 0.000013    Time 0.018245    
2023-01-06 16:45:06,265 - Epoch: [133][  100/  246]    Overall Loss 0.250738    Objective Loss 0.250738                                        LR 0.000013    Time 0.017896    
2023-01-06 16:45:06,418 - Epoch: [133][  110/  246]    Overall Loss 0.252368    Objective Loss 0.252368                                        LR 0.000013    Time 0.017646    
2023-01-06 16:45:06,564 - Epoch: [133][  120/  246]    Overall Loss 0.252437    Objective Loss 0.252437                                        LR 0.000013    Time 0.017385    
2023-01-06 16:45:06,715 - Epoch: [133][  130/  246]    Overall Loss 0.254030    Objective Loss 0.254030                                        LR 0.000013    Time 0.017196    
2023-01-06 16:45:06,863 - Epoch: [133][  140/  246]    Overall Loss 0.254082    Objective Loss 0.254082                                        LR 0.000013    Time 0.017022    
2023-01-06 16:45:07,012 - Epoch: [133][  150/  246]    Overall Loss 0.252381    Objective Loss 0.252381                                        LR 0.000013    Time 0.016869    
2023-01-06 16:45:07,167 - Epoch: [133][  160/  246]    Overall Loss 0.252994    Objective Loss 0.252994                                        LR 0.000013    Time 0.016778    
2023-01-06 16:45:07,321 - Epoch: [133][  170/  246]    Overall Loss 0.254154    Objective Loss 0.254154                                        LR 0.000013    Time 0.016698    
2023-01-06 16:45:07,471 - Epoch: [133][  180/  246]    Overall Loss 0.253776    Objective Loss 0.253776                                        LR 0.000013    Time 0.016597    
2023-01-06 16:45:07,628 - Epoch: [133][  190/  246]    Overall Loss 0.253028    Objective Loss 0.253028                                        LR 0.000013    Time 0.016541    
2023-01-06 16:45:07,771 - Epoch: [133][  200/  246]    Overall Loss 0.252720    Objective Loss 0.252720                                        LR 0.000013    Time 0.016425    
2023-01-06 16:45:07,914 - Epoch: [133][  210/  246]    Overall Loss 0.253026    Objective Loss 0.253026                                        LR 0.000013    Time 0.016324    
2023-01-06 16:45:08,059 - Epoch: [133][  220/  246]    Overall Loss 0.252740    Objective Loss 0.252740                                        LR 0.000013    Time 0.016238    
2023-01-06 16:45:08,217 - Epoch: [133][  230/  246]    Overall Loss 0.252642    Objective Loss 0.252642                                        LR 0.000013    Time 0.016214    
2023-01-06 16:45:08,376 - Epoch: [133][  240/  246]    Overall Loss 0.253084    Objective Loss 0.253084                                        LR 0.000013    Time 0.016200    
2023-01-06 16:45:08,446 - Epoch: [133][  246/  246]    Overall Loss 0.252940    Objective Loss 0.252940    Top1 92.105263    LR 0.000013    Time 0.016086    
2023-01-06 16:45:08,626 - --- validate (epoch=133)-----------
2023-01-06 16:45:08,627 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:09,071 - Epoch: [133][   10/   28]    Loss 0.265815    Top1 90.742188    
2023-01-06 16:45:09,173 - Epoch: [133][   20/   28]    Loss 0.270815    Top1 90.312500    
2023-01-06 16:45:09,226 - Epoch: [133][   28/   28]    Loss 0.272705    Top1 90.137418    
2023-01-06 16:45:09,388 - ==> Top1: 90.137    Loss: 0.273

2023-01-06 16:45:09,389 - ==> Confusion:
[[ 220   12  207]
 [  13  267  322]
 [  53   82 5810]]

2023-01-06 16:45:09,390 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:09,390 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:09,395 - 

2023-01-06 16:45:09,395 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:10,060 - Epoch: [134][   10/  246]    Overall Loss 0.269069    Objective Loss 0.269069                                        LR 0.000013    Time 0.066430    
2023-01-06 16:45:10,198 - Epoch: [134][   20/  246]    Overall Loss 0.258389    Objective Loss 0.258389                                        LR 0.000013    Time 0.040079    
2023-01-06 16:45:10,332 - Epoch: [134][   30/  246]    Overall Loss 0.260642    Objective Loss 0.260642                                        LR 0.000013    Time 0.031186    
2023-01-06 16:45:10,471 - Epoch: [134][   40/  246]    Overall Loss 0.263864    Objective Loss 0.263864                                        LR 0.000013    Time 0.026849    
2023-01-06 16:45:10,609 - Epoch: [134][   50/  246]    Overall Loss 0.260747    Objective Loss 0.260747                                        LR 0.000013    Time 0.024236    
2023-01-06 16:45:10,749 - Epoch: [134][   60/  246]    Overall Loss 0.260723    Objective Loss 0.260723                                        LR 0.000013    Time 0.022524    
2023-01-06 16:45:10,888 - Epoch: [134][   70/  246]    Overall Loss 0.260552    Objective Loss 0.260552                                        LR 0.000013    Time 0.021284    
2023-01-06 16:45:11,022 - Epoch: [134][   80/  246]    Overall Loss 0.259609    Objective Loss 0.259609                                        LR 0.000013    Time 0.020288    
2023-01-06 16:45:11,156 - Epoch: [134][   90/  246]    Overall Loss 0.257873    Objective Loss 0.257873                                        LR 0.000013    Time 0.019528    
2023-01-06 16:45:11,277 - Epoch: [134][  100/  246]    Overall Loss 0.256975    Objective Loss 0.256975                                        LR 0.000013    Time 0.018777    
2023-01-06 16:45:11,403 - Epoch: [134][  110/  246]    Overall Loss 0.255436    Objective Loss 0.255436                                        LR 0.000013    Time 0.018211    
2023-01-06 16:45:11,531 - Epoch: [134][  120/  246]    Overall Loss 0.254993    Objective Loss 0.254993                                        LR 0.000013    Time 0.017751    
2023-01-06 16:45:11,656 - Epoch: [134][  130/  246]    Overall Loss 0.254448    Objective Loss 0.254448                                        LR 0.000013    Time 0.017345    
2023-01-06 16:45:11,781 - Epoch: [134][  140/  246]    Overall Loss 0.254635    Objective Loss 0.254635                                        LR 0.000013    Time 0.016998    
2023-01-06 16:45:11,914 - Epoch: [134][  150/  246]    Overall Loss 0.254995    Objective Loss 0.254995                                        LR 0.000013    Time 0.016745    
2023-01-06 16:45:12,065 - Epoch: [134][  160/  246]    Overall Loss 0.254389    Objective Loss 0.254389                                        LR 0.000013    Time 0.016638    
2023-01-06 16:45:12,214 - Epoch: [134][  170/  246]    Overall Loss 0.254276    Objective Loss 0.254276                                        LR 0.000013    Time 0.016534    
2023-01-06 16:45:12,368 - Epoch: [134][  180/  246]    Overall Loss 0.253253    Objective Loss 0.253253                                        LR 0.000013    Time 0.016472    
2023-01-06 16:45:12,527 - Epoch: [134][  190/  246]    Overall Loss 0.252707    Objective Loss 0.252707                                        LR 0.000013    Time 0.016435    
2023-01-06 16:45:12,675 - Epoch: [134][  200/  246]    Overall Loss 0.253574    Objective Loss 0.253574                                        LR 0.000013    Time 0.016352    
2023-01-06 16:45:12,821 - Epoch: [134][  210/  246]    Overall Loss 0.253727    Objective Loss 0.253727                                        LR 0.000013    Time 0.016264    
2023-01-06 16:45:12,963 - Epoch: [134][  220/  246]    Overall Loss 0.251919    Objective Loss 0.251919                                        LR 0.000013    Time 0.016170    
2023-01-06 16:45:13,095 - Epoch: [134][  230/  246]    Overall Loss 0.251444    Objective Loss 0.251444                                        LR 0.000013    Time 0.016038    
2023-01-06 16:45:13,234 - Epoch: [134][  240/  246]    Overall Loss 0.251919    Objective Loss 0.251919                                        LR 0.000013    Time 0.015947    
2023-01-06 16:45:13,292 - Epoch: [134][  246/  246]    Overall Loss 0.251979    Objective Loss 0.251979    Top1 88.995215    LR 0.000013    Time 0.015795    
2023-01-06 16:45:13,423 - --- validate (epoch=134)-----------
2023-01-06 16:45:13,423 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:13,843 - Epoch: [134][   10/   28]    Loss 0.257581    Top1 91.054688    
2023-01-06 16:45:13,953 - Epoch: [134][   20/   28]    Loss 0.262087    Top1 90.683594    
2023-01-06 16:45:14,001 - Epoch: [134][   28/   28]    Loss 0.263461    Top1 90.466648    
2023-01-06 16:45:14,148 - ==> Top1: 90.467    Loss: 0.263

2023-01-06 16:45:14,148 - ==> Confusion:
[[ 244   14  181]
 [  15  259  328]
 [  60   68 5817]]

2023-01-06 16:45:14,149 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:14,149 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:14,154 - 

2023-01-06 16:45:14,155 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:14,677 - Epoch: [135][   10/  246]    Overall Loss 0.247373    Objective Loss 0.247373                                        LR 0.000013    Time 0.052211    
2023-01-06 16:45:14,815 - Epoch: [135][   20/  246]    Overall Loss 0.249733    Objective Loss 0.249733                                        LR 0.000013    Time 0.032961    
2023-01-06 16:45:14,951 - Epoch: [135][   30/  246]    Overall Loss 0.248391    Objective Loss 0.248391                                        LR 0.000013    Time 0.026495    
2023-01-06 16:45:15,088 - Epoch: [135][   40/  246]    Overall Loss 0.250738    Objective Loss 0.250738                                        LR 0.000013    Time 0.023240    
2023-01-06 16:45:15,227 - Epoch: [135][   50/  246]    Overall Loss 0.251475    Objective Loss 0.251475                                        LR 0.000013    Time 0.021322    
2023-01-06 16:45:15,364 - Epoch: [135][   60/  246]    Overall Loss 0.251062    Objective Loss 0.251062                                        LR 0.000013    Time 0.020058    
2023-01-06 16:45:15,499 - Epoch: [135][   70/  246]    Overall Loss 0.248411    Objective Loss 0.248411                                        LR 0.000013    Time 0.019113    
2023-01-06 16:45:15,636 - Epoch: [135][   80/  246]    Overall Loss 0.250847    Objective Loss 0.250847                                        LR 0.000013    Time 0.018432    
2023-01-06 16:45:15,769 - Epoch: [135][   90/  246]    Overall Loss 0.248763    Objective Loss 0.248763                                        LR 0.000013    Time 0.017849    
2023-01-06 16:45:15,906 - Epoch: [135][  100/  246]    Overall Loss 0.249534    Objective Loss 0.249534                                        LR 0.000013    Time 0.017433    
2023-01-06 16:45:16,034 - Epoch: [135][  110/  246]    Overall Loss 0.249827    Objective Loss 0.249827                                        LR 0.000013    Time 0.017012    
2023-01-06 16:45:16,162 - Epoch: [135][  120/  246]    Overall Loss 0.250939    Objective Loss 0.250939                                        LR 0.000013    Time 0.016657    
2023-01-06 16:45:16,287 - Epoch: [135][  130/  246]    Overall Loss 0.251059    Objective Loss 0.251059                                        LR 0.000013    Time 0.016331    
2023-01-06 16:45:16,413 - Epoch: [135][  140/  246]    Overall Loss 0.249778    Objective Loss 0.249778                                        LR 0.000013    Time 0.016050    
2023-01-06 16:45:16,542 - Epoch: [135][  150/  246]    Overall Loss 0.250618    Objective Loss 0.250618                                        LR 0.000013    Time 0.015822    
2023-01-06 16:45:16,668 - Epoch: [135][  160/  246]    Overall Loss 0.251601    Objective Loss 0.251601                                        LR 0.000013    Time 0.015619    
2023-01-06 16:45:16,796 - Epoch: [135][  170/  246]    Overall Loss 0.251736    Objective Loss 0.251736                                        LR 0.000013    Time 0.015446    
2023-01-06 16:45:16,924 - Epoch: [135][  180/  246]    Overall Loss 0.252387    Objective Loss 0.252387                                        LR 0.000013    Time 0.015296    
2023-01-06 16:45:17,061 - Epoch: [135][  190/  246]    Overall Loss 0.250690    Objective Loss 0.250690                                        LR 0.000013    Time 0.015213    
2023-01-06 16:45:17,207 - Epoch: [135][  200/  246]    Overall Loss 0.250609    Objective Loss 0.250609                                        LR 0.000013    Time 0.015181    
2023-01-06 16:45:17,351 - Epoch: [135][  210/  246]    Overall Loss 0.251247    Objective Loss 0.251247                                        LR 0.000013    Time 0.015141    
2023-01-06 16:45:17,495 - Epoch: [135][  220/  246]    Overall Loss 0.251238    Objective Loss 0.251238                                        LR 0.000013    Time 0.015104    
2023-01-06 16:45:17,639 - Epoch: [135][  230/  246]    Overall Loss 0.252072    Objective Loss 0.252072                                        LR 0.000013    Time 0.015070    
2023-01-06 16:45:17,792 - Epoch: [135][  240/  246]    Overall Loss 0.252681    Objective Loss 0.252681                                        LR 0.000013    Time 0.015077    
2023-01-06 16:45:17,858 - Epoch: [135][  246/  246]    Overall Loss 0.252579    Objective Loss 0.252579    Top1 90.191388    LR 0.000013    Time 0.014979    
2023-01-06 16:45:17,989 - --- validate (epoch=135)-----------
2023-01-06 16:45:17,989 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:18,417 - Epoch: [135][   10/   28]    Loss 0.275167    Top1 90.039062    
2023-01-06 16:45:18,517 - Epoch: [135][   20/   28]    Loss 0.275553    Top1 89.941406    
2023-01-06 16:45:18,566 - Epoch: [135][   28/   28]    Loss 0.264399    Top1 90.294875    
2023-01-06 16:45:18,709 - ==> Top1: 90.295    Loss: 0.264

2023-01-06 16:45:18,710 - ==> Confusion:
[[ 225   15  199]
 [  19  241  342]
 [  54   49 5842]]

2023-01-06 16:45:18,711 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:18,711 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:18,716 - 

2023-01-06 16:45:18,716 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:19,375 - Epoch: [136][   10/  246]    Overall Loss 0.270151    Objective Loss 0.270151                                        LR 0.000013    Time 0.065870    
2023-01-06 16:45:19,509 - Epoch: [136][   20/  246]    Overall Loss 0.254052    Objective Loss 0.254052                                        LR 0.000013    Time 0.039611    
2023-01-06 16:45:19,643 - Epoch: [136][   30/  246]    Overall Loss 0.253947    Objective Loss 0.253947                                        LR 0.000013    Time 0.030869    
2023-01-06 16:45:19,778 - Epoch: [136][   40/  246]    Overall Loss 0.252935    Objective Loss 0.252935                                        LR 0.000013    Time 0.026507    
2023-01-06 16:45:19,926 - Epoch: [136][   50/  246]    Overall Loss 0.249688    Objective Loss 0.249688                                        LR 0.000013    Time 0.024154    
2023-01-06 16:45:20,058 - Epoch: [136][   60/  246]    Overall Loss 0.247143    Objective Loss 0.247143                                        LR 0.000013    Time 0.022313    
2023-01-06 16:45:20,196 - Epoch: [136][   70/  246]    Overall Loss 0.248089    Objective Loss 0.248089                                        LR 0.000013    Time 0.021088    
2023-01-06 16:45:20,326 - Epoch: [136][   80/  246]    Overall Loss 0.248545    Objective Loss 0.248545                                        LR 0.000013    Time 0.020065    
2023-01-06 16:45:20,472 - Epoch: [136][   90/  246]    Overall Loss 0.251902    Objective Loss 0.251902                                        LR 0.000013    Time 0.019460    
2023-01-06 16:45:20,614 - Epoch: [136][  100/  246]    Overall Loss 0.251106    Objective Loss 0.251106                                        LR 0.000013    Time 0.018933    
2023-01-06 16:45:20,740 - Epoch: [136][  110/  246]    Overall Loss 0.251318    Objective Loss 0.251318                                        LR 0.000013    Time 0.018347    
2023-01-06 16:45:20,867 - Epoch: [136][  120/  246]    Overall Loss 0.251531    Objective Loss 0.251531                                        LR 0.000013    Time 0.017876    
2023-01-06 16:45:20,993 - Epoch: [136][  130/  246]    Overall Loss 0.252248    Objective Loss 0.252248                                        LR 0.000013    Time 0.017467    
2023-01-06 16:45:21,120 - Epoch: [136][  140/  246]    Overall Loss 0.252695    Objective Loss 0.252695                                        LR 0.000013    Time 0.017121    
2023-01-06 16:45:21,250 - Epoch: [136][  150/  246]    Overall Loss 0.251922    Objective Loss 0.251922                                        LR 0.000013    Time 0.016834    
2023-01-06 16:45:21,386 - Epoch: [136][  160/  246]    Overall Loss 0.252813    Objective Loss 0.252813                                        LR 0.000013    Time 0.016630    
2023-01-06 16:45:21,522 - Epoch: [136][  170/  246]    Overall Loss 0.253594    Objective Loss 0.253594                                        LR 0.000013    Time 0.016449    
2023-01-06 16:45:21,658 - Epoch: [136][  180/  246]    Overall Loss 0.254298    Objective Loss 0.254298                                        LR 0.000013    Time 0.016291    
2023-01-06 16:45:21,795 - Epoch: [136][  190/  246]    Overall Loss 0.255058    Objective Loss 0.255058                                        LR 0.000013    Time 0.016149    
2023-01-06 16:45:21,932 - Epoch: [136][  200/  246]    Overall Loss 0.254053    Objective Loss 0.254053                                        LR 0.000013    Time 0.016025    
2023-01-06 16:45:22,067 - Epoch: [136][  210/  246]    Overall Loss 0.253750    Objective Loss 0.253750                                        LR 0.000013    Time 0.015902    
2023-01-06 16:45:22,204 - Epoch: [136][  220/  246]    Overall Loss 0.253395    Objective Loss 0.253395                                        LR 0.000013    Time 0.015802    
2023-01-06 16:45:22,339 - Epoch: [136][  230/  246]    Overall Loss 0.252532    Objective Loss 0.252532                                        LR 0.000013    Time 0.015700    
2023-01-06 16:45:22,485 - Epoch: [136][  240/  246]    Overall Loss 0.252188    Objective Loss 0.252188                                        LR 0.000013    Time 0.015652    
2023-01-06 16:45:22,551 - Epoch: [136][  246/  246]    Overall Loss 0.252362    Objective Loss 0.252362    Top1 91.626794    LR 0.000013    Time 0.015539    
2023-01-06 16:45:22,680 - --- validate (epoch=136)-----------
2023-01-06 16:45:22,681 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:23,103 - Epoch: [136][   10/   28]    Loss 0.271349    Top1 90.000000    
2023-01-06 16:45:23,202 - Epoch: [136][   20/   28]    Loss 0.269992    Top1 90.195312    
2023-01-06 16:45:23,251 - Epoch: [136][   28/   28]    Loss 0.265908    Top1 90.566848    
2023-01-06 16:45:23,380 - ==> Top1: 90.567    Loss: 0.266

2023-01-06 16:45:23,380 - ==> Confusion:
[[ 242   16  181]
 [  15  261  326]
 [  57   64 5824]]

2023-01-06 16:45:23,381 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:23,381 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:23,387 - 

2023-01-06 16:45:23,387 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:24,063 - Epoch: [137][   10/  246]    Overall Loss 0.251512    Objective Loss 0.251512                                        LR 0.000013    Time 0.067563    
2023-01-06 16:45:24,205 - Epoch: [137][   20/  246]    Overall Loss 0.252140    Objective Loss 0.252140                                        LR 0.000013    Time 0.040842    
2023-01-06 16:45:24,351 - Epoch: [137][   30/  246]    Overall Loss 0.250622    Objective Loss 0.250622                                        LR 0.000013    Time 0.032103    
2023-01-06 16:45:24,489 - Epoch: [137][   40/  246]    Overall Loss 0.251628    Objective Loss 0.251628                                        LR 0.000013    Time 0.027503    
2023-01-06 16:45:24,626 - Epoch: [137][   50/  246]    Overall Loss 0.250119    Objective Loss 0.250119                                        LR 0.000013    Time 0.024738    
2023-01-06 16:45:24,763 - Epoch: [137][   60/  246]    Overall Loss 0.252363    Objective Loss 0.252363                                        LR 0.000013    Time 0.022900    
2023-01-06 16:45:24,901 - Epoch: [137][   70/  246]    Overall Loss 0.251505    Objective Loss 0.251505                                        LR 0.000013    Time 0.021588    
2023-01-06 16:45:25,036 - Epoch: [137][   80/  246]    Overall Loss 0.255216    Objective Loss 0.255216                                        LR 0.000013    Time 0.020575    
2023-01-06 16:45:25,176 - Epoch: [137][   90/  246]    Overall Loss 0.252656    Objective Loss 0.252656                                        LR 0.000013    Time 0.019838    
2023-01-06 16:45:25,314 - Epoch: [137][  100/  246]    Overall Loss 0.252099    Objective Loss 0.252099                                        LR 0.000013    Time 0.019227    
2023-01-06 16:45:25,451 - Epoch: [137][  110/  246]    Overall Loss 0.252261    Objective Loss 0.252261                                        LR 0.000013    Time 0.018721    
2023-01-06 16:45:25,583 - Epoch: [137][  120/  246]    Overall Loss 0.253031    Objective Loss 0.253031                                        LR 0.000013    Time 0.018258    
2023-01-06 16:45:25,713 - Epoch: [137][  130/  246]    Overall Loss 0.252241    Objective Loss 0.252241                                        LR 0.000013    Time 0.017848    
2023-01-06 16:45:25,848 - Epoch: [137][  140/  246]    Overall Loss 0.252227    Objective Loss 0.252227                                        LR 0.000013    Time 0.017525    
2023-01-06 16:45:25,981 - Epoch: [137][  150/  246]    Overall Loss 0.252237    Objective Loss 0.252237                                        LR 0.000013    Time 0.017241    
2023-01-06 16:45:26,115 - Epoch: [137][  160/  246]    Overall Loss 0.252005    Objective Loss 0.252005                                        LR 0.000013    Time 0.016999    
2023-01-06 16:45:26,249 - Epoch: [137][  170/  246]    Overall Loss 0.251038    Objective Loss 0.251038                                        LR 0.000013    Time 0.016784    
2023-01-06 16:45:26,390 - Epoch: [137][  180/  246]    Overall Loss 0.250558    Objective Loss 0.250558                                        LR 0.000013    Time 0.016632    
2023-01-06 16:45:26,533 - Epoch: [137][  190/  246]    Overall Loss 0.250525    Objective Loss 0.250525                                        LR 0.000013    Time 0.016507    
2023-01-06 16:45:26,674 - Epoch: [137][  200/  246]    Overall Loss 0.250402    Objective Loss 0.250402                                        LR 0.000013    Time 0.016387    
2023-01-06 16:45:26,815 - Epoch: [137][  210/  246]    Overall Loss 0.251122    Objective Loss 0.251122                                        LR 0.000013    Time 0.016277    
2023-01-06 16:45:26,957 - Epoch: [137][  220/  246]    Overall Loss 0.250614    Objective Loss 0.250614                                        LR 0.000013    Time 0.016181    
2023-01-06 16:45:27,099 - Epoch: [137][  230/  246]    Overall Loss 0.251977    Objective Loss 0.251977                                        LR 0.000013    Time 0.016091    
2023-01-06 16:45:27,254 - Epoch: [137][  240/  246]    Overall Loss 0.252101    Objective Loss 0.252101                                        LR 0.000013    Time 0.016064    
2023-01-06 16:45:27,318 - Epoch: [137][  246/  246]    Overall Loss 0.251507    Objective Loss 0.251507    Top1 93.062201    LR 0.000013    Time 0.015934    
2023-01-06 16:45:27,459 - --- validate (epoch=137)-----------
2023-01-06 16:45:27,460 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:27,889 - Epoch: [137][   10/   28]    Loss 0.265385    Top1 90.273438    
2023-01-06 16:45:27,984 - Epoch: [137][   20/   28]    Loss 0.270740    Top1 90.019531    
2023-01-06 16:45:28,036 - Epoch: [137][   28/   28]    Loss 0.266414    Top1 90.251932    
2023-01-06 16:45:28,196 - ==> Top1: 90.252    Loss: 0.266

2023-01-06 16:45:28,196 - ==> Confusion:
[[ 212   15  212]
 [  13  244  345]
 [  42   54 5849]]

2023-01-06 16:45:28,197 - ==> Best [Top1: 90.595   Sparsity:0.00   Params: 151104 on epoch: 119]
2023-01-06 16:45:28,197 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:28,202 - 

2023-01-06 16:45:28,202 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:28,717 - Epoch: [138][   10/  246]    Overall Loss 0.256663    Objective Loss 0.256663                                        LR 0.000013    Time 0.051452    
2023-01-06 16:45:28,851 - Epoch: [138][   20/  246]    Overall Loss 0.260680    Objective Loss 0.260680                                        LR 0.000013    Time 0.032388    
2023-01-06 16:45:28,993 - Epoch: [138][   30/  246]    Overall Loss 0.258984    Objective Loss 0.258984                                        LR 0.000013    Time 0.026321    
2023-01-06 16:45:29,139 - Epoch: [138][   40/  246]    Overall Loss 0.254727    Objective Loss 0.254727                                        LR 0.000013    Time 0.023383    
2023-01-06 16:45:29,272 - Epoch: [138][   50/  246]    Overall Loss 0.257930    Objective Loss 0.257930                                        LR 0.000013    Time 0.021356    
2023-01-06 16:45:29,402 - Epoch: [138][   60/  246]    Overall Loss 0.255027    Objective Loss 0.255027                                        LR 0.000013    Time 0.019946    
2023-01-06 16:45:29,529 - Epoch: [138][   70/  246]    Overall Loss 0.253971    Objective Loss 0.253971                                        LR 0.000013    Time 0.018914    
2023-01-06 16:45:29,658 - Epoch: [138][   80/  246]    Overall Loss 0.255290    Objective Loss 0.255290                                        LR 0.000013    Time 0.018152    
2023-01-06 16:45:29,791 - Epoch: [138][   90/  246]    Overall Loss 0.256411    Objective Loss 0.256411                                        LR 0.000013    Time 0.017592    
2023-01-06 16:45:29,926 - Epoch: [138][  100/  246]    Overall Loss 0.257875    Objective Loss 0.257875                                        LR 0.000013    Time 0.017176    
2023-01-06 16:45:30,061 - Epoch: [138][  110/  246]    Overall Loss 0.256212    Objective Loss 0.256212                                        LR 0.000013    Time 0.016837    
2023-01-06 16:45:30,196 - Epoch: [138][  120/  246]    Overall Loss 0.255490    Objective Loss 0.255490                                        LR 0.000013    Time 0.016560    
2023-01-06 16:45:30,334 - Epoch: [138][  130/  246]    Overall Loss 0.253912    Objective Loss 0.253912                                        LR 0.000013    Time 0.016344    
2023-01-06 16:45:30,472 - Epoch: [138][  140/  246]    Overall Loss 0.253879    Objective Loss 0.253879                                        LR 0.000013    Time 0.016157    
2023-01-06 16:45:30,614 - Epoch: [138][  150/  246]    Overall Loss 0.253993    Objective Loss 0.253993                                        LR 0.000013    Time 0.016027    
2023-01-06 16:45:30,758 - Epoch: [138][  160/  246]    Overall Loss 0.254735    Objective Loss 0.254735                                        LR 0.000013    Time 0.015919    
2023-01-06 16:45:30,900 - Epoch: [138][  170/  246]    Overall Loss 0.254420    Objective Loss 0.254420                                        LR 0.000013    Time 0.015815    
2023-01-06 16:45:31,043 - Epoch: [138][  180/  246]    Overall Loss 0.253378    Objective Loss 0.253378                                        LR 0.000013    Time 0.015731    
2023-01-06 16:45:31,184 - Epoch: [138][  190/  246]    Overall Loss 0.253529    Objective Loss 0.253529                                        LR 0.000013    Time 0.015645    
2023-01-06 16:45:31,327 - Epoch: [138][  200/  246]    Overall Loss 0.252839    Objective Loss 0.252839                                        LR 0.000013    Time 0.015576    
2023-01-06 16:45:31,470 - Epoch: [138][  210/  246]    Overall Loss 0.252958    Objective Loss 0.252958                                        LR 0.000013    Time 0.015510    
2023-01-06 16:45:31,612 - Epoch: [138][  220/  246]    Overall Loss 0.252540    Objective Loss 0.252540                                        LR 0.000013    Time 0.015450    
2023-01-06 16:45:31,753 - Epoch: [138][  230/  246]    Overall Loss 0.252332    Objective Loss 0.252332                                        LR 0.000013    Time 0.015392    
2023-01-06 16:45:31,908 - Epoch: [138][  240/  246]    Overall Loss 0.251601    Objective Loss 0.251601                                        LR 0.000013    Time 0.015391    
2023-01-06 16:45:31,971 - Epoch: [138][  246/  246]    Overall Loss 0.251303    Objective Loss 0.251303    Top1 94.019139    LR 0.000013    Time 0.015274    
2023-01-06 16:45:32,097 - --- validate (epoch=138)-----------
2023-01-06 16:45:32,098 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:32,522 - Epoch: [138][   10/   28]    Loss 0.266775    Top1 90.273438    
2023-01-06 16:45:32,616 - Epoch: [138][   20/   28]    Loss 0.273035    Top1 90.371094    
2023-01-06 16:45:32,669 - Epoch: [138][   28/   28]    Loss 0.263053    Top1 90.667048    
2023-01-06 16:45:32,809 - ==> Top1: 90.667    Loss: 0.263

2023-01-06 16:45:32,809 - ==> Confusion:
[[ 249   17  173]
 [  17  270  315]
 [  63   67 5815]]

2023-01-06 16:45:32,810 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 138]
2023-01-06 16:45:32,810 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:32,817 - 

2023-01-06 16:45:32,817 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:33,485 - Epoch: [139][   10/  246]    Overall Loss 0.252028    Objective Loss 0.252028                                        LR 0.000013    Time 0.066705    
2023-01-06 16:45:33,622 - Epoch: [139][   20/  246]    Overall Loss 0.251861    Objective Loss 0.251861                                        LR 0.000013    Time 0.040207    
2023-01-06 16:45:33,768 - Epoch: [139][   30/  246]    Overall Loss 0.250038    Objective Loss 0.250038                                        LR 0.000013    Time 0.031654    
2023-01-06 16:45:33,911 - Epoch: [139][   40/  246]    Overall Loss 0.245529    Objective Loss 0.245529                                        LR 0.000013    Time 0.027307    
2023-01-06 16:45:34,053 - Epoch: [139][   50/  246]    Overall Loss 0.250275    Objective Loss 0.250275                                        LR 0.000013    Time 0.024673    
2023-01-06 16:45:34,195 - Epoch: [139][   60/  246]    Overall Loss 0.249557    Objective Loss 0.249557                                        LR 0.000013    Time 0.022913    
2023-01-06 16:45:34,331 - Epoch: [139][   70/  246]    Overall Loss 0.248956    Objective Loss 0.248956                                        LR 0.000013    Time 0.021584    
2023-01-06 16:45:34,469 - Epoch: [139][   80/  246]    Overall Loss 0.251375    Objective Loss 0.251375                                        LR 0.000013    Time 0.020598    
2023-01-06 16:45:34,626 - Epoch: [139][   90/  246]    Overall Loss 0.250199    Objective Loss 0.250199                                        LR 0.000013    Time 0.020052    
2023-01-06 16:45:34,778 - Epoch: [139][  100/  246]    Overall Loss 0.249733    Objective Loss 0.249733                                        LR 0.000013    Time 0.019562    
2023-01-06 16:45:34,938 - Epoch: [139][  110/  246]    Overall Loss 0.249870    Objective Loss 0.249870                                        LR 0.000013    Time 0.019237    
2023-01-06 16:45:35,084 - Epoch: [139][  120/  246]    Overall Loss 0.249169    Objective Loss 0.249169                                        LR 0.000013    Time 0.018843    
2023-01-06 16:45:35,227 - Epoch: [139][  130/  246]    Overall Loss 0.249378    Objective Loss 0.249378                                        LR 0.000013    Time 0.018488    
2023-01-06 16:45:35,367 - Epoch: [139][  140/  246]    Overall Loss 0.248281    Objective Loss 0.248281                                        LR 0.000013    Time 0.018165    
2023-01-06 16:45:35,505 - Epoch: [139][  150/  246]    Overall Loss 0.248313    Objective Loss 0.248313                                        LR 0.000013    Time 0.017874    
2023-01-06 16:45:35,636 - Epoch: [139][  160/  246]    Overall Loss 0.248792    Objective Loss 0.248792                                        LR 0.000013    Time 0.017571    
2023-01-06 16:45:35,770 - Epoch: [139][  170/  246]    Overall Loss 0.248514    Objective Loss 0.248514                                        LR 0.000013    Time 0.017321    
2023-01-06 16:45:35,908 - Epoch: [139][  180/  246]    Overall Loss 0.249746    Objective Loss 0.249746                                        LR 0.000013    Time 0.017123    
2023-01-06 16:45:36,046 - Epoch: [139][  190/  246]    Overall Loss 0.250076    Objective Loss 0.250076                                        LR 0.000013    Time 0.016947    
2023-01-06 16:45:36,185 - Epoch: [139][  200/  246]    Overall Loss 0.249717    Objective Loss 0.249717                                        LR 0.000013    Time 0.016792    
2023-01-06 16:45:36,328 - Epoch: [139][  210/  246]    Overall Loss 0.250094    Objective Loss 0.250094                                        LR 0.000013    Time 0.016668    
2023-01-06 16:45:36,466 - Epoch: [139][  220/  246]    Overall Loss 0.250181    Objective Loss 0.250181                                        LR 0.000013    Time 0.016535    
2023-01-06 16:45:36,606 - Epoch: [139][  230/  246]    Overall Loss 0.250386    Objective Loss 0.250386                                        LR 0.000013    Time 0.016425    
2023-01-06 16:45:36,748 - Epoch: [139][  240/  246]    Overall Loss 0.250553    Objective Loss 0.250553                                        LR 0.000013    Time 0.016331    
2023-01-06 16:45:36,808 - Epoch: [139][  246/  246]    Overall Loss 0.250540    Objective Loss 0.250540    Top1 90.430622    LR 0.000013    Time 0.016175    
2023-01-06 16:45:36,979 - --- validate (epoch=139)-----------
2023-01-06 16:45:36,979 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:37,403 - Epoch: [139][   10/   28]    Loss 0.265543    Top1 89.804688    
2023-01-06 16:45:37,503 - Epoch: [139][   20/   28]    Loss 0.277156    Top1 89.882812    
2023-01-06 16:45:37,551 - Epoch: [139][   28/   28]    Loss 0.262312    Top1 90.423705    
2023-01-06 16:45:37,716 - ==> Top1: 90.424    Loss: 0.262

2023-01-06 16:45:37,716 - ==> Confusion:
[[ 222   14  203]
 [  13  269  320]
 [  48   71 5826]]

2023-01-06 16:45:37,717 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 138]
2023-01-06 16:45:37,717 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:37,722 - 

2023-01-06 16:45:37,723 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:38,380 - Epoch: [140][   10/  246]    Overall Loss 0.241721    Objective Loss 0.241721                                        LR 0.000008    Time 0.065684    
2023-01-06 16:45:38,519 - Epoch: [140][   20/  246]    Overall Loss 0.245091    Objective Loss 0.245091                                        LR 0.000008    Time 0.039770    
2023-01-06 16:45:38,653 - Epoch: [140][   30/  246]    Overall Loss 0.240804    Objective Loss 0.240804                                        LR 0.000008    Time 0.030989    
2023-01-06 16:45:38,780 - Epoch: [140][   40/  246]    Overall Loss 0.245212    Objective Loss 0.245212                                        LR 0.000008    Time 0.026382    
2023-01-06 16:45:38,921 - Epoch: [140][   50/  246]    Overall Loss 0.243623    Objective Loss 0.243623                                        LR 0.000008    Time 0.023919    
2023-01-06 16:45:39,044 - Epoch: [140][   60/  246]    Overall Loss 0.245934    Objective Loss 0.245934                                        LR 0.000008    Time 0.021980    
2023-01-06 16:45:39,183 - Epoch: [140][   70/  246]    Overall Loss 0.246156    Objective Loss 0.246156                                        LR 0.000008    Time 0.020819    
2023-01-06 16:45:39,319 - Epoch: [140][   80/  246]    Overall Loss 0.245987    Objective Loss 0.245987                                        LR 0.000008    Time 0.019920    
2023-01-06 16:45:39,460 - Epoch: [140][   90/  246]    Overall Loss 0.245657    Objective Loss 0.245657                                        LR 0.000008    Time 0.019263    
2023-01-06 16:45:39,599 - Epoch: [140][  100/  246]    Overall Loss 0.248593    Objective Loss 0.248593                                        LR 0.000008    Time 0.018721    
2023-01-06 16:45:39,734 - Epoch: [140][  110/  246]    Overall Loss 0.249107    Objective Loss 0.249107                                        LR 0.000008    Time 0.018245    
2023-01-06 16:45:39,872 - Epoch: [140][  120/  246]    Overall Loss 0.249284    Objective Loss 0.249284                                        LR 0.000008    Time 0.017871    
2023-01-06 16:45:40,008 - Epoch: [140][  130/  246]    Overall Loss 0.249385    Objective Loss 0.249385                                        LR 0.000008    Time 0.017546    
2023-01-06 16:45:40,151 - Epoch: [140][  140/  246]    Overall Loss 0.250155    Objective Loss 0.250155                                        LR 0.000008    Time 0.017308    
2023-01-06 16:45:40,287 - Epoch: [140][  150/  246]    Overall Loss 0.250489    Objective Loss 0.250489                                        LR 0.000008    Time 0.017058    
2023-01-06 16:45:40,422 - Epoch: [140][  160/  246]    Overall Loss 0.250822    Objective Loss 0.250822                                        LR 0.000008    Time 0.016837    
2023-01-06 16:45:40,545 - Epoch: [140][  170/  246]    Overall Loss 0.250758    Objective Loss 0.250758                                        LR 0.000008    Time 0.016567    
2023-01-06 16:45:40,675 - Epoch: [140][  180/  246]    Overall Loss 0.251204    Objective Loss 0.251204                                        LR 0.000008    Time 0.016367    
2023-01-06 16:45:40,802 - Epoch: [140][  190/  246]    Overall Loss 0.249881    Objective Loss 0.249881                                        LR 0.000008    Time 0.016167    
2023-01-06 16:45:40,937 - Epoch: [140][  200/  246]    Overall Loss 0.249684    Objective Loss 0.249684                                        LR 0.000008    Time 0.016035    
2023-01-06 16:45:41,077 - Epoch: [140][  210/  246]    Overall Loss 0.249377    Objective Loss 0.249377                                        LR 0.000008    Time 0.015934    
2023-01-06 16:45:41,205 - Epoch: [140][  220/  246]    Overall Loss 0.249026    Objective Loss 0.249026                                        LR 0.000008    Time 0.015791    
2023-01-06 16:45:41,338 - Epoch: [140][  230/  246]    Overall Loss 0.248954    Objective Loss 0.248954                                        LR 0.000008    Time 0.015680    
2023-01-06 16:45:41,488 - Epoch: [140][  240/  246]    Overall Loss 0.249138    Objective Loss 0.249138                                        LR 0.000008    Time 0.015651    
2023-01-06 16:45:41,553 - Epoch: [140][  246/  246]    Overall Loss 0.248935    Objective Loss 0.248935    Top1 91.387560    LR 0.000008    Time 0.015532    
2023-01-06 16:45:41,682 - --- validate (epoch=140)-----------
2023-01-06 16:45:41,682 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:42,120 - Epoch: [140][   10/   28]    Loss 0.274440    Top1 90.117188    
2023-01-06 16:45:42,226 - Epoch: [140][   20/   28]    Loss 0.273694    Top1 90.234375    
2023-01-06 16:45:42,275 - Epoch: [140][   28/   28]    Loss 0.265111    Top1 90.509591    
2023-01-06 16:45:42,415 - ==> Top1: 90.510    Loss: 0.265

2023-01-06 16:45:42,415 - ==> Confusion:
[[ 238   15  186]
 [  17  261  324]
 [  54   67 5824]]

2023-01-06 16:45:42,416 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 138]
2023-01-06 16:45:42,416 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:42,421 - 

2023-01-06 16:45:42,421 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:42,945 - Epoch: [141][   10/  246]    Overall Loss 0.248454    Objective Loss 0.248454                                        LR 0.000008    Time 0.052301    
2023-01-06 16:45:43,069 - Epoch: [141][   20/  246]    Overall Loss 0.251058    Objective Loss 0.251058                                        LR 0.000008    Time 0.032333    
2023-01-06 16:45:43,188 - Epoch: [141][   30/  246]    Overall Loss 0.260694    Objective Loss 0.260694                                        LR 0.000008    Time 0.025518    
2023-01-06 16:45:43,318 - Epoch: [141][   40/  246]    Overall Loss 0.258073    Objective Loss 0.258073                                        LR 0.000008    Time 0.022362    
2023-01-06 16:45:43,444 - Epoch: [141][   50/  246]    Overall Loss 0.254100    Objective Loss 0.254100                                        LR 0.000008    Time 0.020417    
2023-01-06 16:45:43,581 - Epoch: [141][   60/  246]    Overall Loss 0.254399    Objective Loss 0.254399                                        LR 0.000008    Time 0.019283    
2023-01-06 16:45:43,710 - Epoch: [141][   70/  246]    Overall Loss 0.255986    Objective Loss 0.255986                                        LR 0.000008    Time 0.018368    
2023-01-06 16:45:43,838 - Epoch: [141][   80/  246]    Overall Loss 0.256091    Objective Loss 0.256091                                        LR 0.000008    Time 0.017662    
2023-01-06 16:45:43,960 - Epoch: [141][   90/  246]    Overall Loss 0.256036    Objective Loss 0.256036                                        LR 0.000008    Time 0.017035    
2023-01-06 16:45:44,087 - Epoch: [141][  100/  246]    Overall Loss 0.255580    Objective Loss 0.255580                                        LR 0.000008    Time 0.016599    
2023-01-06 16:45:44,229 - Epoch: [141][  110/  246]    Overall Loss 0.255748    Objective Loss 0.255748                                        LR 0.000008    Time 0.016372    
2023-01-06 16:45:44,371 - Epoch: [141][  120/  246]    Overall Loss 0.254620    Objective Loss 0.254620                                        LR 0.000008    Time 0.016190    
2023-01-06 16:45:44,518 - Epoch: [141][  130/  246]    Overall Loss 0.251535    Objective Loss 0.251535                                        LR 0.000008    Time 0.016072    
2023-01-06 16:45:44,659 - Epoch: [141][  140/  246]    Overall Loss 0.250948    Objective Loss 0.250948                                        LR 0.000008    Time 0.015931    
2023-01-06 16:45:44,806 - Epoch: [141][  150/  246]    Overall Loss 0.251380    Objective Loss 0.251380                                        LR 0.000008    Time 0.015843    
2023-01-06 16:45:44,947 - Epoch: [141][  160/  246]    Overall Loss 0.250164    Objective Loss 0.250164                                        LR 0.000008    Time 0.015734    
2023-01-06 16:45:45,094 - Epoch: [141][  170/  246]    Overall Loss 0.250541    Objective Loss 0.250541                                        LR 0.000008    Time 0.015668    
2023-01-06 16:45:45,240 - Epoch: [141][  180/  246]    Overall Loss 0.250473    Objective Loss 0.250473                                        LR 0.000008    Time 0.015605    
2023-01-06 16:45:45,389 - Epoch: [141][  190/  246]    Overall Loss 0.249765    Objective Loss 0.249765                                        LR 0.000008    Time 0.015567    
2023-01-06 16:45:45,534 - Epoch: [141][  200/  246]    Overall Loss 0.250030    Objective Loss 0.250030                                        LR 0.000008    Time 0.015513    
2023-01-06 16:45:45,676 - Epoch: [141][  210/  246]    Overall Loss 0.248949    Objective Loss 0.248949                                        LR 0.000008    Time 0.015448    
2023-01-06 16:45:45,838 - Epoch: [141][  220/  246]    Overall Loss 0.248714    Objective Loss 0.248714                                        LR 0.000008    Time 0.015481    
2023-01-06 16:45:46,023 - Epoch: [141][  230/  246]    Overall Loss 0.249049    Objective Loss 0.249049                                        LR 0.000008    Time 0.015608    
2023-01-06 16:45:46,216 - Epoch: [141][  240/  246]    Overall Loss 0.248767    Objective Loss 0.248767                                        LR 0.000008    Time 0.015760    
2023-01-06 16:45:46,295 - Epoch: [141][  246/  246]    Overall Loss 0.248453    Objective Loss 0.248453    Top1 93.779904    LR 0.000008    Time 0.015695    
2023-01-06 16:45:46,435 - --- validate (epoch=141)-----------
2023-01-06 16:45:46,435 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:46,875 - Epoch: [141][   10/   28]    Loss 0.283788    Top1 90.351562    
2023-01-06 16:45:46,976 - Epoch: [141][   20/   28]    Loss 0.267496    Top1 90.625000    
2023-01-06 16:45:47,029 - Epoch: [141][   28/   28]    Loss 0.261453    Top1 90.667048    
2023-01-06 16:45:47,151 - ==> Top1: 90.667    Loss: 0.261

2023-01-06 16:45:47,152 - ==> Confusion:
[[ 230   14  195]
 [  12  258  332]
 [  47   52 5846]]

2023-01-06 16:45:47,153 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:45:47,153 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:47,159 - 

2023-01-06 16:45:47,159 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:47,811 - Epoch: [142][   10/  246]    Overall Loss 0.256244    Objective Loss 0.256244                                        LR 0.000008    Time 0.065123    
2023-01-06 16:45:47,946 - Epoch: [142][   20/  246]    Overall Loss 0.258124    Objective Loss 0.258124                                        LR 0.000008    Time 0.039302    
2023-01-06 16:45:48,072 - Epoch: [142][   30/  246]    Overall Loss 0.251251    Objective Loss 0.251251                                        LR 0.000008    Time 0.030380    
2023-01-06 16:45:48,197 - Epoch: [142][   40/  246]    Overall Loss 0.251300    Objective Loss 0.251300                                        LR 0.000008    Time 0.025912    
2023-01-06 16:45:48,324 - Epoch: [142][   50/  246]    Overall Loss 0.250524    Objective Loss 0.250524                                        LR 0.000008    Time 0.023250    
2023-01-06 16:45:48,452 - Epoch: [142][   60/  246]    Overall Loss 0.248308    Objective Loss 0.248308                                        LR 0.000008    Time 0.021504    
2023-01-06 16:45:48,579 - Epoch: [142][   70/  246]    Overall Loss 0.247526    Objective Loss 0.247526                                        LR 0.000008    Time 0.020236    
2023-01-06 16:45:48,706 - Epoch: [142][   80/  246]    Overall Loss 0.245549    Objective Loss 0.245549                                        LR 0.000008    Time 0.019297    
2023-01-06 16:45:48,832 - Epoch: [142][   90/  246]    Overall Loss 0.246721    Objective Loss 0.246721                                        LR 0.000008    Time 0.018545    
2023-01-06 16:45:48,959 - Epoch: [142][  100/  246]    Overall Loss 0.246399    Objective Loss 0.246399                                        LR 0.000008    Time 0.017957    
2023-01-06 16:45:49,089 - Epoch: [142][  110/  246]    Overall Loss 0.246432    Objective Loss 0.246432                                        LR 0.000008    Time 0.017503    
2023-01-06 16:45:49,213 - Epoch: [142][  120/  246]    Overall Loss 0.247994    Objective Loss 0.247994                                        LR 0.000008    Time 0.017073    
2023-01-06 16:45:49,339 - Epoch: [142][  130/  246]    Overall Loss 0.247929    Objective Loss 0.247929                                        LR 0.000008    Time 0.016730    
2023-01-06 16:45:49,464 - Epoch: [142][  140/  246]    Overall Loss 0.246709    Objective Loss 0.246709                                        LR 0.000008    Time 0.016423    
2023-01-06 16:45:49,588 - Epoch: [142][  150/  246]    Overall Loss 0.247391    Objective Loss 0.247391                                        LR 0.000008    Time 0.016152    
2023-01-06 16:45:49,711 - Epoch: [142][  160/  246]    Overall Loss 0.249061    Objective Loss 0.249061                                        LR 0.000008    Time 0.015909    
2023-01-06 16:45:49,835 - Epoch: [142][  170/  246]    Overall Loss 0.249148    Objective Loss 0.249148                                        LR 0.000008    Time 0.015699    
2023-01-06 16:45:49,956 - Epoch: [142][  180/  246]    Overall Loss 0.249225    Objective Loss 0.249225                                        LR 0.000008    Time 0.015499    
2023-01-06 16:45:50,081 - Epoch: [142][  190/  246]    Overall Loss 0.249025    Objective Loss 0.249025                                        LR 0.000008    Time 0.015340    
2023-01-06 16:45:50,201 - Epoch: [142][  200/  246]    Overall Loss 0.249339    Objective Loss 0.249339                                        LR 0.000008    Time 0.015169    
2023-01-06 16:45:50,326 - Epoch: [142][  210/  246]    Overall Loss 0.249707    Objective Loss 0.249707                                        LR 0.000008    Time 0.015039    
2023-01-06 16:45:50,448 - Epoch: [142][  220/  246]    Overall Loss 0.249229    Objective Loss 0.249229                                        LR 0.000008    Time 0.014911    
2023-01-06 16:45:50,574 - Epoch: [142][  230/  246]    Overall Loss 0.248753    Objective Loss 0.248753                                        LR 0.000008    Time 0.014807    
2023-01-06 16:45:50,710 - Epoch: [142][  240/  246]    Overall Loss 0.249046    Objective Loss 0.249046                                        LR 0.000008    Time 0.014755    
2023-01-06 16:45:50,772 - Epoch: [142][  246/  246]    Overall Loss 0.249147    Objective Loss 0.249147    Top1 89.952153    LR 0.000008    Time 0.014648    
2023-01-06 16:45:50,914 - --- validate (epoch=142)-----------
2023-01-06 16:45:50,914 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:51,333 - Epoch: [142][   10/   28]    Loss 0.266692    Top1 90.507812    
2023-01-06 16:45:51,429 - Epoch: [142][   20/   28]    Loss 0.266948    Top1 90.136719    
2023-01-06 16:45:51,480 - Epoch: [142][   28/   28]    Loss 0.261379    Top1 90.466648    
2023-01-06 16:45:51,622 - ==> Top1: 90.467    Loss: 0.261

2023-01-06 16:45:51,623 - ==> Confusion:
[[ 244   15  180]
 [  16  271  315]
 [  66   74 5805]]

2023-01-06 16:45:51,624 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:45:51,624 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:51,629 - 

2023-01-06 16:45:51,629 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:52,143 - Epoch: [143][   10/  246]    Overall Loss 0.254021    Objective Loss 0.254021                                        LR 0.000008    Time 0.051341    
2023-01-06 16:45:52,274 - Epoch: [143][   20/  246]    Overall Loss 0.253972    Objective Loss 0.253972                                        LR 0.000008    Time 0.032185    
2023-01-06 16:45:52,409 - Epoch: [143][   30/  246]    Overall Loss 0.247442    Objective Loss 0.247442                                        LR 0.000008    Time 0.025961    
2023-01-06 16:45:52,536 - Epoch: [143][   40/  246]    Overall Loss 0.243644    Objective Loss 0.243644                                        LR 0.000008    Time 0.022645    
2023-01-06 16:45:52,663 - Epoch: [143][   50/  246]    Overall Loss 0.243346    Objective Loss 0.243346                                        LR 0.000008    Time 0.020650    
2023-01-06 16:45:52,798 - Epoch: [143][   60/  246]    Overall Loss 0.243864    Objective Loss 0.243864                                        LR 0.000008    Time 0.019441    
2023-01-06 16:45:52,931 - Epoch: [143][   70/  246]    Overall Loss 0.247236    Objective Loss 0.247236                                        LR 0.000008    Time 0.018556    
2023-01-06 16:45:53,070 - Epoch: [143][   80/  246]    Overall Loss 0.247971    Objective Loss 0.247971                                        LR 0.000008    Time 0.017969    
2023-01-06 16:45:53,211 - Epoch: [143][   90/  246]    Overall Loss 0.246307    Objective Loss 0.246307                                        LR 0.000008    Time 0.017535    
2023-01-06 16:45:53,350 - Epoch: [143][  100/  246]    Overall Loss 0.244418    Objective Loss 0.244418                                        LR 0.000008    Time 0.017175    
2023-01-06 16:45:53,503 - Epoch: [143][  110/  246]    Overall Loss 0.246388    Objective Loss 0.246388                                        LR 0.000008    Time 0.017001    
2023-01-06 16:45:53,659 - Epoch: [143][  120/  246]    Overall Loss 0.247776    Objective Loss 0.247776                                        LR 0.000008    Time 0.016875    
2023-01-06 16:45:53,809 - Epoch: [143][  130/  246]    Overall Loss 0.248123    Objective Loss 0.248123                                        LR 0.000008    Time 0.016732    
2023-01-06 16:45:53,960 - Epoch: [143][  140/  246]    Overall Loss 0.248179    Objective Loss 0.248179                                        LR 0.000008    Time 0.016610    
2023-01-06 16:45:54,098 - Epoch: [143][  150/  246]    Overall Loss 0.247342    Objective Loss 0.247342                                        LR 0.000008    Time 0.016422    
2023-01-06 16:45:54,236 - Epoch: [143][  160/  246]    Overall Loss 0.248280    Objective Loss 0.248280                                        LR 0.000008    Time 0.016257    
2023-01-06 16:45:54,374 - Epoch: [143][  170/  246]    Overall Loss 0.248115    Objective Loss 0.248115                                        LR 0.000008    Time 0.016108    
2023-01-06 16:45:54,514 - Epoch: [143][  180/  246]    Overall Loss 0.248178    Objective Loss 0.248178                                        LR 0.000008    Time 0.015988    
2023-01-06 16:45:54,650 - Epoch: [143][  190/  246]    Overall Loss 0.247470    Objective Loss 0.247470                                        LR 0.000008    Time 0.015862    
2023-01-06 16:45:54,793 - Epoch: [143][  200/  246]    Overall Loss 0.248385    Objective Loss 0.248385                                        LR 0.000008    Time 0.015780    
2023-01-06 16:45:54,930 - Epoch: [143][  210/  246]    Overall Loss 0.248853    Objective Loss 0.248853                                        LR 0.000008    Time 0.015682    
2023-01-06 16:45:55,067 - Epoch: [143][  220/  246]    Overall Loss 0.248766    Objective Loss 0.248766                                        LR 0.000008    Time 0.015588    
2023-01-06 16:45:55,202 - Epoch: [143][  230/  246]    Overall Loss 0.249473    Objective Loss 0.249473                                        LR 0.000008    Time 0.015499    
2023-01-06 16:45:55,353 - Epoch: [143][  240/  246]    Overall Loss 0.249322    Objective Loss 0.249322                                        LR 0.000008    Time 0.015478    
2023-01-06 16:45:55,424 - Epoch: [143][  246/  246]    Overall Loss 0.249354    Objective Loss 0.249354    Top1 89.952153    LR 0.000008    Time 0.015388    
2023-01-06 16:45:55,561 - --- validate (epoch=143)-----------
2023-01-06 16:45:55,561 - 6986 samples (256 per mini-batch)
2023-01-06 16:45:55,984 - Epoch: [143][   10/   28]    Loss 0.265089    Top1 91.210938    
2023-01-06 16:45:56,081 - Epoch: [143][   20/   28]    Loss 0.270142    Top1 90.546875    
2023-01-06 16:45:56,132 - Epoch: [143][   28/   28]    Loss 0.265605    Top1 90.609791    
2023-01-06 16:45:56,268 - ==> Top1: 90.610    Loss: 0.266

2023-01-06 16:45:56,269 - ==> Confusion:
[[ 236   14  189]
 [  12  258  332]
 [  48   61 5836]]

2023-01-06 16:45:56,270 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:45:56,270 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:45:56,275 - 

2023-01-06 16:45:56,275 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:45:56,954 - Epoch: [144][   10/  246]    Overall Loss 0.252344    Objective Loss 0.252344                                        LR 0.000008    Time 0.067865    
2023-01-06 16:45:57,104 - Epoch: [144][   20/  246]    Overall Loss 0.254294    Objective Loss 0.254294                                        LR 0.000008    Time 0.041402    
2023-01-06 16:45:57,261 - Epoch: [144][   30/  246]    Overall Loss 0.252699    Objective Loss 0.252699                                        LR 0.000008    Time 0.032817    
2023-01-06 16:45:57,422 - Epoch: [144][   40/  246]    Overall Loss 0.249583    Objective Loss 0.249583                                        LR 0.000008    Time 0.028639    
2023-01-06 16:45:57,578 - Epoch: [144][   50/  246]    Overall Loss 0.249356    Objective Loss 0.249356                                        LR 0.000008    Time 0.026014    
2023-01-06 16:45:57,736 - Epoch: [144][   60/  246]    Overall Loss 0.252399    Objective Loss 0.252399                                        LR 0.000008    Time 0.024313    
2023-01-06 16:45:57,895 - Epoch: [144][   70/  246]    Overall Loss 0.250868    Objective Loss 0.250868                                        LR 0.000008    Time 0.023100    
2023-01-06 16:45:58,062 - Epoch: [144][   80/  246]    Overall Loss 0.249844    Objective Loss 0.249844                                        LR 0.000008    Time 0.022289    
2023-01-06 16:45:58,231 - Epoch: [144][   90/  246]    Overall Loss 0.249105    Objective Loss 0.249105                                        LR 0.000008    Time 0.021694    
2023-01-06 16:45:58,391 - Epoch: [144][  100/  246]    Overall Loss 0.248141    Objective Loss 0.248141                                        LR 0.000008    Time 0.021114    
2023-01-06 16:45:58,558 - Epoch: [144][  110/  246]    Overall Loss 0.250484    Objective Loss 0.250484                                        LR 0.000008    Time 0.020709    
2023-01-06 16:45:58,729 - Epoch: [144][  120/  246]    Overall Loss 0.249811    Objective Loss 0.249811                                        LR 0.000008    Time 0.020410    
2023-01-06 16:45:58,901 - Epoch: [144][  130/  246]    Overall Loss 0.249729    Objective Loss 0.249729                                        LR 0.000008    Time 0.020159    
2023-01-06 16:45:59,068 - Epoch: [144][  140/  246]    Overall Loss 0.249229    Objective Loss 0.249229                                        LR 0.000008    Time 0.019905    
2023-01-06 16:45:59,237 - Epoch: [144][  150/  246]    Overall Loss 0.249946    Objective Loss 0.249946                                        LR 0.000008    Time 0.019702    
2023-01-06 16:45:59,402 - Epoch: [144][  160/  246]    Overall Loss 0.249711    Objective Loss 0.249711                                        LR 0.000008    Time 0.019499    
2023-01-06 16:45:59,567 - Epoch: [144][  170/  246]    Overall Loss 0.249527    Objective Loss 0.249527                                        LR 0.000008    Time 0.019324    
2023-01-06 16:45:59,733 - Epoch: [144][  180/  246]    Overall Loss 0.249846    Objective Loss 0.249846                                        LR 0.000008    Time 0.019168    
2023-01-06 16:45:59,896 - Epoch: [144][  190/  246]    Overall Loss 0.250100    Objective Loss 0.250100                                        LR 0.000008    Time 0.019016    
2023-01-06 16:46:00,065 - Epoch: [144][  200/  246]    Overall Loss 0.249561    Objective Loss 0.249561                                        LR 0.000008    Time 0.018905    
2023-01-06 16:46:00,236 - Epoch: [144][  210/  246]    Overall Loss 0.250120    Objective Loss 0.250120                                        LR 0.000008    Time 0.018819    
2023-01-06 16:46:00,418 - Epoch: [144][  220/  246]    Overall Loss 0.250105    Objective Loss 0.250105                                        LR 0.000008    Time 0.018786    
2023-01-06 16:46:00,605 - Epoch: [144][  230/  246]    Overall Loss 0.249694    Objective Loss 0.249694                                        LR 0.000008    Time 0.018782    
2023-01-06 16:46:00,807 - Epoch: [144][  240/  246]    Overall Loss 0.249200    Objective Loss 0.249200                                        LR 0.000008    Time 0.018840    
2023-01-06 16:46:00,888 - Epoch: [144][  246/  246]    Overall Loss 0.248847    Objective Loss 0.248847    Top1 92.344498    LR 0.000008    Time 0.018710    
2023-01-06 16:46:01,031 - --- validate (epoch=144)-----------
2023-01-06 16:46:01,032 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:01,458 - Epoch: [144][   10/   28]    Loss 0.251091    Top1 90.585938    
2023-01-06 16:46:01,554 - Epoch: [144][   20/   28]    Loss 0.258947    Top1 90.507812    
2023-01-06 16:46:01,606 - Epoch: [144][   28/   28]    Loss 0.263462    Top1 90.523905    
2023-01-06 16:46:01,763 - ==> Top1: 90.524    Loss: 0.263

2023-01-06 16:46:01,763 - ==> Confusion:
[[ 243   15  181]
 [  14  256  332]
 [  61   59 5825]]

2023-01-06 16:46:01,764 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:01,764 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:01,769 - 

2023-01-06 16:46:01,769 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:02,288 - Epoch: [145][   10/  246]    Overall Loss 0.264812    Objective Loss 0.264812                                        LR 0.000008    Time 0.051842    
2023-01-06 16:46:02,428 - Epoch: [145][   20/  246]    Overall Loss 0.251857    Objective Loss 0.251857                                        LR 0.000008    Time 0.032890    
2023-01-06 16:46:02,568 - Epoch: [145][   30/  246]    Overall Loss 0.249609    Objective Loss 0.249609                                        LR 0.000008    Time 0.026574    
2023-01-06 16:46:02,714 - Epoch: [145][   40/  246]    Overall Loss 0.250942    Objective Loss 0.250942                                        LR 0.000008    Time 0.023579    
2023-01-06 16:46:02,871 - Epoch: [145][   50/  246]    Overall Loss 0.251279    Objective Loss 0.251279                                        LR 0.000008    Time 0.021980    
2023-01-06 16:46:03,028 - Epoch: [145][   60/  246]    Overall Loss 0.250082    Objective Loss 0.250082                                        LR 0.000008    Time 0.020925    
2023-01-06 16:46:03,175 - Epoch: [145][   70/  246]    Overall Loss 0.248794    Objective Loss 0.248794                                        LR 0.000008    Time 0.020027    
2023-01-06 16:46:03,321 - Epoch: [145][   80/  246]    Overall Loss 0.246105    Objective Loss 0.246105                                        LR 0.000008    Time 0.019353    
2023-01-06 16:46:03,476 - Epoch: [145][   90/  246]    Overall Loss 0.248356    Objective Loss 0.248356                                        LR 0.000008    Time 0.018923    
2023-01-06 16:46:03,636 - Epoch: [145][  100/  246]    Overall Loss 0.250717    Objective Loss 0.250717                                        LR 0.000008    Time 0.018620    
2023-01-06 16:46:03,776 - Epoch: [145][  110/  246]    Overall Loss 0.249954    Objective Loss 0.249954                                        LR 0.000008    Time 0.018197    
2023-01-06 16:46:03,914 - Epoch: [145][  120/  246]    Overall Loss 0.249448    Objective Loss 0.249448                                        LR 0.000008    Time 0.017826    
2023-01-06 16:46:04,052 - Epoch: [145][  130/  246]    Overall Loss 0.248665    Objective Loss 0.248665                                        LR 0.000008    Time 0.017512    
2023-01-06 16:46:04,178 - Epoch: [145][  140/  246]    Overall Loss 0.250016    Objective Loss 0.250016                                        LR 0.000008    Time 0.017159    
2023-01-06 16:46:04,318 - Epoch: [145][  150/  246]    Overall Loss 0.251168    Objective Loss 0.251168                                        LR 0.000008    Time 0.016946    
2023-01-06 16:46:04,463 - Epoch: [145][  160/  246]    Overall Loss 0.250824    Objective Loss 0.250824                                        LR 0.000008    Time 0.016794    
2023-01-06 16:46:04,619 - Epoch: [145][  170/  246]    Overall Loss 0.249445    Objective Loss 0.249445                                        LR 0.000008    Time 0.016722    
2023-01-06 16:46:04,786 - Epoch: [145][  180/  246]    Overall Loss 0.249029    Objective Loss 0.249029                                        LR 0.000008    Time 0.016721    
2023-01-06 16:46:04,969 - Epoch: [145][  190/  246]    Overall Loss 0.250135    Objective Loss 0.250135                                        LR 0.000008    Time 0.016798    
2023-01-06 16:46:05,148 - Epoch: [145][  200/  246]    Overall Loss 0.249545    Objective Loss 0.249545                                        LR 0.000008    Time 0.016851    
2023-01-06 16:46:05,327 - Epoch: [145][  210/  246]    Overall Loss 0.249460    Objective Loss 0.249460                                        LR 0.000008    Time 0.016898    
2023-01-06 16:46:05,504 - Epoch: [145][  220/  246]    Overall Loss 0.248766    Objective Loss 0.248766                                        LR 0.000008    Time 0.016933    
2023-01-06 16:46:05,685 - Epoch: [145][  230/  246]    Overall Loss 0.248839    Objective Loss 0.248839                                        LR 0.000008    Time 0.016981    
2023-01-06 16:46:05,878 - Epoch: [145][  240/  246]    Overall Loss 0.248554    Objective Loss 0.248554                                        LR 0.000008    Time 0.017077    
2023-01-06 16:46:05,955 - Epoch: [145][  246/  246]    Overall Loss 0.249057    Objective Loss 0.249057    Top1 90.669856    LR 0.000008    Time 0.016974    
2023-01-06 16:46:06,096 - --- validate (epoch=145)-----------
2023-01-06 16:46:06,096 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:06,642 - Epoch: [145][   10/   28]    Loss 0.286495    Top1 89.453125    
2023-01-06 16:46:06,737 - Epoch: [145][   20/   28]    Loss 0.264566    Top1 90.312500    
2023-01-06 16:46:06,790 - Epoch: [145][   28/   28]    Loss 0.272975    Top1 90.208989    
2023-01-06 16:46:06,933 - ==> Top1: 90.209    Loss: 0.273

2023-01-06 16:46:06,933 - ==> Confusion:
[[ 237   15  187]
 [  16  264  322]
 [  70   74 5801]]

2023-01-06 16:46:06,935 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:06,935 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:06,940 - 

2023-01-06 16:46:06,940 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:07,467 - Epoch: [146][   10/  246]    Overall Loss 0.248260    Objective Loss 0.248260                                        LR 0.000008    Time 0.052644    
2023-01-06 16:46:07,611 - Epoch: [146][   20/  246]    Overall Loss 0.263539    Objective Loss 0.263539                                        LR 0.000008    Time 0.033483    
2023-01-06 16:46:07,757 - Epoch: [146][   30/  246]    Overall Loss 0.256215    Objective Loss 0.256215                                        LR 0.000008    Time 0.027183    
2023-01-06 16:46:07,908 - Epoch: [146][   40/  246]    Overall Loss 0.251380    Objective Loss 0.251380                                        LR 0.000008    Time 0.024159    
2023-01-06 16:46:08,064 - Epoch: [146][   50/  246]    Overall Loss 0.251325    Objective Loss 0.251325                                        LR 0.000008    Time 0.022430    
2023-01-06 16:46:08,215 - Epoch: [146][   60/  246]    Overall Loss 0.248269    Objective Loss 0.248269                                        LR 0.000008    Time 0.021213    
2023-01-06 16:46:08,371 - Epoch: [146][   70/  246]    Overall Loss 0.248523    Objective Loss 0.248523                                        LR 0.000008    Time 0.020401    
2023-01-06 16:46:08,530 - Epoch: [146][   80/  246]    Overall Loss 0.246956    Objective Loss 0.246956                                        LR 0.000008    Time 0.019836    
2023-01-06 16:46:08,689 - Epoch: [146][   90/  246]    Overall Loss 0.248497    Objective Loss 0.248497                                        LR 0.000008    Time 0.019386    
2023-01-06 16:46:08,851 - Epoch: [146][  100/  246]    Overall Loss 0.248981    Objective Loss 0.248981                                        LR 0.000008    Time 0.019065    
2023-01-06 16:46:09,017 - Epoch: [146][  110/  246]    Overall Loss 0.249373    Objective Loss 0.249373                                        LR 0.000008    Time 0.018838    
2023-01-06 16:46:09,183 - Epoch: [146][  120/  246]    Overall Loss 0.251747    Objective Loss 0.251747                                        LR 0.000008    Time 0.018654    
2023-01-06 16:46:09,351 - Epoch: [146][  130/  246]    Overall Loss 0.252876    Objective Loss 0.252876                                        LR 0.000008    Time 0.018501    
2023-01-06 16:46:09,513 - Epoch: [146][  140/  246]    Overall Loss 0.251480    Objective Loss 0.251480                                        LR 0.000008    Time 0.018335    
2023-01-06 16:46:09,677 - Epoch: [146][  150/  246]    Overall Loss 0.251723    Objective Loss 0.251723                                        LR 0.000008    Time 0.018208    
2023-01-06 16:46:09,843 - Epoch: [146][  160/  246]    Overall Loss 0.251964    Objective Loss 0.251964                                        LR 0.000008    Time 0.018102    
2023-01-06 16:46:10,008 - Epoch: [146][  170/  246]    Overall Loss 0.251419    Objective Loss 0.251419                                        LR 0.000008    Time 0.018007    
2023-01-06 16:46:10,175 - Epoch: [146][  180/  246]    Overall Loss 0.249852    Objective Loss 0.249852                                        LR 0.000008    Time 0.017928    
2023-01-06 16:46:10,341 - Epoch: [146][  190/  246]    Overall Loss 0.249939    Objective Loss 0.249939                                        LR 0.000008    Time 0.017857    
2023-01-06 16:46:10,505 - Epoch: [146][  200/  246]    Overall Loss 0.250888    Objective Loss 0.250888                                        LR 0.000008    Time 0.017783    
2023-01-06 16:46:10,674 - Epoch: [146][  210/  246]    Overall Loss 0.250925    Objective Loss 0.250925                                        LR 0.000008    Time 0.017737    
2023-01-06 16:46:10,838 - Epoch: [146][  220/  246]    Overall Loss 0.250057    Objective Loss 0.250057                                        LR 0.000008    Time 0.017677    
2023-01-06 16:46:11,005 - Epoch: [146][  230/  246]    Overall Loss 0.250040    Objective Loss 0.250040                                        LR 0.000008    Time 0.017632    
2023-01-06 16:46:11,179 - Epoch: [146][  240/  246]    Overall Loss 0.248883    Objective Loss 0.248883                                        LR 0.000008    Time 0.017619    
2023-01-06 16:46:11,248 - Epoch: [146][  246/  246]    Overall Loss 0.248536    Objective Loss 0.248536    Top1 90.909091    LR 0.000008    Time 0.017472    
2023-01-06 16:46:11,387 - --- validate (epoch=146)-----------
2023-01-06 16:46:11,388 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:11,807 - Epoch: [146][   10/   28]    Loss 0.278703    Top1 90.000000    
2023-01-06 16:46:11,917 - Epoch: [146][   20/   28]    Loss 0.266731    Top1 90.410156    
2023-01-06 16:46:11,968 - Epoch: [146][   28/   28]    Loss 0.262681    Top1 90.423705    
2023-01-06 16:46:12,126 - ==> Top1: 90.424    Loss: 0.263

2023-01-06 16:46:12,127 - ==> Confusion:
[[ 220   15  204]
 [  13  266  323]
 [  45   69 5831]]

2023-01-06 16:46:12,128 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:12,128 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:12,133 - 

2023-01-06 16:46:12,133 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:12,793 - Epoch: [147][   10/  246]    Overall Loss 0.240648    Objective Loss 0.240648                                        LR 0.000008    Time 0.065959    
2023-01-06 16:46:12,924 - Epoch: [147][   20/  246]    Overall Loss 0.244319    Objective Loss 0.244319                                        LR 0.000008    Time 0.039510    
2023-01-06 16:46:13,052 - Epoch: [147][   30/  246]    Overall Loss 0.248764    Objective Loss 0.248764                                        LR 0.000008    Time 0.030585    
2023-01-06 16:46:13,181 - Epoch: [147][   40/  246]    Overall Loss 0.248204    Objective Loss 0.248204                                        LR 0.000008    Time 0.026150    
2023-01-06 16:46:13,297 - Epoch: [147][   50/  246]    Overall Loss 0.243098    Objective Loss 0.243098                                        LR 0.000008    Time 0.023244    
2023-01-06 16:46:13,418 - Epoch: [147][   60/  246]    Overall Loss 0.244095    Objective Loss 0.244095                                        LR 0.000008    Time 0.021380    
2023-01-06 16:46:13,538 - Epoch: [147][   70/  246]    Overall Loss 0.244616    Objective Loss 0.244616                                        LR 0.000008    Time 0.020024    
2023-01-06 16:46:13,657 - Epoch: [147][   80/  246]    Overall Loss 0.244706    Objective Loss 0.244706                                        LR 0.000008    Time 0.019004    
2023-01-06 16:46:13,776 - Epoch: [147][   90/  246]    Overall Loss 0.247127    Objective Loss 0.247127                                        LR 0.000008    Time 0.018215    
2023-01-06 16:46:13,897 - Epoch: [147][  100/  246]    Overall Loss 0.248357    Objective Loss 0.248357                                        LR 0.000008    Time 0.017599    
2023-01-06 16:46:14,015 - Epoch: [147][  110/  246]    Overall Loss 0.247527    Objective Loss 0.247527                                        LR 0.000008    Time 0.017072    
2023-01-06 16:46:14,139 - Epoch: [147][  120/  246]    Overall Loss 0.247377    Objective Loss 0.247377                                        LR 0.000008    Time 0.016675    
2023-01-06 16:46:14,262 - Epoch: [147][  130/  246]    Overall Loss 0.247202    Objective Loss 0.247202                                        LR 0.000008    Time 0.016338    
2023-01-06 16:46:14,387 - Epoch: [147][  140/  246]    Overall Loss 0.245873    Objective Loss 0.245873                                        LR 0.000008    Time 0.016059    
2023-01-06 16:46:14,512 - Epoch: [147][  150/  246]    Overall Loss 0.246316    Objective Loss 0.246316                                        LR 0.000008    Time 0.015823    
2023-01-06 16:46:14,639 - Epoch: [147][  160/  246]    Overall Loss 0.246639    Objective Loss 0.246639                                        LR 0.000008    Time 0.015627    
2023-01-06 16:46:14,767 - Epoch: [147][  170/  246]    Overall Loss 0.246572    Objective Loss 0.246572                                        LR 0.000008    Time 0.015456    
2023-01-06 16:46:14,895 - Epoch: [147][  180/  246]    Overall Loss 0.246274    Objective Loss 0.246274                                        LR 0.000008    Time 0.015310    
2023-01-06 16:46:15,021 - Epoch: [147][  190/  246]    Overall Loss 0.247190    Objective Loss 0.247190                                        LR 0.000008    Time 0.015165    
2023-01-06 16:46:15,145 - Epoch: [147][  200/  246]    Overall Loss 0.247682    Objective Loss 0.247682                                        LR 0.000008    Time 0.015022    
2023-01-06 16:46:15,269 - Epoch: [147][  210/  246]    Overall Loss 0.247835    Objective Loss 0.247835                                        LR 0.000008    Time 0.014894    
2023-01-06 16:46:15,395 - Epoch: [147][  220/  246]    Overall Loss 0.248039    Objective Loss 0.248039                                        LR 0.000008    Time 0.014791    
2023-01-06 16:46:15,522 - Epoch: [147][  230/  246]    Overall Loss 0.248551    Objective Loss 0.248551                                        LR 0.000008    Time 0.014696    
2023-01-06 16:46:15,664 - Epoch: [147][  240/  246]    Overall Loss 0.248848    Objective Loss 0.248848                                        LR 0.000008    Time 0.014676    
2023-01-06 16:46:15,725 - Epoch: [147][  246/  246]    Overall Loss 0.248498    Objective Loss 0.248498    Top1 89.712919    LR 0.000008    Time 0.014564    
2023-01-06 16:46:15,877 - --- validate (epoch=147)-----------
2023-01-06 16:46:15,877 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:16,300 - Epoch: [147][   10/   28]    Loss 0.253355    Top1 90.742188    
2023-01-06 16:46:16,396 - Epoch: [147][   20/   28]    Loss 0.249310    Top1 90.996094    
2023-01-06 16:46:16,445 - Epoch: [147][   28/   28]    Loss 0.261846    Top1 90.595477    
2023-01-06 16:46:16,587 - ==> Top1: 90.595    Loss: 0.262

2023-01-06 16:46:16,587 - ==> Confusion:
[[ 235   14  190]
 [  16  247  339]
 [  47   51 5847]]

2023-01-06 16:46:16,588 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:16,588 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:16,593 - 

2023-01-06 16:46:16,593 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:17,119 - Epoch: [148][   10/  246]    Overall Loss 0.236646    Objective Loss 0.236646                                        LR 0.000008    Time 0.052460    
2023-01-06 16:46:17,265 - Epoch: [148][   20/  246]    Overall Loss 0.252424    Objective Loss 0.252424                                        LR 0.000008    Time 0.033511    
2023-01-06 16:46:17,406 - Epoch: [148][   30/  246]    Overall Loss 0.246538    Objective Loss 0.246538                                        LR 0.000008    Time 0.027044    
2023-01-06 16:46:17,535 - Epoch: [148][   40/  246]    Overall Loss 0.245548    Objective Loss 0.245548                                        LR 0.000008    Time 0.023493    
2023-01-06 16:46:17,661 - Epoch: [148][   50/  246]    Overall Loss 0.244894    Objective Loss 0.244894                                        LR 0.000008    Time 0.021300    
2023-01-06 16:46:17,790 - Epoch: [148][   60/  246]    Overall Loss 0.245411    Objective Loss 0.245411                                        LR 0.000008    Time 0.019897    
2023-01-06 16:46:17,916 - Epoch: [148][   70/  246]    Overall Loss 0.246451    Objective Loss 0.246451                                        LR 0.000008    Time 0.018840    
2023-01-06 16:46:18,045 - Epoch: [148][   80/  246]    Overall Loss 0.247438    Objective Loss 0.247438                                        LR 0.000008    Time 0.018095    
2023-01-06 16:46:18,173 - Epoch: [148][   90/  246]    Overall Loss 0.247082    Objective Loss 0.247082                                        LR 0.000008    Time 0.017500    
2023-01-06 16:46:18,302 - Epoch: [148][  100/  246]    Overall Loss 0.247558    Objective Loss 0.247558                                        LR 0.000008    Time 0.017040    
2023-01-06 16:46:18,431 - Epoch: [148][  110/  246]    Overall Loss 0.249779    Objective Loss 0.249779                                        LR 0.000008    Time 0.016658    
2023-01-06 16:46:18,558 - Epoch: [148][  120/  246]    Overall Loss 0.250141    Objective Loss 0.250141                                        LR 0.000008    Time 0.016325    
2023-01-06 16:46:18,684 - Epoch: [148][  130/  246]    Overall Loss 0.250035    Objective Loss 0.250035                                        LR 0.000008    Time 0.016028    
2023-01-06 16:46:18,808 - Epoch: [148][  140/  246]    Overall Loss 0.249645    Objective Loss 0.249645                                        LR 0.000008    Time 0.015769    
2023-01-06 16:46:18,934 - Epoch: [148][  150/  246]    Overall Loss 0.247909    Objective Loss 0.247909                                        LR 0.000008    Time 0.015552    
2023-01-06 16:46:19,060 - Epoch: [148][  160/  246]    Overall Loss 0.248737    Objective Loss 0.248737                                        LR 0.000008    Time 0.015365    
2023-01-06 16:46:19,191 - Epoch: [148][  170/  246]    Overall Loss 0.247728    Objective Loss 0.247728                                        LR 0.000008    Time 0.015223    
2023-01-06 16:46:19,319 - Epoch: [148][  180/  246]    Overall Loss 0.247220    Objective Loss 0.247220                                        LR 0.000008    Time 0.015084    
2023-01-06 16:46:19,450 - Epoch: [148][  190/  246]    Overall Loss 0.248284    Objective Loss 0.248284                                        LR 0.000008    Time 0.014974    
2023-01-06 16:46:19,581 - Epoch: [148][  200/  246]    Overall Loss 0.248707    Objective Loss 0.248707                                        LR 0.000008    Time 0.014876    
2023-01-06 16:46:19,714 - Epoch: [148][  210/  246]    Overall Loss 0.248896    Objective Loss 0.248896                                        LR 0.000008    Time 0.014793    
2023-01-06 16:46:19,846 - Epoch: [148][  220/  246]    Overall Loss 0.248771    Objective Loss 0.248771                                        LR 0.000008    Time 0.014717    
2023-01-06 16:46:19,978 - Epoch: [148][  230/  246]    Overall Loss 0.249480    Objective Loss 0.249480                                        LR 0.000008    Time 0.014649    
2023-01-06 16:46:20,121 - Epoch: [148][  240/  246]    Overall Loss 0.248032    Objective Loss 0.248032                                        LR 0.000008    Time 0.014633    
2023-01-06 16:46:20,179 - Epoch: [148][  246/  246]    Overall Loss 0.247885    Objective Loss 0.247885    Top1 92.344498    LR 0.000008    Time 0.014512    
2023-01-06 16:46:20,324 - --- validate (epoch=148)-----------
2023-01-06 16:46:20,325 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:20,761 - Epoch: [148][   10/   28]    Loss 0.252780    Top1 90.468750    
2023-01-06 16:46:20,862 - Epoch: [148][   20/   28]    Loss 0.268489    Top1 90.058594    
2023-01-06 16:46:20,911 - Epoch: [148][   28/   28]    Loss 0.265519    Top1 90.280561    
2023-01-06 16:46:21,050 - ==> Top1: 90.281    Loss: 0.266

2023-01-06 16:46:21,051 - ==> Confusion:
[[ 225   15  199]
 [  13  253  336]
 [  57   59 5829]]

2023-01-06 16:46:21,052 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:21,052 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:21,057 - 

2023-01-06 16:46:21,057 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:21,699 - Epoch: [149][   10/  246]    Overall Loss 0.235208    Objective Loss 0.235208                                        LR 0.000008    Time 0.064133    
2023-01-06 16:46:21,818 - Epoch: [149][   20/  246]    Overall Loss 0.243874    Objective Loss 0.243874                                        LR 0.000008    Time 0.038009    
2023-01-06 16:46:21,937 - Epoch: [149][   30/  246]    Overall Loss 0.244264    Objective Loss 0.244264                                        LR 0.000008    Time 0.029285    
2023-01-06 16:46:22,055 - Epoch: [149][   40/  246]    Overall Loss 0.246010    Objective Loss 0.246010                                        LR 0.000008    Time 0.024914    
2023-01-06 16:46:22,178 - Epoch: [149][   50/  246]    Overall Loss 0.246417    Objective Loss 0.246417                                        LR 0.000008    Time 0.022386    
2023-01-06 16:46:22,296 - Epoch: [149][   60/  246]    Overall Loss 0.243858    Objective Loss 0.243858                                        LR 0.000008    Time 0.020602    
2023-01-06 16:46:22,412 - Epoch: [149][   70/  246]    Overall Loss 0.243881    Objective Loss 0.243881                                        LR 0.000008    Time 0.019321    
2023-01-06 16:46:22,528 - Epoch: [149][   80/  246]    Overall Loss 0.243323    Objective Loss 0.243323                                        LR 0.000008    Time 0.018344    
2023-01-06 16:46:22,643 - Epoch: [149][   90/  246]    Overall Loss 0.244962    Objective Loss 0.244962                                        LR 0.000008    Time 0.017577    
2023-01-06 16:46:22,759 - Epoch: [149][  100/  246]    Overall Loss 0.245206    Objective Loss 0.245206                                        LR 0.000008    Time 0.016979    
2023-01-06 16:46:22,878 - Epoch: [149][  110/  246]    Overall Loss 0.245554    Objective Loss 0.245554                                        LR 0.000008    Time 0.016511    
2023-01-06 16:46:23,009 - Epoch: [149][  120/  246]    Overall Loss 0.246455    Objective Loss 0.246455                                        LR 0.000008    Time 0.016224    
2023-01-06 16:46:23,144 - Epoch: [149][  130/  246]    Overall Loss 0.248010    Objective Loss 0.248010                                        LR 0.000008    Time 0.016014    
2023-01-06 16:46:23,290 - Epoch: [149][  140/  246]    Overall Loss 0.247995    Objective Loss 0.247995                                        LR 0.000008    Time 0.015908    
2023-01-06 16:46:23,436 - Epoch: [149][  150/  246]    Overall Loss 0.248588    Objective Loss 0.248588                                        LR 0.000008    Time 0.015822    
2023-01-06 16:46:23,582 - Epoch: [149][  160/  246]    Overall Loss 0.247888    Objective Loss 0.247888                                        LR 0.000008    Time 0.015737    
2023-01-06 16:46:23,738 - Epoch: [149][  170/  246]    Overall Loss 0.248098    Objective Loss 0.248098                                        LR 0.000008    Time 0.015730    
2023-01-06 16:46:23,889 - Epoch: [149][  180/  246]    Overall Loss 0.249215    Objective Loss 0.249215                                        LR 0.000008    Time 0.015692    
2023-01-06 16:46:24,025 - Epoch: [149][  190/  246]    Overall Loss 0.249151    Objective Loss 0.249151                                        LR 0.000008    Time 0.015576    
2023-01-06 16:46:24,163 - Epoch: [149][  200/  246]    Overall Loss 0.248221    Objective Loss 0.248221                                        LR 0.000008    Time 0.015486    
2023-01-06 16:46:24,301 - Epoch: [149][  210/  246]    Overall Loss 0.248430    Objective Loss 0.248430                                        LR 0.000008    Time 0.015405    
2023-01-06 16:46:24,435 - Epoch: [149][  220/  246]    Overall Loss 0.248361    Objective Loss 0.248361                                        LR 0.000008    Time 0.015311    
2023-01-06 16:46:24,571 - Epoch: [149][  230/  246]    Overall Loss 0.248189    Objective Loss 0.248189                                        LR 0.000008    Time 0.015237    
2023-01-06 16:46:24,723 - Epoch: [149][  240/  246]    Overall Loss 0.248099    Objective Loss 0.248099                                        LR 0.000008    Time 0.015231    
2023-01-06 16:46:24,785 - Epoch: [149][  246/  246]    Overall Loss 0.248244    Objective Loss 0.248244    Top1 89.712919    LR 0.000008    Time 0.015112    
2023-01-06 16:46:24,926 - --- validate (epoch=149)-----------
2023-01-06 16:46:24,926 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:25,364 - Epoch: [149][   10/   28]    Loss 0.264689    Top1 90.351562    
2023-01-06 16:46:25,468 - Epoch: [149][   20/   28]    Loss 0.262487    Top1 90.468750    
2023-01-06 16:46:25,518 - Epoch: [149][   28/   28]    Loss 0.265167    Top1 90.380762    
2023-01-06 16:46:25,679 - ==> Top1: 90.381    Loss: 0.265

2023-01-06 16:46:25,679 - ==> Confusion:
[[ 225   15  199]
 [  14  251  337]
 [  50   57 5838]]

2023-01-06 16:46:25,681 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 141]
2023-01-06 16:46:25,681 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:25,686 - 

2023-01-06 16:46:25,686 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:26,354 - Epoch: [150][   10/  246]    Overall Loss 0.238941    Objective Loss 0.238941                                        LR 0.000008    Time 0.066784    
2023-01-06 16:46:26,480 - Epoch: [150][   20/  246]    Overall Loss 0.232857    Objective Loss 0.232857                                        LR 0.000008    Time 0.039680    
2023-01-06 16:46:26,607 - Epoch: [150][   30/  246]    Overall Loss 0.238122    Objective Loss 0.238122                                        LR 0.000008    Time 0.030644    
2023-01-06 16:46:26,729 - Epoch: [150][   40/  246]    Overall Loss 0.243791    Objective Loss 0.243791                                        LR 0.000008    Time 0.026020    
2023-01-06 16:46:26,860 - Epoch: [150][   50/  246]    Overall Loss 0.244110    Objective Loss 0.244110                                        LR 0.000008    Time 0.023431    
2023-01-06 16:46:26,984 - Epoch: [150][   60/  246]    Overall Loss 0.243267    Objective Loss 0.243267                                        LR 0.000008    Time 0.021596    
2023-01-06 16:46:27,109 - Epoch: [150][   70/  246]    Overall Loss 0.247841    Objective Loss 0.247841                                        LR 0.000008    Time 0.020284    
2023-01-06 16:46:27,231 - Epoch: [150][   80/  246]    Overall Loss 0.248650    Objective Loss 0.248650                                        LR 0.000008    Time 0.019246    
2023-01-06 16:46:27,358 - Epoch: [150][   90/  246]    Overall Loss 0.249371    Objective Loss 0.249371                                        LR 0.000008    Time 0.018517    
2023-01-06 16:46:27,481 - Epoch: [150][  100/  246]    Overall Loss 0.248775    Objective Loss 0.248775                                        LR 0.000008    Time 0.017898    
2023-01-06 16:46:27,603 - Epoch: [150][  110/  246]    Overall Loss 0.247173    Objective Loss 0.247173                                        LR 0.000008    Time 0.017376    
2023-01-06 16:46:27,721 - Epoch: [150][  120/  246]    Overall Loss 0.247129    Objective Loss 0.247129                                        LR 0.000008    Time 0.016909    
2023-01-06 16:46:27,851 - Epoch: [150][  130/  246]    Overall Loss 0.248090    Objective Loss 0.248090                                        LR 0.000008    Time 0.016602    
2023-01-06 16:46:27,983 - Epoch: [150][  140/  246]    Overall Loss 0.247238    Objective Loss 0.247238                                        LR 0.000008    Time 0.016358    
2023-01-06 16:46:28,117 - Epoch: [150][  150/  246]    Overall Loss 0.247320    Objective Loss 0.247320                                        LR 0.000008    Time 0.016154    
2023-01-06 16:46:28,251 - Epoch: [150][  160/  246]    Overall Loss 0.246721    Objective Loss 0.246721                                        LR 0.000008    Time 0.015981    
2023-01-06 16:46:28,388 - Epoch: [150][  170/  246]    Overall Loss 0.245994    Objective Loss 0.245994                                        LR 0.000008    Time 0.015846    
2023-01-06 16:46:28,522 - Epoch: [150][  180/  246]    Overall Loss 0.246443    Objective Loss 0.246443                                        LR 0.000008    Time 0.015709    
2023-01-06 16:46:28,650 - Epoch: [150][  190/  246]    Overall Loss 0.246753    Objective Loss 0.246753                                        LR 0.000008    Time 0.015553    
2023-01-06 16:46:28,779 - Epoch: [150][  200/  246]    Overall Loss 0.247035    Objective Loss 0.247035                                        LR 0.000008    Time 0.015418    
2023-01-06 16:46:28,911 - Epoch: [150][  210/  246]    Overall Loss 0.247738    Objective Loss 0.247738                                        LR 0.000008    Time 0.015312    
2023-01-06 16:46:29,037 - Epoch: [150][  220/  246]    Overall Loss 0.246911    Objective Loss 0.246911                                        LR 0.000008    Time 0.015184    
2023-01-06 16:46:29,170 - Epoch: [150][  230/  246]    Overall Loss 0.247213    Objective Loss 0.247213                                        LR 0.000008    Time 0.015102    
2023-01-06 16:46:29,315 - Epoch: [150][  240/  246]    Overall Loss 0.247919    Objective Loss 0.247919                                        LR 0.000008    Time 0.015075    
2023-01-06 16:46:29,376 - Epoch: [150][  246/  246]    Overall Loss 0.247744    Objective Loss 0.247744    Top1 92.105263    LR 0.000008    Time 0.014957    
2023-01-06 16:46:29,515 - --- validate (epoch=150)-----------
2023-01-06 16:46:29,515 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:29,934 - Epoch: [150][   10/   28]    Loss 0.265153    Top1 90.468750    
2023-01-06 16:46:30,028 - Epoch: [150][   20/   28]    Loss 0.268025    Top1 90.429688    
2023-01-06 16:46:30,079 - Epoch: [150][   28/   28]    Loss 0.259872    Top1 90.667048    
2023-01-06 16:46:30,244 - ==> Top1: 90.667    Loss: 0.260

2023-01-06 16:46:30,245 - ==> Confusion:
[[ 237   14  188]
 [  14  268  320]
 [  54   62 5829]]

2023-01-06 16:46:30,246 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 150]
2023-01-06 16:46:30,246 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:30,252 - 

2023-01-06 16:46:30,252 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:30,768 - Epoch: [151][   10/  246]    Overall Loss 0.233843    Objective Loss 0.233843                                        LR 0.000008    Time 0.051546    
2023-01-06 16:46:30,901 - Epoch: [151][   20/  246]    Overall Loss 0.240435    Objective Loss 0.240435                                        LR 0.000008    Time 0.032378    
2023-01-06 16:46:31,034 - Epoch: [151][   30/  246]    Overall Loss 0.246236    Objective Loss 0.246236                                        LR 0.000008    Time 0.026010    
2023-01-06 16:46:31,170 - Epoch: [151][   40/  246]    Overall Loss 0.248475    Objective Loss 0.248475                                        LR 0.000008    Time 0.022902    
2023-01-06 16:46:31,301 - Epoch: [151][   50/  246]    Overall Loss 0.251698    Objective Loss 0.251698                                        LR 0.000008    Time 0.020936    
2023-01-06 16:46:31,436 - Epoch: [151][   60/  246]    Overall Loss 0.251158    Objective Loss 0.251158                                        LR 0.000008    Time 0.019689    
2023-01-06 16:46:31,568 - Epoch: [151][   70/  246]    Overall Loss 0.248453    Objective Loss 0.248453                                        LR 0.000008    Time 0.018749    
2023-01-06 16:46:31,703 - Epoch: [151][   80/  246]    Overall Loss 0.250650    Objective Loss 0.250650                                        LR 0.000008    Time 0.018095    
2023-01-06 16:46:31,847 - Epoch: [151][   90/  246]    Overall Loss 0.249646    Objective Loss 0.249646                                        LR 0.000008    Time 0.017680    
2023-01-06 16:46:31,978 - Epoch: [151][  100/  246]    Overall Loss 0.248778    Objective Loss 0.248778                                        LR 0.000008    Time 0.017217    
2023-01-06 16:46:32,113 - Epoch: [151][  110/  246]    Overall Loss 0.247832    Objective Loss 0.247832                                        LR 0.000008    Time 0.016859    
2023-01-06 16:46:32,246 - Epoch: [151][  120/  246]    Overall Loss 0.245981    Objective Loss 0.245981                                        LR 0.000008    Time 0.016559    
2023-01-06 16:46:32,381 - Epoch: [151][  130/  246]    Overall Loss 0.245345    Objective Loss 0.245345                                        LR 0.000008    Time 0.016319    
2023-01-06 16:46:32,511 - Epoch: [151][  140/  246]    Overall Loss 0.245238    Objective Loss 0.245238                                        LR 0.000008    Time 0.016078    
2023-01-06 16:46:32,648 - Epoch: [151][  150/  246]    Overall Loss 0.244181    Objective Loss 0.244181                                        LR 0.000008    Time 0.015915    
2023-01-06 16:46:32,777 - Epoch: [151][  160/  246]    Overall Loss 0.245689    Objective Loss 0.245689                                        LR 0.000008    Time 0.015725    
2023-01-06 16:46:32,909 - Epoch: [151][  170/  246]    Overall Loss 0.246229    Objective Loss 0.246229                                        LR 0.000008    Time 0.015564    
2023-01-06 16:46:33,037 - Epoch: [151][  180/  246]    Overall Loss 0.247066    Objective Loss 0.247066                                        LR 0.000008    Time 0.015408    
2023-01-06 16:46:33,187 - Epoch: [151][  190/  246]    Overall Loss 0.246717    Objective Loss 0.246717                                        LR 0.000008    Time 0.015386    
2023-01-06 16:46:33,336 - Epoch: [151][  200/  246]    Overall Loss 0.246279    Objective Loss 0.246279                                        LR 0.000008    Time 0.015359    
2023-01-06 16:46:33,493 - Epoch: [151][  210/  246]    Overall Loss 0.245657    Objective Loss 0.245657                                        LR 0.000008    Time 0.015371    
2023-01-06 16:46:33,648 - Epoch: [151][  220/  246]    Overall Loss 0.246626    Objective Loss 0.246626                                        LR 0.000008    Time 0.015376    
2023-01-06 16:46:33,806 - Epoch: [151][  230/  246]    Overall Loss 0.247276    Objective Loss 0.247276                                        LR 0.000008    Time 0.015392    
2023-01-06 16:46:33,971 - Epoch: [151][  240/  246]    Overall Loss 0.247316    Objective Loss 0.247316                                        LR 0.000008    Time 0.015437    
2023-01-06 16:46:34,036 - Epoch: [151][  246/  246]    Overall Loss 0.247165    Objective Loss 0.247165    Top1 92.105263    LR 0.000008    Time 0.015324    
2023-01-06 16:46:34,195 - --- validate (epoch=151)-----------
2023-01-06 16:46:34,195 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:34,622 - Epoch: [151][   10/   28]    Loss 0.271547    Top1 90.039062    
2023-01-06 16:46:34,721 - Epoch: [151][   20/   28]    Loss 0.263036    Top1 90.449219    
2023-01-06 16:46:34,772 - Epoch: [151][   28/   28]    Loss 0.262165    Top1 90.509591    
2023-01-06 16:46:34,914 - ==> Top1: 90.510    Loss: 0.262

2023-01-06 16:46:34,915 - ==> Confusion:
[[ 222   14  203]
 [  14  258  330]
 [  41   61 5843]]

2023-01-06 16:46:34,916 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 150]
2023-01-06 16:46:34,916 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:34,921 - 

2023-01-06 16:46:34,921 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:35,604 - Epoch: [152][   10/  246]    Overall Loss 0.239562    Objective Loss 0.239562                                        LR 0.000008    Time 0.068232    
2023-01-06 16:46:35,749 - Epoch: [152][   20/  246]    Overall Loss 0.240997    Objective Loss 0.240997                                        LR 0.000008    Time 0.041337    
2023-01-06 16:46:35,896 - Epoch: [152][   30/  246]    Overall Loss 0.244653    Objective Loss 0.244653                                        LR 0.000008    Time 0.032454    
2023-01-06 16:46:36,048 - Epoch: [152][   40/  246]    Overall Loss 0.245779    Objective Loss 0.245779                                        LR 0.000008    Time 0.028132    
2023-01-06 16:46:36,209 - Epoch: [152][   50/  246]    Overall Loss 0.243438    Objective Loss 0.243438                                        LR 0.000008    Time 0.025700    
2023-01-06 16:46:36,382 - Epoch: [152][   60/  246]    Overall Loss 0.242445    Objective Loss 0.242445                                        LR 0.000008    Time 0.024299    
2023-01-06 16:46:36,575 - Epoch: [152][   70/  246]    Overall Loss 0.243864    Objective Loss 0.243864                                        LR 0.000008    Time 0.023583    
2023-01-06 16:46:36,761 - Epoch: [152][   80/  246]    Overall Loss 0.242319    Objective Loss 0.242319                                        LR 0.000008    Time 0.022952    
2023-01-06 16:46:36,942 - Epoch: [152][   90/  246]    Overall Loss 0.243325    Objective Loss 0.243325                                        LR 0.000008    Time 0.022410    
2023-01-06 16:46:37,097 - Epoch: [152][  100/  246]    Overall Loss 0.243626    Objective Loss 0.243626                                        LR 0.000008    Time 0.021715    
2023-01-06 16:46:37,246 - Epoch: [152][  110/  246]    Overall Loss 0.243263    Objective Loss 0.243263                                        LR 0.000008    Time 0.021084    
2023-01-06 16:46:37,386 - Epoch: [152][  120/  246]    Overall Loss 0.243626    Objective Loss 0.243626                                        LR 0.000008    Time 0.020491    
2023-01-06 16:46:37,542 - Epoch: [152][  130/  246]    Overall Loss 0.243447    Objective Loss 0.243447                                        LR 0.000008    Time 0.020117    
2023-01-06 16:46:37,708 - Epoch: [152][  140/  246]    Overall Loss 0.244518    Objective Loss 0.244518                                        LR 0.000008    Time 0.019859    
2023-01-06 16:46:37,874 - Epoch: [152][  150/  246]    Overall Loss 0.244077    Objective Loss 0.244077                                        LR 0.000008    Time 0.019641    
2023-01-06 16:46:38,033 - Epoch: [152][  160/  246]    Overall Loss 0.245246    Objective Loss 0.245246                                        LR 0.000008    Time 0.019404    
2023-01-06 16:46:38,187 - Epoch: [152][  170/  246]    Overall Loss 0.246010    Objective Loss 0.246010                                        LR 0.000008    Time 0.019168    
2023-01-06 16:46:38,364 - Epoch: [152][  180/  246]    Overall Loss 0.244924    Objective Loss 0.244924                                        LR 0.000008    Time 0.019084    
2023-01-06 16:46:38,538 - Epoch: [152][  190/  246]    Overall Loss 0.243616    Objective Loss 0.243616                                        LR 0.000008    Time 0.018991    
2023-01-06 16:46:38,717 - Epoch: [152][  200/  246]    Overall Loss 0.243231    Objective Loss 0.243231                                        LR 0.000008    Time 0.018935    
2023-01-06 16:46:38,902 - Epoch: [152][  210/  246]    Overall Loss 0.245119    Objective Loss 0.245119                                        LR 0.000008    Time 0.018911    
2023-01-06 16:46:39,070 - Epoch: [152][  220/  246]    Overall Loss 0.246361    Objective Loss 0.246361                                        LR 0.000008    Time 0.018815    
2023-01-06 16:46:39,252 - Epoch: [152][  230/  246]    Overall Loss 0.246902    Objective Loss 0.246902                                        LR 0.000008    Time 0.018786    
2023-01-06 16:46:39,432 - Epoch: [152][  240/  246]    Overall Loss 0.247081    Objective Loss 0.247081                                        LR 0.000008    Time 0.018751    
2023-01-06 16:46:39,503 - Epoch: [152][  246/  246]    Overall Loss 0.247280    Objective Loss 0.247280    Top1 89.952153    LR 0.000008    Time 0.018581    
2023-01-06 16:46:39,635 - --- validate (epoch=152)-----------
2023-01-06 16:46:39,636 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:40,061 - Epoch: [152][   10/   28]    Loss 0.254266    Top1 90.820312    
2023-01-06 16:46:40,176 - Epoch: [152][   20/   28]    Loss 0.260736    Top1 90.664062    
2023-01-06 16:46:40,228 - Epoch: [152][   28/   28]    Loss 0.260085    Top1 90.466648    
2023-01-06 16:46:40,387 - ==> Top1: 90.467    Loss: 0.260

2023-01-06 16:46:40,387 - ==> Confusion:
[[ 239   15  185]
 [  15  267  320]
 [  61   70 5814]]

2023-01-06 16:46:40,388 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 150]
2023-01-06 16:46:40,388 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:40,394 - 

2023-01-06 16:46:40,394 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:40,930 - Epoch: [153][   10/  246]    Overall Loss 0.226971    Objective Loss 0.226971                                        LR 0.000008    Time 0.053551    
2023-01-06 16:46:41,078 - Epoch: [153][   20/  246]    Overall Loss 0.246047    Objective Loss 0.246047                                        LR 0.000008    Time 0.034147    
2023-01-06 16:46:41,228 - Epoch: [153][   30/  246]    Overall Loss 0.246907    Objective Loss 0.246907                                        LR 0.000008    Time 0.027757    
2023-01-06 16:46:41,370 - Epoch: [153][   40/  246]    Overall Loss 0.248866    Objective Loss 0.248866                                        LR 0.000008    Time 0.024343    
2023-01-06 16:46:41,502 - Epoch: [153][   50/  246]    Overall Loss 0.244429    Objective Loss 0.244429                                        LR 0.000008    Time 0.022110    
2023-01-06 16:46:41,642 - Epoch: [153][   60/  246]    Overall Loss 0.243948    Objective Loss 0.243948                                        LR 0.000008    Time 0.020757    
2023-01-06 16:46:41,780 - Epoch: [153][   70/  246]    Overall Loss 0.242524    Objective Loss 0.242524                                        LR 0.000008    Time 0.019758    
2023-01-06 16:46:41,923 - Epoch: [153][   80/  246]    Overall Loss 0.243515    Objective Loss 0.243515                                        LR 0.000008    Time 0.019071    
2023-01-06 16:46:42,070 - Epoch: [153][   90/  246]    Overall Loss 0.245382    Objective Loss 0.245382                                        LR 0.000008    Time 0.018576    
2023-01-06 16:46:42,232 - Epoch: [153][  100/  246]    Overall Loss 0.246617    Objective Loss 0.246617                                        LR 0.000008    Time 0.018333    
2023-01-06 16:46:42,409 - Epoch: [153][  110/  246]    Overall Loss 0.245625    Objective Loss 0.245625                                        LR 0.000008    Time 0.018274    
2023-01-06 16:46:42,592 - Epoch: [153][  120/  246]    Overall Loss 0.244694    Objective Loss 0.244694                                        LR 0.000008    Time 0.018276    
2023-01-06 16:46:42,776 - Epoch: [153][  130/  246]    Overall Loss 0.243711    Objective Loss 0.243711                                        LR 0.000008    Time 0.018283    
2023-01-06 16:46:42,960 - Epoch: [153][  140/  246]    Overall Loss 0.243654    Objective Loss 0.243654                                        LR 0.000008    Time 0.018287    
2023-01-06 16:46:43,144 - Epoch: [153][  150/  246]    Overall Loss 0.244771    Objective Loss 0.244771                                        LR 0.000008    Time 0.018292    
2023-01-06 16:46:43,327 - Epoch: [153][  160/  246]    Overall Loss 0.245924    Objective Loss 0.245924                                        LR 0.000008    Time 0.018288    
2023-01-06 16:46:43,513 - Epoch: [153][  170/  246]    Overall Loss 0.246052    Objective Loss 0.246052                                        LR 0.000008    Time 0.018302    
2023-01-06 16:46:43,696 - Epoch: [153][  180/  246]    Overall Loss 0.246385    Objective Loss 0.246385                                        LR 0.000008    Time 0.018300    
2023-01-06 16:46:43,881 - Epoch: [153][  190/  246]    Overall Loss 0.247005    Objective Loss 0.247005                                        LR 0.000008    Time 0.018306    
2023-01-06 16:46:44,052 - Epoch: [153][  200/  246]    Overall Loss 0.246827    Objective Loss 0.246827                                        LR 0.000008    Time 0.018243    
2023-01-06 16:46:44,199 - Epoch: [153][  210/  246]    Overall Loss 0.246543    Objective Loss 0.246543                                        LR 0.000008    Time 0.018075    
2023-01-06 16:46:44,344 - Epoch: [153][  220/  246]    Overall Loss 0.246151    Objective Loss 0.246151                                        LR 0.000008    Time 0.017907    
2023-01-06 16:46:44,488 - Epoch: [153][  230/  246]    Overall Loss 0.246530    Objective Loss 0.246530                                        LR 0.000008    Time 0.017753    
2023-01-06 16:46:44,637 - Epoch: [153][  240/  246]    Overall Loss 0.246868    Objective Loss 0.246868                                        LR 0.000008    Time 0.017636    
2023-01-06 16:46:44,705 - Epoch: [153][  246/  246]    Overall Loss 0.247195    Objective Loss 0.247195    Top1 91.148325    LR 0.000008    Time 0.017481    
2023-01-06 16:46:44,843 - --- validate (epoch=153)-----------
2023-01-06 16:46:44,843 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:45,275 - Epoch: [153][   10/   28]    Loss 0.249323    Top1 91.132812    
2023-01-06 16:46:45,376 - Epoch: [153][   20/   28]    Loss 0.262031    Top1 90.683594    
2023-01-06 16:46:45,425 - Epoch: [153][   28/   28]    Loss 0.266301    Top1 90.495276    
2023-01-06 16:46:45,565 - ==> Top1: 90.495    Loss: 0.266

2023-01-06 16:46:45,565 - ==> Confusion:
[[ 248   14  177]
 [  16  260  326]
 [  75   56 5814]]

2023-01-06 16:46:45,566 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 150]
2023-01-06 16:46:45,566 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:45,571 - 

2023-01-06 16:46:45,572 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:46,205 - Epoch: [154][   10/  246]    Overall Loss 0.247547    Objective Loss 0.247547                                        LR 0.000008    Time 0.063244    
2023-01-06 16:46:46,330 - Epoch: [154][   20/  246]    Overall Loss 0.247114    Objective Loss 0.247114                                        LR 0.000008    Time 0.037871    
2023-01-06 16:46:46,471 - Epoch: [154][   30/  246]    Overall Loss 0.241491    Objective Loss 0.241491                                        LR 0.000008    Time 0.029919    
2023-01-06 16:46:46,613 - Epoch: [154][   40/  246]    Overall Loss 0.238491    Objective Loss 0.238491                                        LR 0.000008    Time 0.025994    
2023-01-06 16:46:46,750 - Epoch: [154][   50/  246]    Overall Loss 0.241727    Objective Loss 0.241727                                        LR 0.000008    Time 0.023522    
2023-01-06 16:46:46,889 - Epoch: [154][   60/  246]    Overall Loss 0.244748    Objective Loss 0.244748                                        LR 0.000008    Time 0.021908    
2023-01-06 16:46:47,028 - Epoch: [154][   70/  246]    Overall Loss 0.244251    Objective Loss 0.244251                                        LR 0.000008    Time 0.020768    
2023-01-06 16:46:47,167 - Epoch: [154][   80/  246]    Overall Loss 0.244245    Objective Loss 0.244245                                        LR 0.000008    Time 0.019902    
2023-01-06 16:46:47,310 - Epoch: [154][   90/  246]    Overall Loss 0.244218    Objective Loss 0.244218                                        LR 0.000008    Time 0.019276    
2023-01-06 16:46:47,448 - Epoch: [154][  100/  246]    Overall Loss 0.245158    Objective Loss 0.245158                                        LR 0.000008    Time 0.018722    
2023-01-06 16:46:47,579 - Epoch: [154][  110/  246]    Overall Loss 0.247944    Objective Loss 0.247944                                        LR 0.000008    Time 0.018203    
2023-01-06 16:46:47,714 - Epoch: [154][  120/  246]    Overall Loss 0.248149    Objective Loss 0.248149                                        LR 0.000008    Time 0.017815    
2023-01-06 16:46:47,853 - Epoch: [154][  130/  246]    Overall Loss 0.247702    Objective Loss 0.247702                                        LR 0.000008    Time 0.017511    
2023-01-06 16:46:47,975 - Epoch: [154][  140/  246]    Overall Loss 0.248067    Objective Loss 0.248067                                        LR 0.000008    Time 0.017123    
2023-01-06 16:46:48,105 - Epoch: [154][  150/  246]    Overall Loss 0.248299    Objective Loss 0.248299                                        LR 0.000008    Time 0.016851    
2023-01-06 16:46:48,233 - Epoch: [154][  160/  246]    Overall Loss 0.248369    Objective Loss 0.248369                                        LR 0.000008    Time 0.016594    
2023-01-06 16:46:48,364 - Epoch: [154][  170/  246]    Overall Loss 0.248148    Objective Loss 0.248148                                        LR 0.000008    Time 0.016385    
2023-01-06 16:46:48,492 - Epoch: [154][  180/  246]    Overall Loss 0.247455    Objective Loss 0.247455                                        LR 0.000008    Time 0.016183    
2023-01-06 16:46:48,621 - Epoch: [154][  190/  246]    Overall Loss 0.247603    Objective Loss 0.247603                                        LR 0.000008    Time 0.016002    
2023-01-06 16:46:48,747 - Epoch: [154][  200/  246]    Overall Loss 0.247418    Objective Loss 0.247418                                        LR 0.000008    Time 0.015829    
2023-01-06 16:46:48,880 - Epoch: [154][  210/  246]    Overall Loss 0.247596    Objective Loss 0.247596                                        LR 0.000008    Time 0.015704    
2023-01-06 16:46:49,020 - Epoch: [154][  220/  246]    Overall Loss 0.247720    Objective Loss 0.247720                                        LR 0.000008    Time 0.015627    
2023-01-06 16:46:49,160 - Epoch: [154][  230/  246]    Overall Loss 0.247749    Objective Loss 0.247749                                        LR 0.000008    Time 0.015552    
2023-01-06 16:46:49,313 - Epoch: [154][  240/  246]    Overall Loss 0.247457    Objective Loss 0.247457                                        LR 0.000008    Time 0.015542    
2023-01-06 16:46:49,378 - Epoch: [154][  246/  246]    Overall Loss 0.247447    Objective Loss 0.247447    Top1 87.799043    LR 0.000008    Time 0.015425    
2023-01-06 16:46:49,535 - --- validate (epoch=154)-----------
2023-01-06 16:46:49,536 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:49,959 - Epoch: [154][   10/   28]    Loss 0.269845    Top1 90.429688    
2023-01-06 16:46:50,058 - Epoch: [154][   20/   28]    Loss 0.259644    Top1 90.546875    
2023-01-06 16:46:50,108 - Epoch: [154][   28/   28]    Loss 0.266176    Top1 90.323504    
2023-01-06 16:46:50,247 - ==> Top1: 90.324    Loss: 0.266

2023-01-06 16:46:50,248 - ==> Confusion:
[[ 232   14  193]
 [  15  253  334]
 [  55   65 5825]]

2023-01-06 16:46:50,249 - ==> Best [Top1: 90.667   Sparsity:0.00   Params: 151104 on epoch: 150]
2023-01-06 16:46:50,249 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:50,254 - 

2023-01-06 16:46:50,254 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:50,927 - Epoch: [155][   10/  246]    Overall Loss 0.258760    Objective Loss 0.258760                                        LR 0.000008    Time 0.067212    
2023-01-06 16:46:51,072 - Epoch: [155][   20/  246]    Overall Loss 0.247706    Objective Loss 0.247706                                        LR 0.000008    Time 0.040844    
2023-01-06 16:46:51,218 - Epoch: [155][   30/  246]    Overall Loss 0.244652    Objective Loss 0.244652                                        LR 0.000008    Time 0.032030    
2023-01-06 16:46:51,355 - Epoch: [155][   40/  246]    Overall Loss 0.248586    Objective Loss 0.248586                                        LR 0.000008    Time 0.027433    
2023-01-06 16:46:51,494 - Epoch: [155][   50/  246]    Overall Loss 0.249818    Objective Loss 0.249818                                        LR 0.000008    Time 0.024704    
2023-01-06 16:46:51,635 - Epoch: [155][   60/  246]    Overall Loss 0.249827    Objective Loss 0.249827                                        LR 0.000008    Time 0.022942    
2023-01-06 16:46:51,777 - Epoch: [155][   70/  246]    Overall Loss 0.251648    Objective Loss 0.251648                                        LR 0.000008    Time 0.021685    
2023-01-06 16:46:51,905 - Epoch: [155][   80/  246]    Overall Loss 0.249192    Objective Loss 0.249192                                        LR 0.000008    Time 0.020563    
2023-01-06 16:46:52,041 - Epoch: [155][   90/  246]    Overall Loss 0.249038    Objective Loss 0.249038                                        LR 0.000008    Time 0.019788    
2023-01-06 16:46:52,176 - Epoch: [155][  100/  246]    Overall Loss 0.248238    Objective Loss 0.248238                                        LR 0.000008    Time 0.019152    
2023-01-06 16:46:52,315 - Epoch: [155][  110/  246]    Overall Loss 0.246947    Objective Loss 0.246947                                        LR 0.000008    Time 0.018673    
2023-01-06 16:46:52,454 - Epoch: [155][  120/  246]    Overall Loss 0.245464    Objective Loss 0.245464                                        LR 0.000008    Time 0.018278    
2023-01-06 16:46:52,595 - Epoch: [155][  130/  246]    Overall Loss 0.246040    Objective Loss 0.246040                                        LR 0.000008    Time 0.017955    
2023-01-06 16:46:52,735 - Epoch: [155][  140/  246]    Overall Loss 0.245108    Objective Loss 0.245108                                        LR 0.000008    Time 0.017668    
2023-01-06 16:46:52,871 - Epoch: [155][  150/  246]    Overall Loss 0.245933    Objective Loss 0.245933                                        LR 0.000008    Time 0.017392    
2023-01-06 16:46:52,988 - Epoch: [155][  160/  246]    Overall Loss 0.245153    Objective Loss 0.245153                                        LR 0.000008    Time 0.017032    
2023-01-06 16:46:53,105 - Epoch: [155][  170/  246]    Overall Loss 0.245771    Objective Loss 0.245771                                        LR 0.000008    Time 0.016717    
2023-01-06 16:46:53,222 - Epoch: [155][  180/  246]    Overall Loss 0.245803    Objective Loss 0.245803                                        LR 0.000008    Time 0.016436    
2023-01-06 16:46:53,338 - Epoch: [155][  190/  246]    Overall Loss 0.245799    Objective Loss 0.245799                                        LR 0.000008    Time 0.016181    
2023-01-06 16:46:53,456 - Epoch: [155][  200/  246]    Overall Loss 0.246419    Objective Loss 0.246419                                        LR 0.000008    Time 0.015962    
2023-01-06 16:46:53,573 - Epoch: [155][  210/  246]    Overall Loss 0.246700    Objective Loss 0.246700                                        LR 0.000008    Time 0.015754    
2023-01-06 16:46:53,689 - Epoch: [155][  220/  246]    Overall Loss 0.246565    Objective Loss 0.246565                                        LR 0.000008    Time 0.015566    
2023-01-06 16:46:53,806 - Epoch: [155][  230/  246]    Overall Loss 0.246860    Objective Loss 0.246860                                        LR 0.000008    Time 0.015397    
2023-01-06 16:46:53,944 - Epoch: [155][  240/  246]    Overall Loss 0.246739    Objective Loss 0.246739                                        LR 0.000008    Time 0.015327    
2023-01-06 16:46:54,009 - Epoch: [155][  246/  246]    Overall Loss 0.246494    Objective Loss 0.246494    Top1 90.669856    LR 0.000008    Time 0.015219    
2023-01-06 16:46:54,165 - --- validate (epoch=155)-----------
2023-01-06 16:46:54,165 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:54,582 - Epoch: [155][   10/   28]    Loss 0.272849    Top1 90.585938    
2023-01-06 16:46:54,675 - Epoch: [155][   20/   28]    Loss 0.267869    Top1 90.429688    
2023-01-06 16:46:54,723 - Epoch: [155][   28/   28]    Loss 0.263803    Top1 90.681363    
2023-01-06 16:46:54,886 - ==> Top1: 90.681    Loss: 0.264

2023-01-06 16:46:54,886 - ==> Confusion:
[[ 245   14  180]
 [  16  274  312]
 [  56   73 5816]]

2023-01-06 16:46:54,887 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:46:54,887 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:54,893 - 

2023-01-06 16:46:54,894 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:46:55,414 - Epoch: [156][   10/  246]    Overall Loss 0.244140    Objective Loss 0.244140                                        LR 0.000008    Time 0.051954    
2023-01-06 16:46:55,543 - Epoch: [156][   20/  246]    Overall Loss 0.246008    Objective Loss 0.246008                                        LR 0.000008    Time 0.032401    
2023-01-06 16:46:55,675 - Epoch: [156][   30/  246]    Overall Loss 0.245974    Objective Loss 0.245974                                        LR 0.000008    Time 0.026009    
2023-01-06 16:46:55,802 - Epoch: [156][   40/  246]    Overall Loss 0.246311    Objective Loss 0.246311                                        LR 0.000008    Time 0.022674    
2023-01-06 16:46:55,929 - Epoch: [156][   50/  246]    Overall Loss 0.242420    Objective Loss 0.242420                                        LR 0.000008    Time 0.020663    
2023-01-06 16:46:56,056 - Epoch: [156][   60/  246]    Overall Loss 0.242275    Objective Loss 0.242275                                        LR 0.000008    Time 0.019328    
2023-01-06 16:46:56,176 - Epoch: [156][   70/  246]    Overall Loss 0.242069    Objective Loss 0.242069                                        LR 0.000008    Time 0.018284    
2023-01-06 16:46:56,309 - Epoch: [156][   80/  246]    Overall Loss 0.243045    Objective Loss 0.243045                                        LR 0.000008    Time 0.017656    
2023-01-06 16:46:56,447 - Epoch: [156][   90/  246]    Overall Loss 0.242342    Objective Loss 0.242342                                        LR 0.000008    Time 0.017222    
2023-01-06 16:46:56,573 - Epoch: [156][  100/  246]    Overall Loss 0.243181    Objective Loss 0.243181                                        LR 0.000008    Time 0.016752    
2023-01-06 16:46:56,709 - Epoch: [156][  110/  246]    Overall Loss 0.243550    Objective Loss 0.243550                                        LR 0.000008    Time 0.016464    
2023-01-06 16:46:56,847 - Epoch: [156][  120/  246]    Overall Loss 0.243828    Objective Loss 0.243828                                        LR 0.000008    Time 0.016239    
2023-01-06 16:46:56,986 - Epoch: [156][  130/  246]    Overall Loss 0.243917    Objective Loss 0.243917                                        LR 0.000008    Time 0.016053    
2023-01-06 16:46:57,119 - Epoch: [156][  140/  246]    Overall Loss 0.245084    Objective Loss 0.245084                                        LR 0.000008    Time 0.015860    
2023-01-06 16:46:57,259 - Epoch: [156][  150/  246]    Overall Loss 0.244136    Objective Loss 0.244136                                        LR 0.000008    Time 0.015730    
2023-01-06 16:46:57,401 - Epoch: [156][  160/  246]    Overall Loss 0.244366    Objective Loss 0.244366                                        LR 0.000008    Time 0.015635    
2023-01-06 16:46:57,541 - Epoch: [156][  170/  246]    Overall Loss 0.243625    Objective Loss 0.243625                                        LR 0.000008    Time 0.015535    
2023-01-06 16:46:57,681 - Epoch: [156][  180/  246]    Overall Loss 0.243954    Objective Loss 0.243954                                        LR 0.000008    Time 0.015447    
2023-01-06 16:46:57,817 - Epoch: [156][  190/  246]    Overall Loss 0.244830    Objective Loss 0.244830                                        LR 0.000008    Time 0.015350    
2023-01-06 16:46:57,960 - Epoch: [156][  200/  246]    Overall Loss 0.245258    Objective Loss 0.245258                                        LR 0.000008    Time 0.015296    
2023-01-06 16:46:58,101 - Epoch: [156][  210/  246]    Overall Loss 0.245102    Objective Loss 0.245102                                        LR 0.000008    Time 0.015235    
2023-01-06 16:46:58,237 - Epoch: [156][  220/  246]    Overall Loss 0.245427    Objective Loss 0.245427                                        LR 0.000008    Time 0.015162    
2023-01-06 16:46:58,366 - Epoch: [156][  230/  246]    Overall Loss 0.246150    Objective Loss 0.246150                                        LR 0.000008    Time 0.015059    
2023-01-06 16:46:58,511 - Epoch: [156][  240/  246]    Overall Loss 0.246777    Objective Loss 0.246777                                        LR 0.000008    Time 0.015037    
2023-01-06 16:46:58,575 - Epoch: [156][  246/  246]    Overall Loss 0.247070    Objective Loss 0.247070    Top1 89.952153    LR 0.000008    Time 0.014929    
2023-01-06 16:46:58,730 - --- validate (epoch=156)-----------
2023-01-06 16:46:58,731 - 6986 samples (256 per mini-batch)
2023-01-06 16:46:59,159 - Epoch: [156][   10/   28]    Loss 0.250444    Top1 91.132812    
2023-01-06 16:46:59,269 - Epoch: [156][   20/   28]    Loss 0.252126    Top1 90.957031    
2023-01-06 16:46:59,318 - Epoch: [156][   28/   28]    Loss 0.264651    Top1 90.438019    
2023-01-06 16:46:59,479 - ==> Top1: 90.438    Loss: 0.265

2023-01-06 16:46:59,479 - ==> Confusion:
[[ 245   15  179]
 [  18  262  322]
 [  68   66 5811]]

2023-01-06 16:46:59,481 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:46:59,481 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:46:59,486 - 

2023-01-06 16:46:59,486 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:00,137 - Epoch: [157][   10/  246]    Overall Loss 0.227514    Objective Loss 0.227514                                        LR 0.000008    Time 0.065011    
2023-01-06 16:47:00,267 - Epoch: [157][   20/  246]    Overall Loss 0.233015    Objective Loss 0.233015                                        LR 0.000008    Time 0.039000    
2023-01-06 16:47:00,399 - Epoch: [157][   30/  246]    Overall Loss 0.238383    Objective Loss 0.238383                                        LR 0.000008    Time 0.030378    
2023-01-06 16:47:00,530 - Epoch: [157][   40/  246]    Overall Loss 0.240199    Objective Loss 0.240199                                        LR 0.000008    Time 0.026061    
2023-01-06 16:47:00,668 - Epoch: [157][   50/  246]    Overall Loss 0.240876    Objective Loss 0.240876                                        LR 0.000008    Time 0.023598    
2023-01-06 16:47:00,810 - Epoch: [157][   60/  246]    Overall Loss 0.242066    Objective Loss 0.242066                                        LR 0.000008    Time 0.022025    
2023-01-06 16:47:00,959 - Epoch: [157][   70/  246]    Overall Loss 0.243012    Objective Loss 0.243012                                        LR 0.000008    Time 0.021004    
2023-01-06 16:47:01,099 - Epoch: [157][   80/  246]    Overall Loss 0.242930    Objective Loss 0.242930                                        LR 0.000008    Time 0.020120    
2023-01-06 16:47:01,243 - Epoch: [157][   90/  246]    Overall Loss 0.241991    Objective Loss 0.241991                                        LR 0.000008    Time 0.019485    
2023-01-06 16:47:01,387 - Epoch: [157][  100/  246]    Overall Loss 0.243751    Objective Loss 0.243751                                        LR 0.000008    Time 0.018970    
2023-01-06 16:47:01,522 - Epoch: [157][  110/  246]    Overall Loss 0.245705    Objective Loss 0.245705                                        LR 0.000008    Time 0.018474    
2023-01-06 16:47:01,656 - Epoch: [157][  120/  246]    Overall Loss 0.246508    Objective Loss 0.246508                                        LR 0.000008    Time 0.018046    
2023-01-06 16:47:01,794 - Epoch: [157][  130/  246]    Overall Loss 0.247671    Objective Loss 0.247671                                        LR 0.000008    Time 0.017711    
2023-01-06 16:47:01,937 - Epoch: [157][  140/  246]    Overall Loss 0.246968    Objective Loss 0.246968                                        LR 0.000008    Time 0.017459    
2023-01-06 16:47:02,076 - Epoch: [157][  150/  246]    Overall Loss 0.246759    Objective Loss 0.246759                                        LR 0.000008    Time 0.017218    
2023-01-06 16:47:02,219 - Epoch: [157][  160/  246]    Overall Loss 0.247221    Objective Loss 0.247221                                        LR 0.000008    Time 0.017031    
2023-01-06 16:47:02,357 - Epoch: [157][  170/  246]    Overall Loss 0.246506    Objective Loss 0.246506                                        LR 0.000008    Time 0.016836    
2023-01-06 16:47:02,490 - Epoch: [157][  180/  246]    Overall Loss 0.247096    Objective Loss 0.247096                                        LR 0.000008    Time 0.016638    
2023-01-06 16:47:02,624 - Epoch: [157][  190/  246]    Overall Loss 0.247510    Objective Loss 0.247510                                        LR 0.000008    Time 0.016465    
2023-01-06 16:47:02,758 - Epoch: [157][  200/  246]    Overall Loss 0.246418    Objective Loss 0.246418                                        LR 0.000008    Time 0.016308    
2023-01-06 16:47:02,893 - Epoch: [157][  210/  246]    Overall Loss 0.246711    Objective Loss 0.246711                                        LR 0.000008    Time 0.016174    
2023-01-06 16:47:03,028 - Epoch: [157][  220/  246]    Overall Loss 0.246123    Objective Loss 0.246123                                        LR 0.000008    Time 0.016049    
2023-01-06 16:47:03,166 - Epoch: [157][  230/  246]    Overall Loss 0.246712    Objective Loss 0.246712                                        LR 0.000008    Time 0.015951    
2023-01-06 16:47:03,320 - Epoch: [157][  240/  246]    Overall Loss 0.246188    Objective Loss 0.246188                                        LR 0.000008    Time 0.015928    
2023-01-06 16:47:03,385 - Epoch: [157][  246/  246]    Overall Loss 0.246748    Objective Loss 0.246748    Top1 91.148325    LR 0.000008    Time 0.015804    
2023-01-06 16:47:03,539 - --- validate (epoch=157)-----------
2023-01-06 16:47:03,539 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:03,957 - Epoch: [157][   10/   28]    Loss 0.249562    Top1 91.328125    
2023-01-06 16:47:04,054 - Epoch: [157][   20/   28]    Loss 0.265455    Top1 90.234375    
2023-01-06 16:47:04,104 - Epoch: [157][   28/   28]    Loss 0.262293    Top1 90.566848    
2023-01-06 16:47:04,233 - ==> Top1: 90.567    Loss: 0.262

2023-01-06 16:47:04,234 - ==> Confusion:
[[ 253   14  172]
 [  17  265  320]
 [  68   68 5809]]

2023-01-06 16:47:04,235 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:04,235 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:04,240 - 

2023-01-06 16:47:04,240 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:04,906 - Epoch: [158][   10/  246]    Overall Loss 0.247046    Objective Loss 0.247046                                        LR 0.000008    Time 0.066554    
2023-01-06 16:47:05,043 - Epoch: [158][   20/  246]    Overall Loss 0.255985    Objective Loss 0.255985                                        LR 0.000008    Time 0.040104    
2023-01-06 16:47:05,180 - Epoch: [158][   30/  246]    Overall Loss 0.252767    Objective Loss 0.252767                                        LR 0.000008    Time 0.031291    
2023-01-06 16:47:05,320 - Epoch: [158][   40/  246]    Overall Loss 0.250470    Objective Loss 0.250470                                        LR 0.000008    Time 0.026942    
2023-01-06 16:47:05,447 - Epoch: [158][   50/  246]    Overall Loss 0.248061    Objective Loss 0.248061                                        LR 0.000008    Time 0.024103    
2023-01-06 16:47:05,562 - Epoch: [158][   60/  246]    Overall Loss 0.248467    Objective Loss 0.248467                                        LR 0.000008    Time 0.021993    
2023-01-06 16:47:05,687 - Epoch: [158][   70/  246]    Overall Loss 0.246499    Objective Loss 0.246499                                        LR 0.000008    Time 0.020635    
2023-01-06 16:47:05,817 - Epoch: [158][   80/  246]    Overall Loss 0.246178    Objective Loss 0.246178                                        LR 0.000008    Time 0.019657    
2023-01-06 16:47:05,948 - Epoch: [158][   90/  246]    Overall Loss 0.245711    Objective Loss 0.245711                                        LR 0.000008    Time 0.018922    
2023-01-06 16:47:06,080 - Epoch: [158][  100/  246]    Overall Loss 0.245810    Objective Loss 0.245810                                        LR 0.000008    Time 0.018339    
2023-01-06 16:47:06,209 - Epoch: [158][  110/  246]    Overall Loss 0.245907    Objective Loss 0.245907                                        LR 0.000008    Time 0.017840    
2023-01-06 16:47:06,338 - Epoch: [158][  120/  246]    Overall Loss 0.245365    Objective Loss 0.245365                                        LR 0.000008    Time 0.017420    
2023-01-06 16:47:06,468 - Epoch: [158][  130/  246]    Overall Loss 0.246342    Objective Loss 0.246342                                        LR 0.000008    Time 0.017077    
2023-01-06 16:47:06,600 - Epoch: [158][  140/  246]    Overall Loss 0.246894    Objective Loss 0.246894                                        LR 0.000008    Time 0.016792    
2023-01-06 16:47:06,729 - Epoch: [158][  150/  246]    Overall Loss 0.247736    Objective Loss 0.247736                                        LR 0.000008    Time 0.016524    
2023-01-06 16:47:06,858 - Epoch: [158][  160/  246]    Overall Loss 0.248192    Objective Loss 0.248192                                        LR 0.000008    Time 0.016299    
2023-01-06 16:47:06,987 - Epoch: [158][  170/  246]    Overall Loss 0.247263    Objective Loss 0.247263                                        LR 0.000008    Time 0.016097    
2023-01-06 16:47:07,118 - Epoch: [158][  180/  246]    Overall Loss 0.246417    Objective Loss 0.246417                                        LR 0.000008    Time 0.015925    
2023-01-06 16:47:07,249 - Epoch: [158][  190/  246]    Overall Loss 0.246249    Objective Loss 0.246249                                        LR 0.000008    Time 0.015777    
2023-01-06 16:47:07,380 - Epoch: [158][  200/  246]    Overall Loss 0.247026    Objective Loss 0.247026                                        LR 0.000008    Time 0.015643    
2023-01-06 16:47:07,511 - Epoch: [158][  210/  246]    Overall Loss 0.247016    Objective Loss 0.247016                                        LR 0.000008    Time 0.015519    
2023-01-06 16:47:07,641 - Epoch: [158][  220/  246]    Overall Loss 0.246616    Objective Loss 0.246616                                        LR 0.000008    Time 0.015403    
2023-01-06 16:47:07,772 - Epoch: [158][  230/  246]    Overall Loss 0.246600    Objective Loss 0.246600                                        LR 0.000008    Time 0.015301    
2023-01-06 16:47:07,923 - Epoch: [158][  240/  246]    Overall Loss 0.247017    Objective Loss 0.247017                                        LR 0.000008    Time 0.015292    
2023-01-06 16:47:07,987 - Epoch: [158][  246/  246]    Overall Loss 0.246893    Objective Loss 0.246893    Top1 90.430622    LR 0.000008    Time 0.015178    
2023-01-06 16:47:08,118 - --- validate (epoch=158)-----------
2023-01-06 16:47:08,118 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:08,543 - Epoch: [158][   10/   28]    Loss 0.284830    Top1 90.312500    
2023-01-06 16:47:08,640 - Epoch: [158][   20/   28]    Loss 0.268382    Top1 90.625000    
2023-01-06 16:47:08,690 - Epoch: [158][   28/   28]    Loss 0.265250    Top1 90.609791    
2023-01-06 16:47:08,845 - ==> Top1: 90.610    Loss: 0.265

2023-01-06 16:47:08,845 - ==> Confusion:
[[ 235   14  190]
 [  17  246  339]
 [  44   52 5849]]

2023-01-06 16:47:08,846 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:08,846 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:08,851 - 

2023-01-06 16:47:08,852 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:09,371 - Epoch: [159][   10/  246]    Overall Loss 0.249477    Objective Loss 0.249477                                        LR 0.000008    Time 0.051914    
2023-01-06 16:47:09,501 - Epoch: [159][   20/  246]    Overall Loss 0.245526    Objective Loss 0.245526                                        LR 0.000008    Time 0.032432    
2023-01-06 16:47:09,644 - Epoch: [159][   30/  246]    Overall Loss 0.247964    Objective Loss 0.247964                                        LR 0.000008    Time 0.026351    
2023-01-06 16:47:09,779 - Epoch: [159][   40/  246]    Overall Loss 0.246452    Objective Loss 0.246452                                        LR 0.000008    Time 0.023138    
2023-01-06 16:47:09,914 - Epoch: [159][   50/  246]    Overall Loss 0.247632    Objective Loss 0.247632                                        LR 0.000008    Time 0.021197    
2023-01-06 16:47:10,048 - Epoch: [159][   60/  246]    Overall Loss 0.245884    Objective Loss 0.245884                                        LR 0.000008    Time 0.019875    
2023-01-06 16:47:10,186 - Epoch: [159][   70/  246]    Overall Loss 0.246289    Objective Loss 0.246289                                        LR 0.000008    Time 0.018977    
2023-01-06 16:47:10,324 - Epoch: [159][   80/  246]    Overall Loss 0.245655    Objective Loss 0.245655                                        LR 0.000008    Time 0.018326    
2023-01-06 16:47:10,469 - Epoch: [159][   90/  246]    Overall Loss 0.245086    Objective Loss 0.245086                                        LR 0.000008    Time 0.017895    
2023-01-06 16:47:10,619 - Epoch: [159][  100/  246]    Overall Loss 0.243059    Objective Loss 0.243059                                        LR 0.000008    Time 0.017596    
2023-01-06 16:47:10,779 - Epoch: [159][  110/  246]    Overall Loss 0.242691    Objective Loss 0.242691                                        LR 0.000008    Time 0.017419    
2023-01-06 16:47:10,927 - Epoch: [159][  120/  246]    Overall Loss 0.242467    Objective Loss 0.242467                                        LR 0.000008    Time 0.017200    
2023-01-06 16:47:11,086 - Epoch: [159][  130/  246]    Overall Loss 0.244352    Objective Loss 0.244352                                        LR 0.000008    Time 0.017077    
2023-01-06 16:47:11,237 - Epoch: [159][  140/  246]    Overall Loss 0.244429    Objective Loss 0.244429                                        LR 0.000008    Time 0.016928    
2023-01-06 16:47:11,390 - Epoch: [159][  150/  246]    Overall Loss 0.244377    Objective Loss 0.244377                                        LR 0.000008    Time 0.016819    
2023-01-06 16:47:11,536 - Epoch: [159][  160/  246]    Overall Loss 0.245910    Objective Loss 0.245910                                        LR 0.000008    Time 0.016675    
2023-01-06 16:47:11,686 - Epoch: [159][  170/  246]    Overall Loss 0.246799    Objective Loss 0.246799                                        LR 0.000008    Time 0.016574    
2023-01-06 16:47:11,832 - Epoch: [159][  180/  246]    Overall Loss 0.247501    Objective Loss 0.247501                                        LR 0.000008    Time 0.016462    
2023-01-06 16:47:11,981 - Epoch: [159][  190/  246]    Overall Loss 0.246828    Objective Loss 0.246828                                        LR 0.000008    Time 0.016377    
2023-01-06 16:47:12,126 - Epoch: [159][  200/  246]    Overall Loss 0.246860    Objective Loss 0.246860                                        LR 0.000008    Time 0.016284    
2023-01-06 16:47:12,268 - Epoch: [159][  210/  246]    Overall Loss 0.247189    Objective Loss 0.247189                                        LR 0.000008    Time 0.016181    
2023-01-06 16:47:12,408 - Epoch: [159][  220/  246]    Overall Loss 0.246550    Objective Loss 0.246550                                        LR 0.000008    Time 0.016082    
2023-01-06 16:47:12,548 - Epoch: [159][  230/  246]    Overall Loss 0.246574    Objective Loss 0.246574                                        LR 0.000008    Time 0.015989    
2023-01-06 16:47:12,705 - Epoch: [159][  240/  246]    Overall Loss 0.246767    Objective Loss 0.246767                                        LR 0.000008    Time 0.015975    
2023-01-06 16:47:12,769 - Epoch: [159][  246/  246]    Overall Loss 0.246305    Objective Loss 0.246305    Top1 91.626794    LR 0.000008    Time 0.015845    
2023-01-06 16:47:12,909 - --- validate (epoch=159)-----------
2023-01-06 16:47:12,909 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:13,352 - Epoch: [159][   10/   28]    Loss 0.278226    Top1 89.882812    
2023-01-06 16:47:13,452 - Epoch: [159][   20/   28]    Loss 0.268076    Top1 90.371094    
2023-01-06 16:47:13,501 - Epoch: [159][   28/   28]    Loss 0.260063    Top1 90.452333    
2023-01-06 16:47:13,664 - ==> Top1: 90.452    Loss: 0.260

2023-01-06 16:47:13,664 - ==> Confusion:
[[ 227   16  196]
 [  12  264  326]
 [  51   66 5828]]

2023-01-06 16:47:13,665 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:13,665 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:13,671 - 

2023-01-06 16:47:13,671 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:14,348 - Epoch: [160][   10/  246]    Overall Loss 0.253041    Objective Loss 0.253041                                        LR 0.000008    Time 0.067666    
2023-01-06 16:47:14,486 - Epoch: [160][   20/  246]    Overall Loss 0.251498    Objective Loss 0.251498                                        LR 0.000008    Time 0.040711    
2023-01-06 16:47:14,621 - Epoch: [160][   30/  246]    Overall Loss 0.246387    Objective Loss 0.246387                                        LR 0.000008    Time 0.031641    
2023-01-06 16:47:14,750 - Epoch: [160][   40/  246]    Overall Loss 0.243792    Objective Loss 0.243792                                        LR 0.000008    Time 0.026947    
2023-01-06 16:47:14,886 - Epoch: [160][   50/  246]    Overall Loss 0.244174    Objective Loss 0.244174                                        LR 0.000008    Time 0.024272    
2023-01-06 16:47:15,026 - Epoch: [160][   60/  246]    Overall Loss 0.244005    Objective Loss 0.244005                                        LR 0.000008    Time 0.022544    
2023-01-06 16:47:15,177 - Epoch: [160][   70/  246]    Overall Loss 0.246028    Objective Loss 0.246028                                        LR 0.000008    Time 0.021478    
2023-01-06 16:47:15,329 - Epoch: [160][   80/  246]    Overall Loss 0.248418    Objective Loss 0.248418                                        LR 0.000008    Time 0.020682    
2023-01-06 16:47:15,480 - Epoch: [160][   90/  246]    Overall Loss 0.248145    Objective Loss 0.248145                                        LR 0.000008    Time 0.020064    
2023-01-06 16:47:15,630 - Epoch: [160][  100/  246]    Overall Loss 0.248353    Objective Loss 0.248353                                        LR 0.000008    Time 0.019549    
2023-01-06 16:47:15,779 - Epoch: [160][  110/  246]    Overall Loss 0.247383    Objective Loss 0.247383                                        LR 0.000008    Time 0.019123    
2023-01-06 16:47:15,927 - Epoch: [160][  120/  246]    Overall Loss 0.247462    Objective Loss 0.247462                                        LR 0.000008    Time 0.018766    
2023-01-06 16:47:16,075 - Epoch: [160][  130/  246]    Overall Loss 0.247993    Objective Loss 0.247993                                        LR 0.000008    Time 0.018459    
2023-01-06 16:47:16,222 - Epoch: [160][  140/  246]    Overall Loss 0.247816    Objective Loss 0.247816                                        LR 0.000008    Time 0.018187    
2023-01-06 16:47:16,371 - Epoch: [160][  150/  246]    Overall Loss 0.247771    Objective Loss 0.247771                                        LR 0.000008    Time 0.017961    
2023-01-06 16:47:16,518 - Epoch: [160][  160/  246]    Overall Loss 0.248344    Objective Loss 0.248344                                        LR 0.000008    Time 0.017757    
2023-01-06 16:47:16,666 - Epoch: [160][  170/  246]    Overall Loss 0.247102    Objective Loss 0.247102                                        LR 0.000008    Time 0.017582    
2023-01-06 16:47:16,815 - Epoch: [160][  180/  246]    Overall Loss 0.247396    Objective Loss 0.247396                                        LR 0.000008    Time 0.017429    
2023-01-06 16:47:16,964 - Epoch: [160][  190/  246]    Overall Loss 0.247662    Objective Loss 0.247662                                        LR 0.000008    Time 0.017294    
2023-01-06 16:47:17,112 - Epoch: [160][  200/  246]    Overall Loss 0.247009    Objective Loss 0.247009                                        LR 0.000008    Time 0.017168    
2023-01-06 16:47:17,256 - Epoch: [160][  210/  246]    Overall Loss 0.246907    Objective Loss 0.246907                                        LR 0.000008    Time 0.017039    
2023-01-06 16:47:17,400 - Epoch: [160][  220/  246]    Overall Loss 0.246806    Objective Loss 0.246806                                        LR 0.000008    Time 0.016916    
2023-01-06 16:47:17,545 - Epoch: [160][  230/  246]    Overall Loss 0.246102    Objective Loss 0.246102                                        LR 0.000008    Time 0.016810    
2023-01-06 16:47:17,702 - Epoch: [160][  240/  246]    Overall Loss 0.246079    Objective Loss 0.246079                                        LR 0.000008    Time 0.016761    
2023-01-06 16:47:17,770 - Epoch: [160][  246/  246]    Overall Loss 0.246112    Objective Loss 0.246112    Top1 92.583732    LR 0.000008    Time 0.016626    
2023-01-06 16:47:17,922 - --- validate (epoch=160)-----------
2023-01-06 16:47:17,922 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:18,348 - Epoch: [160][   10/   28]    Loss 0.256246    Top1 90.195312    
2023-01-06 16:47:18,445 - Epoch: [160][   20/   28]    Loss 0.261624    Top1 90.351562    
2023-01-06 16:47:18,499 - Epoch: [160][   28/   28]    Loss 0.261948    Top1 90.294875    
2023-01-06 16:47:18,633 - ==> Top1: 90.295    Loss: 0.262

2023-01-06 16:47:18,634 - ==> Confusion:
[[ 248   15  176]
 [  20  262  320]
 [  75   72 5798]]

2023-01-06 16:47:18,635 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:18,635 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:18,640 - 

2023-01-06 16:47:18,640 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:19,144 - Epoch: [161][   10/  246]    Overall Loss 0.253006    Objective Loss 0.253006                                        LR 0.000008    Time 0.050282    
2023-01-06 16:47:19,265 - Epoch: [161][   20/  246]    Overall Loss 0.237729    Objective Loss 0.237729                                        LR 0.000008    Time 0.031203    
2023-01-06 16:47:19,385 - Epoch: [161][   30/  246]    Overall Loss 0.244939    Objective Loss 0.244939                                        LR 0.000008    Time 0.024779    
2023-01-06 16:47:19,506 - Epoch: [161][   40/  246]    Overall Loss 0.243270    Objective Loss 0.243270                                        LR 0.000008    Time 0.021593    
2023-01-06 16:47:19,621 - Epoch: [161][   50/  246]    Overall Loss 0.245696    Objective Loss 0.245696                                        LR 0.000008    Time 0.019568    
2023-01-06 16:47:19,742 - Epoch: [161][   60/  246]    Overall Loss 0.244513    Objective Loss 0.244513                                        LR 0.000008    Time 0.018313    
2023-01-06 16:47:19,872 - Epoch: [161][   70/  246]    Overall Loss 0.247241    Objective Loss 0.247241                                        LR 0.000008    Time 0.017558    
2023-01-06 16:47:19,999 - Epoch: [161][   80/  246]    Overall Loss 0.245304    Objective Loss 0.245304                                        LR 0.000008    Time 0.016942    
2023-01-06 16:47:20,125 - Epoch: [161][   90/  246]    Overall Loss 0.246863    Objective Loss 0.246863                                        LR 0.000008    Time 0.016456    
2023-01-06 16:47:20,249 - Epoch: [161][  100/  246]    Overall Loss 0.244492    Objective Loss 0.244492                                        LR 0.000008    Time 0.016048    
2023-01-06 16:47:20,375 - Epoch: [161][  110/  246]    Overall Loss 0.246658    Objective Loss 0.246658                                        LR 0.000008    Time 0.015732    
2023-01-06 16:47:20,503 - Epoch: [161][  120/  246]    Overall Loss 0.246604    Objective Loss 0.246604                                        LR 0.000008    Time 0.015484    
2023-01-06 16:47:20,641 - Epoch: [161][  130/  246]    Overall Loss 0.247142    Objective Loss 0.247142                                        LR 0.000008    Time 0.015351    
2023-01-06 16:47:20,764 - Epoch: [161][  140/  246]    Overall Loss 0.246584    Objective Loss 0.246584                                        LR 0.000008    Time 0.015131    
2023-01-06 16:47:20,886 - Epoch: [161][  150/  246]    Overall Loss 0.247445    Objective Loss 0.247445                                        LR 0.000008    Time 0.014931    
2023-01-06 16:47:21,008 - Epoch: [161][  160/  246]    Overall Loss 0.247835    Objective Loss 0.247835                                        LR 0.000008    Time 0.014758    
2023-01-06 16:47:21,130 - Epoch: [161][  170/  246]    Overall Loss 0.247327    Objective Loss 0.247327                                        LR 0.000008    Time 0.014603    
2023-01-06 16:47:21,251 - Epoch: [161][  180/  246]    Overall Loss 0.246560    Objective Loss 0.246560                                        LR 0.000008    Time 0.014456    
2023-01-06 16:47:21,374 - Epoch: [161][  190/  246]    Overall Loss 0.247032    Objective Loss 0.247032                                        LR 0.000008    Time 0.014340    
2023-01-06 16:47:21,496 - Epoch: [161][  200/  246]    Overall Loss 0.246795    Objective Loss 0.246795                                        LR 0.000008    Time 0.014232    
2023-01-06 16:47:21,616 - Epoch: [161][  210/  246]    Overall Loss 0.246196    Objective Loss 0.246196                                        LR 0.000008    Time 0.014124    
2023-01-06 16:47:21,734 - Epoch: [161][  220/  246]    Overall Loss 0.246591    Objective Loss 0.246591                                        LR 0.000008    Time 0.014015    
2023-01-06 16:47:21,853 - Epoch: [161][  230/  246]    Overall Loss 0.246815    Objective Loss 0.246815                                        LR 0.000008    Time 0.013924    
2023-01-06 16:47:21,988 - Epoch: [161][  240/  246]    Overall Loss 0.246864    Objective Loss 0.246864                                        LR 0.000008    Time 0.013903    
2023-01-06 16:47:22,049 - Epoch: [161][  246/  246]    Overall Loss 0.246657    Objective Loss 0.246657    Top1 92.105263    LR 0.000008    Time 0.013812    
2023-01-06 16:47:22,199 - --- validate (epoch=161)-----------
2023-01-06 16:47:22,199 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:22,630 - Epoch: [161][   10/   28]    Loss 0.275227    Top1 90.234375    
2023-01-06 16:47:22,727 - Epoch: [161][   20/   28]    Loss 0.272366    Top1 90.156250    
2023-01-06 16:47:22,776 - Epoch: [161][   28/   28]    Loss 0.264204    Top1 90.452333    
2023-01-06 16:47:22,935 - ==> Top1: 90.452    Loss: 0.264

2023-01-06 16:47:22,935 - ==> Confusion:
[[ 258   15  166]
 [  19  264  319]
 [  82   66 5797]]

2023-01-06 16:47:22,936 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:22,936 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:22,941 - 

2023-01-06 16:47:22,942 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:23,589 - Epoch: [162][   10/  246]    Overall Loss 0.235603    Objective Loss 0.235603                                        LR 0.000008    Time 0.064671    
2023-01-06 16:47:23,714 - Epoch: [162][   20/  246]    Overall Loss 0.250075    Objective Loss 0.250075                                        LR 0.000008    Time 0.038550    
2023-01-06 16:47:23,840 - Epoch: [162][   30/  246]    Overall Loss 0.246352    Objective Loss 0.246352                                        LR 0.000008    Time 0.029901    
2023-01-06 16:47:23,964 - Epoch: [162][   40/  246]    Overall Loss 0.243688    Objective Loss 0.243688                                        LR 0.000008    Time 0.025509    
2023-01-06 16:47:24,098 - Epoch: [162][   50/  246]    Overall Loss 0.246663    Objective Loss 0.246663                                        LR 0.000008    Time 0.023084    
2023-01-06 16:47:24,228 - Epoch: [162][   60/  246]    Overall Loss 0.244436    Objective Loss 0.244436                                        LR 0.000008    Time 0.021404    
2023-01-06 16:47:24,361 - Epoch: [162][   70/  246]    Overall Loss 0.244279    Objective Loss 0.244279                                        LR 0.000008    Time 0.020236    
2023-01-06 16:47:24,492 - Epoch: [162][   80/  246]    Overall Loss 0.246200    Objective Loss 0.246200                                        LR 0.000008    Time 0.019335    
2023-01-06 16:47:24,629 - Epoch: [162][   90/  246]    Overall Loss 0.246354    Objective Loss 0.246354                                        LR 0.000008    Time 0.018713    
2023-01-06 16:47:24,764 - Epoch: [162][  100/  246]    Overall Loss 0.244433    Objective Loss 0.244433                                        LR 0.000008    Time 0.018184    
2023-01-06 16:47:24,897 - Epoch: [162][  110/  246]    Overall Loss 0.245619    Objective Loss 0.245619                                        LR 0.000008    Time 0.017734    
2023-01-06 16:47:25,036 - Epoch: [162][  120/  246]    Overall Loss 0.245828    Objective Loss 0.245828                                        LR 0.000008    Time 0.017410    
2023-01-06 16:47:25,162 - Epoch: [162][  130/  246]    Overall Loss 0.246751    Objective Loss 0.246751                                        LR 0.000008    Time 0.017044    
2023-01-06 16:47:25,293 - Epoch: [162][  140/  246]    Overall Loss 0.246189    Objective Loss 0.246189                                        LR 0.000008    Time 0.016759    
2023-01-06 16:47:25,429 - Epoch: [162][  150/  246]    Overall Loss 0.246269    Objective Loss 0.246269                                        LR 0.000008    Time 0.016543    
2023-01-06 16:47:25,564 - Epoch: [162][  160/  246]    Overall Loss 0.246712    Objective Loss 0.246712                                        LR 0.000008    Time 0.016350    
2023-01-06 16:47:25,699 - Epoch: [162][  170/  246]    Overall Loss 0.246362    Objective Loss 0.246362                                        LR 0.000008    Time 0.016181    
2023-01-06 16:47:25,833 - Epoch: [162][  180/  246]    Overall Loss 0.246511    Objective Loss 0.246511                                        LR 0.000008    Time 0.016027    
2023-01-06 16:47:25,968 - Epoch: [162][  190/  246]    Overall Loss 0.246310    Objective Loss 0.246310                                        LR 0.000008    Time 0.015889    
2023-01-06 16:47:26,102 - Epoch: [162][  200/  246]    Overall Loss 0.246189    Objective Loss 0.246189                                        LR 0.000008    Time 0.015765    
2023-01-06 16:47:26,237 - Epoch: [162][  210/  246]    Overall Loss 0.246315    Objective Loss 0.246315                                        LR 0.000008    Time 0.015655    
2023-01-06 16:47:26,371 - Epoch: [162][  220/  246]    Overall Loss 0.246061    Objective Loss 0.246061                                        LR 0.000008    Time 0.015553    
2023-01-06 16:47:26,507 - Epoch: [162][  230/  246]    Overall Loss 0.246430    Objective Loss 0.246430                                        LR 0.000008    Time 0.015462    
2023-01-06 16:47:26,652 - Epoch: [162][  240/  246]    Overall Loss 0.246736    Objective Loss 0.246736                                        LR 0.000008    Time 0.015421    
2023-01-06 16:47:26,715 - Epoch: [162][  246/  246]    Overall Loss 0.246788    Objective Loss 0.246788    Top1 89.952153    LR 0.000008    Time 0.015302    
2023-01-06 16:47:26,861 - --- validate (epoch=162)-----------
2023-01-06 16:47:26,861 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:27,292 - Epoch: [162][   10/   28]    Loss 0.263032    Top1 90.546875    
2023-01-06 16:47:27,391 - Epoch: [162][   20/   28]    Loss 0.264735    Top1 90.234375    
2023-01-06 16:47:27,439 - Epoch: [162][   28/   28]    Loss 0.261126    Top1 90.609791    
2023-01-06 16:47:27,596 - ==> Top1: 90.610    Loss: 0.261

2023-01-06 16:47:27,597 - ==> Confusion:
[[ 265   15  159]
 [  19  287  296]
 [  84   83 5778]]

2023-01-06 16:47:27,598 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:27,598 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:27,603 - 

2023-01-06 16:47:27,603 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:28,262 - Epoch: [163][   10/  246]    Overall Loss 0.233966    Objective Loss 0.233966                                        LR 0.000008    Time 0.065817    
2023-01-06 16:47:28,394 - Epoch: [163][   20/  246]    Overall Loss 0.235324    Objective Loss 0.235324                                        LR 0.000008    Time 0.039484    
2023-01-06 16:47:28,528 - Epoch: [163][   30/  246]    Overall Loss 0.238021    Objective Loss 0.238021                                        LR 0.000008    Time 0.030772    
2023-01-06 16:47:28,673 - Epoch: [163][   40/  246]    Overall Loss 0.243447    Objective Loss 0.243447                                        LR 0.000008    Time 0.026706    
2023-01-06 16:47:28,826 - Epoch: [163][   50/  246]    Overall Loss 0.239499    Objective Loss 0.239499                                        LR 0.000008    Time 0.024405    
2023-01-06 16:47:28,967 - Epoch: [163][   60/  246]    Overall Loss 0.238094    Objective Loss 0.238094                                        LR 0.000008    Time 0.022692    
2023-01-06 16:47:29,105 - Epoch: [163][   70/  246]    Overall Loss 0.237891    Objective Loss 0.237891                                        LR 0.000008    Time 0.021407    
2023-01-06 16:47:29,251 - Epoch: [163][   80/  246]    Overall Loss 0.238760    Objective Loss 0.238760                                        LR 0.000008    Time 0.020559    
2023-01-06 16:47:29,393 - Epoch: [163][   90/  246]    Overall Loss 0.239406    Objective Loss 0.239406                                        LR 0.000008    Time 0.019847    
2023-01-06 16:47:29,541 - Epoch: [163][  100/  246]    Overall Loss 0.239855    Objective Loss 0.239855                                        LR 0.000008    Time 0.019335    
2023-01-06 16:47:29,681 - Epoch: [163][  110/  246]    Overall Loss 0.240870    Objective Loss 0.240870                                        LR 0.000008    Time 0.018849    
2023-01-06 16:47:29,821 - Epoch: [163][  120/  246]    Overall Loss 0.241980    Objective Loss 0.241980                                        LR 0.000008    Time 0.018441    
2023-01-06 16:47:29,962 - Epoch: [163][  130/  246]    Overall Loss 0.242021    Objective Loss 0.242021                                        LR 0.000008    Time 0.018089    
2023-01-06 16:47:30,104 - Epoch: [163][  140/  246]    Overall Loss 0.243202    Objective Loss 0.243202                                        LR 0.000008    Time 0.017808    
2023-01-06 16:47:30,246 - Epoch: [163][  150/  246]    Overall Loss 0.244820    Objective Loss 0.244820                                        LR 0.000008    Time 0.017570    
2023-01-06 16:47:30,394 - Epoch: [163][  160/  246]    Overall Loss 0.245638    Objective Loss 0.245638                                        LR 0.000008    Time 0.017393    
2023-01-06 16:47:30,532 - Epoch: [163][  170/  246]    Overall Loss 0.246029    Objective Loss 0.246029                                        LR 0.000008    Time 0.017175    
2023-01-06 16:47:30,676 - Epoch: [163][  180/  246]    Overall Loss 0.246097    Objective Loss 0.246097                                        LR 0.000008    Time 0.017019    
2023-01-06 16:47:30,815 - Epoch: [163][  190/  246]    Overall Loss 0.246720    Objective Loss 0.246720                                        LR 0.000008    Time 0.016854    
2023-01-06 16:47:30,962 - Epoch: [163][  200/  246]    Overall Loss 0.246594    Objective Loss 0.246594                                        LR 0.000008    Time 0.016747    
2023-01-06 16:47:31,098 - Epoch: [163][  210/  246]    Overall Loss 0.247249    Objective Loss 0.247249                                        LR 0.000008    Time 0.016592    
2023-01-06 16:47:31,244 - Epoch: [163][  220/  246]    Overall Loss 0.247214    Objective Loss 0.247214                                        LR 0.000008    Time 0.016501    
2023-01-06 16:47:31,381 - Epoch: [163][  230/  246]    Overall Loss 0.246884    Objective Loss 0.246884                                        LR 0.000008    Time 0.016379    
2023-01-06 16:47:31,531 - Epoch: [163][  240/  246]    Overall Loss 0.246976    Objective Loss 0.246976                                        LR 0.000008    Time 0.016321    
2023-01-06 16:47:31,595 - Epoch: [163][  246/  246]    Overall Loss 0.246303    Objective Loss 0.246303    Top1 91.866029    LR 0.000008    Time 0.016182    
2023-01-06 16:47:31,723 - --- validate (epoch=163)-----------
2023-01-06 16:47:31,723 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:32,151 - Epoch: [163][   10/   28]    Loss 0.289040    Top1 89.296875    
2023-01-06 16:47:32,256 - Epoch: [163][   20/   28]    Loss 0.273782    Top1 89.980469    
2023-01-06 16:47:32,305 - Epoch: [163][   28/   28]    Loss 0.264572    Top1 90.294875    
2023-01-06 16:47:32,438 - ==> Top1: 90.295    Loss: 0.265

2023-01-06 16:47:32,438 - ==> Confusion:
[[ 227   15  197]
 [  13  262  327]
 [  56   70 5819]]

2023-01-06 16:47:32,440 - ==> Best [Top1: 90.681   Sparsity:0.00   Params: 151104 on epoch: 155]
2023-01-06 16:47:32,440 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:32,445 - 

2023-01-06 16:47:32,445 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:32,985 - Epoch: [164][   10/  246]    Overall Loss 0.248936    Objective Loss 0.248936                                        LR 0.000008    Time 0.053961    
2023-01-06 16:47:33,144 - Epoch: [164][   20/  246]    Overall Loss 0.235441    Objective Loss 0.235441                                        LR 0.000008    Time 0.034887    
2023-01-06 16:47:33,291 - Epoch: [164][   30/  246]    Overall Loss 0.238345    Objective Loss 0.238345                                        LR 0.000008    Time 0.028161    
2023-01-06 16:47:33,461 - Epoch: [164][   40/  246]    Overall Loss 0.249095    Objective Loss 0.249095                                        LR 0.000008    Time 0.025347    
2023-01-06 16:47:33,609 - Epoch: [164][   50/  246]    Overall Loss 0.249251    Objective Loss 0.249251                                        LR 0.000008    Time 0.023220    
2023-01-06 16:47:33,771 - Epoch: [164][   60/  246]    Overall Loss 0.248899    Objective Loss 0.248899                                        LR 0.000008    Time 0.022050    
2023-01-06 16:47:33,951 - Epoch: [164][   70/  246]    Overall Loss 0.247589    Objective Loss 0.247589                                        LR 0.000008    Time 0.021460    
2023-01-06 16:47:34,143 - Epoch: [164][   80/  246]    Overall Loss 0.244419    Objective Loss 0.244419                                        LR 0.000008    Time 0.021178    
2023-01-06 16:47:34,320 - Epoch: [164][   90/  246]    Overall Loss 0.242339    Objective Loss 0.242339                                        LR 0.000008    Time 0.020782    
2023-01-06 16:47:34,515 - Epoch: [164][  100/  246]    Overall Loss 0.243391    Objective Loss 0.243391                                        LR 0.000008    Time 0.020646    
2023-01-06 16:47:34,698 - Epoch: [164][  110/  246]    Overall Loss 0.243159    Objective Loss 0.243159                                        LR 0.000008    Time 0.020434    
2023-01-06 16:47:34,886 - Epoch: [164][  120/  246]    Overall Loss 0.242457    Objective Loss 0.242457                                        LR 0.000008    Time 0.020294    
2023-01-06 16:47:35,065 - Epoch: [164][  130/  246]    Overall Loss 0.242492    Objective Loss 0.242492                                        LR 0.000008    Time 0.020101    
2023-01-06 16:47:35,247 - Epoch: [164][  140/  246]    Overall Loss 0.242835    Objective Loss 0.242835                                        LR 0.000008    Time 0.019967    
2023-01-06 16:47:35,424 - Epoch: [164][  150/  246]    Overall Loss 0.242401    Objective Loss 0.242401                                        LR 0.000008    Time 0.019810    
2023-01-06 16:47:35,576 - Epoch: [164][  160/  246]    Overall Loss 0.242411    Objective Loss 0.242411                                        LR 0.000008    Time 0.019518    
2023-01-06 16:47:35,728 - Epoch: [164][  170/  246]    Overall Loss 0.242546    Objective Loss 0.242546                                        LR 0.000008    Time 0.019263    
2023-01-06 16:47:35,868 - Epoch: [164][  180/  246]    Overall Loss 0.243189    Objective Loss 0.243189                                        LR 0.000008    Time 0.018969    
2023-01-06 16:47:36,007 - Epoch: [164][  190/  246]    Overall Loss 0.243815    Objective Loss 0.243815                                        LR 0.000008    Time 0.018700    
2023-01-06 16:47:36,157 - Epoch: [164][  200/  246]    Overall Loss 0.244921    Objective Loss 0.244921                                        LR 0.000008    Time 0.018512    
2023-01-06 16:47:36,309 - Epoch: [164][  210/  246]    Overall Loss 0.245224    Objective Loss 0.245224                                        LR 0.000008    Time 0.018350    
2023-01-06 16:47:36,457 - Epoch: [164][  220/  246]    Overall Loss 0.245220    Objective Loss 0.245220                                        LR 0.000008    Time 0.018191    
2023-01-06 16:47:36,610 - Epoch: [164][  230/  246]    Overall Loss 0.245483    Objective Loss 0.245483                                        LR 0.000008    Time 0.018061    
2023-01-06 16:47:36,766 - Epoch: [164][  240/  246]    Overall Loss 0.245192    Objective Loss 0.245192                                        LR 0.000008    Time 0.017955    
2023-01-06 16:47:36,824 - Epoch: [164][  246/  246]    Overall Loss 0.245247    Objective Loss 0.245247    Top1 90.669856    LR 0.000008    Time 0.017754    
2023-01-06 16:47:36,965 - --- validate (epoch=164)-----------
2023-01-06 16:47:36,965 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:37,407 - Epoch: [164][   10/   28]    Loss 0.258748    Top1 90.859375    
2023-01-06 16:47:37,513 - Epoch: [164][   20/   28]    Loss 0.258884    Top1 90.703125    
2023-01-06 16:47:37,565 - Epoch: [164][   28/   28]    Loss 0.260436    Top1 90.752934    
2023-01-06 16:47:37,713 - ==> Top1: 90.753    Loss: 0.260

2023-01-06 16:47:37,714 - ==> Confusion:
[[ 237   15  187]
 [  14  276  312]
 [  52   66 5827]]

2023-01-06 16:47:37,715 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:47:37,715 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:37,721 - 

2023-01-06 16:47:37,721 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:38,380 - Epoch: [165][   10/  246]    Overall Loss 0.237604    Objective Loss 0.237604                                        LR 0.000008    Time 0.065847    
2023-01-06 16:47:38,503 - Epoch: [165][   20/  246]    Overall Loss 0.243138    Objective Loss 0.243138                                        LR 0.000008    Time 0.039021    
2023-01-06 16:47:38,619 - Epoch: [165][   30/  246]    Overall Loss 0.243933    Objective Loss 0.243933                                        LR 0.000008    Time 0.029893    
2023-01-06 16:47:38,740 - Epoch: [165][   40/  246]    Overall Loss 0.244108    Objective Loss 0.244108                                        LR 0.000008    Time 0.025439    
2023-01-06 16:47:38,869 - Epoch: [165][   50/  246]    Overall Loss 0.244191    Objective Loss 0.244191                                        LR 0.000008    Time 0.022922    
2023-01-06 16:47:38,999 - Epoch: [165][   60/  246]    Overall Loss 0.241984    Objective Loss 0.241984                                        LR 0.000008    Time 0.021261    
2023-01-06 16:47:39,129 - Epoch: [165][   70/  246]    Overall Loss 0.240124    Objective Loss 0.240124                                        LR 0.000008    Time 0.020079    
2023-01-06 16:47:39,268 - Epoch: [165][   80/  246]    Overall Loss 0.241258    Objective Loss 0.241258                                        LR 0.000008    Time 0.019291    
2023-01-06 16:47:39,407 - Epoch: [165][   90/  246]    Overall Loss 0.241751    Objective Loss 0.241751                                        LR 0.000008    Time 0.018692    
2023-01-06 16:47:39,543 - Epoch: [165][  100/  246]    Overall Loss 0.243181    Objective Loss 0.243181                                        LR 0.000008    Time 0.018177    
2023-01-06 16:47:39,679 - Epoch: [165][  110/  246]    Overall Loss 0.243457    Objective Loss 0.243457                                        LR 0.000008    Time 0.017759    
2023-01-06 16:47:39,808 - Epoch: [165][  120/  246]    Overall Loss 0.243398    Objective Loss 0.243398                                        LR 0.000008    Time 0.017358    
2023-01-06 16:47:39,948 - Epoch: [165][  130/  246]    Overall Loss 0.243597    Objective Loss 0.243597                                        LR 0.000008    Time 0.017093    
2023-01-06 16:47:40,084 - Epoch: [165][  140/  246]    Overall Loss 0.244345    Objective Loss 0.244345                                        LR 0.000008    Time 0.016841    
2023-01-06 16:47:40,213 - Epoch: [165][  150/  246]    Overall Loss 0.245570    Objective Loss 0.245570                                        LR 0.000008    Time 0.016573    
2023-01-06 16:47:40,330 - Epoch: [165][  160/  246]    Overall Loss 0.245360    Objective Loss 0.245360                                        LR 0.000008    Time 0.016268    
2023-01-06 16:47:40,466 - Epoch: [165][  170/  246]    Overall Loss 0.245372    Objective Loss 0.245372                                        LR 0.000008    Time 0.016109    
2023-01-06 16:47:40,596 - Epoch: [165][  180/  246]    Overall Loss 0.244485    Objective Loss 0.244485                                        LR 0.000008    Time 0.015931    
2023-01-06 16:47:40,735 - Epoch: [165][  190/  246]    Overall Loss 0.243271    Objective Loss 0.243271                                        LR 0.000008    Time 0.015822    
2023-01-06 16:47:40,868 - Epoch: [165][  200/  246]    Overall Loss 0.244130    Objective Loss 0.244130                                        LR 0.000008    Time 0.015696    
2023-01-06 16:47:40,989 - Epoch: [165][  210/  246]    Overall Loss 0.245411    Objective Loss 0.245411                                        LR 0.000008    Time 0.015525    
2023-01-06 16:47:41,110 - Epoch: [165][  220/  246]    Overall Loss 0.245684    Objective Loss 0.245684                                        LR 0.000008    Time 0.015364    
2023-01-06 16:47:41,242 - Epoch: [165][  230/  246]    Overall Loss 0.246098    Objective Loss 0.246098                                        LR 0.000008    Time 0.015272    
2023-01-06 16:47:41,388 - Epoch: [165][  240/  246]    Overall Loss 0.246297    Objective Loss 0.246297                                        LR 0.000008    Time 0.015241    
2023-01-06 16:47:41,449 - Epoch: [165][  246/  246]    Overall Loss 0.246871    Objective Loss 0.246871    Top1 90.191388    LR 0.000008    Time 0.015117    
2023-01-06 16:47:41,574 - --- validate (epoch=165)-----------
2023-01-06 16:47:41,574 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:41,999 - Epoch: [165][   10/   28]    Loss 0.277714    Top1 90.156250    
2023-01-06 16:47:42,096 - Epoch: [165][   20/   28]    Loss 0.268932    Top1 90.371094    
2023-01-06 16:47:42,146 - Epoch: [165][   28/   28]    Loss 0.265931    Top1 90.438019    
2023-01-06 16:47:42,307 - ==> Top1: 90.438    Loss: 0.266

2023-01-06 16:47:42,307 - ==> Confusion:
[[ 224   14  201]
 [  13  258  331]
 [  49   60 5836]]

2023-01-06 16:47:42,308 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:47:42,308 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:42,313 - 

2023-01-06 16:47:42,313 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:42,826 - Epoch: [166][   10/  246]    Overall Loss 0.249514    Objective Loss 0.249514                                        LR 0.000008    Time 0.051174    
2023-01-06 16:47:42,945 - Epoch: [166][   20/  246]    Overall Loss 0.250780    Objective Loss 0.250780                                        LR 0.000008    Time 0.031526    
2023-01-06 16:47:43,066 - Epoch: [166][   30/  246]    Overall Loss 0.253960    Objective Loss 0.253960                                        LR 0.000008    Time 0.025023    
2023-01-06 16:47:43,209 - Epoch: [166][   40/  246]    Overall Loss 0.253992    Objective Loss 0.253992                                        LR 0.000008    Time 0.022350    
2023-01-06 16:47:43,356 - Epoch: [166][   50/  246]    Overall Loss 0.254798    Objective Loss 0.254798                                        LR 0.000008    Time 0.020798    
2023-01-06 16:47:43,502 - Epoch: [166][   60/  246]    Overall Loss 0.252556    Objective Loss 0.252556                                        LR 0.000008    Time 0.019769    
2023-01-06 16:47:43,652 - Epoch: [166][   70/  246]    Overall Loss 0.247974    Objective Loss 0.247974                                        LR 0.000008    Time 0.019082    
2023-01-06 16:47:43,800 - Epoch: [166][   80/  246]    Overall Loss 0.247138    Objective Loss 0.247138                                        LR 0.000008    Time 0.018534    
2023-01-06 16:47:43,924 - Epoch: [166][   90/  246]    Overall Loss 0.246728    Objective Loss 0.246728                                        LR 0.000008    Time 0.017852    
2023-01-06 16:47:44,066 - Epoch: [166][  100/  246]    Overall Loss 0.247673    Objective Loss 0.247673                                        LR 0.000008    Time 0.017457    
2023-01-06 16:47:44,210 - Epoch: [166][  110/  246]    Overall Loss 0.246808    Objective Loss 0.246808                                        LR 0.000008    Time 0.017177    
2023-01-06 16:47:44,354 - Epoch: [166][  120/  246]    Overall Loss 0.245564    Objective Loss 0.245564                                        LR 0.000008    Time 0.016947    
2023-01-06 16:47:44,493 - Epoch: [166][  130/  246]    Overall Loss 0.245319    Objective Loss 0.245319                                        LR 0.000008    Time 0.016702    
2023-01-06 16:47:44,629 - Epoch: [166][  140/  246]    Overall Loss 0.245343    Objective Loss 0.245343                                        LR 0.000008    Time 0.016479    
2023-01-06 16:47:44,754 - Epoch: [166][  150/  246]    Overall Loss 0.244587    Objective Loss 0.244587                                        LR 0.000008    Time 0.016210    
2023-01-06 16:47:44,896 - Epoch: [166][  160/  246]    Overall Loss 0.244553    Objective Loss 0.244553                                        LR 0.000008    Time 0.016086    
2023-01-06 16:47:45,039 - Epoch: [166][  170/  246]    Overall Loss 0.245470    Objective Loss 0.245470                                        LR 0.000008    Time 0.015978    
2023-01-06 16:47:45,186 - Epoch: [166][  180/  246]    Overall Loss 0.245946    Objective Loss 0.245946                                        LR 0.000008    Time 0.015904    
2023-01-06 16:47:45,332 - Epoch: [166][  190/  246]    Overall Loss 0.245128    Objective Loss 0.245128                                        LR 0.000008    Time 0.015830    
2023-01-06 16:47:45,475 - Epoch: [166][  200/  246]    Overall Loss 0.245489    Objective Loss 0.245489                                        LR 0.000008    Time 0.015752    
2023-01-06 16:47:45,622 - Epoch: [166][  210/  246]    Overall Loss 0.246020    Objective Loss 0.246020                                        LR 0.000008    Time 0.015703    
2023-01-06 16:47:45,768 - Epoch: [166][  220/  246]    Overall Loss 0.245559    Objective Loss 0.245559                                        LR 0.000008    Time 0.015651    
2023-01-06 16:47:45,911 - Epoch: [166][  230/  246]    Overall Loss 0.245560    Objective Loss 0.245560                                        LR 0.000008    Time 0.015591    
2023-01-06 16:47:46,071 - Epoch: [166][  240/  246]    Overall Loss 0.245982    Objective Loss 0.245982                                        LR 0.000008    Time 0.015598    
2023-01-06 16:47:46,137 - Epoch: [166][  246/  246]    Overall Loss 0.245867    Objective Loss 0.245867    Top1 90.669856    LR 0.000008    Time 0.015485    
2023-01-06 16:47:46,252 - --- validate (epoch=166)-----------
2023-01-06 16:47:46,253 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:46,673 - Epoch: [166][   10/   28]    Loss 0.251596    Top1 91.171875    
2023-01-06 16:47:46,774 - Epoch: [166][   20/   28]    Loss 0.263006    Top1 90.761719    
2023-01-06 16:47:46,826 - Epoch: [166][   28/   28]    Loss 0.260901    Top1 90.724306    
2023-01-06 16:47:46,987 - ==> Top1: 90.724    Loss: 0.261

2023-01-06 16:47:46,987 - ==> Confusion:
[[ 249   14  176]
 [  16  286  300]
 [  61   81 5803]]

2023-01-06 16:47:46,988 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:47:46,988 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:46,993 - 

2023-01-06 16:47:46,994 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:47,653 - Epoch: [167][   10/  246]    Overall Loss 0.250777    Objective Loss 0.250777                                        LR 0.000008    Time 0.065933    
2023-01-06 16:47:47,799 - Epoch: [167][   20/  246]    Overall Loss 0.242384    Objective Loss 0.242384                                        LR 0.000008    Time 0.040240    
2023-01-06 16:47:47,945 - Epoch: [167][   30/  246]    Overall Loss 0.245909    Objective Loss 0.245909                                        LR 0.000008    Time 0.031673    
2023-01-06 16:47:48,091 - Epoch: [167][   40/  246]    Overall Loss 0.246152    Objective Loss 0.246152                                        LR 0.000008    Time 0.027395    
2023-01-06 16:47:48,240 - Epoch: [167][   50/  246]    Overall Loss 0.245880    Objective Loss 0.245880                                        LR 0.000008    Time 0.024840    
2023-01-06 16:47:48,389 - Epoch: [167][   60/  246]    Overall Loss 0.245216    Objective Loss 0.245216                                        LR 0.000008    Time 0.023175    
2023-01-06 16:47:48,539 - Epoch: [167][   70/  246]    Overall Loss 0.245652    Objective Loss 0.245652                                        LR 0.000008    Time 0.022011    
2023-01-06 16:47:48,692 - Epoch: [167][   80/  246]    Overall Loss 0.246723    Objective Loss 0.246723                                        LR 0.000008    Time 0.021167    
2023-01-06 16:47:48,841 - Epoch: [167][   90/  246]    Overall Loss 0.245550    Objective Loss 0.245550                                        LR 0.000008    Time 0.020460    
2023-01-06 16:47:48,994 - Epoch: [167][  100/  246]    Overall Loss 0.243340    Objective Loss 0.243340                                        LR 0.000008    Time 0.019940    
2023-01-06 16:47:49,144 - Epoch: [167][  110/  246]    Overall Loss 0.243647    Objective Loss 0.243647                                        LR 0.000008    Time 0.019486    
2023-01-06 16:47:49,299 - Epoch: [167][  120/  246]    Overall Loss 0.244677    Objective Loss 0.244677                                        LR 0.000008    Time 0.019154    
2023-01-06 16:47:49,455 - Epoch: [167][  130/  246]    Overall Loss 0.245695    Objective Loss 0.245695                                        LR 0.000008    Time 0.018873    
2023-01-06 16:47:49,606 - Epoch: [167][  140/  246]    Overall Loss 0.245771    Objective Loss 0.245771                                        LR 0.000008    Time 0.018603    
2023-01-06 16:47:49,756 - Epoch: [167][  150/  246]    Overall Loss 0.245120    Objective Loss 0.245120                                        LR 0.000008    Time 0.018356    
2023-01-06 16:47:49,912 - Epoch: [167][  160/  246]    Overall Loss 0.243956    Objective Loss 0.243956                                        LR 0.000008    Time 0.018181    
2023-01-06 16:47:50,054 - Epoch: [167][  170/  246]    Overall Loss 0.243507    Objective Loss 0.243507                                        LR 0.000008    Time 0.017948    
2023-01-06 16:47:50,185 - Epoch: [167][  180/  246]    Overall Loss 0.243160    Objective Loss 0.243160                                        LR 0.000008    Time 0.017673    
2023-01-06 16:47:50,321 - Epoch: [167][  190/  246]    Overall Loss 0.243549    Objective Loss 0.243549                                        LR 0.000008    Time 0.017456    
2023-01-06 16:47:50,455 - Epoch: [167][  200/  246]    Overall Loss 0.243743    Objective Loss 0.243743                                        LR 0.000008    Time 0.017253    
2023-01-06 16:47:50,594 - Epoch: [167][  210/  246]    Overall Loss 0.243464    Objective Loss 0.243464                                        LR 0.000008    Time 0.017081    
2023-01-06 16:47:50,728 - Epoch: [167][  220/  246]    Overall Loss 0.243255    Objective Loss 0.243255                                        LR 0.000008    Time 0.016915    
2023-01-06 16:47:50,865 - Epoch: [167][  230/  246]    Overall Loss 0.243718    Objective Loss 0.243718                                        LR 0.000008    Time 0.016772    
2023-01-06 16:47:51,012 - Epoch: [167][  240/  246]    Overall Loss 0.244609    Objective Loss 0.244609                                        LR 0.000008    Time 0.016683    
2023-01-06 16:47:51,070 - Epoch: [167][  246/  246]    Overall Loss 0.245056    Objective Loss 0.245056    Top1 88.038278    LR 0.000008    Time 0.016510    
2023-01-06 16:47:51,227 - --- validate (epoch=167)-----------
2023-01-06 16:47:51,227 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:51,658 - Epoch: [167][   10/   28]    Loss 0.261761    Top1 90.781250    
2023-01-06 16:47:51,766 - Epoch: [167][   20/   28]    Loss 0.263090    Top1 90.781250    
2023-01-06 16:47:51,816 - Epoch: [167][   28/   28]    Loss 0.261642    Top1 90.724306    
2023-01-06 16:47:51,977 - ==> Top1: 90.724    Loss: 0.262

2023-01-06 16:47:51,977 - ==> Confusion:
[[ 236   14  189]
 [  17  276  309]
 [  49   70 5826]]

2023-01-06 16:47:51,978 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:47:51,978 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:51,984 - 

2023-01-06 16:47:51,984 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:52,494 - Epoch: [168][   10/  246]    Overall Loss 0.257517    Objective Loss 0.257517                                        LR 0.000008    Time 0.050916    
2023-01-06 16:47:52,622 - Epoch: [168][   20/  246]    Overall Loss 0.245570    Objective Loss 0.245570                                        LR 0.000008    Time 0.031884    
2023-01-06 16:47:52,746 - Epoch: [168][   30/  246]    Overall Loss 0.245750    Objective Loss 0.245750                                        LR 0.000008    Time 0.025354    
2023-01-06 16:47:52,878 - Epoch: [168][   40/  246]    Overall Loss 0.246685    Objective Loss 0.246685                                        LR 0.000008    Time 0.022297    
2023-01-06 16:47:53,000 - Epoch: [168][   50/  246]    Overall Loss 0.246368    Objective Loss 0.246368                                        LR 0.000008    Time 0.020285    
2023-01-06 16:47:53,125 - Epoch: [168][   60/  246]    Overall Loss 0.243094    Objective Loss 0.243094                                        LR 0.000008    Time 0.018980    
2023-01-06 16:47:53,258 - Epoch: [168][   70/  246]    Overall Loss 0.240781    Objective Loss 0.240781                                        LR 0.000008    Time 0.018153    
2023-01-06 16:47:53,386 - Epoch: [168][   80/  246]    Overall Loss 0.241618    Objective Loss 0.241618                                        LR 0.000008    Time 0.017488    
2023-01-06 16:47:53,518 - Epoch: [168][   90/  246]    Overall Loss 0.241764    Objective Loss 0.241764                                        LR 0.000008    Time 0.017008    
2023-01-06 16:47:53,647 - Epoch: [168][  100/  246]    Overall Loss 0.241948    Objective Loss 0.241948                                        LR 0.000008    Time 0.016575    
2023-01-06 16:47:53,785 - Epoch: [168][  110/  246]    Overall Loss 0.242940    Objective Loss 0.242940                                        LR 0.000008    Time 0.016315    
2023-01-06 16:47:53,931 - Epoch: [168][  120/  246]    Overall Loss 0.242347    Objective Loss 0.242347                                        LR 0.000008    Time 0.016166    
2023-01-06 16:47:54,066 - Epoch: [168][  130/  246]    Overall Loss 0.240255    Objective Loss 0.240255                                        LR 0.000008    Time 0.015962    
2023-01-06 16:47:54,198 - Epoch: [168][  140/  246]    Overall Loss 0.240145    Objective Loss 0.240145                                        LR 0.000008    Time 0.015752    
2023-01-06 16:47:54,334 - Epoch: [168][  150/  246]    Overall Loss 0.240418    Objective Loss 0.240418                                        LR 0.000008    Time 0.015602    
2023-01-06 16:47:54,464 - Epoch: [168][  160/  246]    Overall Loss 0.242207    Objective Loss 0.242207                                        LR 0.000008    Time 0.015440    
2023-01-06 16:47:54,597 - Epoch: [168][  170/  246]    Overall Loss 0.242623    Objective Loss 0.242623                                        LR 0.000008    Time 0.015308    
2023-01-06 16:47:54,728 - Epoch: [168][  180/  246]    Overall Loss 0.243085    Objective Loss 0.243085                                        LR 0.000008    Time 0.015182    
2023-01-06 16:47:54,859 - Epoch: [168][  190/  246]    Overall Loss 0.242730    Objective Loss 0.242730                                        LR 0.000008    Time 0.015073    
2023-01-06 16:47:54,997 - Epoch: [168][  200/  246]    Overall Loss 0.242860    Objective Loss 0.242860                                        LR 0.000008    Time 0.015008    
2023-01-06 16:47:55,132 - Epoch: [168][  210/  246]    Overall Loss 0.244095    Objective Loss 0.244095                                        LR 0.000008    Time 0.014932    
2023-01-06 16:47:55,266 - Epoch: [168][  220/  246]    Overall Loss 0.244495    Objective Loss 0.244495                                        LR 0.000008    Time 0.014851    
2023-01-06 16:47:55,404 - Epoch: [168][  230/  246]    Overall Loss 0.245337    Objective Loss 0.245337                                        LR 0.000008    Time 0.014807    
2023-01-06 16:47:55,548 - Epoch: [168][  240/  246]    Overall Loss 0.245528    Objective Loss 0.245528                                        LR 0.000008    Time 0.014789    
2023-01-06 16:47:55,607 - Epoch: [168][  246/  246]    Overall Loss 0.245967    Objective Loss 0.245967    Top1 89.712919    LR 0.000008    Time 0.014664    
2023-01-06 16:47:55,739 - --- validate (epoch=168)-----------
2023-01-06 16:47:55,739 - 6986 samples (256 per mini-batch)
2023-01-06 16:47:56,162 - Epoch: [168][   10/   28]    Loss 0.239232    Top1 91.914062    
2023-01-06 16:47:56,264 - Epoch: [168][   20/   28]    Loss 0.255236    Top1 90.839844    
2023-01-06 16:47:56,312 - Epoch: [168][   28/   28]    Loss 0.257904    Top1 90.609791    
2023-01-06 16:47:56,471 - ==> Top1: 90.610    Loss: 0.258

2023-01-06 16:47:56,471 - ==> Confusion:
[[ 256   15  168]
 [  18  281  303]
 [  72   80 5793]]

2023-01-06 16:47:56,472 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:47:56,472 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:47:56,477 - 

2023-01-06 16:47:56,478 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:47:57,138 - Epoch: [169][   10/  246]    Overall Loss 0.244947    Objective Loss 0.244947                                        LR 0.000008    Time 0.065957    
2023-01-06 16:47:57,280 - Epoch: [169][   20/  246]    Overall Loss 0.244379    Objective Loss 0.244379                                        LR 0.000008    Time 0.040080    
2023-01-06 16:47:57,436 - Epoch: [169][   30/  246]    Overall Loss 0.241555    Objective Loss 0.241555                                        LR 0.000008    Time 0.031892    
2023-01-06 16:47:57,589 - Epoch: [169][   40/  246]    Overall Loss 0.246059    Objective Loss 0.246059                                        LR 0.000008    Time 0.027745    
2023-01-06 16:47:57,745 - Epoch: [169][   50/  246]    Overall Loss 0.245341    Objective Loss 0.245341                                        LR 0.000008    Time 0.025295    
2023-01-06 16:47:57,901 - Epoch: [169][   60/  246]    Overall Loss 0.248038    Objective Loss 0.248038                                        LR 0.000008    Time 0.023675    
2023-01-06 16:47:58,057 - Epoch: [169][   70/  246]    Overall Loss 0.247747    Objective Loss 0.247747                                        LR 0.000008    Time 0.022521    
2023-01-06 16:47:58,214 - Epoch: [169][   80/  246]    Overall Loss 0.247498    Objective Loss 0.247498                                        LR 0.000008    Time 0.021661    
2023-01-06 16:47:58,371 - Epoch: [169][   90/  246]    Overall Loss 0.247447    Objective Loss 0.247447                                        LR 0.000008    Time 0.020994    
2023-01-06 16:47:58,525 - Epoch: [169][  100/  246]    Overall Loss 0.246403    Objective Loss 0.246403                                        LR 0.000008    Time 0.020430    
2023-01-06 16:47:58,679 - Epoch: [169][  110/  246]    Overall Loss 0.245730    Objective Loss 0.245730                                        LR 0.000008    Time 0.019971    
2023-01-06 16:47:58,828 - Epoch: [169][  120/  246]    Overall Loss 0.244930    Objective Loss 0.244930                                        LR 0.000008    Time 0.019539    
2023-01-06 16:47:58,972 - Epoch: [169][  130/  246]    Overall Loss 0.245151    Objective Loss 0.245151                                        LR 0.000008    Time 0.019141    
2023-01-06 16:47:59,110 - Epoch: [169][  140/  246]    Overall Loss 0.244260    Objective Loss 0.244260                                        LR 0.000008    Time 0.018755    
2023-01-06 16:47:59,244 - Epoch: [169][  150/  246]    Overall Loss 0.245421    Objective Loss 0.245421                                        LR 0.000008    Time 0.018400    
2023-01-06 16:47:59,378 - Epoch: [169][  160/  246]    Overall Loss 0.246180    Objective Loss 0.246180                                        LR 0.000008    Time 0.018082    
2023-01-06 16:47:59,513 - Epoch: [169][  170/  246]    Overall Loss 0.245721    Objective Loss 0.245721                                        LR 0.000008    Time 0.017806    
2023-01-06 16:47:59,652 - Epoch: [169][  180/  246]    Overall Loss 0.245769    Objective Loss 0.245769                                        LR 0.000008    Time 0.017588    
2023-01-06 16:47:59,794 - Epoch: [169][  190/  246]    Overall Loss 0.245405    Objective Loss 0.245405                                        LR 0.000008    Time 0.017409    
2023-01-06 16:47:59,937 - Epoch: [169][  200/  246]    Overall Loss 0.246125    Objective Loss 0.246125                                        LR 0.000008    Time 0.017253    
2023-01-06 16:48:00,085 - Epoch: [169][  210/  246]    Overall Loss 0.244763    Objective Loss 0.244763                                        LR 0.000008    Time 0.017132    
2023-01-06 16:48:00,230 - Epoch: [169][  220/  246]    Overall Loss 0.244519    Objective Loss 0.244519                                        LR 0.000008    Time 0.017012    
2023-01-06 16:48:00,374 - Epoch: [169][  230/  246]    Overall Loss 0.244571    Objective Loss 0.244571                                        LR 0.000008    Time 0.016893    
2023-01-06 16:48:00,533 - Epoch: [169][  240/  246]    Overall Loss 0.244878    Objective Loss 0.244878                                        LR 0.000008    Time 0.016851    
2023-01-06 16:48:00,600 - Epoch: [169][  246/  246]    Overall Loss 0.244868    Objective Loss 0.244868    Top1 90.430622    LR 0.000008    Time 0.016713    
2023-01-06 16:48:00,733 - --- validate (epoch=169)-----------
2023-01-06 16:48:00,734 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:01,153 - Epoch: [169][   10/   28]    Loss 0.260499    Top1 90.742188    
2023-01-06 16:48:01,251 - Epoch: [169][   20/   28]    Loss 0.259319    Top1 90.761719    
2023-01-06 16:48:01,302 - Epoch: [169][   28/   28]    Loss 0.258957    Top1 90.667048    
2023-01-06 16:48:01,464 - ==> Top1: 90.667    Loss: 0.259

2023-01-06 16:48:01,464 - ==> Confusion:
[[ 239   13  187]
 [  16  271  315]
 [  53   68 5824]]

2023-01-06 16:48:01,466 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:01,466 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:01,471 - 

2023-01-06 16:48:01,471 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:02,145 - Epoch: [170][   10/  246]    Overall Loss 0.236220    Objective Loss 0.236220                                        LR 0.000008    Time 0.067373    
2023-01-06 16:48:02,278 - Epoch: [170][   20/  246]    Overall Loss 0.233885    Objective Loss 0.233885                                        LR 0.000008    Time 0.040306    
2023-01-06 16:48:02,411 - Epoch: [170][   30/  246]    Overall Loss 0.237030    Objective Loss 0.237030                                        LR 0.000008    Time 0.031274    
2023-01-06 16:48:02,535 - Epoch: [170][   40/  246]    Overall Loss 0.242721    Objective Loss 0.242721                                        LR 0.000008    Time 0.026551    
2023-01-06 16:48:02,665 - Epoch: [170][   50/  246]    Overall Loss 0.248776    Objective Loss 0.248776                                        LR 0.000008    Time 0.023836    
2023-01-06 16:48:02,794 - Epoch: [170][   60/  246]    Overall Loss 0.248628    Objective Loss 0.248628                                        LR 0.000008    Time 0.022008    
2023-01-06 16:48:02,916 - Epoch: [170][   70/  246]    Overall Loss 0.247864    Objective Loss 0.247864                                        LR 0.000008    Time 0.020594    
2023-01-06 16:48:03,040 - Epoch: [170][   80/  246]    Overall Loss 0.248224    Objective Loss 0.248224                                        LR 0.000008    Time 0.019543    
2023-01-06 16:48:03,161 - Epoch: [170][   90/  246]    Overall Loss 0.247596    Objective Loss 0.247596                                        LR 0.000008    Time 0.018722    
2023-01-06 16:48:03,283 - Epoch: [170][  100/  246]    Overall Loss 0.247545    Objective Loss 0.247545                                        LR 0.000008    Time 0.018064    
2023-01-06 16:48:03,405 - Epoch: [170][  110/  246]    Overall Loss 0.246419    Objective Loss 0.246419                                        LR 0.000008    Time 0.017527    
2023-01-06 16:48:03,528 - Epoch: [170][  120/  246]    Overall Loss 0.246421    Objective Loss 0.246421                                        LR 0.000008    Time 0.017085    
2023-01-06 16:48:03,649 - Epoch: [170][  130/  246]    Overall Loss 0.246768    Objective Loss 0.246768                                        LR 0.000008    Time 0.016696    
2023-01-06 16:48:03,772 - Epoch: [170][  140/  246]    Overall Loss 0.244428    Objective Loss 0.244428                                        LR 0.000008    Time 0.016383    
2023-01-06 16:48:03,897 - Epoch: [170][  150/  246]    Overall Loss 0.245762    Objective Loss 0.245762                                        LR 0.000008    Time 0.016118    
2023-01-06 16:48:04,018 - Epoch: [170][  160/  246]    Overall Loss 0.246318    Objective Loss 0.246318                                        LR 0.000008    Time 0.015867    
2023-01-06 16:48:04,135 - Epoch: [170][  170/  246]    Overall Loss 0.244837    Objective Loss 0.244837                                        LR 0.000008    Time 0.015616    
2023-01-06 16:48:04,253 - Epoch: [170][  180/  246]    Overall Loss 0.245261    Objective Loss 0.245261                                        LR 0.000008    Time 0.015402    
2023-01-06 16:48:04,368 - Epoch: [170][  190/  246]    Overall Loss 0.244932    Objective Loss 0.244932                                        LR 0.000008    Time 0.015197    
2023-01-06 16:48:04,485 - Epoch: [170][  200/  246]    Overall Loss 0.245152    Objective Loss 0.245152                                        LR 0.000008    Time 0.015020    
2023-01-06 16:48:04,603 - Epoch: [170][  210/  246]    Overall Loss 0.245337    Objective Loss 0.245337                                        LR 0.000008    Time 0.014863    
2023-01-06 16:48:04,720 - Epoch: [170][  220/  246]    Overall Loss 0.245593    Objective Loss 0.245593                                        LR 0.000008    Time 0.014718    
2023-01-06 16:48:04,836 - Epoch: [170][  230/  246]    Overall Loss 0.245288    Objective Loss 0.245288                                        LR 0.000008    Time 0.014581    
2023-01-06 16:48:04,976 - Epoch: [170][  240/  246]    Overall Loss 0.245280    Objective Loss 0.245280                                        LR 0.000008    Time 0.014557    
2023-01-06 16:48:05,040 - Epoch: [170][  246/  246]    Overall Loss 0.245329    Objective Loss 0.245329    Top1 91.148325    LR 0.000008    Time 0.014461    
2023-01-06 16:48:05,188 - --- validate (epoch=170)-----------
2023-01-06 16:48:05,188 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:05,612 - Epoch: [170][   10/   28]    Loss 0.265306    Top1 90.585938    
2023-01-06 16:48:05,709 - Epoch: [170][   20/   28]    Loss 0.263421    Top1 90.351562    
2023-01-06 16:48:05,762 - Epoch: [170][   28/   28]    Loss 0.261546    Top1 90.438019    
2023-01-06 16:48:05,887 - ==> Top1: 90.438    Loss: 0.262

2023-01-06 16:48:05,887 - ==> Confusion:
[[ 240   16  183]
 [  15  275  312]
 [  59   83 5803]]

2023-01-06 16:48:05,889 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:05,889 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:05,894 - 

2023-01-06 16:48:05,894 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:06,417 - Epoch: [171][   10/  246]    Overall Loss 0.241832    Objective Loss 0.241832                                        LR 0.000008    Time 0.052270    
2023-01-06 16:48:06,550 - Epoch: [171][   20/  246]    Overall Loss 0.251824    Objective Loss 0.251824                                        LR 0.000008    Time 0.032767    
2023-01-06 16:48:06,685 - Epoch: [171][   30/  246]    Overall Loss 0.249996    Objective Loss 0.249996                                        LR 0.000008    Time 0.026322    
2023-01-06 16:48:06,831 - Epoch: [171][   40/  246]    Overall Loss 0.250706    Objective Loss 0.250706                                        LR 0.000008    Time 0.023389    
2023-01-06 16:48:06,981 - Epoch: [171][   50/  246]    Overall Loss 0.248231    Objective Loss 0.248231                                        LR 0.000008    Time 0.021701    
2023-01-06 16:48:07,115 - Epoch: [171][   60/  246]    Overall Loss 0.248996    Objective Loss 0.248996                                        LR 0.000008    Time 0.020310    
2023-01-06 16:48:07,260 - Epoch: [171][   70/  246]    Overall Loss 0.250896    Objective Loss 0.250896                                        LR 0.000008    Time 0.019466    
2023-01-06 16:48:07,402 - Epoch: [171][   80/  246]    Overall Loss 0.250184    Objective Loss 0.250184                                        LR 0.000008    Time 0.018809    
2023-01-06 16:48:07,546 - Epoch: [171][   90/  246]    Overall Loss 0.250578    Objective Loss 0.250578                                        LR 0.000008    Time 0.018313    
2023-01-06 16:48:07,683 - Epoch: [171][  100/  246]    Overall Loss 0.248687    Objective Loss 0.248687                                        LR 0.000008    Time 0.017847    
2023-01-06 16:48:07,830 - Epoch: [171][  110/  246]    Overall Loss 0.248802    Objective Loss 0.248802                                        LR 0.000008    Time 0.017553    
2023-01-06 16:48:07,970 - Epoch: [171][  120/  246]    Overall Loss 0.247979    Objective Loss 0.247979                                        LR 0.000008    Time 0.017253    
2023-01-06 16:48:08,110 - Epoch: [171][  130/  246]    Overall Loss 0.248158    Objective Loss 0.248158                                        LR 0.000008    Time 0.016998    
2023-01-06 16:48:08,234 - Epoch: [171][  140/  246]    Overall Loss 0.246236    Objective Loss 0.246236                                        LR 0.000008    Time 0.016658    
2023-01-06 16:48:08,359 - Epoch: [171][  150/  246]    Overall Loss 0.245829    Objective Loss 0.245829                                        LR 0.000008    Time 0.016378    
2023-01-06 16:48:08,484 - Epoch: [171][  160/  246]    Overall Loss 0.247135    Objective Loss 0.247135                                        LR 0.000008    Time 0.016134    
2023-01-06 16:48:08,611 - Epoch: [171][  170/  246]    Overall Loss 0.247070    Objective Loss 0.247070                                        LR 0.000008    Time 0.015926    
2023-01-06 16:48:08,737 - Epoch: [171][  180/  246]    Overall Loss 0.247813    Objective Loss 0.247813                                        LR 0.000008    Time 0.015741    
2023-01-06 16:48:08,861 - Epoch: [171][  190/  246]    Overall Loss 0.248132    Objective Loss 0.248132                                        LR 0.000008    Time 0.015566    
2023-01-06 16:48:08,987 - Epoch: [171][  200/  246]    Overall Loss 0.249240    Objective Loss 0.249240                                        LR 0.000008    Time 0.015413    
2023-01-06 16:48:09,110 - Epoch: [171][  210/  246]    Overall Loss 0.248109    Objective Loss 0.248109                                        LR 0.000008    Time 0.015265    
2023-01-06 16:48:09,236 - Epoch: [171][  220/  246]    Overall Loss 0.247696    Objective Loss 0.247696                                        LR 0.000008    Time 0.015139    
2023-01-06 16:48:09,362 - Epoch: [171][  230/  246]    Overall Loss 0.246748    Objective Loss 0.246748                                        LR 0.000008    Time 0.015029    
2023-01-06 16:48:09,501 - Epoch: [171][  240/  246]    Overall Loss 0.246767    Objective Loss 0.246767                                        LR 0.000008    Time 0.014980    
2023-01-06 16:48:09,558 - Epoch: [171][  246/  246]    Overall Loss 0.246121    Objective Loss 0.246121    Top1 91.626794    LR 0.000008    Time 0.014846    
2023-01-06 16:48:09,697 - --- validate (epoch=171)-----------
2023-01-06 16:48:09,698 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:10,140 - Epoch: [171][   10/   28]    Loss 0.262184    Top1 91.015625    
2023-01-06 16:48:10,234 - Epoch: [171][   20/   28]    Loss 0.262709    Top1 90.585938    
2023-01-06 16:48:10,283 - Epoch: [171][   28/   28]    Loss 0.263124    Top1 90.652734    
2023-01-06 16:48:10,421 - ==> Top1: 90.653    Loss: 0.263

2023-01-06 16:48:10,422 - ==> Confusion:
[[ 241   15  183]
 [  15  269  318]
 [  52   70 5823]]

2023-01-06 16:48:10,423 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:10,423 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:10,428 - 

2023-01-06 16:48:10,428 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:11,103 - Epoch: [172][   10/  246]    Overall Loss 0.260808    Objective Loss 0.260808                                        LR 0.000008    Time 0.067383    
2023-01-06 16:48:11,247 - Epoch: [172][   20/  246]    Overall Loss 0.243253    Objective Loss 0.243253                                        LR 0.000008    Time 0.040873    
2023-01-06 16:48:11,397 - Epoch: [172][   30/  246]    Overall Loss 0.244289    Objective Loss 0.244289                                        LR 0.000008    Time 0.032252    
2023-01-06 16:48:11,544 - Epoch: [172][   40/  246]    Overall Loss 0.244763    Objective Loss 0.244763                                        LR 0.000008    Time 0.027853    
2023-01-06 16:48:11,697 - Epoch: [172][   50/  246]    Overall Loss 0.244642    Objective Loss 0.244642                                        LR 0.000008    Time 0.025319    
2023-01-06 16:48:11,847 - Epoch: [172][   60/  246]    Overall Loss 0.244422    Objective Loss 0.244422                                        LR 0.000008    Time 0.023603    
2023-01-06 16:48:11,999 - Epoch: [172][   70/  246]    Overall Loss 0.245438    Objective Loss 0.245438                                        LR 0.000008    Time 0.022391    
2023-01-06 16:48:12,148 - Epoch: [172][   80/  246]    Overall Loss 0.243733    Objective Loss 0.243733                                        LR 0.000008    Time 0.021451    
2023-01-06 16:48:12,300 - Epoch: [172][   90/  246]    Overall Loss 0.243000    Objective Loss 0.243000                                        LR 0.000008    Time 0.020753    
2023-01-06 16:48:12,446 - Epoch: [172][  100/  246]    Overall Loss 0.243559    Objective Loss 0.243559                                        LR 0.000008    Time 0.020129    
2023-01-06 16:48:12,590 - Epoch: [172][  110/  246]    Overall Loss 0.244342    Objective Loss 0.244342                                        LR 0.000008    Time 0.019609    
2023-01-06 16:48:12,738 - Epoch: [172][  120/  246]    Overall Loss 0.245548    Objective Loss 0.245548                                        LR 0.000008    Time 0.019204    
2023-01-06 16:48:12,890 - Epoch: [172][  130/  246]    Overall Loss 0.246307    Objective Loss 0.246307                                        LR 0.000008    Time 0.018887    
2023-01-06 16:48:13,042 - Epoch: [172][  140/  246]    Overall Loss 0.246731    Objective Loss 0.246731                                        LR 0.000008    Time 0.018621    
2023-01-06 16:48:13,196 - Epoch: [172][  150/  246]    Overall Loss 0.246038    Objective Loss 0.246038                                        LR 0.000008    Time 0.018403    
2023-01-06 16:48:13,346 - Epoch: [172][  160/  246]    Overall Loss 0.246743    Objective Loss 0.246743                                        LR 0.000008    Time 0.018192    
2023-01-06 16:48:13,502 - Epoch: [172][  170/  246]    Overall Loss 0.246768    Objective Loss 0.246768                                        LR 0.000008    Time 0.018033    
2023-01-06 16:48:13,654 - Epoch: [172][  180/  246]    Overall Loss 0.246687    Objective Loss 0.246687                                        LR 0.000008    Time 0.017876    
2023-01-06 16:48:13,810 - Epoch: [172][  190/  246]    Overall Loss 0.246539    Objective Loss 0.246539                                        LR 0.000008    Time 0.017753    
2023-01-06 16:48:13,962 - Epoch: [172][  200/  246]    Overall Loss 0.246662    Objective Loss 0.246662                                        LR 0.000008    Time 0.017622    
2023-01-06 16:48:14,111 - Epoch: [172][  210/  246]    Overall Loss 0.246266    Objective Loss 0.246266                                        LR 0.000008    Time 0.017492    
2023-01-06 16:48:14,253 - Epoch: [172][  220/  246]    Overall Loss 0.246345    Objective Loss 0.246345                                        LR 0.000008    Time 0.017339    
2023-01-06 16:48:14,397 - Epoch: [172][  230/  246]    Overall Loss 0.246432    Objective Loss 0.246432                                        LR 0.000008    Time 0.017208    
2023-01-06 16:48:14,540 - Epoch: [172][  240/  246]    Overall Loss 0.245997    Objective Loss 0.245997                                        LR 0.000008    Time 0.017084    
2023-01-06 16:48:14,602 - Epoch: [172][  246/  246]    Overall Loss 0.246200    Objective Loss 0.246200    Top1 92.344498    LR 0.000008    Time 0.016922    
2023-01-06 16:48:14,743 - --- validate (epoch=172)-----------
2023-01-06 16:48:14,743 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:15,168 - Epoch: [172][   10/   28]    Loss 0.252224    Top1 90.820312    
2023-01-06 16:48:15,263 - Epoch: [172][   20/   28]    Loss 0.254798    Top1 91.015625    
2023-01-06 16:48:15,314 - Epoch: [172][   28/   28]    Loss 0.264918    Top1 90.638420    
2023-01-06 16:48:15,453 - ==> Top1: 90.638    Loss: 0.265

2023-01-06 16:48:15,453 - ==> Confusion:
[[ 252   16  171]
 [  16  274  312]
 [  68   71 5806]]

2023-01-06 16:48:15,455 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:15,455 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:15,460 - 

2023-01-06 16:48:15,460 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:15,995 - Epoch: [173][   10/  246]    Overall Loss 0.247754    Objective Loss 0.247754                                        LR 0.000008    Time 0.053491    
2023-01-06 16:48:16,138 - Epoch: [173][   20/  246]    Overall Loss 0.249051    Objective Loss 0.249051                                        LR 0.000008    Time 0.033830    
2023-01-06 16:48:16,281 - Epoch: [173][   30/  246]    Overall Loss 0.250095    Objective Loss 0.250095                                        LR 0.000008    Time 0.027315    
2023-01-06 16:48:16,424 - Epoch: [173][   40/  246]    Overall Loss 0.250258    Objective Loss 0.250258                                        LR 0.000008    Time 0.024048    
2023-01-06 16:48:16,569 - Epoch: [173][   50/  246]    Overall Loss 0.248994    Objective Loss 0.248994                                        LR 0.000008    Time 0.022127    
2023-01-06 16:48:16,699 - Epoch: [173][   60/  246]    Overall Loss 0.248952    Objective Loss 0.248952                                        LR 0.000008    Time 0.020606    
2023-01-06 16:48:16,831 - Epoch: [173][   70/  246]    Overall Loss 0.246000    Objective Loss 0.246000                                        LR 0.000008    Time 0.019543    
2023-01-06 16:48:16,971 - Epoch: [173][   80/  246]    Overall Loss 0.246139    Objective Loss 0.246139                                        LR 0.000008    Time 0.018835    
2023-01-06 16:48:17,108 - Epoch: [173][   90/  246]    Overall Loss 0.247265    Objective Loss 0.247265                                        LR 0.000008    Time 0.018260    
2023-01-06 16:48:17,248 - Epoch: [173][  100/  246]    Overall Loss 0.246753    Objective Loss 0.246753                                        LR 0.000008    Time 0.017831    
2023-01-06 16:48:17,391 - Epoch: [173][  110/  246]    Overall Loss 0.247572    Objective Loss 0.247572                                        LR 0.000008    Time 0.017502    
2023-01-06 16:48:17,528 - Epoch: [173][  120/  246]    Overall Loss 0.245210    Objective Loss 0.245210                                        LR 0.000008    Time 0.017182    
2023-01-06 16:48:17,670 - Epoch: [173][  130/  246]    Overall Loss 0.245096    Objective Loss 0.245096                                        LR 0.000008    Time 0.016950    
2023-01-06 16:48:17,812 - Epoch: [173][  140/  246]    Overall Loss 0.243937    Objective Loss 0.243937                                        LR 0.000008    Time 0.016748    
2023-01-06 16:48:17,949 - Epoch: [173][  150/  246]    Overall Loss 0.245354    Objective Loss 0.245354                                        LR 0.000008    Time 0.016547    
2023-01-06 16:48:18,093 - Epoch: [173][  160/  246]    Overall Loss 0.245016    Objective Loss 0.245016                                        LR 0.000008    Time 0.016409    
2023-01-06 16:48:18,237 - Epoch: [173][  170/  246]    Overall Loss 0.244350    Objective Loss 0.244350                                        LR 0.000008    Time 0.016285    
2023-01-06 16:48:18,380 - Epoch: [173][  180/  246]    Overall Loss 0.245234    Objective Loss 0.245234                                        LR 0.000008    Time 0.016172    
2023-01-06 16:48:18,512 - Epoch: [173][  190/  246]    Overall Loss 0.244624    Objective Loss 0.244624                                        LR 0.000008    Time 0.016014    
2023-01-06 16:48:18,633 - Epoch: [173][  200/  246]    Overall Loss 0.244690    Objective Loss 0.244690                                        LR 0.000008    Time 0.015818    
2023-01-06 16:48:18,766 - Epoch: [173][  210/  246]    Overall Loss 0.244979    Objective Loss 0.244979                                        LR 0.000008    Time 0.015694    
2023-01-06 16:48:18,895 - Epoch: [173][  220/  246]    Overall Loss 0.245068    Objective Loss 0.245068                                        LR 0.000008    Time 0.015567    
2023-01-06 16:48:19,031 - Epoch: [173][  230/  246]    Overall Loss 0.245401    Objective Loss 0.245401                                        LR 0.000008    Time 0.015478    
2023-01-06 16:48:19,170 - Epoch: [173][  240/  246]    Overall Loss 0.244729    Objective Loss 0.244729                                        LR 0.000008    Time 0.015410    
2023-01-06 16:48:19,229 - Epoch: [173][  246/  246]    Overall Loss 0.244646    Objective Loss 0.244646    Top1 90.669856    LR 0.000008    Time 0.015272    
2023-01-06 16:48:19,361 - --- validate (epoch=173)-----------
2023-01-06 16:48:19,362 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:19,800 - Epoch: [173][   10/   28]    Loss 0.257770    Top1 91.367188    
2023-01-06 16:48:19,901 - Epoch: [173][   20/   28]    Loss 0.264948    Top1 90.664062    
2023-01-06 16:48:19,951 - Epoch: [173][   28/   28]    Loss 0.260662    Top1 90.638420    
2023-01-06 16:48:20,108 - ==> Top1: 90.638    Loss: 0.261

2023-01-06 16:48:20,108 - ==> Confusion:
[[ 248   16  175]
 [  16  264  322]
 [  63   62 5820]]

2023-01-06 16:48:20,110 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:20,110 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:20,115 - 

2023-01-06 16:48:20,115 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:20,765 - Epoch: [174][   10/  246]    Overall Loss 0.239307    Objective Loss 0.239307                                        LR 0.000008    Time 0.064936    
2023-01-06 16:48:20,887 - Epoch: [174][   20/  246]    Overall Loss 0.234952    Objective Loss 0.234952                                        LR 0.000008    Time 0.038526    
2023-01-06 16:48:21,017 - Epoch: [174][   30/  246]    Overall Loss 0.237849    Objective Loss 0.237849                                        LR 0.000008    Time 0.030000    
2023-01-06 16:48:21,154 - Epoch: [174][   40/  246]    Overall Loss 0.236890    Objective Loss 0.236890                                        LR 0.000008    Time 0.025916    
2023-01-06 16:48:21,296 - Epoch: [174][   50/  246]    Overall Loss 0.236629    Objective Loss 0.236629                                        LR 0.000008    Time 0.023537    
2023-01-06 16:48:21,440 - Epoch: [174][   60/  246]    Overall Loss 0.236709    Objective Loss 0.236709                                        LR 0.000008    Time 0.022004    
2023-01-06 16:48:21,580 - Epoch: [174][   70/  246]    Overall Loss 0.239462    Objective Loss 0.239462                                        LR 0.000008    Time 0.020852    
2023-01-06 16:48:21,721 - Epoch: [174][   80/  246]    Overall Loss 0.239826    Objective Loss 0.239826                                        LR 0.000008    Time 0.020008    
2023-01-06 16:48:21,861 - Epoch: [174][   90/  246]    Overall Loss 0.240548    Objective Loss 0.240548                                        LR 0.000008    Time 0.019331    
2023-01-06 16:48:22,002 - Epoch: [174][  100/  246]    Overall Loss 0.242345    Objective Loss 0.242345                                        LR 0.000008    Time 0.018804    
2023-01-06 16:48:22,138 - Epoch: [174][  110/  246]    Overall Loss 0.242600    Objective Loss 0.242600                                        LR 0.000008    Time 0.018325    
2023-01-06 16:48:22,255 - Epoch: [174][  120/  246]    Overall Loss 0.243211    Objective Loss 0.243211                                        LR 0.000008    Time 0.017772    
2023-01-06 16:48:22,371 - Epoch: [174][  130/  246]    Overall Loss 0.241466    Objective Loss 0.241466                                        LR 0.000008    Time 0.017297    
2023-01-06 16:48:22,487 - Epoch: [174][  140/  246]    Overall Loss 0.242814    Objective Loss 0.242814                                        LR 0.000008    Time 0.016890    
2023-01-06 16:48:22,603 - Epoch: [174][  150/  246]    Overall Loss 0.243100    Objective Loss 0.243100                                        LR 0.000008    Time 0.016529    
2023-01-06 16:48:22,720 - Epoch: [174][  160/  246]    Overall Loss 0.243772    Objective Loss 0.243772                                        LR 0.000008    Time 0.016226    
2023-01-06 16:48:22,835 - Epoch: [174][  170/  246]    Overall Loss 0.244844    Objective Loss 0.244844                                        LR 0.000008    Time 0.015950    
2023-01-06 16:48:22,961 - Epoch: [174][  180/  246]    Overall Loss 0.244756    Objective Loss 0.244756                                        LR 0.000008    Time 0.015759    
2023-01-06 16:48:23,081 - Epoch: [174][  190/  246]    Overall Loss 0.244431    Objective Loss 0.244431                                        LR 0.000008    Time 0.015563    
2023-01-06 16:48:23,214 - Epoch: [174][  200/  246]    Overall Loss 0.244854    Objective Loss 0.244854                                        LR 0.000008    Time 0.015445    
2023-01-06 16:48:23,345 - Epoch: [174][  210/  246]    Overall Loss 0.244427    Objective Loss 0.244427                                        LR 0.000008    Time 0.015330    
2023-01-06 16:48:23,479 - Epoch: [174][  220/  246]    Overall Loss 0.244352    Objective Loss 0.244352                                        LR 0.000008    Time 0.015242    
2023-01-06 16:48:23,610 - Epoch: [174][  230/  246]    Overall Loss 0.244601    Objective Loss 0.244601                                        LR 0.000008    Time 0.015149    
2023-01-06 16:48:23,757 - Epoch: [174][  240/  246]    Overall Loss 0.245030    Objective Loss 0.245030                                        LR 0.000008    Time 0.015126    
2023-01-06 16:48:23,819 - Epoch: [174][  246/  246]    Overall Loss 0.244882    Objective Loss 0.244882    Top1 93.779904    LR 0.000008    Time 0.015011    
2023-01-06 16:48:23,947 - --- validate (epoch=174)-----------
2023-01-06 16:48:23,947 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:24,374 - Epoch: [174][   10/   28]    Loss 0.259096    Top1 90.468750    
2023-01-06 16:48:24,470 - Epoch: [174][   20/   28]    Loss 0.260081    Top1 90.507812    
2023-01-06 16:48:24,521 - Epoch: [174][   28/   28]    Loss 0.261151    Top1 90.466648    
2023-01-06 16:48:24,659 - ==> Top1: 90.467    Loss: 0.261

2023-01-06 16:48:24,659 - ==> Confusion:
[[ 250   15  174]
 [  15  264  323]
 [  69   70 5806]]

2023-01-06 16:48:24,660 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:24,660 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:24,665 - 

2023-01-06 16:48:24,666 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:25,184 - Epoch: [175][   10/  246]    Overall Loss 0.230209    Objective Loss 0.230209                                        LR 0.000008    Time 0.051807    
2023-01-06 16:48:25,318 - Epoch: [175][   20/  246]    Overall Loss 0.228309    Objective Loss 0.228309                                        LR 0.000008    Time 0.032559    
2023-01-06 16:48:25,448 - Epoch: [175][   30/  246]    Overall Loss 0.239099    Objective Loss 0.239099                                        LR 0.000008    Time 0.026030    
2023-01-06 16:48:25,582 - Epoch: [175][   40/  246]    Overall Loss 0.241800    Objective Loss 0.241800                                        LR 0.000008    Time 0.022873    
2023-01-06 16:48:25,711 - Epoch: [175][   50/  246]    Overall Loss 0.243080    Objective Loss 0.243080                                        LR 0.000008    Time 0.020867    
2023-01-06 16:48:25,855 - Epoch: [175][   60/  246]    Overall Loss 0.243979    Objective Loss 0.243979                                        LR 0.000008    Time 0.019749    
2023-01-06 16:48:25,985 - Epoch: [175][   70/  246]    Overall Loss 0.241843    Objective Loss 0.241843                                        LR 0.000008    Time 0.018778    
2023-01-06 16:48:26,128 - Epoch: [175][   80/  246]    Overall Loss 0.241813    Objective Loss 0.241813                                        LR 0.000008    Time 0.018219    
2023-01-06 16:48:26,268 - Epoch: [175][   90/  246]    Overall Loss 0.242525    Objective Loss 0.242525                                        LR 0.000008    Time 0.017747    
2023-01-06 16:48:26,403 - Epoch: [175][  100/  246]    Overall Loss 0.243411    Objective Loss 0.243411                                        LR 0.000008    Time 0.017319    
2023-01-06 16:48:26,544 - Epoch: [175][  110/  246]    Overall Loss 0.243815    Objective Loss 0.243815                                        LR 0.000008    Time 0.017021    
2023-01-06 16:48:26,688 - Epoch: [175][  120/  246]    Overall Loss 0.244277    Objective Loss 0.244277                                        LR 0.000008    Time 0.016789    
2023-01-06 16:48:26,829 - Epoch: [175][  130/  246]    Overall Loss 0.243293    Objective Loss 0.243293                                        LR 0.000008    Time 0.016576    
2023-01-06 16:48:26,974 - Epoch: [175][  140/  246]    Overall Loss 0.244022    Objective Loss 0.244022                                        LR 0.000008    Time 0.016427    
2023-01-06 16:48:27,110 - Epoch: [175][  150/  246]    Overall Loss 0.244958    Objective Loss 0.244958                                        LR 0.000008    Time 0.016239    
2023-01-06 16:48:27,253 - Epoch: [175][  160/  246]    Overall Loss 0.244117    Objective Loss 0.244117                                        LR 0.000008    Time 0.016115    
2023-01-06 16:48:27,394 - Epoch: [175][  170/  246]    Overall Loss 0.242717    Objective Loss 0.242717                                        LR 0.000008    Time 0.015993    
2023-01-06 16:48:27,537 - Epoch: [175][  180/  246]    Overall Loss 0.242562    Objective Loss 0.242562                                        LR 0.000008    Time 0.015898    
2023-01-06 16:48:27,678 - Epoch: [175][  190/  246]    Overall Loss 0.242154    Objective Loss 0.242154                                        LR 0.000008    Time 0.015800    
2023-01-06 16:48:27,820 - Epoch: [175][  200/  246]    Overall Loss 0.242986    Objective Loss 0.242986                                        LR 0.000008    Time 0.015718    
2023-01-06 16:48:27,960 - Epoch: [175][  210/  246]    Overall Loss 0.243984    Objective Loss 0.243984                                        LR 0.000008    Time 0.015638    
2023-01-06 16:48:28,101 - Epoch: [175][  220/  246]    Overall Loss 0.244783    Objective Loss 0.244783                                        LR 0.000008    Time 0.015562    
2023-01-06 16:48:28,237 - Epoch: [175][  230/  246]    Overall Loss 0.244629    Objective Loss 0.244629                                        LR 0.000008    Time 0.015477    
2023-01-06 16:48:28,388 - Epoch: [175][  240/  246]    Overall Loss 0.244725    Objective Loss 0.244725                                        LR 0.000008    Time 0.015460    
2023-01-06 16:48:28,453 - Epoch: [175][  246/  246]    Overall Loss 0.244735    Objective Loss 0.244735    Top1 89.234450    LR 0.000008    Time 0.015345    
2023-01-06 16:48:28,589 - --- validate (epoch=175)-----------
2023-01-06 16:48:28,589 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:29,347 - Epoch: [175][   10/   28]    Loss 0.268912    Top1 89.804688    
2023-01-06 16:48:29,456 - Epoch: [175][   20/   28]    Loss 0.260689    Top1 90.449219    
2023-01-06 16:48:29,504 - Epoch: [175][   28/   28]    Loss 0.260408    Top1 90.538219    
2023-01-06 16:48:29,650 - ==> Top1: 90.538    Loss: 0.260

2023-01-06 16:48:29,651 - ==> Confusion:
[[ 222   16  201]
 [   9  262  331]
 [  43   61 5841]]

2023-01-06 16:48:29,652 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:29,652 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:29,657 - 

2023-01-06 16:48:29,657 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:30,183 - Epoch: [176][   10/  246]    Overall Loss 0.215575    Objective Loss 0.215575                                        LR 0.000008    Time 0.052550    
2023-01-06 16:48:30,334 - Epoch: [176][   20/  246]    Overall Loss 0.234959    Objective Loss 0.234959                                        LR 0.000008    Time 0.033800    
2023-01-06 16:48:30,483 - Epoch: [176][   30/  246]    Overall Loss 0.239432    Objective Loss 0.239432                                        LR 0.000008    Time 0.027467    
2023-01-06 16:48:30,634 - Epoch: [176][   40/  246]    Overall Loss 0.237993    Objective Loss 0.237993                                        LR 0.000008    Time 0.024385    
2023-01-06 16:48:30,781 - Epoch: [176][   50/  246]    Overall Loss 0.239467    Objective Loss 0.239467                                        LR 0.000008    Time 0.022436    
2023-01-06 16:48:30,931 - Epoch: [176][   60/  246]    Overall Loss 0.240906    Objective Loss 0.240906                                        LR 0.000008    Time 0.021189    
2023-01-06 16:48:31,076 - Epoch: [176][   70/  246]    Overall Loss 0.241059    Objective Loss 0.241059                                        LR 0.000008    Time 0.020230    
2023-01-06 16:48:31,226 - Epoch: [176][   80/  246]    Overall Loss 0.242545    Objective Loss 0.242545                                        LR 0.000008    Time 0.019563    
2023-01-06 16:48:31,361 - Epoch: [176][   90/  246]    Overall Loss 0.243520    Objective Loss 0.243520                                        LR 0.000008    Time 0.018883    
2023-01-06 16:48:31,498 - Epoch: [176][  100/  246]    Overall Loss 0.242832    Objective Loss 0.242832                                        LR 0.000008    Time 0.018361    
2023-01-06 16:48:31,633 - Epoch: [176][  110/  246]    Overall Loss 0.241888    Objective Loss 0.241888                                        LR 0.000008    Time 0.017919    
2023-01-06 16:48:31,769 - Epoch: [176][  120/  246]    Overall Loss 0.243512    Objective Loss 0.243512                                        LR 0.000008    Time 0.017556    
2023-01-06 16:48:31,903 - Epoch: [176][  130/  246]    Overall Loss 0.242962    Objective Loss 0.242962                                        LR 0.000008    Time 0.017235    
2023-01-06 16:48:32,039 - Epoch: [176][  140/  246]    Overall Loss 0.243572    Objective Loss 0.243572                                        LR 0.000008    Time 0.016966    
2023-01-06 16:48:32,180 - Epoch: [176][  150/  246]    Overall Loss 0.242003    Objective Loss 0.242003                                        LR 0.000008    Time 0.016778    
2023-01-06 16:48:32,322 - Epoch: [176][  160/  246]    Overall Loss 0.240908    Objective Loss 0.240908                                        LR 0.000008    Time 0.016613    
2023-01-06 16:48:32,472 - Epoch: [176][  170/  246]    Overall Loss 0.241173    Objective Loss 0.241173                                        LR 0.000008    Time 0.016517    
2023-01-06 16:48:32,633 - Epoch: [176][  180/  246]    Overall Loss 0.241476    Objective Loss 0.241476                                        LR 0.000008    Time 0.016489    
2023-01-06 16:48:32,796 - Epoch: [176][  190/  246]    Overall Loss 0.242458    Objective Loss 0.242458                                        LR 0.000008    Time 0.016479    
2023-01-06 16:48:32,955 - Epoch: [176][  200/  246]    Overall Loss 0.243061    Objective Loss 0.243061                                        LR 0.000008    Time 0.016447    
2023-01-06 16:48:33,122 - Epoch: [176][  210/  246]    Overall Loss 0.243430    Objective Loss 0.243430                                        LR 0.000008    Time 0.016457    
2023-01-06 16:48:33,287 - Epoch: [176][  220/  246]    Overall Loss 0.244871    Objective Loss 0.244871                                        LR 0.000008    Time 0.016454    
2023-01-06 16:48:33,424 - Epoch: [176][  230/  246]    Overall Loss 0.244643    Objective Loss 0.244643                                        LR 0.000008    Time 0.016335    
2023-01-06 16:48:33,565 - Epoch: [176][  240/  246]    Overall Loss 0.245185    Objective Loss 0.245185                                        LR 0.000008    Time 0.016241    
2023-01-06 16:48:33,626 - Epoch: [176][  246/  246]    Overall Loss 0.245026    Objective Loss 0.245026    Top1 92.344498    LR 0.000008    Time 0.016093    
2023-01-06 16:48:33,785 - --- validate (epoch=176)-----------
2023-01-06 16:48:33,785 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:34,205 - Epoch: [176][   10/   28]    Loss 0.265774    Top1 90.468750    
2023-01-06 16:48:34,302 - Epoch: [176][   20/   28]    Loss 0.258556    Top1 90.761719    
2023-01-06 16:48:34,353 - Epoch: [176][   28/   28]    Loss 0.259375    Top1 90.738620    
2023-01-06 16:48:34,487 - ==> Top1: 90.739    Loss: 0.259

2023-01-06 16:48:34,488 - ==> Confusion:
[[ 243   15  181]
 [  15  276  311]
 [  53   72 5820]]

2023-01-06 16:48:34,489 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:34,489 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:34,494 - 

2023-01-06 16:48:34,494 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:35,163 - Epoch: [177][   10/  246]    Overall Loss 0.235011    Objective Loss 0.235011                                        LR 0.000008    Time 0.066855    
2023-01-06 16:48:35,316 - Epoch: [177][   20/  246]    Overall Loss 0.236294    Objective Loss 0.236294                                        LR 0.000008    Time 0.041050    
2023-01-06 16:48:35,477 - Epoch: [177][   30/  246]    Overall Loss 0.238631    Objective Loss 0.238631                                        LR 0.000008    Time 0.032668    
2023-01-06 16:48:35,636 - Epoch: [177][   40/  246]    Overall Loss 0.240920    Objective Loss 0.240920                                        LR 0.000008    Time 0.028453    
2023-01-06 16:48:35,781 - Epoch: [177][   50/  246]    Overall Loss 0.247375    Objective Loss 0.247375                                        LR 0.000008    Time 0.025650    
2023-01-06 16:48:35,935 - Epoch: [177][   60/  246]    Overall Loss 0.249541    Objective Loss 0.249541                                        LR 0.000008    Time 0.023939    
2023-01-06 16:48:36,082 - Epoch: [177][   70/  246]    Overall Loss 0.250344    Objective Loss 0.250344                                        LR 0.000008    Time 0.022616    
2023-01-06 16:48:36,229 - Epoch: [177][   80/  246]    Overall Loss 0.247637    Objective Loss 0.247637                                        LR 0.000008    Time 0.021615    
2023-01-06 16:48:36,371 - Epoch: [177][   90/  246]    Overall Loss 0.248983    Objective Loss 0.248983                                        LR 0.000008    Time 0.020786    
2023-01-06 16:48:36,515 - Epoch: [177][  100/  246]    Overall Loss 0.248009    Objective Loss 0.248009                                        LR 0.000008    Time 0.020136    
2023-01-06 16:48:36,662 - Epoch: [177][  110/  246]    Overall Loss 0.248048    Objective Loss 0.248048                                        LR 0.000008    Time 0.019632    
2023-01-06 16:48:36,808 - Epoch: [177][  120/  246]    Overall Loss 0.248730    Objective Loss 0.248730                                        LR 0.000008    Time 0.019210    
2023-01-06 16:48:36,954 - Epoch: [177][  130/  246]    Overall Loss 0.248105    Objective Loss 0.248105                                        LR 0.000008    Time 0.018854    
2023-01-06 16:48:37,104 - Epoch: [177][  140/  246]    Overall Loss 0.247656    Objective Loss 0.247656                                        LR 0.000008    Time 0.018576    
2023-01-06 16:48:37,256 - Epoch: [177][  150/  246]    Overall Loss 0.246230    Objective Loss 0.246230                                        LR 0.000008    Time 0.018344    
2023-01-06 16:48:37,400 - Epoch: [177][  160/  246]    Overall Loss 0.245617    Objective Loss 0.245617                                        LR 0.000008    Time 0.018097    
2023-01-06 16:48:37,551 - Epoch: [177][  170/  246]    Overall Loss 0.245150    Objective Loss 0.245150                                        LR 0.000008    Time 0.017917    
2023-01-06 16:48:37,697 - Epoch: [177][  180/  246]    Overall Loss 0.244179    Objective Loss 0.244179                                        LR 0.000008    Time 0.017729    
2023-01-06 16:48:37,842 - Epoch: [177][  190/  246]    Overall Loss 0.243758    Objective Loss 0.243758                                        LR 0.000008    Time 0.017561    
2023-01-06 16:48:37,983 - Epoch: [177][  200/  246]    Overall Loss 0.242975    Objective Loss 0.242975                                        LR 0.000008    Time 0.017382    
2023-01-06 16:48:38,129 - Epoch: [177][  210/  246]    Overall Loss 0.243970    Objective Loss 0.243970                                        LR 0.000008    Time 0.017238    
2023-01-06 16:48:38,279 - Epoch: [177][  220/  246]    Overall Loss 0.243719    Objective Loss 0.243719                                        LR 0.000008    Time 0.017131    
2023-01-06 16:48:38,433 - Epoch: [177][  230/  246]    Overall Loss 0.243511    Objective Loss 0.243511                                        LR 0.000008    Time 0.017053    
2023-01-06 16:48:38,594 - Epoch: [177][  240/  246]    Overall Loss 0.244113    Objective Loss 0.244113                                        LR 0.000008    Time 0.017011    
2023-01-06 16:48:38,659 - Epoch: [177][  246/  246]    Overall Loss 0.244284    Objective Loss 0.244284    Top1 93.301435    LR 0.000008    Time 0.016863    
2023-01-06 16:48:38,806 - --- validate (epoch=177)-----------
2023-01-06 16:48:38,806 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:39,234 - Epoch: [177][   10/   28]    Loss 0.259320    Top1 90.429688    
2023-01-06 16:48:39,336 - Epoch: [177][   20/   28]    Loss 0.262542    Top1 90.585938    
2023-01-06 16:48:39,389 - Epoch: [177][   28/   28]    Loss 0.265534    Top1 90.595477    
2023-01-06 16:48:39,554 - ==> Top1: 90.595    Loss: 0.266

2023-01-06 16:48:39,555 - ==> Confusion:
[[ 225   13  201]
 [  14  275  313]
 [  41   75 5829]]

2023-01-06 16:48:39,556 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:39,556 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:39,563 - 

2023-01-06 16:48:39,563 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:40,085 - Epoch: [178][   10/  246]    Overall Loss 0.240851    Objective Loss 0.240851                                        LR 0.000008    Time 0.052124    
2023-01-06 16:48:40,232 - Epoch: [178][   20/  246]    Overall Loss 0.251447    Objective Loss 0.251447                                        LR 0.000008    Time 0.033377    
2023-01-06 16:48:40,379 - Epoch: [178][   30/  246]    Overall Loss 0.251265    Objective Loss 0.251265                                        LR 0.000008    Time 0.027133    
2023-01-06 16:48:40,522 - Epoch: [178][   40/  246]    Overall Loss 0.257017    Objective Loss 0.257017                                        LR 0.000008    Time 0.023931    
2023-01-06 16:48:40,671 - Epoch: [178][   50/  246]    Overall Loss 0.252429    Objective Loss 0.252429                                        LR 0.000008    Time 0.022116    
2023-01-06 16:48:40,818 - Epoch: [178][   60/  246]    Overall Loss 0.250469    Objective Loss 0.250469                                        LR 0.000008    Time 0.020868    
2023-01-06 16:48:40,967 - Epoch: [178][   70/  246]    Overall Loss 0.250865    Objective Loss 0.250865                                        LR 0.000008    Time 0.020010    
2023-01-06 16:48:41,104 - Epoch: [178][   80/  246]    Overall Loss 0.252086    Objective Loss 0.252086                                        LR 0.000008    Time 0.019220    
2023-01-06 16:48:41,245 - Epoch: [178][   90/  246]    Overall Loss 0.249497    Objective Loss 0.249497                                        LR 0.000008    Time 0.018642    
2023-01-06 16:48:41,386 - Epoch: [178][  100/  246]    Overall Loss 0.247561    Objective Loss 0.247561                                        LR 0.000008    Time 0.018187    
2023-01-06 16:48:41,524 - Epoch: [178][  110/  246]    Overall Loss 0.246778    Objective Loss 0.246778                                        LR 0.000008    Time 0.017784    
2023-01-06 16:48:41,668 - Epoch: [178][  120/  246]    Overall Loss 0.245028    Objective Loss 0.245028                                        LR 0.000008    Time 0.017494    
2023-01-06 16:48:41,809 - Epoch: [178][  130/  246]    Overall Loss 0.243907    Objective Loss 0.243907                                        LR 0.000008    Time 0.017229    
2023-01-06 16:48:41,949 - Epoch: [178][  140/  246]    Overall Loss 0.244683    Objective Loss 0.244683                                        LR 0.000008    Time 0.017000    
2023-01-06 16:48:42,090 - Epoch: [178][  150/  246]    Overall Loss 0.244122    Objective Loss 0.244122                                        LR 0.000008    Time 0.016802    
2023-01-06 16:48:42,232 - Epoch: [178][  160/  246]    Overall Loss 0.245960    Objective Loss 0.245960                                        LR 0.000008    Time 0.016637    
2023-01-06 16:48:42,373 - Epoch: [178][  170/  246]    Overall Loss 0.245580    Objective Loss 0.245580                                        LR 0.000008    Time 0.016486    
2023-01-06 16:48:42,514 - Epoch: [178][  180/  246]    Overall Loss 0.246107    Objective Loss 0.246107                                        LR 0.000008    Time 0.016353    
2023-01-06 16:48:42,654 - Epoch: [178][  190/  246]    Overall Loss 0.244825    Objective Loss 0.244825                                        LR 0.000008    Time 0.016228    
2023-01-06 16:48:42,794 - Epoch: [178][  200/  246]    Overall Loss 0.245192    Objective Loss 0.245192                                        LR 0.000008    Time 0.016114    
2023-01-06 16:48:42,935 - Epoch: [178][  210/  246]    Overall Loss 0.245139    Objective Loss 0.245139                                        LR 0.000008    Time 0.016016    
2023-01-06 16:48:43,076 - Epoch: [178][  220/  246]    Overall Loss 0.244804    Objective Loss 0.244804                                        LR 0.000008    Time 0.015926    
2023-01-06 16:48:43,216 - Epoch: [178][  230/  246]    Overall Loss 0.243798    Objective Loss 0.243798                                        LR 0.000008    Time 0.015840    
2023-01-06 16:48:43,370 - Epoch: [178][  240/  246]    Overall Loss 0.244281    Objective Loss 0.244281                                        LR 0.000008    Time 0.015822    
2023-01-06 16:48:43,434 - Epoch: [178][  246/  246]    Overall Loss 0.244762    Objective Loss 0.244762    Top1 88.995215    LR 0.000008    Time 0.015696    
2023-01-06 16:48:43,567 - --- validate (epoch=178)-----------
2023-01-06 16:48:43,567 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:44,002 - Epoch: [178][   10/   28]    Loss 0.260696    Top1 90.664062    
2023-01-06 16:48:44,110 - Epoch: [178][   20/   28]    Loss 0.261224    Top1 90.468750    
2023-01-06 16:48:44,160 - Epoch: [178][   28/   28]    Loss 0.262886    Top1 90.638420    
2023-01-06 16:48:44,324 - ==> Top1: 90.638    Loss: 0.263

2023-01-06 16:48:44,324 - ==> Confusion:
[[ 242   14  183]
 [  16  282  304]
 [  56   81 5808]]

2023-01-06 16:48:44,326 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:44,326 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:44,331 - 

2023-01-06 16:48:44,331 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:44,982 - Epoch: [179][   10/  246]    Overall Loss 0.247369    Objective Loss 0.247369                                        LR 0.000008    Time 0.065047    
2023-01-06 16:48:45,120 - Epoch: [179][   20/  246]    Overall Loss 0.249295    Objective Loss 0.249295                                        LR 0.000008    Time 0.039379    
2023-01-06 16:48:45,267 - Epoch: [179][   30/  246]    Overall Loss 0.247168    Objective Loss 0.247168                                        LR 0.000008    Time 0.031163    
2023-01-06 16:48:45,415 - Epoch: [179][   40/  246]    Overall Loss 0.241805    Objective Loss 0.241805                                        LR 0.000008    Time 0.027041    
2023-01-06 16:48:45,558 - Epoch: [179][   50/  246]    Overall Loss 0.240421    Objective Loss 0.240421                                        LR 0.000008    Time 0.024484    
2023-01-06 16:48:45,702 - Epoch: [179][   60/  246]    Overall Loss 0.242397    Objective Loss 0.242397                                        LR 0.000008    Time 0.022807    
2023-01-06 16:48:45,845 - Epoch: [179][   70/  246]    Overall Loss 0.243732    Objective Loss 0.243732                                        LR 0.000008    Time 0.021581    
2023-01-06 16:48:45,991 - Epoch: [179][   80/  246]    Overall Loss 0.241987    Objective Loss 0.241987                                        LR 0.000008    Time 0.020706    
2023-01-06 16:48:46,135 - Epoch: [179][   90/  246]    Overall Loss 0.242811    Objective Loss 0.242811                                        LR 0.000008    Time 0.019996    
2023-01-06 16:48:46,277 - Epoch: [179][  100/  246]    Overall Loss 0.245392    Objective Loss 0.245392                                        LR 0.000008    Time 0.019413    
2023-01-06 16:48:46,422 - Epoch: [179][  110/  246]    Overall Loss 0.245382    Objective Loss 0.245382                                        LR 0.000008    Time 0.018958    
2023-01-06 16:48:46,562 - Epoch: [179][  120/  246]    Overall Loss 0.246101    Objective Loss 0.246101                                        LR 0.000008    Time 0.018548    
2023-01-06 16:48:46,694 - Epoch: [179][  130/  246]    Overall Loss 0.246535    Objective Loss 0.246535                                        LR 0.000008    Time 0.018131    
2023-01-06 16:48:46,827 - Epoch: [179][  140/  246]    Overall Loss 0.246122    Objective Loss 0.246122                                        LR 0.000008    Time 0.017779    
2023-01-06 16:48:46,959 - Epoch: [179][  150/  246]    Overall Loss 0.244773    Objective Loss 0.244773                                        LR 0.000008    Time 0.017472    
2023-01-06 16:48:47,091 - Epoch: [179][  160/  246]    Overall Loss 0.243998    Objective Loss 0.243998                                        LR 0.000008    Time 0.017201    
2023-01-06 16:48:47,226 - Epoch: [179][  170/  246]    Overall Loss 0.243714    Objective Loss 0.243714                                        LR 0.000008    Time 0.016983    
2023-01-06 16:48:47,366 - Epoch: [179][  180/  246]    Overall Loss 0.244174    Objective Loss 0.244174                                        LR 0.000008    Time 0.016816    
2023-01-06 16:48:47,504 - Epoch: [179][  190/  246]    Overall Loss 0.243964    Objective Loss 0.243964                                        LR 0.000008    Time 0.016654    
2023-01-06 16:48:47,643 - Epoch: [179][  200/  246]    Overall Loss 0.244016    Objective Loss 0.244016                                        LR 0.000008    Time 0.016516    
2023-01-06 16:48:47,780 - Epoch: [179][  210/  246]    Overall Loss 0.243415    Objective Loss 0.243415                                        LR 0.000008    Time 0.016379    
2023-01-06 16:48:47,926 - Epoch: [179][  220/  246]    Overall Loss 0.243694    Objective Loss 0.243694                                        LR 0.000008    Time 0.016294    
2023-01-06 16:48:48,079 - Epoch: [179][  230/  246]    Overall Loss 0.243782    Objective Loss 0.243782                                        LR 0.000008    Time 0.016249    
2023-01-06 16:48:48,236 - Epoch: [179][  240/  246]    Overall Loss 0.242824    Objective Loss 0.242824                                        LR 0.000008    Time 0.016226    
2023-01-06 16:48:48,301 - Epoch: [179][  246/  246]    Overall Loss 0.243597    Objective Loss 0.243597    Top1 92.105263    LR 0.000008    Time 0.016093    
2023-01-06 16:48:48,449 - --- validate (epoch=179)-----------
2023-01-06 16:48:48,449 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:48,882 - Epoch: [179][   10/   28]    Loss 0.278101    Top1 89.960938    
2023-01-06 16:48:48,987 - Epoch: [179][   20/   28]    Loss 0.261559    Top1 90.664062    
2023-01-06 16:48:49,035 - Epoch: [179][   28/   28]    Loss 0.260601    Top1 90.667048    
2023-01-06 16:48:49,169 - ==> Top1: 90.667    Loss: 0.261

2023-01-06 16:48:49,169 - ==> Confusion:
[[ 236   15  188]
 [  13  269  320]
 [  52   64 5829]]

2023-01-06 16:48:49,171 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:49,171 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:49,176 - 

2023-01-06 16:48:49,176 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:49,852 - Epoch: [180][   10/  246]    Overall Loss 0.235478    Objective Loss 0.235478                                        LR 0.000005    Time 0.067557    
2023-01-06 16:48:50,001 - Epoch: [180][   20/  246]    Overall Loss 0.239086    Objective Loss 0.239086                                        LR 0.000005    Time 0.041183    
2023-01-06 16:48:50,152 - Epoch: [180][   30/  246]    Overall Loss 0.243819    Objective Loss 0.243819                                        LR 0.000005    Time 0.032482    
2023-01-06 16:48:50,294 - Epoch: [180][   40/  246]    Overall Loss 0.239792    Objective Loss 0.239792                                        LR 0.000005    Time 0.027890    
2023-01-06 16:48:50,429 - Epoch: [180][   50/  246]    Overall Loss 0.237298    Objective Loss 0.237298                                        LR 0.000005    Time 0.025012    
2023-01-06 16:48:50,568 - Epoch: [180][   60/  246]    Overall Loss 0.236974    Objective Loss 0.236974                                        LR 0.000005    Time 0.023146    
2023-01-06 16:48:50,707 - Epoch: [180][   70/  246]    Overall Loss 0.238771    Objective Loss 0.238771                                        LR 0.000005    Time 0.021826    
2023-01-06 16:48:50,846 - Epoch: [180][   80/  246]    Overall Loss 0.237300    Objective Loss 0.237300                                        LR 0.000005    Time 0.020825    
2023-01-06 16:48:50,985 - Epoch: [180][   90/  246]    Overall Loss 0.237135    Objective Loss 0.237135                                        LR 0.000005    Time 0.020059    
2023-01-06 16:48:51,126 - Epoch: [180][  100/  246]    Overall Loss 0.240196    Objective Loss 0.240196                                        LR 0.000005    Time 0.019460    
2023-01-06 16:48:51,266 - Epoch: [180][  110/  246]    Overall Loss 0.242866    Objective Loss 0.242866                                        LR 0.000005    Time 0.018953    
2023-01-06 16:48:51,408 - Epoch: [180][  120/  246]    Overall Loss 0.241310    Objective Loss 0.241310                                        LR 0.000005    Time 0.018561    
2023-01-06 16:48:51,544 - Epoch: [180][  130/  246]    Overall Loss 0.241992    Objective Loss 0.241992                                        LR 0.000005    Time 0.018172    
2023-01-06 16:48:51,677 - Epoch: [180][  140/  246]    Overall Loss 0.243573    Objective Loss 0.243573                                        LR 0.000005    Time 0.017820    
2023-01-06 16:48:51,811 - Epoch: [180][  150/  246]    Overall Loss 0.245026    Objective Loss 0.245026                                        LR 0.000005    Time 0.017523    
2023-01-06 16:48:51,944 - Epoch: [180][  160/  246]    Overall Loss 0.244775    Objective Loss 0.244775                                        LR 0.000005    Time 0.017258    
2023-01-06 16:48:52,077 - Epoch: [180][  170/  246]    Overall Loss 0.244765    Objective Loss 0.244765                                        LR 0.000005    Time 0.017024    
2023-01-06 16:48:52,209 - Epoch: [180][  180/  246]    Overall Loss 0.245033    Objective Loss 0.245033                                        LR 0.000005    Time 0.016811    
2023-01-06 16:48:52,341 - Epoch: [180][  190/  246]    Overall Loss 0.245077    Objective Loss 0.245077                                        LR 0.000005    Time 0.016620    
2023-01-06 16:48:52,475 - Epoch: [180][  200/  246]    Overall Loss 0.244285    Objective Loss 0.244285                                        LR 0.000005    Time 0.016456    
2023-01-06 16:48:52,608 - Epoch: [180][  210/  246]    Overall Loss 0.243795    Objective Loss 0.243795                                        LR 0.000005    Time 0.016303    
2023-01-06 16:48:52,739 - Epoch: [180][  220/  246]    Overall Loss 0.244726    Objective Loss 0.244726                                        LR 0.000005    Time 0.016158    
2023-01-06 16:48:52,883 - Epoch: [180][  230/  246]    Overall Loss 0.243841    Objective Loss 0.243841                                        LR 0.000005    Time 0.016078    
2023-01-06 16:48:53,043 - Epoch: [180][  240/  246]    Overall Loss 0.243732    Objective Loss 0.243732                                        LR 0.000005    Time 0.016072    
2023-01-06 16:48:53,106 - Epoch: [180][  246/  246]    Overall Loss 0.243458    Objective Loss 0.243458    Top1 91.866029    LR 0.000005    Time 0.015939    
2023-01-06 16:48:53,239 - --- validate (epoch=180)-----------
2023-01-06 16:48:53,240 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:53,658 - Epoch: [180][   10/   28]    Loss 0.272596    Top1 90.195312    
2023-01-06 16:48:53,753 - Epoch: [180][   20/   28]    Loss 0.263636    Top1 90.468750    
2023-01-06 16:48:53,807 - Epoch: [180][   28/   28]    Loss 0.259425    Top1 90.652734    
2023-01-06 16:48:53,937 - ==> Top1: 90.653    Loss: 0.259

2023-01-06 16:48:53,937 - ==> Confusion:
[[ 231   14  194]
 [  12  256  334]
 [  41   58 5846]]

2023-01-06 16:48:53,938 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:53,938 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:53,943 - 

2023-01-06 16:48:53,943 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:54,490 - Epoch: [181][   10/  246]    Overall Loss 0.233142    Objective Loss 0.233142                                        LR 0.000005    Time 0.054615    
2023-01-06 16:48:54,644 - Epoch: [181][   20/  246]    Overall Loss 0.235217    Objective Loss 0.235217                                        LR 0.000005    Time 0.034978    
2023-01-06 16:48:54,779 - Epoch: [181][   30/  246]    Overall Loss 0.236022    Objective Loss 0.236022                                        LR 0.000005    Time 0.027803    
2023-01-06 16:48:54,919 - Epoch: [181][   40/  246]    Overall Loss 0.235498    Objective Loss 0.235498                                        LR 0.000005    Time 0.024346    
2023-01-06 16:48:55,064 - Epoch: [181][   50/  246]    Overall Loss 0.235294    Objective Loss 0.235294                                        LR 0.000005    Time 0.022335    
2023-01-06 16:48:55,206 - Epoch: [181][   60/  246]    Overall Loss 0.237015    Objective Loss 0.237015                                        LR 0.000005    Time 0.020970    
2023-01-06 16:48:55,347 - Epoch: [181][   70/  246]    Overall Loss 0.235312    Objective Loss 0.235312                                        LR 0.000005    Time 0.019984    
2023-01-06 16:48:55,489 - Epoch: [181][   80/  246]    Overall Loss 0.234704    Objective Loss 0.234704                                        LR 0.000005    Time 0.019252    
2023-01-06 16:48:55,633 - Epoch: [181][   90/  246]    Overall Loss 0.235584    Objective Loss 0.235584                                        LR 0.000005    Time 0.018713    
2023-01-06 16:48:55,781 - Epoch: [181][  100/  246]    Overall Loss 0.237896    Objective Loss 0.237896                                        LR 0.000005    Time 0.018311    
2023-01-06 16:48:55,926 - Epoch: [181][  110/  246]    Overall Loss 0.238147    Objective Loss 0.238147                                        LR 0.000005    Time 0.017963    
2023-01-06 16:48:56,078 - Epoch: [181][  120/  246]    Overall Loss 0.239534    Objective Loss 0.239534                                        LR 0.000005    Time 0.017729    
2023-01-06 16:48:56,233 - Epoch: [181][  130/  246]    Overall Loss 0.240923    Objective Loss 0.240923                                        LR 0.000005    Time 0.017554    
2023-01-06 16:48:56,385 - Epoch: [181][  140/  246]    Overall Loss 0.241637    Objective Loss 0.241637                                        LR 0.000005    Time 0.017385    
2023-01-06 16:48:56,538 - Epoch: [181][  150/  246]    Overall Loss 0.242534    Objective Loss 0.242534                                        LR 0.000005    Time 0.017241    
2023-01-06 16:48:56,695 - Epoch: [181][  160/  246]    Overall Loss 0.241766    Objective Loss 0.241766                                        LR 0.000005    Time 0.017138    
2023-01-06 16:48:56,851 - Epoch: [181][  170/  246]    Overall Loss 0.242572    Objective Loss 0.242572                                        LR 0.000005    Time 0.017051    
2023-01-06 16:48:57,009 - Epoch: [181][  180/  246]    Overall Loss 0.243496    Objective Loss 0.243496                                        LR 0.000005    Time 0.016974    
2023-01-06 16:48:57,165 - Epoch: [181][  190/  246]    Overall Loss 0.243298    Objective Loss 0.243298                                        LR 0.000005    Time 0.016904    
2023-01-06 16:48:57,324 - Epoch: [181][  200/  246]    Overall Loss 0.243210    Objective Loss 0.243210                                        LR 0.000005    Time 0.016848    
2023-01-06 16:48:57,478 - Epoch: [181][  210/  246]    Overall Loss 0.243499    Objective Loss 0.243499                                        LR 0.000005    Time 0.016780    
2023-01-06 16:48:57,638 - Epoch: [181][  220/  246]    Overall Loss 0.243240    Objective Loss 0.243240                                        LR 0.000005    Time 0.016742    
2023-01-06 16:48:57,794 - Epoch: [181][  230/  246]    Overall Loss 0.242822    Objective Loss 0.242822                                        LR 0.000005    Time 0.016690    
2023-01-06 16:48:57,960 - Epoch: [181][  240/  246]    Overall Loss 0.242177    Objective Loss 0.242177                                        LR 0.000005    Time 0.016683    
2023-01-06 16:48:58,026 - Epoch: [181][  246/  246]    Overall Loss 0.242214    Objective Loss 0.242214    Top1 90.430622    LR 0.000005    Time 0.016546    
2023-01-06 16:48:58,141 - --- validate (epoch=181)-----------
2023-01-06 16:48:58,141 - 6986 samples (256 per mini-batch)
2023-01-06 16:48:58,580 - Epoch: [181][   10/   28]    Loss 0.243150    Top1 91.523438    
2023-01-06 16:48:58,680 - Epoch: [181][   20/   28]    Loss 0.252638    Top1 91.035156    
2023-01-06 16:48:58,733 - Epoch: [181][   28/   28]    Loss 0.263787    Top1 90.552534    
2023-01-06 16:48:58,870 - ==> Top1: 90.553    Loss: 0.264

2023-01-06 16:48:58,871 - ==> Confusion:
[[ 232   15  192]
 [  14  255  333]
 [  50   56 5839]]

2023-01-06 16:48:58,872 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:48:58,872 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:48:58,879 - 

2023-01-06 16:48:58,879 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:48:59,538 - Epoch: [182][   10/  246]    Overall Loss 0.235034    Objective Loss 0.235034                                        LR 0.000005    Time 0.065795    
2023-01-06 16:48:59,670 - Epoch: [182][   20/  246]    Overall Loss 0.240455    Objective Loss 0.240455                                        LR 0.000005    Time 0.039483    
2023-01-06 16:48:59,791 - Epoch: [182][   30/  246]    Overall Loss 0.235827    Objective Loss 0.235827                                        LR 0.000005    Time 0.030330    
2023-01-06 16:48:59,911 - Epoch: [182][   40/  246]    Overall Loss 0.234030    Objective Loss 0.234030                                        LR 0.000005    Time 0.025732    
2023-01-06 16:49:00,031 - Epoch: [182][   50/  246]    Overall Loss 0.241659    Objective Loss 0.241659                                        LR 0.000005    Time 0.022992    
2023-01-06 16:49:00,152 - Epoch: [182][   60/  246]    Overall Loss 0.240950    Objective Loss 0.240950                                        LR 0.000005    Time 0.021164    
2023-01-06 16:49:00,270 - Epoch: [182][   70/  246]    Overall Loss 0.241470    Objective Loss 0.241470                                        LR 0.000005    Time 0.019818    
2023-01-06 16:49:00,393 - Epoch: [182][   80/  246]    Overall Loss 0.240094    Objective Loss 0.240094                                        LR 0.000005    Time 0.018872    
2023-01-06 16:49:00,510 - Epoch: [182][   90/  246]    Overall Loss 0.240013    Objective Loss 0.240013                                        LR 0.000005    Time 0.018079    
2023-01-06 16:49:00,633 - Epoch: [182][  100/  246]    Overall Loss 0.239899    Objective Loss 0.239899                                        LR 0.000005    Time 0.017495    
2023-01-06 16:49:00,751 - Epoch: [182][  110/  246]    Overall Loss 0.240035    Objective Loss 0.240035                                        LR 0.000005    Time 0.016973    
2023-01-06 16:49:00,874 - Epoch: [182][  120/  246]    Overall Loss 0.241352    Objective Loss 0.241352                                        LR 0.000005    Time 0.016575    
2023-01-06 16:49:00,997 - Epoch: [182][  130/  246]    Overall Loss 0.240836    Objective Loss 0.240836                                        LR 0.000005    Time 0.016246    
2023-01-06 16:49:01,131 - Epoch: [182][  140/  246]    Overall Loss 0.240238    Objective Loss 0.240238                                        LR 0.000005    Time 0.016039    
2023-01-06 16:49:01,257 - Epoch: [182][  150/  246]    Overall Loss 0.239380    Objective Loss 0.239380                                        LR 0.000005    Time 0.015804    
2023-01-06 16:49:01,390 - Epoch: [182][  160/  246]    Overall Loss 0.240167    Objective Loss 0.240167                                        LR 0.000005    Time 0.015648    
2023-01-06 16:49:01,523 - Epoch: [182][  170/  246]    Overall Loss 0.239859    Objective Loss 0.239859                                        LR 0.000005    Time 0.015505    
2023-01-06 16:49:01,659 - Epoch: [182][  180/  246]    Overall Loss 0.240023    Objective Loss 0.240023                                        LR 0.000005    Time 0.015397    
2023-01-06 16:49:01,814 - Epoch: [182][  190/  246]    Overall Loss 0.241261    Objective Loss 0.241261                                        LR 0.000005    Time 0.015404    
2023-01-06 16:49:01,959 - Epoch: [182][  200/  246]    Overall Loss 0.240953    Objective Loss 0.240953                                        LR 0.000005    Time 0.015353    
2023-01-06 16:49:02,086 - Epoch: [182][  210/  246]    Overall Loss 0.242035    Objective Loss 0.242035                                        LR 0.000005    Time 0.015224    
2023-01-06 16:49:02,215 - Epoch: [182][  220/  246]    Overall Loss 0.241503    Objective Loss 0.241503                                        LR 0.000005    Time 0.015117    
2023-01-06 16:49:02,342 - Epoch: [182][  230/  246]    Overall Loss 0.242571    Objective Loss 0.242571                                        LR 0.000005    Time 0.015011    
2023-01-06 16:49:02,481 - Epoch: [182][  240/  246]    Overall Loss 0.242175    Objective Loss 0.242175                                        LR 0.000005    Time 0.014965    
2023-01-06 16:49:02,542 - Epoch: [182][  246/  246]    Overall Loss 0.242578    Objective Loss 0.242578    Top1 92.105263    LR 0.000005    Time 0.014845    
2023-01-06 16:49:02,678 - --- validate (epoch=182)-----------
2023-01-06 16:49:02,678 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:03,106 - Epoch: [182][   10/   28]    Loss 0.260334    Top1 90.703125    
2023-01-06 16:49:03,201 - Epoch: [182][   20/   28]    Loss 0.261378    Top1 90.820312    
2023-01-06 16:49:03,252 - Epoch: [182][   28/   28]    Loss 0.261018    Top1 90.581162    
2023-01-06 16:49:03,383 - ==> Top1: 90.581    Loss: 0.261

2023-01-06 16:49:03,383 - ==> Confusion:
[[ 240   14  185]
 [  17  276  309]
 [  58   75 5812]]

2023-01-06 16:49:03,384 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:49:03,384 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:03,389 - 

2023-01-06 16:49:03,389 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:04,062 - Epoch: [183][   10/  246]    Overall Loss 0.243761    Objective Loss 0.243761                                        LR 0.000005    Time 0.067254    
2023-01-06 16:49:04,197 - Epoch: [183][   20/  246]    Overall Loss 0.242374    Objective Loss 0.242374                                        LR 0.000005    Time 0.040320    
2023-01-06 16:49:04,338 - Epoch: [183][   30/  246]    Overall Loss 0.241128    Objective Loss 0.241128                                        LR 0.000005    Time 0.031572    
2023-01-06 16:49:04,475 - Epoch: [183][   40/  246]    Overall Loss 0.242274    Objective Loss 0.242274                                        LR 0.000005    Time 0.027104    
2023-01-06 16:49:04,620 - Epoch: [183][   50/  246]    Overall Loss 0.246636    Objective Loss 0.246636                                        LR 0.000005    Time 0.024570    
2023-01-06 16:49:04,763 - Epoch: [183][   60/  246]    Overall Loss 0.246234    Objective Loss 0.246234                                        LR 0.000005    Time 0.022860    
2023-01-06 16:49:04,904 - Epoch: [183][   70/  246]    Overall Loss 0.245540    Objective Loss 0.245540                                        LR 0.000005    Time 0.021603    
2023-01-06 16:49:05,055 - Epoch: [183][   80/  246]    Overall Loss 0.245669    Objective Loss 0.245669                                        LR 0.000005    Time 0.020781    
2023-01-06 16:49:05,214 - Epoch: [183][   90/  246]    Overall Loss 0.243407    Objective Loss 0.243407                                        LR 0.000005    Time 0.020216    
2023-01-06 16:49:05,365 - Epoch: [183][  100/  246]    Overall Loss 0.244324    Objective Loss 0.244324                                        LR 0.000005    Time 0.019692    
2023-01-06 16:49:05,524 - Epoch: [183][  110/  246]    Overall Loss 0.245357    Objective Loss 0.245357                                        LR 0.000005    Time 0.019345    
2023-01-06 16:49:05,673 - Epoch: [183][  120/  246]    Overall Loss 0.243439    Objective Loss 0.243439                                        LR 0.000005    Time 0.018975    
2023-01-06 16:49:05,834 - Epoch: [183][  130/  246]    Overall Loss 0.242705    Objective Loss 0.242705                                        LR 0.000005    Time 0.018752    
2023-01-06 16:49:05,982 - Epoch: [183][  140/  246]    Overall Loss 0.244179    Objective Loss 0.244179                                        LR 0.000005    Time 0.018470    
2023-01-06 16:49:06,141 - Epoch: [183][  150/  246]    Overall Loss 0.244417    Objective Loss 0.244417                                        LR 0.000005    Time 0.018291    
2023-01-06 16:49:06,289 - Epoch: [183][  160/  246]    Overall Loss 0.244222    Objective Loss 0.244222                                        LR 0.000005    Time 0.018071    
2023-01-06 16:49:06,442 - Epoch: [183][  170/  246]    Overall Loss 0.244314    Objective Loss 0.244314                                        LR 0.000005    Time 0.017905    
2023-01-06 16:49:06,591 - Epoch: [183][  180/  246]    Overall Loss 0.244804    Objective Loss 0.244804                                        LR 0.000005    Time 0.017740    
2023-01-06 16:49:06,745 - Epoch: [183][  190/  246]    Overall Loss 0.244226    Objective Loss 0.244226                                        LR 0.000005    Time 0.017611    
2023-01-06 16:49:06,887 - Epoch: [183][  200/  246]    Overall Loss 0.243347    Objective Loss 0.243347                                        LR 0.000005    Time 0.017439    
2023-01-06 16:49:07,040 - Epoch: [183][  210/  246]    Overall Loss 0.243736    Objective Loss 0.243736                                        LR 0.000005    Time 0.017337    
2023-01-06 16:49:07,189 - Epoch: [183][  220/  246]    Overall Loss 0.244320    Objective Loss 0.244320                                        LR 0.000005    Time 0.017225    
2023-01-06 16:49:07,343 - Epoch: [183][  230/  246]    Overall Loss 0.243062    Objective Loss 0.243062                                        LR 0.000005    Time 0.017144    
2023-01-06 16:49:07,495 - Epoch: [183][  240/  246]    Overall Loss 0.242524    Objective Loss 0.242524                                        LR 0.000005    Time 0.017061    
2023-01-06 16:49:07,563 - Epoch: [183][  246/  246]    Overall Loss 0.242793    Objective Loss 0.242793    Top1 89.473684    LR 0.000005    Time 0.016921    
2023-01-06 16:49:07,704 - --- validate (epoch=183)-----------
2023-01-06 16:49:07,705 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:08,132 - Epoch: [183][   10/   28]    Loss 0.257820    Top1 91.289062    
2023-01-06 16:49:08,225 - Epoch: [183][   20/   28]    Loss 0.259161    Top1 90.937500    
2023-01-06 16:49:08,275 - Epoch: [183][   28/   28]    Loss 0.261706    Top1 90.695677    
2023-01-06 16:49:08,431 - ==> Top1: 90.696    Loss: 0.262

2023-01-06 16:49:08,431 - ==> Confusion:
[[ 243   16  180]
 [  16  274  312]
 [  53   73 5819]]

2023-01-06 16:49:08,432 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:49:08,432 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:08,437 - 

2023-01-06 16:49:08,437 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:08,960 - Epoch: [184][   10/  246]    Overall Loss 0.234892    Objective Loss 0.234892                                        LR 0.000005    Time 0.052233    
2023-01-06 16:49:09,093 - Epoch: [184][   20/  246]    Overall Loss 0.235314    Objective Loss 0.235314                                        LR 0.000005    Time 0.032739    
2023-01-06 16:49:09,231 - Epoch: [184][   30/  246]    Overall Loss 0.239789    Objective Loss 0.239789                                        LR 0.000005    Time 0.026422    
2023-01-06 16:49:09,376 - Epoch: [184][   40/  246]    Overall Loss 0.237010    Objective Loss 0.237010                                        LR 0.000005    Time 0.023418    
2023-01-06 16:49:09,522 - Epoch: [184][   50/  246]    Overall Loss 0.236334    Objective Loss 0.236334                                        LR 0.000005    Time 0.021641    
2023-01-06 16:49:09,677 - Epoch: [184][   60/  246]    Overall Loss 0.236457    Objective Loss 0.236457                                        LR 0.000005    Time 0.020619    
2023-01-06 16:49:09,827 - Epoch: [184][   70/  246]    Overall Loss 0.237973    Objective Loss 0.237973                                        LR 0.000005    Time 0.019803    
2023-01-06 16:49:09,979 - Epoch: [184][   80/  246]    Overall Loss 0.242261    Objective Loss 0.242261                                        LR 0.000005    Time 0.019231    
2023-01-06 16:49:10,124 - Epoch: [184][   90/  246]    Overall Loss 0.241846    Objective Loss 0.241846                                        LR 0.000005    Time 0.018693    
2023-01-06 16:49:10,275 - Epoch: [184][  100/  246]    Overall Loss 0.241930    Objective Loss 0.241930                                        LR 0.000005    Time 0.018334    
2023-01-06 16:49:10,425 - Epoch: [184][  110/  246]    Overall Loss 0.240931    Objective Loss 0.240931                                        LR 0.000005    Time 0.018024    
2023-01-06 16:49:10,575 - Epoch: [184][  120/  246]    Overall Loss 0.241128    Objective Loss 0.241128                                        LR 0.000005    Time 0.017770    
2023-01-06 16:49:10,721 - Epoch: [184][  130/  246]    Overall Loss 0.241522    Objective Loss 0.241522                                        LR 0.000005    Time 0.017525    
2023-01-06 16:49:10,868 - Epoch: [184][  140/  246]    Overall Loss 0.241762    Objective Loss 0.241762                                        LR 0.000005    Time 0.017317    
2023-01-06 16:49:11,011 - Epoch: [184][  150/  246]    Overall Loss 0.240936    Objective Loss 0.240936                                        LR 0.000005    Time 0.017114    
2023-01-06 16:49:11,158 - Epoch: [184][  160/  246]    Overall Loss 0.242706    Objective Loss 0.242706                                        LR 0.000005    Time 0.016960    
2023-01-06 16:49:11,309 - Epoch: [184][  170/  246]    Overall Loss 0.243039    Objective Loss 0.243039                                        LR 0.000005    Time 0.016845    
2023-01-06 16:49:11,463 - Epoch: [184][  180/  246]    Overall Loss 0.243016    Objective Loss 0.243016                                        LR 0.000005    Time 0.016764    
2023-01-06 16:49:11,616 - Epoch: [184][  190/  246]    Overall Loss 0.243874    Objective Loss 0.243874                                        LR 0.000005    Time 0.016684    
2023-01-06 16:49:11,771 - Epoch: [184][  200/  246]    Overall Loss 0.243893    Objective Loss 0.243893                                        LR 0.000005    Time 0.016625    
2023-01-06 16:49:11,924 - Epoch: [184][  210/  246]    Overall Loss 0.243505    Objective Loss 0.243505                                        LR 0.000005    Time 0.016560    
2023-01-06 16:49:12,076 - Epoch: [184][  220/  246]    Overall Loss 0.243413    Objective Loss 0.243413                                        LR 0.000005    Time 0.016496    
2023-01-06 16:49:12,230 - Epoch: [184][  230/  246]    Overall Loss 0.243073    Objective Loss 0.243073                                        LR 0.000005    Time 0.016446    
2023-01-06 16:49:12,392 - Epoch: [184][  240/  246]    Overall Loss 0.243207    Objective Loss 0.243207                                        LR 0.000005    Time 0.016431    
2023-01-06 16:49:12,460 - Epoch: [184][  246/  246]    Overall Loss 0.242913    Objective Loss 0.242913    Top1 91.387560    LR 0.000005    Time 0.016306    
2023-01-06 16:49:12,597 - --- validate (epoch=184)-----------
2023-01-06 16:49:12,598 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:13,029 - Epoch: [184][   10/   28]    Loss 0.240361    Top1 91.445312    
2023-01-06 16:49:13,127 - Epoch: [184][   20/   28]    Loss 0.253524    Top1 90.742188    
2023-01-06 16:49:13,180 - Epoch: [184][   28/   28]    Loss 0.262979    Top1 90.709991    
2023-01-06 16:49:13,341 - ==> Top1: 90.710    Loss: 0.263

2023-01-06 16:49:13,341 - ==> Confusion:
[[ 245   15  179]
 [  14  277  311]
 [  60   70 5815]]

2023-01-06 16:49:13,342 - ==> Best [Top1: 90.753   Sparsity:0.00   Params: 151104 on epoch: 164]
2023-01-06 16:49:13,342 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:13,347 - 

2023-01-06 16:49:13,348 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:14,027 - Epoch: [185][   10/  246]    Overall Loss 0.253849    Objective Loss 0.253849                                        LR 0.000005    Time 0.067851    
2023-01-06 16:49:14,159 - Epoch: [185][   20/  246]    Overall Loss 0.247362    Objective Loss 0.247362                                        LR 0.000005    Time 0.040529    
2023-01-06 16:49:14,290 - Epoch: [185][   30/  246]    Overall Loss 0.247530    Objective Loss 0.247530                                        LR 0.000005    Time 0.031362    
2023-01-06 16:49:14,419 - Epoch: [185][   40/  246]    Overall Loss 0.249547    Objective Loss 0.249547                                        LR 0.000005    Time 0.026749    
2023-01-06 16:49:14,562 - Epoch: [185][   50/  246]    Overall Loss 0.248380    Objective Loss 0.248380                                        LR 0.000005    Time 0.024252    
2023-01-06 16:49:14,704 - Epoch: [185][   60/  246]    Overall Loss 0.249882    Objective Loss 0.249882                                        LR 0.000005    Time 0.022570    
2023-01-06 16:49:14,850 - Epoch: [185][   70/  246]    Overall Loss 0.247074    Objective Loss 0.247074                                        LR 0.000005    Time 0.021415    
2023-01-06 16:49:14,999 - Epoch: [185][   80/  246]    Overall Loss 0.247792    Objective Loss 0.247792                                        LR 0.000005    Time 0.020603    
2023-01-06 16:49:15,150 - Epoch: [185][   90/  246]    Overall Loss 0.248832    Objective Loss 0.248832                                        LR 0.000005    Time 0.019986    
2023-01-06 16:49:15,298 - Epoch: [185][  100/  246]    Overall Loss 0.248610    Objective Loss 0.248610                                        LR 0.000005    Time 0.019464    
2023-01-06 16:49:15,449 - Epoch: [185][  110/  246]    Overall Loss 0.247816    Objective Loss 0.247816                                        LR 0.000005    Time 0.019057    
2023-01-06 16:49:15,595 - Epoch: [185][  120/  246]    Overall Loss 0.246905    Objective Loss 0.246905                                        LR 0.000005    Time 0.018684    
2023-01-06 16:49:15,745 - Epoch: [185][  130/  246]    Overall Loss 0.247571    Objective Loss 0.247571                                        LR 0.000005    Time 0.018397    
2023-01-06 16:49:15,891 - Epoch: [185][  140/  246]    Overall Loss 0.246480    Objective Loss 0.246480                                        LR 0.000005    Time 0.018123    
2023-01-06 16:49:16,034 - Epoch: [185][  150/  246]    Overall Loss 0.245800    Objective Loss 0.245800                                        LR 0.000005    Time 0.017866    
2023-01-06 16:49:16,171 - Epoch: [185][  160/  246]    Overall Loss 0.245256    Objective Loss 0.245256                                        LR 0.000005    Time 0.017602    
2023-01-06 16:49:16,309 - Epoch: [185][  170/  246]    Overall Loss 0.244699    Objective Loss 0.244699                                        LR 0.000005    Time 0.017379    
2023-01-06 16:49:16,447 - Epoch: [185][  180/  246]    Overall Loss 0.244628    Objective Loss 0.244628                                        LR 0.000005    Time 0.017177    
2023-01-06 16:49:16,582 - Epoch: [185][  190/  246]    Overall Loss 0.245066    Objective Loss 0.245066                                        LR 0.000005    Time 0.016982    
2023-01-06 16:49:16,719 - Epoch: [185][  200/  246]    Overall Loss 0.244553    Objective Loss 0.244553                                        LR 0.000005    Time 0.016815    
2023-01-06 16:49:16,857 - Epoch: [185][  210/  246]    Overall Loss 0.243269    Objective Loss 0.243269                                        LR 0.000005    Time 0.016670    
2023-01-06 16:49:16,995 - Epoch: [185][  220/  246]    Overall Loss 0.243621    Objective Loss 0.243621                                        LR 0.000005    Time 0.016536    
2023-01-06 16:49:17,132 - Epoch: [185][  230/  246]    Overall Loss 0.242922    Objective Loss 0.242922                                        LR 0.000005    Time 0.016409    
2023-01-06 16:49:17,285 - Epoch: [185][  240/  246]    Overall Loss 0.243016    Objective Loss 0.243016                                        LR 0.000005    Time 0.016362    
2023-01-06 16:49:17,352 - Epoch: [185][  246/  246]    Overall Loss 0.242619    Objective Loss 0.242619    Top1 91.387560    LR 0.000005    Time 0.016233    
2023-01-06 16:49:17,475 - --- validate (epoch=185)-----------
2023-01-06 16:49:17,476 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:17,908 - Epoch: [185][   10/   28]    Loss 0.250834    Top1 91.328125    
2023-01-06 16:49:18,001 - Epoch: [185][   20/   28]    Loss 0.257402    Top1 91.054688    
2023-01-06 16:49:18,051 - Epoch: [185][   28/   28]    Loss 0.262105    Top1 90.810192    
2023-01-06 16:49:18,192 - ==> Top1: 90.810    Loss: 0.262

2023-01-06 16:49:18,192 - ==> Confusion:
[[ 245   15  179]
 [  14  277  311]
 [  51   72 5822]]

2023-01-06 16:49:18,194 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:18,194 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:18,200 - 

2023-01-06 16:49:18,200 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:18,725 - Epoch: [186][   10/  246]    Overall Loss 0.222395    Objective Loss 0.222395                                        LR 0.000005    Time 0.052357    
2023-01-06 16:49:18,864 - Epoch: [186][   20/  246]    Overall Loss 0.218354    Objective Loss 0.218354                                        LR 0.000005    Time 0.033133    
2023-01-06 16:49:18,996 - Epoch: [186][   30/  246]    Overall Loss 0.222320    Objective Loss 0.222320                                        LR 0.000005    Time 0.026466    
2023-01-06 16:49:19,131 - Epoch: [186][   40/  246]    Overall Loss 0.229020    Objective Loss 0.229020                                        LR 0.000005    Time 0.023229    
2023-01-06 16:49:19,265 - Epoch: [186][   50/  246]    Overall Loss 0.226044    Objective Loss 0.226044                                        LR 0.000005    Time 0.021243    
2023-01-06 16:49:19,399 - Epoch: [186][   60/  246]    Overall Loss 0.227580    Objective Loss 0.227580                                        LR 0.000005    Time 0.019927    
2023-01-06 16:49:19,525 - Epoch: [186][   70/  246]    Overall Loss 0.231404    Objective Loss 0.231404                                        LR 0.000005    Time 0.018876    
2023-01-06 16:49:19,658 - Epoch: [186][   80/  246]    Overall Loss 0.232760    Objective Loss 0.232760                                        LR 0.000005    Time 0.018180    
2023-01-06 16:49:19,796 - Epoch: [186][   90/  246]    Overall Loss 0.233954    Objective Loss 0.233954                                        LR 0.000005    Time 0.017681    
2023-01-06 16:49:19,939 - Epoch: [186][  100/  246]    Overall Loss 0.234492    Objective Loss 0.234492                                        LR 0.000005    Time 0.017338    
2023-01-06 16:49:20,078 - Epoch: [186][  110/  246]    Overall Loss 0.235496    Objective Loss 0.235496                                        LR 0.000005    Time 0.017029    
2023-01-06 16:49:20,223 - Epoch: [186][  120/  246]    Overall Loss 0.236650    Objective Loss 0.236650                                        LR 0.000005    Time 0.016815    
2023-01-06 16:49:20,367 - Epoch: [186][  130/  246]    Overall Loss 0.236761    Objective Loss 0.236761                                        LR 0.000005    Time 0.016624    
2023-01-06 16:49:20,513 - Epoch: [186][  140/  246]    Overall Loss 0.238233    Objective Loss 0.238233                                        LR 0.000005    Time 0.016474    
2023-01-06 16:49:20,657 - Epoch: [186][  150/  246]    Overall Loss 0.238995    Objective Loss 0.238995                                        LR 0.000005    Time 0.016333    
2023-01-06 16:49:20,801 - Epoch: [186][  160/  246]    Overall Loss 0.239016    Objective Loss 0.239016                                        LR 0.000005    Time 0.016215    
2023-01-06 16:49:20,960 - Epoch: [186][  170/  246]    Overall Loss 0.239013    Objective Loss 0.239013                                        LR 0.000005    Time 0.016191    
2023-01-06 16:49:21,122 - Epoch: [186][  180/  246]    Overall Loss 0.239167    Objective Loss 0.239167                                        LR 0.000005    Time 0.016188    
2023-01-06 16:49:21,281 - Epoch: [186][  190/  246]    Overall Loss 0.239862    Objective Loss 0.239862                                        LR 0.000005    Time 0.016172    
2023-01-06 16:49:21,452 - Epoch: [186][  200/  246]    Overall Loss 0.241353    Objective Loss 0.241353                                        LR 0.000005    Time 0.016218    
2023-01-06 16:49:21,630 - Epoch: [186][  210/  246]    Overall Loss 0.241933    Objective Loss 0.241933                                        LR 0.000005    Time 0.016287    
2023-01-06 16:49:21,812 - Epoch: [186][  220/  246]    Overall Loss 0.241531    Objective Loss 0.241531                                        LR 0.000005    Time 0.016376    
2023-01-06 16:49:21,992 - Epoch: [186][  230/  246]    Overall Loss 0.241617    Objective Loss 0.241617                                        LR 0.000005    Time 0.016443    
2023-01-06 16:49:22,187 - Epoch: [186][  240/  246]    Overall Loss 0.241606    Objective Loss 0.241606                                        LR 0.000005    Time 0.016569    
2023-01-06 16:49:22,266 - Epoch: [186][  246/  246]    Overall Loss 0.241935    Objective Loss 0.241935    Top1 91.626794    LR 0.000005    Time 0.016484    
2023-01-06 16:49:22,411 - --- validate (epoch=186)-----------
2023-01-06 16:49:22,412 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:22,853 - Epoch: [186][   10/   28]    Loss 0.262087    Top1 90.820312    
2023-01-06 16:49:22,952 - Epoch: [186][   20/   28]    Loss 0.266894    Top1 90.390625    
2023-01-06 16:49:23,005 - Epoch: [186][   28/   28]    Loss 0.263618    Top1 90.695677    
2023-01-06 16:49:23,136 - ==> Top1: 90.696    Loss: 0.264

2023-01-06 16:49:23,136 - ==> Confusion:
[[ 241   15  183]
 [  14  262  326]
 [  54   58 5833]]

2023-01-06 16:49:23,138 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:23,138 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:23,143 - 

2023-01-06 16:49:23,143 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:23,824 - Epoch: [187][   10/  246]    Overall Loss 0.230647    Objective Loss 0.230647                                        LR 0.000005    Time 0.068042    
2023-01-06 16:49:23,967 - Epoch: [187][   20/  246]    Overall Loss 0.235292    Objective Loss 0.235292                                        LR 0.000005    Time 0.041141    
2023-01-06 16:49:24,099 - Epoch: [187][   30/  246]    Overall Loss 0.240566    Objective Loss 0.240566                                        LR 0.000005    Time 0.031816    
2023-01-06 16:49:24,238 - Epoch: [187][   40/  246]    Overall Loss 0.235215    Objective Loss 0.235215                                        LR 0.000005    Time 0.027331    
2023-01-06 16:49:24,376 - Epoch: [187][   50/  246]    Overall Loss 0.236740    Objective Loss 0.236740                                        LR 0.000005    Time 0.024626    
2023-01-06 16:49:24,517 - Epoch: [187][   60/  246]    Overall Loss 0.239566    Objective Loss 0.239566                                        LR 0.000005    Time 0.022851    
2023-01-06 16:49:24,653 - Epoch: [187][   70/  246]    Overall Loss 0.240880    Objective Loss 0.240880                                        LR 0.000005    Time 0.021536    
2023-01-06 16:49:24,787 - Epoch: [187][   80/  246]    Overall Loss 0.241151    Objective Loss 0.241151                                        LR 0.000005    Time 0.020508    
2023-01-06 16:49:24,922 - Epoch: [187][   90/  246]    Overall Loss 0.241829    Objective Loss 0.241829                                        LR 0.000005    Time 0.019730    
2023-01-06 16:49:25,061 - Epoch: [187][  100/  246]    Overall Loss 0.241018    Objective Loss 0.241018                                        LR 0.000005    Time 0.019135    
2023-01-06 16:49:25,203 - Epoch: [187][  110/  246]    Overall Loss 0.241729    Objective Loss 0.241729                                        LR 0.000005    Time 0.018685    
2023-01-06 16:49:25,343 - Epoch: [187][  120/  246]    Overall Loss 0.242307    Objective Loss 0.242307                                        LR 0.000005    Time 0.018293    
2023-01-06 16:49:25,483 - Epoch: [187][  130/  246]    Overall Loss 0.242350    Objective Loss 0.242350                                        LR 0.000005    Time 0.017961    
2023-01-06 16:49:25,623 - Epoch: [187][  140/  246]    Overall Loss 0.242978    Objective Loss 0.242978                                        LR 0.000005    Time 0.017678    
2023-01-06 16:49:25,766 - Epoch: [187][  150/  246]    Overall Loss 0.243372    Objective Loss 0.243372                                        LR 0.000005    Time 0.017447    
2023-01-06 16:49:25,912 - Epoch: [187][  160/  246]    Overall Loss 0.243685    Objective Loss 0.243685                                        LR 0.000005    Time 0.017265    
2023-01-06 16:49:26,054 - Epoch: [187][  170/  246]    Overall Loss 0.243432    Objective Loss 0.243432                                        LR 0.000005    Time 0.017086    
2023-01-06 16:49:26,194 - Epoch: [187][  180/  246]    Overall Loss 0.243202    Objective Loss 0.243202                                        LR 0.000005    Time 0.016910    
2023-01-06 16:49:26,333 - Epoch: [187][  190/  246]    Overall Loss 0.243219    Objective Loss 0.243219                                        LR 0.000005    Time 0.016751    
2023-01-06 16:49:26,474 - Epoch: [187][  200/  246]    Overall Loss 0.242927    Objective Loss 0.242927                                        LR 0.000005    Time 0.016613    
2023-01-06 16:49:26,612 - Epoch: [187][  210/  246]    Overall Loss 0.242789    Objective Loss 0.242789                                        LR 0.000005    Time 0.016480    
2023-01-06 16:49:26,753 - Epoch: [187][  220/  246]    Overall Loss 0.243001    Objective Loss 0.243001                                        LR 0.000005    Time 0.016372    
2023-01-06 16:49:26,894 - Epoch: [187][  230/  246]    Overall Loss 0.241696    Objective Loss 0.241696                                        LR 0.000005    Time 0.016271    
2023-01-06 16:49:27,046 - Epoch: [187][  240/  246]    Overall Loss 0.241649    Objective Loss 0.241649                                        LR 0.000005    Time 0.016223    
2023-01-06 16:49:27,111 - Epoch: [187][  246/  246]    Overall Loss 0.242182    Objective Loss 0.242182    Top1 91.387560    LR 0.000005    Time 0.016090    
2023-01-06 16:49:27,268 - --- validate (epoch=187)-----------
2023-01-06 16:49:27,268 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:27,712 - Epoch: [187][   10/   28]    Loss 0.284307    Top1 88.984375    
2023-01-06 16:49:27,805 - Epoch: [187][   20/   28]    Loss 0.270246    Top1 90.156250    
2023-01-06 16:49:27,856 - Epoch: [187][   28/   28]    Loss 0.259862    Top1 90.538219    
2023-01-06 16:49:28,013 - ==> Top1: 90.538    Loss: 0.260

2023-01-06 16:49:28,013 - ==> Confusion:
[[ 233   14  192]
 [  17  271  314]
 [  53   71 5821]]

2023-01-06 16:49:28,014 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:28,014 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:28,019 - 

2023-01-06 16:49:28,019 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:28,564 - Epoch: [188][   10/  246]    Overall Loss 0.246592    Objective Loss 0.246592                                        LR 0.000005    Time 0.054445    
2023-01-06 16:49:28,718 - Epoch: [188][   20/  246]    Overall Loss 0.236045    Objective Loss 0.236045                                        LR 0.000005    Time 0.034891    
2023-01-06 16:49:28,867 - Epoch: [188][   30/  246]    Overall Loss 0.250871    Objective Loss 0.250871                                        LR 0.000005    Time 0.028220    
2023-01-06 16:49:29,013 - Epoch: [188][   40/  246]    Overall Loss 0.254958    Objective Loss 0.254958                                        LR 0.000005    Time 0.024787    
2023-01-06 16:49:29,158 - Epoch: [188][   50/  246]    Overall Loss 0.252681    Objective Loss 0.252681                                        LR 0.000005    Time 0.022723    
2023-01-06 16:49:29,304 - Epoch: [188][   60/  246]    Overall Loss 0.252365    Objective Loss 0.252365                                        LR 0.000005    Time 0.021357    
2023-01-06 16:49:29,449 - Epoch: [188][   70/  246]    Overall Loss 0.249416    Objective Loss 0.249416                                        LR 0.000005    Time 0.020373    
2023-01-06 16:49:29,598 - Epoch: [188][   80/  246]    Overall Loss 0.249115    Objective Loss 0.249115                                        LR 0.000005    Time 0.019687    
2023-01-06 16:49:29,740 - Epoch: [188][   90/  246]    Overall Loss 0.247977    Objective Loss 0.247977                                        LR 0.000005    Time 0.019075    
2023-01-06 16:49:29,883 - Epoch: [188][  100/  246]    Overall Loss 0.247599    Objective Loss 0.247599                                        LR 0.000005    Time 0.018587    
2023-01-06 16:49:30,026 - Epoch: [188][  110/  246]    Overall Loss 0.248406    Objective Loss 0.248406                                        LR 0.000005    Time 0.018198    
2023-01-06 16:49:30,168 - Epoch: [188][  120/  246]    Overall Loss 0.246593    Objective Loss 0.246593                                        LR 0.000005    Time 0.017863    
2023-01-06 16:49:30,312 - Epoch: [188][  130/  246]    Overall Loss 0.245940    Objective Loss 0.245940                                        LR 0.000005    Time 0.017588    
2023-01-06 16:49:30,460 - Epoch: [188][  140/  246]    Overall Loss 0.246952    Objective Loss 0.246952                                        LR 0.000005    Time 0.017386    
2023-01-06 16:49:30,616 - Epoch: [188][  150/  246]    Overall Loss 0.246545    Objective Loss 0.246545                                        LR 0.000005    Time 0.017264    
2023-01-06 16:49:30,781 - Epoch: [188][  160/  246]    Overall Loss 0.245902    Objective Loss 0.245902                                        LR 0.000005    Time 0.017217    
2023-01-06 16:49:30,944 - Epoch: [188][  170/  246]    Overall Loss 0.244113    Objective Loss 0.244113                                        LR 0.000005    Time 0.017157    
2023-01-06 16:49:31,100 - Epoch: [188][  180/  246]    Overall Loss 0.242978    Objective Loss 0.242978                                        LR 0.000005    Time 0.017070    
2023-01-06 16:49:31,257 - Epoch: [188][  190/  246]    Overall Loss 0.243622    Objective Loss 0.243622                                        LR 0.000005    Time 0.016993    
2023-01-06 16:49:31,419 - Epoch: [188][  200/  246]    Overall Loss 0.242939    Objective Loss 0.242939                                        LR 0.000005    Time 0.016954    
2023-01-06 16:49:31,577 - Epoch: [188][  210/  246]    Overall Loss 0.242319    Objective Loss 0.242319                                        LR 0.000005    Time 0.016895    
2023-01-06 16:49:31,736 - Epoch: [188][  220/  246]    Overall Loss 0.242055    Objective Loss 0.242055                                        LR 0.000005    Time 0.016849    
2023-01-06 16:49:31,897 - Epoch: [188][  230/  246]    Overall Loss 0.242684    Objective Loss 0.242684                                        LR 0.000005    Time 0.016817    
2023-01-06 16:49:32,071 - Epoch: [188][  240/  246]    Overall Loss 0.242242    Objective Loss 0.242242                                        LR 0.000005    Time 0.016838    
2023-01-06 16:49:32,138 - Epoch: [188][  246/  246]    Overall Loss 0.242193    Objective Loss 0.242193    Top1 91.866029    LR 0.000005    Time 0.016700    
2023-01-06 16:49:32,276 - --- validate (epoch=188)-----------
2023-01-06 16:49:32,276 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:32,694 - Epoch: [188][   10/   28]    Loss 0.252632    Top1 91.328125    
2023-01-06 16:49:32,800 - Epoch: [188][   20/   28]    Loss 0.252862    Top1 91.074219    
2023-01-06 16:49:32,853 - Epoch: [188][   28/   28]    Loss 0.258743    Top1 90.681363    
2023-01-06 16:49:32,992 - ==> Top1: 90.681    Loss: 0.259

2023-01-06 16:49:32,993 - ==> Confusion:
[[ 242   14  183]
 [  14  279  309]
 [  60   71 5814]]

2023-01-06 16:49:32,994 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:32,994 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:32,999 - 

2023-01-06 16:49:32,999 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:33,660 - Epoch: [189][   10/  246]    Overall Loss 0.235949    Objective Loss 0.235949                                        LR 0.000005    Time 0.065992    
2023-01-06 16:49:33,796 - Epoch: [189][   20/  246]    Overall Loss 0.240041    Objective Loss 0.240041                                        LR 0.000005    Time 0.039806    
2023-01-06 16:49:33,932 - Epoch: [189][   30/  246]    Overall Loss 0.236464    Objective Loss 0.236464                                        LR 0.000005    Time 0.031038    
2023-01-06 16:49:34,069 - Epoch: [189][   40/  246]    Overall Loss 0.242915    Objective Loss 0.242915                                        LR 0.000005    Time 0.026700    
2023-01-06 16:49:34,207 - Epoch: [189][   50/  246]    Overall Loss 0.240910    Objective Loss 0.240910                                        LR 0.000005    Time 0.024114    
2023-01-06 16:49:34,343 - Epoch: [189][   60/  246]    Overall Loss 0.239309    Objective Loss 0.239309                                        LR 0.000005    Time 0.022359    
2023-01-06 16:49:34,474 - Epoch: [189][   70/  246]    Overall Loss 0.240860    Objective Loss 0.240860                                        LR 0.000005    Time 0.021021    
2023-01-06 16:49:34,612 - Epoch: [189][   80/  246]    Overall Loss 0.241123    Objective Loss 0.241123                                        LR 0.000005    Time 0.020122    
2023-01-06 16:49:34,749 - Epoch: [189][   90/  246]    Overall Loss 0.239520    Objective Loss 0.239520                                        LR 0.000005    Time 0.019400    
2023-01-06 16:49:34,888 - Epoch: [189][  100/  246]    Overall Loss 0.240421    Objective Loss 0.240421                                        LR 0.000005    Time 0.018848    
2023-01-06 16:49:35,025 - Epoch: [189][  110/  246]    Overall Loss 0.241589    Objective Loss 0.241589                                        LR 0.000005    Time 0.018375    
2023-01-06 16:49:35,163 - Epoch: [189][  120/  246]    Overall Loss 0.240478    Objective Loss 0.240478                                        LR 0.000005    Time 0.017993    
2023-01-06 16:49:35,296 - Epoch: [189][  130/  246]    Overall Loss 0.239725    Objective Loss 0.239725                                        LR 0.000005    Time 0.017632    
2023-01-06 16:49:35,433 - Epoch: [189][  140/  246]    Overall Loss 0.240675    Objective Loss 0.240675                                        LR 0.000005    Time 0.017345    
2023-01-06 16:49:35,568 - Epoch: [189][  150/  246]    Overall Loss 0.239229    Objective Loss 0.239229                                        LR 0.000005    Time 0.017086    
2023-01-06 16:49:35,705 - Epoch: [189][  160/  246]    Overall Loss 0.240079    Objective Loss 0.240079                                        LR 0.000005    Time 0.016876    
2023-01-06 16:49:35,845 - Epoch: [189][  170/  246]    Overall Loss 0.240022    Objective Loss 0.240022                                        LR 0.000005    Time 0.016701    
2023-01-06 16:49:35,984 - Epoch: [189][  180/  246]    Overall Loss 0.240098    Objective Loss 0.240098                                        LR 0.000005    Time 0.016545    
2023-01-06 16:49:36,128 - Epoch: [189][  190/  246]    Overall Loss 0.240676    Objective Loss 0.240676                                        LR 0.000005    Time 0.016432    
2023-01-06 16:49:36,265 - Epoch: [189][  200/  246]    Overall Loss 0.240606    Objective Loss 0.240606                                        LR 0.000005    Time 0.016294    
2023-01-06 16:49:36,408 - Epoch: [189][  210/  246]    Overall Loss 0.241262    Objective Loss 0.241262                                        LR 0.000005    Time 0.016198    
2023-01-06 16:49:36,553 - Epoch: [189][  220/  246]    Overall Loss 0.241385    Objective Loss 0.241385                                        LR 0.000005    Time 0.016116    
2023-01-06 16:49:36,698 - Epoch: [189][  230/  246]    Overall Loss 0.241402    Objective Loss 0.241402                                        LR 0.000005    Time 0.016047    
2023-01-06 16:49:36,845 - Epoch: [189][  240/  246]    Overall Loss 0.241616    Objective Loss 0.241616                                        LR 0.000005    Time 0.015990    
2023-01-06 16:49:36,911 - Epoch: [189][  246/  246]    Overall Loss 0.241943    Objective Loss 0.241943    Top1 92.105263    LR 0.000005    Time 0.015866    
2023-01-06 16:49:37,056 - --- validate (epoch=189)-----------
2023-01-06 16:49:37,056 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:37,481 - Epoch: [189][   10/   28]    Loss 0.245970    Top1 91.210938    
2023-01-06 16:49:37,580 - Epoch: [189][   20/   28]    Loss 0.258668    Top1 90.664062    
2023-01-06 16:49:37,631 - Epoch: [189][   28/   28]    Loss 0.259717    Top1 90.581162    
2023-01-06 16:49:37,769 - ==> Top1: 90.581    Loss: 0.260

2023-01-06 16:49:37,769 - ==> Confusion:
[[ 237   15  187]
 [  17  270  315]
 [  56   68 5821]]

2023-01-06 16:49:37,771 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:37,771 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:37,776 - 

2023-01-06 16:49:37,776 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:38,455 - Epoch: [190][   10/  246]    Overall Loss 0.234788    Objective Loss 0.234788                                        LR 0.000003    Time 0.067817    
2023-01-06 16:49:38,600 - Epoch: [190][   20/  246]    Overall Loss 0.245452    Objective Loss 0.245452                                        LR 0.000003    Time 0.041144    
2023-01-06 16:49:38,742 - Epoch: [190][   30/  246]    Overall Loss 0.245730    Objective Loss 0.245730                                        LR 0.000003    Time 0.032155    
2023-01-06 16:49:38,880 - Epoch: [190][   40/  246]    Overall Loss 0.250126    Objective Loss 0.250126                                        LR 0.000003    Time 0.027531    
2023-01-06 16:49:39,008 - Epoch: [190][   50/  246]    Overall Loss 0.246720    Objective Loss 0.246720                                        LR 0.000003    Time 0.024575    
2023-01-06 16:49:39,129 - Epoch: [190][   60/  246]    Overall Loss 0.245421    Objective Loss 0.245421                                        LR 0.000003    Time 0.022482    
2023-01-06 16:49:39,259 - Epoch: [190][   70/  246]    Overall Loss 0.244872    Objective Loss 0.244872                                        LR 0.000003    Time 0.021126    
2023-01-06 16:49:39,377 - Epoch: [190][   80/  246]    Overall Loss 0.244979    Objective Loss 0.244979                                        LR 0.000003    Time 0.019950    
2023-01-06 16:49:39,523 - Epoch: [190][   90/  246]    Overall Loss 0.244271    Objective Loss 0.244271                                        LR 0.000003    Time 0.019359    
2023-01-06 16:49:39,677 - Epoch: [190][  100/  246]    Overall Loss 0.242628    Objective Loss 0.242628                                        LR 0.000003    Time 0.018959    
2023-01-06 16:49:39,829 - Epoch: [190][  110/  246]    Overall Loss 0.242113    Objective Loss 0.242113                                        LR 0.000003    Time 0.018609    
2023-01-06 16:49:39,977 - Epoch: [190][  120/  246]    Overall Loss 0.242440    Objective Loss 0.242440                                        LR 0.000003    Time 0.018292    
2023-01-06 16:49:40,127 - Epoch: [190][  130/  246]    Overall Loss 0.241773    Objective Loss 0.241773                                        LR 0.000003    Time 0.018036    
2023-01-06 16:49:40,272 - Epoch: [190][  140/  246]    Overall Loss 0.240648    Objective Loss 0.240648                                        LR 0.000003    Time 0.017783    
2023-01-06 16:49:40,421 - Epoch: [190][  150/  246]    Overall Loss 0.240795    Objective Loss 0.240795                                        LR 0.000003    Time 0.017574    
2023-01-06 16:49:40,564 - Epoch: [190][  160/  246]    Overall Loss 0.238988    Objective Loss 0.238988                                        LR 0.000003    Time 0.017369    
2023-01-06 16:49:40,718 - Epoch: [190][  170/  246]    Overall Loss 0.239060    Objective Loss 0.239060                                        LR 0.000003    Time 0.017250    
2023-01-06 16:49:40,862 - Epoch: [190][  180/  246]    Overall Loss 0.240638    Objective Loss 0.240638                                        LR 0.000003    Time 0.017088    
2023-01-06 16:49:41,015 - Epoch: [190][  190/  246]    Overall Loss 0.241426    Objective Loss 0.241426                                        LR 0.000003    Time 0.016991    
2023-01-06 16:49:41,165 - Epoch: [190][  200/  246]    Overall Loss 0.241454    Objective Loss 0.241454                                        LR 0.000003    Time 0.016892    
2023-01-06 16:49:41,322 - Epoch: [190][  210/  246]    Overall Loss 0.241026    Objective Loss 0.241026                                        LR 0.000003    Time 0.016835    
2023-01-06 16:49:41,473 - Epoch: [190][  220/  246]    Overall Loss 0.240089    Objective Loss 0.240089                                        LR 0.000003    Time 0.016753    
2023-01-06 16:49:41,627 - Epoch: [190][  230/  246]    Overall Loss 0.240661    Objective Loss 0.240661                                        LR 0.000003    Time 0.016692    
2023-01-06 16:49:41,788 - Epoch: [190][  240/  246]    Overall Loss 0.240799    Objective Loss 0.240799                                        LR 0.000003    Time 0.016666    
2023-01-06 16:49:41,856 - Epoch: [190][  246/  246]    Overall Loss 0.241046    Objective Loss 0.241046    Top1 91.626794    LR 0.000003    Time 0.016536    
2023-01-06 16:49:42,018 - --- validate (epoch=190)-----------
2023-01-06 16:49:42,018 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:42,456 - Epoch: [190][   10/   28]    Loss 0.261571    Top1 90.625000    
2023-01-06 16:49:42,560 - Epoch: [190][   20/   28]    Loss 0.254874    Top1 90.937500    
2023-01-06 16:49:42,612 - Epoch: [190][   28/   28]    Loss 0.258010    Top1 90.709991    
2023-01-06 16:49:42,735 - ==> Top1: 90.710    Loss: 0.258

2023-01-06 16:49:42,736 - ==> Confusion:
[[ 236   15  188]
 [  14  276  312]
 [  49   71 5825]]

2023-01-06 16:49:42,737 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:42,737 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:42,742 - 

2023-01-06 16:49:42,742 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:43,279 - Epoch: [191][   10/  246]    Overall Loss 0.231050    Objective Loss 0.231050                                        LR 0.000003    Time 0.053583    
2023-01-06 16:49:43,424 - Epoch: [191][   20/  246]    Overall Loss 0.233313    Objective Loss 0.233313                                        LR 0.000003    Time 0.034045    
2023-01-06 16:49:43,568 - Epoch: [191][   30/  246]    Overall Loss 0.236463    Objective Loss 0.236463                                        LR 0.000003    Time 0.027473    
2023-01-06 16:49:43,706 - Epoch: [191][   40/  246]    Overall Loss 0.241202    Objective Loss 0.241202                                        LR 0.000003    Time 0.024053    
2023-01-06 16:49:43,852 - Epoch: [191][   50/  246]    Overall Loss 0.243286    Objective Loss 0.243286                                        LR 0.000003    Time 0.022155    
2023-01-06 16:49:43,996 - Epoch: [191][   60/  246]    Overall Loss 0.241093    Objective Loss 0.241093                                        LR 0.000003    Time 0.020852    
2023-01-06 16:49:44,137 - Epoch: [191][   70/  246]    Overall Loss 0.240896    Objective Loss 0.240896                                        LR 0.000003    Time 0.019876    
2023-01-06 16:49:44,275 - Epoch: [191][   80/  246]    Overall Loss 0.242384    Objective Loss 0.242384                                        LR 0.000003    Time 0.019115    
2023-01-06 16:49:44,414 - Epoch: [191][   90/  246]    Overall Loss 0.242692    Objective Loss 0.242692                                        LR 0.000003    Time 0.018527    
2023-01-06 16:49:44,554 - Epoch: [191][  100/  246]    Overall Loss 0.241885    Objective Loss 0.241885                                        LR 0.000003    Time 0.018073    
2023-01-06 16:49:44,695 - Epoch: [191][  110/  246]    Overall Loss 0.241947    Objective Loss 0.241947                                        LR 0.000003    Time 0.017713    
2023-01-06 16:49:44,833 - Epoch: [191][  120/  246]    Overall Loss 0.241182    Objective Loss 0.241182                                        LR 0.000003    Time 0.017384    
2023-01-06 16:49:44,969 - Epoch: [191][  130/  246]    Overall Loss 0.240194    Objective Loss 0.240194                                        LR 0.000003    Time 0.017084    
2023-01-06 16:49:45,106 - Epoch: [191][  140/  246]    Overall Loss 0.241139    Objective Loss 0.241139                                        LR 0.000003    Time 0.016843    
2023-01-06 16:49:45,241 - Epoch: [191][  150/  246]    Overall Loss 0.241684    Objective Loss 0.241684                                        LR 0.000003    Time 0.016615    
2023-01-06 16:49:45,376 - Epoch: [191][  160/  246]    Overall Loss 0.241882    Objective Loss 0.241882                                        LR 0.000003    Time 0.016418    
2023-01-06 16:49:45,511 - Epoch: [191][  170/  246]    Overall Loss 0.241937    Objective Loss 0.241937                                        LR 0.000003    Time 0.016243    
2023-01-06 16:49:45,646 - Epoch: [191][  180/  246]    Overall Loss 0.242108    Objective Loss 0.242108                                        LR 0.000003    Time 0.016089    
2023-01-06 16:49:45,779 - Epoch: [191][  190/  246]    Overall Loss 0.242308    Objective Loss 0.242308                                        LR 0.000003    Time 0.015939    
2023-01-06 16:49:45,912 - Epoch: [191][  200/  246]    Overall Loss 0.242393    Objective Loss 0.242393                                        LR 0.000003    Time 0.015804    
2023-01-06 16:49:46,043 - Epoch: [191][  210/  246]    Overall Loss 0.241094    Objective Loss 0.241094                                        LR 0.000003    Time 0.015675    
2023-01-06 16:49:46,175 - Epoch: [191][  220/  246]    Overall Loss 0.240446    Objective Loss 0.240446                                        LR 0.000003    Time 0.015558    
2023-01-06 16:49:46,308 - Epoch: [191][  230/  246]    Overall Loss 0.240528    Objective Loss 0.240528                                        LR 0.000003    Time 0.015459    
2023-01-06 16:49:46,457 - Epoch: [191][  240/  246]    Overall Loss 0.240296    Objective Loss 0.240296                                        LR 0.000003    Time 0.015433    
2023-01-06 16:49:46,521 - Epoch: [191][  246/  246]    Overall Loss 0.241012    Objective Loss 0.241012    Top1 89.473684    LR 0.000003    Time 0.015318    
2023-01-06 16:49:46,656 - --- validate (epoch=191)-----------
2023-01-06 16:49:46,656 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:47,097 - Epoch: [191][   10/   28]    Loss 0.266397    Top1 90.468750    
2023-01-06 16:49:47,193 - Epoch: [191][   20/   28]    Loss 0.264020    Top1 90.332031    
2023-01-06 16:49:47,244 - Epoch: [191][   28/   28]    Loss 0.258457    Top1 90.695677    
2023-01-06 16:49:47,388 - ==> Top1: 90.696    Loss: 0.258

2023-01-06 16:49:47,388 - ==> Confusion:
[[ 243   15  181]
 [  14  277  311]
 [  55   74 5816]]

2023-01-06 16:49:47,389 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:47,389 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:47,394 - 

2023-01-06 16:49:47,394 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:48,048 - Epoch: [192][   10/  246]    Overall Loss 0.241366    Objective Loss 0.241366                                        LR 0.000003    Time 0.065326    
2023-01-06 16:49:48,191 - Epoch: [192][   20/  246]    Overall Loss 0.234274    Objective Loss 0.234274                                        LR 0.000003    Time 0.039784    
2023-01-06 16:49:48,338 - Epoch: [192][   30/  246]    Overall Loss 0.240114    Objective Loss 0.240114                                        LR 0.000003    Time 0.031416    
2023-01-06 16:49:48,482 - Epoch: [192][   40/  246]    Overall Loss 0.237926    Objective Loss 0.237926                                        LR 0.000003    Time 0.027146    
2023-01-06 16:49:48,625 - Epoch: [192][   50/  246]    Overall Loss 0.240060    Objective Loss 0.240060                                        LR 0.000003    Time 0.024574    
2023-01-06 16:49:48,767 - Epoch: [192][   60/  246]    Overall Loss 0.244571    Objective Loss 0.244571                                        LR 0.000003    Time 0.022830    
2023-01-06 16:49:48,902 - Epoch: [192][   70/  246]    Overall Loss 0.242028    Objective Loss 0.242028                                        LR 0.000003    Time 0.021494    
2023-01-06 16:49:49,040 - Epoch: [192][   80/  246]    Overall Loss 0.241575    Objective Loss 0.241575                                        LR 0.000003    Time 0.020526    
2023-01-06 16:49:49,176 - Epoch: [192][   90/  246]    Overall Loss 0.240863    Objective Loss 0.240863                                        LR 0.000003    Time 0.019753    
2023-01-06 16:49:49,313 - Epoch: [192][  100/  246]    Overall Loss 0.241293    Objective Loss 0.241293                                        LR 0.000003    Time 0.019143    
2023-01-06 16:49:49,450 - Epoch: [192][  110/  246]    Overall Loss 0.240931    Objective Loss 0.240931                                        LR 0.000003    Time 0.018641    
2023-01-06 16:49:49,582 - Epoch: [192][  120/  246]    Overall Loss 0.241600    Objective Loss 0.241600                                        LR 0.000003    Time 0.018183    
2023-01-06 16:49:49,716 - Epoch: [192][  130/  246]    Overall Loss 0.242855    Objective Loss 0.242855                                        LR 0.000003    Time 0.017817    
2023-01-06 16:49:49,850 - Epoch: [192][  140/  246]    Overall Loss 0.242759    Objective Loss 0.242759                                        LR 0.000003    Time 0.017498    
2023-01-06 16:49:49,983 - Epoch: [192][  150/  246]    Overall Loss 0.242051    Objective Loss 0.242051                                        LR 0.000003    Time 0.017213    
2023-01-06 16:49:50,116 - Epoch: [192][  160/  246]    Overall Loss 0.242626    Objective Loss 0.242626                                        LR 0.000003    Time 0.016967    
2023-01-06 16:49:50,248 - Epoch: [192][  170/  246]    Overall Loss 0.241696    Objective Loss 0.241696                                        LR 0.000003    Time 0.016739    
2023-01-06 16:49:50,384 - Epoch: [192][  180/  246]    Overall Loss 0.240771    Objective Loss 0.240771                                        LR 0.000003    Time 0.016562    
2023-01-06 16:49:50,525 - Epoch: [192][  190/  246]    Overall Loss 0.240927    Objective Loss 0.240927                                        LR 0.000003    Time 0.016434    
2023-01-06 16:49:50,662 - Epoch: [192][  200/  246]    Overall Loss 0.240810    Objective Loss 0.240810                                        LR 0.000003    Time 0.016291    
2023-01-06 16:49:50,803 - Epoch: [192][  210/  246]    Overall Loss 0.241390    Objective Loss 0.241390                                        LR 0.000003    Time 0.016185    
2023-01-06 16:49:50,943 - Epoch: [192][  220/  246]    Overall Loss 0.241741    Objective Loss 0.241741                                        LR 0.000003    Time 0.016085    
2023-01-06 16:49:51,080 - Epoch: [192][  230/  246]    Overall Loss 0.241537    Objective Loss 0.241537                                        LR 0.000003    Time 0.015981    
2023-01-06 16:49:51,234 - Epoch: [192][  240/  246]    Overall Loss 0.241730    Objective Loss 0.241730                                        LR 0.000003    Time 0.015954    
2023-01-06 16:49:51,299 - Epoch: [192][  246/  246]    Overall Loss 0.241333    Objective Loss 0.241333    Top1 94.019139    LR 0.000003    Time 0.015829    
2023-01-06 16:49:51,439 - --- validate (epoch=192)-----------
2023-01-06 16:49:51,440 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:51,863 - Epoch: [192][   10/   28]    Loss 0.254092    Top1 91.171875    
2023-01-06 16:49:51,959 - Epoch: [192][   20/   28]    Loss 0.261094    Top1 90.546875    
2023-01-06 16:49:52,010 - Epoch: [192][   28/   28]    Loss 0.259086    Top1 90.667048    
2023-01-06 16:49:52,147 - ==> Top1: 90.667    Loss: 0.259

2023-01-06 16:49:52,148 - ==> Confusion:
[[ 241   15  183]
 [  13  272  317]
 [  53   71 5821]]

2023-01-06 16:49:52,149 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:52,149 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:52,154 - 

2023-01-06 16:49:52,155 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:52,815 - Epoch: [193][   10/  246]    Overall Loss 0.227683    Objective Loss 0.227683                                        LR 0.000003    Time 0.065968    
2023-01-06 16:49:52,954 - Epoch: [193][   20/  246]    Overall Loss 0.231646    Objective Loss 0.231646                                        LR 0.000003    Time 0.039927    
2023-01-06 16:49:53,090 - Epoch: [193][   30/  246]    Overall Loss 0.230706    Objective Loss 0.230706                                        LR 0.000003    Time 0.031117    
2023-01-06 16:49:53,226 - Epoch: [193][   40/  246]    Overall Loss 0.234018    Objective Loss 0.234018                                        LR 0.000003    Time 0.026739    
2023-01-06 16:49:53,362 - Epoch: [193][   50/  246]    Overall Loss 0.231345    Objective Loss 0.231345                                        LR 0.000003    Time 0.024103    
2023-01-06 16:49:53,497 - Epoch: [193][   60/  246]    Overall Loss 0.235050    Objective Loss 0.235050                                        LR 0.000003    Time 0.022328    
2023-01-06 16:49:53,632 - Epoch: [193][   70/  246]    Overall Loss 0.235961    Objective Loss 0.235961                                        LR 0.000003    Time 0.021060    
2023-01-06 16:49:53,767 - Epoch: [193][   80/  246]    Overall Loss 0.236330    Objective Loss 0.236330                                        LR 0.000003    Time 0.020117    
2023-01-06 16:49:53,903 - Epoch: [193][   90/  246]    Overall Loss 0.237009    Objective Loss 0.237009                                        LR 0.000003    Time 0.019382    
2023-01-06 16:49:54,038 - Epoch: [193][  100/  246]    Overall Loss 0.236766    Objective Loss 0.236766                                        LR 0.000003    Time 0.018792    
2023-01-06 16:49:54,170 - Epoch: [193][  110/  246]    Overall Loss 0.235645    Objective Loss 0.235645                                        LR 0.000003    Time 0.018283    
2023-01-06 16:49:54,301 - Epoch: [193][  120/  246]    Overall Loss 0.237613    Objective Loss 0.237613                                        LR 0.000003    Time 0.017844    
2023-01-06 16:49:54,434 - Epoch: [193][  130/  246]    Overall Loss 0.238480    Objective Loss 0.238480                                        LR 0.000003    Time 0.017489    
2023-01-06 16:49:54,566 - Epoch: [193][  140/  246]    Overall Loss 0.239662    Objective Loss 0.239662                                        LR 0.000003    Time 0.017181    
2023-01-06 16:49:54,696 - Epoch: [193][  150/  246]    Overall Loss 0.239256    Objective Loss 0.239256                                        LR 0.000003    Time 0.016900    
2023-01-06 16:49:54,835 - Epoch: [193][  160/  246]    Overall Loss 0.239629    Objective Loss 0.239629                                        LR 0.000003    Time 0.016714    
2023-01-06 16:49:54,975 - Epoch: [193][  170/  246]    Overall Loss 0.239735    Objective Loss 0.239735                                        LR 0.000003    Time 0.016550    
2023-01-06 16:49:55,116 - Epoch: [193][  180/  246]    Overall Loss 0.240669    Objective Loss 0.240669                                        LR 0.000003    Time 0.016402    
2023-01-06 16:49:55,255 - Epoch: [193][  190/  246]    Overall Loss 0.240008    Objective Loss 0.240008                                        LR 0.000003    Time 0.016268    
2023-01-06 16:49:55,394 - Epoch: [193][  200/  246]    Overall Loss 0.239505    Objective Loss 0.239505                                        LR 0.000003    Time 0.016148    
2023-01-06 16:49:55,531 - Epoch: [193][  210/  246]    Overall Loss 0.240924    Objective Loss 0.240924                                        LR 0.000003    Time 0.016033    
2023-01-06 16:49:55,671 - Epoch: [193][  220/  246]    Overall Loss 0.241328    Objective Loss 0.241328                                        LR 0.000003    Time 0.015938    
2023-01-06 16:49:55,808 - Epoch: [193][  230/  246]    Overall Loss 0.242274    Objective Loss 0.242274                                        LR 0.000003    Time 0.015840    
2023-01-06 16:49:55,963 - Epoch: [193][  240/  246]    Overall Loss 0.241720    Objective Loss 0.241720                                        LR 0.000003    Time 0.015822    
2023-01-06 16:49:56,030 - Epoch: [193][  246/  246]    Overall Loss 0.241371    Objective Loss 0.241371    Top1 91.866029    LR 0.000003    Time 0.015707    
2023-01-06 16:49:56,165 - --- validate (epoch=193)-----------
2023-01-06 16:49:56,165 - 6986 samples (256 per mini-batch)
2023-01-06 16:49:56,592 - Epoch: [193][   10/   28]    Loss 0.247851    Top1 91.328125    
2023-01-06 16:49:56,689 - Epoch: [193][   20/   28]    Loss 0.251442    Top1 90.976562    
2023-01-06 16:49:56,740 - Epoch: [193][   28/   28]    Loss 0.260590    Top1 90.638420    
2023-01-06 16:49:56,881 - ==> Top1: 90.638    Loss: 0.261

2023-01-06 16:49:56,881 - ==> Confusion:
[[ 239   16  184]
 [  15  271  316]
 [  57   66 5822]]

2023-01-06 16:49:56,882 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:49:56,882 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:49:56,887 - 

2023-01-06 16:49:56,887 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:49:57,398 - Epoch: [194][   10/  246]    Overall Loss 0.236257    Objective Loss 0.236257                                        LR 0.000003    Time 0.051020    
2023-01-06 16:49:57,514 - Epoch: [194][   20/  246]    Overall Loss 0.237396    Objective Loss 0.237396                                        LR 0.000003    Time 0.031256    
2023-01-06 16:49:57,632 - Epoch: [194][   30/  246]    Overall Loss 0.243513    Objective Loss 0.243513                                        LR 0.000003    Time 0.024735    
2023-01-06 16:49:57,750 - Epoch: [194][   40/  246]    Overall Loss 0.246998    Objective Loss 0.246998                                        LR 0.000003    Time 0.021491    
2023-01-06 16:49:57,871 - Epoch: [194][   50/  246]    Overall Loss 0.247490    Objective Loss 0.247490                                        LR 0.000003    Time 0.019609    
2023-01-06 16:49:57,989 - Epoch: [194][   60/  246]    Overall Loss 0.245947    Objective Loss 0.245947                                        LR 0.000003    Time 0.018300    
2023-01-06 16:49:58,107 - Epoch: [194][   70/  246]    Overall Loss 0.243558    Objective Loss 0.243558                                        LR 0.000003    Time 0.017370    
2023-01-06 16:49:58,223 - Epoch: [194][   80/  246]    Overall Loss 0.243439    Objective Loss 0.243439                                        LR 0.000003    Time 0.016637    
2023-01-06 16:49:58,341 - Epoch: [194][   90/  246]    Overall Loss 0.242554    Objective Loss 0.242554                                        LR 0.000003    Time 0.016095    
2023-01-06 16:49:58,456 - Epoch: [194][  100/  246]    Overall Loss 0.242402    Objective Loss 0.242402                                        LR 0.000003    Time 0.015631    
2023-01-06 16:49:58,573 - Epoch: [194][  110/  246]    Overall Loss 0.241354    Objective Loss 0.241354                                        LR 0.000003    Time 0.015275    
2023-01-06 16:49:58,688 - Epoch: [194][  120/  246]    Overall Loss 0.239849    Objective Loss 0.239849                                        LR 0.000003    Time 0.014961    
2023-01-06 16:49:58,815 - Epoch: [194][  130/  246]    Overall Loss 0.239721    Objective Loss 0.239721                                        LR 0.000003    Time 0.014778    
2023-01-06 16:49:58,946 - Epoch: [194][  140/  246]    Overall Loss 0.241702    Objective Loss 0.241702                                        LR 0.000003    Time 0.014656    
2023-01-06 16:49:59,072 - Epoch: [194][  150/  246]    Overall Loss 0.243119    Objective Loss 0.243119                                        LR 0.000003    Time 0.014519    
2023-01-06 16:49:59,195 - Epoch: [194][  160/  246]    Overall Loss 0.242744    Objective Loss 0.242744                                        LR 0.000003    Time 0.014375    
2023-01-06 16:49:59,316 - Epoch: [194][  170/  246]    Overall Loss 0.242662    Objective Loss 0.242662                                        LR 0.000003    Time 0.014239    
2023-01-06 16:49:59,442 - Epoch: [194][  180/  246]    Overall Loss 0.242719    Objective Loss 0.242719                                        LR 0.000003    Time 0.014145    
2023-01-06 16:49:59,582 - Epoch: [194][  190/  246]    Overall Loss 0.243319    Objective Loss 0.243319                                        LR 0.000003    Time 0.014140    
2023-01-06 16:49:59,721 - Epoch: [194][  200/  246]    Overall Loss 0.243072    Objective Loss 0.243072                                        LR 0.000003    Time 0.014124    
2023-01-06 16:49:59,862 - Epoch: [194][  210/  246]    Overall Loss 0.242788    Objective Loss 0.242788                                        LR 0.000003    Time 0.014120    
2023-01-06 16:50:00,003 - Epoch: [194][  220/  246]    Overall Loss 0.242762    Objective Loss 0.242762                                        LR 0.000003    Time 0.014117    
2023-01-06 16:50:00,142 - Epoch: [194][  230/  246]    Overall Loss 0.241875    Objective Loss 0.241875                                        LR 0.000003    Time 0.014108    
2023-01-06 16:50:00,296 - Epoch: [194][  240/  246]    Overall Loss 0.241159    Objective Loss 0.241159                                        LR 0.000003    Time 0.014162    
2023-01-06 16:50:00,362 - Epoch: [194][  246/  246]    Overall Loss 0.241304    Objective Loss 0.241304    Top1 91.626794    LR 0.000003    Time 0.014082    
2023-01-06 16:50:00,501 - --- validate (epoch=194)-----------
2023-01-06 16:50:00,501 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:00,938 - Epoch: [194][   10/   28]    Loss 0.254954    Top1 91.093750    
2023-01-06 16:50:01,036 - Epoch: [194][   20/   28]    Loss 0.261415    Top1 90.664062    
2023-01-06 16:50:01,085 - Epoch: [194][   28/   28]    Loss 0.259661    Top1 90.738620    
2023-01-06 16:50:01,222 - ==> Top1: 90.739    Loss: 0.260

2023-01-06 16:50:01,222 - ==> Confusion:
[[ 248   14  177]
 [  15  284  303]
 [  58   80 5807]]

2023-01-06 16:50:01,223 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:01,223 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:01,228 - 

2023-01-06 16:50:01,228 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:50:01,874 - Epoch: [195][   10/  246]    Overall Loss 0.253232    Objective Loss 0.253232                                        LR 0.000003    Time 0.064482    
2023-01-06 16:50:01,994 - Epoch: [195][   20/  246]    Overall Loss 0.249630    Objective Loss 0.249630                                        LR 0.000003    Time 0.038228    
2023-01-06 16:50:02,114 - Epoch: [195][   30/  246]    Overall Loss 0.250211    Objective Loss 0.250211                                        LR 0.000003    Time 0.029485    
2023-01-06 16:50:02,232 - Epoch: [195][   40/  246]    Overall Loss 0.246946    Objective Loss 0.246946                                        LR 0.000003    Time 0.025038    
2023-01-06 16:50:02,349 - Epoch: [195][   50/  246]    Overall Loss 0.245073    Objective Loss 0.245073                                        LR 0.000003    Time 0.022360    
2023-01-06 16:50:02,467 - Epoch: [195][   60/  246]    Overall Loss 0.243018    Objective Loss 0.243018                                        LR 0.000003    Time 0.020602    
2023-01-06 16:50:02,588 - Epoch: [195][   70/  246]    Overall Loss 0.244722    Objective Loss 0.244722                                        LR 0.000003    Time 0.019377    
2023-01-06 16:50:02,710 - Epoch: [195][   80/  246]    Overall Loss 0.248074    Objective Loss 0.248074                                        LR 0.000003    Time 0.018479    
2023-01-06 16:50:02,831 - Epoch: [195][   90/  246]    Overall Loss 0.246791    Objective Loss 0.246791                                        LR 0.000003    Time 0.017767    
2023-01-06 16:50:02,951 - Epoch: [195][  100/  246]    Overall Loss 0.247247    Objective Loss 0.247247                                        LR 0.000003    Time 0.017182    
2023-01-06 16:50:03,069 - Epoch: [195][  110/  246]    Overall Loss 0.246088    Objective Loss 0.246088                                        LR 0.000003    Time 0.016691    
2023-01-06 16:50:03,190 - Epoch: [195][  120/  246]    Overall Loss 0.245604    Objective Loss 0.245604                                        LR 0.000003    Time 0.016307    
2023-01-06 16:50:03,314 - Epoch: [195][  130/  246]    Overall Loss 0.244035    Objective Loss 0.244035                                        LR 0.000003    Time 0.016002    
2023-01-06 16:50:03,437 - Epoch: [195][  140/  246]    Overall Loss 0.244254    Objective Loss 0.244254                                        LR 0.000003    Time 0.015735    
2023-01-06 16:50:03,560 - Epoch: [195][  150/  246]    Overall Loss 0.243571    Objective Loss 0.243571                                        LR 0.000003    Time 0.015501    
2023-01-06 16:50:03,679 - Epoch: [195][  160/  246]    Overall Loss 0.243444    Objective Loss 0.243444                                        LR 0.000003    Time 0.015277    
2023-01-06 16:50:03,802 - Epoch: [195][  170/  246]    Overall Loss 0.241849    Objective Loss 0.241849                                        LR 0.000003    Time 0.015098    
2023-01-06 16:50:03,923 - Epoch: [195][  180/  246]    Overall Loss 0.241739    Objective Loss 0.241739                                        LR 0.000003    Time 0.014927    
2023-01-06 16:50:04,048 - Epoch: [195][  190/  246]    Overall Loss 0.241968    Objective Loss 0.241968                                        LR 0.000003    Time 0.014798    
2023-01-06 16:50:04,178 - Epoch: [195][  200/  246]    Overall Loss 0.241409    Objective Loss 0.241409                                        LR 0.000003    Time 0.014705    
2023-01-06 16:50:04,310 - Epoch: [195][  210/  246]    Overall Loss 0.241078    Objective Loss 0.241078                                        LR 0.000003    Time 0.014634    
2023-01-06 16:50:04,441 - Epoch: [195][  220/  246]    Overall Loss 0.241202    Objective Loss 0.241202                                        LR 0.000003    Time 0.014562    
2023-01-06 16:50:04,572 - Epoch: [195][  230/  246]    Overall Loss 0.241188    Objective Loss 0.241188                                        LR 0.000003    Time 0.014494    
2023-01-06 16:50:04,720 - Epoch: [195][  240/  246]    Overall Loss 0.240542    Objective Loss 0.240542                                        LR 0.000003    Time 0.014506    
2023-01-06 16:50:04,786 - Epoch: [195][  246/  246]    Overall Loss 0.240834    Objective Loss 0.240834    Top1 89.952153    LR 0.000003    Time 0.014423    
2023-01-06 16:50:04,916 - --- validate (epoch=195)-----------
2023-01-06 16:50:04,917 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:05,332 - Epoch: [195][   10/   28]    Loss 0.241892    Top1 90.625000    
2023-01-06 16:50:05,430 - Epoch: [195][   20/   28]    Loss 0.257258    Top1 90.683594    
2023-01-06 16:50:05,478 - Epoch: [195][   28/   28]    Loss 0.261318    Top1 90.624105    
2023-01-06 16:50:05,642 - ==> Top1: 90.624    Loss: 0.261

2023-01-06 16:50:05,642 - ==> Confusion:
[[ 233   14  192]
 [  15  268  319]
 [  51   64 5830]]

2023-01-06 16:50:05,643 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:05,643 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:05,649 - 

2023-01-06 16:50:05,649 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:50:06,185 - Epoch: [196][   10/  246]    Overall Loss 0.254662    Objective Loss 0.254662                                        LR 0.000003    Time 0.053571    
2023-01-06 16:50:06,329 - Epoch: [196][   20/  246]    Overall Loss 0.248276    Objective Loss 0.248276                                        LR 0.000003    Time 0.033965    
2023-01-06 16:50:06,485 - Epoch: [196][   30/  246]    Overall Loss 0.243256    Objective Loss 0.243256                                        LR 0.000003    Time 0.027832    
2023-01-06 16:50:06,637 - Epoch: [196][   40/  246]    Overall Loss 0.243160    Objective Loss 0.243160                                        LR 0.000003    Time 0.024658    
2023-01-06 16:50:06,786 - Epoch: [196][   50/  246]    Overall Loss 0.242917    Objective Loss 0.242917                                        LR 0.000003    Time 0.022704    
2023-01-06 16:50:06,938 - Epoch: [196][   60/  246]    Overall Loss 0.242413    Objective Loss 0.242413                                        LR 0.000003    Time 0.021442    
2023-01-06 16:50:07,090 - Epoch: [196][   70/  246]    Overall Loss 0.242468    Objective Loss 0.242468                                        LR 0.000003    Time 0.020540    
2023-01-06 16:50:07,245 - Epoch: [196][   80/  246]    Overall Loss 0.241521    Objective Loss 0.241521                                        LR 0.000003    Time 0.019912    
2023-01-06 16:50:07,393 - Epoch: [196][   90/  246]    Overall Loss 0.243443    Objective Loss 0.243443                                        LR 0.000003    Time 0.019341    
2023-01-06 16:50:07,547 - Epoch: [196][  100/  246]    Overall Loss 0.244740    Objective Loss 0.244740                                        LR 0.000003    Time 0.018939    
2023-01-06 16:50:07,694 - Epoch: [196][  110/  246]    Overall Loss 0.246082    Objective Loss 0.246082                                        LR 0.000003    Time 0.018548    
2023-01-06 16:50:07,852 - Epoch: [196][  120/  246]    Overall Loss 0.245324    Objective Loss 0.245324                                        LR 0.000003    Time 0.018301    
2023-01-06 16:50:08,001 - Epoch: [196][  130/  246]    Overall Loss 0.243505    Objective Loss 0.243505                                        LR 0.000003    Time 0.018038    
2023-01-06 16:50:08,151 - Epoch: [196][  140/  246]    Overall Loss 0.243101    Objective Loss 0.243101                                        LR 0.000003    Time 0.017815    
2023-01-06 16:50:08,300 - Epoch: [196][  150/  246]    Overall Loss 0.242644    Objective Loss 0.242644                                        LR 0.000003    Time 0.017621    
2023-01-06 16:50:08,453 - Epoch: [196][  160/  246]    Overall Loss 0.243420    Objective Loss 0.243420                                        LR 0.000003    Time 0.017476    
2023-01-06 16:50:08,618 - Epoch: [196][  170/  246]    Overall Loss 0.243427    Objective Loss 0.243427                                        LR 0.000003    Time 0.017417    
2023-01-06 16:50:08,806 - Epoch: [196][  180/  246]    Overall Loss 0.243412    Objective Loss 0.243412                                        LR 0.000003    Time 0.017489    
2023-01-06 16:50:08,990 - Epoch: [196][  190/  246]    Overall Loss 0.242809    Objective Loss 0.242809                                        LR 0.000003    Time 0.017536    
2023-01-06 16:50:09,163 - Epoch: [196][  200/  246]    Overall Loss 0.242629    Objective Loss 0.242629                                        LR 0.000003    Time 0.017523    
2023-01-06 16:50:09,347 - Epoch: [196][  210/  246]    Overall Loss 0.242858    Objective Loss 0.242858                                        LR 0.000003    Time 0.017560    
2023-01-06 16:50:09,532 - Epoch: [196][  220/  246]    Overall Loss 0.242608    Objective Loss 0.242608                                        LR 0.000003    Time 0.017601    
2023-01-06 16:50:09,721 - Epoch: [196][  230/  246]    Overall Loss 0.242106    Objective Loss 0.242106                                        LR 0.000003    Time 0.017658    
2023-01-06 16:50:09,921 - Epoch: [196][  240/  246]    Overall Loss 0.241242    Objective Loss 0.241242                                        LR 0.000003    Time 0.017750    
2023-01-06 16:50:09,999 - Epoch: [196][  246/  246]    Overall Loss 0.240741    Objective Loss 0.240741    Top1 91.626794    LR 0.000003    Time 0.017635    
2023-01-06 16:50:10,144 - --- validate (epoch=196)-----------
2023-01-06 16:50:10,145 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:10,574 - Epoch: [196][   10/   28]    Loss 0.253089    Top1 91.328125    
2023-01-06 16:50:10,679 - Epoch: [196][   20/   28]    Loss 0.249498    Top1 90.976562    
2023-01-06 16:50:10,731 - Epoch: [196][   28/   28]    Loss 0.259865    Top1 90.609791    
2023-01-06 16:50:10,894 - ==> Top1: 90.610    Loss: 0.260

2023-01-06 16:50:10,895 - ==> Confusion:
[[ 232   14  193]
 [  12  262  328]
 [  48   61 5836]]

2023-01-06 16:50:10,896 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:10,896 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:10,901 - 

2023-01-06 16:50:10,901 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:50:11,558 - Epoch: [197][   10/  246]    Overall Loss 0.237892    Objective Loss 0.237892                                        LR 0.000003    Time 0.065601    
2023-01-06 16:50:11,697 - Epoch: [197][   20/  246]    Overall Loss 0.237563    Objective Loss 0.237563                                        LR 0.000003    Time 0.039721    
2023-01-06 16:50:11,842 - Epoch: [197][   30/  246]    Overall Loss 0.234942    Objective Loss 0.234942                                        LR 0.000003    Time 0.031314    
2023-01-06 16:50:11,989 - Epoch: [197][   40/  246]    Overall Loss 0.237543    Objective Loss 0.237543                                        LR 0.000003    Time 0.027145    
2023-01-06 16:50:12,126 - Epoch: [197][   50/  246]    Overall Loss 0.237035    Objective Loss 0.237035                                        LR 0.000003    Time 0.024443    
2023-01-06 16:50:12,267 - Epoch: [197][   60/  246]    Overall Loss 0.236462    Objective Loss 0.236462                                        LR 0.000003    Time 0.022712    
2023-01-06 16:50:12,410 - Epoch: [197][   70/  246]    Overall Loss 0.233273    Objective Loss 0.233273                                        LR 0.000003    Time 0.021491    
2023-01-06 16:50:12,549 - Epoch: [197][   80/  246]    Overall Loss 0.234776    Objective Loss 0.234776                                        LR 0.000003    Time 0.020528    
2023-01-06 16:50:12,681 - Epoch: [197][   90/  246]    Overall Loss 0.233738    Objective Loss 0.233738                                        LR 0.000003    Time 0.019718    
2023-01-06 16:50:12,813 - Epoch: [197][  100/  246]    Overall Loss 0.233274    Objective Loss 0.233274                                        LR 0.000003    Time 0.019059    
2023-01-06 16:50:12,942 - Epoch: [197][  110/  246]    Overall Loss 0.236235    Objective Loss 0.236235                                        LR 0.000003    Time 0.018491    
2023-01-06 16:50:13,070 - Epoch: [197][  120/  246]    Overall Loss 0.239567    Objective Loss 0.239567                                        LR 0.000003    Time 0.018020    
2023-01-06 16:50:13,202 - Epoch: [197][  130/  246]    Overall Loss 0.239024    Objective Loss 0.239024                                        LR 0.000003    Time 0.017643    
2023-01-06 16:50:13,335 - Epoch: [197][  140/  246]    Overall Loss 0.238976    Objective Loss 0.238976                                        LR 0.000003    Time 0.017329    
2023-01-06 16:50:13,466 - Epoch: [197][  150/  246]    Overall Loss 0.239839    Objective Loss 0.239839                                        LR 0.000003    Time 0.017045    
2023-01-06 16:50:13,598 - Epoch: [197][  160/  246]    Overall Loss 0.240073    Objective Loss 0.240073                                        LR 0.000003    Time 0.016801    
2023-01-06 16:50:13,730 - Epoch: [197][  170/  246]    Overall Loss 0.241161    Objective Loss 0.241161                                        LR 0.000003    Time 0.016589    
2023-01-06 16:50:13,865 - Epoch: [197][  180/  246]    Overall Loss 0.241830    Objective Loss 0.241830                                        LR 0.000003    Time 0.016412    
2023-01-06 16:50:13,998 - Epoch: [197][  190/  246]    Overall Loss 0.241498    Objective Loss 0.241498                                        LR 0.000003    Time 0.016246    
2023-01-06 16:50:14,146 - Epoch: [197][  200/  246]    Overall Loss 0.242791    Objective Loss 0.242791                                        LR 0.000003    Time 0.016170    
2023-01-06 16:50:14,282 - Epoch: [197][  210/  246]    Overall Loss 0.242102    Objective Loss 0.242102                                        LR 0.000003    Time 0.016049    
2023-01-06 16:50:14,422 - Epoch: [197][  220/  246]    Overall Loss 0.241620    Objective Loss 0.241620                                        LR 0.000003    Time 0.015950    
2023-01-06 16:50:14,558 - Epoch: [197][  230/  246]    Overall Loss 0.241401    Objective Loss 0.241401                                        LR 0.000003    Time 0.015847    
2023-01-06 16:50:14,710 - Epoch: [197][  240/  246]    Overall Loss 0.241014    Objective Loss 0.241014                                        LR 0.000003    Time 0.015821    
2023-01-06 16:50:14,777 - Epoch: [197][  246/  246]    Overall Loss 0.241246    Objective Loss 0.241246    Top1 88.995215    LR 0.000003    Time 0.015707    
2023-01-06 16:50:14,933 - --- validate (epoch=197)-----------
2023-01-06 16:50:14,934 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:15,360 - Epoch: [197][   10/   28]    Loss 0.263513    Top1 90.703125    
2023-01-06 16:50:15,456 - Epoch: [197][   20/   28]    Loss 0.262006    Top1 90.488281    
2023-01-06 16:50:15,507 - Epoch: [197][   28/   28]    Loss 0.260555    Top1 90.552534    
2023-01-06 16:50:15,644 - ==> Top1: 90.553    Loss: 0.261

2023-01-06 16:50:15,645 - ==> Confusion:
[[ 233   15  191]
 [  14  265  323]
 [  55   62 5828]]

2023-01-06 16:50:15,646 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:15,646 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:15,651 - 

2023-01-06 16:50:15,651 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:50:16,321 - Epoch: [198][   10/  246]    Overall Loss 0.255917    Objective Loss 0.255917                                        LR 0.000003    Time 0.066902    
2023-01-06 16:50:16,477 - Epoch: [198][   20/  246]    Overall Loss 0.258496    Objective Loss 0.258496                                        LR 0.000003    Time 0.041223    
2023-01-06 16:50:16,629 - Epoch: [198][   30/  246]    Overall Loss 0.248382    Objective Loss 0.248382                                        LR 0.000003    Time 0.032548    
2023-01-06 16:50:16,785 - Epoch: [198][   40/  246]    Overall Loss 0.243503    Objective Loss 0.243503                                        LR 0.000003    Time 0.028293    
2023-01-06 16:50:16,936 - Epoch: [198][   50/  246]    Overall Loss 0.241400    Objective Loss 0.241400                                        LR 0.000003    Time 0.025653    
2023-01-06 16:50:17,082 - Epoch: [198][   60/  246]    Overall Loss 0.240550    Objective Loss 0.240550                                        LR 0.000003    Time 0.023797    
2023-01-06 16:50:17,222 - Epoch: [198][   70/  246]    Overall Loss 0.239181    Objective Loss 0.239181                                        LR 0.000003    Time 0.022401    
2023-01-06 16:50:17,364 - Epoch: [198][   80/  246]    Overall Loss 0.239627    Objective Loss 0.239627                                        LR 0.000003    Time 0.021363    
2023-01-06 16:50:17,508 - Epoch: [198][   90/  246]    Overall Loss 0.241179    Objective Loss 0.241179                                        LR 0.000003    Time 0.020588    
2023-01-06 16:50:17,649 - Epoch: [198][  100/  246]    Overall Loss 0.241677    Objective Loss 0.241677                                        LR 0.000003    Time 0.019936    
2023-01-06 16:50:17,793 - Epoch: [198][  110/  246]    Overall Loss 0.242439    Objective Loss 0.242439                                        LR 0.000003    Time 0.019426    
2023-01-06 16:50:17,937 - Epoch: [198][  120/  246]    Overall Loss 0.241609    Objective Loss 0.241609                                        LR 0.000003    Time 0.019009    
2023-01-06 16:50:18,078 - Epoch: [198][  130/  246]    Overall Loss 0.241747    Objective Loss 0.241747                                        LR 0.000003    Time 0.018622    
2023-01-06 16:50:18,217 - Epoch: [198][  140/  246]    Overall Loss 0.242209    Objective Loss 0.242209                                        LR 0.000003    Time 0.018285    
2023-01-06 16:50:18,359 - Epoch: [198][  150/  246]    Overall Loss 0.242829    Objective Loss 0.242829                                        LR 0.000003    Time 0.018006    
2023-01-06 16:50:18,499 - Epoch: [198][  160/  246]    Overall Loss 0.243625    Objective Loss 0.243625                                        LR 0.000003    Time 0.017756    
2023-01-06 16:50:18,643 - Epoch: [198][  170/  246]    Overall Loss 0.244006    Objective Loss 0.244006                                        LR 0.000003    Time 0.017553    
2023-01-06 16:50:18,784 - Epoch: [198][  180/  246]    Overall Loss 0.243237    Objective Loss 0.243237                                        LR 0.000003    Time 0.017358    
2023-01-06 16:50:18,926 - Epoch: [198][  190/  246]    Overall Loss 0.242917    Objective Loss 0.242917                                        LR 0.000003    Time 0.017191    
2023-01-06 16:50:19,064 - Epoch: [198][  200/  246]    Overall Loss 0.242539    Objective Loss 0.242539                                        LR 0.000003    Time 0.017023    
2023-01-06 16:50:19,207 - Epoch: [198][  210/  246]    Overall Loss 0.242294    Objective Loss 0.242294                                        LR 0.000003    Time 0.016887    
2023-01-06 16:50:19,345 - Epoch: [198][  220/  246]    Overall Loss 0.241031    Objective Loss 0.241031                                        LR 0.000003    Time 0.016748    
2023-01-06 16:50:19,487 - Epoch: [198][  230/  246]    Overall Loss 0.240403    Objective Loss 0.240403                                        LR 0.000003    Time 0.016636    
2023-01-06 16:50:19,645 - Epoch: [198][  240/  246]    Overall Loss 0.240056    Objective Loss 0.240056                                        LR 0.000003    Time 0.016597    
2023-01-06 16:50:19,711 - Epoch: [198][  246/  246]    Overall Loss 0.240505    Objective Loss 0.240505    Top1 89.712919    LR 0.000003    Time 0.016462    
2023-01-06 16:50:19,830 - --- validate (epoch=198)-----------
2023-01-06 16:50:19,830 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:20,258 - Epoch: [198][   10/   28]    Loss 0.255238    Top1 90.468750    
2023-01-06 16:50:20,356 - Epoch: [198][   20/   28]    Loss 0.255148    Top1 90.585938    
2023-01-06 16:50:20,404 - Epoch: [198][   28/   28]    Loss 0.260381    Top1 90.538219    
2023-01-06 16:50:20,543 - ==> Top1: 90.538    Loss: 0.260

2023-01-06 16:50:20,544 - ==> Confusion:
[[ 233   14  192]
 [  16  267  319]
 [  45   75 5825]]

2023-01-06 16:50:20,545 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:20,545 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:20,550 - 

2023-01-06 16:50:20,550 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:50:21,084 - Epoch: [199][   10/  246]    Overall Loss 0.243113    Objective Loss 0.243113                                        LR 0.000003    Time 0.053283    
2023-01-06 16:50:21,228 - Epoch: [199][   20/  246]    Overall Loss 0.240290    Objective Loss 0.240290                                        LR 0.000003    Time 0.033828    
2023-01-06 16:50:21,373 - Epoch: [199][   30/  246]    Overall Loss 0.239758    Objective Loss 0.239758                                        LR 0.000003    Time 0.027391    
2023-01-06 16:50:21,522 - Epoch: [199][   40/  246]    Overall Loss 0.234837    Objective Loss 0.234837                                        LR 0.000003    Time 0.024250    
2023-01-06 16:50:21,646 - Epoch: [199][   50/  246]    Overall Loss 0.237289    Objective Loss 0.237289                                        LR 0.000003    Time 0.021876    
2023-01-06 16:50:21,771 - Epoch: [199][   60/  246]    Overall Loss 0.242021    Objective Loss 0.242021                                        LR 0.000003    Time 0.020295    
2023-01-06 16:50:21,896 - Epoch: [199][   70/  246]    Overall Loss 0.239897    Objective Loss 0.239897                                        LR 0.000003    Time 0.019174    
2023-01-06 16:50:22,021 - Epoch: [199][   80/  246]    Overall Loss 0.241533    Objective Loss 0.241533                                        LR 0.000003    Time 0.018335    
2023-01-06 16:50:22,148 - Epoch: [199][   90/  246]    Overall Loss 0.242031    Objective Loss 0.242031                                        LR 0.000003    Time 0.017704    
2023-01-06 16:50:22,274 - Epoch: [199][  100/  246]    Overall Loss 0.242039    Objective Loss 0.242039                                        LR 0.000003    Time 0.017190    
2023-01-06 16:50:22,401 - Epoch: [199][  110/  246]    Overall Loss 0.242521    Objective Loss 0.242521                                        LR 0.000003    Time 0.016782    
2023-01-06 16:50:22,544 - Epoch: [199][  120/  246]    Overall Loss 0.241745    Objective Loss 0.241745                                        LR 0.000003    Time 0.016567    
2023-01-06 16:50:22,710 - Epoch: [199][  130/  246]    Overall Loss 0.239629    Objective Loss 0.239629                                        LR 0.000003    Time 0.016568    
2023-01-06 16:50:22,876 - Epoch: [199][  140/  246]    Overall Loss 0.238959    Objective Loss 0.238959                                        LR 0.000003    Time 0.016567    
2023-01-06 16:50:23,045 - Epoch: [199][  150/  246]    Overall Loss 0.239194    Objective Loss 0.239194                                        LR 0.000003    Time 0.016589    
2023-01-06 16:50:23,224 - Epoch: [199][  160/  246]    Overall Loss 0.241032    Objective Loss 0.241032                                        LR 0.000003    Time 0.016669    
2023-01-06 16:50:23,409 - Epoch: [199][  170/  246]    Overall Loss 0.241130    Objective Loss 0.241130                                        LR 0.000003    Time 0.016774    
2023-01-06 16:50:23,592 - Epoch: [199][  180/  246]    Overall Loss 0.240223    Objective Loss 0.240223                                        LR 0.000003    Time 0.016855    
2023-01-06 16:50:23,778 - Epoch: [199][  190/  246]    Overall Loss 0.241618    Objective Loss 0.241618                                        LR 0.000003    Time 0.016946    
2023-01-06 16:50:23,959 - Epoch: [199][  200/  246]    Overall Loss 0.241165    Objective Loss 0.241165                                        LR 0.000003    Time 0.016998    
2023-01-06 16:50:24,145 - Epoch: [199][  210/  246]    Overall Loss 0.240447    Objective Loss 0.240447                                        LR 0.000003    Time 0.017072    
2023-01-06 16:50:24,329 - Epoch: [199][  220/  246]    Overall Loss 0.240796    Objective Loss 0.240796                                        LR 0.000003    Time 0.017129    
2023-01-06 16:50:24,513 - Epoch: [199][  230/  246]    Overall Loss 0.240747    Objective Loss 0.240747                                        LR 0.000003    Time 0.017186    
2023-01-06 16:50:24,713 - Epoch: [199][  240/  246]    Overall Loss 0.241241    Objective Loss 0.241241                                        LR 0.000003    Time 0.017298    
2023-01-06 16:50:24,792 - Epoch: [199][  246/  246]    Overall Loss 0.241069    Objective Loss 0.241069    Top1 90.669856    LR 0.000003    Time 0.017197    
2023-01-06 16:50:24,925 - --- validate (epoch=199)-----------
2023-01-06 16:50:24,926 - 6986 samples (256 per mini-batch)
2023-01-06 16:50:25,352 - Epoch: [199][   10/   28]    Loss 0.260210    Top1 90.312500    
2023-01-06 16:50:25,449 - Epoch: [199][   20/   28]    Loss 0.256129    Top1 90.742188    
2023-01-06 16:50:25,502 - Epoch: [199][   28/   28]    Loss 0.259850    Top1 90.652734    
2023-01-06 16:50:25,644 - ==> Top1: 90.653    Loss: 0.260

2023-01-06 16:50:25,644 - ==> Confusion:
[[ 243   16  180]
 [  17  281  304]
 [  59   77 5809]]

2023-01-06 16:50:25,645 - ==> Best [Top1: 90.810   Sparsity:0.00   Params: 151104 on epoch: 185]
2023-01-06 16:50:25,645 - Saving checkpoint to: logs/2023.01.06-163327/qat_checkpoint.pth.tar
2023-01-06 16:50:25,651 - --- test ---------------------
2023-01-06 16:50:25,651 - 13117 samples (256 per mini-batch)
2023-01-06 16:50:26,086 - Test: [   10/   52]    Loss 0.217057    Top1 92.617188    
2023-01-06 16:50:26,181 - Test: [   20/   52]    Loss 0.220525    Top1 92.285156    
2023-01-06 16:50:26,273 - Test: [   30/   52]    Loss 0.219618    Top1 92.382812    
2023-01-06 16:50:26,363 - Test: [   40/   52]    Loss 0.228755    Top1 91.962891    
2023-01-06 16:50:26,443 - Test: [   50/   52]    Loss 0.227847    Top1 91.921875    
2023-01-06 16:50:26,455 - Test: [   52/   52]    Loss 0.226743    Top1 91.964626    
2023-01-06 16:50:26,596 - ==> Top1: 91.965    Loss: 0.227

2023-01-06 16:50:26,596 - ==> Confusion:
[[  262    23   276]
 [   24   320   412]
 [  126   193 11481]]

2023-01-06 16:50:26,669 - 
2023-01-06 16:50:26,669 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-163327/2023.01.06-163327.log
