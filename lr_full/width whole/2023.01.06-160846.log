2023-01-06 16:08:46,085 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-160846/2023.01.06-160846.log
2023-01-06 16:08:48,134 - => loading checkpoint qat_best.pth.tar
2023-01-06 16:08:48,137 - => Checkpoint contents:
+----------------------+-------------+----------------+
| Key                  | Type        | Value          |
|----------------------+-------------+----------------|
| arch                 | str         | ai85kws20netv2 |
| compression_sched    | dict        |                |
| epoch                | int         | 197            |
| extras               | dict        |                |
| optimizer_state_dict | dict        |                |
| optimizer_type       | type        | Adam           |
| state_dict           | OrderedDict |                |
+----------------------+-------------+----------------+

2023-01-06 16:08:48,137 - => Checkpoint['extras'] contents:
+--------------+--------+----------+
| Key          | Type   |    Value |
|--------------+--------+----------|
| best_epoch   | int    | 197      |
| best_mAP     | int    |   0      |
| best_top1    | float  |  83.2061 |
| current_mAP  | int    |   0      |
| current_top1 | float  |  83.2061 |
+--------------+--------+----------+

2023-01-06 16:08:48,137 - Loaded compression schedule from checkpoint (epoch 197)
2023-01-06 16:08:48,139 - => loaded 'state_dict' from checkpoint 'qat_best.pth.tar'
2023-01-06 16:08:48,147 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-06 16:08:48,147 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-06 16:09:37,651 - Dataset sizes:
	training=62882
	validation=6986
	test=13117
2023-01-06 16:09:37,651 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-06 16:09:37,654 - 

2023-01-06 16:09:37,654 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:09:38,622 - Epoch: [0][   10/  246]    Overall Loss 9.129253    Objective Loss 9.129253                                        LR 0.000060    Time 0.096699    
2023-01-06 16:09:38,786 - Epoch: [0][   20/  246]    Overall Loss 7.138987    Objective Loss 7.138987                                        LR 0.000060    Time 0.056552    
2023-01-06 16:09:38,959 - Epoch: [0][   30/  246]    Overall Loss 5.905020    Objective Loss 5.905020                                        LR 0.000060    Time 0.043448    
2023-01-06 16:09:39,138 - Epoch: [0][   40/  246]    Overall Loss 5.098354    Objective Loss 5.098354                                        LR 0.000060    Time 0.036991    
2023-01-06 16:09:39,308 - Epoch: [0][   50/  246]    Overall Loss 4.506221    Objective Loss 4.506221                                        LR 0.000060    Time 0.032999    
2023-01-06 16:09:39,486 - Epoch: [0][   60/  246]    Overall Loss 4.050820    Objective Loss 4.050820                                        LR 0.000060    Time 0.030464    
2023-01-06 16:09:39,660 - Epoch: [0][   70/  246]    Overall Loss 3.691300    Objective Loss 3.691300                                        LR 0.000060    Time 0.028553    
2023-01-06 16:09:39,837 - Epoch: [0][   80/  246]    Overall Loss 3.403297    Objective Loss 3.403297                                        LR 0.000060    Time 0.027193    
2023-01-06 16:09:39,999 - Epoch: [0][   90/  246]    Overall Loss 3.164601    Objective Loss 3.164601                                        LR 0.000060    Time 0.025977    
2023-01-06 16:09:40,166 - Epoch: [0][  100/  246]    Overall Loss 2.962863    Objective Loss 2.962863                                        LR 0.000060    Time 0.025039    
2023-01-06 16:09:40,330 - Epoch: [0][  110/  246]    Overall Loss 2.787712    Objective Loss 2.787712                                        LR 0.000060    Time 0.024253    
2023-01-06 16:09:40,495 - Epoch: [0][  120/  246]    Overall Loss 2.633561    Objective Loss 2.633561                                        LR 0.000060    Time 0.023607    
2023-01-06 16:09:40,658 - Epoch: [0][  130/  246]    Overall Loss 2.497281    Objective Loss 2.497281                                        LR 0.000060    Time 0.023035    
2023-01-06 16:09:40,839 - Epoch: [0][  140/  246]    Overall Loss 2.374246    Objective Loss 2.374246                                        LR 0.000060    Time 0.022681    
2023-01-06 16:09:41,009 - Epoch: [0][  150/  246]    Overall Loss 2.263135    Objective Loss 2.263135                                        LR 0.000060    Time 0.022305    
2023-01-06 16:09:41,186 - Epoch: [0][  160/  246]    Overall Loss 2.162050    Objective Loss 2.162050                                        LR 0.000060    Time 0.022014    
2023-01-06 16:09:41,357 - Epoch: [0][  170/  246]    Overall Loss 2.070461    Objective Loss 2.070461                                        LR 0.000060    Time 0.021719    
2023-01-06 16:09:41,522 - Epoch: [0][  180/  246]    Overall Loss 1.987544    Objective Loss 1.987544                                        LR 0.000060    Time 0.021429    
2023-01-06 16:09:41,687 - Epoch: [0][  190/  246]    Overall Loss 1.910948    Objective Loss 1.910948                                        LR 0.000060    Time 0.021168    
2023-01-06 16:09:41,856 - Epoch: [0][  200/  246]    Overall Loss 1.840749    Objective Loss 1.840749                                        LR 0.000060    Time 0.020953    
2023-01-06 16:09:42,026 - Epoch: [0][  210/  246]    Overall Loss 1.778454    Objective Loss 1.778454                                        LR 0.000060    Time 0.020760    
2023-01-06 16:09:42,189 - Epoch: [0][  220/  246]    Overall Loss 1.720653    Objective Loss 1.720653                                        LR 0.000060    Time 0.020559    
2023-01-06 16:09:42,354 - Epoch: [0][  230/  246]    Overall Loss 1.667178    Objective Loss 1.667178                                        LR 0.000060    Time 0.020378    
2023-01-06 16:09:42,521 - Epoch: [0][  240/  246]    Overall Loss 1.618249    Objective Loss 1.618249                                        LR 0.000060    Time 0.020225    
2023-01-06 16:09:42,609 - Epoch: [0][  246/  246]    Overall Loss 1.590446    Objective Loss 1.590446    Top1 84.449761    LR 0.000060    Time 0.020090    
2023-01-06 16:09:42,745 - --- validate (epoch=0)-----------
2023-01-06 16:09:42,745 - 6986 samples (256 per mini-batch)
2023-01-06 16:09:43,177 - Epoch: [0][   10/   28]    Loss 0.472553    Top1 86.367188    
2023-01-06 16:09:43,289 - Epoch: [0][   20/   28]    Loss 0.500990    Top1 85.097656    
2023-01-06 16:09:43,346 - Epoch: [0][   28/   28]    Loss 0.495241    Top1 85.098769    
2023-01-06 16:09:43,490 - ==> Top1: 85.099    Loss: 0.495

2023-01-06 16:09:43,494 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:09:43,497 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 46192 on epoch: 0]
2023-01-06 16:09:43,497 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:09:43,503 - 

2023-01-06 16:09:43,503 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:09:44,033 - Epoch: [1][   10/  246]    Overall Loss 0.499956    Objective Loss 0.499956                                        LR 0.000060    Time 0.052902    
2023-01-06 16:09:44,214 - Epoch: [1][   20/  246]    Overall Loss 0.494481    Objective Loss 0.494481                                        LR 0.000060    Time 0.035510    
2023-01-06 16:09:44,402 - Epoch: [1][   30/  246]    Overall Loss 0.494743    Objective Loss 0.494743                                        LR 0.000060    Time 0.029907    
2023-01-06 16:09:44,590 - Epoch: [1][   40/  246]    Overall Loss 0.496298    Objective Loss 0.496298                                        LR 0.000060    Time 0.027136    
2023-01-06 16:09:44,778 - Epoch: [1][   50/  246]    Overall Loss 0.498009    Objective Loss 0.498009                                        LR 0.000060    Time 0.025459    
2023-01-06 16:09:44,965 - Epoch: [1][   60/  246]    Overall Loss 0.500468    Objective Loss 0.500468                                        LR 0.000060    Time 0.024292    
2023-01-06 16:09:45,149 - Epoch: [1][   70/  246]    Overall Loss 0.498759    Objective Loss 0.498759                                        LR 0.000060    Time 0.023441    
2023-01-06 16:09:45,327 - Epoch: [1][   80/  246]    Overall Loss 0.496327    Objective Loss 0.496327                                        LR 0.000060    Time 0.022734    
2023-01-06 16:09:45,498 - Epoch: [1][   90/  246]    Overall Loss 0.493490    Objective Loss 0.493490                                        LR 0.000060    Time 0.022083    
2023-01-06 16:09:45,668 - Epoch: [1][  100/  246]    Overall Loss 0.493577    Objective Loss 0.493577                                        LR 0.000060    Time 0.021571    
2023-01-06 16:09:45,851 - Epoch: [1][  110/  246]    Overall Loss 0.494542    Objective Loss 0.494542                                        LR 0.000060    Time 0.021273    
2023-01-06 16:09:46,032 - Epoch: [1][  120/  246]    Overall Loss 0.494387    Objective Loss 0.494387                                        LR 0.000060    Time 0.020987    
2023-01-06 16:09:46,219 - Epoch: [1][  130/  246]    Overall Loss 0.495759    Objective Loss 0.495759                                        LR 0.000060    Time 0.020810    
2023-01-06 16:09:46,400 - Epoch: [1][  140/  246]    Overall Loss 0.494813    Objective Loss 0.494813                                        LR 0.000060    Time 0.020619    
2023-01-06 16:09:46,588 - Epoch: [1][  150/  246]    Overall Loss 0.493267    Objective Loss 0.493267                                        LR 0.000060    Time 0.020492    
2023-01-06 16:09:46,792 - Epoch: [1][  160/  246]    Overall Loss 0.493430    Objective Loss 0.493430                                        LR 0.000060    Time 0.020473    
2023-01-06 16:09:47,001 - Epoch: [1][  170/  246]    Overall Loss 0.492405    Objective Loss 0.492405                                        LR 0.000060    Time 0.020495    
2023-01-06 16:09:47,215 - Epoch: [1][  180/  246]    Overall Loss 0.492841    Objective Loss 0.492841                                        LR 0.000060    Time 0.020546    
2023-01-06 16:09:47,423 - Epoch: [1][  190/  246]    Overall Loss 0.491109    Objective Loss 0.491109                                        LR 0.000060    Time 0.020558    
2023-01-06 16:09:47,639 - Epoch: [1][  200/  246]    Overall Loss 0.489064    Objective Loss 0.489064                                        LR 0.000060    Time 0.020606    
2023-01-06 16:09:47,846 - Epoch: [1][  210/  246]    Overall Loss 0.489352    Objective Loss 0.489352                                        LR 0.000060    Time 0.020607    
2023-01-06 16:09:48,049 - Epoch: [1][  220/  246]    Overall Loss 0.488704    Objective Loss 0.488704                                        LR 0.000060    Time 0.020591    
2023-01-06 16:09:48,245 - Epoch: [1][  230/  246]    Overall Loss 0.487796    Objective Loss 0.487796                                        LR 0.000060    Time 0.020546    
2023-01-06 16:09:48,444 - Epoch: [1][  240/  246]    Overall Loss 0.487102    Objective Loss 0.487102                                        LR 0.000060    Time 0.020520    
2023-01-06 16:09:48,527 - Epoch: [1][  246/  246]    Overall Loss 0.486428    Objective Loss 0.486428    Top1 85.645933    LR 0.000060    Time 0.020357    
2023-01-06 16:09:48,671 - --- validate (epoch=1)-----------
2023-01-06 16:09:48,672 - 6986 samples (256 per mini-batch)
2023-01-06 16:09:49,122 - Epoch: [1][   10/   28]    Loss 0.458952    Top1 85.468750    
2023-01-06 16:09:49,228 - Epoch: [1][   20/   28]    Loss 0.467549    Top1 85.019531    
2023-01-06 16:09:49,287 - Epoch: [1][   28/   28]    Loss 0.464125    Top1 85.098769    
2023-01-06 16:09:49,421 - ==> Top1: 85.099    Loss: 0.464

2023-01-06 16:09:49,421 - ==> Confusion:
[[   0    0  439]
 [   0    0  602]
 [   0    0 5945]]

2023-01-06 16:09:49,422 - ==> Best [Top1: 85.099   Sparsity:0.00   Params: 46192 on epoch: 1]
2023-01-06 16:09:49,422 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:09:49,427 - 

2023-01-06 16:09:49,427 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:09:50,072 - Epoch: [2][   10/  246]    Overall Loss 0.445515    Objective Loss 0.445515                                        LR 0.000060    Time 0.064425    
2023-01-06 16:09:50,215 - Epoch: [2][   20/  246]    Overall Loss 0.451958    Objective Loss 0.451958                                        LR 0.000060    Time 0.039336    
2023-01-06 16:09:50,357 - Epoch: [2][   30/  246]    Overall Loss 0.460283    Objective Loss 0.460283                                        LR 0.000060    Time 0.030972    
2023-01-06 16:09:50,501 - Epoch: [2][   40/  246]    Overall Loss 0.466424    Objective Loss 0.466424                                        LR 0.000060    Time 0.026797    
2023-01-06 16:09:50,644 - Epoch: [2][   50/  246]    Overall Loss 0.468457    Objective Loss 0.468457                                        LR 0.000060    Time 0.024292    
2023-01-06 16:09:50,784 - Epoch: [2][   60/  246]    Overall Loss 0.465348    Objective Loss 0.465348                                        LR 0.000060    Time 0.022572    
2023-01-06 16:09:50,930 - Epoch: [2][   70/  246]    Overall Loss 0.465613    Objective Loss 0.465613                                        LR 0.000060    Time 0.021424    
2023-01-06 16:09:51,092 - Epoch: [2][   80/  246]    Overall Loss 0.463462    Objective Loss 0.463462                                        LR 0.000060    Time 0.020770    
2023-01-06 16:09:51,264 - Epoch: [2][   90/  246]    Overall Loss 0.464444    Objective Loss 0.464444                                        LR 0.000060    Time 0.020372    
2023-01-06 16:09:51,428 - Epoch: [2][  100/  246]    Overall Loss 0.462644    Objective Loss 0.462644                                        LR 0.000060    Time 0.019970    
2023-01-06 16:09:51,598 - Epoch: [2][  110/  246]    Overall Loss 0.463729    Objective Loss 0.463729                                        LR 0.000060    Time 0.019697    
2023-01-06 16:09:51,762 - Epoch: [2][  120/  246]    Overall Loss 0.463026    Objective Loss 0.463026                                        LR 0.000060    Time 0.019418    
2023-01-06 16:09:51,909 - Epoch: [2][  130/  246]    Overall Loss 0.461825    Objective Loss 0.461825                                        LR 0.000060    Time 0.019055    
2023-01-06 16:09:52,064 - Epoch: [2][  140/  246]    Overall Loss 0.460676    Objective Loss 0.460676                                        LR 0.000060    Time 0.018799    
2023-01-06 16:09:52,212 - Epoch: [2][  150/  246]    Overall Loss 0.459250    Objective Loss 0.459250                                        LR 0.000060    Time 0.018526    
2023-01-06 16:09:52,347 - Epoch: [2][  160/  246]    Overall Loss 0.458368    Objective Loss 0.458368                                        LR 0.000060    Time 0.018213    
2023-01-06 16:09:52,482 - Epoch: [2][  170/  246]    Overall Loss 0.457497    Objective Loss 0.457497                                        LR 0.000060    Time 0.017931    
2023-01-06 16:09:52,617 - Epoch: [2][  180/  246]    Overall Loss 0.456674    Objective Loss 0.456674                                        LR 0.000060    Time 0.017683    
2023-01-06 16:09:52,751 - Epoch: [2][  190/  246]    Overall Loss 0.456665    Objective Loss 0.456665                                        LR 0.000060    Time 0.017455    
2023-01-06 16:09:52,885 - Epoch: [2][  200/  246]    Overall Loss 0.456362    Objective Loss 0.456362                                        LR 0.000060    Time 0.017251    
2023-01-06 16:09:53,019 - Epoch: [2][  210/  246]    Overall Loss 0.457979    Objective Loss 0.457979                                        LR 0.000060    Time 0.017066    
2023-01-06 16:09:53,155 - Epoch: [2][  220/  246]    Overall Loss 0.457205    Objective Loss 0.457205                                        LR 0.000060    Time 0.016904    
2023-01-06 16:09:53,290 - Epoch: [2][  230/  246]    Overall Loss 0.456255    Objective Loss 0.456255                                        LR 0.000060    Time 0.016755    
2023-01-06 16:09:53,441 - Epoch: [2][  240/  246]    Overall Loss 0.456089    Objective Loss 0.456089                                        LR 0.000060    Time 0.016685    
2023-01-06 16:09:53,519 - Epoch: [2][  246/  246]    Overall Loss 0.455333    Objective Loss 0.455333    Top1 85.885167    LR 0.000060    Time 0.016593    
2023-01-06 16:09:53,665 - --- validate (epoch=2)-----------
2023-01-06 16:09:53,665 - 6986 samples (256 per mini-batch)
2023-01-06 16:09:54,107 - Epoch: [2][   10/   28]    Loss 0.455959    Top1 84.843750    
2023-01-06 16:09:54,219 - Epoch: [2][   20/   28]    Loss 0.447429    Top1 85.000000    
2023-01-06 16:09:54,275 - Epoch: [2][   28/   28]    Loss 0.447159    Top1 85.141712    
2023-01-06 16:09:54,412 - ==> Top1: 85.142    Loss: 0.447

2023-01-06 16:09:54,412 - ==> Confusion:
[[   5    0  434]
 [   0    0  602]
 [   2    0 5943]]

2023-01-06 16:09:54,413 - ==> Best [Top1: 85.142   Sparsity:0.00   Params: 46192 on epoch: 2]
2023-01-06 16:09:54,413 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:09:54,422 - 

2023-01-06 16:09:54,422 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:09:54,972 - Epoch: [3][   10/  246]    Overall Loss 0.440500    Objective Loss 0.440500                                        LR 0.000060    Time 0.054973    
2023-01-06 16:09:55,115 - Epoch: [3][   20/  246]    Overall Loss 0.449099    Objective Loss 0.449099                                        LR 0.000060    Time 0.034578    
2023-01-06 16:09:55,262 - Epoch: [3][   30/  246]    Overall Loss 0.449227    Objective Loss 0.449227                                        LR 0.000060    Time 0.027946    
2023-01-06 16:09:55,404 - Epoch: [3][   40/  246]    Overall Loss 0.446207    Objective Loss 0.446207                                        LR 0.000060    Time 0.024504    
2023-01-06 16:09:55,551 - Epoch: [3][   50/  246]    Overall Loss 0.445929    Objective Loss 0.445929                                        LR 0.000060    Time 0.022529    
2023-01-06 16:09:55,694 - Epoch: [3][   60/  246]    Overall Loss 0.445459    Objective Loss 0.445459                                        LR 0.000060    Time 0.021146    
2023-01-06 16:09:55,840 - Epoch: [3][   70/  246]    Overall Loss 0.444780    Objective Loss 0.444780                                        LR 0.000060    Time 0.020209    
2023-01-06 16:09:55,982 - Epoch: [3][   80/  246]    Overall Loss 0.443213    Objective Loss 0.443213                                        LR 0.000060    Time 0.019457    
2023-01-06 16:09:56,143 - Epoch: [3][   90/  246]    Overall Loss 0.442282    Objective Loss 0.442282                                        LR 0.000060    Time 0.019072    
2023-01-06 16:09:56,298 - Epoch: [3][  100/  246]    Overall Loss 0.440992    Objective Loss 0.440992                                        LR 0.000060    Time 0.018701    
2023-01-06 16:09:56,459 - Epoch: [3][  110/  246]    Overall Loss 0.442689    Objective Loss 0.442689                                        LR 0.000060    Time 0.018459    
2023-01-06 16:09:56,615 - Epoch: [3][  120/  246]    Overall Loss 0.443490    Objective Loss 0.443490                                        LR 0.000060    Time 0.018216    
2023-01-06 16:09:56,776 - Epoch: [3][  130/  246]    Overall Loss 0.442609    Objective Loss 0.442609                                        LR 0.000060    Time 0.018050    
2023-01-06 16:09:56,932 - Epoch: [3][  140/  246]    Overall Loss 0.443095    Objective Loss 0.443095                                        LR 0.000060    Time 0.017859    
2023-01-06 16:09:57,095 - Epoch: [3][  150/  246]    Overall Loss 0.440779    Objective Loss 0.440779                                        LR 0.000060    Time 0.017753    
2023-01-06 16:09:57,257 - Epoch: [3][  160/  246]    Overall Loss 0.440789    Objective Loss 0.440789                                        LR 0.000060    Time 0.017654    
2023-01-06 16:09:57,406 - Epoch: [3][  170/  246]    Overall Loss 0.440605    Objective Loss 0.440605                                        LR 0.000060    Time 0.017493    
2023-01-06 16:09:57,552 - Epoch: [3][  180/  246]    Overall Loss 0.440107    Objective Loss 0.440107                                        LR 0.000060    Time 0.017330    
2023-01-06 16:09:57,687 - Epoch: [3][  190/  246]    Overall Loss 0.441828    Objective Loss 0.441828                                        LR 0.000060    Time 0.017125    
2023-01-06 16:09:57,820 - Epoch: [3][  200/  246]    Overall Loss 0.441537    Objective Loss 0.441537                                        LR 0.000060    Time 0.016931    
2023-01-06 16:09:57,953 - Epoch: [3][  210/  246]    Overall Loss 0.441782    Objective Loss 0.441782                                        LR 0.000060    Time 0.016760    
2023-01-06 16:09:58,089 - Epoch: [3][  220/  246]    Overall Loss 0.441485    Objective Loss 0.441485                                        LR 0.000060    Time 0.016614    
2023-01-06 16:09:58,223 - Epoch: [3][  230/  246]    Overall Loss 0.440630    Objective Loss 0.440630                                        LR 0.000060    Time 0.016472    
2023-01-06 16:09:58,368 - Epoch: [3][  240/  246]    Overall Loss 0.440634    Objective Loss 0.440634                                        LR 0.000060    Time 0.016388    
2023-01-06 16:09:58,440 - Epoch: [3][  246/  246]    Overall Loss 0.440307    Objective Loss 0.440307    Top1 85.167464    LR 0.000060    Time 0.016281    
2023-01-06 16:09:58,582 - --- validate (epoch=3)-----------
2023-01-06 16:09:58,582 - 6986 samples (256 per mini-batch)
2023-01-06 16:09:59,322 - Epoch: [3][   10/   28]    Loss 0.414885    Top1 85.820312    
2023-01-06 16:09:59,430 - Epoch: [3][   20/   28]    Loss 0.428968    Top1 85.292969    
2023-01-06 16:09:59,488 - Epoch: [3][   28/   28]    Loss 0.430402    Top1 85.113083    
2023-01-06 16:09:59,654 - ==> Top1: 85.113    Loss: 0.430

2023-01-06 16:09:59,654 - ==> Confusion:
[[  24    0  415]
 [   0    0  602]
 [  22    1 5922]]

2023-01-06 16:09:59,655 - ==> Best [Top1: 85.142   Sparsity:0.00   Params: 46192 on epoch: 2]
2023-01-06 16:09:59,655 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:09:59,660 - 

2023-01-06 16:09:59,660 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:00,175 - Epoch: [4][   10/  246]    Overall Loss 0.438288    Objective Loss 0.438288                                        LR 0.000060    Time 0.051465    
2023-01-06 16:10:00,320 - Epoch: [4][   20/  246]    Overall Loss 0.432254    Objective Loss 0.432254                                        LR 0.000060    Time 0.032947    
2023-01-06 16:10:00,467 - Epoch: [4][   30/  246]    Overall Loss 0.435002    Objective Loss 0.435002                                        LR 0.000060    Time 0.026831    
2023-01-06 16:10:00,614 - Epoch: [4][   40/  246]    Overall Loss 0.434909    Objective Loss 0.434909                                        LR 0.000060    Time 0.023812    
2023-01-06 16:10:00,760 - Epoch: [4][   50/  246]    Overall Loss 0.433636    Objective Loss 0.433636                                        LR 0.000060    Time 0.021944    
2023-01-06 16:10:00,907 - Epoch: [4][   60/  246]    Overall Loss 0.432816    Objective Loss 0.432816                                        LR 0.000060    Time 0.020731    
2023-01-06 16:10:01,046 - Epoch: [4][   70/  246]    Overall Loss 0.431565    Objective Loss 0.431565                                        LR 0.000060    Time 0.019760    
2023-01-06 16:10:01,185 - Epoch: [4][   80/  246]    Overall Loss 0.432934    Objective Loss 0.432934                                        LR 0.000060    Time 0.019024    
2023-01-06 16:10:01,325 - Epoch: [4][   90/  246]    Overall Loss 0.431092    Objective Loss 0.431092                                        LR 0.000060    Time 0.018454    
2023-01-06 16:10:01,464 - Epoch: [4][  100/  246]    Overall Loss 0.430300    Objective Loss 0.430300                                        LR 0.000060    Time 0.018001    
2023-01-06 16:10:01,604 - Epoch: [4][  110/  246]    Overall Loss 0.430291    Objective Loss 0.430291                                        LR 0.000060    Time 0.017631    
2023-01-06 16:10:01,744 - Epoch: [4][  120/  246]    Overall Loss 0.427407    Objective Loss 0.427407                                        LR 0.000060    Time 0.017327    
2023-01-06 16:10:01,882 - Epoch: [4][  130/  246]    Overall Loss 0.428943    Objective Loss 0.428943                                        LR 0.000060    Time 0.017051    
2023-01-06 16:10:02,021 - Epoch: [4][  140/  246]    Overall Loss 0.429122    Objective Loss 0.429122                                        LR 0.000060    Time 0.016820    
2023-01-06 16:10:02,160 - Epoch: [4][  150/  246]    Overall Loss 0.429564    Objective Loss 0.429564                                        LR 0.000060    Time 0.016621    
2023-01-06 16:10:02,298 - Epoch: [4][  160/  246]    Overall Loss 0.430680    Objective Loss 0.430680                                        LR 0.000060    Time 0.016446    
2023-01-06 16:10:02,438 - Epoch: [4][  170/  246]    Overall Loss 0.429591    Objective Loss 0.429591                                        LR 0.000060    Time 0.016298    
2023-01-06 16:10:02,578 - Epoch: [4][  180/  246]    Overall Loss 0.429990    Objective Loss 0.429990                                        LR 0.000060    Time 0.016166    
2023-01-06 16:10:02,717 - Epoch: [4][  190/  246]    Overall Loss 0.429143    Objective Loss 0.429143                                        LR 0.000060    Time 0.016045    
2023-01-06 16:10:02,856 - Epoch: [4][  200/  246]    Overall Loss 0.427709    Objective Loss 0.427709                                        LR 0.000060    Time 0.015939    
2023-01-06 16:10:02,996 - Epoch: [4][  210/  246]    Overall Loss 0.427925    Objective Loss 0.427925                                        LR 0.000060    Time 0.015841    
2023-01-06 16:10:03,136 - Epoch: [4][  220/  246]    Overall Loss 0.428664    Objective Loss 0.428664                                        LR 0.000060    Time 0.015755    
2023-01-06 16:10:03,275 - Epoch: [4][  230/  246]    Overall Loss 0.429238    Objective Loss 0.429238                                        LR 0.000060    Time 0.015677    
2023-01-06 16:10:03,427 - Epoch: [4][  240/  246]    Overall Loss 0.429823    Objective Loss 0.429823                                        LR 0.000060    Time 0.015653    
2023-01-06 16:10:03,498 - Epoch: [4][  246/  246]    Overall Loss 0.429962    Objective Loss 0.429962    Top1 83.253589    LR 0.000060    Time 0.015558    
2023-01-06 16:10:03,630 - --- validate (epoch=4)-----------
2023-01-06 16:10:03,630 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:04,059 - Epoch: [4][   10/   28]    Loss 0.437636    Top1 84.843750    
2023-01-06 16:10:04,159 - Epoch: [4][   20/   28]    Loss 0.433619    Top1 84.902344    
2023-01-06 16:10:04,217 - Epoch: [4][   28/   28]    Loss 0.423114    Top1 85.213284    
2023-01-06 16:10:04,360 - ==> Top1: 85.213    Loss: 0.423

2023-01-06 16:10:04,360 - ==> Confusion:
[[  52    1  386]
 [   0    4  598]
 [  47    1 5897]]

2023-01-06 16:10:04,361 - ==> Best [Top1: 85.213   Sparsity:0.00   Params: 46192 on epoch: 4]
2023-01-06 16:10:04,361 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:04,366 - 

2023-01-06 16:10:04,366 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:05,026 - Epoch: [5][   10/  246]    Overall Loss 0.434625    Objective Loss 0.434625                                        LR 0.000060    Time 0.065876    
2023-01-06 16:10:05,171 - Epoch: [5][   20/  246]    Overall Loss 0.421939    Objective Loss 0.421939                                        LR 0.000060    Time 0.040187    
2023-01-06 16:10:05,311 - Epoch: [5][   30/  246]    Overall Loss 0.421512    Objective Loss 0.421512                                        LR 0.000060    Time 0.031456    
2023-01-06 16:10:05,457 - Epoch: [5][   40/  246]    Overall Loss 0.423071    Objective Loss 0.423071                                        LR 0.000060    Time 0.027186    
2023-01-06 16:10:05,602 - Epoch: [5][   50/  246]    Overall Loss 0.423077    Objective Loss 0.423077                                        LR 0.000060    Time 0.024649    
2023-01-06 16:10:05,762 - Epoch: [5][   60/  246]    Overall Loss 0.424030    Objective Loss 0.424030                                        LR 0.000060    Time 0.023198    
2023-01-06 16:10:05,932 - Epoch: [5][   70/  246]    Overall Loss 0.427148    Objective Loss 0.427148                                        LR 0.000060    Time 0.022285    
2023-01-06 16:10:06,099 - Epoch: [5][   80/  246]    Overall Loss 0.429836    Objective Loss 0.429836                                        LR 0.000060    Time 0.021573    
2023-01-06 16:10:06,271 - Epoch: [5][   90/  246]    Overall Loss 0.429010    Objective Loss 0.429010                                        LR 0.000060    Time 0.021067    
2023-01-06 16:10:06,437 - Epoch: [5][  100/  246]    Overall Loss 0.426792    Objective Loss 0.426792                                        LR 0.000060    Time 0.020612    
2023-01-06 16:10:06,607 - Epoch: [5][  110/  246]    Overall Loss 0.426929    Objective Loss 0.426929                                        LR 0.000060    Time 0.020283    
2023-01-06 16:10:06,773 - Epoch: [5][  120/  246]    Overall Loss 0.426498    Objective Loss 0.426498                                        LR 0.000060    Time 0.019970    
2023-01-06 16:10:06,930 - Epoch: [5][  130/  246]    Overall Loss 0.425983    Objective Loss 0.425983                                        LR 0.000060    Time 0.019645    
2023-01-06 16:10:07,094 - Epoch: [5][  140/  246]    Overall Loss 0.425333    Objective Loss 0.425333                                        LR 0.000060    Time 0.019409    
2023-01-06 16:10:07,261 - Epoch: [5][  150/  246]    Overall Loss 0.423892    Objective Loss 0.423892                                        LR 0.000060    Time 0.019223    
2023-01-06 16:10:07,428 - Epoch: [5][  160/  246]    Overall Loss 0.425110    Objective Loss 0.425110                                        LR 0.000060    Time 0.019060    
2023-01-06 16:10:07,594 - Epoch: [5][  170/  246]    Overall Loss 0.425971    Objective Loss 0.425971                                        LR 0.000060    Time 0.018918    
2023-01-06 16:10:07,752 - Epoch: [5][  180/  246]    Overall Loss 0.424848    Objective Loss 0.424848                                        LR 0.000060    Time 0.018741    
2023-01-06 16:10:07,919 - Epoch: [5][  190/  246]    Overall Loss 0.424376    Objective Loss 0.424376                                        LR 0.000060    Time 0.018632    
2023-01-06 16:10:08,081 - Epoch: [5][  200/  246]    Overall Loss 0.423874    Objective Loss 0.423874                                        LR 0.000060    Time 0.018507    
2023-01-06 16:10:08,242 - Epoch: [5][  210/  246]    Overall Loss 0.424575    Objective Loss 0.424575                                        LR 0.000060    Time 0.018383    
2023-01-06 16:10:08,405 - Epoch: [5][  220/  246]    Overall Loss 0.424136    Objective Loss 0.424136                                        LR 0.000060    Time 0.018287    
2023-01-06 16:10:08,565 - Epoch: [5][  230/  246]    Overall Loss 0.424559    Objective Loss 0.424559                                        LR 0.000060    Time 0.018187    
2023-01-06 16:10:08,742 - Epoch: [5][  240/  246]    Overall Loss 0.424285    Objective Loss 0.424285                                        LR 0.000060    Time 0.018165    
2023-01-06 16:10:08,822 - Epoch: [5][  246/  246]    Overall Loss 0.424238    Objective Loss 0.424238    Top1 82.296651    LR 0.000060    Time 0.018046    
2023-01-06 16:10:08,974 - --- validate (epoch=5)-----------
2023-01-06 16:10:08,975 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:09,431 - Epoch: [5][   10/   28]    Loss 0.428710    Top1 84.726562    
2023-01-06 16:10:09,544 - Epoch: [5][   20/   28]    Loss 0.421385    Top1 85.117188    
2023-01-06 16:10:09,600 - Epoch: [5][   28/   28]    Loss 0.417135    Top1 85.270541    
2023-01-06 16:10:09,743 - ==> Top1: 85.271    Loss: 0.417

2023-01-06 16:10:09,743 - ==> Confusion:
[[  38    1  400]
 [   0    6  596]
 [  31    1 5913]]

2023-01-06 16:10:09,744 - ==> Best [Top1: 85.271   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:10:09,744 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:09,749 - 

2023-01-06 16:10:09,749 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:10,294 - Epoch: [6][   10/  246]    Overall Loss 0.429165    Objective Loss 0.429165                                        LR 0.000060    Time 0.054427    
2023-01-06 16:10:10,440 - Epoch: [6][   20/  246]    Overall Loss 0.437479    Objective Loss 0.437479                                        LR 0.000060    Time 0.034489    
2023-01-06 16:10:10,581 - Epoch: [6][   30/  246]    Overall Loss 0.432334    Objective Loss 0.432334                                        LR 0.000060    Time 0.027665    
2023-01-06 16:10:10,719 - Epoch: [6][   40/  246]    Overall Loss 0.426087    Objective Loss 0.426087                                        LR 0.000060    Time 0.024191    
2023-01-06 16:10:10,860 - Epoch: [6][   50/  246]    Overall Loss 0.419530    Objective Loss 0.419530                                        LR 0.000060    Time 0.022163    
2023-01-06 16:10:11,009 - Epoch: [6][   60/  246]    Overall Loss 0.416297    Objective Loss 0.416297                                        LR 0.000060    Time 0.020940    
2023-01-06 16:10:11,170 - Epoch: [6][   70/  246]    Overall Loss 0.417881    Objective Loss 0.417881                                        LR 0.000060    Time 0.020250    
2023-01-06 16:10:11,336 - Epoch: [6][   80/  246]    Overall Loss 0.419566    Objective Loss 0.419566                                        LR 0.000060    Time 0.019794    
2023-01-06 16:10:11,508 - Epoch: [6][   90/  246]    Overall Loss 0.417557    Objective Loss 0.417557                                        LR 0.000060    Time 0.019491    
2023-01-06 16:10:11,680 - Epoch: [6][  100/  246]    Overall Loss 0.417552    Objective Loss 0.417552                                        LR 0.000060    Time 0.019258    
2023-01-06 16:10:11,856 - Epoch: [6][  110/  246]    Overall Loss 0.417271    Objective Loss 0.417271                                        LR 0.000060    Time 0.019108    
2023-01-06 16:10:12,020 - Epoch: [6][  120/  246]    Overall Loss 0.417618    Objective Loss 0.417618                                        LR 0.000060    Time 0.018876    
2023-01-06 16:10:12,189 - Epoch: [6][  130/  246]    Overall Loss 0.416218    Objective Loss 0.416218                                        LR 0.000060    Time 0.018722    
2023-01-06 16:10:12,359 - Epoch: [6][  140/  246]    Overall Loss 0.418004    Objective Loss 0.418004                                        LR 0.000060    Time 0.018598    
2023-01-06 16:10:12,532 - Epoch: [6][  150/  246]    Overall Loss 0.418603    Objective Loss 0.418603                                        LR 0.000060    Time 0.018509    
2023-01-06 16:10:12,672 - Epoch: [6][  160/  246]    Overall Loss 0.419846    Objective Loss 0.419846                                        LR 0.000060    Time 0.018221    
2023-01-06 16:10:12,816 - Epoch: [6][  170/  246]    Overall Loss 0.419223    Objective Loss 0.419223                                        LR 0.000060    Time 0.017994    
2023-01-06 16:10:12,972 - Epoch: [6][  180/  246]    Overall Loss 0.419645    Objective Loss 0.419645                                        LR 0.000060    Time 0.017858    
2023-01-06 16:10:13,134 - Epoch: [6][  190/  246]    Overall Loss 0.420360    Objective Loss 0.420360                                        LR 0.000060    Time 0.017773    
2023-01-06 16:10:13,302 - Epoch: [6][  200/  246]    Overall Loss 0.420465    Objective Loss 0.420465                                        LR 0.000060    Time 0.017719    
2023-01-06 16:10:13,468 - Epoch: [6][  210/  246]    Overall Loss 0.420649    Objective Loss 0.420649                                        LR 0.000060    Time 0.017665    
2023-01-06 16:10:13,648 - Epoch: [6][  220/  246]    Overall Loss 0.420012    Objective Loss 0.420012                                        LR 0.000060    Time 0.017678    
2023-01-06 16:10:13,845 - Epoch: [6][  230/  246]    Overall Loss 0.420512    Objective Loss 0.420512                                        LR 0.000060    Time 0.017765    
2023-01-06 16:10:14,056 - Epoch: [6][  240/  246]    Overall Loss 0.420043    Objective Loss 0.420043                                        LR 0.000060    Time 0.017891    
2023-01-06 16:10:14,150 - Epoch: [6][  246/  246]    Overall Loss 0.419226    Objective Loss 0.419226    Top1 87.799043    LR 0.000060    Time 0.017839    
2023-01-06 16:10:14,291 - --- validate (epoch=6)-----------
2023-01-06 16:10:14,291 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:14,712 - Epoch: [6][   10/   28]    Loss 0.426500    Top1 84.843750    
2023-01-06 16:10:14,813 - Epoch: [6][   20/   28]    Loss 0.411932    Top1 85.410156    
2023-01-06 16:10:14,871 - Epoch: [6][   28/   28]    Loss 0.421270    Top1 85.027197    
2023-01-06 16:10:15,026 - ==> Top1: 85.027    Loss: 0.421

2023-01-06 16:10:15,027 - ==> Confusion:
[[   2    0  437]
 [   0    0  602]
 [   7    0 5938]]

2023-01-06 16:10:15,028 - ==> Best [Top1: 85.271   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:10:15,028 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:15,032 - 

2023-01-06 16:10:15,032 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:15,703 - Epoch: [7][   10/  246]    Overall Loss 0.450842    Objective Loss 0.450842                                        LR 0.000060    Time 0.066938    
2023-01-06 16:10:15,881 - Epoch: [7][   20/  246]    Overall Loss 0.420996    Objective Loss 0.420996                                        LR 0.000060    Time 0.042349    
2023-01-06 16:10:16,087 - Epoch: [7][   30/  246]    Overall Loss 0.419124    Objective Loss 0.419124                                        LR 0.000060    Time 0.035086    
2023-01-06 16:10:16,288 - Epoch: [7][   40/  246]    Overall Loss 0.422462    Objective Loss 0.422462                                        LR 0.000060    Time 0.031320    
2023-01-06 16:10:16,494 - Epoch: [7][   50/  246]    Overall Loss 0.419728    Objective Loss 0.419728                                        LR 0.000060    Time 0.029179    
2023-01-06 16:10:16,700 - Epoch: [7][   60/  246]    Overall Loss 0.421121    Objective Loss 0.421121                                        LR 0.000060    Time 0.027742    
2023-01-06 16:10:16,904 - Epoch: [7][   70/  246]    Overall Loss 0.418390    Objective Loss 0.418390                                        LR 0.000060    Time 0.026678    
2023-01-06 16:10:17,097 - Epoch: [7][   80/  246]    Overall Loss 0.416471    Objective Loss 0.416471                                        LR 0.000060    Time 0.025759    
2023-01-06 16:10:17,260 - Epoch: [7][   90/  246]    Overall Loss 0.416662    Objective Loss 0.416662                                        LR 0.000060    Time 0.024704    
2023-01-06 16:10:17,413 - Epoch: [7][  100/  246]    Overall Loss 0.415966    Objective Loss 0.415966                                        LR 0.000060    Time 0.023756    
2023-01-06 16:10:17,592 - Epoch: [7][  110/  246]    Overall Loss 0.415971    Objective Loss 0.415971                                        LR 0.000060    Time 0.023217    
2023-01-06 16:10:17,735 - Epoch: [7][  120/  246]    Overall Loss 0.414804    Objective Loss 0.414804                                        LR 0.000060    Time 0.022467    
2023-01-06 16:10:17,894 - Epoch: [7][  130/  246]    Overall Loss 0.415966    Objective Loss 0.415966                                        LR 0.000060    Time 0.021962    
2023-01-06 16:10:18,064 - Epoch: [7][  140/  246]    Overall Loss 0.415878    Objective Loss 0.415878                                        LR 0.000060    Time 0.021606    
2023-01-06 16:10:18,218 - Epoch: [7][  150/  246]    Overall Loss 0.416908    Objective Loss 0.416908                                        LR 0.000060    Time 0.021182    
2023-01-06 16:10:18,387 - Epoch: [7][  160/  246]    Overall Loss 0.419144    Objective Loss 0.419144                                        LR 0.000060    Time 0.020912    
2023-01-06 16:10:18,546 - Epoch: [7][  170/  246]    Overall Loss 0.419844    Objective Loss 0.419844                                        LR 0.000060    Time 0.020609    
2023-01-06 16:10:18,706 - Epoch: [7][  180/  246]    Overall Loss 0.419669    Objective Loss 0.419669                                        LR 0.000060    Time 0.020347    
2023-01-06 16:10:18,863 - Epoch: [7][  190/  246]    Overall Loss 0.419621    Objective Loss 0.419621                                        LR 0.000060    Time 0.020093    
2023-01-06 16:10:19,038 - Epoch: [7][  200/  246]    Overall Loss 0.418294    Objective Loss 0.418294                                        LR 0.000060    Time 0.019965    
2023-01-06 16:10:19,220 - Epoch: [7][  210/  246]    Overall Loss 0.417635    Objective Loss 0.417635                                        LR 0.000060    Time 0.019879    
2023-01-06 16:10:19,402 - Epoch: [7][  220/  246]    Overall Loss 0.417032    Objective Loss 0.417032                                        LR 0.000060    Time 0.019801    
2023-01-06 16:10:19,590 - Epoch: [7][  230/  246]    Overall Loss 0.417791    Objective Loss 0.417791                                        LR 0.000060    Time 0.019747    
2023-01-06 16:10:19,778 - Epoch: [7][  240/  246]    Overall Loss 0.417236    Objective Loss 0.417236                                        LR 0.000060    Time 0.019704    
2023-01-06 16:10:19,860 - Epoch: [7][  246/  246]    Overall Loss 0.417544    Objective Loss 0.417544    Top1 82.296651    LR 0.000060    Time 0.019556    
2023-01-06 16:10:20,021 - --- validate (epoch=7)-----------
2023-01-06 16:10:20,021 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:20,461 - Epoch: [7][   10/   28]    Loss 0.415063    Top1 84.921875    
2023-01-06 16:10:20,569 - Epoch: [7][   20/   28]    Loss 0.404213    Top1 85.566406    
2023-01-06 16:10:20,627 - Epoch: [7][   28/   28]    Loss 0.412302    Top1 85.170341    
2023-01-06 16:10:20,764 - ==> Top1: 85.170    Loss: 0.412

2023-01-06 16:10:20,764 - ==> Confusion:
[[  68    1  370]
 [   2   10  590]
 [  66    7 5872]]

2023-01-06 16:10:20,765 - ==> Best [Top1: 85.271   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:10:20,765 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:20,770 - 

2023-01-06 16:10:20,770 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:21,442 - Epoch: [8][   10/  246]    Overall Loss 0.402969    Objective Loss 0.402969                                        LR 0.000060    Time 0.067128    
2023-01-06 16:10:21,599 - Epoch: [8][   20/  246]    Overall Loss 0.417175    Objective Loss 0.417175                                        LR 0.000060    Time 0.041421    
2023-01-06 16:10:21,758 - Epoch: [8][   30/  246]    Overall Loss 0.414786    Objective Loss 0.414786                                        LR 0.000060    Time 0.032891    
2023-01-06 16:10:21,916 - Epoch: [8][   40/  246]    Overall Loss 0.414411    Objective Loss 0.414411                                        LR 0.000060    Time 0.028619    
2023-01-06 16:10:22,075 - Epoch: [8][   50/  246]    Overall Loss 0.414575    Objective Loss 0.414575                                        LR 0.000060    Time 0.026065    
2023-01-06 16:10:22,234 - Epoch: [8][   60/  246]    Overall Loss 0.413145    Objective Loss 0.413145                                        LR 0.000060    Time 0.024355    
2023-01-06 16:10:22,401 - Epoch: [8][   70/  246]    Overall Loss 0.415756    Objective Loss 0.415756                                        LR 0.000060    Time 0.023265    
2023-01-06 16:10:22,576 - Epoch: [8][   80/  246]    Overall Loss 0.413138    Objective Loss 0.413138                                        LR 0.000060    Time 0.022530    
2023-01-06 16:10:22,747 - Epoch: [8][   90/  246]    Overall Loss 0.413393    Objective Loss 0.413393                                        LR 0.000060    Time 0.021926    
2023-01-06 16:10:22,918 - Epoch: [8][  100/  246]    Overall Loss 0.414305    Objective Loss 0.414305                                        LR 0.000060    Time 0.021439    
2023-01-06 16:10:23,096 - Epoch: [8][  110/  246]    Overall Loss 0.414335    Objective Loss 0.414335                                        LR 0.000060    Time 0.021108    
2023-01-06 16:10:23,287 - Epoch: [8][  120/  246]    Overall Loss 0.415143    Objective Loss 0.415143                                        LR 0.000060    Time 0.020933    
2023-01-06 16:10:23,493 - Epoch: [8][  130/  246]    Overall Loss 0.415169    Objective Loss 0.415169                                        LR 0.000060    Time 0.020907    
2023-01-06 16:10:23,680 - Epoch: [8][  140/  246]    Overall Loss 0.415468    Objective Loss 0.415468                                        LR 0.000060    Time 0.020743    
2023-01-06 16:10:23,852 - Epoch: [8][  150/  246]    Overall Loss 0.414704    Objective Loss 0.414704                                        LR 0.000060    Time 0.020505    
2023-01-06 16:10:24,029 - Epoch: [8][  160/  246]    Overall Loss 0.415103    Objective Loss 0.415103                                        LR 0.000060    Time 0.020331    
2023-01-06 16:10:24,201 - Epoch: [8][  170/  246]    Overall Loss 0.413826    Objective Loss 0.413826                                        LR 0.000060    Time 0.020142    
2023-01-06 16:10:24,379 - Epoch: [8][  180/  246]    Overall Loss 0.413038    Objective Loss 0.413038                                        LR 0.000060    Time 0.020013    
2023-01-06 16:10:24,551 - Epoch: [8][  190/  246]    Overall Loss 0.413441    Objective Loss 0.413441                                        LR 0.000060    Time 0.019861    
2023-01-06 16:10:24,730 - Epoch: [8][  200/  246]    Overall Loss 0.413045    Objective Loss 0.413045                                        LR 0.000060    Time 0.019755    
2023-01-06 16:10:24,905 - Epoch: [8][  210/  246]    Overall Loss 0.413878    Objective Loss 0.413878                                        LR 0.000060    Time 0.019647    
2023-01-06 16:10:25,084 - Epoch: [8][  220/  246]    Overall Loss 0.413242    Objective Loss 0.413242                                        LR 0.000060    Time 0.019566    
2023-01-06 16:10:25,250 - Epoch: [8][  230/  246]    Overall Loss 0.413396    Objective Loss 0.413396                                        LR 0.000060    Time 0.019435    
2023-01-06 16:10:25,424 - Epoch: [8][  240/  246]    Overall Loss 0.413792    Objective Loss 0.413792                                        LR 0.000060    Time 0.019350    
2023-01-06 16:10:25,507 - Epoch: [8][  246/  246]    Overall Loss 0.414095    Objective Loss 0.414095    Top1 85.406699    LR 0.000060    Time 0.019214    
2023-01-06 16:10:25,649 - --- validate (epoch=8)-----------
2023-01-06 16:10:25,649 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:26,075 - Epoch: [8][   10/   28]    Loss 0.419642    Top1 84.570312    
2023-01-06 16:10:26,179 - Epoch: [8][   20/   28]    Loss 0.410255    Top1 85.214844    
2023-01-06 16:10:26,238 - Epoch: [8][   28/   28]    Loss 0.408963    Top1 85.241912    
2023-01-06 16:10:26,385 - ==> Top1: 85.242    Loss: 0.409

2023-01-06 16:10:26,385 - ==> Confusion:
[[  24    1  414]
 [   0    3  599]
 [  16    1 5928]]

2023-01-06 16:10:26,386 - ==> Best [Top1: 85.271   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:10:26,386 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:26,391 - 

2023-01-06 16:10:26,391 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:26,936 - Epoch: [9][   10/  246]    Overall Loss 0.414761    Objective Loss 0.414761                                        LR 0.000060    Time 0.054484    
2023-01-06 16:10:27,093 - Epoch: [9][   20/  246]    Overall Loss 0.403519    Objective Loss 0.403519                                        LR 0.000060    Time 0.035067    
2023-01-06 16:10:27,248 - Epoch: [9][   30/  246]    Overall Loss 0.402222    Objective Loss 0.402222                                        LR 0.000060    Time 0.028520    
2023-01-06 16:10:27,410 - Epoch: [9][   40/  246]    Overall Loss 0.403614    Objective Loss 0.403614                                        LR 0.000060    Time 0.025423    
2023-01-06 16:10:27,565 - Epoch: [9][   50/  246]    Overall Loss 0.402288    Objective Loss 0.402288                                        LR 0.000060    Time 0.023441    
2023-01-06 16:10:27,731 - Epoch: [9][   60/  246]    Overall Loss 0.405620    Objective Loss 0.405620                                        LR 0.000060    Time 0.022284    
2023-01-06 16:10:27,887 - Epoch: [9][   70/  246]    Overall Loss 0.409924    Objective Loss 0.409924                                        LR 0.000060    Time 0.021313    
2023-01-06 16:10:28,051 - Epoch: [9][   80/  246]    Overall Loss 0.407714    Objective Loss 0.407714                                        LR 0.000060    Time 0.020682    
2023-01-06 16:10:28,207 - Epoch: [9][   90/  246]    Overall Loss 0.408231    Objective Loss 0.408231                                        LR 0.000060    Time 0.020086    
2023-01-06 16:10:28,361 - Epoch: [9][  100/  246]    Overall Loss 0.408325    Objective Loss 0.408325                                        LR 0.000060    Time 0.019620    
2023-01-06 16:10:28,515 - Epoch: [9][  110/  246]    Overall Loss 0.408684    Objective Loss 0.408684                                        LR 0.000060    Time 0.019232    
2023-01-06 16:10:28,667 - Epoch: [9][  120/  246]    Overall Loss 0.408306    Objective Loss 0.408306                                        LR 0.000060    Time 0.018890    
2023-01-06 16:10:28,823 - Epoch: [9][  130/  246]    Overall Loss 0.407245    Objective Loss 0.407245                                        LR 0.000060    Time 0.018633    
2023-01-06 16:10:28,975 - Epoch: [9][  140/  246]    Overall Loss 0.408002    Objective Loss 0.408002                                        LR 0.000060    Time 0.018388    
2023-01-06 16:10:29,130 - Epoch: [9][  150/  246]    Overall Loss 0.409067    Objective Loss 0.409067                                        LR 0.000060    Time 0.018192    
2023-01-06 16:10:29,285 - Epoch: [9][  160/  246]    Overall Loss 0.409213    Objective Loss 0.409213                                        LR 0.000060    Time 0.018017    
2023-01-06 16:10:29,444 - Epoch: [9][  170/  246]    Overall Loss 0.410010    Objective Loss 0.410010                                        LR 0.000060    Time 0.017895    
2023-01-06 16:10:29,601 - Epoch: [9][  180/  246]    Overall Loss 0.409501    Objective Loss 0.409501                                        LR 0.000060    Time 0.017768    
2023-01-06 16:10:29,762 - Epoch: [9][  190/  246]    Overall Loss 0.410041    Objective Loss 0.410041                                        LR 0.000060    Time 0.017680    
2023-01-06 16:10:29,921 - Epoch: [9][  200/  246]    Overall Loss 0.410023    Objective Loss 0.410023                                        LR 0.000060    Time 0.017588    
2023-01-06 16:10:30,084 - Epoch: [9][  210/  246]    Overall Loss 0.410087    Objective Loss 0.410087                                        LR 0.000060    Time 0.017524    
2023-01-06 16:10:30,243 - Epoch: [9][  220/  246]    Overall Loss 0.409859    Objective Loss 0.409859                                        LR 0.000060    Time 0.017450    
2023-01-06 16:10:30,405 - Epoch: [9][  230/  246]    Overall Loss 0.409578    Objective Loss 0.409578                                        LR 0.000060    Time 0.017393    
2023-01-06 16:10:30,578 - Epoch: [9][  240/  246]    Overall Loss 0.410186    Objective Loss 0.410186                                        LR 0.000060    Time 0.017387    
2023-01-06 16:10:30,659 - Epoch: [9][  246/  246]    Overall Loss 0.410154    Objective Loss 0.410154    Top1 85.167464    LR 0.000060    Time 0.017291    
2023-01-06 16:10:30,775 - --- validate (epoch=9)-----------
2023-01-06 16:10:30,775 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:31,211 - Epoch: [9][   10/   28]    Loss 0.419648    Top1 84.492188    
2023-01-06 16:10:31,312 - Epoch: [9][   20/   28]    Loss 0.400266    Top1 85.566406    
2023-01-06 16:10:31,371 - Epoch: [9][   28/   28]    Loss 0.406207    Top1 85.241912    
2023-01-06 16:10:31,512 - ==> Top1: 85.242    Loss: 0.406

2023-01-06 16:10:31,512 - ==> Confusion:
[[  27    1  411]
 [   0    6  596]
 [  18    5 5922]]

2023-01-06 16:10:31,513 - ==> Best [Top1: 85.271   Sparsity:0.00   Params: 46192 on epoch: 5]
2023-01-06 16:10:31,513 - Saving checkpoint to: logs/2023.01.06-160846/checkpoint.pth.tar
2023-01-06 16:10:31,531 - 

2023-01-06 16:10:31,531 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:32,213 - Epoch: [10][   10/  246]    Overall Loss 0.434388    Objective Loss 0.434388                                        LR 0.000060    Time 0.068176    
2023-01-06 16:10:32,377 - Epoch: [10][   20/  246]    Overall Loss 0.429890    Objective Loss 0.429890                                        LR 0.000060    Time 0.042234    
2023-01-06 16:10:32,535 - Epoch: [10][   30/  246]    Overall Loss 0.421520    Objective Loss 0.421520                                        LR 0.000060    Time 0.033429    
2023-01-06 16:10:32,690 - Epoch: [10][   40/  246]    Overall Loss 0.421226    Objective Loss 0.421226                                        LR 0.000060    Time 0.028925    
2023-01-06 16:10:32,852 - Epoch: [10][   50/  246]    Overall Loss 0.415680    Objective Loss 0.415680                                        LR 0.000060    Time 0.026372    
2023-01-06 16:10:33,015 - Epoch: [10][   60/  246]    Overall Loss 0.409954    Objective Loss 0.409954                                        LR 0.000060    Time 0.024692    
2023-01-06 16:10:33,176 - Epoch: [10][   70/  246]    Overall Loss 0.411837    Objective Loss 0.411837                                        LR 0.000060    Time 0.023451    
2023-01-06 16:10:33,335 - Epoch: [10][   80/  246]    Overall Loss 0.415201    Objective Loss 0.415201                                        LR 0.000060    Time 0.022511    
2023-01-06 16:10:33,495 - Epoch: [10][   90/  246]    Overall Loss 0.414217    Objective Loss 0.414217                                        LR 0.000060    Time 0.021782    
2023-01-06 16:10:33,655 - Epoch: [10][  100/  246]    Overall Loss 0.414383    Objective Loss 0.414383                                        LR 0.000060    Time 0.021201    
2023-01-06 16:10:33,815 - Epoch: [10][  110/  246]    Overall Loss 0.411494    Objective Loss 0.411494                                        LR 0.000060    Time 0.020725    
2023-01-06 16:10:33,975 - Epoch: [10][  120/  246]    Overall Loss 0.409330    Objective Loss 0.409330                                        LR 0.000060    Time 0.020328    
2023-01-06 16:10:34,138 - Epoch: [10][  130/  246]    Overall Loss 0.408735    Objective Loss 0.408735                                        LR 0.000060    Time 0.020018    
2023-01-06 16:10:34,303 - Epoch: [10][  140/  246]    Overall Loss 0.408692    Objective Loss 0.408692                                        LR 0.000060    Time 0.019760    
2023-01-06 16:10:34,471 - Epoch: [10][  150/  246]    Overall Loss 0.408272    Objective Loss 0.408272                                        LR 0.000060    Time 0.019565    
2023-01-06 16:10:34,643 - Epoch: [10][  160/  246]    Overall Loss 0.407170    Objective Loss 0.407170                                        LR 0.000060    Time 0.019411    
2023-01-06 16:10:34,812 - Epoch: [10][  170/  246]    Overall Loss 0.407128    Objective Loss 0.407128                                        LR 0.000060    Time 0.019264    
2023-01-06 16:10:34,985 - Epoch: [10][  180/  246]    Overall Loss 0.406003    Objective Loss 0.406003                                        LR 0.000060    Time 0.019150    
2023-01-06 16:10:35,159 - Epoch: [10][  190/  246]    Overall Loss 0.407547    Objective Loss 0.407547                                        LR 0.000060    Time 0.019057    
2023-01-06 16:10:35,333 - Epoch: [10][  200/  246]    Overall Loss 0.408179    Objective Loss 0.408179                                        LR 0.000060    Time 0.018972    
2023-01-06 16:10:35,495 - Epoch: [10][  210/  246]    Overall Loss 0.406536    Objective Loss 0.406536                                        LR 0.000060    Time 0.018840    
2023-01-06 16:10:35,658 - Epoch: [10][  220/  246]    Overall Loss 0.406349    Objective Loss 0.406349                                        LR 0.000060    Time 0.018725    
2023-01-06 16:10:35,822 - Epoch: [10][  230/  246]    Overall Loss 0.406614    Objective Loss 0.406614                                        LR 0.000060    Time 0.018621    
2023-01-06 16:10:35,999 - Epoch: [10][  240/  246]    Overall Loss 0.406979    Objective Loss 0.406979                                        LR 0.000060    Time 0.018582    
2023-01-06 16:10:36,078 - Epoch: [10][  246/  246]    Overall Loss 0.406656    Objective Loss 0.406656    Top1 86.363636    LR 0.000060    Time 0.018450    
2023-01-06 16:10:36,229 - --- validate (epoch=10)-----------
2023-01-06 16:10:36,230 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:36,653 - Epoch: [10][   10/   28]    Loss 0.401360    Top1 85.468750    
2023-01-06 16:10:36,753 - Epoch: [10][   20/   28]    Loss 0.405391    Top1 85.429688    
2023-01-06 16:10:36,813 - Epoch: [10][   28/   28]    Loss 0.396946    Top1 85.499571    
2023-01-06 16:10:36,974 - ==> Top1: 85.500    Loss: 0.397

2023-01-06 16:10:36,974 - ==> Confusion:
[[  63    1  375]
 [   0   11  591]
 [  36   10 5899]]

2023-01-06 16:10:36,975 - ==> Best [Top1: 85.500   Sparsity:0.00   Params: 46192 on epoch: 10]
2023-01-06 16:10:36,975 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:10:36,980 - 

2023-01-06 16:10:36,980 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:37,518 - Epoch: [11][   10/  246]    Overall Loss 0.402966    Objective Loss 0.402966                                        LR 0.000060    Time 0.053663    
2023-01-06 16:10:37,681 - Epoch: [11][   20/  246]    Overall Loss 0.412786    Objective Loss 0.412786                                        LR 0.000060    Time 0.034933    
2023-01-06 16:10:37,839 - Epoch: [11][   30/  246]    Overall Loss 0.406263    Objective Loss 0.406263                                        LR 0.000060    Time 0.028530    
2023-01-06 16:10:37,993 - Epoch: [11][   40/  246]    Overall Loss 0.411448    Objective Loss 0.411448                                        LR 0.000060    Time 0.025242    
2023-01-06 16:10:38,153 - Epoch: [11][   50/  246]    Overall Loss 0.409645    Objective Loss 0.409645                                        LR 0.000060    Time 0.023379    
2023-01-06 16:10:38,313 - Epoch: [11][   60/  246]    Overall Loss 0.409836    Objective Loss 0.409836                                        LR 0.000060    Time 0.022151    
2023-01-06 16:10:38,474 - Epoch: [11][   70/  246]    Overall Loss 0.407521    Objective Loss 0.407521                                        LR 0.000060    Time 0.021282    
2023-01-06 16:10:38,638 - Epoch: [11][   80/  246]    Overall Loss 0.407474    Objective Loss 0.407474                                        LR 0.000060    Time 0.020670    
2023-01-06 16:10:38,807 - Epoch: [11][   90/  246]    Overall Loss 0.406647    Objective Loss 0.406647                                        LR 0.000060    Time 0.020249    
2023-01-06 16:10:38,975 - Epoch: [11][  100/  246]    Overall Loss 0.407439    Objective Loss 0.407439                                        LR 0.000060    Time 0.019899    
2023-01-06 16:10:39,142 - Epoch: [11][  110/  246]    Overall Loss 0.406747    Objective Loss 0.406747                                        LR 0.000060    Time 0.019601    
2023-01-06 16:10:39,314 - Epoch: [11][  120/  246]    Overall Loss 0.406011    Objective Loss 0.406011                                        LR 0.000060    Time 0.019403    
2023-01-06 16:10:39,484 - Epoch: [11][  130/  246]    Overall Loss 0.406330    Objective Loss 0.406330                                        LR 0.000060    Time 0.019210    
2023-01-06 16:10:39,654 - Epoch: [11][  140/  246]    Overall Loss 0.406393    Objective Loss 0.406393                                        LR 0.000060    Time 0.019050    
2023-01-06 16:10:39,820 - Epoch: [11][  150/  246]    Overall Loss 0.405201    Objective Loss 0.405201                                        LR 0.000060    Time 0.018886    
2023-01-06 16:10:39,993 - Epoch: [11][  160/  246]    Overall Loss 0.406970    Objective Loss 0.406970                                        LR 0.000060    Time 0.018783    
2023-01-06 16:10:40,162 - Epoch: [11][  170/  246]    Overall Loss 0.407602    Objective Loss 0.407602                                        LR 0.000060    Time 0.018675    
2023-01-06 16:10:40,335 - Epoch: [11][  180/  246]    Overall Loss 0.409293    Objective Loss 0.409293                                        LR 0.000060    Time 0.018595    
2023-01-06 16:10:40,505 - Epoch: [11][  190/  246]    Overall Loss 0.407012    Objective Loss 0.407012                                        LR 0.000060    Time 0.018509    
2023-01-06 16:10:40,677 - Epoch: [11][  200/  246]    Overall Loss 0.407222    Objective Loss 0.407222                                        LR 0.000060    Time 0.018442    
2023-01-06 16:10:40,846 - Epoch: [11][  210/  246]    Overall Loss 0.406832    Objective Loss 0.406832                                        LR 0.000060    Time 0.018367    
2023-01-06 16:10:41,018 - Epoch: [11][  220/  246]    Overall Loss 0.406278    Objective Loss 0.406278                                        LR 0.000060    Time 0.018314    
2023-01-06 16:10:41,188 - Epoch: [11][  230/  246]    Overall Loss 0.406134    Objective Loss 0.406134                                        LR 0.000060    Time 0.018254    
2023-01-06 16:10:41,372 - Epoch: [11][  240/  246]    Overall Loss 0.405367    Objective Loss 0.405367                                        LR 0.000060    Time 0.018259    
2023-01-06 16:10:41,454 - Epoch: [11][  246/  246]    Overall Loss 0.404285    Objective Loss 0.404285    Top1 85.167464    LR 0.000060    Time 0.018147    
2023-01-06 16:10:41,592 - --- validate (epoch=11)-----------
2023-01-06 16:10:41,593 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:42,177 - Epoch: [11][   10/   28]    Loss 0.407335    Top1 85.390625    
2023-01-06 16:10:42,290 - Epoch: [11][   20/   28]    Loss 0.401169    Top1 85.664062    
2023-01-06 16:10:42,346 - Epoch: [11][   28/   28]    Loss 0.401575    Top1 85.542514    
2023-01-06 16:10:42,499 - ==> Top1: 85.543    Loss: 0.402

2023-01-06 16:10:42,499 - ==> Confusion:
[[  53    1  385]
 [   1   10  591]
 [  25    7 5913]]

2023-01-06 16:10:42,500 - ==> Best [Top1: 85.543   Sparsity:0.00   Params: 46192 on epoch: 11]
2023-01-06 16:10:42,500 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:10:42,505 - 

2023-01-06 16:10:42,505 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:43,076 - Epoch: [12][   10/  246]    Overall Loss 0.399435    Objective Loss 0.399435                                        LR 0.000060    Time 0.057051    
2023-01-06 16:10:43,256 - Epoch: [12][   20/  246]    Overall Loss 0.395468    Objective Loss 0.395468                                        LR 0.000060    Time 0.037493    
2023-01-06 16:10:43,436 - Epoch: [12][   30/  246]    Overall Loss 0.399574    Objective Loss 0.399574                                        LR 0.000060    Time 0.030976    
2023-01-06 16:10:43,618 - Epoch: [12][   40/  246]    Overall Loss 0.397550    Objective Loss 0.397550                                        LR 0.000060    Time 0.027784    
2023-01-06 16:10:43,800 - Epoch: [12][   50/  246]    Overall Loss 0.399134    Objective Loss 0.399134                                        LR 0.000060    Time 0.025841    
2023-01-06 16:10:43,977 - Epoch: [12][   60/  246]    Overall Loss 0.400817    Objective Loss 0.400817                                        LR 0.000060    Time 0.024457    
2023-01-06 16:10:44,145 - Epoch: [12][   70/  246]    Overall Loss 0.403541    Objective Loss 0.403541                                        LR 0.000060    Time 0.023356    
2023-01-06 16:10:44,313 - Epoch: [12][   80/  246]    Overall Loss 0.407059    Objective Loss 0.407059                                        LR 0.000060    Time 0.022529    
2023-01-06 16:10:44,482 - Epoch: [12][   90/  246]    Overall Loss 0.407693    Objective Loss 0.407693                                        LR 0.000060    Time 0.021904    
2023-01-06 16:10:44,654 - Epoch: [12][  100/  246]    Overall Loss 0.405357    Objective Loss 0.405357                                        LR 0.000060    Time 0.021433    
2023-01-06 16:10:44,830 - Epoch: [12][  110/  246]    Overall Loss 0.407675    Objective Loss 0.407675                                        LR 0.000060    Time 0.021076    
2023-01-06 16:10:45,002 - Epoch: [12][  120/  246]    Overall Loss 0.406980    Objective Loss 0.406980                                        LR 0.000060    Time 0.020750    
2023-01-06 16:10:45,177 - Epoch: [12][  130/  246]    Overall Loss 0.406379    Objective Loss 0.406379                                        LR 0.000060    Time 0.020499    
2023-01-06 16:10:45,346 - Epoch: [12][  140/  246]    Overall Loss 0.406116    Objective Loss 0.406116                                        LR 0.000060    Time 0.020241    
2023-01-06 16:10:45,517 - Epoch: [12][  150/  246]    Overall Loss 0.404954    Objective Loss 0.404954                                        LR 0.000060    Time 0.020027    
2023-01-06 16:10:45,679 - Epoch: [12][  160/  246]    Overall Loss 0.403062    Objective Loss 0.403062                                        LR 0.000060    Time 0.019788    
2023-01-06 16:10:45,845 - Epoch: [12][  170/  246]    Overall Loss 0.402929    Objective Loss 0.402929                                        LR 0.000060    Time 0.019595    
2023-01-06 16:10:46,015 - Epoch: [12][  180/  246]    Overall Loss 0.402065    Objective Loss 0.402065                                        LR 0.000060    Time 0.019453    
2023-01-06 16:10:46,175 - Epoch: [12][  190/  246]    Overall Loss 0.403292    Objective Loss 0.403292                                        LR 0.000060    Time 0.019265    
2023-01-06 16:10:46,340 - Epoch: [12][  200/  246]    Overall Loss 0.404077    Objective Loss 0.404077                                        LR 0.000060    Time 0.019127    
2023-01-06 16:10:46,501 - Epoch: [12][  210/  246]    Overall Loss 0.404405    Objective Loss 0.404405                                        LR 0.000060    Time 0.018980    
2023-01-06 16:10:46,665 - Epoch: [12][  220/  246]    Overall Loss 0.404274    Objective Loss 0.404274                                        LR 0.000060    Time 0.018864    
2023-01-06 16:10:46,828 - Epoch: [12][  230/  246]    Overall Loss 0.403699    Objective Loss 0.403699                                        LR 0.000060    Time 0.018739    
2023-01-06 16:10:47,005 - Epoch: [12][  240/  246]    Overall Loss 0.403195    Objective Loss 0.403195                                        LR 0.000060    Time 0.018695    
2023-01-06 16:10:47,085 - Epoch: [12][  246/  246]    Overall Loss 0.402847    Objective Loss 0.402847    Top1 85.885167    LR 0.000060    Time 0.018565    
2023-01-06 16:10:47,221 - --- validate (epoch=12)-----------
2023-01-06 16:10:47,222 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:47,667 - Epoch: [12][   10/   28]    Loss 0.396409    Top1 85.000000    
2023-01-06 16:10:47,767 - Epoch: [12][   20/   28]    Loss 0.397151    Top1 85.429688    
2023-01-06 16:10:47,824 - Epoch: [12][   28/   28]    Loss 0.396372    Top1 85.542514    
2023-01-06 16:10:47,964 - ==> Top1: 85.543    Loss: 0.396

2023-01-06 16:10:47,965 - ==> Confusion:
[[  52    1  386]
 [   0   12  590]
 [  21   12 5912]]

2023-01-06 16:10:47,966 - ==> Best [Top1: 85.543   Sparsity:0.00   Params: 46192 on epoch: 12]
2023-01-06 16:10:47,966 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:10:47,971 - 

2023-01-06 16:10:47,971 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:48,645 - Epoch: [13][   10/  246]    Overall Loss 0.414955    Objective Loss 0.414955                                        LR 0.000060    Time 0.067328    
2023-01-06 16:10:48,795 - Epoch: [13][   20/  246]    Overall Loss 0.408974    Objective Loss 0.408974                                        LR 0.000060    Time 0.041141    
2023-01-06 16:10:48,949 - Epoch: [13][   30/  246]    Overall Loss 0.410515    Objective Loss 0.410515                                        LR 0.000060    Time 0.032542    
2023-01-06 16:10:49,104 - Epoch: [13][   40/  246]    Overall Loss 0.408338    Objective Loss 0.408338                                        LR 0.000060    Time 0.028248    
2023-01-06 16:10:49,267 - Epoch: [13][   50/  246]    Overall Loss 0.406846    Objective Loss 0.406846                                        LR 0.000060    Time 0.025853    
2023-01-06 16:10:49,430 - Epoch: [13][   60/  246]    Overall Loss 0.403214    Objective Loss 0.403214                                        LR 0.000060    Time 0.024254    
2023-01-06 16:10:49,591 - Epoch: [13][   70/  246]    Overall Loss 0.399473    Objective Loss 0.399473                                        LR 0.000060    Time 0.023079    
2023-01-06 16:10:49,748 - Epoch: [13][   80/  246]    Overall Loss 0.401373    Objective Loss 0.401373                                        LR 0.000060    Time 0.022158    
2023-01-06 16:10:49,891 - Epoch: [13][   90/  246]    Overall Loss 0.399261    Objective Loss 0.399261                                        LR 0.000060    Time 0.021276    
2023-01-06 16:10:50,036 - Epoch: [13][  100/  246]    Overall Loss 0.398280    Objective Loss 0.398280                                        LR 0.000060    Time 0.020602    
2023-01-06 16:10:50,178 - Epoch: [13][  110/  246]    Overall Loss 0.394655    Objective Loss 0.394655                                        LR 0.000060    Time 0.020017    
2023-01-06 16:10:50,318 - Epoch: [13][  120/  246]    Overall Loss 0.394603    Objective Loss 0.394603                                        LR 0.000060    Time 0.019514    
2023-01-06 16:10:50,460 - Epoch: [13][  130/  246]    Overall Loss 0.395093    Objective Loss 0.395093                                        LR 0.000060    Time 0.019098    
2023-01-06 16:10:50,601 - Epoch: [13][  140/  246]    Overall Loss 0.395312    Objective Loss 0.395312                                        LR 0.000060    Time 0.018739    
2023-01-06 16:10:50,740 - Epoch: [13][  150/  246]    Overall Loss 0.395063    Objective Loss 0.395063                                        LR 0.000060    Time 0.018412    
2023-01-06 16:10:50,880 - Epoch: [13][  160/  246]    Overall Loss 0.393727    Objective Loss 0.393727                                        LR 0.000060    Time 0.018134    
2023-01-06 16:10:51,034 - Epoch: [13][  170/  246]    Overall Loss 0.394769    Objective Loss 0.394769                                        LR 0.000060    Time 0.017975    
2023-01-06 16:10:51,173 - Epoch: [13][  180/  246]    Overall Loss 0.395402    Objective Loss 0.395402                                        LR 0.000060    Time 0.017745    
2023-01-06 16:10:51,325 - Epoch: [13][  190/  246]    Overall Loss 0.395624    Objective Loss 0.395624                                        LR 0.000060    Time 0.017608    
2023-01-06 16:10:51,484 - Epoch: [13][  200/  246]    Overall Loss 0.396993    Objective Loss 0.396993                                        LR 0.000060    Time 0.017519    
2023-01-06 16:10:51,641 - Epoch: [13][  210/  246]    Overall Loss 0.397011    Objective Loss 0.397011                                        LR 0.000060    Time 0.017435    
2023-01-06 16:10:51,800 - Epoch: [13][  220/  246]    Overall Loss 0.396786    Objective Loss 0.396786                                        LR 0.000060    Time 0.017362    
2023-01-06 16:10:51,951 - Epoch: [13][  230/  246]    Overall Loss 0.396494    Objective Loss 0.396494                                        LR 0.000060    Time 0.017264    
2023-01-06 16:10:52,110 - Epoch: [13][  240/  246]    Overall Loss 0.395793    Objective Loss 0.395793                                        LR 0.000060    Time 0.017205    
2023-01-06 16:10:52,181 - Epoch: [13][  246/  246]    Overall Loss 0.395959    Objective Loss 0.395959    Top1 86.363636    LR 0.000060    Time 0.017070    
2023-01-06 16:10:52,310 - --- validate (epoch=13)-----------
2023-01-06 16:10:52,310 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:52,732 - Epoch: [13][   10/   28]    Loss 0.389853    Top1 85.859375    
2023-01-06 16:10:52,831 - Epoch: [13][   20/   28]    Loss 0.387380    Top1 85.957031    
2023-01-06 16:10:52,891 - Epoch: [13][   28/   28]    Loss 0.391363    Top1 85.800172    
2023-01-06 16:10:53,043 - ==> Top1: 85.800    Loss: 0.391

2023-01-06 16:10:53,043 - ==> Confusion:
[[  86    1  352]
 [   4   26  572]
 [  49   14 5882]]

2023-01-06 16:10:53,045 - ==> Best [Top1: 85.800   Sparsity:0.00   Params: 46192 on epoch: 13]
2023-01-06 16:10:53,045 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:10:53,050 - 

2023-01-06 16:10:53,050 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:53,585 - Epoch: [14][   10/  246]    Overall Loss 0.384521    Objective Loss 0.384521                                        LR 0.000060    Time 0.053451    
2023-01-06 16:10:53,746 - Epoch: [14][   20/  246]    Overall Loss 0.398062    Objective Loss 0.398062                                        LR 0.000060    Time 0.034727    
2023-01-06 16:10:53,909 - Epoch: [14][   30/  246]    Overall Loss 0.402535    Objective Loss 0.402535                                        LR 0.000060    Time 0.028579    
2023-01-06 16:10:54,066 - Epoch: [14][   40/  246]    Overall Loss 0.402730    Objective Loss 0.402730                                        LR 0.000060    Time 0.025312    
2023-01-06 16:10:54,219 - Epoch: [14][   50/  246]    Overall Loss 0.401052    Objective Loss 0.401052                                        LR 0.000060    Time 0.023294    
2023-01-06 16:10:54,384 - Epoch: [14][   60/  246]    Overall Loss 0.398857    Objective Loss 0.398857                                        LR 0.000060    Time 0.022154    
2023-01-06 16:10:54,526 - Epoch: [14][   70/  246]    Overall Loss 0.398932    Objective Loss 0.398932                                        LR 0.000060    Time 0.021013    
2023-01-06 16:10:54,663 - Epoch: [14][   80/  246]    Overall Loss 0.398604    Objective Loss 0.398604                                        LR 0.000060    Time 0.020097    
2023-01-06 16:10:54,805 - Epoch: [14][   90/  246]    Overall Loss 0.394655    Objective Loss 0.394655                                        LR 0.000060    Time 0.019439    
2023-01-06 16:10:54,948 - Epoch: [14][  100/  246]    Overall Loss 0.396531    Objective Loss 0.396531                                        LR 0.000060    Time 0.018923    
2023-01-06 16:10:55,113 - Epoch: [14][  110/  246]    Overall Loss 0.395985    Objective Loss 0.395985                                        LR 0.000060    Time 0.018691    
2023-01-06 16:10:55,267 - Epoch: [14][  120/  246]    Overall Loss 0.394173    Objective Loss 0.394173                                        LR 0.000060    Time 0.018415    
2023-01-06 16:10:55,417 - Epoch: [14][  130/  246]    Overall Loss 0.392423    Objective Loss 0.392423                                        LR 0.000060    Time 0.018152    
2023-01-06 16:10:55,564 - Epoch: [14][  140/  246]    Overall Loss 0.392071    Objective Loss 0.392071                                        LR 0.000060    Time 0.017898    
2023-01-06 16:10:55,712 - Epoch: [14][  150/  246]    Overall Loss 0.392399    Objective Loss 0.392399                                        LR 0.000060    Time 0.017693    
2023-01-06 16:10:55,850 - Epoch: [14][  160/  246]    Overall Loss 0.390407    Objective Loss 0.390407                                        LR 0.000060    Time 0.017445    
2023-01-06 16:10:55,988 - Epoch: [14][  170/  246]    Overall Loss 0.390247    Objective Loss 0.390247                                        LR 0.000060    Time 0.017230    
2023-01-06 16:10:56,125 - Epoch: [14][  180/  246]    Overall Loss 0.391267    Objective Loss 0.391267                                        LR 0.000060    Time 0.017028    
2023-01-06 16:10:56,262 - Epoch: [14][  190/  246]    Overall Loss 0.392497    Objective Loss 0.392497                                        LR 0.000060    Time 0.016851    
2023-01-06 16:10:56,398 - Epoch: [14][  200/  246]    Overall Loss 0.392890    Objective Loss 0.392890                                        LR 0.000060    Time 0.016688    
2023-01-06 16:10:56,537 - Epoch: [14][  210/  246]    Overall Loss 0.392252    Objective Loss 0.392252                                        LR 0.000060    Time 0.016551    
2023-01-06 16:10:56,674 - Epoch: [14][  220/  246]    Overall Loss 0.392266    Objective Loss 0.392266                                        LR 0.000060    Time 0.016419    
2023-01-06 16:10:56,812 - Epoch: [14][  230/  246]    Overall Loss 0.392828    Objective Loss 0.392828                                        LR 0.000060    Time 0.016305    
2023-01-06 16:10:56,966 - Epoch: [14][  240/  246]    Overall Loss 0.393732    Objective Loss 0.393732                                        LR 0.000060    Time 0.016265    
2023-01-06 16:10:57,034 - Epoch: [14][  246/  246]    Overall Loss 0.393314    Objective Loss 0.393314    Top1 85.885167    LR 0.000060    Time 0.016147    
2023-01-06 16:10:57,167 - --- validate (epoch=14)-----------
2023-01-06 16:10:57,168 - 6986 samples (256 per mini-batch)
2023-01-06 16:10:57,620 - Epoch: [14][   10/   28]    Loss 0.376670    Top1 86.015625    
2023-01-06 16:10:57,729 - Epoch: [14][   20/   28]    Loss 0.391895    Top1 85.410156    
2023-01-06 16:10:57,788 - Epoch: [14][   28/   28]    Loss 0.387160    Top1 85.685657    
2023-01-06 16:10:57,924 - ==> Top1: 85.686    Loss: 0.387

2023-01-06 16:10:57,925 - ==> Confusion:
[[  60    1  378]
 [   2   17  583]
 [  22   14 5909]]

2023-01-06 16:10:57,926 - ==> Best [Top1: 85.800   Sparsity:0.00   Params: 46192 on epoch: 13]
2023-01-06 16:10:57,926 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:10:57,930 - 

2023-01-06 16:10:57,930 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:10:58,592 - Epoch: [15][   10/  246]    Overall Loss 0.385540    Objective Loss 0.385540                                        LR 0.000060    Time 0.066097    
2023-01-06 16:10:58,733 - Epoch: [15][   20/  246]    Overall Loss 0.387716    Objective Loss 0.387716                                        LR 0.000060    Time 0.040067    
2023-01-06 16:10:58,870 - Epoch: [15][   30/  246]    Overall Loss 0.392068    Objective Loss 0.392068                                        LR 0.000060    Time 0.031289    
2023-01-06 16:10:59,027 - Epoch: [15][   40/  246]    Overall Loss 0.393072    Objective Loss 0.393072                                        LR 0.000060    Time 0.027360    
2023-01-06 16:10:59,178 - Epoch: [15][   50/  246]    Overall Loss 0.395464    Objective Loss 0.395464                                        LR 0.000060    Time 0.024908    
2023-01-06 16:10:59,327 - Epoch: [15][   60/  246]    Overall Loss 0.394846    Objective Loss 0.394846                                        LR 0.000060    Time 0.023235    
2023-01-06 16:10:59,475 - Epoch: [15][   70/  246]    Overall Loss 0.394922    Objective Loss 0.394922                                        LR 0.000060    Time 0.022023    
2023-01-06 16:10:59,615 - Epoch: [15][   80/  246]    Overall Loss 0.392771    Objective Loss 0.392771                                        LR 0.000060    Time 0.021021    
2023-01-06 16:10:59,767 - Epoch: [15][   90/  246]    Overall Loss 0.393759    Objective Loss 0.393759                                        LR 0.000060    Time 0.020361    
2023-01-06 16:10:59,917 - Epoch: [15][  100/  246]    Overall Loss 0.394944    Objective Loss 0.394944                                        LR 0.000060    Time 0.019826    
2023-01-06 16:11:00,066 - Epoch: [15][  110/  246]    Overall Loss 0.394545    Objective Loss 0.394545                                        LR 0.000060    Time 0.019374    
2023-01-06 16:11:00,211 - Epoch: [15][  120/  246]    Overall Loss 0.394453    Objective Loss 0.394453                                        LR 0.000060    Time 0.018962    
2023-01-06 16:11:00,352 - Epoch: [15][  130/  246]    Overall Loss 0.392910    Objective Loss 0.392910                                        LR 0.000060    Time 0.018581    
2023-01-06 16:11:00,492 - Epoch: [15][  140/  246]    Overall Loss 0.393441    Objective Loss 0.393441                                        LR 0.000060    Time 0.018252    
2023-01-06 16:11:00,639 - Epoch: [15][  150/  246]    Overall Loss 0.393471    Objective Loss 0.393471                                        LR 0.000060    Time 0.018016    
2023-01-06 16:11:00,781 - Epoch: [15][  160/  246]    Overall Loss 0.391309    Objective Loss 0.391309                                        LR 0.000060    Time 0.017771    
2023-01-06 16:11:00,923 - Epoch: [15][  170/  246]    Overall Loss 0.391943    Objective Loss 0.391943                                        LR 0.000060    Time 0.017562    
2023-01-06 16:11:01,070 - Epoch: [15][  180/  246]    Overall Loss 0.391222    Objective Loss 0.391222                                        LR 0.000060    Time 0.017399    
2023-01-06 16:11:01,213 - Epoch: [15][  190/  246]    Overall Loss 0.391887    Objective Loss 0.391887                                        LR 0.000060    Time 0.017237    
2023-01-06 16:11:01,350 - Epoch: [15][  200/  246]    Overall Loss 0.392509    Objective Loss 0.392509                                        LR 0.000060    Time 0.017056    
2023-01-06 16:11:01,491 - Epoch: [15][  210/  246]    Overall Loss 0.391961    Objective Loss 0.391961                                        LR 0.000060    Time 0.016913    
2023-01-06 16:11:01,630 - Epoch: [15][  220/  246]    Overall Loss 0.392560    Objective Loss 0.392560                                        LR 0.000060    Time 0.016774    
2023-01-06 16:11:01,769 - Epoch: [15][  230/  246]    Overall Loss 0.392562    Objective Loss 0.392562                                        LR 0.000060    Time 0.016646    
2023-01-06 16:11:01,925 - Epoch: [15][  240/  246]    Overall Loss 0.392077    Objective Loss 0.392077                                        LR 0.000060    Time 0.016601    
2023-01-06 16:11:01,993 - Epoch: [15][  246/  246]    Overall Loss 0.392072    Objective Loss 0.392072    Top1 88.038278    LR 0.000060    Time 0.016472    
2023-01-06 16:11:02,133 - --- validate (epoch=15)-----------
2023-01-06 16:11:02,133 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:02,561 - Epoch: [15][   10/   28]    Loss 0.392416    Top1 85.468750    
2023-01-06 16:11:02,661 - Epoch: [15][   20/   28]    Loss 0.394427    Top1 85.117188    
2023-01-06 16:11:02,719 - Epoch: [15][   28/   28]    Loss 0.389419    Top1 85.699971    
2023-01-06 16:11:02,854 - ==> Top1: 85.700    Loss: 0.389

2023-01-06 16:11:02,854 - ==> Confusion:
[[  92    1  346]
 [   3   18  581]
 [  55   13 5877]]

2023-01-06 16:11:02,855 - ==> Best [Top1: 85.800   Sparsity:0.00   Params: 46192 on epoch: 13]
2023-01-06 16:11:02,856 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:02,860 - 

2023-01-06 16:11:02,860 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:03,536 - Epoch: [16][   10/  246]    Overall Loss 0.411234    Objective Loss 0.411234                                        LR 0.000060    Time 0.067535    
2023-01-06 16:11:03,686 - Epoch: [16][   20/  246]    Overall Loss 0.396173    Objective Loss 0.396173                                        LR 0.000060    Time 0.041218    
2023-01-06 16:11:03,831 - Epoch: [16][   30/  246]    Overall Loss 0.390907    Objective Loss 0.390907                                        LR 0.000060    Time 0.032300    
2023-01-06 16:11:03,981 - Epoch: [16][   40/  246]    Overall Loss 0.385873    Objective Loss 0.385873                                        LR 0.000060    Time 0.027964    
2023-01-06 16:11:04,126 - Epoch: [16][   50/  246]    Overall Loss 0.387214    Objective Loss 0.387214                                        LR 0.000060    Time 0.025276    
2023-01-06 16:11:04,274 - Epoch: [16][   60/  246]    Overall Loss 0.385430    Objective Loss 0.385430                                        LR 0.000060    Time 0.023513    
2023-01-06 16:11:04,422 - Epoch: [16][   70/  246]    Overall Loss 0.384185    Objective Loss 0.384185                                        LR 0.000060    Time 0.022264    
2023-01-06 16:11:04,574 - Epoch: [16][   80/  246]    Overall Loss 0.383279    Objective Loss 0.383279                                        LR 0.000060    Time 0.021372    
2023-01-06 16:11:04,723 - Epoch: [16][   90/  246]    Overall Loss 0.385319    Objective Loss 0.385319                                        LR 0.000060    Time 0.020651    
2023-01-06 16:11:04,890 - Epoch: [16][  100/  246]    Overall Loss 0.386305    Objective Loss 0.386305                                        LR 0.000060    Time 0.020251    
2023-01-06 16:11:05,065 - Epoch: [16][  110/  246]    Overall Loss 0.387114    Objective Loss 0.387114                                        LR 0.000060    Time 0.019980    
2023-01-06 16:11:05,269 - Epoch: [16][  120/  246]    Overall Loss 0.388731    Objective Loss 0.388731                                        LR 0.000060    Time 0.020016    
2023-01-06 16:11:05,494 - Epoch: [16][  130/  246]    Overall Loss 0.388101    Objective Loss 0.388101                                        LR 0.000060    Time 0.020199    
2023-01-06 16:11:05,716 - Epoch: [16][  140/  246]    Overall Loss 0.389021    Objective Loss 0.389021                                        LR 0.000060    Time 0.020344    
2023-01-06 16:11:05,942 - Epoch: [16][  150/  246]    Overall Loss 0.388628    Objective Loss 0.388628                                        LR 0.000060    Time 0.020487    
2023-01-06 16:11:06,136 - Epoch: [16][  160/  246]    Overall Loss 0.389257    Objective Loss 0.389257                                        LR 0.000060    Time 0.020419    
2023-01-06 16:11:06,313 - Epoch: [16][  170/  246]    Overall Loss 0.389582    Objective Loss 0.389582                                        LR 0.000060    Time 0.020254    
2023-01-06 16:11:06,487 - Epoch: [16][  180/  246]    Overall Loss 0.389875    Objective Loss 0.389875                                        LR 0.000060    Time 0.020093    
2023-01-06 16:11:06,656 - Epoch: [16][  190/  246]    Overall Loss 0.388009    Objective Loss 0.388009                                        LR 0.000060    Time 0.019924    
2023-01-06 16:11:06,830 - Epoch: [16][  200/  246]    Overall Loss 0.388198    Objective Loss 0.388198                                        LR 0.000060    Time 0.019796    
2023-01-06 16:11:07,002 - Epoch: [16][  210/  246]    Overall Loss 0.387503    Objective Loss 0.387503                                        LR 0.000060    Time 0.019673    
2023-01-06 16:11:07,155 - Epoch: [16][  220/  246]    Overall Loss 0.386987    Objective Loss 0.386987                                        LR 0.000060    Time 0.019472    
2023-01-06 16:11:07,293 - Epoch: [16][  230/  246]    Overall Loss 0.386897    Objective Loss 0.386897                                        LR 0.000060    Time 0.019222    
2023-01-06 16:11:07,448 - Epoch: [16][  240/  246]    Overall Loss 0.388575    Objective Loss 0.388575                                        LR 0.000060    Time 0.019065    
2023-01-06 16:11:07,517 - Epoch: [16][  246/  246]    Overall Loss 0.388422    Objective Loss 0.388422    Top1 84.449761    LR 0.000060    Time 0.018882    
2023-01-06 16:11:07,649 - --- validate (epoch=16)-----------
2023-01-06 16:11:07,650 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:08,084 - Epoch: [16][   10/   28]    Loss 0.378027    Top1 86.093750    
2023-01-06 16:11:08,179 - Epoch: [16][   20/   28]    Loss 0.382477    Top1 85.839844    
2023-01-06 16:11:08,237 - Epoch: [16][   28/   28]    Loss 0.384766    Top1 85.828800    
2023-01-06 16:11:08,379 - ==> Top1: 85.829    Loss: 0.385

2023-01-06 16:11:08,379 - ==> Confusion:
[[  53    1  385]
 [   2   28  572]
 [  18   12 5915]]

2023-01-06 16:11:08,380 - ==> Best [Top1: 85.829   Sparsity:0.00   Params: 46192 on epoch: 16]
2023-01-06 16:11:08,381 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:08,385 - 

2023-01-06 16:11:08,386 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:08,906 - Epoch: [17][   10/  246]    Overall Loss 0.393024    Objective Loss 0.393024                                        LR 0.000060    Time 0.052000    
2023-01-06 16:11:09,053 - Epoch: [17][   20/  246]    Overall Loss 0.387550    Objective Loss 0.387550                                        LR 0.000060    Time 0.033321    
2023-01-06 16:11:09,206 - Epoch: [17][   30/  246]    Overall Loss 0.390740    Objective Loss 0.390740                                        LR 0.000060    Time 0.027311    
2023-01-06 16:11:09,359 - Epoch: [17][   40/  246]    Overall Loss 0.389293    Objective Loss 0.389293                                        LR 0.000060    Time 0.024298    
2023-01-06 16:11:09,511 - Epoch: [17][   50/  246]    Overall Loss 0.388447    Objective Loss 0.388447                                        LR 0.000060    Time 0.022457    
2023-01-06 16:11:09,663 - Epoch: [17][   60/  246]    Overall Loss 0.389159    Objective Loss 0.389159                                        LR 0.000060    Time 0.021217    
2023-01-06 16:11:09,816 - Epoch: [17][   70/  246]    Overall Loss 0.386522    Objective Loss 0.386522                                        LR 0.000060    Time 0.020370    
2023-01-06 16:11:09,974 - Epoch: [17][   80/  246]    Overall Loss 0.387333    Objective Loss 0.387333                                        LR 0.000060    Time 0.019785    
2023-01-06 16:11:10,133 - Epoch: [17][   90/  246]    Overall Loss 0.385085    Objective Loss 0.385085                                        LR 0.000060    Time 0.019351    
2023-01-06 16:11:10,288 - Epoch: [17][  100/  246]    Overall Loss 0.386445    Objective Loss 0.386445                                        LR 0.000060    Time 0.018964    
2023-01-06 16:11:10,447 - Epoch: [17][  110/  246]    Overall Loss 0.385526    Objective Loss 0.385526                                        LR 0.000060    Time 0.018683    
2023-01-06 16:11:10,602 - Epoch: [17][  120/  246]    Overall Loss 0.386345    Objective Loss 0.386345                                        LR 0.000060    Time 0.018415    
2023-01-06 16:11:10,746 - Epoch: [17][  130/  246]    Overall Loss 0.387571    Objective Loss 0.387571                                        LR 0.000060    Time 0.018104    
2023-01-06 16:11:10,894 - Epoch: [17][  140/  246]    Overall Loss 0.387939    Objective Loss 0.387939                                        LR 0.000060    Time 0.017864    
2023-01-06 16:11:11,035 - Epoch: [17][  150/  246]    Overall Loss 0.388229    Objective Loss 0.388229                                        LR 0.000060    Time 0.017609    
2023-01-06 16:11:11,176 - Epoch: [17][  160/  246]    Overall Loss 0.388188    Objective Loss 0.388188                                        LR 0.000060    Time 0.017389    
2023-01-06 16:11:11,317 - Epoch: [17][  170/  246]    Overall Loss 0.388220    Objective Loss 0.388220                                        LR 0.000060    Time 0.017192    
2023-01-06 16:11:11,457 - Epoch: [17][  180/  246]    Overall Loss 0.387356    Objective Loss 0.387356                                        LR 0.000060    Time 0.017013    
2023-01-06 16:11:11,596 - Epoch: [17][  190/  246]    Overall Loss 0.385788    Objective Loss 0.385788                                        LR 0.000060    Time 0.016849    
2023-01-06 16:11:11,760 - Epoch: [17][  200/  246]    Overall Loss 0.386352    Objective Loss 0.386352                                        LR 0.000060    Time 0.016826    
2023-01-06 16:11:11,939 - Epoch: [17][  210/  246]    Overall Loss 0.386425    Objective Loss 0.386425                                        LR 0.000060    Time 0.016874    
2023-01-06 16:11:12,115 - Epoch: [17][  220/  246]    Overall Loss 0.386657    Objective Loss 0.386657                                        LR 0.000060    Time 0.016899    
2023-01-06 16:11:12,295 - Epoch: [17][  230/  246]    Overall Loss 0.386856    Objective Loss 0.386856                                        LR 0.000060    Time 0.016943    
2023-01-06 16:11:12,478 - Epoch: [17][  240/  246]    Overall Loss 0.386279    Objective Loss 0.386279                                        LR 0.000060    Time 0.016999    
2023-01-06 16:11:12,558 - Epoch: [17][  246/  246]    Overall Loss 0.386217    Objective Loss 0.386217    Top1 84.928230    LR 0.000060    Time 0.016907    
2023-01-06 16:11:12,705 - --- validate (epoch=17)-----------
2023-01-06 16:11:12,705 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:13,137 - Epoch: [17][   10/   28]    Loss 0.373068    Top1 86.523438    
2023-01-06 16:11:13,236 - Epoch: [17][   20/   28]    Loss 0.387747    Top1 86.035156    
2023-01-06 16:11:13,294 - Epoch: [17][   28/   28]    Loss 0.382725    Top1 86.258231    
2023-01-06 16:11:13,445 - ==> Top1: 86.258    Loss: 0.383

2023-01-06 16:11:13,446 - ==> Confusion:
[[ 105    2  332]
 [   4   39  559]
 [  48   15 5882]]

2023-01-06 16:11:13,447 - ==> Best [Top1: 86.258   Sparsity:0.00   Params: 46192 on epoch: 17]
2023-01-06 16:11:13,447 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:13,452 - 

2023-01-06 16:11:13,452 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:14,122 - Epoch: [18][   10/  246]    Overall Loss 0.377867    Objective Loss 0.377867                                        LR 0.000060    Time 0.067010    
2023-01-06 16:11:14,276 - Epoch: [18][   20/  246]    Overall Loss 0.375057    Objective Loss 0.375057                                        LR 0.000060    Time 0.041180    
2023-01-06 16:11:14,424 - Epoch: [18][   30/  246]    Overall Loss 0.376336    Objective Loss 0.376336                                        LR 0.000060    Time 0.032367    
2023-01-06 16:11:14,576 - Epoch: [18][   40/  246]    Overall Loss 0.377377    Objective Loss 0.377377                                        LR 0.000060    Time 0.028059    
2023-01-06 16:11:14,723 - Epoch: [18][   50/  246]    Overall Loss 0.378573    Objective Loss 0.378573                                        LR 0.000060    Time 0.025378    
2023-01-06 16:11:14,872 - Epoch: [18][   60/  246]    Overall Loss 0.379532    Objective Loss 0.379532                                        LR 0.000060    Time 0.023633    
2023-01-06 16:11:15,018 - Epoch: [18][   70/  246]    Overall Loss 0.381539    Objective Loss 0.381539                                        LR 0.000060    Time 0.022334    
2023-01-06 16:11:15,162 - Epoch: [18][   80/  246]    Overall Loss 0.383515    Objective Loss 0.383515                                        LR 0.000060    Time 0.021338    
2023-01-06 16:11:15,326 - Epoch: [18][   90/  246]    Overall Loss 0.384523    Objective Loss 0.384523                                        LR 0.000060    Time 0.020780    
2023-01-06 16:11:15,489 - Epoch: [18][  100/  246]    Overall Loss 0.384683    Objective Loss 0.384683                                        LR 0.000060    Time 0.020334    
2023-01-06 16:11:15,655 - Epoch: [18][  110/  246]    Overall Loss 0.387184    Objective Loss 0.387184                                        LR 0.000060    Time 0.019974    
2023-01-06 16:11:15,818 - Epoch: [18][  120/  246]    Overall Loss 0.386601    Objective Loss 0.386601                                        LR 0.000060    Time 0.019664    
2023-01-06 16:11:15,988 - Epoch: [18][  130/  246]    Overall Loss 0.386986    Objective Loss 0.386986                                        LR 0.000060    Time 0.019449    
2023-01-06 16:11:16,186 - Epoch: [18][  140/  246]    Overall Loss 0.385339    Objective Loss 0.385339                                        LR 0.000060    Time 0.019472    
2023-01-06 16:11:16,391 - Epoch: [18][  150/  246]    Overall Loss 0.385633    Objective Loss 0.385633                                        LR 0.000060    Time 0.019536    
2023-01-06 16:11:16,611 - Epoch: [18][  160/  246]    Overall Loss 0.386361    Objective Loss 0.386361                                        LR 0.000060    Time 0.019685    
2023-01-06 16:11:16,827 - Epoch: [18][  170/  246]    Overall Loss 0.386025    Objective Loss 0.386025                                        LR 0.000060    Time 0.019800    
2023-01-06 16:11:17,039 - Epoch: [18][  180/  246]    Overall Loss 0.385273    Objective Loss 0.385273                                        LR 0.000060    Time 0.019875    
2023-01-06 16:11:17,212 - Epoch: [18][  190/  246]    Overall Loss 0.385337    Objective Loss 0.385337                                        LR 0.000060    Time 0.019734    
2023-01-06 16:11:17,380 - Epoch: [18][  200/  246]    Overall Loss 0.385440    Objective Loss 0.385440                                        LR 0.000060    Time 0.019588    
2023-01-06 16:11:17,529 - Epoch: [18][  210/  246]    Overall Loss 0.385673    Objective Loss 0.385673                                        LR 0.000060    Time 0.019362    
2023-01-06 16:11:17,671 - Epoch: [18][  220/  246]    Overall Loss 0.386332    Objective Loss 0.386332                                        LR 0.000060    Time 0.019126    
2023-01-06 16:11:17,813 - Epoch: [18][  230/  246]    Overall Loss 0.386733    Objective Loss 0.386733                                        LR 0.000060    Time 0.018909    
2023-01-06 16:11:17,968 - Epoch: [18][  240/  246]    Overall Loss 0.386078    Objective Loss 0.386078                                        LR 0.000060    Time 0.018766    
2023-01-06 16:11:18,037 - Epoch: [18][  246/  246]    Overall Loss 0.386002    Objective Loss 0.386002    Top1 89.473684    LR 0.000060    Time 0.018590    
2023-01-06 16:11:18,168 - --- validate (epoch=18)-----------
2023-01-06 16:11:18,169 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:18,611 - Epoch: [18][   10/   28]    Loss 0.383384    Top1 86.210938    
2023-01-06 16:11:18,720 - Epoch: [18][   20/   28]    Loss 0.378028    Top1 86.328125    
2023-01-06 16:11:18,775 - Epoch: [18][   28/   28]    Loss 0.379996    Top1 86.286859    
2023-01-06 16:11:18,936 - ==> Top1: 86.287    Loss: 0.380

2023-01-06 16:11:18,937 - ==> Confusion:
[[ 110    1  328]
 [   4   51  547]
 [  57   21 5867]]

2023-01-06 16:11:18,938 - ==> Best [Top1: 86.287   Sparsity:0.00   Params: 46192 on epoch: 18]
2023-01-06 16:11:18,938 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:18,946 - 

2023-01-06 16:11:18,946 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:19,491 - Epoch: [19][   10/  246]    Overall Loss 0.384896    Objective Loss 0.384896                                        LR 0.000060    Time 0.054448    
2023-01-06 16:11:19,666 - Epoch: [19][   20/  246]    Overall Loss 0.393089    Objective Loss 0.393089                                        LR 0.000060    Time 0.035934    
2023-01-06 16:11:19,833 - Epoch: [19][   30/  246]    Overall Loss 0.380461    Objective Loss 0.380461                                        LR 0.000060    Time 0.029503    
2023-01-06 16:11:20,010 - Epoch: [19][   40/  246]    Overall Loss 0.385164    Objective Loss 0.385164                                        LR 0.000060    Time 0.026554    
2023-01-06 16:11:20,173 - Epoch: [19][   50/  246]    Overall Loss 0.384916    Objective Loss 0.384916                                        LR 0.000060    Time 0.024460    
2023-01-06 16:11:20,350 - Epoch: [19][   60/  246]    Overall Loss 0.387987    Objective Loss 0.387987                                        LR 0.000060    Time 0.023334    
2023-01-06 16:11:20,524 - Epoch: [19][   70/  246]    Overall Loss 0.384149    Objective Loss 0.384149                                        LR 0.000060    Time 0.022457    
2023-01-06 16:11:20,706 - Epoch: [19][   80/  246]    Overall Loss 0.385586    Objective Loss 0.385586                                        LR 0.000060    Time 0.021920    
2023-01-06 16:11:20,879 - Epoch: [19][   90/  246]    Overall Loss 0.384662    Objective Loss 0.384662                                        LR 0.000060    Time 0.021394    
2023-01-06 16:11:21,060 - Epoch: [19][  100/  246]    Overall Loss 0.385987    Objective Loss 0.385987                                        LR 0.000060    Time 0.021064    
2023-01-06 16:11:21,233 - Epoch: [19][  110/  246]    Overall Loss 0.386768    Objective Loss 0.386768                                        LR 0.000060    Time 0.020715    
2023-01-06 16:11:21,408 - Epoch: [19][  120/  246]    Overall Loss 0.385800    Objective Loss 0.385800                                        LR 0.000060    Time 0.020447    
2023-01-06 16:11:21,583 - Epoch: [19][  130/  246]    Overall Loss 0.387098    Objective Loss 0.387098                                        LR 0.000060    Time 0.020199    
2023-01-06 16:11:21,764 - Epoch: [19][  140/  246]    Overall Loss 0.386980    Objective Loss 0.386980                                        LR 0.000060    Time 0.020046    
2023-01-06 16:11:21,936 - Epoch: [19][  150/  246]    Overall Loss 0.387198    Objective Loss 0.387198                                        LR 0.000060    Time 0.019854    
2023-01-06 16:11:22,104 - Epoch: [19][  160/  246]    Overall Loss 0.386360    Objective Loss 0.386360                                        LR 0.000060    Time 0.019662    
2023-01-06 16:11:22,253 - Epoch: [19][  170/  246]    Overall Loss 0.385190    Objective Loss 0.385190                                        LR 0.000060    Time 0.019383    
2023-01-06 16:11:22,402 - Epoch: [19][  180/  246]    Overall Loss 0.383810    Objective Loss 0.383810                                        LR 0.000060    Time 0.019130    
2023-01-06 16:11:22,549 - Epoch: [19][  190/  246]    Overall Loss 0.384340    Objective Loss 0.384340                                        LR 0.000060    Time 0.018895    
2023-01-06 16:11:22,695 - Epoch: [19][  200/  246]    Overall Loss 0.384210    Objective Loss 0.384210                                        LR 0.000060    Time 0.018679    
2023-01-06 16:11:22,842 - Epoch: [19][  210/  246]    Overall Loss 0.384842    Objective Loss 0.384842                                        LR 0.000060    Time 0.018486    
2023-01-06 16:11:22,989 - Epoch: [19][  220/  246]    Overall Loss 0.383259    Objective Loss 0.383259                                        LR 0.000060    Time 0.018313    
2023-01-06 16:11:23,137 - Epoch: [19][  230/  246]    Overall Loss 0.383253    Objective Loss 0.383253                                        LR 0.000060    Time 0.018156    
2023-01-06 16:11:23,294 - Epoch: [19][  240/  246]    Overall Loss 0.384305    Objective Loss 0.384305                                        LR 0.000060    Time 0.018055    
2023-01-06 16:11:23,365 - Epoch: [19][  246/  246]    Overall Loss 0.384181    Objective Loss 0.384181    Top1 88.995215    LR 0.000060    Time 0.017902    
2023-01-06 16:11:23,496 - --- validate (epoch=19)-----------
2023-01-06 16:11:23,496 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:23,922 - Epoch: [19][   10/   28]    Loss 0.373513    Top1 86.523438    
2023-01-06 16:11:24,023 - Epoch: [19][   20/   28]    Loss 0.381588    Top1 86.054688    
2023-01-06 16:11:24,080 - Epoch: [19][   28/   28]    Loss 0.381004    Top1 85.986258    
2023-01-06 16:11:24,214 - ==> Top1: 85.986    Loss: 0.381

2023-01-06 16:11:24,215 - ==> Confusion:
[[  65    1  373]
 [   3   26  573]
 [  19   10 5916]]

2023-01-06 16:11:24,216 - ==> Best [Top1: 86.287   Sparsity:0.00   Params: 46192 on epoch: 18]
2023-01-06 16:11:24,216 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:24,220 - 

2023-01-06 16:11:24,221 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:24,892 - Epoch: [20][   10/  246]    Overall Loss 0.394881    Objective Loss 0.394881                                        LR 0.000060    Time 0.067102    
2023-01-06 16:11:25,044 - Epoch: [20][   20/  246]    Overall Loss 0.386083    Objective Loss 0.386083                                        LR 0.000060    Time 0.041122    
2023-01-06 16:11:25,201 - Epoch: [20][   30/  246]    Overall Loss 0.392843    Objective Loss 0.392843                                        LR 0.000060    Time 0.032624    
2023-01-06 16:11:25,359 - Epoch: [20][   40/  246]    Overall Loss 0.391599    Objective Loss 0.391599                                        LR 0.000060    Time 0.028404    
2023-01-06 16:11:25,524 - Epoch: [20][   50/  246]    Overall Loss 0.393845    Objective Loss 0.393845                                        LR 0.000060    Time 0.026013    
2023-01-06 16:11:25,693 - Epoch: [20][   60/  246]    Overall Loss 0.391290    Objective Loss 0.391290                                        LR 0.000060    Time 0.024503    
2023-01-06 16:11:25,859 - Epoch: [20][   70/  246]    Overall Loss 0.390482    Objective Loss 0.390482                                        LR 0.000060    Time 0.023352    
2023-01-06 16:11:26,029 - Epoch: [20][   80/  246]    Overall Loss 0.389437    Objective Loss 0.389437                                        LR 0.000060    Time 0.022545    
2023-01-06 16:11:26,196 - Epoch: [20][   90/  246]    Overall Loss 0.387859    Objective Loss 0.387859                                        LR 0.000060    Time 0.021894    
2023-01-06 16:11:26,369 - Epoch: [20][  100/  246]    Overall Loss 0.387587    Objective Loss 0.387587                                        LR 0.000060    Time 0.021425    
2023-01-06 16:11:26,534 - Epoch: [20][  110/  246]    Overall Loss 0.387799    Objective Loss 0.387799                                        LR 0.000060    Time 0.020982    
2023-01-06 16:11:26,707 - Epoch: [20][  120/  246]    Overall Loss 0.386945    Objective Loss 0.386945                                        LR 0.000060    Time 0.020657    
2023-01-06 16:11:26,878 - Epoch: [20][  130/  246]    Overall Loss 0.386625    Objective Loss 0.386625                                        LR 0.000060    Time 0.020364    
2023-01-06 16:11:27,051 - Epoch: [20][  140/  246]    Overall Loss 0.386370    Objective Loss 0.386370                                        LR 0.000060    Time 0.020143    
2023-01-06 16:11:27,217 - Epoch: [20][  150/  246]    Overall Loss 0.385011    Objective Loss 0.385011                                        LR 0.000060    Time 0.019893    
2023-01-06 16:11:27,393 - Epoch: [20][  160/  246]    Overall Loss 0.385615    Objective Loss 0.385615                                        LR 0.000060    Time 0.019746    
2023-01-06 16:11:27,563 - Epoch: [20][  170/  246]    Overall Loss 0.384760    Objective Loss 0.384760                                        LR 0.000060    Time 0.019574    
2023-01-06 16:11:27,733 - Epoch: [20][  180/  246]    Overall Loss 0.384192    Objective Loss 0.384192                                        LR 0.000060    Time 0.019428    
2023-01-06 16:11:27,899 - Epoch: [20][  190/  246]    Overall Loss 0.383499    Objective Loss 0.383499                                        LR 0.000060    Time 0.019274    
2023-01-06 16:11:28,070 - Epoch: [20][  200/  246]    Overall Loss 0.383130    Objective Loss 0.383130                                        LR 0.000060    Time 0.019164    
2023-01-06 16:11:28,238 - Epoch: [20][  210/  246]    Overall Loss 0.383384    Objective Loss 0.383384                                        LR 0.000060    Time 0.019046    
2023-01-06 16:11:28,396 - Epoch: [20][  220/  246]    Overall Loss 0.383711    Objective Loss 0.383711                                        LR 0.000060    Time 0.018897    
2023-01-06 16:11:28,561 - Epoch: [20][  230/  246]    Overall Loss 0.383540    Objective Loss 0.383540                                        LR 0.000060    Time 0.018787    
2023-01-06 16:11:28,733 - Epoch: [20][  240/  246]    Overall Loss 0.382686    Objective Loss 0.382686                                        LR 0.000060    Time 0.018722    
2023-01-06 16:11:28,815 - Epoch: [20][  246/  246]    Overall Loss 0.382534    Objective Loss 0.382534    Top1 87.799043    LR 0.000060    Time 0.018596    
2023-01-06 16:11:28,973 - --- validate (epoch=20)-----------
2023-01-06 16:11:28,974 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:29,404 - Epoch: [20][   10/   28]    Loss 0.385117    Top1 86.171875    
2023-01-06 16:11:29,512 - Epoch: [20][   20/   28]    Loss 0.382174    Top1 86.308594    
2023-01-06 16:11:29,570 - Epoch: [20][   28/   28]    Loss 0.382790    Top1 86.315488    
2023-01-06 16:11:29,698 - ==> Top1: 86.315    Loss: 0.383

2023-01-06 16:11:29,699 - ==> Confusion:
[[ 132    5  302]
 [   6   90  506]
 [  83   54 5808]]

2023-01-06 16:11:29,700 - ==> Best [Top1: 86.315   Sparsity:0.00   Params: 46192 on epoch: 20]
2023-01-06 16:11:29,700 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:29,705 - 

2023-01-06 16:11:29,705 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:30,380 - Epoch: [21][   10/  246]    Overall Loss 0.397676    Objective Loss 0.397676                                        LR 0.000060    Time 0.067505    
2023-01-06 16:11:30,534 - Epoch: [21][   20/  246]    Overall Loss 0.383061    Objective Loss 0.383061                                        LR 0.000060    Time 0.041390    
2023-01-06 16:11:30,693 - Epoch: [21][   30/  246]    Overall Loss 0.386338    Objective Loss 0.386338                                        LR 0.000060    Time 0.032913    
2023-01-06 16:11:30,857 - Epoch: [21][   40/  246]    Overall Loss 0.385563    Objective Loss 0.385563                                        LR 0.000060    Time 0.028759    
2023-01-06 16:11:31,023 - Epoch: [21][   50/  246]    Overall Loss 0.383874    Objective Loss 0.383874                                        LR 0.000060    Time 0.026317    
2023-01-06 16:11:31,188 - Epoch: [21][   60/  246]    Overall Loss 0.381639    Objective Loss 0.381639                                        LR 0.000060    Time 0.024673    
2023-01-06 16:11:31,340 - Epoch: [21][   70/  246]    Overall Loss 0.381394    Objective Loss 0.381394                                        LR 0.000060    Time 0.023319    
2023-01-06 16:11:31,484 - Epoch: [21][   80/  246]    Overall Loss 0.378549    Objective Loss 0.378549                                        LR 0.000060    Time 0.022200    
2023-01-06 16:11:31,635 - Epoch: [21][   90/  246]    Overall Loss 0.378423    Objective Loss 0.378423                                        LR 0.000060    Time 0.021405    
2023-01-06 16:11:31,778 - Epoch: [21][  100/  246]    Overall Loss 0.376429    Objective Loss 0.376429                                        LR 0.000060    Time 0.020691    
2023-01-06 16:11:31,930 - Epoch: [21][  110/  246]    Overall Loss 0.374764    Objective Loss 0.374764                                        LR 0.000060    Time 0.020181    
2023-01-06 16:11:32,079 - Epoch: [21][  120/  246]    Overall Loss 0.376172    Objective Loss 0.376172                                        LR 0.000060    Time 0.019738    
2023-01-06 16:11:32,226 - Epoch: [21][  130/  246]    Overall Loss 0.377814    Objective Loss 0.377814                                        LR 0.000060    Time 0.019346    
2023-01-06 16:11:32,377 - Epoch: [21][  140/  246]    Overall Loss 0.376182    Objective Loss 0.376182                                        LR 0.000060    Time 0.019042    
2023-01-06 16:11:32,527 - Epoch: [21][  150/  246]    Overall Loss 0.378289    Objective Loss 0.378289                                        LR 0.000060    Time 0.018771    
2023-01-06 16:11:32,671 - Epoch: [21][  160/  246]    Overall Loss 0.379189    Objective Loss 0.379189                                        LR 0.000060    Time 0.018495    
2023-01-06 16:11:32,823 - Epoch: [21][  170/  246]    Overall Loss 0.380529    Objective Loss 0.380529                                        LR 0.000060    Time 0.018296    
2023-01-06 16:11:32,974 - Epoch: [21][  180/  246]    Overall Loss 0.380255    Objective Loss 0.380255                                        LR 0.000060    Time 0.018116    
2023-01-06 16:11:33,120 - Epoch: [21][  190/  246]    Overall Loss 0.380511    Objective Loss 0.380511                                        LR 0.000060    Time 0.017931    
2023-01-06 16:11:33,275 - Epoch: [21][  200/  246]    Overall Loss 0.381593    Objective Loss 0.381593                                        LR 0.000060    Time 0.017810    
2023-01-06 16:11:33,429 - Epoch: [21][  210/  246]    Overall Loss 0.382039    Objective Loss 0.382039                                        LR 0.000060    Time 0.017693    
2023-01-06 16:11:33,591 - Epoch: [21][  220/  246]    Overall Loss 0.381939    Objective Loss 0.381939                                        LR 0.000060    Time 0.017622    
2023-01-06 16:11:33,737 - Epoch: [21][  230/  246]    Overall Loss 0.381406    Objective Loss 0.381406                                        LR 0.000060    Time 0.017492    
2023-01-06 16:11:33,895 - Epoch: [21][  240/  246]    Overall Loss 0.380711    Objective Loss 0.380711                                        LR 0.000060    Time 0.017420    
2023-01-06 16:11:33,967 - Epoch: [21][  246/  246]    Overall Loss 0.380558    Objective Loss 0.380558    Top1 85.885167    LR 0.000060    Time 0.017285    
2023-01-06 16:11:34,089 - --- validate (epoch=21)-----------
2023-01-06 16:11:34,089 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:34,515 - Epoch: [21][   10/   28]    Loss 0.382913    Top1 86.015625    
2023-01-06 16:11:34,615 - Epoch: [21][   20/   28]    Loss 0.374404    Top1 86.250000    
2023-01-06 16:11:34,674 - Epoch: [21][   28/   28]    Loss 0.372837    Top1 86.372745    
2023-01-06 16:11:34,813 - ==> Top1: 86.373    Loss: 0.373

2023-01-06 16:11:34,813 - ==> Confusion:
[[ 108    2  329]
 [   3   56  543]
 [  50   25 5870]]

2023-01-06 16:11:34,814 - ==> Best [Top1: 86.373   Sparsity:0.00   Params: 46192 on epoch: 21]
2023-01-06 16:11:34,814 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:34,819 - 

2023-01-06 16:11:34,819 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:35,346 - Epoch: [22][   10/  246]    Overall Loss 0.393130    Objective Loss 0.393130                                        LR 0.000060    Time 0.052618    
2023-01-06 16:11:35,503 - Epoch: [22][   20/  246]    Overall Loss 0.377604    Objective Loss 0.377604                                        LR 0.000060    Time 0.034104    
2023-01-06 16:11:35,658 - Epoch: [22][   30/  246]    Overall Loss 0.382350    Objective Loss 0.382350                                        LR 0.000060    Time 0.027891    
2023-01-06 16:11:35,816 - Epoch: [22][   40/  246]    Overall Loss 0.380105    Objective Loss 0.380105                                        LR 0.000060    Time 0.024859    
2023-01-06 16:11:35,965 - Epoch: [22][   50/  246]    Overall Loss 0.379105    Objective Loss 0.379105                                        LR 0.000060    Time 0.022867    
2023-01-06 16:11:36,123 - Epoch: [22][   60/  246]    Overall Loss 0.378165    Objective Loss 0.378165                                        LR 0.000060    Time 0.021678    
2023-01-06 16:11:36,283 - Epoch: [22][   70/  246]    Overall Loss 0.379953    Objective Loss 0.379953                                        LR 0.000060    Time 0.020865    
2023-01-06 16:11:36,446 - Epoch: [22][   80/  246]    Overall Loss 0.376585    Objective Loss 0.376585                                        LR 0.000060    Time 0.020295    
2023-01-06 16:11:36,610 - Epoch: [22][   90/  246]    Overall Loss 0.378223    Objective Loss 0.378223                                        LR 0.000060    Time 0.019853    
2023-01-06 16:11:36,766 - Epoch: [22][  100/  246]    Overall Loss 0.379625    Objective Loss 0.379625                                        LR 0.000060    Time 0.019422    
2023-01-06 16:11:36,927 - Epoch: [22][  110/  246]    Overall Loss 0.379558    Objective Loss 0.379558                                        LR 0.000060    Time 0.019117    
2023-01-06 16:11:37,083 - Epoch: [22][  120/  246]    Overall Loss 0.377534    Objective Loss 0.377534                                        LR 0.000060    Time 0.018825    
2023-01-06 16:11:37,241 - Epoch: [22][  130/  246]    Overall Loss 0.377133    Objective Loss 0.377133                                        LR 0.000060    Time 0.018586    
2023-01-06 16:11:37,397 - Epoch: [22][  140/  246]    Overall Loss 0.378271    Objective Loss 0.378271                                        LR 0.000060    Time 0.018369    
2023-01-06 16:11:37,557 - Epoch: [22][  150/  246]    Overall Loss 0.378932    Objective Loss 0.378932                                        LR 0.000060    Time 0.018209    
2023-01-06 16:11:37,712 - Epoch: [22][  160/  246]    Overall Loss 0.377809    Objective Loss 0.377809                                        LR 0.000060    Time 0.018039    
2023-01-06 16:11:37,869 - Epoch: [22][  170/  246]    Overall Loss 0.377710    Objective Loss 0.377710                                        LR 0.000060    Time 0.017898    
2023-01-06 16:11:38,018 - Epoch: [22][  180/  246]    Overall Loss 0.377287    Objective Loss 0.377287                                        LR 0.000060    Time 0.017732    
2023-01-06 16:11:38,160 - Epoch: [22][  190/  246]    Overall Loss 0.377178    Objective Loss 0.377178                                        LR 0.000060    Time 0.017544    
2023-01-06 16:11:38,313 - Epoch: [22][  200/  246]    Overall Loss 0.377459    Objective Loss 0.377459                                        LR 0.000060    Time 0.017429    
2023-01-06 16:11:38,453 - Epoch: [22][  210/  246]    Overall Loss 0.378864    Objective Loss 0.378864                                        LR 0.000060    Time 0.017264    
2023-01-06 16:11:38,600 - Epoch: [22][  220/  246]    Overall Loss 0.378734    Objective Loss 0.378734                                        LR 0.000060    Time 0.017142    
2023-01-06 16:11:38,746 - Epoch: [22][  230/  246]    Overall Loss 0.377587    Objective Loss 0.377587                                        LR 0.000060    Time 0.017030    
2023-01-06 16:11:38,907 - Epoch: [22][  240/  246]    Overall Loss 0.377979    Objective Loss 0.377979                                        LR 0.000060    Time 0.016992    
2023-01-06 16:11:38,984 - Epoch: [22][  246/  246]    Overall Loss 0.377520    Objective Loss 0.377520    Top1 85.885167    LR 0.000060    Time 0.016887    
2023-01-06 16:11:39,139 - --- validate (epoch=22)-----------
2023-01-06 16:11:39,139 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:39,574 - Epoch: [22][   10/   28]    Loss 0.369182    Top1 86.601562    
2023-01-06 16:11:39,677 - Epoch: [22][   20/   28]    Loss 0.371740    Top1 86.445312    
2023-01-06 16:11:39,734 - Epoch: [22][   28/   28]    Loss 0.371769    Top1 86.086459    
2023-01-06 16:11:39,859 - ==> Top1: 86.086    Loss: 0.372

2023-01-06 16:11:39,859 - ==> Confusion:
[[ 111    1  327]
 [   3   32  567]
 [  61   13 5871]]

2023-01-06 16:11:39,860 - ==> Best [Top1: 86.373   Sparsity:0.00   Params: 46192 on epoch: 21]
2023-01-06 16:11:39,861 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:39,865 - 

2023-01-06 16:11:39,865 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:40,559 - Epoch: [23][   10/  246]    Overall Loss 0.361552    Objective Loss 0.361552                                        LR 0.000060    Time 0.069309    
2023-01-06 16:11:40,712 - Epoch: [23][   20/  246]    Overall Loss 0.373483    Objective Loss 0.373483                                        LR 0.000060    Time 0.042273    
2023-01-06 16:11:40,868 - Epoch: [23][   30/  246]    Overall Loss 0.381046    Objective Loss 0.381046                                        LR 0.000060    Time 0.033378    
2023-01-06 16:11:41,031 - Epoch: [23][   40/  246]    Overall Loss 0.375762    Objective Loss 0.375762                                        LR 0.000060    Time 0.029114    
2023-01-06 16:11:41,193 - Epoch: [23][   50/  246]    Overall Loss 0.373233    Objective Loss 0.373233                                        LR 0.000060    Time 0.026515    
2023-01-06 16:11:41,344 - Epoch: [23][   60/  246]    Overall Loss 0.373895    Objective Loss 0.373895                                        LR 0.000060    Time 0.024615    
2023-01-06 16:11:41,502 - Epoch: [23][   70/  246]    Overall Loss 0.374368    Objective Loss 0.374368                                        LR 0.000060    Time 0.023348    
2023-01-06 16:11:41,665 - Epoch: [23][   80/  246]    Overall Loss 0.376139    Objective Loss 0.376139                                        LR 0.000060    Time 0.022455    
2023-01-06 16:11:41,829 - Epoch: [23][   90/  246]    Overall Loss 0.376406    Objective Loss 0.376406                                        LR 0.000060    Time 0.021783    
2023-01-06 16:11:41,978 - Epoch: [23][  100/  246]    Overall Loss 0.379005    Objective Loss 0.379005                                        LR 0.000060    Time 0.021093    
2023-01-06 16:11:42,130 - Epoch: [23][  110/  246]    Overall Loss 0.376436    Objective Loss 0.376436                                        LR 0.000060    Time 0.020549    
2023-01-06 16:11:42,305 - Epoch: [23][  120/  246]    Overall Loss 0.376469    Objective Loss 0.376469                                        LR 0.000060    Time 0.020294    
2023-01-06 16:11:42,502 - Epoch: [23][  130/  246]    Overall Loss 0.378317    Objective Loss 0.378317                                        LR 0.000060    Time 0.020248    
2023-01-06 16:11:42,718 - Epoch: [23][  140/  246]    Overall Loss 0.376272    Objective Loss 0.376272                                        LR 0.000060    Time 0.020336    
2023-01-06 16:11:42,914 - Epoch: [23][  150/  246]    Overall Loss 0.375028    Objective Loss 0.375028                                        LR 0.000060    Time 0.020289    
2023-01-06 16:11:43,089 - Epoch: [23][  160/  246]    Overall Loss 0.374636    Objective Loss 0.374636                                        LR 0.000060    Time 0.020110    
2023-01-06 16:11:43,263 - Epoch: [23][  170/  246]    Overall Loss 0.375294    Objective Loss 0.375294                                        LR 0.000060    Time 0.019948    
2023-01-06 16:11:43,433 - Epoch: [23][  180/  246]    Overall Loss 0.375148    Objective Loss 0.375148                                        LR 0.000060    Time 0.019784    
2023-01-06 16:11:43,608 - Epoch: [23][  190/  246]    Overall Loss 0.376007    Objective Loss 0.376007                                        LR 0.000060    Time 0.019661    
2023-01-06 16:11:43,778 - Epoch: [23][  200/  246]    Overall Loss 0.376205    Objective Loss 0.376205                                        LR 0.000060    Time 0.019529    
2023-01-06 16:11:43,953 - Epoch: [23][  210/  246]    Overall Loss 0.376820    Objective Loss 0.376820                                        LR 0.000060    Time 0.019428    
2023-01-06 16:11:44,116 - Epoch: [23][  220/  246]    Overall Loss 0.375920    Objective Loss 0.375920                                        LR 0.000060    Time 0.019283    
2023-01-06 16:11:44,285 - Epoch: [23][  230/  246]    Overall Loss 0.375892    Objective Loss 0.375892                                        LR 0.000060    Time 0.019181    
2023-01-06 16:11:44,467 - Epoch: [23][  240/  246]    Overall Loss 0.375619    Objective Loss 0.375619                                        LR 0.000060    Time 0.019136    
2023-01-06 16:11:44,549 - Epoch: [23][  246/  246]    Overall Loss 0.375631    Objective Loss 0.375631    Top1 86.842105    LR 0.000060    Time 0.019002    
2023-01-06 16:11:44,699 - --- validate (epoch=23)-----------
2023-01-06 16:11:44,699 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:45,135 - Epoch: [23][   10/   28]    Loss 0.371147    Top1 87.070312    
2023-01-06 16:11:45,240 - Epoch: [23][   20/   28]    Loss 0.369603    Top1 86.796875    
2023-01-06 16:11:45,300 - Epoch: [23][   28/   28]    Loss 0.373919    Top1 86.687661    
2023-01-06 16:11:45,460 - ==> Top1: 86.688    Loss: 0.374

2023-01-06 16:11:45,461 - ==> Confusion:
[[ 146    3  290]
 [   8   84  510]
 [  83   36 5826]]

2023-01-06 16:11:45,462 - ==> Best [Top1: 86.688   Sparsity:0.00   Params: 46192 on epoch: 23]
2023-01-06 16:11:45,462 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:45,467 - 

2023-01-06 16:11:45,467 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:45,998 - Epoch: [24][   10/  246]    Overall Loss 0.370299    Objective Loss 0.370299                                        LR 0.000060    Time 0.053049    
2023-01-06 16:11:46,155 - Epoch: [24][   20/  246]    Overall Loss 0.371472    Objective Loss 0.371472                                        LR 0.000060    Time 0.034352    
2023-01-06 16:11:46,305 - Epoch: [24][   30/  246]    Overall Loss 0.373863    Objective Loss 0.373863                                        LR 0.000060    Time 0.027895    
2023-01-06 16:11:46,460 - Epoch: [24][   40/  246]    Overall Loss 0.375194    Objective Loss 0.375194                                        LR 0.000060    Time 0.024790    
2023-01-06 16:11:46,621 - Epoch: [24][   50/  246]    Overall Loss 0.375898    Objective Loss 0.375898                                        LR 0.000060    Time 0.023037    
2023-01-06 16:11:46,789 - Epoch: [24][   60/  246]    Overall Loss 0.374940    Objective Loss 0.374940                                        LR 0.000060    Time 0.021995    
2023-01-06 16:11:46,976 - Epoch: [24][   70/  246]    Overall Loss 0.375528    Objective Loss 0.375528                                        LR 0.000060    Time 0.021519    
2023-01-06 16:11:47,140 - Epoch: [24][   80/  246]    Overall Loss 0.377387    Objective Loss 0.377387                                        LR 0.000060    Time 0.020875    
2023-01-06 16:11:47,310 - Epoch: [24][   90/  246]    Overall Loss 0.380850    Objective Loss 0.380850                                        LR 0.000060    Time 0.020442    
2023-01-06 16:11:47,471 - Epoch: [24][  100/  246]    Overall Loss 0.379143    Objective Loss 0.379143                                        LR 0.000060    Time 0.020004    
2023-01-06 16:11:47,636 - Epoch: [24][  110/  246]    Overall Loss 0.376939    Objective Loss 0.376939                                        LR 0.000060    Time 0.019675    
2023-01-06 16:11:47,798 - Epoch: [24][  120/  246]    Overall Loss 0.377699    Objective Loss 0.377699                                        LR 0.000060    Time 0.019383    
2023-01-06 16:11:47,952 - Epoch: [24][  130/  246]    Overall Loss 0.377057    Objective Loss 0.377057                                        LR 0.000060    Time 0.019077    
2023-01-06 16:11:48,114 - Epoch: [24][  140/  246]    Overall Loss 0.375403    Objective Loss 0.375403                                        LR 0.000060    Time 0.018866    
2023-01-06 16:11:48,276 - Epoch: [24][  150/  246]    Overall Loss 0.374525    Objective Loss 0.374525                                        LR 0.000060    Time 0.018690    
2023-01-06 16:11:48,464 - Epoch: [24][  160/  246]    Overall Loss 0.374952    Objective Loss 0.374952                                        LR 0.000060    Time 0.018690    
2023-01-06 16:11:48,654 - Epoch: [24][  170/  246]    Overall Loss 0.374315    Objective Loss 0.374315                                        LR 0.000060    Time 0.018706    
2023-01-06 16:11:48,844 - Epoch: [24][  180/  246]    Overall Loss 0.375114    Objective Loss 0.375114                                        LR 0.000060    Time 0.018722    
2023-01-06 16:11:49,036 - Epoch: [24][  190/  246]    Overall Loss 0.375258    Objective Loss 0.375258                                        LR 0.000060    Time 0.018744    
2023-01-06 16:11:49,224 - Epoch: [24][  200/  246]    Overall Loss 0.374097    Objective Loss 0.374097                                        LR 0.000060    Time 0.018744    
2023-01-06 16:11:49,414 - Epoch: [24][  210/  246]    Overall Loss 0.373310    Objective Loss 0.373310                                        LR 0.000060    Time 0.018757    
2023-01-06 16:11:49,598 - Epoch: [24][  220/  246]    Overall Loss 0.373804    Objective Loss 0.373804                                        LR 0.000060    Time 0.018737    
2023-01-06 16:11:49,785 - Epoch: [24][  230/  246]    Overall Loss 0.374338    Objective Loss 0.374338                                        LR 0.000060    Time 0.018735    
2023-01-06 16:11:49,984 - Epoch: [24][  240/  246]    Overall Loss 0.374416    Objective Loss 0.374416                                        LR 0.000060    Time 0.018783    
2023-01-06 16:11:50,067 - Epoch: [24][  246/  246]    Overall Loss 0.373925    Objective Loss 0.373925    Top1 87.799043    LR 0.000060    Time 0.018661    
2023-01-06 16:11:50,207 - --- validate (epoch=24)-----------
2023-01-06 16:11:50,207 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:50,655 - Epoch: [24][   10/   28]    Loss 0.388357    Top1 85.546875    
2023-01-06 16:11:50,756 - Epoch: [24][   20/   28]    Loss 0.373853    Top1 86.132812    
2023-01-06 16:11:50,814 - Epoch: [24][   28/   28]    Loss 0.368884    Top1 86.301174    
2023-01-06 16:11:50,974 - ==> Top1: 86.301    Loss: 0.369

2023-01-06 16:11:50,975 - ==> Confusion:
[[  88    3  348]
 [   3   49  550]
 [  36   17 5892]]

2023-01-06 16:11:50,976 - ==> Best [Top1: 86.688   Sparsity:0.00   Params: 46192 on epoch: 23]
2023-01-06 16:11:50,977 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:50,981 - 

2023-01-06 16:11:50,981 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:51,668 - Epoch: [25][   10/  246]    Overall Loss 0.374044    Objective Loss 0.374044                                        LR 0.000060    Time 0.068613    
2023-01-06 16:11:51,833 - Epoch: [25][   20/  246]    Overall Loss 0.390043    Objective Loss 0.390043                                        LR 0.000060    Time 0.042521    
2023-01-06 16:11:51,978 - Epoch: [25][   30/  246]    Overall Loss 0.386264    Objective Loss 0.386264                                        LR 0.000060    Time 0.033106    
2023-01-06 16:11:52,118 - Epoch: [25][   40/  246]    Overall Loss 0.383057    Objective Loss 0.383057                                        LR 0.000060    Time 0.028315    
2023-01-06 16:11:52,258 - Epoch: [25][   50/  246]    Overall Loss 0.381517    Objective Loss 0.381517                                        LR 0.000060    Time 0.025454    
2023-01-06 16:11:52,395 - Epoch: [25][   60/  246]    Overall Loss 0.379505    Objective Loss 0.379505                                        LR 0.000060    Time 0.023488    
2023-01-06 16:11:52,532 - Epoch: [25][   70/  246]    Overall Loss 0.376807    Objective Loss 0.376807                                        LR 0.000060    Time 0.022079    
2023-01-06 16:11:52,678 - Epoch: [25][   80/  246]    Overall Loss 0.380103    Objective Loss 0.380103                                        LR 0.000060    Time 0.021145    
2023-01-06 16:11:52,831 - Epoch: [25][   90/  246]    Overall Loss 0.381046    Objective Loss 0.381046                                        LR 0.000060    Time 0.020495    
2023-01-06 16:11:52,985 - Epoch: [25][  100/  246]    Overall Loss 0.380048    Objective Loss 0.380048                                        LR 0.000060    Time 0.019974    
2023-01-06 16:11:53,148 - Epoch: [25][  110/  246]    Overall Loss 0.378642    Objective Loss 0.378642                                        LR 0.000060    Time 0.019639    
2023-01-06 16:11:53,285 - Epoch: [25][  120/  246]    Overall Loss 0.378261    Objective Loss 0.378261                                        LR 0.000060    Time 0.019135    
2023-01-06 16:11:53,426 - Epoch: [25][  130/  246]    Overall Loss 0.377493    Objective Loss 0.377493                                        LR 0.000060    Time 0.018743    
2023-01-06 16:11:53,591 - Epoch: [25][  140/  246]    Overall Loss 0.376884    Objective Loss 0.376884                                        LR 0.000060    Time 0.018582    
2023-01-06 16:11:53,755 - Epoch: [25][  150/  246]    Overall Loss 0.377193    Objective Loss 0.377193                                        LR 0.000060    Time 0.018437    
2023-01-06 16:11:53,916 - Epoch: [25][  160/  246]    Overall Loss 0.377037    Objective Loss 0.377037                                        LR 0.000060    Time 0.018290    
2023-01-06 16:11:54,059 - Epoch: [25][  170/  246]    Overall Loss 0.378412    Objective Loss 0.378412                                        LR 0.000060    Time 0.018049    
2023-01-06 16:11:54,203 - Epoch: [25][  180/  246]    Overall Loss 0.379041    Objective Loss 0.379041                                        LR 0.000060    Time 0.017848    
2023-01-06 16:11:54,353 - Epoch: [25][  190/  246]    Overall Loss 0.378979    Objective Loss 0.378979                                        LR 0.000060    Time 0.017694    
2023-01-06 16:11:54,502 - Epoch: [25][  200/  246]    Overall Loss 0.377387    Objective Loss 0.377387                                        LR 0.000060    Time 0.017552    
2023-01-06 16:11:54,653 - Epoch: [25][  210/  246]    Overall Loss 0.377642    Objective Loss 0.377642                                        LR 0.000060    Time 0.017434    
2023-01-06 16:11:54,801 - Epoch: [25][  220/  246]    Overall Loss 0.377790    Objective Loss 0.377790                                        LR 0.000060    Time 0.017312    
2023-01-06 16:11:54,951 - Epoch: [25][  230/  246]    Overall Loss 0.375679    Objective Loss 0.375679                                        LR 0.000060    Time 0.017213    
2023-01-06 16:11:55,113 - Epoch: [25][  240/  246]    Overall Loss 0.375755    Objective Loss 0.375755                                        LR 0.000060    Time 0.017167    
2023-01-06 16:11:55,184 - Epoch: [25][  246/  246]    Overall Loss 0.375305    Objective Loss 0.375305    Top1 86.363636    LR 0.000060    Time 0.017039    
2023-01-06 16:11:55,324 - --- validate (epoch=25)-----------
2023-01-06 16:11:55,324 - 6986 samples (256 per mini-batch)
2023-01-06 16:11:55,777 - Epoch: [25][   10/   28]    Loss 0.384613    Top1 85.937500    
2023-01-06 16:11:55,897 - Epoch: [25][   20/   28]    Loss 0.378753    Top1 86.093750    
2023-01-06 16:11:55,956 - Epoch: [25][   28/   28]    Loss 0.369454    Top1 86.329802    
2023-01-06 16:11:56,102 - ==> Top1: 86.330    Loss: 0.369

2023-01-06 16:11:56,102 - ==> Confusion:
[[ 179    2  258]
 [  18   81  503]
 [ 135   39 5771]]

2023-01-06 16:11:56,104 - ==> Best [Top1: 86.688   Sparsity:0.00   Params: 46192 on epoch: 23]
2023-01-06 16:11:56,104 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:11:56,108 - 

2023-01-06 16:11:56,108 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:11:56,636 - Epoch: [26][   10/  246]    Overall Loss 0.384437    Objective Loss 0.384437                                        LR 0.000060    Time 0.052667    
2023-01-06 16:11:56,782 - Epoch: [26][   20/  246]    Overall Loss 0.374087    Objective Loss 0.374087                                        LR 0.000060    Time 0.033615    
2023-01-06 16:11:56,922 - Epoch: [26][   30/  246]    Overall Loss 0.372011    Objective Loss 0.372011                                        LR 0.000060    Time 0.027087    
2023-01-06 16:11:57,067 - Epoch: [26][   40/  246]    Overall Loss 0.370165    Objective Loss 0.370165                                        LR 0.000060    Time 0.023930    
2023-01-06 16:11:57,208 - Epoch: [26][   50/  246]    Overall Loss 0.371890    Objective Loss 0.371890                                        LR 0.000060    Time 0.021949    
2023-01-06 16:11:57,348 - Epoch: [26][   60/  246]    Overall Loss 0.371257    Objective Loss 0.371257                                        LR 0.000060    Time 0.020618    
2023-01-06 16:11:57,488 - Epoch: [26][   70/  246]    Overall Loss 0.372455    Objective Loss 0.372455                                        LR 0.000060    Time 0.019663    
2023-01-06 16:11:57,638 - Epoch: [26][   80/  246]    Overall Loss 0.373265    Objective Loss 0.373265                                        LR 0.000060    Time 0.019076    
2023-01-06 16:11:57,795 - Epoch: [26][   90/  246]    Overall Loss 0.374341    Objective Loss 0.374341                                        LR 0.000060    Time 0.018700    
2023-01-06 16:11:57,949 - Epoch: [26][  100/  246]    Overall Loss 0.376064    Objective Loss 0.376064                                        LR 0.000060    Time 0.018362    
2023-01-06 16:11:58,113 - Epoch: [26][  110/  246]    Overall Loss 0.375942    Objective Loss 0.375942                                        LR 0.000060    Time 0.018186    
2023-01-06 16:11:58,267 - Epoch: [26][  120/  246]    Overall Loss 0.377036    Objective Loss 0.377036                                        LR 0.000060    Time 0.017944    
2023-01-06 16:11:58,428 - Epoch: [26][  130/  246]    Overall Loss 0.374403    Objective Loss 0.374403                                        LR 0.000060    Time 0.017803    
2023-01-06 16:11:58,579 - Epoch: [26][  140/  246]    Overall Loss 0.374380    Objective Loss 0.374380                                        LR 0.000060    Time 0.017602    
2023-01-06 16:11:58,737 - Epoch: [26][  150/  246]    Overall Loss 0.374318    Objective Loss 0.374318                                        LR 0.000060    Time 0.017472    
2023-01-06 16:11:58,902 - Epoch: [26][  160/  246]    Overall Loss 0.375300    Objective Loss 0.375300                                        LR 0.000060    Time 0.017408    
2023-01-06 16:11:59,068 - Epoch: [26][  170/  246]    Overall Loss 0.374464    Objective Loss 0.374464                                        LR 0.000060    Time 0.017359    
2023-01-06 16:11:59,225 - Epoch: [26][  180/  246]    Overall Loss 0.374686    Objective Loss 0.374686                                        LR 0.000060    Time 0.017264    
2023-01-06 16:11:59,378 - Epoch: [26][  190/  246]    Overall Loss 0.373887    Objective Loss 0.373887                                        LR 0.000060    Time 0.017158    
2023-01-06 16:11:59,534 - Epoch: [26][  200/  246]    Overall Loss 0.375180    Objective Loss 0.375180                                        LR 0.000060    Time 0.017079    
2023-01-06 16:11:59,698 - Epoch: [26][  210/  246]    Overall Loss 0.375255    Objective Loss 0.375255                                        LR 0.000060    Time 0.017041    
2023-01-06 16:11:59,852 - Epoch: [26][  220/  246]    Overall Loss 0.374835    Objective Loss 0.374835                                        LR 0.000060    Time 0.016969    
2023-01-06 16:12:00,014 - Epoch: [26][  230/  246]    Overall Loss 0.374485    Objective Loss 0.374485                                        LR 0.000060    Time 0.016934    
2023-01-06 16:12:00,186 - Epoch: [26][  240/  246]    Overall Loss 0.373690    Objective Loss 0.373690                                        LR 0.000060    Time 0.016944    
2023-01-06 16:12:00,264 - Epoch: [26][  246/  246]    Overall Loss 0.373303    Objective Loss 0.373303    Top1 86.363636    LR 0.000060    Time 0.016846    
2023-01-06 16:12:00,389 - --- validate (epoch=26)-----------
2023-01-06 16:12:00,389 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:01,125 - Epoch: [26][   10/   28]    Loss 0.361712    Top1 86.406250    
2023-01-06 16:12:01,230 - Epoch: [26][   20/   28]    Loss 0.371938    Top1 86.074219    
2023-01-06 16:12:01,292 - Epoch: [26][   28/   28]    Loss 0.369408    Top1 86.401374    
2023-01-06 16:12:01,455 - ==> Top1: 86.401    Loss: 0.369

2023-01-06 16:12:01,455 - ==> Confusion:
[[  75    3  361]
 [   3   60  539]
 [  24   20 5901]]

2023-01-06 16:12:01,456 - ==> Best [Top1: 86.688   Sparsity:0.00   Params: 46192 on epoch: 23]
2023-01-06 16:12:01,456 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:01,461 - 

2023-01-06 16:12:01,461 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:01,996 - Epoch: [27][   10/  246]    Overall Loss 0.367340    Objective Loss 0.367340                                        LR 0.000060    Time 0.053517    
2023-01-06 16:12:02,167 - Epoch: [27][   20/  246]    Overall Loss 0.363075    Objective Loss 0.363075                                        LR 0.000060    Time 0.035280    
2023-01-06 16:12:02,347 - Epoch: [27][   30/  246]    Overall Loss 0.372928    Objective Loss 0.372928                                        LR 0.000060    Time 0.029504    
2023-01-06 16:12:02,527 - Epoch: [27][   40/  246]    Overall Loss 0.367753    Objective Loss 0.367753                                        LR 0.000060    Time 0.026601    
2023-01-06 16:12:02,700 - Epoch: [27][   50/  246]    Overall Loss 0.372863    Objective Loss 0.372863                                        LR 0.000060    Time 0.024695    
2023-01-06 16:12:02,880 - Epoch: [27][   60/  246]    Overall Loss 0.371735    Objective Loss 0.371735                                        LR 0.000060    Time 0.023584    
2023-01-06 16:12:03,061 - Epoch: [27][   70/  246]    Overall Loss 0.372947    Objective Loss 0.372947                                        LR 0.000060    Time 0.022788    
2023-01-06 16:12:03,242 - Epoch: [27][   80/  246]    Overall Loss 0.373090    Objective Loss 0.373090                                        LR 0.000060    Time 0.022199    
2023-01-06 16:12:03,420 - Epoch: [27][   90/  246]    Overall Loss 0.371074    Objective Loss 0.371074                                        LR 0.000060    Time 0.021687    
2023-01-06 16:12:03,598 - Epoch: [27][  100/  246]    Overall Loss 0.368894    Objective Loss 0.368894                                        LR 0.000060    Time 0.021294    
2023-01-06 16:12:03,777 - Epoch: [27][  110/  246]    Overall Loss 0.370843    Objective Loss 0.370843                                        LR 0.000060    Time 0.020962    
2023-01-06 16:12:03,955 - Epoch: [27][  120/  246]    Overall Loss 0.371498    Objective Loss 0.371498                                        LR 0.000060    Time 0.020690    
2023-01-06 16:12:04,135 - Epoch: [27][  130/  246]    Overall Loss 0.371439    Objective Loss 0.371439                                        LR 0.000060    Time 0.020465    
2023-01-06 16:12:04,323 - Epoch: [27][  140/  246]    Overall Loss 0.370323    Objective Loss 0.370323                                        LR 0.000060    Time 0.020342    
2023-01-06 16:12:04,500 - Epoch: [27][  150/  246]    Overall Loss 0.369338    Objective Loss 0.369338                                        LR 0.000060    Time 0.020156    
2023-01-06 16:12:04,678 - Epoch: [27][  160/  246]    Overall Loss 0.369150    Objective Loss 0.369150                                        LR 0.000060    Time 0.020006    
2023-01-06 16:12:04,847 - Epoch: [27][  170/  246]    Overall Loss 0.369500    Objective Loss 0.369500                                        LR 0.000060    Time 0.019809    
2023-01-06 16:12:05,021 - Epoch: [27][  180/  246]    Overall Loss 0.368680    Objective Loss 0.368680                                        LR 0.000060    Time 0.019677    
2023-01-06 16:12:05,191 - Epoch: [27][  190/  246]    Overall Loss 0.369752    Objective Loss 0.369752                                        LR 0.000060    Time 0.019533    
2023-01-06 16:12:05,366 - Epoch: [27][  200/  246]    Overall Loss 0.368969    Objective Loss 0.368969                                        LR 0.000060    Time 0.019419    
2023-01-06 16:12:05,535 - Epoch: [27][  210/  246]    Overall Loss 0.370478    Objective Loss 0.370478                                        LR 0.000060    Time 0.019294    
2023-01-06 16:12:05,714 - Epoch: [27][  220/  246]    Overall Loss 0.370232    Objective Loss 0.370232                                        LR 0.000060    Time 0.019229    
2023-01-06 16:12:05,890 - Epoch: [27][  230/  246]    Overall Loss 0.369960    Objective Loss 0.369960                                        LR 0.000060    Time 0.019160    
2023-01-06 16:12:06,078 - Epoch: [27][  240/  246]    Overall Loss 0.370460    Objective Loss 0.370460                                        LR 0.000060    Time 0.019143    
2023-01-06 16:12:06,161 - Epoch: [27][  246/  246]    Overall Loss 0.370358    Objective Loss 0.370358    Top1 87.320574    LR 0.000060    Time 0.019013    
2023-01-06 16:12:06,302 - --- validate (epoch=27)-----------
2023-01-06 16:12:06,302 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:06,727 - Epoch: [27][   10/   28]    Loss 0.353553    Top1 87.343750    
2023-01-06 16:12:06,842 - Epoch: [27][   20/   28]    Loss 0.361031    Top1 87.011719    
2023-01-06 16:12:06,899 - Epoch: [27][   28/   28]    Loss 0.364070    Top1 86.845119    
2023-01-06 16:12:07,056 - ==> Top1: 86.845    Loss: 0.364

2023-01-06 16:12:07,056 - ==> Confusion:
[[ 149    3  287]
 [   8  101  493]
 [  81   47 5817]]

2023-01-06 16:12:07,057 - ==> Best [Top1: 86.845   Sparsity:0.00   Params: 46192 on epoch: 27]
2023-01-06 16:12:07,057 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:07,062 - 

2023-01-06 16:12:07,062 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:07,719 - Epoch: [28][   10/  246]    Overall Loss 0.373015    Objective Loss 0.373015                                        LR 0.000060    Time 0.065638    
2023-01-06 16:12:07,878 - Epoch: [28][   20/  246]    Overall Loss 0.358280    Objective Loss 0.358280                                        LR 0.000060    Time 0.040737    
2023-01-06 16:12:08,036 - Epoch: [28][   30/  246]    Overall Loss 0.357894    Objective Loss 0.357894                                        LR 0.000060    Time 0.032363    
2023-01-06 16:12:08,181 - Epoch: [28][   40/  246]    Overall Loss 0.356917    Objective Loss 0.356917                                        LR 0.000060    Time 0.027882    
2023-01-06 16:12:08,325 - Epoch: [28][   50/  246]    Overall Loss 0.358137    Objective Loss 0.358137                                        LR 0.000060    Time 0.025185    
2023-01-06 16:12:08,473 - Epoch: [28][   60/  246]    Overall Loss 0.361070    Objective Loss 0.361070                                        LR 0.000060    Time 0.023416    
2023-01-06 16:12:08,618 - Epoch: [28][   70/  246]    Overall Loss 0.364721    Objective Loss 0.364721                                        LR 0.000060    Time 0.022135    
2023-01-06 16:12:08,766 - Epoch: [28][   80/  246]    Overall Loss 0.362164    Objective Loss 0.362164                                        LR 0.000060    Time 0.021222    
2023-01-06 16:12:08,911 - Epoch: [28][   90/  246]    Overall Loss 0.364340    Objective Loss 0.364340                                        LR 0.000060    Time 0.020466    
2023-01-06 16:12:09,056 - Epoch: [28][  100/  246]    Overall Loss 0.365739    Objective Loss 0.365739                                        LR 0.000060    Time 0.019863    
2023-01-06 16:12:09,220 - Epoch: [28][  110/  246]    Overall Loss 0.365915    Objective Loss 0.365915                                        LR 0.000060    Time 0.019548    
2023-01-06 16:12:09,378 - Epoch: [28][  120/  246]    Overall Loss 0.364495    Objective Loss 0.364495                                        LR 0.000060    Time 0.019232    
2023-01-06 16:12:09,548 - Epoch: [28][  130/  246]    Overall Loss 0.363223    Objective Loss 0.363223                                        LR 0.000060    Time 0.019058    
2023-01-06 16:12:09,702 - Epoch: [28][  140/  246]    Overall Loss 0.364692    Objective Loss 0.364692                                        LR 0.000060    Time 0.018793    
2023-01-06 16:12:09,870 - Epoch: [28][  150/  246]    Overall Loss 0.365133    Objective Loss 0.365133                                        LR 0.000060    Time 0.018658    
2023-01-06 16:12:10,029 - Epoch: [28][  160/  246]    Overall Loss 0.363805    Objective Loss 0.363805                                        LR 0.000060    Time 0.018481    
2023-01-06 16:12:10,190 - Epoch: [28][  170/  246]    Overall Loss 0.365430    Objective Loss 0.365430                                        LR 0.000060    Time 0.018338    
2023-01-06 16:12:10,338 - Epoch: [28][  180/  246]    Overall Loss 0.365687    Objective Loss 0.365687                                        LR 0.000060    Time 0.018141    
2023-01-06 16:12:10,493 - Epoch: [28][  190/  246]    Overall Loss 0.366038    Objective Loss 0.366038                                        LR 0.000060    Time 0.018003    
2023-01-06 16:12:10,643 - Epoch: [28][  200/  246]    Overall Loss 0.367147    Objective Loss 0.367147                                        LR 0.000060    Time 0.017851    
2023-01-06 16:12:10,797 - Epoch: [28][  210/  246]    Overall Loss 0.367963    Objective Loss 0.367963                                        LR 0.000060    Time 0.017722    
2023-01-06 16:12:10,942 - Epoch: [28][  220/  246]    Overall Loss 0.368047    Objective Loss 0.368047                                        LR 0.000060    Time 0.017576    
2023-01-06 16:12:11,095 - Epoch: [28][  230/  246]    Overall Loss 0.369272    Objective Loss 0.369272                                        LR 0.000060    Time 0.017472    
2023-01-06 16:12:11,259 - Epoch: [28][  240/  246]    Overall Loss 0.369704    Objective Loss 0.369704                                        LR 0.000060    Time 0.017428    
2023-01-06 16:12:11,336 - Epoch: [28][  246/  246]    Overall Loss 0.369518    Objective Loss 0.369518    Top1 88.516746    LR 0.000060    Time 0.017314    
2023-01-06 16:12:11,475 - --- validate (epoch=28)-----------
2023-01-06 16:12:11,475 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:11,927 - Epoch: [28][   10/   28]    Loss 0.386686    Top1 86.054688    
2023-01-06 16:12:12,047 - Epoch: [28][   20/   28]    Loss 0.368789    Top1 86.464844    
2023-01-06 16:12:12,107 - Epoch: [28][   28/   28]    Loss 0.366626    Top1 86.601775    
2023-01-06 16:12:12,240 - ==> Top1: 86.602    Loss: 0.367

2023-01-06 16:12:12,240 - ==> Confusion:
[[ 136    0  303]
 [  10   56  536]
 [  69   18 5858]]

2023-01-06 16:12:12,241 - ==> Best [Top1: 86.845   Sparsity:0.00   Params: 46192 on epoch: 27]
2023-01-06 16:12:12,241 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:12,245 - 

2023-01-06 16:12:12,245 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:12,813 - Epoch: [29][   10/  246]    Overall Loss 0.375815    Objective Loss 0.375815                                        LR 0.000060    Time 0.056727    
2023-01-06 16:12:12,998 - Epoch: [29][   20/  246]    Overall Loss 0.371960    Objective Loss 0.371960                                        LR 0.000060    Time 0.037572    
2023-01-06 16:12:13,167 - Epoch: [29][   30/  246]    Overall Loss 0.368307    Objective Loss 0.368307                                        LR 0.000060    Time 0.030655    
2023-01-06 16:12:13,332 - Epoch: [29][   40/  246]    Overall Loss 0.364605    Objective Loss 0.364605                                        LR 0.000060    Time 0.027118    
2023-01-06 16:12:13,498 - Epoch: [29][   50/  246]    Overall Loss 0.368369    Objective Loss 0.368369                                        LR 0.000060    Time 0.024999    
2023-01-06 16:12:13,664 - Epoch: [29][   60/  246]    Overall Loss 0.363578    Objective Loss 0.363578                                        LR 0.000060    Time 0.023595    
2023-01-06 16:12:13,830 - Epoch: [29][   70/  246]    Overall Loss 0.363680    Objective Loss 0.363680                                        LR 0.000060    Time 0.022585    
2023-01-06 16:12:13,994 - Epoch: [29][   80/  246]    Overall Loss 0.363002    Objective Loss 0.363002                                        LR 0.000060    Time 0.021810    
2023-01-06 16:12:14,172 - Epoch: [29][   90/  246]    Overall Loss 0.361811    Objective Loss 0.361811                                        LR 0.000060    Time 0.021355    
2023-01-06 16:12:14,348 - Epoch: [29][  100/  246]    Overall Loss 0.360566    Objective Loss 0.360566                                        LR 0.000060    Time 0.020978    
2023-01-06 16:12:14,531 - Epoch: [29][  110/  246]    Overall Loss 0.360231    Objective Loss 0.360231                                        LR 0.000060    Time 0.020732    
2023-01-06 16:12:14,713 - Epoch: [29][  120/  246]    Overall Loss 0.358282    Objective Loss 0.358282                                        LR 0.000060    Time 0.020521    
2023-01-06 16:12:14,891 - Epoch: [29][  130/  246]    Overall Loss 0.357996    Objective Loss 0.357996                                        LR 0.000060    Time 0.020289    
2023-01-06 16:12:15,061 - Epoch: [29][  140/  246]    Overall Loss 0.358460    Objective Loss 0.358460                                        LR 0.000060    Time 0.020054    
2023-01-06 16:12:15,212 - Epoch: [29][  150/  246]    Overall Loss 0.359975    Objective Loss 0.359975                                        LR 0.000060    Time 0.019721    
2023-01-06 16:12:15,364 - Epoch: [29][  160/  246]    Overall Loss 0.362131    Objective Loss 0.362131                                        LR 0.000060    Time 0.019436    
2023-01-06 16:12:15,509 - Epoch: [29][  170/  246]    Overall Loss 0.362746    Objective Loss 0.362746                                        LR 0.000060    Time 0.019144    
2023-01-06 16:12:15,661 - Epoch: [29][  180/  246]    Overall Loss 0.363817    Objective Loss 0.363817                                        LR 0.000060    Time 0.018921    
2023-01-06 16:12:15,804 - Epoch: [29][  190/  246]    Overall Loss 0.364217    Objective Loss 0.364217                                        LR 0.000060    Time 0.018676    
2023-01-06 16:12:15,954 - Epoch: [29][  200/  246]    Overall Loss 0.365184    Objective Loss 0.365184                                        LR 0.000060    Time 0.018493    
2023-01-06 16:12:16,113 - Epoch: [29][  210/  246]    Overall Loss 0.366690    Objective Loss 0.366690                                        LR 0.000060    Time 0.018364    
2023-01-06 16:12:16,274 - Epoch: [29][  220/  246]    Overall Loss 0.367231    Objective Loss 0.367231                                        LR 0.000060    Time 0.018261    
2023-01-06 16:12:16,428 - Epoch: [29][  230/  246]    Overall Loss 0.368303    Objective Loss 0.368303                                        LR 0.000060    Time 0.018135    
2023-01-06 16:12:16,609 - Epoch: [29][  240/  246]    Overall Loss 0.368904    Objective Loss 0.368904                                        LR 0.000060    Time 0.018133    
2023-01-06 16:12:16,688 - Epoch: [29][  246/  246]    Overall Loss 0.368648    Objective Loss 0.368648    Top1 87.320574    LR 0.000060    Time 0.018009    
2023-01-06 16:12:16,815 - --- validate (epoch=29)-----------
2023-01-06 16:12:16,815 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:17,255 - Epoch: [29][   10/   28]    Loss 0.352278    Top1 86.718750    
2023-01-06 16:12:17,370 - Epoch: [29][   20/   28]    Loss 0.358028    Top1 86.601562    
2023-01-06 16:12:17,427 - Epoch: [29][   28/   28]    Loss 0.362981    Top1 86.573146    
2023-01-06 16:12:17,562 - ==> Top1: 86.573    Loss: 0.363

2023-01-06 16:12:17,562 - ==> Confusion:
[[ 118    1  320]
 [   4   47  551]
 [  48   14 5883]]

2023-01-06 16:12:17,564 - ==> Best [Top1: 86.845   Sparsity:0.00   Params: 46192 on epoch: 27]
2023-01-06 16:12:17,564 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:17,568 - 

2023-01-06 16:12:17,568 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:18,235 - Epoch: [30][   10/  246]    Overall Loss 0.370802    Objective Loss 0.370802                                        LR 0.000060    Time 0.066614    
2023-01-06 16:12:18,383 - Epoch: [30][   20/  246]    Overall Loss 0.372872    Objective Loss 0.372872                                        LR 0.000060    Time 0.040659    
2023-01-06 16:12:18,534 - Epoch: [30][   30/  246]    Overall Loss 0.363294    Objective Loss 0.363294                                        LR 0.000060    Time 0.032125    
2023-01-06 16:12:18,688 - Epoch: [30][   40/  246]    Overall Loss 0.371202    Objective Loss 0.371202                                        LR 0.000060    Time 0.027940    
2023-01-06 16:12:18,849 - Epoch: [30][   50/  246]    Overall Loss 0.373866    Objective Loss 0.373866                                        LR 0.000060    Time 0.025569    
2023-01-06 16:12:19,000 - Epoch: [30][   60/  246]    Overall Loss 0.375610    Objective Loss 0.375610                                        LR 0.000060    Time 0.023791    
2023-01-06 16:12:19,153 - Epoch: [30][   70/  246]    Overall Loss 0.375156    Objective Loss 0.375156                                        LR 0.000060    Time 0.022566    
2023-01-06 16:12:19,295 - Epoch: [30][   80/  246]    Overall Loss 0.376180    Objective Loss 0.376180                                        LR 0.000060    Time 0.021526    
2023-01-06 16:12:19,445 - Epoch: [30][   90/  246]    Overall Loss 0.375048    Objective Loss 0.375048                                        LR 0.000060    Time 0.020794    
2023-01-06 16:12:19,592 - Epoch: [30][  100/  246]    Overall Loss 0.372880    Objective Loss 0.372880                                        LR 0.000060    Time 0.020176    
2023-01-06 16:12:19,745 - Epoch: [30][  110/  246]    Overall Loss 0.373208    Objective Loss 0.373208                                        LR 0.000060    Time 0.019730    
2023-01-06 16:12:19,892 - Epoch: [30][  120/  246]    Overall Loss 0.370596    Objective Loss 0.370596                                        LR 0.000060    Time 0.019312    
2023-01-06 16:12:20,047 - Epoch: [30][  130/  246]    Overall Loss 0.369253    Objective Loss 0.369253                                        LR 0.000060    Time 0.019012    
2023-01-06 16:12:20,196 - Epoch: [30][  140/  246]    Overall Loss 0.367978    Objective Loss 0.367978                                        LR 0.000060    Time 0.018716    
2023-01-06 16:12:20,339 - Epoch: [30][  150/  246]    Overall Loss 0.368142    Objective Loss 0.368142                                        LR 0.000060    Time 0.018423    
2023-01-06 16:12:20,479 - Epoch: [30][  160/  246]    Overall Loss 0.367284    Objective Loss 0.367284                                        LR 0.000060    Time 0.018142    
2023-01-06 16:12:20,612 - Epoch: [30][  170/  246]    Overall Loss 0.367049    Objective Loss 0.367049                                        LR 0.000060    Time 0.017857    
2023-01-06 16:12:20,743 - Epoch: [30][  180/  246]    Overall Loss 0.365792    Objective Loss 0.365792                                        LR 0.000060    Time 0.017587    
2023-01-06 16:12:20,874 - Epoch: [30][  190/  246]    Overall Loss 0.366365    Objective Loss 0.366365                                        LR 0.000060    Time 0.017351    
2023-01-06 16:12:21,023 - Epoch: [30][  200/  246]    Overall Loss 0.367107    Objective Loss 0.367107                                        LR 0.000060    Time 0.017229    
2023-01-06 16:12:21,189 - Epoch: [30][  210/  246]    Overall Loss 0.367339    Objective Loss 0.367339                                        LR 0.000060    Time 0.017193    
2023-01-06 16:12:21,361 - Epoch: [30][  220/  246]    Overall Loss 0.366620    Objective Loss 0.366620                                        LR 0.000060    Time 0.017194    
2023-01-06 16:12:21,528 - Epoch: [30][  230/  246]    Overall Loss 0.366360    Objective Loss 0.366360                                        LR 0.000060    Time 0.017161    
2023-01-06 16:12:21,710 - Epoch: [30][  240/  246]    Overall Loss 0.365436    Objective Loss 0.365436                                        LR 0.000060    Time 0.017203    
2023-01-06 16:12:21,789 - Epoch: [30][  246/  246]    Overall Loss 0.366379    Objective Loss 0.366379    Top1 84.449761    LR 0.000060    Time 0.017103    
2023-01-06 16:12:21,928 - --- validate (epoch=30)-----------
2023-01-06 16:12:21,928 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:22,362 - Epoch: [30][   10/   28]    Loss 0.373188    Top1 86.132812    
2023-01-06 16:12:22,463 - Epoch: [30][   20/   28]    Loss 0.358383    Top1 86.718750    
2023-01-06 16:12:22,518 - Epoch: [30][   28/   28]    Loss 0.357030    Top1 86.902376    
2023-01-06 16:12:22,653 - ==> Top1: 86.902    Loss: 0.357

2023-01-06 16:12:22,653 - ==> Confusion:
[[ 144    1  294]
 [  10   84  508]
 [  72   30 5843]]

2023-01-06 16:12:22,654 - ==> Best [Top1: 86.902   Sparsity:0.00   Params: 46192 on epoch: 30]
2023-01-06 16:12:22,654 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:22,660 - 

2023-01-06 16:12:22,660 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:23,360 - Epoch: [31][   10/  246]    Overall Loss 0.366514    Objective Loss 0.366514                                        LR 0.000060    Time 0.069973    
2023-01-06 16:12:23,532 - Epoch: [31][   20/  246]    Overall Loss 0.365311    Objective Loss 0.365311                                        LR 0.000060    Time 0.043548    
2023-01-06 16:12:23,702 - Epoch: [31][   30/  246]    Overall Loss 0.366411    Objective Loss 0.366411                                        LR 0.000060    Time 0.034633    
2023-01-06 16:12:23,867 - Epoch: [31][   40/  246]    Overall Loss 0.364310    Objective Loss 0.364310                                        LR 0.000060    Time 0.030104    
2023-01-06 16:12:24,032 - Epoch: [31][   50/  246]    Overall Loss 0.367894    Objective Loss 0.367894                                        LR 0.000060    Time 0.027353    
2023-01-06 16:12:24,195 - Epoch: [31][   60/  246]    Overall Loss 0.368821    Objective Loss 0.368821                                        LR 0.000060    Time 0.025510    
2023-01-06 16:12:24,372 - Epoch: [31][   70/  246]    Overall Loss 0.366446    Objective Loss 0.366446                                        LR 0.000060    Time 0.024383    
2023-01-06 16:12:24,557 - Epoch: [31][   80/  246]    Overall Loss 0.366339    Objective Loss 0.366339                                        LR 0.000060    Time 0.023643    
2023-01-06 16:12:24,732 - Epoch: [31][   90/  246]    Overall Loss 0.366966    Objective Loss 0.366966                                        LR 0.000060    Time 0.022958    
2023-01-06 16:12:24,916 - Epoch: [31][  100/  246]    Overall Loss 0.365946    Objective Loss 0.365946                                        LR 0.000060    Time 0.022499    
2023-01-06 16:12:25,091 - Epoch: [31][  110/  246]    Overall Loss 0.365408    Objective Loss 0.365408                                        LR 0.000060    Time 0.022036    
2023-01-06 16:12:25,270 - Epoch: [31][  120/  246]    Overall Loss 0.365645    Objective Loss 0.365645                                        LR 0.000060    Time 0.021692    
2023-01-06 16:12:25,440 - Epoch: [31][  130/  246]    Overall Loss 0.364706    Objective Loss 0.364706                                        LR 0.000060    Time 0.021323    
2023-01-06 16:12:25,618 - Epoch: [31][  140/  246]    Overall Loss 0.364695    Objective Loss 0.364695                                        LR 0.000060    Time 0.021073    
2023-01-06 16:12:25,793 - Epoch: [31][  150/  246]    Overall Loss 0.364444    Objective Loss 0.364444                                        LR 0.000060    Time 0.020833    
2023-01-06 16:12:25,977 - Epoch: [31][  160/  246]    Overall Loss 0.364969    Objective Loss 0.364969                                        LR 0.000060    Time 0.020677    
2023-01-06 16:12:26,156 - Epoch: [31][  170/  246]    Overall Loss 0.365895    Objective Loss 0.365895                                        LR 0.000060    Time 0.020510    
2023-01-06 16:12:26,339 - Epoch: [31][  180/  246]    Overall Loss 0.367220    Objective Loss 0.367220                                        LR 0.000060    Time 0.020387    
2023-01-06 16:12:26,512 - Epoch: [31][  190/  246]    Overall Loss 0.366889    Objective Loss 0.366889                                        LR 0.000060    Time 0.020224    
2023-01-06 16:12:26,694 - Epoch: [31][  200/  246]    Overall Loss 0.365977    Objective Loss 0.365977                                        LR 0.000060    Time 0.020122    
2023-01-06 16:12:26,874 - Epoch: [31][  210/  246]    Overall Loss 0.365867    Objective Loss 0.365867                                        LR 0.000060    Time 0.020020    
2023-01-06 16:12:27,058 - Epoch: [31][  220/  246]    Overall Loss 0.365608    Objective Loss 0.365608                                        LR 0.000060    Time 0.019940    
2023-01-06 16:12:27,235 - Epoch: [31][  230/  246]    Overall Loss 0.365476    Objective Loss 0.365476                                        LR 0.000060    Time 0.019845    
2023-01-06 16:12:27,425 - Epoch: [31][  240/  246]    Overall Loss 0.365243    Objective Loss 0.365243                                        LR 0.000060    Time 0.019806    
2023-01-06 16:12:27,505 - Epoch: [31][  246/  246]    Overall Loss 0.364704    Objective Loss 0.364704    Top1 84.449761    LR 0.000060    Time 0.019650    
2023-01-06 16:12:27,648 - --- validate (epoch=31)-----------
2023-01-06 16:12:27,648 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:28,099 - Epoch: [31][   10/   28]    Loss 0.361903    Top1 87.031250    
2023-01-06 16:12:28,216 - Epoch: [31][   20/   28]    Loss 0.362932    Top1 86.914062    
2023-01-06 16:12:28,273 - Epoch: [31][   28/   28]    Loss 0.358541    Top1 87.031205    
2023-01-06 16:12:28,408 - ==> Top1: 87.031    Loss: 0.359

2023-01-06 16:12:28,408 - ==> Confusion:
[[ 122    5  312]
 [   4  109  489]
 [  50   46 5849]]

2023-01-06 16:12:28,409 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:28,409 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:28,414 - 

2023-01-06 16:12:28,414 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:28,951 - Epoch: [32][   10/  246]    Overall Loss 0.379148    Objective Loss 0.379148                                        LR 0.000060    Time 0.053588    
2023-01-06 16:12:29,114 - Epoch: [32][   20/  246]    Overall Loss 0.383502    Objective Loss 0.383502                                        LR 0.000060    Time 0.034927    
2023-01-06 16:12:29,271 - Epoch: [32][   30/  246]    Overall Loss 0.371673    Objective Loss 0.371673                                        LR 0.000060    Time 0.028511    
2023-01-06 16:12:29,429 - Epoch: [32][   40/  246]    Overall Loss 0.364703    Objective Loss 0.364703                                        LR 0.000060    Time 0.025287    
2023-01-06 16:12:29,589 - Epoch: [32][   50/  246]    Overall Loss 0.362848    Objective Loss 0.362848                                        LR 0.000060    Time 0.023423    
2023-01-06 16:12:29,747 - Epoch: [32][   60/  246]    Overall Loss 0.360942    Objective Loss 0.360942                                        LR 0.000060    Time 0.022134    
2023-01-06 16:12:29,907 - Epoch: [32][   70/  246]    Overall Loss 0.360933    Objective Loss 0.360933                                        LR 0.000060    Time 0.021258    
2023-01-06 16:12:30,055 - Epoch: [32][   80/  246]    Overall Loss 0.362348    Objective Loss 0.362348                                        LR 0.000060    Time 0.020444    
2023-01-06 16:12:30,203 - Epoch: [32][   90/  246]    Overall Loss 0.362626    Objective Loss 0.362626                                        LR 0.000060    Time 0.019809    
2023-01-06 16:12:30,350 - Epoch: [32][  100/  246]    Overall Loss 0.362424    Objective Loss 0.362424                                        LR 0.000060    Time 0.019293    
2023-01-06 16:12:30,493 - Epoch: [32][  110/  246]    Overall Loss 0.363056    Objective Loss 0.363056                                        LR 0.000060    Time 0.018838    
2023-01-06 16:12:30,639 - Epoch: [32][  120/  246]    Overall Loss 0.362977    Objective Loss 0.362977                                        LR 0.000060    Time 0.018481    
2023-01-06 16:12:30,781 - Epoch: [32][  130/  246]    Overall Loss 0.362989    Objective Loss 0.362989                                        LR 0.000060    Time 0.018148    
2023-01-06 16:12:30,925 - Epoch: [32][  140/  246]    Overall Loss 0.363484    Objective Loss 0.363484                                        LR 0.000060    Time 0.017878    
2023-01-06 16:12:31,069 - Epoch: [32][  150/  246]    Overall Loss 0.364852    Objective Loss 0.364852                                        LR 0.000060    Time 0.017642    
2023-01-06 16:12:31,212 - Epoch: [32][  160/  246]    Overall Loss 0.364853    Objective Loss 0.364853                                        LR 0.000060    Time 0.017429    
2023-01-06 16:12:31,357 - Epoch: [32][  170/  246]    Overall Loss 0.363082    Objective Loss 0.363082                                        LR 0.000060    Time 0.017255    
2023-01-06 16:12:31,501 - Epoch: [32][  180/  246]    Overall Loss 0.363626    Objective Loss 0.363626                                        LR 0.000060    Time 0.017096    
2023-01-06 16:12:31,646 - Epoch: [32][  190/  246]    Overall Loss 0.363230    Objective Loss 0.363230                                        LR 0.000060    Time 0.016956    
2023-01-06 16:12:31,788 - Epoch: [32][  200/  246]    Overall Loss 0.363165    Objective Loss 0.363165                                        LR 0.000060    Time 0.016815    
2023-01-06 16:12:31,931 - Epoch: [32][  210/  246]    Overall Loss 0.363108    Objective Loss 0.363108                                        LR 0.000060    Time 0.016696    
2023-01-06 16:12:32,074 - Epoch: [32][  220/  246]    Overall Loss 0.362956    Objective Loss 0.362956                                        LR 0.000060    Time 0.016583    
2023-01-06 16:12:32,209 - Epoch: [32][  230/  246]    Overall Loss 0.363265    Objective Loss 0.363265                                        LR 0.000060    Time 0.016449    
2023-01-06 16:12:32,360 - Epoch: [32][  240/  246]    Overall Loss 0.363306    Objective Loss 0.363306                                        LR 0.000060    Time 0.016389    
2023-01-06 16:12:32,428 - Epoch: [32][  246/  246]    Overall Loss 0.362463    Objective Loss 0.362463    Top1 87.320574    LR 0.000060    Time 0.016265    
2023-01-06 16:12:32,547 - --- validate (epoch=32)-----------
2023-01-06 16:12:32,547 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:32,987 - Epoch: [32][   10/   28]    Loss 0.356021    Top1 86.718750    
2023-01-06 16:12:33,084 - Epoch: [32][   20/   28]    Loss 0.356254    Top1 86.835938    
2023-01-06 16:12:33,141 - Epoch: [32][   28/   28]    Loss 0.361585    Top1 86.587461    
2023-01-06 16:12:33,276 - ==> Top1: 86.587    Loss: 0.362

2023-01-06 16:12:33,277 - ==> Confusion:
[[  90    3  346]
 [   3   68  531]
 [  31   23 5891]]

2023-01-06 16:12:33,278 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:33,278 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:33,282 - 

2023-01-06 16:12:33,282 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:33,933 - Epoch: [33][   10/  246]    Overall Loss 0.373237    Objective Loss 0.373237                                        LR 0.000060    Time 0.065007    
2023-01-06 16:12:34,093 - Epoch: [33][   20/  246]    Overall Loss 0.367986    Objective Loss 0.367986                                        LR 0.000060    Time 0.040500    
2023-01-06 16:12:34,245 - Epoch: [33][   30/  246]    Overall Loss 0.365763    Objective Loss 0.365763                                        LR 0.000060    Time 0.032055    
2023-01-06 16:12:34,411 - Epoch: [33][   40/  246]    Overall Loss 0.362320    Objective Loss 0.362320                                        LR 0.000060    Time 0.028159    
2023-01-06 16:12:34,570 - Epoch: [33][   50/  246]    Overall Loss 0.364416    Objective Loss 0.364416                                        LR 0.000060    Time 0.025683    
2023-01-06 16:12:34,735 - Epoch: [33][   60/  246]    Overall Loss 0.364386    Objective Loss 0.364386                                        LR 0.000060    Time 0.024154    
2023-01-06 16:12:34,896 - Epoch: [33][   70/  246]    Overall Loss 0.366882    Objective Loss 0.366882                                        LR 0.000060    Time 0.023001    
2023-01-06 16:12:35,056 - Epoch: [33][   80/  246]    Overall Loss 0.367056    Objective Loss 0.367056                                        LR 0.000060    Time 0.022108    
2023-01-06 16:12:35,227 - Epoch: [33][   90/  246]    Overall Loss 0.365911    Objective Loss 0.365911                                        LR 0.000060    Time 0.021538    
2023-01-06 16:12:35,398 - Epoch: [33][  100/  246]    Overall Loss 0.365619    Objective Loss 0.365619                                        LR 0.000060    Time 0.021096    
2023-01-06 16:12:35,565 - Epoch: [33][  110/  246]    Overall Loss 0.366074    Objective Loss 0.366074                                        LR 0.000060    Time 0.020675    
2023-01-06 16:12:35,737 - Epoch: [33][  120/  246]    Overall Loss 0.366647    Objective Loss 0.366647                                        LR 0.000060    Time 0.020371    
2023-01-06 16:12:35,905 - Epoch: [33][  130/  246]    Overall Loss 0.367023    Objective Loss 0.367023                                        LR 0.000060    Time 0.020081    
2023-01-06 16:12:36,080 - Epoch: [33][  140/  246]    Overall Loss 0.368074    Objective Loss 0.368074                                        LR 0.000060    Time 0.019888    
2023-01-06 16:12:36,257 - Epoch: [33][  150/  246]    Overall Loss 0.367955    Objective Loss 0.367955                                        LR 0.000060    Time 0.019740    
2023-01-06 16:12:36,453 - Epoch: [33][  160/  246]    Overall Loss 0.367548    Objective Loss 0.367548                                        LR 0.000060    Time 0.019732    
2023-01-06 16:12:36,652 - Epoch: [33][  170/  246]    Overall Loss 0.366542    Objective Loss 0.366542                                        LR 0.000060    Time 0.019736    
2023-01-06 16:12:36,851 - Epoch: [33][  180/  246]    Overall Loss 0.366168    Objective Loss 0.366168                                        LR 0.000060    Time 0.019747    
2023-01-06 16:12:37,030 - Epoch: [33][  190/  246]    Overall Loss 0.365233    Objective Loss 0.365233                                        LR 0.000060    Time 0.019645    
2023-01-06 16:12:37,211 - Epoch: [33][  200/  246]    Overall Loss 0.363517    Objective Loss 0.363517                                        LR 0.000060    Time 0.019564    
2023-01-06 16:12:37,390 - Epoch: [33][  210/  246]    Overall Loss 0.362688    Objective Loss 0.362688                                        LR 0.000060    Time 0.019486    
2023-01-06 16:12:37,574 - Epoch: [33][  220/  246]    Overall Loss 0.362581    Objective Loss 0.362581                                        LR 0.000060    Time 0.019432    
2023-01-06 16:12:37,753 - Epoch: [33][  230/  246]    Overall Loss 0.363312    Objective Loss 0.363312                                        LR 0.000060    Time 0.019364    
2023-01-06 16:12:37,948 - Epoch: [33][  240/  246]    Overall Loss 0.361284    Objective Loss 0.361284                                        LR 0.000060    Time 0.019371    
2023-01-06 16:12:38,025 - Epoch: [33][  246/  246]    Overall Loss 0.361202    Objective Loss 0.361202    Top1 88.995215    LR 0.000060    Time 0.019207    
2023-01-06 16:12:38,167 - --- validate (epoch=33)-----------
2023-01-06 16:12:38,167 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:38,630 - Epoch: [33][   10/   28]    Loss 0.378645    Top1 86.015625    
2023-01-06 16:12:38,745 - Epoch: [33][   20/   28]    Loss 0.366498    Top1 86.425781    
2023-01-06 16:12:38,802 - Epoch: [33][   28/   28]    Loss 0.360869    Top1 86.587461    
2023-01-06 16:12:38,941 - ==> Top1: 86.587    Loss: 0.361

2023-01-06 16:12:38,941 - ==> Confusion:
[[ 135    1  303]
 [  10   60  532]
 [  74   17 5854]]

2023-01-06 16:12:38,942 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:38,942 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:38,947 - 

2023-01-06 16:12:38,947 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:39,633 - Epoch: [34][   10/  246]    Overall Loss 0.376866    Objective Loss 0.376866                                        LR 0.000060    Time 0.068593    
2023-01-06 16:12:39,799 - Epoch: [34][   20/  246]    Overall Loss 0.370021    Objective Loss 0.370021                                        LR 0.000060    Time 0.042551    
2023-01-06 16:12:39,965 - Epoch: [34][   30/  246]    Overall Loss 0.370588    Objective Loss 0.370588                                        LR 0.000060    Time 0.033877    
2023-01-06 16:12:40,129 - Epoch: [34][   40/  246]    Overall Loss 0.364635    Objective Loss 0.364635                                        LR 0.000060    Time 0.029500    
2023-01-06 16:12:40,289 - Epoch: [34][   50/  246]    Overall Loss 0.367800    Objective Loss 0.367800                                        LR 0.000060    Time 0.026790    
2023-01-06 16:12:40,443 - Epoch: [34][   60/  246]    Overall Loss 0.368585    Objective Loss 0.368585                                        LR 0.000060    Time 0.024883    
2023-01-06 16:12:40,605 - Epoch: [34][   70/  246]    Overall Loss 0.366279    Objective Loss 0.366279                                        LR 0.000060    Time 0.023642    
2023-01-06 16:12:40,760 - Epoch: [34][   80/  246]    Overall Loss 0.364300    Objective Loss 0.364300                                        LR 0.000060    Time 0.022623    
2023-01-06 16:12:40,915 - Epoch: [34][   90/  246]    Overall Loss 0.364341    Objective Loss 0.364341                                        LR 0.000060    Time 0.021825    
2023-01-06 16:12:41,072 - Epoch: [34][  100/  246]    Overall Loss 0.362326    Objective Loss 0.362326                                        LR 0.000060    Time 0.021213    
2023-01-06 16:12:41,229 - Epoch: [34][  110/  246]    Overall Loss 0.363843    Objective Loss 0.363843                                        LR 0.000060    Time 0.020707    
2023-01-06 16:12:41,392 - Epoch: [34][  120/  246]    Overall Loss 0.361474    Objective Loss 0.361474                                        LR 0.000060    Time 0.020335    
2023-01-06 16:12:41,548 - Epoch: [34][  130/  246]    Overall Loss 0.360042    Objective Loss 0.360042                                        LR 0.000060    Time 0.019969    
2023-01-06 16:12:41,709 - Epoch: [34][  140/  246]    Overall Loss 0.361491    Objective Loss 0.361491                                        LR 0.000060    Time 0.019688    
2023-01-06 16:12:41,859 - Epoch: [34][  150/  246]    Overall Loss 0.362542    Objective Loss 0.362542                                        LR 0.000060    Time 0.019378    
2023-01-06 16:12:42,017 - Epoch: [34][  160/  246]    Overall Loss 0.361677    Objective Loss 0.361677                                        LR 0.000060    Time 0.019146    
2023-01-06 16:12:42,175 - Epoch: [34][  170/  246]    Overall Loss 0.363221    Objective Loss 0.363221                                        LR 0.000060    Time 0.018949    
2023-01-06 16:12:42,332 - Epoch: [34][  180/  246]    Overall Loss 0.362150    Objective Loss 0.362150                                        LR 0.000060    Time 0.018768    
2023-01-06 16:12:42,487 - Epoch: [34][  190/  246]    Overall Loss 0.362574    Objective Loss 0.362574                                        LR 0.000060    Time 0.018595    
2023-01-06 16:12:42,643 - Epoch: [34][  200/  246]    Overall Loss 0.361829    Objective Loss 0.361829                                        LR 0.000060    Time 0.018444    
2023-01-06 16:12:42,800 - Epoch: [34][  210/  246]    Overall Loss 0.362588    Objective Loss 0.362588                                        LR 0.000060    Time 0.018309    
2023-01-06 16:12:42,959 - Epoch: [34][  220/  246]    Overall Loss 0.362783    Objective Loss 0.362783                                        LR 0.000060    Time 0.018197    
2023-01-06 16:12:43,115 - Epoch: [34][  230/  246]    Overall Loss 0.361536    Objective Loss 0.361536                                        LR 0.000060    Time 0.018084    
2023-01-06 16:12:43,284 - Epoch: [34][  240/  246]    Overall Loss 0.360983    Objective Loss 0.360983                                        LR 0.000060    Time 0.018032    
2023-01-06 16:12:43,360 - Epoch: [34][  246/  246]    Overall Loss 0.361188    Objective Loss 0.361188    Top1 86.124402    LR 0.000060    Time 0.017901    
2023-01-06 16:12:43,497 - --- validate (epoch=34)-----------
2023-01-06 16:12:43,497 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:43,935 - Epoch: [34][   10/   28]    Loss 0.355397    Top1 86.992188    
2023-01-06 16:12:44,048 - Epoch: [34][   20/   28]    Loss 0.361106    Top1 86.660156    
2023-01-06 16:12:44,108 - Epoch: [34][   28/   28]    Loss 0.353004    Top1 86.816490    
2023-01-06 16:12:44,246 - ==> Top1: 86.816    Loss: 0.353

2023-01-06 16:12:44,247 - ==> Confusion:
[[ 140    4  295]
 [   6   93  503]
 [  75   38 5832]]

2023-01-06 16:12:44,248 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:44,248 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:44,253 - 

2023-01-06 16:12:44,253 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:44,768 - Epoch: [35][   10/  246]    Overall Loss 0.346669    Objective Loss 0.346669                                        LR 0.000060    Time 0.051475    
2023-01-06 16:12:44,909 - Epoch: [35][   20/  246]    Overall Loss 0.356911    Objective Loss 0.356911                                        LR 0.000060    Time 0.032680    
2023-01-06 16:12:45,053 - Epoch: [35][   30/  246]    Overall Loss 0.358522    Objective Loss 0.358522                                        LR 0.000060    Time 0.026591    
2023-01-06 16:12:45,215 - Epoch: [35][   40/  246]    Overall Loss 0.357286    Objective Loss 0.357286                                        LR 0.000060    Time 0.023994    
2023-01-06 16:12:45,375 - Epoch: [35][   50/  246]    Overall Loss 0.361871    Objective Loss 0.361871                                        LR 0.000060    Time 0.022381    
2023-01-06 16:12:45,535 - Epoch: [35][   60/  246]    Overall Loss 0.366787    Objective Loss 0.366787                                        LR 0.000060    Time 0.021314    
2023-01-06 16:12:45,697 - Epoch: [35][   70/  246]    Overall Loss 0.367071    Objective Loss 0.367071                                        LR 0.000060    Time 0.020573    
2023-01-06 16:12:45,857 - Epoch: [35][   80/  246]    Overall Loss 0.364464    Objective Loss 0.364464                                        LR 0.000060    Time 0.020000    
2023-01-06 16:12:46,018 - Epoch: [35][   90/  246]    Overall Loss 0.364072    Objective Loss 0.364072                                        LR 0.000060    Time 0.019560    
2023-01-06 16:12:46,178 - Epoch: [35][  100/  246]    Overall Loss 0.363378    Objective Loss 0.363378                                        LR 0.000060    Time 0.019204    
2023-01-06 16:12:46,343 - Epoch: [35][  110/  246]    Overall Loss 0.363560    Objective Loss 0.363560                                        LR 0.000060    Time 0.018951    
2023-01-06 16:12:46,502 - Epoch: [35][  120/  246]    Overall Loss 0.363985    Objective Loss 0.363985                                        LR 0.000060    Time 0.018700    
2023-01-06 16:12:46,675 - Epoch: [35][  130/  246]    Overall Loss 0.363254    Objective Loss 0.363254                                        LR 0.000060    Time 0.018585    
2023-01-06 16:12:46,853 - Epoch: [35][  140/  246]    Overall Loss 0.362293    Objective Loss 0.362293                                        LR 0.000060    Time 0.018521    
2023-01-06 16:12:47,037 - Epoch: [35][  150/  246]    Overall Loss 0.361147    Objective Loss 0.361147                                        LR 0.000060    Time 0.018510    
2023-01-06 16:12:47,212 - Epoch: [35][  160/  246]    Overall Loss 0.361043    Objective Loss 0.361043                                        LR 0.000060    Time 0.018434    
2023-01-06 16:12:47,401 - Epoch: [35][  170/  246]    Overall Loss 0.360052    Objective Loss 0.360052                                        LR 0.000060    Time 0.018461    
2023-01-06 16:12:47,588 - Epoch: [35][  180/  246]    Overall Loss 0.359029    Objective Loss 0.359029                                        LR 0.000060    Time 0.018471    
2023-01-06 16:12:47,779 - Epoch: [35][  190/  246]    Overall Loss 0.357279    Objective Loss 0.357279                                        LR 0.000060    Time 0.018501    
2023-01-06 16:12:47,963 - Epoch: [35][  200/  246]    Overall Loss 0.357417    Objective Loss 0.357417                                        LR 0.000060    Time 0.018496    
2023-01-06 16:12:48,150 - Epoch: [35][  210/  246]    Overall Loss 0.358645    Objective Loss 0.358645                                        LR 0.000060    Time 0.018504    
2023-01-06 16:12:48,354 - Epoch: [35][  220/  246]    Overall Loss 0.358647    Objective Loss 0.358647                                        LR 0.000060    Time 0.018585    
2023-01-06 16:12:48,555 - Epoch: [35][  230/  246]    Overall Loss 0.357881    Objective Loss 0.357881                                        LR 0.000060    Time 0.018651    
2023-01-06 16:12:48,758 - Epoch: [35][  240/  246]    Overall Loss 0.358916    Objective Loss 0.358916                                        LR 0.000060    Time 0.018718    
2023-01-06 16:12:48,837 - Epoch: [35][  246/  246]    Overall Loss 0.358668    Objective Loss 0.358668    Top1 86.124402    LR 0.000060    Time 0.018580    
2023-01-06 16:12:48,970 - --- validate (epoch=35)-----------
2023-01-06 16:12:48,970 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:49,391 - Epoch: [35][   10/   28]    Loss 0.340925    Top1 87.539062    
2023-01-06 16:12:49,493 - Epoch: [35][   20/   28]    Loss 0.349901    Top1 87.148438    
2023-01-06 16:12:49,553 - Epoch: [35][   28/   28]    Loss 0.352075    Top1 86.931005    
2023-01-06 16:12:49,691 - ==> Top1: 86.931    Loss: 0.352

2023-01-06 16:12:49,691 - ==> Confusion:
[[ 137    4  298]
 [   6   98  498]
 [  69   38 5838]]

2023-01-06 16:12:49,692 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:49,692 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:49,696 - 

2023-01-06 16:12:49,697 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:50,371 - Epoch: [36][   10/  246]    Overall Loss 0.394120    Objective Loss 0.394120                                        LR 0.000060    Time 0.067413    
2023-01-06 16:12:50,517 - Epoch: [36][   20/  246]    Overall Loss 0.377569    Objective Loss 0.377569                                        LR 0.000060    Time 0.040981    
2023-01-06 16:12:50,671 - Epoch: [36][   30/  246]    Overall Loss 0.372898    Objective Loss 0.372898                                        LR 0.000060    Time 0.032446    
2023-01-06 16:12:50,835 - Epoch: [36][   40/  246]    Overall Loss 0.369543    Objective Loss 0.369543                                        LR 0.000060    Time 0.028417    
2023-01-06 16:12:51,001 - Epoch: [36][   50/  246]    Overall Loss 0.366892    Objective Loss 0.366892                                        LR 0.000060    Time 0.026037    
2023-01-06 16:12:51,161 - Epoch: [36][   60/  246]    Overall Loss 0.362800    Objective Loss 0.362800                                        LR 0.000060    Time 0.024369    
2023-01-06 16:12:51,326 - Epoch: [36][   70/  246]    Overall Loss 0.361856    Objective Loss 0.361856                                        LR 0.000060    Time 0.023232    
2023-01-06 16:12:51,485 - Epoch: [36][   80/  246]    Overall Loss 0.360119    Objective Loss 0.360119                                        LR 0.000060    Time 0.022317    
2023-01-06 16:12:51,645 - Epoch: [36][   90/  246]    Overall Loss 0.361950    Objective Loss 0.361950                                        LR 0.000060    Time 0.021607    
2023-01-06 16:12:51,802 - Epoch: [36][  100/  246]    Overall Loss 0.360206    Objective Loss 0.360206                                        LR 0.000060    Time 0.021011    
2023-01-06 16:12:51,965 - Epoch: [36][  110/  246]    Overall Loss 0.359449    Objective Loss 0.359449                                        LR 0.000060    Time 0.020585    
2023-01-06 16:12:52,132 - Epoch: [36][  120/  246]    Overall Loss 0.358423    Objective Loss 0.358423                                        LR 0.000060    Time 0.020254    
2023-01-06 16:12:52,300 - Epoch: [36][  130/  246]    Overall Loss 0.357786    Objective Loss 0.357786                                        LR 0.000060    Time 0.019984    
2023-01-06 16:12:52,466 - Epoch: [36][  140/  246]    Overall Loss 0.357202    Objective Loss 0.357202                                        LR 0.000060    Time 0.019744    
2023-01-06 16:12:52,627 - Epoch: [36][  150/  246]    Overall Loss 0.358300    Objective Loss 0.358300                                        LR 0.000060    Time 0.019498    
2023-01-06 16:12:52,786 - Epoch: [36][  160/  246]    Overall Loss 0.357269    Objective Loss 0.357269                                        LR 0.000060    Time 0.019270    
2023-01-06 16:12:52,951 - Epoch: [36][  170/  246]    Overall Loss 0.357477    Objective Loss 0.357477                                        LR 0.000060    Time 0.019105    
2023-01-06 16:12:53,114 - Epoch: [36][  180/  246]    Overall Loss 0.358679    Objective Loss 0.358679                                        LR 0.000060    Time 0.018949    
2023-01-06 16:12:53,284 - Epoch: [36][  190/  246]    Overall Loss 0.358097    Objective Loss 0.358097                                        LR 0.000060    Time 0.018840    
2023-01-06 16:12:53,448 - Epoch: [36][  200/  246]    Overall Loss 0.358308    Objective Loss 0.358308                                        LR 0.000060    Time 0.018717    
2023-01-06 16:12:53,601 - Epoch: [36][  210/  246]    Overall Loss 0.358409    Objective Loss 0.358409                                        LR 0.000060    Time 0.018556    
2023-01-06 16:12:53,769 - Epoch: [36][  220/  246]    Overall Loss 0.358765    Objective Loss 0.358765                                        LR 0.000060    Time 0.018473    
2023-01-06 16:12:53,935 - Epoch: [36][  230/  246]    Overall Loss 0.357881    Objective Loss 0.357881                                        LR 0.000060    Time 0.018389    
2023-01-06 16:12:54,115 - Epoch: [36][  240/  246]    Overall Loss 0.358139    Objective Loss 0.358139                                        LR 0.000060    Time 0.018371    
2023-01-06 16:12:54,194 - Epoch: [36][  246/  246]    Overall Loss 0.357661    Objective Loss 0.357661    Top1 86.842105    LR 0.000060    Time 0.018245    
2023-01-06 16:12:54,344 - --- validate (epoch=36)-----------
2023-01-06 16:12:54,345 - 6986 samples (256 per mini-batch)
2023-01-06 16:12:54,782 - Epoch: [36][   10/   28]    Loss 0.367454    Top1 87.070312    
2023-01-06 16:12:54,897 - Epoch: [36][   20/   28]    Loss 0.354660    Top1 86.933594    
2023-01-06 16:12:54,954 - Epoch: [36][   28/   28]    Loss 0.359236    Top1 86.959634    
2023-01-06 16:12:55,094 - ==> Top1: 86.960    Loss: 0.359

2023-01-06 16:12:55,094 - ==> Confusion:
[[ 165    3  271]
 [   8  102  492]
 [  92   45 5808]]

2023-01-06 16:12:55,096 - ==> Best [Top1: 87.031   Sparsity:0.00   Params: 46192 on epoch: 31]
2023-01-06 16:12:55,096 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:12:55,100 - 

2023-01-06 16:12:55,100 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:12:55,627 - Epoch: [37][   10/  246]    Overall Loss 0.370351    Objective Loss 0.370351                                        LR 0.000060    Time 0.052651    
2023-01-06 16:12:55,781 - Epoch: [37][   20/  246]    Overall Loss 0.362024    Objective Loss 0.362024                                        LR 0.000060    Time 0.033989    
2023-01-06 16:12:55,956 - Epoch: [37][   30/  246]    Overall Loss 0.359804    Objective Loss 0.359804                                        LR 0.000060    Time 0.028472    
2023-01-06 16:12:56,119 - Epoch: [37][   40/  246]    Overall Loss 0.360213    Objective Loss 0.360213                                        LR 0.000060    Time 0.025417    
2023-01-06 16:12:56,281 - Epoch: [37][   50/  246]    Overall Loss 0.362281    Objective Loss 0.362281                                        LR 0.000060    Time 0.023578    
2023-01-06 16:12:56,443 - Epoch: [37][   60/  246]    Overall Loss 0.361176    Objective Loss 0.361176                                        LR 0.000060    Time 0.022337    
2023-01-06 16:12:56,613 - Epoch: [37][   70/  246]    Overall Loss 0.360754    Objective Loss 0.360754                                        LR 0.000060    Time 0.021572    
2023-01-06 16:12:56,778 - Epoch: [37][   80/  246]    Overall Loss 0.355521    Objective Loss 0.355521                                        LR 0.000060    Time 0.020939    
2023-01-06 16:12:56,951 - Epoch: [37][   90/  246]    Overall Loss 0.357441    Objective Loss 0.357441                                        LR 0.000060    Time 0.020523    
2023-01-06 16:12:57,119 - Epoch: [37][  100/  246]    Overall Loss 0.357045    Objective Loss 0.357045                                        LR 0.000060    Time 0.020152    
2023-01-06 16:12:57,285 - Epoch: [37][  110/  246]    Overall Loss 0.355730    Objective Loss 0.355730                                        LR 0.000060    Time 0.019826    
2023-01-06 16:12:57,453 - Epoch: [37][  120/  246]    Overall Loss 0.354936    Objective Loss 0.354936                                        LR 0.000060    Time 0.019569    
2023-01-06 16:12:57,618 - Epoch: [37][  130/  246]    Overall Loss 0.354097    Objective Loss 0.354097                                        LR 0.000060    Time 0.019329    
2023-01-06 16:12:57,786 - Epoch: [37][  140/  246]    Overall Loss 0.354117    Objective Loss 0.354117                                        LR 0.000060    Time 0.019146    
2023-01-06 16:12:57,952 - Epoch: [37][  150/  246]    Overall Loss 0.355101    Objective Loss 0.355101                                        LR 0.000060    Time 0.018975    
2023-01-06 16:12:58,120 - Epoch: [37][  160/  246]    Overall Loss 0.356775    Objective Loss 0.356775                                        LR 0.000060    Time 0.018835    
2023-01-06 16:12:58,282 - Epoch: [37][  170/  246]    Overall Loss 0.356324    Objective Loss 0.356324                                        LR 0.000060    Time 0.018679    
2023-01-06 16:12:58,452 - Epoch: [37][  180/  246]    Overall Loss 0.356348    Objective Loss 0.356348                                        LR 0.000060    Time 0.018585    
2023-01-06 16:12:58,616 - Epoch: [37][  190/  246]    Overall Loss 0.357050    Objective Loss 0.357050                                        LR 0.000060    Time 0.018467    
2023-01-06 16:12:58,785 - Epoch: [37][  200/  246]    Overall Loss 0.356429    Objective Loss 0.356429                                        LR 0.000060    Time 0.018386    
2023-01-06 16:12:58,952 - Epoch: [37][  210/  246]    Overall Loss 0.355784    Objective Loss 0.355784                                        LR 0.000060    Time 0.018306    
2023-01-06 16:12:59,119 - Epoch: [37][  220/  246]    Overall Loss 0.355935    Objective Loss 0.355935                                        LR 0.000060    Time 0.018231    
2023-01-06 16:12:59,286 - Epoch: [37][  230/  246]    Overall Loss 0.356075    Objective Loss 0.356075                                        LR 0.000060    Time 0.018161    
2023-01-06 16:12:59,461 - Epoch: [37][  240/  246]    Overall Loss 0.356288    Objective Loss 0.356288                                        LR 0.000060    Time 0.018133    
2023-01-06 16:12:59,539 - Epoch: [37][  246/  246]    Overall Loss 0.356330    Objective Loss 0.356330    Top1 86.842105    LR 0.000060    Time 0.018009    
2023-01-06 16:12:59,667 - --- validate (epoch=37)-----------
2023-01-06 16:12:59,668 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:00,096 - Epoch: [37][   10/   28]    Loss 0.358849    Top1 86.992188    
2023-01-06 16:13:00,200 - Epoch: [37][   20/   28]    Loss 0.349707    Top1 87.207031    
2023-01-06 16:13:00,258 - Epoch: [37][   28/   28]    Loss 0.352594    Top1 87.174349    
2023-01-06 16:13:00,380 - ==> Top1: 87.174    Loss: 0.353

2023-01-06 16:13:00,381 - ==> Confusion:
[[ 164    2  273]
 [  12  111  479]
 [  83   47 5815]]

2023-01-06 16:13:00,382 - ==> Best [Top1: 87.174   Sparsity:0.00   Params: 46192 on epoch: 37]
2023-01-06 16:13:00,382 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:00,386 - 

2023-01-06 16:13:00,387 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:01,062 - Epoch: [38][   10/  246]    Overall Loss 0.352991    Objective Loss 0.352991                                        LR 0.000060    Time 0.067475    
2023-01-06 16:13:01,223 - Epoch: [38][   20/  246]    Overall Loss 0.363866    Objective Loss 0.363866                                        LR 0.000060    Time 0.041736    
2023-01-06 16:13:01,394 - Epoch: [38][   30/  246]    Overall Loss 0.357823    Objective Loss 0.357823                                        LR 0.000060    Time 0.033515    
2023-01-06 16:13:01,570 - Epoch: [38][   40/  246]    Overall Loss 0.356253    Objective Loss 0.356253                                        LR 0.000060    Time 0.029542    
2023-01-06 16:13:01,749 - Epoch: [38][   50/  246]    Overall Loss 0.355485    Objective Loss 0.355485                                        LR 0.000060    Time 0.027200    
2023-01-06 16:13:01,954 - Epoch: [38][   60/  246]    Overall Loss 0.354968    Objective Loss 0.354968                                        LR 0.000060    Time 0.026080    
2023-01-06 16:13:02,154 - Epoch: [38][   70/  246]    Overall Loss 0.353961    Objective Loss 0.353961                                        LR 0.000060    Time 0.025201    
2023-01-06 16:13:02,351 - Epoch: [38][   80/  246]    Overall Loss 0.354813    Objective Loss 0.354813                                        LR 0.000060    Time 0.024517    
2023-01-06 16:13:02,550 - Epoch: [38][   90/  246]    Overall Loss 0.354726    Objective Loss 0.354726                                        LR 0.000060    Time 0.024000    
2023-01-06 16:13:02,747 - Epoch: [38][  100/  246]    Overall Loss 0.353945    Objective Loss 0.353945                                        LR 0.000060    Time 0.023559    
2023-01-06 16:13:02,942 - Epoch: [38][  110/  246]    Overall Loss 0.352768    Objective Loss 0.352768                                        LR 0.000060    Time 0.023191    
2023-01-06 16:13:03,146 - Epoch: [38][  120/  246]    Overall Loss 0.352620    Objective Loss 0.352620                                        LR 0.000060    Time 0.022950    
2023-01-06 16:13:03,347 - Epoch: [38][  130/  246]    Overall Loss 0.351411    Objective Loss 0.351411                                        LR 0.000060    Time 0.022731    
2023-01-06 16:13:03,549 - Epoch: [38][  140/  246]    Overall Loss 0.351837    Objective Loss 0.351837                                        LR 0.000060    Time 0.022549    
2023-01-06 16:13:03,750 - Epoch: [38][  150/  246]    Overall Loss 0.351259    Objective Loss 0.351259                                        LR 0.000060    Time 0.022378    
2023-01-06 16:13:03,952 - Epoch: [38][  160/  246]    Overall Loss 0.350193    Objective Loss 0.350193                                        LR 0.000060    Time 0.022243    
2023-01-06 16:13:04,125 - Epoch: [38][  170/  246]    Overall Loss 0.349567    Objective Loss 0.349567                                        LR 0.000060    Time 0.021948    
2023-01-06 16:13:04,280 - Epoch: [38][  180/  246]    Overall Loss 0.351291    Objective Loss 0.351291                                        LR 0.000060    Time 0.021589    
2023-01-06 16:13:04,434 - Epoch: [38][  190/  246]    Overall Loss 0.350753    Objective Loss 0.350753                                        LR 0.000060    Time 0.021261    
2023-01-06 16:13:04,585 - Epoch: [38][  200/  246]    Overall Loss 0.351358    Objective Loss 0.351358                                        LR 0.000060    Time 0.020948    
2023-01-06 16:13:04,733 - Epoch: [38][  210/  246]    Overall Loss 0.350897    Objective Loss 0.350897                                        LR 0.000060    Time 0.020655    
2023-01-06 16:13:04,882 - Epoch: [38][  220/  246]    Overall Loss 0.352001    Objective Loss 0.352001                                        LR 0.000060    Time 0.020392    
2023-01-06 16:13:05,038 - Epoch: [38][  230/  246]    Overall Loss 0.352664    Objective Loss 0.352664                                        LR 0.000060    Time 0.020183    
2023-01-06 16:13:05,199 - Epoch: [38][  240/  246]    Overall Loss 0.353732    Objective Loss 0.353732                                        LR 0.000060    Time 0.020009    
2023-01-06 16:13:05,271 - Epoch: [38][  246/  246]    Overall Loss 0.353799    Objective Loss 0.353799    Top1 87.559809    LR 0.000060    Time 0.019815    
2023-01-06 16:13:05,411 - --- validate (epoch=38)-----------
2023-01-06 16:13:05,412 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:05,840 - Epoch: [38][   10/   28]    Loss 0.344297    Top1 87.070312    
2023-01-06 16:13:05,941 - Epoch: [38][   20/   28]    Loss 0.357567    Top1 86.718750    
2023-01-06 16:13:05,998 - Epoch: [38][   28/   28]    Loss 0.356583    Top1 86.787861    
2023-01-06 16:13:06,158 - ==> Top1: 86.788    Loss: 0.357

2023-01-06 16:13:06,158 - ==> Confusion:
[[  93    2  344]
 [   2   75  525]
 [  29   21 5895]]

2023-01-06 16:13:06,159 - ==> Best [Top1: 87.174   Sparsity:0.00   Params: 46192 on epoch: 37]
2023-01-06 16:13:06,159 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:06,164 - 

2023-01-06 16:13:06,164 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:06,855 - Epoch: [39][   10/  246]    Overall Loss 0.349266    Objective Loss 0.349266                                        LR 0.000060    Time 0.069068    
2023-01-06 16:13:07,024 - Epoch: [39][   20/  246]    Overall Loss 0.352046    Objective Loss 0.352046                                        LR 0.000060    Time 0.042929    
2023-01-06 16:13:07,192 - Epoch: [39][   30/  246]    Overall Loss 0.359434    Objective Loss 0.359434                                        LR 0.000060    Time 0.034203    
2023-01-06 16:13:07,364 - Epoch: [39][   40/  246]    Overall Loss 0.356664    Objective Loss 0.356664                                        LR 0.000060    Time 0.029918    
2023-01-06 16:13:07,532 - Epoch: [39][   50/  246]    Overall Loss 0.354833    Objective Loss 0.354833                                        LR 0.000060    Time 0.027282    
2023-01-06 16:13:07,704 - Epoch: [39][   60/  246]    Overall Loss 0.358041    Objective Loss 0.358041                                        LR 0.000060    Time 0.025593    
2023-01-06 16:13:07,889 - Epoch: [39][   70/  246]    Overall Loss 0.356905    Objective Loss 0.356905                                        LR 0.000060    Time 0.024576    
2023-01-06 16:13:08,055 - Epoch: [39][   80/  246]    Overall Loss 0.359686    Objective Loss 0.359686                                        LR 0.000060    Time 0.023570    
2023-01-06 16:13:08,226 - Epoch: [39][   90/  246]    Overall Loss 0.359332    Objective Loss 0.359332                                        LR 0.000060    Time 0.022847    
2023-01-06 16:13:08,389 - Epoch: [39][  100/  246]    Overall Loss 0.357131    Objective Loss 0.357131                                        LR 0.000060    Time 0.022189    
2023-01-06 16:13:08,549 - Epoch: [39][  110/  246]    Overall Loss 0.356991    Objective Loss 0.356991                                        LR 0.000060    Time 0.021620    
2023-01-06 16:13:08,701 - Epoch: [39][  120/  246]    Overall Loss 0.356009    Objective Loss 0.356009                                        LR 0.000060    Time 0.021083    
2023-01-06 16:13:08,864 - Epoch: [39][  130/  246]    Overall Loss 0.355900    Objective Loss 0.355900                                        LR 0.000060    Time 0.020714    
2023-01-06 16:13:09,034 - Epoch: [39][  140/  246]    Overall Loss 0.355596    Objective Loss 0.355596                                        LR 0.000060    Time 0.020441    
2023-01-06 16:13:09,199 - Epoch: [39][  150/  246]    Overall Loss 0.355005    Objective Loss 0.355005                                        LR 0.000060    Time 0.020180    
2023-01-06 16:13:09,365 - Epoch: [39][  160/  246]    Overall Loss 0.354294    Objective Loss 0.354294                                        LR 0.000060    Time 0.019954    
2023-01-06 16:13:09,532 - Epoch: [39][  170/  246]    Overall Loss 0.353698    Objective Loss 0.353698                                        LR 0.000060    Time 0.019757    
2023-01-06 16:13:09,699 - Epoch: [39][  180/  246]    Overall Loss 0.354921    Objective Loss 0.354921                                        LR 0.000060    Time 0.019586    
2023-01-06 16:13:09,867 - Epoch: [39][  190/  246]    Overall Loss 0.353444    Objective Loss 0.353444                                        LR 0.000060    Time 0.019439    
2023-01-06 16:13:10,035 - Epoch: [39][  200/  246]    Overall Loss 0.353182    Objective Loss 0.353182                                        LR 0.000060    Time 0.019305    
2023-01-06 16:13:10,203 - Epoch: [39][  210/  246]    Overall Loss 0.353705    Objective Loss 0.353705                                        LR 0.000060    Time 0.019184    
2023-01-06 16:13:10,370 - Epoch: [39][  220/  246]    Overall Loss 0.353332    Objective Loss 0.353332                                        LR 0.000060    Time 0.019068    
2023-01-06 16:13:10,535 - Epoch: [39][  230/  246]    Overall Loss 0.352937    Objective Loss 0.352937                                        LR 0.000060    Time 0.018957    
2023-01-06 16:13:10,715 - Epoch: [39][  240/  246]    Overall Loss 0.353435    Objective Loss 0.353435                                        LR 0.000060    Time 0.018914    
2023-01-06 16:13:10,793 - Epoch: [39][  246/  246]    Overall Loss 0.353014    Objective Loss 0.353014    Top1 86.842105    LR 0.000060    Time 0.018769    
2023-01-06 16:13:10,937 - --- validate (epoch=39)-----------
2023-01-06 16:13:10,937 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:11,363 - Epoch: [39][   10/   28]    Loss 0.344046    Top1 87.343750    
2023-01-06 16:13:11,465 - Epoch: [39][   20/   28]    Loss 0.347613    Top1 87.207031    
2023-01-06 16:13:11,522 - Epoch: [39][   28/   28]    Loss 0.343166    Top1 87.145720    
2023-01-06 16:13:11,677 - ==> Top1: 87.146    Loss: 0.343

2023-01-06 16:13:11,677 - ==> Confusion:
[[ 147    2  290]
 [   6  101  495]
 [  65   40 5840]]

2023-01-06 16:13:11,678 - ==> Best [Top1: 87.174   Sparsity:0.00   Params: 46192 on epoch: 37]
2023-01-06 16:13:11,678 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:11,683 - 

2023-01-06 16:13:11,683 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:12,203 - Epoch: [40][   10/  246]    Overall Loss 0.343255    Objective Loss 0.343255                                        LR 0.000036    Time 0.051974    
2023-01-06 16:13:12,351 - Epoch: [40][   20/  246]    Overall Loss 0.345354    Objective Loss 0.345354                                        LR 0.000036    Time 0.033330    
2023-01-06 16:13:12,506 - Epoch: [40][   30/  246]    Overall Loss 0.337939    Objective Loss 0.337939                                        LR 0.000036    Time 0.027383    
2023-01-06 16:13:12,674 - Epoch: [40][   40/  246]    Overall Loss 0.341643    Objective Loss 0.341643                                        LR 0.000036    Time 0.024724    
2023-01-06 16:13:12,847 - Epoch: [40][   50/  246]    Overall Loss 0.341567    Objective Loss 0.341567                                        LR 0.000036    Time 0.023241    
2023-01-06 16:13:13,017 - Epoch: [40][   60/  246]    Overall Loss 0.338393    Objective Loss 0.338393                                        LR 0.000036    Time 0.022196    
2023-01-06 16:13:13,189 - Epoch: [40][   70/  246]    Overall Loss 0.342150    Objective Loss 0.342150                                        LR 0.000036    Time 0.021478    
2023-01-06 16:13:13,362 - Epoch: [40][   80/  246]    Overall Loss 0.340617    Objective Loss 0.340617                                        LR 0.000036    Time 0.020942    
2023-01-06 16:13:13,536 - Epoch: [40][   90/  246]    Overall Loss 0.343569    Objective Loss 0.343569                                        LR 0.000036    Time 0.020546    
2023-01-06 16:13:13,707 - Epoch: [40][  100/  246]    Overall Loss 0.344449    Objective Loss 0.344449                                        LR 0.000036    Time 0.020200    
2023-01-06 16:13:13,881 - Epoch: [40][  110/  246]    Overall Loss 0.344597    Objective Loss 0.344597                                        LR 0.000036    Time 0.019942    
2023-01-06 16:13:14,070 - Epoch: [40][  120/  246]    Overall Loss 0.346752    Objective Loss 0.346752                                        LR 0.000036    Time 0.019856    
2023-01-06 16:13:14,263 - Epoch: [40][  130/  246]    Overall Loss 0.347316    Objective Loss 0.347316                                        LR 0.000036    Time 0.019805    
2023-01-06 16:13:14,460 - Epoch: [40][  140/  246]    Overall Loss 0.346596    Objective Loss 0.346596                                        LR 0.000036    Time 0.019797    
2023-01-06 16:13:14,657 - Epoch: [40][  150/  246]    Overall Loss 0.347040    Objective Loss 0.347040                                        LR 0.000036    Time 0.019790    
2023-01-06 16:13:14,855 - Epoch: [40][  160/  246]    Overall Loss 0.347349    Objective Loss 0.347349                                        LR 0.000036    Time 0.019783    
2023-01-06 16:13:15,054 - Epoch: [40][  170/  246]    Overall Loss 0.346808    Objective Loss 0.346808                                        LR 0.000036    Time 0.019787    
2023-01-06 16:13:15,247 - Epoch: [40][  180/  246]    Overall Loss 0.347915    Objective Loss 0.347915                                        LR 0.000036    Time 0.019760    
2023-01-06 16:13:15,444 - Epoch: [40][  190/  246]    Overall Loss 0.348343    Objective Loss 0.348343                                        LR 0.000036    Time 0.019757    
2023-01-06 16:13:15,642 - Epoch: [40][  200/  246]    Overall Loss 0.349070    Objective Loss 0.349070                                        LR 0.000036    Time 0.019757    
2023-01-06 16:13:15,837 - Epoch: [40][  210/  246]    Overall Loss 0.348384    Objective Loss 0.348384                                        LR 0.000036    Time 0.019739    
2023-01-06 16:13:16,024 - Epoch: [40][  220/  246]    Overall Loss 0.349170    Objective Loss 0.349170                                        LR 0.000036    Time 0.019694    
2023-01-06 16:13:16,216 - Epoch: [40][  230/  246]    Overall Loss 0.349904    Objective Loss 0.349904                                        LR 0.000036    Time 0.019671    
2023-01-06 16:13:16,416 - Epoch: [40][  240/  246]    Overall Loss 0.350458    Objective Loss 0.350458                                        LR 0.000036    Time 0.019680    
2023-01-06 16:13:16,497 - Epoch: [40][  246/  246]    Overall Loss 0.350297    Objective Loss 0.350297    Top1 88.038278    LR 0.000036    Time 0.019528    
2023-01-06 16:13:16,630 - --- validate (epoch=40)-----------
2023-01-06 16:13:16,630 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:17,080 - Epoch: [40][   10/   28]    Loss 0.332601    Top1 87.382812    
2023-01-06 16:13:17,185 - Epoch: [40][   20/   28]    Loss 0.340884    Top1 87.226562    
2023-01-06 16:13:17,245 - Epoch: [40][   28/   28]    Loss 0.342972    Top1 87.303178    
2023-01-06 16:13:17,381 - ==> Top1: 87.303    Loss: 0.343

2023-01-06 16:13:17,381 - ==> Confusion:
[[ 154    5  280]
 [  10  124  468]
 [  68   56 5821]]

2023-01-06 16:13:17,383 - ==> Best [Top1: 87.303   Sparsity:0.00   Params: 46192 on epoch: 40]
2023-01-06 16:13:17,383 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:17,387 - 

2023-01-06 16:13:17,388 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:18,068 - Epoch: [41][   10/  246]    Overall Loss 0.337661    Objective Loss 0.337661                                        LR 0.000036    Time 0.067978    
2023-01-06 16:13:18,227 - Epoch: [41][   20/  246]    Overall Loss 0.358162    Objective Loss 0.358162                                        LR 0.000036    Time 0.041943    
2023-01-06 16:13:18,390 - Epoch: [41][   30/  246]    Overall Loss 0.351918    Objective Loss 0.351918                                        LR 0.000036    Time 0.033362    
2023-01-06 16:13:18,549 - Epoch: [41][   40/  246]    Overall Loss 0.347885    Objective Loss 0.347885                                        LR 0.000036    Time 0.028984    
2023-01-06 16:13:18,705 - Epoch: [41][   50/  246]    Overall Loss 0.349478    Objective Loss 0.349478                                        LR 0.000036    Time 0.026308    
2023-01-06 16:13:18,865 - Epoch: [41][   60/  246]    Overall Loss 0.350464    Objective Loss 0.350464                                        LR 0.000036    Time 0.024583    
2023-01-06 16:13:19,027 - Epoch: [41][   70/  246]    Overall Loss 0.351879    Objective Loss 0.351879                                        LR 0.000036    Time 0.023379    
2023-01-06 16:13:19,193 - Epoch: [41][   80/  246]    Overall Loss 0.352933    Objective Loss 0.352933                                        LR 0.000036    Time 0.022527    
2023-01-06 16:13:19,364 - Epoch: [41][   90/  246]    Overall Loss 0.356173    Objective Loss 0.356173                                        LR 0.000036    Time 0.021920    
2023-01-06 16:13:19,535 - Epoch: [41][  100/  246]    Overall Loss 0.354121    Objective Loss 0.354121                                        LR 0.000036    Time 0.021438    
2023-01-06 16:13:19,706 - Epoch: [41][  110/  246]    Overall Loss 0.351087    Objective Loss 0.351087                                        LR 0.000036    Time 0.021036    
2023-01-06 16:13:19,877 - Epoch: [41][  120/  246]    Overall Loss 0.349760    Objective Loss 0.349760                                        LR 0.000036    Time 0.020702    
2023-01-06 16:13:20,047 - Epoch: [41][  130/  246]    Overall Loss 0.348919    Objective Loss 0.348919                                        LR 0.000036    Time 0.020419    
2023-01-06 16:13:20,189 - Epoch: [41][  140/  246]    Overall Loss 0.347166    Objective Loss 0.347166                                        LR 0.000036    Time 0.019970    
2023-01-06 16:13:20,328 - Epoch: [41][  150/  246]    Overall Loss 0.347409    Objective Loss 0.347409                                        LR 0.000036    Time 0.019564    
2023-01-06 16:13:20,492 - Epoch: [41][  160/  246]    Overall Loss 0.348342    Objective Loss 0.348342                                        LR 0.000036    Time 0.019364    
2023-01-06 16:13:20,670 - Epoch: [41][  170/  246]    Overall Loss 0.347687    Objective Loss 0.347687                                        LR 0.000036    Time 0.019268    
2023-01-06 16:13:20,839 - Epoch: [41][  180/  246]    Overall Loss 0.348980    Objective Loss 0.348980                                        LR 0.000036    Time 0.019132    
2023-01-06 16:13:21,001 - Epoch: [41][  190/  246]    Overall Loss 0.349340    Objective Loss 0.349340                                        LR 0.000036    Time 0.018980    
2023-01-06 16:13:21,158 - Epoch: [41][  200/  246]    Overall Loss 0.349227    Objective Loss 0.349227                                        LR 0.000036    Time 0.018811    
2023-01-06 16:13:21,300 - Epoch: [41][  210/  246]    Overall Loss 0.349197    Objective Loss 0.349197                                        LR 0.000036    Time 0.018589    
2023-01-06 16:13:21,439 - Epoch: [41][  220/  246]    Overall Loss 0.349785    Objective Loss 0.349785                                        LR 0.000036    Time 0.018373    
2023-01-06 16:13:21,580 - Epoch: [41][  230/  246]    Overall Loss 0.349187    Objective Loss 0.349187                                        LR 0.000036    Time 0.018186    
2023-01-06 16:13:21,731 - Epoch: [41][  240/  246]    Overall Loss 0.350050    Objective Loss 0.350050                                        LR 0.000036    Time 0.018059    
2023-01-06 16:13:21,799 - Epoch: [41][  246/  246]    Overall Loss 0.349483    Objective Loss 0.349483    Top1 89.712919    LR 0.000036    Time 0.017893    
2023-01-06 16:13:21,933 - --- validate (epoch=41)-----------
2023-01-06 16:13:21,933 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:22,384 - Epoch: [41][   10/   28]    Loss 0.335823    Top1 87.578125    
2023-01-06 16:13:22,483 - Epoch: [41][   20/   28]    Loss 0.344212    Top1 87.187500    
2023-01-06 16:13:22,539 - Epoch: [41][   28/   28]    Loss 0.344320    Top1 87.202977    
2023-01-06 16:13:22,683 - ==> Top1: 87.203    Loss: 0.344

2023-01-06 16:13:22,683 - ==> Confusion:
[[ 124    2  313]
 [   5  109  488]
 [  42   44 5859]]

2023-01-06 16:13:22,685 - ==> Best [Top1: 87.303   Sparsity:0.00   Params: 46192 on epoch: 40]
2023-01-06 16:13:22,685 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:22,689 - 

2023-01-06 16:13:22,689 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:23,225 - Epoch: [42][   10/  246]    Overall Loss 0.333614    Objective Loss 0.333614                                        LR 0.000036    Time 0.053479    
2023-01-06 16:13:23,388 - Epoch: [42][   20/  246]    Overall Loss 0.330157    Objective Loss 0.330157                                        LR 0.000036    Time 0.034894    
2023-01-06 16:13:23,568 - Epoch: [42][   30/  246]    Overall Loss 0.335312    Objective Loss 0.335312                                        LR 0.000036    Time 0.029252    
2023-01-06 16:13:23,736 - Epoch: [42][   40/  246]    Overall Loss 0.340872    Objective Loss 0.340872                                        LR 0.000036    Time 0.026115    
2023-01-06 16:13:23,916 - Epoch: [42][   50/  246]    Overall Loss 0.342889    Objective Loss 0.342889                                        LR 0.000036    Time 0.024495    
2023-01-06 16:13:24,091 - Epoch: [42][   60/  246]    Overall Loss 0.345027    Objective Loss 0.345027                                        LR 0.000036    Time 0.023319    
2023-01-06 16:13:24,268 - Epoch: [42][   70/  246]    Overall Loss 0.346325    Objective Loss 0.346325                                        LR 0.000036    Time 0.022491    
2023-01-06 16:13:24,438 - Epoch: [42][   80/  246]    Overall Loss 0.345481    Objective Loss 0.345481                                        LR 0.000036    Time 0.021773    
2023-01-06 16:13:24,611 - Epoch: [42][   90/  246]    Overall Loss 0.349371    Objective Loss 0.349371                                        LR 0.000036    Time 0.021261    
2023-01-06 16:13:24,783 - Epoch: [42][  100/  246]    Overall Loss 0.350260    Objective Loss 0.350260                                        LR 0.000036    Time 0.020846    
2023-01-06 16:13:24,956 - Epoch: [42][  110/  246]    Overall Loss 0.348999    Objective Loss 0.348999                                        LR 0.000036    Time 0.020520    
2023-01-06 16:13:25,127 - Epoch: [42][  120/  246]    Overall Loss 0.348919    Objective Loss 0.348919                                        LR 0.000036    Time 0.020218    
2023-01-06 16:13:25,297 - Epoch: [42][  130/  246]    Overall Loss 0.350059    Objective Loss 0.350059                                        LR 0.000036    Time 0.019970    
2023-01-06 16:13:25,468 - Epoch: [42][  140/  246]    Overall Loss 0.350289    Objective Loss 0.350289                                        LR 0.000036    Time 0.019752    
2023-01-06 16:13:25,643 - Epoch: [42][  150/  246]    Overall Loss 0.351242    Objective Loss 0.351242                                        LR 0.000036    Time 0.019601    
2023-01-06 16:13:25,815 - Epoch: [42][  160/  246]    Overall Loss 0.350228    Objective Loss 0.350228                                        LR 0.000036    Time 0.019445    
2023-01-06 16:13:25,991 - Epoch: [42][  170/  246]    Overall Loss 0.348835    Objective Loss 0.348835                                        LR 0.000036    Time 0.019333    
2023-01-06 16:13:26,162 - Epoch: [42][  180/  246]    Overall Loss 0.348828    Objective Loss 0.348828                                        LR 0.000036    Time 0.019196    
2023-01-06 16:13:26,338 - Epoch: [42][  190/  246]    Overall Loss 0.348029    Objective Loss 0.348029                                        LR 0.000036    Time 0.019111    
2023-01-06 16:13:26,509 - Epoch: [42][  200/  246]    Overall Loss 0.348143    Objective Loss 0.348143                                        LR 0.000036    Time 0.019001    
2023-01-06 16:13:26,685 - Epoch: [42][  210/  246]    Overall Loss 0.348794    Objective Loss 0.348794                                        LR 0.000036    Time 0.018932    
2023-01-06 16:13:26,855 - Epoch: [42][  220/  246]    Overall Loss 0.348912    Objective Loss 0.348912                                        LR 0.000036    Time 0.018837    
2023-01-06 16:13:27,032 - Epoch: [42][  230/  246]    Overall Loss 0.348543    Objective Loss 0.348543                                        LR 0.000036    Time 0.018786    
2023-01-06 16:13:27,213 - Epoch: [42][  240/  246]    Overall Loss 0.347631    Objective Loss 0.347631                                        LR 0.000036    Time 0.018749    
2023-01-06 16:13:27,297 - Epoch: [42][  246/  246]    Overall Loss 0.348513    Objective Loss 0.348513    Top1 85.406699    LR 0.000036    Time 0.018629    
2023-01-06 16:13:27,458 - --- validate (epoch=42)-----------
2023-01-06 16:13:27,458 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:27,906 - Epoch: [42][   10/   28]    Loss 0.344527    Top1 87.265625    
2023-01-06 16:13:28,037 - Epoch: [42][   20/   28]    Loss 0.341622    Top1 87.246094    
2023-01-06 16:13:28,095 - Epoch: [42][   28/   28]    Loss 0.344491    Top1 87.346121    
2023-01-06 16:13:28,256 - ==> Top1: 87.346    Loss: 0.344

2023-01-06 16:13:28,256 - ==> Confusion:
[[ 157    6  276]
 [   9  137  456]
 [  71   66 5808]]

2023-01-06 16:13:28,257 - ==> Best [Top1: 87.346   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:13:28,257 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:28,262 - 

2023-01-06 16:13:28,262 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:28,947 - Epoch: [43][   10/  246]    Overall Loss 0.356433    Objective Loss 0.356433                                        LR 0.000036    Time 0.068385    
2023-01-06 16:13:29,107 - Epoch: [43][   20/  246]    Overall Loss 0.340394    Objective Loss 0.340394                                        LR 0.000036    Time 0.042173    
2023-01-06 16:13:29,253 - Epoch: [43][   30/  246]    Overall Loss 0.344634    Objective Loss 0.344634                                        LR 0.000036    Time 0.032965    
2023-01-06 16:13:29,404 - Epoch: [43][   40/  246]    Overall Loss 0.343358    Objective Loss 0.343358                                        LR 0.000036    Time 0.028496    
2023-01-06 16:13:29,567 - Epoch: [43][   50/  246]    Overall Loss 0.342966    Objective Loss 0.342966                                        LR 0.000036    Time 0.026015    
2023-01-06 16:13:29,734 - Epoch: [43][   60/  246]    Overall Loss 0.341682    Objective Loss 0.341682                                        LR 0.000036    Time 0.024452    
2023-01-06 16:13:29,886 - Epoch: [43][   70/  246]    Overall Loss 0.341325    Objective Loss 0.341325                                        LR 0.000036    Time 0.023131    
2023-01-06 16:13:30,049 - Epoch: [43][   80/  246]    Overall Loss 0.342530    Objective Loss 0.342530                                        LR 0.000036    Time 0.022272    
2023-01-06 16:13:30,207 - Epoch: [43][   90/  246]    Overall Loss 0.343849    Objective Loss 0.343849                                        LR 0.000036    Time 0.021539    
2023-01-06 16:13:30,380 - Epoch: [43][  100/  246]    Overall Loss 0.345502    Objective Loss 0.345502                                        LR 0.000036    Time 0.021116    
2023-01-06 16:13:30,547 - Epoch: [43][  110/  246]    Overall Loss 0.345602    Objective Loss 0.345602                                        LR 0.000036    Time 0.020708    
2023-01-06 16:13:30,720 - Epoch: [43][  120/  246]    Overall Loss 0.345915    Objective Loss 0.345915                                        LR 0.000036    Time 0.020425    
2023-01-06 16:13:30,892 - Epoch: [43][  130/  246]    Overall Loss 0.345844    Objective Loss 0.345844                                        LR 0.000036    Time 0.020170    
2023-01-06 16:13:31,075 - Epoch: [43][  140/  246]    Overall Loss 0.346532    Objective Loss 0.346532                                        LR 0.000036    Time 0.020032    
2023-01-06 16:13:31,250 - Epoch: [43][  150/  246]    Overall Loss 0.346367    Objective Loss 0.346367                                        LR 0.000036    Time 0.019862    
2023-01-06 16:13:31,433 - Epoch: [43][  160/  246]    Overall Loss 0.345641    Objective Loss 0.345641                                        LR 0.000036    Time 0.019761    
2023-01-06 16:13:31,609 - Epoch: [43][  170/  246]    Overall Loss 0.347555    Objective Loss 0.347555                                        LR 0.000036    Time 0.019629    
2023-01-06 16:13:31,789 - Epoch: [43][  180/  246]    Overall Loss 0.348577    Objective Loss 0.348577                                        LR 0.000036    Time 0.019538    
2023-01-06 16:13:31,958 - Epoch: [43][  190/  246]    Overall Loss 0.348666    Objective Loss 0.348666                                        LR 0.000036    Time 0.019391    
2023-01-06 16:13:32,128 - Epoch: [43][  200/  246]    Overall Loss 0.348178    Objective Loss 0.348178                                        LR 0.000036    Time 0.019266    
2023-01-06 16:13:32,300 - Epoch: [43][  210/  246]    Overall Loss 0.348447    Objective Loss 0.348447                                        LR 0.000036    Time 0.019167    
2023-01-06 16:13:32,468 - Epoch: [43][  220/  246]    Overall Loss 0.348648    Objective Loss 0.348648                                        LR 0.000036    Time 0.019059    
2023-01-06 16:13:32,638 - Epoch: [43][  230/  246]    Overall Loss 0.349489    Objective Loss 0.349489                                        LR 0.000036    Time 0.018967    
2023-01-06 16:13:32,815 - Epoch: [43][  240/  246]    Overall Loss 0.348050    Objective Loss 0.348050                                        LR 0.000036    Time 0.018914    
2023-01-06 16:13:32,898 - Epoch: [43][  246/  246]    Overall Loss 0.347535    Objective Loss 0.347535    Top1 87.320574    LR 0.000036    Time 0.018787    
2023-01-06 16:13:33,039 - --- validate (epoch=43)-----------
2023-01-06 16:13:33,040 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:33,493 - Epoch: [43][   10/   28]    Loss 0.364911    Top1 86.562500    
2023-01-06 16:13:33,603 - Epoch: [43][   20/   28]    Loss 0.347001    Top1 87.402344    
2023-01-06 16:13:33,662 - Epoch: [43][   28/   28]    Loss 0.346301    Top1 87.202977    
2023-01-06 16:13:33,804 - ==> Top1: 87.203    Loss: 0.346

2023-01-06 16:13:33,805 - ==> Confusion:
[[ 140    3  296]
 [   7  106  489]
 [  56   43 5846]]

2023-01-06 16:13:33,806 - ==> Best [Top1: 87.346   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:13:33,806 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:33,811 - 

2023-01-06 16:13:33,811 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:34,496 - Epoch: [44][   10/  246]    Overall Loss 0.350100    Objective Loss 0.350100                                        LR 0.000036    Time 0.068495    
2023-01-06 16:13:34,656 - Epoch: [44][   20/  246]    Overall Loss 0.349763    Objective Loss 0.349763                                        LR 0.000036    Time 0.042197    
2023-01-06 16:13:34,811 - Epoch: [44][   30/  246]    Overall Loss 0.355053    Objective Loss 0.355053                                        LR 0.000036    Time 0.033291    
2023-01-06 16:13:34,967 - Epoch: [44][   40/  246]    Overall Loss 0.354146    Objective Loss 0.354146                                        LR 0.000036    Time 0.028872    
2023-01-06 16:13:35,139 - Epoch: [44][   50/  246]    Overall Loss 0.354441    Objective Loss 0.354441                                        LR 0.000036    Time 0.026513    
2023-01-06 16:13:35,299 - Epoch: [44][   60/  246]    Overall Loss 0.353498    Objective Loss 0.353498                                        LR 0.000036    Time 0.024754    
2023-01-06 16:13:35,453 - Epoch: [44][   70/  246]    Overall Loss 0.352261    Objective Loss 0.352261                                        LR 0.000036    Time 0.023416    
2023-01-06 16:13:35,612 - Epoch: [44][   80/  246]    Overall Loss 0.352592    Objective Loss 0.352592                                        LR 0.000036    Time 0.022476    
2023-01-06 16:13:35,769 - Epoch: [44][   90/  246]    Overall Loss 0.351870    Objective Loss 0.351870                                        LR 0.000036    Time 0.021722    
2023-01-06 16:13:35,929 - Epoch: [44][  100/  246]    Overall Loss 0.350125    Objective Loss 0.350125                                        LR 0.000036    Time 0.021143    
2023-01-06 16:13:36,087 - Epoch: [44][  110/  246]    Overall Loss 0.348249    Objective Loss 0.348249                                        LR 0.000036    Time 0.020657    
2023-01-06 16:13:36,246 - Epoch: [44][  120/  246]    Overall Loss 0.350044    Objective Loss 0.350044                                        LR 0.000036    Time 0.020251    
2023-01-06 16:13:36,400 - Epoch: [44][  130/  246]    Overall Loss 0.351939    Objective Loss 0.351939                                        LR 0.000036    Time 0.019875    
2023-01-06 16:13:36,549 - Epoch: [44][  140/  246]    Overall Loss 0.350492    Objective Loss 0.350492                                        LR 0.000036    Time 0.019522    
2023-01-06 16:13:36,704 - Epoch: [44][  150/  246]    Overall Loss 0.349712    Objective Loss 0.349712                                        LR 0.000036    Time 0.019247    
2023-01-06 16:13:36,859 - Epoch: [44][  160/  246]    Overall Loss 0.351019    Objective Loss 0.351019                                        LR 0.000036    Time 0.019013    
2023-01-06 16:13:37,032 - Epoch: [44][  170/  246]    Overall Loss 0.350519    Objective Loss 0.350519                                        LR 0.000036    Time 0.018911    
2023-01-06 16:13:37,207 - Epoch: [44][  180/  246]    Overall Loss 0.349931    Objective Loss 0.349931                                        LR 0.000036    Time 0.018829    
2023-01-06 16:13:37,380 - Epoch: [44][  190/  246]    Overall Loss 0.349942    Objective Loss 0.349942                                        LR 0.000036    Time 0.018750    
2023-01-06 16:13:37,554 - Epoch: [44][  200/  246]    Overall Loss 0.348874    Objective Loss 0.348874                                        LR 0.000036    Time 0.018677    
2023-01-06 16:13:37,728 - Epoch: [44][  210/  246]    Overall Loss 0.348006    Objective Loss 0.348006                                        LR 0.000036    Time 0.018618    
2023-01-06 16:13:37,909 - Epoch: [44][  220/  246]    Overall Loss 0.347449    Objective Loss 0.347449                                        LR 0.000036    Time 0.018593    
2023-01-06 16:13:38,089 - Epoch: [44][  230/  246]    Overall Loss 0.346976    Objective Loss 0.346976                                        LR 0.000036    Time 0.018564    
2023-01-06 16:13:38,285 - Epoch: [44][  240/  246]    Overall Loss 0.346870    Objective Loss 0.346870                                        LR 0.000036    Time 0.018607    
2023-01-06 16:13:38,386 - Epoch: [44][  246/  246]    Overall Loss 0.346550    Objective Loss 0.346550    Top1 89.712919    LR 0.000036    Time 0.018561    
2023-01-06 16:13:38,552 - --- validate (epoch=44)-----------
2023-01-06 16:13:38,552 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:38,982 - Epoch: [44][   10/   28]    Loss 0.360410    Top1 86.835938    
2023-01-06 16:13:39,085 - Epoch: [44][   20/   28]    Loss 0.350279    Top1 86.738281    
2023-01-06 16:13:39,147 - Epoch: [44][   28/   28]    Loss 0.344470    Top1 87.016891    
2023-01-06 16:13:39,306 - ==> Top1: 87.017    Loss: 0.344

2023-01-06 16:13:39,306 - ==> Confusion:
[[ 109    4  326]
 [   3   88  511]
 [  35   28 5882]]

2023-01-06 16:13:39,307 - ==> Best [Top1: 87.346   Sparsity:0.00   Params: 46192 on epoch: 42]
2023-01-06 16:13:39,307 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:39,312 - 

2023-01-06 16:13:39,312 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:39,861 - Epoch: [45][   10/  246]    Overall Loss 0.334712    Objective Loss 0.334712                                        LR 0.000036    Time 0.054895    
2023-01-06 16:13:40,025 - Epoch: [45][   20/  246]    Overall Loss 0.341209    Objective Loss 0.341209                                        LR 0.000036    Time 0.035500    
2023-01-06 16:13:40,185 - Epoch: [45][   30/  246]    Overall Loss 0.340826    Objective Loss 0.340826                                        LR 0.000036    Time 0.028984    
2023-01-06 16:13:40,346 - Epoch: [45][   40/  246]    Overall Loss 0.339723    Objective Loss 0.339723                                        LR 0.000036    Time 0.025759    
2023-01-06 16:13:40,509 - Epoch: [45][   50/  246]    Overall Loss 0.338237    Objective Loss 0.338237                                        LR 0.000036    Time 0.023867    
2023-01-06 16:13:40,669 - Epoch: [45][   60/  246]    Overall Loss 0.335463    Objective Loss 0.335463                                        LR 0.000036    Time 0.022545    
2023-01-06 16:13:40,831 - Epoch: [45][   70/  246]    Overall Loss 0.338123    Objective Loss 0.338123                                        LR 0.000036    Time 0.021638    
2023-01-06 16:13:40,992 - Epoch: [45][   80/  246]    Overall Loss 0.342876    Objective Loss 0.342876                                        LR 0.000036    Time 0.020942    
2023-01-06 16:13:41,154 - Epoch: [45][   90/  246]    Overall Loss 0.343031    Objective Loss 0.343031                                        LR 0.000036    Time 0.020407    
2023-01-06 16:13:41,291 - Epoch: [45][  100/  246]    Overall Loss 0.341062    Objective Loss 0.341062                                        LR 0.000036    Time 0.019732    
2023-01-06 16:13:41,432 - Epoch: [45][  110/  246]    Overall Loss 0.342304    Objective Loss 0.342304                                        LR 0.000036    Time 0.019218    
2023-01-06 16:13:41,573 - Epoch: [45][  120/  246]    Overall Loss 0.342856    Objective Loss 0.342856                                        LR 0.000036    Time 0.018783    
2023-01-06 16:13:41,713 - Epoch: [45][  130/  246]    Overall Loss 0.343947    Objective Loss 0.343947                                        LR 0.000036    Time 0.018412    
2023-01-06 16:13:41,854 - Epoch: [45][  140/  246]    Overall Loss 0.344646    Objective Loss 0.344646                                        LR 0.000036    Time 0.018102    
2023-01-06 16:13:41,993 - Epoch: [45][  150/  246]    Overall Loss 0.344674    Objective Loss 0.344674                                        LR 0.000036    Time 0.017822    
2023-01-06 16:13:42,132 - Epoch: [45][  160/  246]    Overall Loss 0.344094    Objective Loss 0.344094                                        LR 0.000036    Time 0.017571    
2023-01-06 16:13:42,272 - Epoch: [45][  170/  246]    Overall Loss 0.343031    Objective Loss 0.343031                                        LR 0.000036    Time 0.017360    
2023-01-06 16:13:42,412 - Epoch: [45][  180/  246]    Overall Loss 0.343442    Objective Loss 0.343442                                        LR 0.000036    Time 0.017171    
2023-01-06 16:13:42,553 - Epoch: [45][  190/  246]    Overall Loss 0.343538    Objective Loss 0.343538                                        LR 0.000036    Time 0.017005    
2023-01-06 16:13:42,693 - Epoch: [45][  200/  246]    Overall Loss 0.344481    Objective Loss 0.344481                                        LR 0.000036    Time 0.016851    
2023-01-06 16:13:42,831 - Epoch: [45][  210/  246]    Overall Loss 0.344826    Objective Loss 0.344826                                        LR 0.000036    Time 0.016707    
2023-01-06 16:13:42,978 - Epoch: [45][  220/  246]    Overall Loss 0.344905    Objective Loss 0.344905                                        LR 0.000036    Time 0.016613    
2023-01-06 16:13:43,120 - Epoch: [45][  230/  246]    Overall Loss 0.344673    Objective Loss 0.344673                                        LR 0.000036    Time 0.016506    
2023-01-06 16:13:43,285 - Epoch: [45][  240/  246]    Overall Loss 0.344570    Objective Loss 0.344570                                        LR 0.000036    Time 0.016504    
2023-01-06 16:13:43,362 - Epoch: [45][  246/  246]    Overall Loss 0.344996    Objective Loss 0.344996    Top1 84.688995    LR 0.000036    Time 0.016412    
2023-01-06 16:13:43,498 - --- validate (epoch=45)-----------
2023-01-06 16:13:43,498 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:43,943 - Epoch: [45][   10/   28]    Loss 0.341627    Top1 87.851562    
2023-01-06 16:13:44,045 - Epoch: [45][   20/   28]    Loss 0.350411    Top1 87.031250    
2023-01-06 16:13:44,104 - Epoch: [45][   28/   28]    Loss 0.339327    Top1 87.389064    
2023-01-06 16:13:44,242 - ==> Top1: 87.389    Loss: 0.339

2023-01-06 16:13:44,243 - ==> Confusion:
[[ 152    2  285]
 [   8  110  484]
 [  63   39 5843]]

2023-01-06 16:13:44,244 - ==> Best [Top1: 87.389   Sparsity:0.00   Params: 46192 on epoch: 45]
2023-01-06 16:13:44,244 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:44,249 - 

2023-01-06 16:13:44,249 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:44,922 - Epoch: [46][   10/  246]    Overall Loss 0.347474    Objective Loss 0.347474                                        LR 0.000036    Time 0.067255    
2023-01-06 16:13:45,088 - Epoch: [46][   20/  246]    Overall Loss 0.350899    Objective Loss 0.350899                                        LR 0.000036    Time 0.041919    
2023-01-06 16:13:45,244 - Epoch: [46][   30/  246]    Overall Loss 0.343058    Objective Loss 0.343058                                        LR 0.000036    Time 0.033122    
2023-01-06 16:13:45,403 - Epoch: [46][   40/  246]    Overall Loss 0.342550    Objective Loss 0.342550                                        LR 0.000036    Time 0.028813    
2023-01-06 16:13:45,568 - Epoch: [46][   50/  246]    Overall Loss 0.350877    Objective Loss 0.350877                                        LR 0.000036    Time 0.026335    
2023-01-06 16:13:45,735 - Epoch: [46][   60/  246]    Overall Loss 0.349425    Objective Loss 0.349425                                        LR 0.000036    Time 0.024737    
2023-01-06 16:13:45,898 - Epoch: [46][   70/  246]    Overall Loss 0.351153    Objective Loss 0.351153                                        LR 0.000036    Time 0.023515    
2023-01-06 16:13:46,059 - Epoch: [46][   80/  246]    Overall Loss 0.348964    Objective Loss 0.348964                                        LR 0.000036    Time 0.022587    
2023-01-06 16:13:46,219 - Epoch: [46][   90/  246]    Overall Loss 0.350699    Objective Loss 0.350699                                        LR 0.000036    Time 0.021855    
2023-01-06 16:13:46,380 - Epoch: [46][  100/  246]    Overall Loss 0.349784    Objective Loss 0.349784                                        LR 0.000036    Time 0.021271    
2023-01-06 16:13:46,540 - Epoch: [46][  110/  246]    Overall Loss 0.347969    Objective Loss 0.347969                                        LR 0.000036    Time 0.020796    
2023-01-06 16:13:46,700 - Epoch: [46][  120/  246]    Overall Loss 0.349060    Objective Loss 0.349060                                        LR 0.000036    Time 0.020386    
2023-01-06 16:13:46,860 - Epoch: [46][  130/  246]    Overall Loss 0.348083    Objective Loss 0.348083                                        LR 0.000036    Time 0.020047    
2023-01-06 16:13:47,019 - Epoch: [46][  140/  246]    Overall Loss 0.347254    Objective Loss 0.347254                                        LR 0.000036    Time 0.019751    
2023-01-06 16:13:47,184 - Epoch: [46][  150/  246]    Overall Loss 0.348213    Objective Loss 0.348213                                        LR 0.000036    Time 0.019529    
2023-01-06 16:13:47,353 - Epoch: [46][  160/  246]    Overall Loss 0.348902    Objective Loss 0.348902                                        LR 0.000036    Time 0.019363    
2023-01-06 16:13:47,519 - Epoch: [46][  170/  246]    Overall Loss 0.347926    Objective Loss 0.347926                                        LR 0.000036    Time 0.019202    
2023-01-06 16:13:47,679 - Epoch: [46][  180/  246]    Overall Loss 0.348692    Objective Loss 0.348692                                        LR 0.000036    Time 0.019019    
2023-01-06 16:13:47,845 - Epoch: [46][  190/  246]    Overall Loss 0.348282    Objective Loss 0.348282                                        LR 0.000036    Time 0.018893    
2023-01-06 16:13:48,011 - Epoch: [46][  200/  246]    Overall Loss 0.346973    Objective Loss 0.346973                                        LR 0.000036    Time 0.018776    
2023-01-06 16:13:48,176 - Epoch: [46][  210/  246]    Overall Loss 0.345377    Objective Loss 0.345377                                        LR 0.000036    Time 0.018667    
2023-01-06 16:13:48,346 - Epoch: [46][  220/  246]    Overall Loss 0.345374    Objective Loss 0.345374                                        LR 0.000036    Time 0.018589    
2023-01-06 16:13:48,505 - Epoch: [46][  230/  246]    Overall Loss 0.344977    Objective Loss 0.344977                                        LR 0.000036    Time 0.018469    
2023-01-06 16:13:48,680 - Epoch: [46][  240/  246]    Overall Loss 0.345323    Objective Loss 0.345323                                        LR 0.000036    Time 0.018429    
2023-01-06 16:13:48,758 - Epoch: [46][  246/  246]    Overall Loss 0.345476    Objective Loss 0.345476    Top1 83.732057    LR 0.000036    Time 0.018294    
2023-01-06 16:13:48,896 - --- validate (epoch=46)-----------
2023-01-06 16:13:48,897 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:49,335 - Epoch: [46][   10/   28]    Loss 0.345709    Top1 86.953125    
2023-01-06 16:13:49,458 - Epoch: [46][   20/   28]    Loss 0.355797    Top1 86.699219    
2023-01-06 16:13:49,516 - Epoch: [46][   28/   28]    Loss 0.348592    Top1 87.145720    
2023-01-06 16:13:49,675 - ==> Top1: 87.146    Loss: 0.349

2023-01-06 16:13:49,675 - ==> Confusion:
[[ 165    2  272]
 [  10   80  512]
 [  83   19 5843]]

2023-01-06 16:13:49,676 - ==> Best [Top1: 87.389   Sparsity:0.00   Params: 46192 on epoch: 45]
2023-01-06 16:13:49,676 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:49,681 - 

2023-01-06 16:13:49,681 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:50,211 - Epoch: [47][   10/  246]    Overall Loss 0.356376    Objective Loss 0.356376                                        LR 0.000036    Time 0.052941    
2023-01-06 16:13:50,369 - Epoch: [47][   20/  246]    Overall Loss 0.345220    Objective Loss 0.345220                                        LR 0.000036    Time 0.034361    
2023-01-06 16:13:50,527 - Epoch: [47][   30/  246]    Overall Loss 0.344192    Objective Loss 0.344192                                        LR 0.000036    Time 0.028147    
2023-01-06 16:13:50,675 - Epoch: [47][   40/  246]    Overall Loss 0.346089    Objective Loss 0.346089                                        LR 0.000036    Time 0.024805    
2023-01-06 16:13:50,833 - Epoch: [47][   50/  246]    Overall Loss 0.349588    Objective Loss 0.349588                                        LR 0.000036    Time 0.022984    
2023-01-06 16:13:50,984 - Epoch: [47][   60/  246]    Overall Loss 0.345422    Objective Loss 0.345422                                        LR 0.000036    Time 0.021679    
2023-01-06 16:13:51,145 - Epoch: [47][   70/  246]    Overall Loss 0.343440    Objective Loss 0.343440                                        LR 0.000036    Time 0.020871    
2023-01-06 16:13:51,312 - Epoch: [47][   80/  246]    Overall Loss 0.344938    Objective Loss 0.344938                                        LR 0.000036    Time 0.020343    
2023-01-06 16:13:51,469 - Epoch: [47][   90/  246]    Overall Loss 0.345006    Objective Loss 0.345006                                        LR 0.000036    Time 0.019801    
2023-01-06 16:13:51,631 - Epoch: [47][  100/  246]    Overall Loss 0.343543    Objective Loss 0.343543                                        LR 0.000036    Time 0.019445    
2023-01-06 16:13:51,777 - Epoch: [47][  110/  246]    Overall Loss 0.342839    Objective Loss 0.342839                                        LR 0.000036    Time 0.018995    
2023-01-06 16:13:51,917 - Epoch: [47][  120/  246]    Overall Loss 0.344177    Objective Loss 0.344177                                        LR 0.000036    Time 0.018580    
2023-01-06 16:13:52,066 - Epoch: [47][  130/  246]    Overall Loss 0.343898    Objective Loss 0.343898                                        LR 0.000036    Time 0.018281    
2023-01-06 16:13:52,206 - Epoch: [47][  140/  246]    Overall Loss 0.343677    Objective Loss 0.343677                                        LR 0.000036    Time 0.017969    
2023-01-06 16:13:52,345 - Epoch: [47][  150/  246]    Overall Loss 0.343720    Objective Loss 0.343720                                        LR 0.000036    Time 0.017696    
2023-01-06 16:13:52,482 - Epoch: [47][  160/  246]    Overall Loss 0.343308    Objective Loss 0.343308                                        LR 0.000036    Time 0.017445    
2023-01-06 16:13:52,621 - Epoch: [47][  170/  246]    Overall Loss 0.343167    Objective Loss 0.343167                                        LR 0.000036    Time 0.017238    
2023-01-06 16:13:52,760 - Epoch: [47][  180/  246]    Overall Loss 0.342774    Objective Loss 0.342774                                        LR 0.000036    Time 0.017050    
2023-01-06 16:13:52,898 - Epoch: [47][  190/  246]    Overall Loss 0.343254    Objective Loss 0.343254                                        LR 0.000036    Time 0.016876    
2023-01-06 16:13:53,036 - Epoch: [47][  200/  246]    Overall Loss 0.342552    Objective Loss 0.342552                                        LR 0.000036    Time 0.016722    
2023-01-06 16:13:53,177 - Epoch: [47][  210/  246]    Overall Loss 0.344104    Objective Loss 0.344104                                        LR 0.000036    Time 0.016593    
2023-01-06 16:13:53,320 - Epoch: [47][  220/  246]    Overall Loss 0.343831    Objective Loss 0.343831                                        LR 0.000036    Time 0.016489    
2023-01-06 16:13:53,461 - Epoch: [47][  230/  246]    Overall Loss 0.343600    Objective Loss 0.343600                                        LR 0.000036    Time 0.016386    
2023-01-06 16:13:53,618 - Epoch: [47][  240/  246]    Overall Loss 0.342922    Objective Loss 0.342922                                        LR 0.000036    Time 0.016355    
2023-01-06 16:13:53,692 - Epoch: [47][  246/  246]    Overall Loss 0.342465    Objective Loss 0.342465    Top1 88.995215    LR 0.000036    Time 0.016255    
2023-01-06 16:13:53,827 - --- validate (epoch=47)-----------
2023-01-06 16:13:53,828 - 6986 samples (256 per mini-batch)
2023-01-06 16:13:54,276 - Epoch: [47][   10/   28]    Loss 0.368529    Top1 86.367188    
2023-01-06 16:13:54,394 - Epoch: [47][   20/   28]    Loss 0.347045    Top1 87.207031    
2023-01-06 16:13:54,448 - Epoch: [47][   28/   28]    Loss 0.340873    Top1 87.489264    
2023-01-06 16:13:54,602 - ==> Top1: 87.489    Loss: 0.341

2023-01-06 16:13:54,602 - ==> Confusion:
[[ 131    3  305]
 [   6  121  475]
 [  44   41 5860]]

2023-01-06 16:13:54,604 - ==> Best [Top1: 87.489   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:13:54,604 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:13:54,610 - 

2023-01-06 16:13:54,610 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:13:55,278 - Epoch: [48][   10/  246]    Overall Loss 0.344608    Objective Loss 0.344608                                        LR 0.000036    Time 0.066723    
2023-01-06 16:13:55,424 - Epoch: [48][   20/  246]    Overall Loss 0.343551    Objective Loss 0.343551                                        LR 0.000036    Time 0.040668    
2023-01-06 16:13:55,594 - Epoch: [48][   30/  246]    Overall Loss 0.339939    Objective Loss 0.339939                                        LR 0.000036    Time 0.032767    
2023-01-06 16:13:55,758 - Epoch: [48][   40/  246]    Overall Loss 0.340929    Objective Loss 0.340929                                        LR 0.000036    Time 0.028651    
2023-01-06 16:13:55,937 - Epoch: [48][   50/  246]    Overall Loss 0.345555    Objective Loss 0.345555                                        LR 0.000036    Time 0.026492    
2023-01-06 16:13:56,084 - Epoch: [48][   60/  246]    Overall Loss 0.344282    Objective Loss 0.344282                                        LR 0.000036    Time 0.024518    
2023-01-06 16:13:56,271 - Epoch: [48][   70/  246]    Overall Loss 0.344373    Objective Loss 0.344373                                        LR 0.000036    Time 0.023687    
2023-01-06 16:13:56,483 - Epoch: [48][   80/  246]    Overall Loss 0.345266    Objective Loss 0.345266                                        LR 0.000036    Time 0.023367    
2023-01-06 16:13:56,709 - Epoch: [48][   90/  246]    Overall Loss 0.345008    Objective Loss 0.345008                                        LR 0.000036    Time 0.023281    
2023-01-06 16:13:56,931 - Epoch: [48][  100/  246]    Overall Loss 0.343052    Objective Loss 0.343052                                        LR 0.000036    Time 0.023168    
2023-01-06 16:13:57,156 - Epoch: [48][  110/  246]    Overall Loss 0.342551    Objective Loss 0.342551                                        LR 0.000036    Time 0.023102    
2023-01-06 16:13:57,380 - Epoch: [48][  120/  246]    Overall Loss 0.342234    Objective Loss 0.342234                                        LR 0.000036    Time 0.023039    
2023-01-06 16:13:57,579 - Epoch: [48][  130/  246]    Overall Loss 0.344299    Objective Loss 0.344299                                        LR 0.000036    Time 0.022796    
2023-01-06 16:13:57,789 - Epoch: [48][  140/  246]    Overall Loss 0.343728    Objective Loss 0.343728                                        LR 0.000036    Time 0.022668    
2023-01-06 16:13:58,016 - Epoch: [48][  150/  246]    Overall Loss 0.343940    Objective Loss 0.343940                                        LR 0.000036    Time 0.022660    
2023-01-06 16:13:58,236 - Epoch: [48][  160/  246]    Overall Loss 0.343458    Objective Loss 0.343458                                        LR 0.000036    Time 0.022617    
2023-01-06 16:13:58,455 - Epoch: [48][  170/  246]    Overall Loss 0.344340    Objective Loss 0.344340                                        LR 0.000036    Time 0.022572    
2023-01-06 16:13:58,673 - Epoch: [48][  180/  246]    Overall Loss 0.343581    Objective Loss 0.343581                                        LR 0.000036    Time 0.022529    
2023-01-06 16:13:58,893 - Epoch: [48][  190/  246]    Overall Loss 0.342964    Objective Loss 0.342964                                        LR 0.000036    Time 0.022497    
2023-01-06 16:13:59,110 - Epoch: [48][  200/  246]    Overall Loss 0.343312    Objective Loss 0.343312                                        LR 0.000036    Time 0.022457    
2023-01-06 16:13:59,322 - Epoch: [48][  210/  246]    Overall Loss 0.343840    Objective Loss 0.343840                                        LR 0.000036    Time 0.022396    
2023-01-06 16:13:59,544 - Epoch: [48][  220/  246]    Overall Loss 0.343440    Objective Loss 0.343440                                        LR 0.000036    Time 0.022383    
2023-01-06 16:13:59,763 - Epoch: [48][  230/  246]    Overall Loss 0.344501    Objective Loss 0.344501                                        LR 0.000036    Time 0.022359    
2023-01-06 16:13:59,990 - Epoch: [48][  240/  246]    Overall Loss 0.343303    Objective Loss 0.343303                                        LR 0.000036    Time 0.022372    
2023-01-06 16:14:00,085 - Epoch: [48][  246/  246]    Overall Loss 0.342796    Objective Loss 0.342796    Top1 89.234450    LR 0.000036    Time 0.022210    
2023-01-06 16:14:00,233 - --- validate (epoch=48)-----------
2023-01-06 16:14:00,233 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:00,664 - Epoch: [48][   10/   28]    Loss 0.355777    Top1 86.796875    
2023-01-06 16:14:00,759 - Epoch: [48][   20/   28]    Loss 0.344681    Top1 87.050781    
2023-01-06 16:14:00,819 - Epoch: [48][   28/   28]    Loss 0.343757    Top1 87.374749    
2023-01-06 16:14:00,993 - ==> Top1: 87.375    Loss: 0.344

2023-01-06 16:14:00,994 - ==> Confusion:
[[ 170    2  267]
 [  10  104  488]
 [  82   33 5830]]

2023-01-06 16:14:00,995 - ==> Best [Top1: 87.489   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:14:00,995 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:00,999 - 

2023-01-06 16:14:00,999 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:01,521 - Epoch: [49][   10/  246]    Overall Loss 0.328379    Objective Loss 0.328379                                        LR 0.000036    Time 0.052116    
2023-01-06 16:14:01,679 - Epoch: [49][   20/  246]    Overall Loss 0.333430    Objective Loss 0.333430                                        LR 0.000036    Time 0.033934    
2023-01-06 16:14:01,825 - Epoch: [49][   30/  246]    Overall Loss 0.333882    Objective Loss 0.333882                                        LR 0.000036    Time 0.027463    
2023-01-06 16:14:01,969 - Epoch: [49][   40/  246]    Overall Loss 0.336207    Objective Loss 0.336207                                        LR 0.000036    Time 0.024185    
2023-01-06 16:14:02,110 - Epoch: [49][   50/  246]    Overall Loss 0.336351    Objective Loss 0.336351                                        LR 0.000036    Time 0.022171    
2023-01-06 16:14:02,246 - Epoch: [49][   60/  246]    Overall Loss 0.335644    Objective Loss 0.335644                                        LR 0.000036    Time 0.020733    
2023-01-06 16:14:02,391 - Epoch: [49][   70/  246]    Overall Loss 0.337750    Objective Loss 0.337750                                        LR 0.000036    Time 0.019835    
2023-01-06 16:14:02,543 - Epoch: [49][   80/  246]    Overall Loss 0.337130    Objective Loss 0.337130                                        LR 0.000036    Time 0.019249    
2023-01-06 16:14:02,691 - Epoch: [49][   90/  246]    Overall Loss 0.336799    Objective Loss 0.336799                                        LR 0.000036    Time 0.018751    
2023-01-06 16:14:02,841 - Epoch: [49][  100/  246]    Overall Loss 0.335114    Objective Loss 0.335114                                        LR 0.000036    Time 0.018374    
2023-01-06 16:14:03,012 - Epoch: [49][  110/  246]    Overall Loss 0.336540    Objective Loss 0.336540                                        LR 0.000036    Time 0.018252    
2023-01-06 16:14:03,162 - Epoch: [49][  120/  246]    Overall Loss 0.338185    Objective Loss 0.338185                                        LR 0.000036    Time 0.017983    
2023-01-06 16:14:03,322 - Epoch: [49][  130/  246]    Overall Loss 0.337779    Objective Loss 0.337779                                        LR 0.000036    Time 0.017826    
2023-01-06 16:14:03,470 - Epoch: [49][  140/  246]    Overall Loss 0.338940    Objective Loss 0.338940                                        LR 0.000036    Time 0.017606    
2023-01-06 16:14:03,637 - Epoch: [49][  150/  246]    Overall Loss 0.339975    Objective Loss 0.339975                                        LR 0.000036    Time 0.017542    
2023-01-06 16:14:03,794 - Epoch: [49][  160/  246]    Overall Loss 0.341054    Objective Loss 0.341054                                        LR 0.000036    Time 0.017425    
2023-01-06 16:14:03,951 - Epoch: [49][  170/  246]    Overall Loss 0.341896    Objective Loss 0.341896                                        LR 0.000036    Time 0.017325    
2023-01-06 16:14:04,113 - Epoch: [49][  180/  246]    Overall Loss 0.341418    Objective Loss 0.341418                                        LR 0.000036    Time 0.017261    
2023-01-06 16:14:04,281 - Epoch: [49][  190/  246]    Overall Loss 0.341058    Objective Loss 0.341058                                        LR 0.000036    Time 0.017234    
2023-01-06 16:14:04,449 - Epoch: [49][  200/  246]    Overall Loss 0.341325    Objective Loss 0.341325                                        LR 0.000036    Time 0.017211    
2023-01-06 16:14:04,635 - Epoch: [49][  210/  246]    Overall Loss 0.341796    Objective Loss 0.341796                                        LR 0.000036    Time 0.017276    
2023-01-06 16:14:04,844 - Epoch: [49][  220/  246]    Overall Loss 0.342621    Objective Loss 0.342621                                        LR 0.000036    Time 0.017437    
2023-01-06 16:14:05,046 - Epoch: [49][  230/  246]    Overall Loss 0.341905    Objective Loss 0.341905                                        LR 0.000036    Time 0.017558    
2023-01-06 16:14:05,257 - Epoch: [49][  240/  246]    Overall Loss 0.342828    Objective Loss 0.342828                                        LR 0.000036    Time 0.017702    
2023-01-06 16:14:05,355 - Epoch: [49][  246/  246]    Overall Loss 0.342625    Objective Loss 0.342625    Top1 88.995215    LR 0.000036    Time 0.017666    
2023-01-06 16:14:05,486 - --- validate (epoch=49)-----------
2023-01-06 16:14:05,486 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:06,225 - Epoch: [49][   10/   28]    Loss 0.337776    Top1 87.343750    
2023-01-06 16:14:06,330 - Epoch: [49][   20/   28]    Loss 0.342451    Top1 87.480469    
2023-01-06 16:14:06,388 - Epoch: [49][   28/   28]    Loss 0.343759    Top1 87.231606    
2023-01-06 16:14:06,533 - ==> Top1: 87.232    Loss: 0.344

2023-01-06 16:14:06,534 - ==> Confusion:
[[ 145    3  291]
 [   8   89  505]
 [  56   29 5860]]

2023-01-06 16:14:06,535 - ==> Best [Top1: 87.489   Sparsity:0.00   Params: 46192 on epoch: 47]
2023-01-06 16:14:06,535 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:06,539 - 

2023-01-06 16:14:06,539 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:07,111 - Epoch: [50][   10/  246]    Overall Loss 0.333307    Objective Loss 0.333307                                        LR 0.000036    Time 0.057093    
2023-01-06 16:14:07,271 - Epoch: [50][   20/  246]    Overall Loss 0.339914    Objective Loss 0.339914                                        LR 0.000036    Time 0.036532    
2023-01-06 16:14:07,444 - Epoch: [50][   30/  246]    Overall Loss 0.335819    Objective Loss 0.335819                                        LR 0.000036    Time 0.030110    
2023-01-06 16:14:07,612 - Epoch: [50][   40/  246]    Overall Loss 0.338700    Objective Loss 0.338700                                        LR 0.000036    Time 0.026771    
2023-01-06 16:14:07,787 - Epoch: [50][   50/  246]    Overall Loss 0.337032    Objective Loss 0.337032                                        LR 0.000036    Time 0.024914    
2023-01-06 16:14:07,955 - Epoch: [50][   60/  246]    Overall Loss 0.337129    Objective Loss 0.337129                                        LR 0.000036    Time 0.023550    
2023-01-06 16:14:08,125 - Epoch: [50][   70/  246]    Overall Loss 0.335470    Objective Loss 0.335470                                        LR 0.000036    Time 0.022601    
2023-01-06 16:14:08,297 - Epoch: [50][   80/  246]    Overall Loss 0.337196    Objective Loss 0.337196                                        LR 0.000036    Time 0.021922    
2023-01-06 16:14:08,466 - Epoch: [50][   90/  246]    Overall Loss 0.336092    Objective Loss 0.336092                                        LR 0.000036    Time 0.021364    
2023-01-06 16:14:08,634 - Epoch: [50][  100/  246]    Overall Loss 0.338175    Objective Loss 0.338175                                        LR 0.000036    Time 0.020901    
2023-01-06 16:14:08,799 - Epoch: [50][  110/  246]    Overall Loss 0.338217    Objective Loss 0.338217                                        LR 0.000036    Time 0.020497    
2023-01-06 16:14:08,959 - Epoch: [50][  120/  246]    Overall Loss 0.337330    Objective Loss 0.337330                                        LR 0.000036    Time 0.020122    
2023-01-06 16:14:09,122 - Epoch: [50][  130/  246]    Overall Loss 0.336744    Objective Loss 0.336744                                        LR 0.000036    Time 0.019828    
2023-01-06 16:14:09,293 - Epoch: [50][  140/  246]    Overall Loss 0.336998    Objective Loss 0.336998                                        LR 0.000036    Time 0.019628    
2023-01-06 16:14:09,466 - Epoch: [50][  150/  246]    Overall Loss 0.337261    Objective Loss 0.337261                                        LR 0.000036    Time 0.019470    
2023-01-06 16:14:09,637 - Epoch: [50][  160/  246]    Overall Loss 0.337545    Objective Loss 0.337545                                        LR 0.000036    Time 0.019322    
2023-01-06 16:14:09,809 - Epoch: [50][  170/  246]    Overall Loss 0.338586    Objective Loss 0.338586                                        LR 0.000036    Time 0.019190    
2023-01-06 16:14:09,979 - Epoch: [50][  180/  246]    Overall Loss 0.338409    Objective Loss 0.338409                                        LR 0.000036    Time 0.019067    
2023-01-06 16:14:10,153 - Epoch: [50][  190/  246]    Overall Loss 0.339515    Objective Loss 0.339515                                        LR 0.000036    Time 0.018976    
2023-01-06 16:14:10,311 - Epoch: [50][  200/  246]    Overall Loss 0.340053    Objective Loss 0.340053                                        LR 0.000036    Time 0.018819    
2023-01-06 16:14:10,478 - Epoch: [50][  210/  246]    Overall Loss 0.339465    Objective Loss 0.339465                                        LR 0.000036    Time 0.018715    
2023-01-06 16:14:10,642 - Epoch: [50][  220/  246]    Overall Loss 0.340456    Objective Loss 0.340456                                        LR 0.000036    Time 0.018607    
2023-01-06 16:14:10,808 - Epoch: [50][  230/  246]    Overall Loss 0.341414    Objective Loss 0.341414                                        LR 0.000036    Time 0.018519    
2023-01-06 16:14:10,982 - Epoch: [50][  240/  246]    Overall Loss 0.340967    Objective Loss 0.340967                                        LR 0.000036    Time 0.018472    
2023-01-06 16:14:11,058 - Epoch: [50][  246/  246]    Overall Loss 0.341012    Objective Loss 0.341012    Top1 88.755981    LR 0.000036    Time 0.018329    
2023-01-06 16:14:11,196 - --- validate (epoch=50)-----------
2023-01-06 16:14:11,196 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:11,639 - Epoch: [50][   10/   28]    Loss 0.333375    Top1 88.164062    
2023-01-06 16:14:11,740 - Epoch: [50][   20/   28]    Loss 0.334718    Top1 87.812500    
2023-01-06 16:14:11,798 - Epoch: [50][   28/   28]    Loss 0.334833    Top1 87.517893    
2023-01-06 16:14:11,940 - ==> Top1: 87.518    Loss: 0.335

2023-01-06 16:14:11,941 - ==> Confusion:
[[ 136    6  297]
 [   7  130  465]
 [  43   54 5848]]

2023-01-06 16:14:11,942 - ==> Best [Top1: 87.518   Sparsity:0.00   Params: 46192 on epoch: 50]
2023-01-06 16:14:11,942 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:11,947 - 

2023-01-06 16:14:11,947 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:12,645 - Epoch: [51][   10/  246]    Overall Loss 0.329192    Objective Loss 0.329192                                        LR 0.000036    Time 0.069793    
2023-01-06 16:14:12,814 - Epoch: [51][   20/  246]    Overall Loss 0.330571    Objective Loss 0.330571                                        LR 0.000036    Time 0.043337    
2023-01-06 16:14:12,989 - Epoch: [51][   30/  246]    Overall Loss 0.335852    Objective Loss 0.335852                                        LR 0.000036    Time 0.034712    
2023-01-06 16:14:13,163 - Epoch: [51][   40/  246]    Overall Loss 0.339978    Objective Loss 0.339978                                        LR 0.000036    Time 0.030363    
2023-01-06 16:14:13,341 - Epoch: [51][   50/  246]    Overall Loss 0.342250    Objective Loss 0.342250                                        LR 0.000036    Time 0.027847    
2023-01-06 16:14:13,515 - Epoch: [51][   60/  246]    Overall Loss 0.344201    Objective Loss 0.344201                                        LR 0.000036    Time 0.026069    
2023-01-06 16:14:13,694 - Epoch: [51][   70/  246]    Overall Loss 0.344067    Objective Loss 0.344067                                        LR 0.000036    Time 0.024889    
2023-01-06 16:14:13,868 - Epoch: [51][   80/  246]    Overall Loss 0.342265    Objective Loss 0.342265                                        LR 0.000036    Time 0.023938    
2023-01-06 16:14:14,043 - Epoch: [51][   90/  246]    Overall Loss 0.342077    Objective Loss 0.342077                                        LR 0.000036    Time 0.023219    
2023-01-06 16:14:14,221 - Epoch: [51][  100/  246]    Overall Loss 0.343133    Objective Loss 0.343133                                        LR 0.000036    Time 0.022669    
2023-01-06 16:14:14,396 - Epoch: [51][  110/  246]    Overall Loss 0.342142    Objective Loss 0.342142                                        LR 0.000036    Time 0.022176    
2023-01-06 16:14:14,575 - Epoch: [51][  120/  246]    Overall Loss 0.342211    Objective Loss 0.342211                                        LR 0.000036    Time 0.021821    
2023-01-06 16:14:14,750 - Epoch: [51][  130/  246]    Overall Loss 0.341643    Objective Loss 0.341643                                        LR 0.000036    Time 0.021470    
2023-01-06 16:14:14,929 - Epoch: [51][  140/  246]    Overall Loss 0.341158    Objective Loss 0.341158                                        LR 0.000036    Time 0.021212    
2023-01-06 16:14:15,101 - Epoch: [51][  150/  246]    Overall Loss 0.340838    Objective Loss 0.340838                                        LR 0.000036    Time 0.020931    
2023-01-06 16:14:15,277 - Epoch: [51][  160/  246]    Overall Loss 0.341307    Objective Loss 0.341307                                        LR 0.000036    Time 0.020723    
2023-01-06 16:14:15,448 - Epoch: [51][  170/  246]    Overall Loss 0.339736    Objective Loss 0.339736                                        LR 0.000036    Time 0.020508    
2023-01-06 16:14:15,624 - Epoch: [51][  180/  246]    Overall Loss 0.340437    Objective Loss 0.340437                                        LR 0.000036    Time 0.020346    
2023-01-06 16:14:15,795 - Epoch: [51][  190/  246]    Overall Loss 0.339805    Objective Loss 0.339805                                        LR 0.000036    Time 0.020169    
2023-01-06 16:14:15,965 - Epoch: [51][  200/  246]    Overall Loss 0.339875    Objective Loss 0.339875                                        LR 0.000036    Time 0.020009    
2023-01-06 16:14:16,136 - Epoch: [51][  210/  246]    Overall Loss 0.339706    Objective Loss 0.339706                                        LR 0.000036    Time 0.019870    
2023-01-06 16:14:16,312 - Epoch: [51][  220/  246]    Overall Loss 0.338811    Objective Loss 0.338811                                        LR 0.000036    Time 0.019764    
2023-01-06 16:14:16,482 - Epoch: [51][  230/  246]    Overall Loss 0.339429    Objective Loss 0.339429                                        LR 0.000036    Time 0.019644    
2023-01-06 16:14:16,667 - Epoch: [51][  240/  246]    Overall Loss 0.339348    Objective Loss 0.339348                                        LR 0.000036    Time 0.019594    
2023-01-06 16:14:16,745 - Epoch: [51][  246/  246]    Overall Loss 0.339026    Objective Loss 0.339026    Top1 88.755981    LR 0.000036    Time 0.019431    
2023-01-06 16:14:16,882 - --- validate (epoch=51)-----------
2023-01-06 16:14:16,883 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:17,308 - Epoch: [51][   10/   28]    Loss 0.338870    Top1 88.398438    
2023-01-06 16:14:17,409 - Epoch: [51][   20/   28]    Loss 0.342623    Top1 87.832031    
2023-01-06 16:14:17,467 - Epoch: [51][   28/   28]    Loss 0.345856    Top1 87.703979    
2023-01-06 16:14:17,626 - ==> Top1: 87.704    Loss: 0.346

2023-01-06 16:14:17,626 - ==> Confusion:
[[ 204    6  229]
 [  15  170  417]
 [ 110   82 5753]]

2023-01-06 16:14:17,627 - ==> Best [Top1: 87.704   Sparsity:0.00   Params: 46192 on epoch: 51]
2023-01-06 16:14:17,628 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:17,633 - 

2023-01-06 16:14:17,633 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:18,193 - Epoch: [52][   10/  246]    Overall Loss 0.331207    Objective Loss 0.331207                                        LR 0.000036    Time 0.055964    
2023-01-06 16:14:18,361 - Epoch: [52][   20/  246]    Overall Loss 0.340664    Objective Loss 0.340664                                        LR 0.000036    Time 0.036376    
2023-01-06 16:14:18,519 - Epoch: [52][   30/  246]    Overall Loss 0.337868    Objective Loss 0.337868                                        LR 0.000036    Time 0.029483    
2023-01-06 16:14:18,686 - Epoch: [52][   40/  246]    Overall Loss 0.339601    Objective Loss 0.339601                                        LR 0.000036    Time 0.026269    
2023-01-06 16:14:18,855 - Epoch: [52][   50/  246]    Overall Loss 0.337076    Objective Loss 0.337076                                        LR 0.000036    Time 0.024406    
2023-01-06 16:14:19,024 - Epoch: [52][   60/  246]    Overall Loss 0.338070    Objective Loss 0.338070                                        LR 0.000036    Time 0.023142    
2023-01-06 16:14:19,193 - Epoch: [52][   70/  246]    Overall Loss 0.339373    Objective Loss 0.339373                                        LR 0.000036    Time 0.022244    
2023-01-06 16:14:19,361 - Epoch: [52][   80/  246]    Overall Loss 0.339213    Objective Loss 0.339213                                        LR 0.000036    Time 0.021558    
2023-01-06 16:14:19,531 - Epoch: [52][   90/  246]    Overall Loss 0.336448    Objective Loss 0.336448                                        LR 0.000036    Time 0.021046    
2023-01-06 16:14:19,698 - Epoch: [52][  100/  246]    Overall Loss 0.336742    Objective Loss 0.336742                                        LR 0.000036    Time 0.020612    
2023-01-06 16:14:19,868 - Epoch: [52][  110/  246]    Overall Loss 0.338378    Objective Loss 0.338378                                        LR 0.000036    Time 0.020281    
2023-01-06 16:14:20,036 - Epoch: [52][  120/  246]    Overall Loss 0.338106    Objective Loss 0.338106                                        LR 0.000036    Time 0.019990    
2023-01-06 16:14:20,206 - Epoch: [52][  130/  246]    Overall Loss 0.337661    Objective Loss 0.337661                                        LR 0.000036    Time 0.019758    
2023-01-06 16:14:20,377 - Epoch: [52][  140/  246]    Overall Loss 0.338735    Objective Loss 0.338735                                        LR 0.000036    Time 0.019564    
2023-01-06 16:14:20,547 - Epoch: [52][  150/  246]    Overall Loss 0.338261    Objective Loss 0.338261                                        LR 0.000036    Time 0.019392    
2023-01-06 16:14:20,718 - Epoch: [52][  160/  246]    Overall Loss 0.340155    Objective Loss 0.340155                                        LR 0.000036    Time 0.019246    
2023-01-06 16:14:20,888 - Epoch: [52][  170/  246]    Overall Loss 0.339941    Objective Loss 0.339941                                        LR 0.000036    Time 0.019112    
2023-01-06 16:14:21,071 - Epoch: [52][  180/  246]    Overall Loss 0.340796    Objective Loss 0.340796                                        LR 0.000036    Time 0.019062    
2023-01-06 16:14:21,255 - Epoch: [52][  190/  246]    Overall Loss 0.342140    Objective Loss 0.342140                                        LR 0.000036    Time 0.019017    
2023-01-06 16:14:21,438 - Epoch: [52][  200/  246]    Overall Loss 0.341492    Objective Loss 0.341492                                        LR 0.000036    Time 0.018974    
2023-01-06 16:14:21,624 - Epoch: [52][  210/  246]    Overall Loss 0.340500    Objective Loss 0.340500                                        LR 0.000036    Time 0.018945    
2023-01-06 16:14:21,808 - Epoch: [52][  220/  246]    Overall Loss 0.341473    Objective Loss 0.341473                                        LR 0.000036    Time 0.018919    
2023-01-06 16:14:21,980 - Epoch: [52][  230/  246]    Overall Loss 0.341076    Objective Loss 0.341076                                        LR 0.000036    Time 0.018842    
2023-01-06 16:14:22,154 - Epoch: [52][  240/  246]    Overall Loss 0.340401    Objective Loss 0.340401                                        LR 0.000036    Time 0.018781    
2023-01-06 16:14:22,235 - Epoch: [52][  246/  246]    Overall Loss 0.340065    Objective Loss 0.340065    Top1 88.038278    LR 0.000036    Time 0.018653    
2023-01-06 16:14:22,377 - --- validate (epoch=52)-----------
2023-01-06 16:14:22,377 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:22,812 - Epoch: [52][   10/   28]    Loss 0.334266    Top1 87.617188    
2023-01-06 16:14:22,917 - Epoch: [52][   20/   28]    Loss 0.333730    Top1 87.656250    
2023-01-06 16:14:22,975 - Epoch: [52][   28/   28]    Loss 0.334247    Top1 87.432007    
2023-01-06 16:14:23,116 - ==> Top1: 87.432    Loss: 0.334

2023-01-06 16:14:23,117 - ==> Confusion:
[[ 133    4  302]
 [   7  115  480]
 [  46   39 5860]]

2023-01-06 16:14:23,118 - ==> Best [Top1: 87.704   Sparsity:0.00   Params: 46192 on epoch: 51]
2023-01-06 16:14:23,118 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:23,126 - 

2023-01-06 16:14:23,126 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:23,832 - Epoch: [53][   10/  246]    Overall Loss 0.342543    Objective Loss 0.342543                                        LR 0.000036    Time 0.070533    
2023-01-06 16:14:24,003 - Epoch: [53][   20/  246]    Overall Loss 0.352793    Objective Loss 0.352793                                        LR 0.000036    Time 0.043802    
2023-01-06 16:14:24,166 - Epoch: [53][   30/  246]    Overall Loss 0.345172    Objective Loss 0.345172                                        LR 0.000036    Time 0.034583    
2023-01-06 16:14:24,333 - Epoch: [53][   40/  246]    Overall Loss 0.346789    Objective Loss 0.346789                                        LR 0.000036    Time 0.030101    
2023-01-06 16:14:24,501 - Epoch: [53][   50/  246]    Overall Loss 0.345541    Objective Loss 0.345541                                        LR 0.000036    Time 0.027422    
2023-01-06 16:14:24,667 - Epoch: [53][   60/  246]    Overall Loss 0.343244    Objective Loss 0.343244                                        LR 0.000036    Time 0.025611    
2023-01-06 16:14:24,835 - Epoch: [53][   70/  246]    Overall Loss 0.343239    Objective Loss 0.343239                                        LR 0.000036    Time 0.024355    
2023-01-06 16:14:25,002 - Epoch: [53][   80/  246]    Overall Loss 0.341823    Objective Loss 0.341823                                        LR 0.000036    Time 0.023398    
2023-01-06 16:14:25,170 - Epoch: [53][   90/  246]    Overall Loss 0.340733    Objective Loss 0.340733                                        LR 0.000036    Time 0.022659    
2023-01-06 16:14:25,339 - Epoch: [53][  100/  246]    Overall Loss 0.338655    Objective Loss 0.338655                                        LR 0.000036    Time 0.022073    
2023-01-06 16:14:25,504 - Epoch: [53][  110/  246]    Overall Loss 0.338573    Objective Loss 0.338573                                        LR 0.000036    Time 0.021568    
2023-01-06 16:14:25,672 - Epoch: [53][  120/  246]    Overall Loss 0.337659    Objective Loss 0.337659                                        LR 0.000036    Time 0.021166    
2023-01-06 16:14:25,841 - Epoch: [53][  130/  246]    Overall Loss 0.336871    Objective Loss 0.336871                                        LR 0.000036    Time 0.020835    
2023-01-06 16:14:26,008 - Epoch: [53][  140/  246]    Overall Loss 0.336623    Objective Loss 0.336623                                        LR 0.000036    Time 0.020539    
2023-01-06 16:14:26,174 - Epoch: [53][  150/  246]    Overall Loss 0.337358    Objective Loss 0.337358                                        LR 0.000036    Time 0.020276    
2023-01-06 16:14:26,341 - Epoch: [53][  160/  246]    Overall Loss 0.337864    Objective Loss 0.337864                                        LR 0.000036    Time 0.020051    
2023-01-06 16:14:26,507 - Epoch: [53][  170/  246]    Overall Loss 0.339985    Objective Loss 0.339985                                        LR 0.000036    Time 0.019842    
2023-01-06 16:14:26,676 - Epoch: [53][  180/  246]    Overall Loss 0.339740    Objective Loss 0.339740                                        LR 0.000036    Time 0.019680    
2023-01-06 16:14:26,834 - Epoch: [53][  190/  246]    Overall Loss 0.340052    Objective Loss 0.340052                                        LR 0.000036    Time 0.019474    
2023-01-06 16:14:26,992 - Epoch: [53][  200/  246]    Overall Loss 0.339286    Objective Loss 0.339286                                        LR 0.000036    Time 0.019286    
2023-01-06 16:14:27,152 - Epoch: [53][  210/  246]    Overall Loss 0.338927    Objective Loss 0.338927                                        LR 0.000036    Time 0.019129    
2023-01-06 16:14:27,308 - Epoch: [53][  220/  246]    Overall Loss 0.339142    Objective Loss 0.339142                                        LR 0.000036    Time 0.018966    
2023-01-06 16:14:27,468 - Epoch: [53][  230/  246]    Overall Loss 0.339015    Objective Loss 0.339015                                        LR 0.000036    Time 0.018835    
2023-01-06 16:14:27,639 - Epoch: [53][  240/  246]    Overall Loss 0.338448    Objective Loss 0.338448                                        LR 0.000036    Time 0.018763    
2023-01-06 16:14:27,718 - Epoch: [53][  246/  246]    Overall Loss 0.338035    Objective Loss 0.338035    Top1 87.320574    LR 0.000036    Time 0.018625    
2023-01-06 16:14:27,901 - --- validate (epoch=53)-----------
2023-01-06 16:14:27,902 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:28,345 - Epoch: [53][   10/   28]    Loss 0.338787    Top1 87.421875    
2023-01-06 16:14:28,446 - Epoch: [53][   20/   28]    Loss 0.334035    Top1 87.617188    
2023-01-06 16:14:28,502 - Epoch: [53][   28/   28]    Loss 0.335309    Top1 87.517893    
2023-01-06 16:14:28,658 - ==> Top1: 87.518    Loss: 0.335

2023-01-06 16:14:28,658 - ==> Confusion:
[[ 131    4  304]
 [   6  122  474]
 [  41   43 5861]]

2023-01-06 16:14:28,659 - ==> Best [Top1: 87.704   Sparsity:0.00   Params: 46192 on epoch: 51]
2023-01-06 16:14:28,660 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:28,664 - 

2023-01-06 16:14:28,664 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:29,340 - Epoch: [54][   10/  246]    Overall Loss 0.334221    Objective Loss 0.334221                                        LR 0.000036    Time 0.067568    
2023-01-06 16:14:29,497 - Epoch: [54][   20/  246]    Overall Loss 0.331722    Objective Loss 0.331722                                        LR 0.000036    Time 0.041619    
2023-01-06 16:14:29,648 - Epoch: [54][   30/  246]    Overall Loss 0.337492    Objective Loss 0.337492                                        LR 0.000036    Time 0.032772    
2023-01-06 16:14:29,806 - Epoch: [54][   40/  246]    Overall Loss 0.342172    Objective Loss 0.342172                                        LR 0.000036    Time 0.028509    
2023-01-06 16:14:29,969 - Epoch: [54][   50/  246]    Overall Loss 0.342614    Objective Loss 0.342614                                        LR 0.000036    Time 0.026058    
2023-01-06 16:14:30,131 - Epoch: [54][   60/  246]    Overall Loss 0.341283    Objective Loss 0.341283                                        LR 0.000036    Time 0.024415    
2023-01-06 16:14:30,296 - Epoch: [54][   70/  246]    Overall Loss 0.342000    Objective Loss 0.342000                                        LR 0.000036    Time 0.023272    
2023-01-06 16:14:30,462 - Epoch: [54][   80/  246]    Overall Loss 0.342864    Objective Loss 0.342864                                        LR 0.000036    Time 0.022442    
2023-01-06 16:14:30,635 - Epoch: [54][   90/  246]    Overall Loss 0.342194    Objective Loss 0.342194                                        LR 0.000036    Time 0.021862    
2023-01-06 16:14:30,821 - Epoch: [54][  100/  246]    Overall Loss 0.341735    Objective Loss 0.341735                                        LR 0.000036    Time 0.021529    
2023-01-06 16:14:31,013 - Epoch: [54][  110/  246]    Overall Loss 0.340759    Objective Loss 0.340759                                        LR 0.000036    Time 0.021314    
2023-01-06 16:14:31,205 - Epoch: [54][  120/  246]    Overall Loss 0.340299    Objective Loss 0.340299                                        LR 0.000036    Time 0.021136    
2023-01-06 16:14:31,394 - Epoch: [54][  130/  246]    Overall Loss 0.339049    Objective Loss 0.339049                                        LR 0.000036    Time 0.020964    
2023-01-06 16:14:31,601 - Epoch: [54][  140/  246]    Overall Loss 0.338610    Objective Loss 0.338610                                        LR 0.000036    Time 0.020937    
2023-01-06 16:14:31,808 - Epoch: [54][  150/  246]    Overall Loss 0.339857    Objective Loss 0.339857                                        LR 0.000036    Time 0.020918    
2023-01-06 16:14:32,009 - Epoch: [54][  160/  246]    Overall Loss 0.339955    Objective Loss 0.339955                                        LR 0.000036    Time 0.020866    
2023-01-06 16:14:32,204 - Epoch: [54][  170/  246]    Overall Loss 0.339790    Objective Loss 0.339790                                        LR 0.000036    Time 0.020786    
2023-01-06 16:14:32,375 - Epoch: [54][  180/  246]    Overall Loss 0.339042    Objective Loss 0.339042                                        LR 0.000036    Time 0.020574    
2023-01-06 16:14:32,547 - Epoch: [54][  190/  246]    Overall Loss 0.339366    Objective Loss 0.339366                                        LR 0.000036    Time 0.020400    
2023-01-06 16:14:32,715 - Epoch: [54][  200/  246]    Overall Loss 0.339546    Objective Loss 0.339546                                        LR 0.000036    Time 0.020214    
2023-01-06 16:14:32,874 - Epoch: [54][  210/  246]    Overall Loss 0.338987    Objective Loss 0.338987                                        LR 0.000036    Time 0.020010    
2023-01-06 16:14:33,034 - Epoch: [54][  220/  246]    Overall Loss 0.338933    Objective Loss 0.338933                                        LR 0.000036    Time 0.019826    
2023-01-06 16:14:33,195 - Epoch: [54][  230/  246]    Overall Loss 0.338103    Objective Loss 0.338103                                        LR 0.000036    Time 0.019659    
2023-01-06 16:14:33,369 - Epoch: [54][  240/  246]    Overall Loss 0.338317    Objective Loss 0.338317                                        LR 0.000036    Time 0.019565    
2023-01-06 16:14:33,446 - Epoch: [54][  246/  246]    Overall Loss 0.337540    Objective Loss 0.337540    Top1 90.909091    LR 0.000036    Time 0.019400    
2023-01-06 16:14:33,585 - --- validate (epoch=54)-----------
2023-01-06 16:14:33,585 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:34,014 - Epoch: [54][   10/   28]    Loss 0.347324    Top1 87.070312    
2023-01-06 16:14:34,116 - Epoch: [54][   20/   28]    Loss 0.342569    Top1 87.148438    
2023-01-06 16:14:34,173 - Epoch: [54][   28/   28]    Loss 0.338641    Top1 87.331806    
2023-01-06 16:14:34,314 - ==> Top1: 87.332    Loss: 0.339

2023-01-06 16:14:34,314 - ==> Confusion:
[[ 126    4  309]
 [   8  114  480]
 [  44   40 5861]]

2023-01-06 16:14:34,316 - ==> Best [Top1: 87.704   Sparsity:0.00   Params: 46192 on epoch: 51]
2023-01-06 16:14:34,316 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:34,320 - 

2023-01-06 16:14:34,320 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:34,872 - Epoch: [55][   10/  246]    Overall Loss 0.321774    Objective Loss 0.321774                                        LR 0.000036    Time 0.055089    
2023-01-06 16:14:35,046 - Epoch: [55][   20/  246]    Overall Loss 0.329042    Objective Loss 0.329042                                        LR 0.000036    Time 0.036154    
2023-01-06 16:14:35,215 - Epoch: [55][   30/  246]    Overall Loss 0.325148    Objective Loss 0.325148                                        LR 0.000036    Time 0.029714    
2023-01-06 16:14:35,383 - Epoch: [55][   40/  246]    Overall Loss 0.330486    Objective Loss 0.330486                                        LR 0.000036    Time 0.026486    
2023-01-06 16:14:35,550 - Epoch: [55][   50/  246]    Overall Loss 0.327109    Objective Loss 0.327109                                        LR 0.000036    Time 0.024519    
2023-01-06 16:14:35,718 - Epoch: [55][   60/  246]    Overall Loss 0.327854    Objective Loss 0.327854                                        LR 0.000036    Time 0.023218    
2023-01-06 16:14:35,897 - Epoch: [55][   70/  246]    Overall Loss 0.328612    Objective Loss 0.328612                                        LR 0.000036    Time 0.022449    
2023-01-06 16:14:36,075 - Epoch: [55][   80/  246]    Overall Loss 0.331075    Objective Loss 0.331075                                        LR 0.000036    Time 0.021865    
2023-01-06 16:14:36,240 - Epoch: [55][   90/  246]    Overall Loss 0.332761    Objective Loss 0.332761                                        LR 0.000036    Time 0.021270    
2023-01-06 16:14:36,383 - Epoch: [55][  100/  246]    Overall Loss 0.334006    Objective Loss 0.334006                                        LR 0.000036    Time 0.020566    
2023-01-06 16:14:36,527 - Epoch: [55][  110/  246]    Overall Loss 0.337235    Objective Loss 0.337235                                        LR 0.000036    Time 0.020002    
2023-01-06 16:14:36,683 - Epoch: [55][  120/  246]    Overall Loss 0.338507    Objective Loss 0.338507                                        LR 0.000036    Time 0.019634    
2023-01-06 16:14:36,837 - Epoch: [55][  130/  246]    Overall Loss 0.337437    Objective Loss 0.337437                                        LR 0.000036    Time 0.019305    
2023-01-06 16:14:37,003 - Epoch: [55][  140/  246]    Overall Loss 0.338402    Objective Loss 0.338402                                        LR 0.000036    Time 0.019106    
2023-01-06 16:14:37,173 - Epoch: [55][  150/  246]    Overall Loss 0.337522    Objective Loss 0.337522                                        LR 0.000036    Time 0.018963    
2023-01-06 16:14:37,350 - Epoch: [55][  160/  246]    Overall Loss 0.337406    Objective Loss 0.337406                                        LR 0.000036    Time 0.018880    
2023-01-06 16:14:37,524 - Epoch: [55][  170/  246]    Overall Loss 0.337692    Objective Loss 0.337692                                        LR 0.000036    Time 0.018793    
2023-01-06 16:14:37,690 - Epoch: [55][  180/  246]    Overall Loss 0.338408    Objective Loss 0.338408                                        LR 0.000036    Time 0.018669    
2023-01-06 16:14:37,851 - Epoch: [55][  190/  246]    Overall Loss 0.337525    Objective Loss 0.337525                                        LR 0.000036    Time 0.018533    
2023-01-06 16:14:38,029 - Epoch: [55][  200/  246]    Overall Loss 0.337252    Objective Loss 0.337252                                        LR 0.000036    Time 0.018493    
2023-01-06 16:14:38,204 - Epoch: [55][  210/  246]    Overall Loss 0.337374    Objective Loss 0.337374                                        LR 0.000036    Time 0.018442    
2023-01-06 16:14:38,382 - Epoch: [55][  220/  246]    Overall Loss 0.336461    Objective Loss 0.336461                                        LR 0.000036    Time 0.018411    
2023-01-06 16:14:38,549 - Epoch: [55][  230/  246]    Overall Loss 0.336460    Objective Loss 0.336460                                        LR 0.000036    Time 0.018337    
2023-01-06 16:14:38,721 - Epoch: [55][  240/  246]    Overall Loss 0.336632    Objective Loss 0.336632                                        LR 0.000036    Time 0.018290    
2023-01-06 16:14:38,796 - Epoch: [55][  246/  246]    Overall Loss 0.337528    Objective Loss 0.337528    Top1 86.842105    LR 0.000036    Time 0.018145    
2023-01-06 16:14:38,942 - --- validate (epoch=55)-----------
2023-01-06 16:14:38,942 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:39,385 - Epoch: [55][   10/   28]    Loss 0.333925    Top1 88.046875    
2023-01-06 16:14:39,486 - Epoch: [55][   20/   28]    Loss 0.333317    Top1 88.027344    
2023-01-06 16:14:39,544 - Epoch: [55][   28/   28]    Loss 0.338389    Top1 87.732608    
2023-01-06 16:14:39,681 - ==> Top1: 87.733    Loss: 0.338

2023-01-06 16:14:39,681 - ==> Confusion:
[[ 195    6  238]
 [  11  147  444]
 [  94   64 5787]]

2023-01-06 16:14:39,682 - ==> Best [Top1: 87.733   Sparsity:0.00   Params: 46192 on epoch: 55]
2023-01-06 16:14:39,682 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:39,687 - 

2023-01-06 16:14:39,687 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:40,392 - Epoch: [56][   10/  246]    Overall Loss 0.329433    Objective Loss 0.329433                                        LR 0.000036    Time 0.070385    
2023-01-06 16:14:40,541 - Epoch: [56][   20/  246]    Overall Loss 0.331919    Objective Loss 0.331919                                        LR 0.000036    Time 0.042623    
2023-01-06 16:14:40,694 - Epoch: [56][   30/  246]    Overall Loss 0.325543    Objective Loss 0.325543                                        LR 0.000036    Time 0.033506    
2023-01-06 16:14:40,857 - Epoch: [56][   40/  246]    Overall Loss 0.337556    Objective Loss 0.337556                                        LR 0.000036    Time 0.029168    
2023-01-06 16:14:41,027 - Epoch: [56][   50/  246]    Overall Loss 0.337358    Objective Loss 0.337358                                        LR 0.000036    Time 0.026730    
2023-01-06 16:14:41,208 - Epoch: [56][   60/  246]    Overall Loss 0.339645    Objective Loss 0.339645                                        LR 0.000036    Time 0.025278    
2023-01-06 16:14:41,388 - Epoch: [56][   70/  246]    Overall Loss 0.340038    Objective Loss 0.340038                                        LR 0.000036    Time 0.024237    
2023-01-06 16:14:41,565 - Epoch: [56][   80/  246]    Overall Loss 0.338618    Objective Loss 0.338618                                        LR 0.000036    Time 0.023421    
2023-01-06 16:14:41,727 - Epoch: [56][   90/  246]    Overall Loss 0.335946    Objective Loss 0.335946                                        LR 0.000036    Time 0.022610    
2023-01-06 16:14:41,874 - Epoch: [56][  100/  246]    Overall Loss 0.337187    Objective Loss 0.337187                                        LR 0.000036    Time 0.021814    
2023-01-06 16:14:42,026 - Epoch: [56][  110/  246]    Overall Loss 0.338238    Objective Loss 0.338238                                        LR 0.000036    Time 0.021212    
2023-01-06 16:14:42,177 - Epoch: [56][  120/  246]    Overall Loss 0.338433    Objective Loss 0.338433                                        LR 0.000036    Time 0.020700    
2023-01-06 16:14:42,314 - Epoch: [56][  130/  246]    Overall Loss 0.337158    Objective Loss 0.337158                                        LR 0.000036    Time 0.020155    
2023-01-06 16:14:42,454 - Epoch: [56][  140/  246]    Overall Loss 0.337117    Objective Loss 0.337117                                        LR 0.000036    Time 0.019709    
2023-01-06 16:14:42,588 - Epoch: [56][  150/  246]    Overall Loss 0.335135    Objective Loss 0.335135                                        LR 0.000036    Time 0.019291    
2023-01-06 16:14:42,732 - Epoch: [56][  160/  246]    Overall Loss 0.336251    Objective Loss 0.336251                                        LR 0.000036    Time 0.018979    
2023-01-06 16:14:42,886 - Epoch: [56][  170/  246]    Overall Loss 0.336500    Objective Loss 0.336500                                        LR 0.000036    Time 0.018766    
2023-01-06 16:14:43,076 - Epoch: [56][  180/  246]    Overall Loss 0.336867    Objective Loss 0.336867                                        LR 0.000036    Time 0.018781    
2023-01-06 16:14:43,273 - Epoch: [56][  190/  246]    Overall Loss 0.336896    Objective Loss 0.336896                                        LR 0.000036    Time 0.018823    
2023-01-06 16:14:43,468 - Epoch: [56][  200/  246]    Overall Loss 0.336204    Objective Loss 0.336204                                        LR 0.000036    Time 0.018858    
2023-01-06 16:14:43,664 - Epoch: [56][  210/  246]    Overall Loss 0.336612    Objective Loss 0.336612                                        LR 0.000036    Time 0.018893    
2023-01-06 16:14:43,861 - Epoch: [56][  220/  246]    Overall Loss 0.336439    Objective Loss 0.336439                                        LR 0.000036    Time 0.018924    
2023-01-06 16:14:44,055 - Epoch: [56][  230/  246]    Overall Loss 0.337260    Objective Loss 0.337260                                        LR 0.000036    Time 0.018943    
2023-01-06 16:14:44,255 - Epoch: [56][  240/  246]    Overall Loss 0.336538    Objective Loss 0.336538                                        LR 0.000036    Time 0.018989    
2023-01-06 16:14:44,348 - Epoch: [56][  246/  246]    Overall Loss 0.335854    Objective Loss 0.335854    Top1 91.148325    LR 0.000036    Time 0.018902    
2023-01-06 16:14:44,488 - --- validate (epoch=56)-----------
2023-01-06 16:14:44,488 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:44,914 - Epoch: [56][   10/   28]    Loss 0.322592    Top1 87.851562    
2023-01-06 16:14:45,016 - Epoch: [56][   20/   28]    Loss 0.327130    Top1 87.832031    
2023-01-06 16:14:45,074 - Epoch: [56][   28/   28]    Loss 0.332965    Top1 87.761237    
2023-01-06 16:14:45,232 - ==> Top1: 87.761    Loss: 0.333

2023-01-06 16:14:45,232 - ==> Confusion:
[[ 160    4  275]
 [   9  119  474]
 [  57   36 5852]]

2023-01-06 16:14:45,233 - ==> Best [Top1: 87.761   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:14:45,234 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:45,239 - 

2023-01-06 16:14:45,239 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:45,943 - Epoch: [57][   10/  246]    Overall Loss 0.359990    Objective Loss 0.359990                                        LR 0.000036    Time 0.070300    
2023-01-06 16:14:46,121 - Epoch: [57][   20/  246]    Overall Loss 0.339461    Objective Loss 0.339461                                        LR 0.000036    Time 0.044060    
2023-01-06 16:14:46,295 - Epoch: [57][   30/  246]    Overall Loss 0.335081    Objective Loss 0.335081                                        LR 0.000036    Time 0.035089    
2023-01-06 16:14:46,471 - Epoch: [57][   40/  246]    Overall Loss 0.337002    Objective Loss 0.337002                                        LR 0.000036    Time 0.030704    
2023-01-06 16:14:46,634 - Epoch: [57][   50/  246]    Overall Loss 0.335030    Objective Loss 0.335030                                        LR 0.000036    Time 0.027813    
2023-01-06 16:14:46,792 - Epoch: [57][   60/  246]    Overall Loss 0.332566    Objective Loss 0.332566                                        LR 0.000036    Time 0.025805    
2023-01-06 16:14:46,959 - Epoch: [57][   70/  246]    Overall Loss 0.330787    Objective Loss 0.330787                                        LR 0.000036    Time 0.024498    
2023-01-06 16:14:47,153 - Epoch: [57][   80/  246]    Overall Loss 0.333759    Objective Loss 0.333759                                        LR 0.000036    Time 0.023858    
2023-01-06 16:14:47,349 - Epoch: [57][   90/  246]    Overall Loss 0.336385    Objective Loss 0.336385                                        LR 0.000036    Time 0.023380    
2023-01-06 16:14:47,547 - Epoch: [57][  100/  246]    Overall Loss 0.336027    Objective Loss 0.336027                                        LR 0.000036    Time 0.023020    
2023-01-06 16:14:47,716 - Epoch: [57][  110/  246]    Overall Loss 0.334293    Objective Loss 0.334293                                        LR 0.000036    Time 0.022465    
2023-01-06 16:14:47,876 - Epoch: [57][  120/  246]    Overall Loss 0.334469    Objective Loss 0.334469                                        LR 0.000036    Time 0.021923    
2023-01-06 16:14:48,034 - Epoch: [57][  130/  246]    Overall Loss 0.334727    Objective Loss 0.334727                                        LR 0.000036    Time 0.021449    
2023-01-06 16:14:48,196 - Epoch: [57][  140/  246]    Overall Loss 0.335349    Objective Loss 0.335349                                        LR 0.000036    Time 0.021068    
2023-01-06 16:14:48,356 - Epoch: [57][  150/  246]    Overall Loss 0.335784    Objective Loss 0.335784                                        LR 0.000036    Time 0.020727    
2023-01-06 16:14:48,516 - Epoch: [57][  160/  246]    Overall Loss 0.334542    Objective Loss 0.334542                                        LR 0.000036    Time 0.020431    
2023-01-06 16:14:48,683 - Epoch: [57][  170/  246]    Overall Loss 0.337015    Objective Loss 0.337015                                        LR 0.000036    Time 0.020213    
2023-01-06 16:14:48,847 - Epoch: [57][  180/  246]    Overall Loss 0.339151    Objective Loss 0.339151                                        LR 0.000036    Time 0.019995    
2023-01-06 16:14:49,014 - Epoch: [57][  190/  246]    Overall Loss 0.339029    Objective Loss 0.339029                                        LR 0.000036    Time 0.019819    
2023-01-06 16:14:49,179 - Epoch: [57][  200/  246]    Overall Loss 0.338385    Objective Loss 0.338385                                        LR 0.000036    Time 0.019653    
2023-01-06 16:14:49,347 - Epoch: [57][  210/  246]    Overall Loss 0.338450    Objective Loss 0.338450                                        LR 0.000036    Time 0.019514    
2023-01-06 16:14:49,515 - Epoch: [57][  220/  246]    Overall Loss 0.337665    Objective Loss 0.337665                                        LR 0.000036    Time 0.019392    
2023-01-06 16:14:49,683 - Epoch: [57][  230/  246]    Overall Loss 0.336810    Objective Loss 0.336810                                        LR 0.000036    Time 0.019276    
2023-01-06 16:14:49,866 - Epoch: [57][  240/  246]    Overall Loss 0.335894    Objective Loss 0.335894                                        LR 0.000036    Time 0.019236    
2023-01-06 16:14:49,945 - Epoch: [57][  246/  246]    Overall Loss 0.335331    Objective Loss 0.335331    Top1 87.559809    LR 0.000036    Time 0.019085    
2023-01-06 16:14:50,120 - --- validate (epoch=57)-----------
2023-01-06 16:14:50,120 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:50,566 - Epoch: [57][   10/   28]    Loss 0.358776    Top1 86.523438    
2023-01-06 16:14:50,668 - Epoch: [57][   20/   28]    Loss 0.343971    Top1 87.246094    
2023-01-06 16:14:50,724 - Epoch: [57][   28/   28]    Loss 0.333180    Top1 87.689665    
2023-01-06 16:14:50,858 - ==> Top1: 87.690    Loss: 0.333

2023-01-06 16:14:50,858 - ==> Confusion:
[[ 160    5  274]
 [   9  141  452]
 [  61   59 5825]]

2023-01-06 16:14:50,860 - ==> Best [Top1: 87.761   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:14:50,860 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:50,864 - 

2023-01-06 16:14:50,865 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:51,398 - Epoch: [58][   10/  246]    Overall Loss 0.337006    Objective Loss 0.337006                                        LR 0.000036    Time 0.053292    
2023-01-06 16:14:51,565 - Epoch: [58][   20/  246]    Overall Loss 0.339016    Objective Loss 0.339016                                        LR 0.000036    Time 0.034957    
2023-01-06 16:14:51,737 - Epoch: [58][   30/  246]    Overall Loss 0.337400    Objective Loss 0.337400                                        LR 0.000036    Time 0.029025    
2023-01-06 16:14:51,902 - Epoch: [58][   40/  246]    Overall Loss 0.340387    Objective Loss 0.340387                                        LR 0.000036    Time 0.025873    
2023-01-06 16:14:52,075 - Epoch: [58][   50/  246]    Overall Loss 0.335313    Objective Loss 0.335313                                        LR 0.000036    Time 0.024157    
2023-01-06 16:14:52,252 - Epoch: [58][   60/  246]    Overall Loss 0.339449    Objective Loss 0.339449                                        LR 0.000036    Time 0.023081    
2023-01-06 16:14:52,426 - Epoch: [58][   70/  246]    Overall Loss 0.337264    Objective Loss 0.337264                                        LR 0.000036    Time 0.022261    
2023-01-06 16:14:52,604 - Epoch: [58][   80/  246]    Overall Loss 0.340200    Objective Loss 0.340200                                        LR 0.000036    Time 0.021701    
2023-01-06 16:14:52,780 - Epoch: [58][   90/  246]    Overall Loss 0.342170    Objective Loss 0.342170                                        LR 0.000036    Time 0.021234    
2023-01-06 16:14:52,959 - Epoch: [58][  100/  246]    Overall Loss 0.339937    Objective Loss 0.339937                                        LR 0.000036    Time 0.020896    
2023-01-06 16:14:53,138 - Epoch: [58][  110/  246]    Overall Loss 0.338055    Objective Loss 0.338055                                        LR 0.000036    Time 0.020620    
2023-01-06 16:14:53,320 - Epoch: [58][  120/  246]    Overall Loss 0.335519    Objective Loss 0.335519                                        LR 0.000036    Time 0.020414    
2023-01-06 16:14:53,495 - Epoch: [58][  130/  246]    Overall Loss 0.335457    Objective Loss 0.335457                                        LR 0.000036    Time 0.020189    
2023-01-06 16:14:53,673 - Epoch: [58][  140/  246]    Overall Loss 0.333910    Objective Loss 0.333910                                        LR 0.000036    Time 0.020019    
2023-01-06 16:14:53,849 - Epoch: [58][  150/  246]    Overall Loss 0.333075    Objective Loss 0.333075                                        LR 0.000036    Time 0.019855    
2023-01-06 16:14:54,025 - Epoch: [58][  160/  246]    Overall Loss 0.332273    Objective Loss 0.332273                                        LR 0.000036    Time 0.019710    
2023-01-06 16:14:54,193 - Epoch: [58][  170/  246]    Overall Loss 0.331550    Objective Loss 0.331550                                        LR 0.000036    Time 0.019536    
2023-01-06 16:14:54,359 - Epoch: [58][  180/  246]    Overall Loss 0.332009    Objective Loss 0.332009                                        LR 0.000036    Time 0.019374    
2023-01-06 16:14:54,526 - Epoch: [58][  190/  246]    Overall Loss 0.333161    Objective Loss 0.333161                                        LR 0.000036    Time 0.019229    
2023-01-06 16:14:54,694 - Epoch: [58][  200/  246]    Overall Loss 0.333441    Objective Loss 0.333441                                        LR 0.000036    Time 0.019104    
2023-01-06 16:14:54,861 - Epoch: [58][  210/  246]    Overall Loss 0.334116    Objective Loss 0.334116                                        LR 0.000036    Time 0.018989    
2023-01-06 16:14:55,029 - Epoch: [58][  220/  246]    Overall Loss 0.334943    Objective Loss 0.334943                                        LR 0.000036    Time 0.018888    
2023-01-06 16:14:55,197 - Epoch: [58][  230/  246]    Overall Loss 0.335280    Objective Loss 0.335280                                        LR 0.000036    Time 0.018796    
2023-01-06 16:14:55,377 - Epoch: [58][  240/  246]    Overall Loss 0.334870    Objective Loss 0.334870                                        LR 0.000036    Time 0.018759    
2023-01-06 16:14:55,458 - Epoch: [58][  246/  246]    Overall Loss 0.334545    Objective Loss 0.334545    Top1 87.559809    LR 0.000036    Time 0.018633    
2023-01-06 16:14:55,605 - --- validate (epoch=58)-----------
2023-01-06 16:14:55,605 - 6986 samples (256 per mini-batch)
2023-01-06 16:14:56,040 - Epoch: [58][   10/   28]    Loss 0.330725    Top1 87.773438    
2023-01-06 16:14:56,160 - Epoch: [58][   20/   28]    Loss 0.333197    Top1 87.636719    
2023-01-06 16:14:56,215 - Epoch: [58][   28/   28]    Loss 0.337270    Top1 87.546522    
2023-01-06 16:14:56,347 - ==> Top1: 87.547    Loss: 0.337

2023-01-06 16:14:56,347 - ==> Confusion:
[[ 125    7  307]
 [   6  138  458]
 [  37   55 5853]]

2023-01-06 16:14:56,348 - ==> Best [Top1: 87.761   Sparsity:0.00   Params: 46192 on epoch: 56]
2023-01-06 16:14:56,348 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:14:56,352 - 

2023-01-06 16:14:56,353 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:14:57,045 - Epoch: [59][   10/  246]    Overall Loss 0.329880    Objective Loss 0.329880                                        LR 0.000036    Time 0.069225    
2023-01-06 16:14:57,200 - Epoch: [59][   20/  246]    Overall Loss 0.333715    Objective Loss 0.333715                                        LR 0.000036    Time 0.042310    
2023-01-06 16:14:57,364 - Epoch: [59][   30/  246]    Overall Loss 0.332401    Objective Loss 0.332401                                        LR 0.000036    Time 0.033683    
2023-01-06 16:14:57,529 - Epoch: [59][   40/  246]    Overall Loss 0.334954    Objective Loss 0.334954                                        LR 0.000036    Time 0.029376    
2023-01-06 16:14:57,693 - Epoch: [59][   50/  246]    Overall Loss 0.332951    Objective Loss 0.332951                                        LR 0.000036    Time 0.026773    
2023-01-06 16:14:57,857 - Epoch: [59][   60/  246]    Overall Loss 0.333916    Objective Loss 0.333916                                        LR 0.000036    Time 0.025046    
2023-01-06 16:14:58,031 - Epoch: [59][   70/  246]    Overall Loss 0.332235    Objective Loss 0.332235                                        LR 0.000036    Time 0.023945    
2023-01-06 16:14:58,201 - Epoch: [59][   80/  246]    Overall Loss 0.331053    Objective Loss 0.331053                                        LR 0.000036    Time 0.023075    
2023-01-06 16:14:58,376 - Epoch: [59][   90/  246]    Overall Loss 0.333584    Objective Loss 0.333584                                        LR 0.000036    Time 0.022443    
2023-01-06 16:14:58,545 - Epoch: [59][  100/  246]    Overall Loss 0.334628    Objective Loss 0.334628                                        LR 0.000036    Time 0.021894    
2023-01-06 16:14:58,721 - Epoch: [59][  110/  246]    Overall Loss 0.337300    Objective Loss 0.337300                                        LR 0.000036    Time 0.021493    
2023-01-06 16:14:58,891 - Epoch: [59][  120/  246]    Overall Loss 0.334656    Objective Loss 0.334656                                        LR 0.000036    Time 0.021121    
2023-01-06 16:14:59,065 - Epoch: [59][  130/  246]    Overall Loss 0.334580    Objective Loss 0.334580                                        LR 0.000036    Time 0.020828    
2023-01-06 16:14:59,235 - Epoch: [59][  140/  246]    Overall Loss 0.335581    Objective Loss 0.335581                                        LR 0.000036    Time 0.020557    
2023-01-06 16:14:59,407 - Epoch: [59][  150/  246]    Overall Loss 0.335529    Objective Loss 0.335529                                        LR 0.000036    Time 0.020331    
2023-01-06 16:14:59,580 - Epoch: [59][  160/  246]    Overall Loss 0.335868    Objective Loss 0.335868                                        LR 0.000036    Time 0.020138    
2023-01-06 16:14:59,753 - Epoch: [59][  170/  246]    Overall Loss 0.334878    Objective Loss 0.334878                                        LR 0.000036    Time 0.019966    
2023-01-06 16:14:59,925 - Epoch: [59][  180/  246]    Overall Loss 0.333839    Objective Loss 0.333839                                        LR 0.000036    Time 0.019813    
2023-01-06 16:15:00,101 - Epoch: [59][  190/  246]    Overall Loss 0.333350    Objective Loss 0.333350                                        LR 0.000036    Time 0.019695    
2023-01-06 16:15:00,275 - Epoch: [59][  200/  246]    Overall Loss 0.333144    Objective Loss 0.333144                                        LR 0.000036    Time 0.019577    
2023-01-06 16:15:00,453 - Epoch: [59][  210/  246]    Overall Loss 0.333284    Objective Loss 0.333284                                        LR 0.000036    Time 0.019490    
2023-01-06 16:15:00,626 - Epoch: [59][  220/  246]    Overall Loss 0.335054    Objective Loss 0.335054                                        LR 0.000036    Time 0.019390    
2023-01-06 16:15:00,804 - Epoch: [59][  230/  246]    Overall Loss 0.334377    Objective Loss 0.334377                                        LR 0.000036    Time 0.019319    
2023-01-06 16:15:00,986 - Epoch: [59][  240/  246]    Overall Loss 0.334005    Objective Loss 0.334005                                        LR 0.000036    Time 0.019272    
2023-01-06 16:15:01,067 - Epoch: [59][  246/  246]    Overall Loss 0.334481    Objective Loss 0.334481    Top1 87.559809    LR 0.000036    Time 0.019132    
2023-01-06 16:15:01,245 - --- validate (epoch=59)-----------
2023-01-06 16:15:01,246 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:01,669 - Epoch: [59][   10/   28]    Loss 0.332974    Top1 87.734375    
2023-01-06 16:15:01,769 - Epoch: [59][   20/   28]    Loss 0.334961    Top1 87.597656    
2023-01-06 16:15:01,828 - Epoch: [59][   28/   28]    Loss 0.328590    Top1 87.761237    
2023-01-06 16:15:01,983 - ==> Top1: 87.761    Loss: 0.329

2023-01-06 16:15:01,983 - ==> Confusion:
[[ 169    6  264]
 [   8  153  441]
 [  67   69 5809]]

2023-01-06 16:15:01,985 - ==> Best [Top1: 87.761   Sparsity:0.00   Params: 46192 on epoch: 59]
2023-01-06 16:15:01,985 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:01,989 - 

2023-01-06 16:15:01,990 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:02,521 - Epoch: [60][   10/  246]    Overall Loss 0.345363    Objective Loss 0.345363                                        LR 0.000036    Time 0.053061    
2023-01-06 16:15:02,670 - Epoch: [60][   20/  246]    Overall Loss 0.333548    Objective Loss 0.333548                                        LR 0.000036    Time 0.033985    
2023-01-06 16:15:02,814 - Epoch: [60][   30/  246]    Overall Loss 0.326303    Objective Loss 0.326303                                        LR 0.000036    Time 0.027423    
2023-01-06 16:15:02,955 - Epoch: [60][   40/  246]    Overall Loss 0.330766    Objective Loss 0.330766                                        LR 0.000036    Time 0.024092    
2023-01-06 16:15:03,094 - Epoch: [60][   50/  246]    Overall Loss 0.331913    Objective Loss 0.331913                                        LR 0.000036    Time 0.022035    
2023-01-06 16:15:03,232 - Epoch: [60][   60/  246]    Overall Loss 0.333962    Objective Loss 0.333962                                        LR 0.000036    Time 0.020671    
2023-01-06 16:15:03,372 - Epoch: [60][   70/  246]    Overall Loss 0.334595    Objective Loss 0.334595                                        LR 0.000036    Time 0.019708    
2023-01-06 16:15:03,513 - Epoch: [60][   80/  246]    Overall Loss 0.334890    Objective Loss 0.334890                                        LR 0.000036    Time 0.019006    
2023-01-06 16:15:03,654 - Epoch: [60][   90/  246]    Overall Loss 0.336585    Objective Loss 0.336585                                        LR 0.000036    Time 0.018456    
2023-01-06 16:15:03,805 - Epoch: [60][  100/  246]    Overall Loss 0.336389    Objective Loss 0.336389                                        LR 0.000036    Time 0.018112    
2023-01-06 16:15:03,956 - Epoch: [60][  110/  246]    Overall Loss 0.335593    Objective Loss 0.335593                                        LR 0.000036    Time 0.017832    
2023-01-06 16:15:04,120 - Epoch: [60][  120/  246]    Overall Loss 0.334364    Objective Loss 0.334364                                        LR 0.000036    Time 0.017715    
2023-01-06 16:15:04,282 - Epoch: [60][  130/  246]    Overall Loss 0.332682    Objective Loss 0.332682                                        LR 0.000036    Time 0.017593    
2023-01-06 16:15:04,445 - Epoch: [60][  140/  246]    Overall Loss 0.332812    Objective Loss 0.332812                                        LR 0.000036    Time 0.017499    
2023-01-06 16:15:04,608 - Epoch: [60][  150/  246]    Overall Loss 0.332503    Objective Loss 0.332503                                        LR 0.000036    Time 0.017414    
2023-01-06 16:15:04,770 - Epoch: [60][  160/  246]    Overall Loss 0.331996    Objective Loss 0.331996                                        LR 0.000036    Time 0.017341    
2023-01-06 16:15:04,931 - Epoch: [60][  170/  246]    Overall Loss 0.330997    Objective Loss 0.330997                                        LR 0.000036    Time 0.017262    
2023-01-06 16:15:05,093 - Epoch: [60][  180/  246]    Overall Loss 0.332086    Objective Loss 0.332086                                        LR 0.000036    Time 0.017202    
2023-01-06 16:15:05,259 - Epoch: [60][  190/  246]    Overall Loss 0.332878    Objective Loss 0.332878                                        LR 0.000036    Time 0.017166    
2023-01-06 16:15:05,425 - Epoch: [60][  200/  246]    Overall Loss 0.332671    Objective Loss 0.332671                                        LR 0.000036    Time 0.017136    
2023-01-06 16:15:05,590 - Epoch: [60][  210/  246]    Overall Loss 0.332969    Objective Loss 0.332969                                        LR 0.000036    Time 0.017106    
2023-01-06 16:15:05,754 - Epoch: [60][  220/  246]    Overall Loss 0.332773    Objective Loss 0.332773                                        LR 0.000036    Time 0.017074    
2023-01-06 16:15:05,920 - Epoch: [60][  230/  246]    Overall Loss 0.332708    Objective Loss 0.332708                                        LR 0.000036    Time 0.017050    
2023-01-06 16:15:06,098 - Epoch: [60][  240/  246]    Overall Loss 0.333483    Objective Loss 0.333483                                        LR 0.000036    Time 0.017081    
2023-01-06 16:15:06,178 - Epoch: [60][  246/  246]    Overall Loss 0.333344    Objective Loss 0.333344    Top1 86.842105    LR 0.000036    Time 0.016988    
2023-01-06 16:15:06,316 - --- validate (epoch=60)-----------
2023-01-06 16:15:06,317 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:06,746 - Epoch: [60][   10/   28]    Loss 0.346535    Top1 87.460938    
2023-01-06 16:15:06,846 - Epoch: [60][   20/   28]    Loss 0.328026    Top1 87.890625    
2023-01-06 16:15:06,900 - Epoch: [60][   28/   28]    Loss 0.335576    Top1 87.804180    
2023-01-06 16:15:07,047 - ==> Top1: 87.804    Loss: 0.336

2023-01-06 16:15:07,047 - ==> Confusion:
[[ 157    4  278]
 [  10  140  452]
 [  52   56 5837]]

2023-01-06 16:15:07,048 - ==> Best [Top1: 87.804   Sparsity:0.00   Params: 46192 on epoch: 60]
2023-01-06 16:15:07,048 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:07,054 - 

2023-01-06 16:15:07,054 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:07,737 - Epoch: [61][   10/  246]    Overall Loss 0.331220    Objective Loss 0.331220                                        LR 0.000036    Time 0.068285    
2023-01-06 16:15:07,902 - Epoch: [61][   20/  246]    Overall Loss 0.333203    Objective Loss 0.333203                                        LR 0.000036    Time 0.042347    
2023-01-06 16:15:08,072 - Epoch: [61][   30/  246]    Overall Loss 0.334370    Objective Loss 0.334370                                        LR 0.000036    Time 0.033901    
2023-01-06 16:15:08,243 - Epoch: [61][   40/  246]    Overall Loss 0.325772    Objective Loss 0.325772                                        LR 0.000036    Time 0.029681    
2023-01-06 16:15:08,414 - Epoch: [61][   50/  246]    Overall Loss 0.331862    Objective Loss 0.331862                                        LR 0.000036    Time 0.027159    
2023-01-06 16:15:08,584 - Epoch: [61][   60/  246]    Overall Loss 0.332145    Objective Loss 0.332145                                        LR 0.000036    Time 0.025461    
2023-01-06 16:15:08,746 - Epoch: [61][   70/  246]    Overall Loss 0.335017    Objective Loss 0.335017                                        LR 0.000036    Time 0.024142    
2023-01-06 16:15:08,907 - Epoch: [61][   80/  246]    Overall Loss 0.333003    Objective Loss 0.333003                                        LR 0.000036    Time 0.023123    
2023-01-06 16:15:09,067 - Epoch: [61][   90/  246]    Overall Loss 0.332844    Objective Loss 0.332844                                        LR 0.000036    Time 0.022331    
2023-01-06 16:15:09,227 - Epoch: [61][  100/  246]    Overall Loss 0.331688    Objective Loss 0.331688                                        LR 0.000036    Time 0.021701    
2023-01-06 16:15:09,387 - Epoch: [61][  110/  246]    Overall Loss 0.331746    Objective Loss 0.331746                                        LR 0.000036    Time 0.021176    
2023-01-06 16:15:09,546 - Epoch: [61][  120/  246]    Overall Loss 0.331006    Objective Loss 0.331006                                        LR 0.000036    Time 0.020734    
2023-01-06 16:15:09,705 - Epoch: [61][  130/  246]    Overall Loss 0.331258    Objective Loss 0.331258                                        LR 0.000036    Time 0.020359    
2023-01-06 16:15:09,864 - Epoch: [61][  140/  246]    Overall Loss 0.331904    Objective Loss 0.331904                                        LR 0.000036    Time 0.020040    
2023-01-06 16:15:10,024 - Epoch: [61][  150/  246]    Overall Loss 0.331813    Objective Loss 0.331813                                        LR 0.000036    Time 0.019764    
2023-01-06 16:15:10,184 - Epoch: [61][  160/  246]    Overall Loss 0.330841    Objective Loss 0.330841                                        LR 0.000036    Time 0.019526    
2023-01-06 16:15:10,342 - Epoch: [61][  170/  246]    Overall Loss 0.330443    Objective Loss 0.330443                                        LR 0.000036    Time 0.019310    
2023-01-06 16:15:10,503 - Epoch: [61][  180/  246]    Overall Loss 0.331281    Objective Loss 0.331281                                        LR 0.000036    Time 0.019127    
2023-01-06 16:15:10,663 - Epoch: [61][  190/  246]    Overall Loss 0.331605    Objective Loss 0.331605                                        LR 0.000036    Time 0.018960    
2023-01-06 16:15:10,822 - Epoch: [61][  200/  246]    Overall Loss 0.332080    Objective Loss 0.332080                                        LR 0.000036    Time 0.018808    
2023-01-06 16:15:10,982 - Epoch: [61][  210/  246]    Overall Loss 0.331406    Objective Loss 0.331406                                        LR 0.000036    Time 0.018669    
2023-01-06 16:15:11,141 - Epoch: [61][  220/  246]    Overall Loss 0.330933    Objective Loss 0.330933                                        LR 0.000036    Time 0.018543    
2023-01-06 16:15:11,300 - Epoch: [61][  230/  246]    Overall Loss 0.331874    Objective Loss 0.331874                                        LR 0.000036    Time 0.018428    
2023-01-06 16:15:11,474 - Epoch: [61][  240/  246]    Overall Loss 0.332312    Objective Loss 0.332312                                        LR 0.000036    Time 0.018382    
2023-01-06 16:15:11,551 - Epoch: [61][  246/  246]    Overall Loss 0.332083    Objective Loss 0.332083    Top1 86.602871    LR 0.000036    Time 0.018249    
2023-01-06 16:15:11,744 - --- validate (epoch=61)-----------
2023-01-06 16:15:11,745 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:12,189 - Epoch: [61][   10/   28]    Loss 0.330934    Top1 87.773438    
2023-01-06 16:15:12,296 - Epoch: [61][   20/   28]    Loss 0.319841    Top1 88.164062    
2023-01-06 16:15:12,353 - Epoch: [61][   28/   28]    Loss 0.331478    Top1 87.818494    
2023-01-06 16:15:12,502 - ==> Top1: 87.818    Loss: 0.331

2023-01-06 16:15:12,502 - ==> Confusion:
[[ 159    5  275]
 [  10  140  452]
 [  56   53 5836]]

2023-01-06 16:15:12,504 - ==> Best [Top1: 87.818   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:15:12,504 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:12,509 - 

2023-01-06 16:15:12,509 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:13,178 - Epoch: [62][   10/  246]    Overall Loss 0.308702    Objective Loss 0.308702                                        LR 0.000036    Time 0.066830    
2023-01-06 16:15:13,316 - Epoch: [62][   20/  246]    Overall Loss 0.314531    Objective Loss 0.314531                                        LR 0.000036    Time 0.040307    
2023-01-06 16:15:13,467 - Epoch: [62][   30/  246]    Overall Loss 0.319482    Objective Loss 0.319482                                        LR 0.000036    Time 0.031885    
2023-01-06 16:15:13,625 - Epoch: [62][   40/  246]    Overall Loss 0.324212    Objective Loss 0.324212                                        LR 0.000036    Time 0.027848    
2023-01-06 16:15:13,784 - Epoch: [62][   50/  246]    Overall Loss 0.327836    Objective Loss 0.327836                                        LR 0.000036    Time 0.025461    
2023-01-06 16:15:13,946 - Epoch: [62][   60/  246]    Overall Loss 0.327862    Objective Loss 0.327862                                        LR 0.000036    Time 0.023907    
2023-01-06 16:15:14,107 - Epoch: [62][   70/  246]    Overall Loss 0.323616    Objective Loss 0.323616                                        LR 0.000036    Time 0.022782    
2023-01-06 16:15:14,266 - Epoch: [62][   80/  246]    Overall Loss 0.327934    Objective Loss 0.327934                                        LR 0.000036    Time 0.021923    
2023-01-06 16:15:14,425 - Epoch: [62][   90/  246]    Overall Loss 0.328333    Objective Loss 0.328333                                        LR 0.000036    Time 0.021247    
2023-01-06 16:15:14,585 - Epoch: [62][  100/  246]    Overall Loss 0.327827    Objective Loss 0.327827                                        LR 0.000036    Time 0.020720    
2023-01-06 16:15:14,754 - Epoch: [62][  110/  246]    Overall Loss 0.326052    Objective Loss 0.326052                                        LR 0.000036    Time 0.020368    
2023-01-06 16:15:14,921 - Epoch: [62][  120/  246]    Overall Loss 0.326599    Objective Loss 0.326599                                        LR 0.000036    Time 0.020064    
2023-01-06 16:15:15,091 - Epoch: [62][  130/  246]    Overall Loss 0.327715    Objective Loss 0.327715                                        LR 0.000036    Time 0.019824    
2023-01-06 16:15:15,281 - Epoch: [62][  140/  246]    Overall Loss 0.328755    Objective Loss 0.328755                                        LR 0.000036    Time 0.019765    
2023-01-06 16:15:15,471 - Epoch: [62][  150/  246]    Overall Loss 0.329799    Objective Loss 0.329799                                        LR 0.000036    Time 0.019710    
2023-01-06 16:15:15,661 - Epoch: [62][  160/  246]    Overall Loss 0.329478    Objective Loss 0.329478                                        LR 0.000036    Time 0.019660    
2023-01-06 16:15:15,852 - Epoch: [62][  170/  246]    Overall Loss 0.329215    Objective Loss 0.329215                                        LR 0.000036    Time 0.019628    
2023-01-06 16:15:16,042 - Epoch: [62][  180/  246]    Overall Loss 0.329590    Objective Loss 0.329590                                        LR 0.000036    Time 0.019590    
2023-01-06 16:15:16,232 - Epoch: [62][  190/  246]    Overall Loss 0.329531    Objective Loss 0.329531                                        LR 0.000036    Time 0.019558    
2023-01-06 16:15:16,422 - Epoch: [62][  200/  246]    Overall Loss 0.329481    Objective Loss 0.329481                                        LR 0.000036    Time 0.019527    
2023-01-06 16:15:16,613 - Epoch: [62][  210/  246]    Overall Loss 0.329814    Objective Loss 0.329814                                        LR 0.000036    Time 0.019504    
2023-01-06 16:15:16,792 - Epoch: [62][  220/  246]    Overall Loss 0.330821    Objective Loss 0.330821                                        LR 0.000036    Time 0.019430    
2023-01-06 16:15:16,968 - Epoch: [62][  230/  246]    Overall Loss 0.332053    Objective Loss 0.332053                                        LR 0.000036    Time 0.019351    
2023-01-06 16:15:17,154 - Epoch: [62][  240/  246]    Overall Loss 0.331852    Objective Loss 0.331852                                        LR 0.000036    Time 0.019315    
2023-01-06 16:15:17,234 - Epoch: [62][  246/  246]    Overall Loss 0.331652    Objective Loss 0.331652    Top1 84.688995    LR 0.000036    Time 0.019171    
2023-01-06 16:15:17,380 - --- validate (epoch=62)-----------
2023-01-06 16:15:17,380 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:17,823 - Epoch: [62][   10/   28]    Loss 0.336429    Top1 87.070312    
2023-01-06 16:15:17,923 - Epoch: [62][   20/   28]    Loss 0.333038    Top1 87.578125    
2023-01-06 16:15:17,978 - Epoch: [62][   28/   28]    Loss 0.329404    Top1 87.746922    
2023-01-06 16:15:18,139 - ==> Top1: 87.747    Loss: 0.329

2023-01-06 16:15:18,139 - ==> Confusion:
[[ 134    5  300]
 [   6  138  458]
 [  35   52 5858]]

2023-01-06 16:15:18,140 - ==> Best [Top1: 87.818   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:15:18,140 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:18,145 - 

2023-01-06 16:15:18,145 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:18,678 - Epoch: [63][   10/  246]    Overall Loss 0.332178    Objective Loss 0.332178                                        LR 0.000036    Time 0.053256    
2023-01-06 16:15:18,841 - Epoch: [63][   20/  246]    Overall Loss 0.338889    Objective Loss 0.338889                                        LR 0.000036    Time 0.034764    
2023-01-06 16:15:19,009 - Epoch: [63][   30/  246]    Overall Loss 0.344115    Objective Loss 0.344115                                        LR 0.000036    Time 0.028760    
2023-01-06 16:15:19,172 - Epoch: [63][   40/  246]    Overall Loss 0.340000    Objective Loss 0.340000                                        LR 0.000036    Time 0.025609    
2023-01-06 16:15:19,339 - Epoch: [63][   50/  246]    Overall Loss 0.339721    Objective Loss 0.339721                                        LR 0.000036    Time 0.023812    
2023-01-06 16:15:19,502 - Epoch: [63][   60/  246]    Overall Loss 0.339948    Objective Loss 0.339948                                        LR 0.000036    Time 0.022549    
2023-01-06 16:15:19,679 - Epoch: [63][   70/  246]    Overall Loss 0.340072    Objective Loss 0.340072                                        LR 0.000036    Time 0.021846    
2023-01-06 16:15:19,852 - Epoch: [63][   80/  246]    Overall Loss 0.339150    Objective Loss 0.339150                                        LR 0.000036    Time 0.021273    
2023-01-06 16:15:20,032 - Epoch: [63][   90/  246]    Overall Loss 0.338319    Objective Loss 0.338319                                        LR 0.000036    Time 0.020906    
2023-01-06 16:15:20,202 - Epoch: [63][  100/  246]    Overall Loss 0.336341    Objective Loss 0.336341                                        LR 0.000036    Time 0.020489    
2023-01-06 16:15:20,375 - Epoch: [63][  110/  246]    Overall Loss 0.334753    Objective Loss 0.334753                                        LR 0.000036    Time 0.020199    
2023-01-06 16:15:20,544 - Epoch: [63][  120/  246]    Overall Loss 0.333337    Objective Loss 0.333337                                        LR 0.000036    Time 0.019921    
2023-01-06 16:15:20,712 - Epoch: [63][  130/  246]    Overall Loss 0.333196    Objective Loss 0.333196                                        LR 0.000036    Time 0.019678    
2023-01-06 16:15:20,875 - Epoch: [63][  140/  246]    Overall Loss 0.334062    Objective Loss 0.334062                                        LR 0.000036    Time 0.019435    
2023-01-06 16:15:21,035 - Epoch: [63][  150/  246]    Overall Loss 0.334376    Objective Loss 0.334376                                        LR 0.000036    Time 0.019207    
2023-01-06 16:15:21,196 - Epoch: [63][  160/  246]    Overall Loss 0.333702    Objective Loss 0.333702                                        LR 0.000036    Time 0.019009    
2023-01-06 16:15:21,357 - Epoch: [63][  170/  246]    Overall Loss 0.333666    Objective Loss 0.333666                                        LR 0.000036    Time 0.018835    
2023-01-06 16:15:21,516 - Epoch: [63][  180/  246]    Overall Loss 0.332836    Objective Loss 0.332836                                        LR 0.000036    Time 0.018672    
2023-01-06 16:15:21,678 - Epoch: [63][  190/  246]    Overall Loss 0.331300    Objective Loss 0.331300                                        LR 0.000036    Time 0.018539    
2023-01-06 16:15:21,837 - Epoch: [63][  200/  246]    Overall Loss 0.330896    Objective Loss 0.330896                                        LR 0.000036    Time 0.018403    
2023-01-06 16:15:21,997 - Epoch: [63][  210/  246]    Overall Loss 0.331348    Objective Loss 0.331348                                        LR 0.000036    Time 0.018291    
2023-01-06 16:15:22,156 - Epoch: [63][  220/  246]    Overall Loss 0.332103    Objective Loss 0.332103                                        LR 0.000036    Time 0.018181    
2023-01-06 16:15:22,318 - Epoch: [63][  230/  246]    Overall Loss 0.331298    Objective Loss 0.331298                                        LR 0.000036    Time 0.018092    
2023-01-06 16:15:22,492 - Epoch: [63][  240/  246]    Overall Loss 0.330820    Objective Loss 0.330820                                        LR 0.000036    Time 0.018063    
2023-01-06 16:15:22,573 - Epoch: [63][  246/  246]    Overall Loss 0.330548    Objective Loss 0.330548    Top1 89.712919    LR 0.000036    Time 0.017949    
2023-01-06 16:15:22,713 - --- validate (epoch=63)-----------
2023-01-06 16:15:22,713 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:23,152 - Epoch: [63][   10/   28]    Loss 0.333528    Top1 87.773438    
2023-01-06 16:15:23,261 - Epoch: [63][   20/   28]    Loss 0.334729    Top1 87.734375    
2023-01-06 16:15:23,315 - Epoch: [63][   28/   28]    Loss 0.332711    Top1 87.661036    
2023-01-06 16:15:23,454 - ==> Top1: 87.661    Loss: 0.333

2023-01-06 16:15:23,454 - ==> Confusion:
[[ 184    5  250]
 [  11  132  459]
 [  83   54 5808]]

2023-01-06 16:15:23,455 - ==> Best [Top1: 87.818   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:15:23,455 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:23,460 - 

2023-01-06 16:15:23,460 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:24,158 - Epoch: [64][   10/  246]    Overall Loss 0.323554    Objective Loss 0.323554                                        LR 0.000036    Time 0.069711    
2023-01-06 16:15:24,329 - Epoch: [64][   20/  246]    Overall Loss 0.322654    Objective Loss 0.322654                                        LR 0.000036    Time 0.043403    
2023-01-06 16:15:24,500 - Epoch: [64][   30/  246]    Overall Loss 0.322625    Objective Loss 0.322625                                        LR 0.000036    Time 0.034585    
2023-01-06 16:15:24,676 - Epoch: [64][   40/  246]    Overall Loss 0.319932    Objective Loss 0.319932                                        LR 0.000036    Time 0.030341    
2023-01-06 16:15:24,843 - Epoch: [64][   50/  246]    Overall Loss 0.325100    Objective Loss 0.325100                                        LR 0.000036    Time 0.027610    
2023-01-06 16:15:25,020 - Epoch: [64][   60/  246]    Overall Loss 0.327536    Objective Loss 0.327536                                        LR 0.000036    Time 0.025943    
2023-01-06 16:15:25,191 - Epoch: [64][   70/  246]    Overall Loss 0.330501    Objective Loss 0.330501                                        LR 0.000036    Time 0.024657    
2023-01-06 16:15:25,373 - Epoch: [64][   80/  246]    Overall Loss 0.332543    Objective Loss 0.332543                                        LR 0.000036    Time 0.023845    
2023-01-06 16:15:25,544 - Epoch: [64][   90/  246]    Overall Loss 0.332629    Objective Loss 0.332629                                        LR 0.000036    Time 0.023092    
2023-01-06 16:15:25,727 - Epoch: [64][  100/  246]    Overall Loss 0.332624    Objective Loss 0.332624                                        LR 0.000036    Time 0.022614    
2023-01-06 16:15:25,899 - Epoch: [64][  110/  246]    Overall Loss 0.333528    Objective Loss 0.333528                                        LR 0.000036    Time 0.022118    
2023-01-06 16:15:26,078 - Epoch: [64][  120/  246]    Overall Loss 0.334311    Objective Loss 0.334311                                        LR 0.000036    Time 0.021764    
2023-01-06 16:15:26,256 - Epoch: [64][  130/  246]    Overall Loss 0.333269    Objective Loss 0.333269                                        LR 0.000036    Time 0.021456    
2023-01-06 16:15:26,434 - Epoch: [64][  140/  246]    Overall Loss 0.332630    Objective Loss 0.332630                                        LR 0.000036    Time 0.021188    
2023-01-06 16:15:26,612 - Epoch: [64][  150/  246]    Overall Loss 0.331870    Objective Loss 0.331870                                        LR 0.000036    Time 0.020964    
2023-01-06 16:15:26,784 - Epoch: [64][  160/  246]    Overall Loss 0.331867    Objective Loss 0.331867                                        LR 0.000036    Time 0.020723    
2023-01-06 16:15:26,953 - Epoch: [64][  170/  246]    Overall Loss 0.332196    Objective Loss 0.332196                                        LR 0.000036    Time 0.020494    
2023-01-06 16:15:27,125 - Epoch: [64][  180/  246]    Overall Loss 0.331749    Objective Loss 0.331749                                        LR 0.000036    Time 0.020313    
2023-01-06 16:15:27,285 - Epoch: [64][  190/  246]    Overall Loss 0.330751    Objective Loss 0.330751                                        LR 0.000036    Time 0.020084    
2023-01-06 16:15:27,444 - Epoch: [64][  200/  246]    Overall Loss 0.331528    Objective Loss 0.331528                                        LR 0.000036    Time 0.019870    
2023-01-06 16:15:27,603 - Epoch: [64][  210/  246]    Overall Loss 0.330619    Objective Loss 0.330619                                        LR 0.000036    Time 0.019680    
2023-01-06 16:15:27,763 - Epoch: [64][  220/  246]    Overall Loss 0.331243    Objective Loss 0.331243                                        LR 0.000036    Time 0.019513    
2023-01-06 16:15:27,928 - Epoch: [64][  230/  246]    Overall Loss 0.331433    Objective Loss 0.331433                                        LR 0.000036    Time 0.019379    
2023-01-06 16:15:28,101 - Epoch: [64][  240/  246]    Overall Loss 0.331350    Objective Loss 0.331350                                        LR 0.000036    Time 0.019293    
2023-01-06 16:15:28,179 - Epoch: [64][  246/  246]    Overall Loss 0.330487    Objective Loss 0.330487    Top1 89.952153    LR 0.000036    Time 0.019139    
2023-01-06 16:15:28,316 - --- validate (epoch=64)-----------
2023-01-06 16:15:28,317 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:28,756 - Epoch: [64][   10/   28]    Loss 0.345289    Top1 86.796875    
2023-01-06 16:15:28,858 - Epoch: [64][   20/   28]    Loss 0.334358    Top1 87.519531    
2023-01-06 16:15:28,914 - Epoch: [64][   28/   28]    Loss 0.329041    Top1 87.689665    
2023-01-06 16:15:29,062 - ==> Top1: 87.690    Loss: 0.329

2023-01-06 16:15:29,062 - ==> Confusion:
[[ 178    7  254]
 [   9  144  449]
 [  85   56 5804]]

2023-01-06 16:15:29,063 - ==> Best [Top1: 87.818   Sparsity:0.00   Params: 46192 on epoch: 61]
2023-01-06 16:15:29,064 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:29,068 - 

2023-01-06 16:15:29,068 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:29,607 - Epoch: [65][   10/  246]    Overall Loss 0.346541    Objective Loss 0.346541                                        LR 0.000036    Time 0.053835    
2023-01-06 16:15:29,750 - Epoch: [65][   20/  246]    Overall Loss 0.340576    Objective Loss 0.340576                                        LR 0.000036    Time 0.034041    
2023-01-06 16:15:29,885 - Epoch: [65][   30/  246]    Overall Loss 0.340184    Objective Loss 0.340184                                        LR 0.000036    Time 0.027185    
2023-01-06 16:15:30,029 - Epoch: [65][   40/  246]    Overall Loss 0.329702    Objective Loss 0.329702                                        LR 0.000036    Time 0.023979    
2023-01-06 16:15:30,200 - Epoch: [65][   50/  246]    Overall Loss 0.328418    Objective Loss 0.328418                                        LR 0.000036    Time 0.022586    
2023-01-06 16:15:30,341 - Epoch: [65][   60/  246]    Overall Loss 0.329515    Objective Loss 0.329515                                        LR 0.000036    Time 0.021168    
2023-01-06 16:15:30,482 - Epoch: [65][   70/  246]    Overall Loss 0.329896    Objective Loss 0.329896                                        LR 0.000036    Time 0.020155    
2023-01-06 16:15:30,623 - Epoch: [65][   80/  246]    Overall Loss 0.331810    Objective Loss 0.331810                                        LR 0.000036    Time 0.019398    
2023-01-06 16:15:30,762 - Epoch: [65][   90/  246]    Overall Loss 0.329320    Objective Loss 0.329320                                        LR 0.000036    Time 0.018786    
2023-01-06 16:15:30,902 - Epoch: [65][  100/  246]    Overall Loss 0.329089    Objective Loss 0.329089                                        LR 0.000036    Time 0.018305    
2023-01-06 16:15:31,041 - Epoch: [65][  110/  246]    Overall Loss 0.329555    Objective Loss 0.329555                                        LR 0.000036    Time 0.017896    
2023-01-06 16:15:31,180 - Epoch: [65][  120/  246]    Overall Loss 0.329413    Objective Loss 0.329413                                        LR 0.000036    Time 0.017559    
2023-01-06 16:15:31,319 - Epoch: [65][  130/  246]    Overall Loss 0.328505    Objective Loss 0.328505                                        LR 0.000036    Time 0.017276    
2023-01-06 16:15:31,458 - Epoch: [65][  140/  246]    Overall Loss 0.328701    Objective Loss 0.328701                                        LR 0.000036    Time 0.017034    
2023-01-06 16:15:31,597 - Epoch: [65][  150/  246]    Overall Loss 0.329880    Objective Loss 0.329880                                        LR 0.000036    Time 0.016824    
2023-01-06 16:15:31,736 - Epoch: [65][  160/  246]    Overall Loss 0.329923    Objective Loss 0.329923                                        LR 0.000036    Time 0.016638    
2023-01-06 16:15:31,888 - Epoch: [65][  170/  246]    Overall Loss 0.330109    Objective Loss 0.330109                                        LR 0.000036    Time 0.016553    
2023-01-06 16:15:32,040 - Epoch: [65][  180/  246]    Overall Loss 0.330589    Objective Loss 0.330589                                        LR 0.000036    Time 0.016475    
2023-01-06 16:15:32,191 - Epoch: [65][  190/  246]    Overall Loss 0.329832    Objective Loss 0.329832                                        LR 0.000036    Time 0.016390    
2023-01-06 16:15:32,348 - Epoch: [65][  200/  246]    Overall Loss 0.329647    Objective Loss 0.329647                                        LR 0.000036    Time 0.016357    
2023-01-06 16:15:32,499 - Epoch: [65][  210/  246]    Overall Loss 0.328623    Objective Loss 0.328623                                        LR 0.000036    Time 0.016291    
2023-01-06 16:15:32,657 - Epoch: [65][  220/  246]    Overall Loss 0.328909    Objective Loss 0.328909                                        LR 0.000036    Time 0.016263    
2023-01-06 16:15:32,808 - Epoch: [65][  230/  246]    Overall Loss 0.328850    Objective Loss 0.328850                                        LR 0.000036    Time 0.016205    
2023-01-06 16:15:32,981 - Epoch: [65][  240/  246]    Overall Loss 0.328588    Objective Loss 0.328588                                        LR 0.000036    Time 0.016250    
2023-01-06 16:15:33,058 - Epoch: [65][  246/  246]    Overall Loss 0.328682    Objective Loss 0.328682    Top1 88.038278    LR 0.000036    Time 0.016164    
2023-01-06 16:15:33,191 - --- validate (epoch=65)-----------
2023-01-06 16:15:33,191 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:33,646 - Epoch: [65][   10/   28]    Loss 0.336396    Top1 87.734375    
2023-01-06 16:15:33,764 - Epoch: [65][   20/   28]    Loss 0.339115    Top1 87.519531    
2023-01-06 16:15:33,819 - Epoch: [65][   28/   28]    Loss 0.329146    Top1 87.875752    
2023-01-06 16:15:33,959 - ==> Top1: 87.876    Loss: 0.329

2023-01-06 16:15:33,960 - ==> Confusion:
[[ 201    7  231]
 [  15  186  401]
 [ 105   88 5752]]

2023-01-06 16:15:33,961 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 46192 on epoch: 65]
2023-01-06 16:15:33,961 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:33,966 - 

2023-01-06 16:15:33,966 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:34,666 - Epoch: [66][   10/  246]    Overall Loss 0.314431    Objective Loss 0.314431                                        LR 0.000036    Time 0.069888    
2023-01-06 16:15:34,844 - Epoch: [66][   20/  246]    Overall Loss 0.320967    Objective Loss 0.320967                                        LR 0.000036    Time 0.043835    
2023-01-06 16:15:35,019 - Epoch: [66][   30/  246]    Overall Loss 0.321976    Objective Loss 0.321976                                        LR 0.000036    Time 0.035040    
2023-01-06 16:15:35,198 - Epoch: [66][   40/  246]    Overall Loss 0.323897    Objective Loss 0.323897                                        LR 0.000036    Time 0.030738    
2023-01-06 16:15:35,357 - Epoch: [66][   50/  246]    Overall Loss 0.326243    Objective Loss 0.326243                                        LR 0.000036    Time 0.027740    
2023-01-06 16:15:35,527 - Epoch: [66][   60/  246]    Overall Loss 0.327844    Objective Loss 0.327844                                        LR 0.000036    Time 0.025939    
2023-01-06 16:15:35,697 - Epoch: [66][   70/  246]    Overall Loss 0.326711    Objective Loss 0.326711                                        LR 0.000036    Time 0.024659    
2023-01-06 16:15:35,863 - Epoch: [66][   80/  246]    Overall Loss 0.328162    Objective Loss 0.328162                                        LR 0.000036    Time 0.023651    
2023-01-06 16:15:36,034 - Epoch: [66][   90/  246]    Overall Loss 0.328694    Objective Loss 0.328694                                        LR 0.000036    Time 0.022916    
2023-01-06 16:15:36,202 - Epoch: [66][  100/  246]    Overall Loss 0.327300    Objective Loss 0.327300                                        LR 0.000036    Time 0.022296    
2023-01-06 16:15:36,360 - Epoch: [66][  110/  246]    Overall Loss 0.325662    Objective Loss 0.325662                                        LR 0.000036    Time 0.021701    
2023-01-06 16:15:36,513 - Epoch: [66][  120/  246]    Overall Loss 0.327145    Objective Loss 0.327145                                        LR 0.000036    Time 0.021167    
2023-01-06 16:15:36,684 - Epoch: [66][  130/  246]    Overall Loss 0.326934    Objective Loss 0.326934                                        LR 0.000036    Time 0.020851    
2023-01-06 16:15:36,847 - Epoch: [66][  140/  246]    Overall Loss 0.327005    Objective Loss 0.327005                                        LR 0.000036    Time 0.020527    
2023-01-06 16:15:37,029 - Epoch: [66][  150/  246]    Overall Loss 0.327498    Objective Loss 0.327498                                        LR 0.000036    Time 0.020367    
2023-01-06 16:15:37,228 - Epoch: [66][  160/  246]    Overall Loss 0.328279    Objective Loss 0.328279                                        LR 0.000036    Time 0.020337    
2023-01-06 16:15:37,417 - Epoch: [66][  170/  246]    Overall Loss 0.327901    Objective Loss 0.327901                                        LR 0.000036    Time 0.020246    
2023-01-06 16:15:37,594 - Epoch: [66][  180/  246]    Overall Loss 0.328855    Objective Loss 0.328855                                        LR 0.000036    Time 0.020103    
2023-01-06 16:15:37,773 - Epoch: [66][  190/  246]    Overall Loss 0.329098    Objective Loss 0.329098                                        LR 0.000036    Time 0.019985    
2023-01-06 16:15:37,955 - Epoch: [66][  200/  246]    Overall Loss 0.328700    Objective Loss 0.328700                                        LR 0.000036    Time 0.019894    
2023-01-06 16:15:38,148 - Epoch: [66][  210/  246]    Overall Loss 0.328597    Objective Loss 0.328597                                        LR 0.000036    Time 0.019866    
2023-01-06 16:15:38,340 - Epoch: [66][  220/  246]    Overall Loss 0.328499    Objective Loss 0.328499                                        LR 0.000036    Time 0.019834    
2023-01-06 16:15:38,535 - Epoch: [66][  230/  246]    Overall Loss 0.328889    Objective Loss 0.328889                                        LR 0.000036    Time 0.019818    
2023-01-06 16:15:38,738 - Epoch: [66][  240/  246]    Overall Loss 0.329794    Objective Loss 0.329794                                        LR 0.000036    Time 0.019837    
2023-01-06 16:15:38,818 - Epoch: [66][  246/  246]    Overall Loss 0.329261    Objective Loss 0.329261    Top1 88.277512    LR 0.000036    Time 0.019677    
2023-01-06 16:15:38,953 - --- validate (epoch=66)-----------
2023-01-06 16:15:38,953 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:39,398 - Epoch: [66][   10/   28]    Loss 0.308595    Top1 88.554688    
2023-01-06 16:15:39,498 - Epoch: [66][   20/   28]    Loss 0.327095    Top1 87.812500    
2023-01-06 16:15:39,553 - Epoch: [66][   28/   28]    Loss 0.330928    Top1 87.632408    
2023-01-06 16:15:39,692 - ==> Top1: 87.632    Loss: 0.331

2023-01-06 16:15:39,692 - ==> Confusion:
[[ 134    6  299]
 [   7  129  466]
 [  40   46 5859]]

2023-01-06 16:15:39,693 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 46192 on epoch: 65]
2023-01-06 16:15:39,693 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:39,698 - 

2023-01-06 16:15:39,698 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:40,236 - Epoch: [67][   10/  246]    Overall Loss 0.320004    Objective Loss 0.320004                                        LR 0.000036    Time 0.053721    
2023-01-06 16:15:40,400 - Epoch: [67][   20/  246]    Overall Loss 0.321099    Objective Loss 0.321099                                        LR 0.000036    Time 0.035041    
2023-01-06 16:15:40,566 - Epoch: [67][   30/  246]    Overall Loss 0.325965    Objective Loss 0.325965                                        LR 0.000036    Time 0.028879    
2023-01-06 16:15:40,732 - Epoch: [67][   40/  246]    Overall Loss 0.326096    Objective Loss 0.326096                                        LR 0.000036    Time 0.025813    
2023-01-06 16:15:40,904 - Epoch: [67][   50/  246]    Overall Loss 0.325314    Objective Loss 0.325314                                        LR 0.000036    Time 0.024069    
2023-01-06 16:15:41,074 - Epoch: [67][   60/  246]    Overall Loss 0.326057    Objective Loss 0.326057                                        LR 0.000036    Time 0.022856    
2023-01-06 16:15:41,254 - Epoch: [67][   70/  246]    Overall Loss 0.328385    Objective Loss 0.328385                                        LR 0.000036    Time 0.022167    
2023-01-06 16:15:41,426 - Epoch: [67][   80/  246]    Overall Loss 0.325756    Objective Loss 0.325756                                        LR 0.000036    Time 0.021541    
2023-01-06 16:15:41,608 - Epoch: [67][   90/  246]    Overall Loss 0.324424    Objective Loss 0.324424                                        LR 0.000036    Time 0.021161    
2023-01-06 16:15:41,779 - Epoch: [67][  100/  246]    Overall Loss 0.324626    Objective Loss 0.324626                                        LR 0.000036    Time 0.020754    
2023-01-06 16:15:41,954 - Epoch: [67][  110/  246]    Overall Loss 0.325012    Objective Loss 0.325012                                        LR 0.000036    Time 0.020448    
2023-01-06 16:15:42,124 - Epoch: [67][  120/  246]    Overall Loss 0.325039    Objective Loss 0.325039                                        LR 0.000036    Time 0.020164    
2023-01-06 16:15:42,299 - Epoch: [67][  130/  246]    Overall Loss 0.323914    Objective Loss 0.323914                                        LR 0.000036    Time 0.019953    
2023-01-06 16:15:42,469 - Epoch: [67][  140/  246]    Overall Loss 0.324148    Objective Loss 0.324148                                        LR 0.000036    Time 0.019741    
2023-01-06 16:15:42,638 - Epoch: [67][  150/  246]    Overall Loss 0.323907    Objective Loss 0.323907                                        LR 0.000036    Time 0.019545    
2023-01-06 16:15:42,803 - Epoch: [67][  160/  246]    Overall Loss 0.324940    Objective Loss 0.324940                                        LR 0.000036    Time 0.019356    
2023-01-06 16:15:42,973 - Epoch: [67][  170/  246]    Overall Loss 0.324626    Objective Loss 0.324626                                        LR 0.000036    Time 0.019214    
2023-01-06 16:15:43,138 - Epoch: [67][  180/  246]    Overall Loss 0.324593    Objective Loss 0.324593                                        LR 0.000036    Time 0.019063    
2023-01-06 16:15:43,307 - Epoch: [67][  190/  246]    Overall Loss 0.324081    Objective Loss 0.324081                                        LR 0.000036    Time 0.018945    
2023-01-06 16:15:43,471 - Epoch: [67][  200/  246]    Overall Loss 0.324521    Objective Loss 0.324521                                        LR 0.000036    Time 0.018819    
2023-01-06 16:15:43,637 - Epoch: [67][  210/  246]    Overall Loss 0.324725    Objective Loss 0.324725                                        LR 0.000036    Time 0.018709    
2023-01-06 16:15:43,796 - Epoch: [67][  220/  246]    Overall Loss 0.325740    Objective Loss 0.325740                                        LR 0.000036    Time 0.018582    
2023-01-06 16:15:43,954 - Epoch: [67][  230/  246]    Overall Loss 0.326167    Objective Loss 0.326167                                        LR 0.000036    Time 0.018457    
2023-01-06 16:15:44,128 - Epoch: [67][  240/  246]    Overall Loss 0.327047    Objective Loss 0.327047                                        LR 0.000036    Time 0.018413    
2023-01-06 16:15:44,209 - Epoch: [67][  246/  246]    Overall Loss 0.327444    Objective Loss 0.327444    Top1 88.516746    LR 0.000036    Time 0.018292    
2023-01-06 16:15:44,395 - --- validate (epoch=67)-----------
2023-01-06 16:15:44,396 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:44,840 - Epoch: [67][   10/   28]    Loss 0.324461    Top1 87.851562    
2023-01-06 16:15:44,954 - Epoch: [67][   20/   28]    Loss 0.322290    Top1 87.695312    
2023-01-06 16:15:45,008 - Epoch: [67][   28/   28]    Loss 0.322264    Top1 87.818494    
2023-01-06 16:15:45,146 - ==> Top1: 87.818    Loss: 0.322

2023-01-06 16:15:45,146 - ==> Confusion:
[[ 170    7  262]
 [   8  161  433]
 [  66   75 5804]]

2023-01-06 16:15:45,147 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 46192 on epoch: 65]
2023-01-06 16:15:45,148 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:45,152 - 

2023-01-06 16:15:45,152 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:45,818 - Epoch: [68][   10/  246]    Overall Loss 0.303151    Objective Loss 0.303151                                        LR 0.000036    Time 0.066535    
2023-01-06 16:15:45,982 - Epoch: [68][   20/  246]    Overall Loss 0.308358    Objective Loss 0.308358                                        LR 0.000036    Time 0.041436    
2023-01-06 16:15:46,148 - Epoch: [68][   30/  246]    Overall Loss 0.320853    Objective Loss 0.320853                                        LR 0.000036    Time 0.033145    
2023-01-06 16:15:46,315 - Epoch: [68][   40/  246]    Overall Loss 0.326315    Objective Loss 0.326315                                        LR 0.000036    Time 0.029027    
2023-01-06 16:15:46,488 - Epoch: [68][   50/  246]    Overall Loss 0.324875    Objective Loss 0.324875                                        LR 0.000036    Time 0.026661    
2023-01-06 16:15:46,658 - Epoch: [68][   60/  246]    Overall Loss 0.323479    Objective Loss 0.323479                                        LR 0.000036    Time 0.025012    
2023-01-06 16:15:46,838 - Epoch: [68][   70/  246]    Overall Loss 0.322724    Objective Loss 0.322724                                        LR 0.000036    Time 0.024004    
2023-01-06 16:15:47,002 - Epoch: [68][   80/  246]    Overall Loss 0.324265    Objective Loss 0.324265                                        LR 0.000036    Time 0.023060    
2023-01-06 16:15:47,179 - Epoch: [68][   90/  246]    Overall Loss 0.323807    Objective Loss 0.323807                                        LR 0.000036    Time 0.022459    
2023-01-06 16:15:47,354 - Epoch: [68][  100/  246]    Overall Loss 0.323732    Objective Loss 0.323732                                        LR 0.000036    Time 0.021953    
2023-01-06 16:15:47,532 - Epoch: [68][  110/  246]    Overall Loss 0.324216    Objective Loss 0.324216                                        LR 0.000036    Time 0.021562    
2023-01-06 16:15:47,706 - Epoch: [68][  120/  246]    Overall Loss 0.323932    Objective Loss 0.323932                                        LR 0.000036    Time 0.021213    
2023-01-06 16:15:47,888 - Epoch: [68][  130/  246]    Overall Loss 0.323476    Objective Loss 0.323476                                        LR 0.000036    Time 0.020980    
2023-01-06 16:15:48,059 - Epoch: [68][  140/  246]    Overall Loss 0.324443    Objective Loss 0.324443                                        LR 0.000036    Time 0.020701    
2023-01-06 16:15:48,238 - Epoch: [68][  150/  246]    Overall Loss 0.324565    Objective Loss 0.324565                                        LR 0.000036    Time 0.020508    
2023-01-06 16:15:48,412 - Epoch: [68][  160/  246]    Overall Loss 0.326131    Objective Loss 0.326131                                        LR 0.000036    Time 0.020312    
2023-01-06 16:15:48,595 - Epoch: [68][  170/  246]    Overall Loss 0.327917    Objective Loss 0.327917                                        LR 0.000036    Time 0.020194    
2023-01-06 16:15:48,769 - Epoch: [68][  180/  246]    Overall Loss 0.327686    Objective Loss 0.327686                                        LR 0.000036    Time 0.020038    
2023-01-06 16:15:48,952 - Epoch: [68][  190/  246]    Overall Loss 0.326747    Objective Loss 0.326747                                        LR 0.000036    Time 0.019940    
2023-01-06 16:15:49,126 - Epoch: [68][  200/  246]    Overall Loss 0.326257    Objective Loss 0.326257                                        LR 0.000036    Time 0.019811    
2023-01-06 16:15:49,308 - Epoch: [68][  210/  246]    Overall Loss 0.326978    Objective Loss 0.326978                                        LR 0.000036    Time 0.019736    
2023-01-06 16:15:49,483 - Epoch: [68][  220/  246]    Overall Loss 0.326431    Objective Loss 0.326431                                        LR 0.000036    Time 0.019630    
2023-01-06 16:15:49,666 - Epoch: [68][  230/  246]    Overall Loss 0.326552    Objective Loss 0.326552                                        LR 0.000036    Time 0.019572    
2023-01-06 16:15:49,838 - Epoch: [68][  240/  246]    Overall Loss 0.326343    Objective Loss 0.326343                                        LR 0.000036    Time 0.019472    
2023-01-06 16:15:49,916 - Epoch: [68][  246/  246]    Overall Loss 0.326633    Objective Loss 0.326633    Top1 87.320574    LR 0.000036    Time 0.019314    
2023-01-06 16:15:50,090 - --- validate (epoch=68)-----------
2023-01-06 16:15:50,090 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:50,536 - Epoch: [68][   10/   28]    Loss 0.332727    Top1 87.382812    
2023-01-06 16:15:50,650 - Epoch: [68][   20/   28]    Loss 0.328623    Top1 87.656250    
2023-01-06 16:15:50,705 - Epoch: [68][   28/   28]    Loss 0.321858    Top1 87.818494    
2023-01-06 16:15:50,835 - ==> Top1: 87.818    Loss: 0.322

2023-01-06 16:15:50,835 - ==> Confusion:
[[ 148    8  283]
 [   8  156  438]
 [  54   60 5831]]

2023-01-06 16:15:50,836 - ==> Best [Top1: 87.876   Sparsity:0.00   Params: 46192 on epoch: 65]
2023-01-06 16:15:50,836 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:50,841 - 

2023-01-06 16:15:50,841 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:51,510 - Epoch: [69][   10/  246]    Overall Loss 0.321584    Objective Loss 0.321584                                        LR 0.000036    Time 0.066866    
2023-01-06 16:15:51,656 - Epoch: [69][   20/  246]    Overall Loss 0.333254    Objective Loss 0.333254                                        LR 0.000036    Time 0.040712    
2023-01-06 16:15:51,799 - Epoch: [69][   30/  246]    Overall Loss 0.334505    Objective Loss 0.334505                                        LR 0.000036    Time 0.031889    
2023-01-06 16:15:51,937 - Epoch: [69][   40/  246]    Overall Loss 0.332883    Objective Loss 0.332883                                        LR 0.000036    Time 0.027356    
2023-01-06 16:15:52,078 - Epoch: [69][   50/  246]    Overall Loss 0.330624    Objective Loss 0.330624                                        LR 0.000036    Time 0.024695    
2023-01-06 16:15:52,234 - Epoch: [69][   60/  246]    Overall Loss 0.331154    Objective Loss 0.331154                                        LR 0.000036    Time 0.023172    
2023-01-06 16:15:52,404 - Epoch: [69][   70/  246]    Overall Loss 0.330875    Objective Loss 0.330875                                        LR 0.000036    Time 0.022295    
2023-01-06 16:15:52,577 - Epoch: [69][   80/  246]    Overall Loss 0.329674    Objective Loss 0.329674                                        LR 0.000036    Time 0.021663    
2023-01-06 16:15:52,748 - Epoch: [69][   90/  246]    Overall Loss 0.330743    Objective Loss 0.330743                                        LR 0.000036    Time 0.021146    
2023-01-06 16:15:52,919 - Epoch: [69][  100/  246]    Overall Loss 0.329466    Objective Loss 0.329466                                        LR 0.000036    Time 0.020741    
2023-01-06 16:15:53,090 - Epoch: [69][  110/  246]    Overall Loss 0.329007    Objective Loss 0.329007                                        LR 0.000036    Time 0.020408    
2023-01-06 16:15:53,264 - Epoch: [69][  120/  246]    Overall Loss 0.328979    Objective Loss 0.328979                                        LR 0.000036    Time 0.020152    
2023-01-06 16:15:53,458 - Epoch: [69][  130/  246]    Overall Loss 0.329979    Objective Loss 0.329979                                        LR 0.000036    Time 0.020094    
2023-01-06 16:15:53,656 - Epoch: [69][  140/  246]    Overall Loss 0.328573    Objective Loss 0.328573                                        LR 0.000036    Time 0.020067    
2023-01-06 16:15:53,848 - Epoch: [69][  150/  246]    Overall Loss 0.329814    Objective Loss 0.329814                                        LR 0.000036    Time 0.020011    
2023-01-06 16:15:54,054 - Epoch: [69][  160/  246]    Overall Loss 0.329276    Objective Loss 0.329276                                        LR 0.000036    Time 0.020042    
2023-01-06 16:15:54,256 - Epoch: [69][  170/  246]    Overall Loss 0.328694    Objective Loss 0.328694                                        LR 0.000036    Time 0.020051    
2023-01-06 16:15:54,467 - Epoch: [69][  180/  246]    Overall Loss 0.329530    Objective Loss 0.329530                                        LR 0.000036    Time 0.020105    
2023-01-06 16:15:54,671 - Epoch: [69][  190/  246]    Overall Loss 0.329396    Objective Loss 0.329396                                        LR 0.000036    Time 0.020117    
2023-01-06 16:15:54,882 - Epoch: [69][  200/  246]    Overall Loss 0.328710    Objective Loss 0.328710                                        LR 0.000036    Time 0.020166    
2023-01-06 16:15:55,094 - Epoch: [69][  210/  246]    Overall Loss 0.328480    Objective Loss 0.328480                                        LR 0.000036    Time 0.020214    
2023-01-06 16:15:55,308 - Epoch: [69][  220/  246]    Overall Loss 0.327667    Objective Loss 0.327667                                        LR 0.000036    Time 0.020264    
2023-01-06 16:15:55,521 - Epoch: [69][  230/  246]    Overall Loss 0.327119    Objective Loss 0.327119                                        LR 0.000036    Time 0.020304    
2023-01-06 16:15:55,713 - Epoch: [69][  240/  246]    Overall Loss 0.328201    Objective Loss 0.328201                                        LR 0.000036    Time 0.020257    
2023-01-06 16:15:55,794 - Epoch: [69][  246/  246]    Overall Loss 0.327952    Objective Loss 0.327952    Top1 87.559809    LR 0.000036    Time 0.020094    
2023-01-06 16:15:55,934 - --- validate (epoch=69)-----------
2023-01-06 16:15:55,934 - 6986 samples (256 per mini-batch)
2023-01-06 16:15:56,394 - Epoch: [69][   10/   28]    Loss 0.337480    Top1 87.734375    
2023-01-06 16:15:56,503 - Epoch: [69][   20/   28]    Loss 0.328472    Top1 88.164062    
2023-01-06 16:15:56,560 - Epoch: [69][   28/   28]    Loss 0.330278    Top1 87.947323    
2023-01-06 16:15:56,685 - ==> Top1: 87.947    Loss: 0.330

2023-01-06 16:15:56,685 - ==> Confusion:
[[ 143   10  286]
 [   8  190  404]
 [  38   96 5811]]

2023-01-06 16:15:56,686 - ==> Best [Top1: 87.947   Sparsity:0.00   Params: 46192 on epoch: 69]
2023-01-06 16:15:56,686 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:15:56,691 - 

2023-01-06 16:15:56,691 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:15:57,228 - Epoch: [70][   10/  246]    Overall Loss 0.313568    Objective Loss 0.313568                                        LR 0.000022    Time 0.053664    
2023-01-06 16:15:57,383 - Epoch: [70][   20/  246]    Overall Loss 0.330803    Objective Loss 0.330803                                        LR 0.000022    Time 0.034516    
2023-01-06 16:15:57,554 - Epoch: [70][   30/  246]    Overall Loss 0.329411    Objective Loss 0.329411                                        LR 0.000022    Time 0.028721    
2023-01-06 16:15:57,727 - Epoch: [70][   40/  246]    Overall Loss 0.326703    Objective Loss 0.326703                                        LR 0.000022    Time 0.025840    
2023-01-06 16:15:57,898 - Epoch: [70][   50/  246]    Overall Loss 0.331257    Objective Loss 0.331257                                        LR 0.000022    Time 0.024059    
2023-01-06 16:15:58,074 - Epoch: [70][   60/  246]    Overall Loss 0.330564    Objective Loss 0.330564                                        LR 0.000022    Time 0.022986    
2023-01-06 16:15:58,237 - Epoch: [70][   70/  246]    Overall Loss 0.331721    Objective Loss 0.331721                                        LR 0.000022    Time 0.022027    
2023-01-06 16:15:58,420 - Epoch: [70][   80/  246]    Overall Loss 0.330737    Objective Loss 0.330737                                        LR 0.000022    Time 0.021545    
2023-01-06 16:15:58,600 - Epoch: [70][   90/  246]    Overall Loss 0.329490    Objective Loss 0.329490                                        LR 0.000022    Time 0.021147    
2023-01-06 16:15:58,781 - Epoch: [70][  100/  246]    Overall Loss 0.328859    Objective Loss 0.328859                                        LR 0.000022    Time 0.020848    
2023-01-06 16:15:58,960 - Epoch: [70][  110/  246]    Overall Loss 0.327144    Objective Loss 0.327144                                        LR 0.000022    Time 0.020574    
2023-01-06 16:15:59,138 - Epoch: [70][  120/  246]    Overall Loss 0.326830    Objective Loss 0.326830                                        LR 0.000022    Time 0.020335    
2023-01-06 16:15:59,313 - Epoch: [70][  130/  246]    Overall Loss 0.327333    Objective Loss 0.327333                                        LR 0.000022    Time 0.020117    
2023-01-06 16:15:59,487 - Epoch: [70][  140/  246]    Overall Loss 0.326626    Objective Loss 0.326626                                        LR 0.000022    Time 0.019918    
2023-01-06 16:15:59,659 - Epoch: [70][  150/  246]    Overall Loss 0.326064    Objective Loss 0.326064                                        LR 0.000022    Time 0.019740    
2023-01-06 16:15:59,833 - Epoch: [70][  160/  246]    Overall Loss 0.325890    Objective Loss 0.325890                                        LR 0.000022    Time 0.019590    
2023-01-06 16:16:00,004 - Epoch: [70][  170/  246]    Overall Loss 0.325817    Objective Loss 0.325817                                        LR 0.000022    Time 0.019443    
2023-01-06 16:16:00,180 - Epoch: [70][  180/  246]    Overall Loss 0.324856    Objective Loss 0.324856                                        LR 0.000022    Time 0.019337    
2023-01-06 16:16:00,347 - Epoch: [70][  190/  246]    Overall Loss 0.324678    Objective Loss 0.324678                                        LR 0.000022    Time 0.019196    
2023-01-06 16:16:00,521 - Epoch: [70][  200/  246]    Overall Loss 0.324748    Objective Loss 0.324748                                        LR 0.000022    Time 0.019103    
2023-01-06 16:16:00,691 - Epoch: [70][  210/  246]    Overall Loss 0.325553    Objective Loss 0.325553                                        LR 0.000022    Time 0.019002    
2023-01-06 16:16:00,865 - Epoch: [70][  220/  246]    Overall Loss 0.325401    Objective Loss 0.325401                                        LR 0.000022    Time 0.018927    
2023-01-06 16:16:01,035 - Epoch: [70][  230/  246]    Overall Loss 0.325814    Objective Loss 0.325814                                        LR 0.000022    Time 0.018841    
2023-01-06 16:16:01,216 - Epoch: [70][  240/  246]    Overall Loss 0.325329    Objective Loss 0.325329                                        LR 0.000022    Time 0.018809    
2023-01-06 16:16:01,294 - Epoch: [70][  246/  246]    Overall Loss 0.324747    Objective Loss 0.324747    Top1 88.516746    LR 0.000022    Time 0.018667    
2023-01-06 16:16:01,443 - --- validate (epoch=70)-----------
2023-01-06 16:16:01,443 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:01,882 - Epoch: [70][   10/   28]    Loss 0.319800    Top1 88.164062    
2023-01-06 16:16:01,984 - Epoch: [70][   20/   28]    Loss 0.337348    Top1 87.324219    
2023-01-06 16:16:02,042 - Epoch: [70][   28/   28]    Loss 0.325383    Top1 87.689665    
2023-01-06 16:16:02,178 - ==> Top1: 87.690    Loss: 0.325

2023-01-06 16:16:02,179 - ==> Confusion:
[[ 153    9  277]
 [   8  136  458]
 [  54   54 5837]]

2023-01-06 16:16:02,180 - ==> Best [Top1: 87.947   Sparsity:0.00   Params: 46192 on epoch: 69]
2023-01-06 16:16:02,180 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:02,186 - 

2023-01-06 16:16:02,187 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:02,894 - Epoch: [71][   10/  246]    Overall Loss 0.313705    Objective Loss 0.313705                                        LR 0.000022    Time 0.070712    
2023-01-06 16:16:03,050 - Epoch: [71][   20/  246]    Overall Loss 0.319482    Objective Loss 0.319482                                        LR 0.000022    Time 0.043105    
2023-01-06 16:16:03,202 - Epoch: [71][   30/  246]    Overall Loss 0.320896    Objective Loss 0.320896                                        LR 0.000022    Time 0.033807    
2023-01-06 16:16:03,366 - Epoch: [71][   40/  246]    Overall Loss 0.319012    Objective Loss 0.319012                                        LR 0.000022    Time 0.029434    
2023-01-06 16:16:03,530 - Epoch: [71][   50/  246]    Overall Loss 0.320610    Objective Loss 0.320610                                        LR 0.000022    Time 0.026820    
2023-01-06 16:16:03,694 - Epoch: [71][   60/  246]    Overall Loss 0.321812    Objective Loss 0.321812                                        LR 0.000022    Time 0.025074    
2023-01-06 16:16:03,854 - Epoch: [71][   70/  246]    Overall Loss 0.325902    Objective Loss 0.325902                                        LR 0.000022    Time 0.023774    
2023-01-06 16:16:04,020 - Epoch: [71][   80/  246]    Overall Loss 0.325607    Objective Loss 0.325607                                        LR 0.000022    Time 0.022873    
2023-01-06 16:16:04,177 - Epoch: [71][   90/  246]    Overall Loss 0.324917    Objective Loss 0.324917                                        LR 0.000022    Time 0.022077    
2023-01-06 16:16:04,336 - Epoch: [71][  100/  246]    Overall Loss 0.326122    Objective Loss 0.326122                                        LR 0.000022    Time 0.021457    
2023-01-06 16:16:04,496 - Epoch: [71][  110/  246]    Overall Loss 0.326057    Objective Loss 0.326057                                        LR 0.000022    Time 0.020953    
2023-01-06 16:16:04,663 - Epoch: [71][  120/  246]    Overall Loss 0.326514    Objective Loss 0.326514                                        LR 0.000022    Time 0.020591    
2023-01-06 16:16:04,822 - Epoch: [71][  130/  246]    Overall Loss 0.326767    Objective Loss 0.326767                                        LR 0.000022    Time 0.020231    
2023-01-06 16:16:04,984 - Epoch: [71][  140/  246]    Overall Loss 0.325839    Objective Loss 0.325839                                        LR 0.000022    Time 0.019943    
2023-01-06 16:16:05,144 - Epoch: [71][  150/  246]    Overall Loss 0.325535    Objective Loss 0.325535                                        LR 0.000022    Time 0.019673    
2023-01-06 16:16:05,294 - Epoch: [71][  160/  246]    Overall Loss 0.326367    Objective Loss 0.326367                                        LR 0.000022    Time 0.019379    
2023-01-06 16:16:05,464 - Epoch: [71][  170/  246]    Overall Loss 0.326227    Objective Loss 0.326227                                        LR 0.000022    Time 0.019241    
2023-01-06 16:16:05,643 - Epoch: [71][  180/  246]    Overall Loss 0.326055    Objective Loss 0.326055                                        LR 0.000022    Time 0.019163    
2023-01-06 16:16:05,786 - Epoch: [71][  190/  246]    Overall Loss 0.324347    Objective Loss 0.324347                                        LR 0.000022    Time 0.018905    
2023-01-06 16:16:05,958 - Epoch: [71][  200/  246]    Overall Loss 0.323717    Objective Loss 0.323717                                        LR 0.000022    Time 0.018818    
2023-01-06 16:16:06,133 - Epoch: [71][  210/  246]    Overall Loss 0.324172    Objective Loss 0.324172                                        LR 0.000022    Time 0.018751    
2023-01-06 16:16:06,306 - Epoch: [71][  220/  246]    Overall Loss 0.324486    Objective Loss 0.324486                                        LR 0.000022    Time 0.018685    
2023-01-06 16:16:06,481 - Epoch: [71][  230/  246]    Overall Loss 0.323568    Objective Loss 0.323568                                        LR 0.000022    Time 0.018634    
2023-01-06 16:16:06,664 - Epoch: [71][  240/  246]    Overall Loss 0.322853    Objective Loss 0.322853                                        LR 0.000022    Time 0.018616    
2023-01-06 16:16:06,740 - Epoch: [71][  246/  246]    Overall Loss 0.322813    Objective Loss 0.322813    Top1 86.842105    LR 0.000022    Time 0.018473    
2023-01-06 16:16:06,877 - --- validate (epoch=71)-----------
2023-01-06 16:16:06,877 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:07,334 - Epoch: [71][   10/   28]    Loss 0.342835    Top1 87.187500    
2023-01-06 16:16:07,449 - Epoch: [71][   20/   28]    Loss 0.330632    Top1 87.734375    
2023-01-06 16:16:07,503 - Epoch: [71][   28/   28]    Loss 0.323209    Top1 88.018895    
2023-01-06 16:16:07,656 - ==> Top1: 88.019    Loss: 0.323

2023-01-06 16:16:07,656 - ==> Confusion:
[[ 173    8  258]
 [   9  172  421]
 [  67   74 5804]]

2023-01-06 16:16:07,657 - ==> Best [Top1: 88.019   Sparsity:0.00   Params: 46192 on epoch: 71]
2023-01-06 16:16:07,658 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:07,662 - 

2023-01-06 16:16:07,663 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:08,202 - Epoch: [72][   10/  246]    Overall Loss 0.343099    Objective Loss 0.343099                                        LR 0.000022    Time 0.053903    
2023-01-06 16:16:08,365 - Epoch: [72][   20/  246]    Overall Loss 0.343922    Objective Loss 0.343922                                        LR 0.000022    Time 0.035065    
2023-01-06 16:16:08,526 - Epoch: [72][   30/  246]    Overall Loss 0.336857    Objective Loss 0.336857                                        LR 0.000022    Time 0.028732    
2023-01-06 16:16:08,687 - Epoch: [72][   40/  246]    Overall Loss 0.332826    Objective Loss 0.332826                                        LR 0.000022    Time 0.025555    
2023-01-06 16:16:08,847 - Epoch: [72][   50/  246]    Overall Loss 0.328659    Objective Loss 0.328659                                        LR 0.000022    Time 0.023643    
2023-01-06 16:16:09,012 - Epoch: [72][   60/  246]    Overall Loss 0.332661    Objective Loss 0.332661                                        LR 0.000022    Time 0.022444    
2023-01-06 16:16:09,181 - Epoch: [72][   70/  246]    Overall Loss 0.329329    Objective Loss 0.329329                                        LR 0.000022    Time 0.021650    
2023-01-06 16:16:09,355 - Epoch: [72][   80/  246]    Overall Loss 0.328395    Objective Loss 0.328395                                        LR 0.000022    Time 0.021115    
2023-01-06 16:16:09,530 - Epoch: [72][   90/  246]    Overall Loss 0.327453    Objective Loss 0.327453                                        LR 0.000022    Time 0.020703    
2023-01-06 16:16:09,704 - Epoch: [72][  100/  246]    Overall Loss 0.325046    Objective Loss 0.325046                                        LR 0.000022    Time 0.020369    
2023-01-06 16:16:09,875 - Epoch: [72][  110/  246]    Overall Loss 0.322805    Objective Loss 0.322805                                        LR 0.000022    Time 0.020074    
2023-01-06 16:16:10,051 - Epoch: [72][  120/  246]    Overall Loss 0.325095    Objective Loss 0.325095                                        LR 0.000022    Time 0.019860    
2023-01-06 16:16:10,226 - Epoch: [72][  130/  246]    Overall Loss 0.323529    Objective Loss 0.323529                                        LR 0.000022    Time 0.019678    
2023-01-06 16:16:10,392 - Epoch: [72][  140/  246]    Overall Loss 0.323859    Objective Loss 0.323859                                        LR 0.000022    Time 0.019455    
2023-01-06 16:16:10,560 - Epoch: [72][  150/  246]    Overall Loss 0.325125    Objective Loss 0.325125                                        LR 0.000022    Time 0.019274    
2023-01-06 16:16:10,727 - Epoch: [72][  160/  246]    Overall Loss 0.324634    Objective Loss 0.324634                                        LR 0.000022    Time 0.019111    
2023-01-06 16:16:10,896 - Epoch: [72][  170/  246]    Overall Loss 0.324180    Objective Loss 0.324180                                        LR 0.000022    Time 0.018981    
2023-01-06 16:16:11,064 - Epoch: [72][  180/  246]    Overall Loss 0.323770    Objective Loss 0.323770                                        LR 0.000022    Time 0.018859    
2023-01-06 16:16:11,232 - Epoch: [72][  190/  246]    Overall Loss 0.323276    Objective Loss 0.323276                                        LR 0.000022    Time 0.018748    
2023-01-06 16:16:11,397 - Epoch: [72][  200/  246]    Overall Loss 0.323337    Objective Loss 0.323337                                        LR 0.000022    Time 0.018634    
2023-01-06 16:16:11,555 - Epoch: [72][  210/  246]    Overall Loss 0.323238    Objective Loss 0.323238                                        LR 0.000022    Time 0.018495    
2023-01-06 16:16:11,712 - Epoch: [72][  220/  246]    Overall Loss 0.324071    Objective Loss 0.324071                                        LR 0.000022    Time 0.018370    
2023-01-06 16:16:11,871 - Epoch: [72][  230/  246]    Overall Loss 0.323679    Objective Loss 0.323679                                        LR 0.000022    Time 0.018257    
2023-01-06 16:16:12,043 - Epoch: [72][  240/  246]    Overall Loss 0.323454    Objective Loss 0.323454                                        LR 0.000022    Time 0.018215    
2023-01-06 16:16:12,120 - Epoch: [72][  246/  246]    Overall Loss 0.323637    Objective Loss 0.323637    Top1 85.167464    LR 0.000022    Time 0.018083    
2023-01-06 16:16:12,248 - --- validate (epoch=72)-----------
2023-01-06 16:16:12,248 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:12,691 - Epoch: [72][   10/   28]    Loss 0.308021    Top1 89.023438    
2023-01-06 16:16:12,797 - Epoch: [72][   20/   28]    Loss 0.329973    Top1 87.968750    
2023-01-06 16:16:12,854 - Epoch: [72][   28/   28]    Loss 0.322374    Top1 87.904380    
2023-01-06 16:16:12,983 - ==> Top1: 87.904    Loss: 0.322

2023-01-06 16:16:12,983 - ==> Confusion:
[[ 156    8  275]
 [   8  162  432]
 [  54   68 5823]]

2023-01-06 16:16:12,984 - ==> Best [Top1: 88.019   Sparsity:0.00   Params: 46192 on epoch: 71]
2023-01-06 16:16:12,985 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:12,989 - 

2023-01-06 16:16:12,989 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:13,660 - Epoch: [73][   10/  246]    Overall Loss 0.310740    Objective Loss 0.310740                                        LR 0.000022    Time 0.067002    
2023-01-06 16:16:13,821 - Epoch: [73][   20/  246]    Overall Loss 0.320924    Objective Loss 0.320924                                        LR 0.000022    Time 0.041500    
2023-01-06 16:16:13,983 - Epoch: [73][   30/  246]    Overall Loss 0.326908    Objective Loss 0.326908                                        LR 0.000022    Time 0.033054    
2023-01-06 16:16:14,135 - Epoch: [73][   40/  246]    Overall Loss 0.325837    Objective Loss 0.325837                                        LR 0.000022    Time 0.028585    
2023-01-06 16:16:14,275 - Epoch: [73][   50/  246]    Overall Loss 0.326134    Objective Loss 0.326134                                        LR 0.000022    Time 0.025659    
2023-01-06 16:16:14,415 - Epoch: [73][   60/  246]    Overall Loss 0.326555    Objective Loss 0.326555                                        LR 0.000022    Time 0.023703    
2023-01-06 16:16:14,575 - Epoch: [73][   70/  246]    Overall Loss 0.324647    Objective Loss 0.324647                                        LR 0.000022    Time 0.022597    
2023-01-06 16:16:14,738 - Epoch: [73][   80/  246]    Overall Loss 0.327796    Objective Loss 0.327796                                        LR 0.000022    Time 0.021808    
2023-01-06 16:16:14,902 - Epoch: [73][   90/  246]    Overall Loss 0.326305    Objective Loss 0.326305                                        LR 0.000022    Time 0.021204    
2023-01-06 16:16:15,065 - Epoch: [73][  100/  246]    Overall Loss 0.326626    Objective Loss 0.326626                                        LR 0.000022    Time 0.020708    
2023-01-06 16:16:15,228 - Epoch: [73][  110/  246]    Overall Loss 0.327829    Objective Loss 0.327829                                        LR 0.000022    Time 0.020307    
2023-01-06 16:16:15,392 - Epoch: [73][  120/  246]    Overall Loss 0.328265    Objective Loss 0.328265                                        LR 0.000022    Time 0.019972    
2023-01-06 16:16:15,556 - Epoch: [73][  130/  246]    Overall Loss 0.327151    Objective Loss 0.327151                                        LR 0.000022    Time 0.019694    
2023-01-06 16:16:15,721 - Epoch: [73][  140/  246]    Overall Loss 0.327125    Objective Loss 0.327125                                        LR 0.000022    Time 0.019463    
2023-01-06 16:16:15,886 - Epoch: [73][  150/  246]    Overall Loss 0.327091    Objective Loss 0.327091                                        LR 0.000022    Time 0.019266    
2023-01-06 16:16:16,049 - Epoch: [73][  160/  246]    Overall Loss 0.325744    Objective Loss 0.325744                                        LR 0.000022    Time 0.019082    
2023-01-06 16:16:16,213 - Epoch: [73][  170/  246]    Overall Loss 0.324925    Objective Loss 0.324925                                        LR 0.000022    Time 0.018919    
2023-01-06 16:16:16,377 - Epoch: [73][  180/  246]    Overall Loss 0.324713    Objective Loss 0.324713                                        LR 0.000022    Time 0.018776    
2023-01-06 16:16:16,541 - Epoch: [73][  190/  246]    Overall Loss 0.323470    Objective Loss 0.323470                                        LR 0.000022    Time 0.018651    
2023-01-06 16:16:16,704 - Epoch: [73][  200/  246]    Overall Loss 0.324357    Objective Loss 0.324357                                        LR 0.000022    Time 0.018531    
2023-01-06 16:16:16,868 - Epoch: [73][  210/  246]    Overall Loss 0.323274    Objective Loss 0.323274                                        LR 0.000022    Time 0.018428    
2023-01-06 16:16:17,051 - Epoch: [73][  220/  246]    Overall Loss 0.323148    Objective Loss 0.323148                                        LR 0.000022    Time 0.018420    
2023-01-06 16:16:17,232 - Epoch: [73][  230/  246]    Overall Loss 0.323016    Objective Loss 0.323016                                        LR 0.000022    Time 0.018398    
2023-01-06 16:16:17,421 - Epoch: [73][  240/  246]    Overall Loss 0.322851    Objective Loss 0.322851                                        LR 0.000022    Time 0.018409    
2023-01-06 16:16:17,499 - Epoch: [73][  246/  246]    Overall Loss 0.323066    Objective Loss 0.323066    Top1 87.081340    LR 0.000022    Time 0.018278    
2023-01-06 16:16:17,646 - --- validate (epoch=73)-----------
2023-01-06 16:16:17,646 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:18,092 - Epoch: [73][   10/   28]    Loss 0.314687    Top1 88.046875    
2023-01-06 16:16:18,192 - Epoch: [73][   20/   28]    Loss 0.324647    Top1 88.046875    
2023-01-06 16:16:18,249 - Epoch: [73][   28/   28]    Loss 0.328452    Top1 88.047524    
2023-01-06 16:16:18,387 - ==> Top1: 88.048    Loss: 0.328

2023-01-06 16:16:18,387 - ==> Confusion:
[[ 170   10  259]
 [  11  199  392]
 [  66   97 5782]]

2023-01-06 16:16:18,389 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:18,389 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:18,394 - 

2023-01-06 16:16:18,394 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:19,065 - Epoch: [74][   10/  246]    Overall Loss 0.325123    Objective Loss 0.325123                                        LR 0.000022    Time 0.067067    
2023-01-06 16:16:19,222 - Epoch: [74][   20/  246]    Overall Loss 0.329399    Objective Loss 0.329399                                        LR 0.000022    Time 0.041377    
2023-01-06 16:16:19,400 - Epoch: [74][   30/  246]    Overall Loss 0.350841    Objective Loss 0.350841                                        LR 0.000022    Time 0.033482    
2023-01-06 16:16:19,579 - Epoch: [74][   40/  246]    Overall Loss 0.358160    Objective Loss 0.358160                                        LR 0.000022    Time 0.029585    
2023-01-06 16:16:19,766 - Epoch: [74][   50/  246]    Overall Loss 0.360916    Objective Loss 0.360916                                        LR 0.000022    Time 0.027399    
2023-01-06 16:16:19,943 - Epoch: [74][   60/  246]    Overall Loss 0.362459    Objective Loss 0.362459                                        LR 0.000022    Time 0.025776    
2023-01-06 16:16:20,132 - Epoch: [74][   70/  246]    Overall Loss 0.359157    Objective Loss 0.359157                                        LR 0.000022    Time 0.024790    
2023-01-06 16:16:20,327 - Epoch: [74][   80/  246]    Overall Loss 0.357271    Objective Loss 0.357271                                        LR 0.000022    Time 0.024126    
2023-01-06 16:16:20,525 - Epoch: [74][   90/  246]    Overall Loss 0.357710    Objective Loss 0.357710                                        LR 0.000022    Time 0.023633    
2023-01-06 16:16:20,703 - Epoch: [74][  100/  246]    Overall Loss 0.356434    Objective Loss 0.356434                                        LR 0.000022    Time 0.023052    
2023-01-06 16:16:20,883 - Epoch: [74][  110/  246]    Overall Loss 0.356900    Objective Loss 0.356900                                        LR 0.000022    Time 0.022585    
2023-01-06 16:16:21,059 - Epoch: [74][  120/  246]    Overall Loss 0.358311    Objective Loss 0.358311                                        LR 0.000022    Time 0.022171    
2023-01-06 16:16:21,235 - Epoch: [74][  130/  246]    Overall Loss 0.357779    Objective Loss 0.357779                                        LR 0.000022    Time 0.021814    
2023-01-06 16:16:21,412 - Epoch: [74][  140/  246]    Overall Loss 0.356724    Objective Loss 0.356724                                        LR 0.000022    Time 0.021516    
2023-01-06 16:16:21,587 - Epoch: [74][  150/  246]    Overall Loss 0.355992    Objective Loss 0.355992                                        LR 0.000022    Time 0.021245    
2023-01-06 16:16:21,762 - Epoch: [74][  160/  246]    Overall Loss 0.354746    Objective Loss 0.354746                                        LR 0.000022    Time 0.021007    
2023-01-06 16:16:21,937 - Epoch: [74][  170/  246]    Overall Loss 0.355233    Objective Loss 0.355233                                        LR 0.000022    Time 0.020802    
2023-01-06 16:16:22,113 - Epoch: [74][  180/  246]    Overall Loss 0.354530    Objective Loss 0.354530                                        LR 0.000022    Time 0.020619    
2023-01-06 16:16:22,288 - Epoch: [74][  190/  246]    Overall Loss 0.354831    Objective Loss 0.354831                                        LR 0.000022    Time 0.020455    
2023-01-06 16:16:22,464 - Epoch: [74][  200/  246]    Overall Loss 0.354554    Objective Loss 0.354554                                        LR 0.000022    Time 0.020308    
2023-01-06 16:16:22,639 - Epoch: [74][  210/  246]    Overall Loss 0.355433    Objective Loss 0.355433                                        LR 0.000022    Time 0.020175    
2023-01-06 16:16:22,814 - Epoch: [74][  220/  246]    Overall Loss 0.356124    Objective Loss 0.356124                                        LR 0.000022    Time 0.020049    
2023-01-06 16:16:22,988 - Epoch: [74][  230/  246]    Overall Loss 0.355814    Objective Loss 0.355814                                        LR 0.000022    Time 0.019934    
2023-01-06 16:16:23,170 - Epoch: [74][  240/  246]    Overall Loss 0.355077    Objective Loss 0.355077                                        LR 0.000022    Time 0.019861    
2023-01-06 16:16:23,244 - Epoch: [74][  246/  246]    Overall Loss 0.354760    Objective Loss 0.354760    Top1 89.234450    LR 0.000022    Time 0.019677    
2023-01-06 16:16:23,389 - --- validate (epoch=74)-----------
2023-01-06 16:16:23,389 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:23,824 - Epoch: [74][   10/   28]    Loss 0.332860    Top1 88.281250    
2023-01-06 16:16:23,926 - Epoch: [74][   20/   28]    Loss 0.341922    Top1 87.382812    
2023-01-06 16:16:23,983 - Epoch: [74][   28/   28]    Loss 0.345853    Top1 87.160034    
2023-01-06 16:16:24,118 - ==> Top1: 87.160    Loss: 0.346

2023-01-06 16:16:24,118 - ==> Confusion:
[[ 111    2  326]
 [   2   70  530]
 [  25   12 5908]]

2023-01-06 16:16:24,120 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:24,120 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:24,124 - 

2023-01-06 16:16:24,124 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:24,650 - Epoch: [75][   10/  246]    Overall Loss 0.352005    Objective Loss 0.352005                                        LR 0.000022    Time 0.052534    
2023-01-06 16:16:24,792 - Epoch: [75][   20/  246]    Overall Loss 0.347126    Objective Loss 0.347126                                        LR 0.000022    Time 0.033353    
2023-01-06 16:16:24,950 - Epoch: [75][   30/  246]    Overall Loss 0.349847    Objective Loss 0.349847                                        LR 0.000022    Time 0.027479    
2023-01-06 16:16:25,125 - Epoch: [75][   40/  246]    Overall Loss 0.349771    Objective Loss 0.349771                                        LR 0.000022    Time 0.024973    
2023-01-06 16:16:25,299 - Epoch: [75][   50/  246]    Overall Loss 0.353943    Objective Loss 0.353943                                        LR 0.000022    Time 0.023463    
2023-01-06 16:16:25,497 - Epoch: [75][   60/  246]    Overall Loss 0.354321    Objective Loss 0.354321                                        LR 0.000022    Time 0.022834    
2023-01-06 16:16:25,699 - Epoch: [75][   70/  246]    Overall Loss 0.352083    Objective Loss 0.352083                                        LR 0.000022    Time 0.022450    
2023-01-06 16:16:25,897 - Epoch: [75][   80/  246]    Overall Loss 0.352232    Objective Loss 0.352232                                        LR 0.000022    Time 0.022117    
2023-01-06 16:16:26,108 - Epoch: [75][   90/  246]    Overall Loss 0.351438    Objective Loss 0.351438                                        LR 0.000022    Time 0.022000    
2023-01-06 16:16:26,321 - Epoch: [75][  100/  246]    Overall Loss 0.352768    Objective Loss 0.352768                                        LR 0.000022    Time 0.021931    
2023-01-06 16:16:26,538 - Epoch: [75][  110/  246]    Overall Loss 0.353303    Objective Loss 0.353303                                        LR 0.000022    Time 0.021906    
2023-01-06 16:16:26,752 - Epoch: [75][  120/  246]    Overall Loss 0.353425    Objective Loss 0.353425                                        LR 0.000022    Time 0.021860    
2023-01-06 16:16:26,966 - Epoch: [75][  130/  246]    Overall Loss 0.352204    Objective Loss 0.352204                                        LR 0.000022    Time 0.021821    
2023-01-06 16:16:27,186 - Epoch: [75][  140/  246]    Overall Loss 0.350486    Objective Loss 0.350486                                        LR 0.000022    Time 0.021832    
2023-01-06 16:16:27,403 - Epoch: [75][  150/  246]    Overall Loss 0.348248    Objective Loss 0.348248                                        LR 0.000022    Time 0.021816    
2023-01-06 16:16:27,619 - Epoch: [75][  160/  246]    Overall Loss 0.348617    Objective Loss 0.348617                                        LR 0.000022    Time 0.021805    
2023-01-06 16:16:27,829 - Epoch: [75][  170/  246]    Overall Loss 0.348889    Objective Loss 0.348889                                        LR 0.000022    Time 0.021753    
2023-01-06 16:16:28,046 - Epoch: [75][  180/  246]    Overall Loss 0.349639    Objective Loss 0.349639                                        LR 0.000022    Time 0.021747    
2023-01-06 16:16:28,256 - Epoch: [75][  190/  246]    Overall Loss 0.349497    Objective Loss 0.349497                                        LR 0.000022    Time 0.021704    
2023-01-06 16:16:28,470 - Epoch: [75][  200/  246]    Overall Loss 0.349057    Objective Loss 0.349057                                        LR 0.000022    Time 0.021686    
2023-01-06 16:16:28,677 - Epoch: [75][  210/  246]    Overall Loss 0.348689    Objective Loss 0.348689                                        LR 0.000022    Time 0.021639    
2023-01-06 16:16:28,891 - Epoch: [75][  220/  246]    Overall Loss 0.347816    Objective Loss 0.347816                                        LR 0.000022    Time 0.021625    
2023-01-06 16:16:29,102 - Epoch: [75][  230/  246]    Overall Loss 0.347663    Objective Loss 0.347663                                        LR 0.000022    Time 0.021602    
2023-01-06 16:16:29,308 - Epoch: [75][  240/  246]    Overall Loss 0.347573    Objective Loss 0.347573                                        LR 0.000022    Time 0.021559    
2023-01-06 16:16:29,386 - Epoch: [75][  246/  246]    Overall Loss 0.347320    Objective Loss 0.347320    Top1 85.645933    LR 0.000022    Time 0.021351    
2023-01-06 16:16:29,530 - --- validate (epoch=75)-----------
2023-01-06 16:16:29,530 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:29,975 - Epoch: [75][   10/   28]    Loss 0.354766    Top1 86.914062    
2023-01-06 16:16:30,080 - Epoch: [75][   20/   28]    Loss 0.346285    Top1 87.402344    
2023-01-06 16:16:30,134 - Epoch: [75][   28/   28]    Loss 0.346747    Top1 87.575150    
2023-01-06 16:16:30,278 - ==> Top1: 87.575    Loss: 0.347

2023-01-06 16:16:30,278 - ==> Confusion:
[[ 130    3  306]
 [   7   98  497]
 [  32   23 5890]]

2023-01-06 16:16:30,280 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:30,280 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:30,284 - 

2023-01-06 16:16:30,284 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:30,976 - Epoch: [76][   10/  246]    Overall Loss 0.351609    Objective Loss 0.351609                                        LR 0.000022    Time 0.069140    
2023-01-06 16:16:31,146 - Epoch: [76][   20/  246]    Overall Loss 0.349505    Objective Loss 0.349505                                        LR 0.000022    Time 0.043057    
2023-01-06 16:16:31,321 - Epoch: [76][   30/  246]    Overall Loss 0.348733    Objective Loss 0.348733                                        LR 0.000022    Time 0.034510    
2023-01-06 16:16:31,497 - Epoch: [76][   40/  246]    Overall Loss 0.346254    Objective Loss 0.346254                                        LR 0.000022    Time 0.030263    
2023-01-06 16:16:31,663 - Epoch: [76][   50/  246]    Overall Loss 0.345869    Objective Loss 0.345869                                        LR 0.000022    Time 0.027536    
2023-01-06 16:16:31,848 - Epoch: [76][   60/  246]    Overall Loss 0.345875    Objective Loss 0.345875                                        LR 0.000022    Time 0.026021    
2023-01-06 16:16:32,041 - Epoch: [76][   70/  246]    Overall Loss 0.347970    Objective Loss 0.347970                                        LR 0.000022    Time 0.025048    
2023-01-06 16:16:32,216 - Epoch: [76][   80/  246]    Overall Loss 0.347116    Objective Loss 0.347116                                        LR 0.000022    Time 0.024098    
2023-01-06 16:16:32,385 - Epoch: [76][   90/  246]    Overall Loss 0.346582    Objective Loss 0.346582                                        LR 0.000022    Time 0.023295    
2023-01-06 16:16:32,546 - Epoch: [76][  100/  246]    Overall Loss 0.346143    Objective Loss 0.346143                                        LR 0.000022    Time 0.022575    
2023-01-06 16:16:32,709 - Epoch: [76][  110/  246]    Overall Loss 0.344733    Objective Loss 0.344733                                        LR 0.000022    Time 0.022004    
2023-01-06 16:16:32,874 - Epoch: [76][  120/  246]    Overall Loss 0.345762    Objective Loss 0.345762                                        LR 0.000022    Time 0.021538    
2023-01-06 16:16:33,037 - Epoch: [76][  130/  246]    Overall Loss 0.345919    Objective Loss 0.345919                                        LR 0.000022    Time 0.021136    
2023-01-06 16:16:33,211 - Epoch: [76][  140/  246]    Overall Loss 0.344844    Objective Loss 0.344844                                        LR 0.000022    Time 0.020864    
2023-01-06 16:16:33,392 - Epoch: [76][  150/  246]    Overall Loss 0.343753    Objective Loss 0.343753                                        LR 0.000022    Time 0.020676    
2023-01-06 16:16:33,568 - Epoch: [76][  160/  246]    Overall Loss 0.342946    Objective Loss 0.342946                                        LR 0.000022    Time 0.020483    
2023-01-06 16:16:33,743 - Epoch: [76][  170/  246]    Overall Loss 0.342383    Objective Loss 0.342383                                        LR 0.000022    Time 0.020307    
2023-01-06 16:16:33,918 - Epoch: [76][  180/  246]    Overall Loss 0.342883    Objective Loss 0.342883                                        LR 0.000022    Time 0.020149    
2023-01-06 16:16:34,097 - Epoch: [76][  190/  246]    Overall Loss 0.343275    Objective Loss 0.343275                                        LR 0.000022    Time 0.020029    
2023-01-06 16:16:34,272 - Epoch: [76][  200/  246]    Overall Loss 0.343486    Objective Loss 0.343486                                        LR 0.000022    Time 0.019897    
2023-01-06 16:16:34,449 - Epoch: [76][  210/  246]    Overall Loss 0.343686    Objective Loss 0.343686                                        LR 0.000022    Time 0.019790    
2023-01-06 16:16:34,622 - Epoch: [76][  220/  246]    Overall Loss 0.344064    Objective Loss 0.344064                                        LR 0.000022    Time 0.019677    
2023-01-06 16:16:34,800 - Epoch: [76][  230/  246]    Overall Loss 0.343775    Objective Loss 0.343775                                        LR 0.000022    Time 0.019593    
2023-01-06 16:16:34,978 - Epoch: [76][  240/  246]    Overall Loss 0.343771    Objective Loss 0.343771                                        LR 0.000022    Time 0.019520    
2023-01-06 16:16:35,062 - Epoch: [76][  246/  246]    Overall Loss 0.344107    Objective Loss 0.344107    Top1 83.492823    LR 0.000022    Time 0.019382    
2023-01-06 16:16:35,208 - --- validate (epoch=76)-----------
2023-01-06 16:16:35,208 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:35,651 - Epoch: [76][   10/   28]    Loss 0.344551    Top1 87.109375    
2023-01-06 16:16:35,757 - Epoch: [76][   20/   28]    Loss 0.341878    Top1 87.480469    
2023-01-06 16:16:35,818 - Epoch: [76][   28/   28]    Loss 0.336983    Top1 87.489264    
2023-01-06 16:16:35,972 - ==> Top1: 87.489    Loss: 0.337

2023-01-06 16:16:35,973 - ==> Confusion:
[[ 128    2  309]
 [   7   98  497]
 [  32   27 5886]]

2023-01-06 16:16:35,974 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:35,974 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:35,978 - 

2023-01-06 16:16:35,978 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:36,505 - Epoch: [77][   10/  246]    Overall Loss 0.331765    Objective Loss 0.331765                                        LR 0.000022    Time 0.052576    
2023-01-06 16:16:36,656 - Epoch: [77][   20/  246]    Overall Loss 0.334530    Objective Loss 0.334530                                        LR 0.000022    Time 0.033838    
2023-01-06 16:16:36,813 - Epoch: [77][   30/  246]    Overall Loss 0.345773    Objective Loss 0.345773                                        LR 0.000022    Time 0.027765    
2023-01-06 16:16:36,972 - Epoch: [77][   40/  246]    Overall Loss 0.345851    Objective Loss 0.345851                                        LR 0.000022    Time 0.024792    
2023-01-06 16:16:37,121 - Epoch: [77][   50/  246]    Overall Loss 0.344939    Objective Loss 0.344939                                        LR 0.000022    Time 0.022804    
2023-01-06 16:16:37,279 - Epoch: [77][   60/  246]    Overall Loss 0.343684    Objective Loss 0.343684                                        LR 0.000022    Time 0.021605    
2023-01-06 16:16:37,431 - Epoch: [77][   70/  246]    Overall Loss 0.345328    Objective Loss 0.345328                                        LR 0.000022    Time 0.020680    
2023-01-06 16:16:37,587 - Epoch: [77][   80/  246]    Overall Loss 0.344366    Objective Loss 0.344366                                        LR 0.000022    Time 0.020036    
2023-01-06 16:16:37,750 - Epoch: [77][   90/  246]    Overall Loss 0.344951    Objective Loss 0.344951                                        LR 0.000022    Time 0.019620    
2023-01-06 16:16:37,919 - Epoch: [77][  100/  246]    Overall Loss 0.344521    Objective Loss 0.344521                                        LR 0.000022    Time 0.019348    
2023-01-06 16:16:38,087 - Epoch: [77][  110/  246]    Overall Loss 0.343273    Objective Loss 0.343273                                        LR 0.000022    Time 0.019110    
2023-01-06 16:16:38,228 - Epoch: [77][  120/  246]    Overall Loss 0.343067    Objective Loss 0.343067                                        LR 0.000022    Time 0.018690    
2023-01-06 16:16:38,364 - Epoch: [77][  130/  246]    Overall Loss 0.343327    Objective Loss 0.343327                                        LR 0.000022    Time 0.018299    
2023-01-06 16:16:38,499 - Epoch: [77][  140/  246]    Overall Loss 0.342355    Objective Loss 0.342355                                        LR 0.000022    Time 0.017954    
2023-01-06 16:16:38,635 - Epoch: [77][  150/  246]    Overall Loss 0.342334    Objective Loss 0.342334                                        LR 0.000022    Time 0.017661    
2023-01-06 16:16:38,771 - Epoch: [77][  160/  246]    Overall Loss 0.342085    Objective Loss 0.342085                                        LR 0.000022    Time 0.017405    
2023-01-06 16:16:38,907 - Epoch: [77][  170/  246]    Overall Loss 0.341752    Objective Loss 0.341752                                        LR 0.000022    Time 0.017178    
2023-01-06 16:16:39,044 - Epoch: [77][  180/  246]    Overall Loss 0.341204    Objective Loss 0.341204                                        LR 0.000022    Time 0.016984    
2023-01-06 16:16:39,190 - Epoch: [77][  190/  246]    Overall Loss 0.340693    Objective Loss 0.340693                                        LR 0.000022    Time 0.016858    
2023-01-06 16:16:39,329 - Epoch: [77][  200/  246]    Overall Loss 0.339677    Objective Loss 0.339677                                        LR 0.000022    Time 0.016709    
2023-01-06 16:16:39,469 - Epoch: [77][  210/  246]    Overall Loss 0.340352    Objective Loss 0.340352                                        LR 0.000022    Time 0.016577    
2023-01-06 16:16:39,620 - Epoch: [77][  220/  246]    Overall Loss 0.340598    Objective Loss 0.340598                                        LR 0.000022    Time 0.016510    
2023-01-06 16:16:39,774 - Epoch: [77][  230/  246]    Overall Loss 0.341512    Objective Loss 0.341512                                        LR 0.000022    Time 0.016460    
2023-01-06 16:16:39,942 - Epoch: [77][  240/  246]    Overall Loss 0.341731    Objective Loss 0.341731                                        LR 0.000022    Time 0.016473    
2023-01-06 16:16:40,012 - Epoch: [77][  246/  246]    Overall Loss 0.341748    Objective Loss 0.341748    Top1 85.645933    LR 0.000022    Time 0.016353    
2023-01-06 16:16:40,153 - --- validate (epoch=77)-----------
2023-01-06 16:16:40,153 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:40,712 - Epoch: [77][   10/   28]    Loss 0.343009    Top1 87.382812    
2023-01-06 16:16:40,821 - Epoch: [77][   20/   28]    Loss 0.335794    Top1 88.007812    
2023-01-06 16:16:40,878 - Epoch: [77][   28/   28]    Loss 0.338633    Top1 87.890066    
2023-01-06 16:16:41,027 - ==> Top1: 87.890    Loss: 0.339

2023-01-06 16:16:41,027 - ==> Confusion:
[[ 160    4  275]
 [   8  130  464]
 [  57   38 5850]]

2023-01-06 16:16:41,028 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:41,029 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:41,033 - 

2023-01-06 16:16:41,033 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:41,573 - Epoch: [78][   10/  246]    Overall Loss 0.331970    Objective Loss 0.331970                                        LR 0.000022    Time 0.053869    
2023-01-06 16:16:41,729 - Epoch: [78][   20/  246]    Overall Loss 0.332085    Objective Loss 0.332085                                        LR 0.000022    Time 0.034657    
2023-01-06 16:16:41,884 - Epoch: [78][   30/  246]    Overall Loss 0.335714    Objective Loss 0.335714                                        LR 0.000022    Time 0.028278    
2023-01-06 16:16:42,043 - Epoch: [78][   40/  246]    Overall Loss 0.334698    Objective Loss 0.334698                                        LR 0.000022    Time 0.025166    
2023-01-06 16:16:42,191 - Epoch: [78][   50/  246]    Overall Loss 0.334913    Objective Loss 0.334913                                        LR 0.000022    Time 0.023097    
2023-01-06 16:16:42,354 - Epoch: [78][   60/  246]    Overall Loss 0.336257    Objective Loss 0.336257                                        LR 0.000022    Time 0.021962    
2023-01-06 16:16:42,517 - Epoch: [78][   70/  246]    Overall Loss 0.334890    Objective Loss 0.334890                                        LR 0.000022    Time 0.021149    
2023-01-06 16:16:42,673 - Epoch: [78][   80/  246]    Overall Loss 0.336455    Objective Loss 0.336455                                        LR 0.000022    Time 0.020443    
2023-01-06 16:16:42,826 - Epoch: [78][   90/  246]    Overall Loss 0.337138    Objective Loss 0.337138                                        LR 0.000022    Time 0.019874    
2023-01-06 16:16:42,984 - Epoch: [78][  100/  246]    Overall Loss 0.337360    Objective Loss 0.337360                                        LR 0.000022    Time 0.019461    
2023-01-06 16:16:43,146 - Epoch: [78][  110/  246]    Overall Loss 0.337137    Objective Loss 0.337137                                        LR 0.000022    Time 0.019161    
2023-01-06 16:16:43,301 - Epoch: [78][  120/  246]    Overall Loss 0.337958    Objective Loss 0.337958                                        LR 0.000022    Time 0.018856    
2023-01-06 16:16:43,463 - Epoch: [78][  130/  246]    Overall Loss 0.340241    Objective Loss 0.340241                                        LR 0.000022    Time 0.018646    
2023-01-06 16:16:43,618 - Epoch: [78][  140/  246]    Overall Loss 0.341046    Objective Loss 0.341046                                        LR 0.000022    Time 0.018419    
2023-01-06 16:16:43,781 - Epoch: [78][  150/  246]    Overall Loss 0.339174    Objective Loss 0.339174                                        LR 0.000022    Time 0.018276    
2023-01-06 16:16:43,947 - Epoch: [78][  160/  246]    Overall Loss 0.338667    Objective Loss 0.338667                                        LR 0.000022    Time 0.018168    
2023-01-06 16:16:44,117 - Epoch: [78][  170/  246]    Overall Loss 0.338372    Objective Loss 0.338372                                        LR 0.000022    Time 0.018099    
2023-01-06 16:16:44,290 - Epoch: [78][  180/  246]    Overall Loss 0.337954    Objective Loss 0.337954                                        LR 0.000022    Time 0.018053    
2023-01-06 16:16:44,455 - Epoch: [78][  190/  246]    Overall Loss 0.337235    Objective Loss 0.337235                                        LR 0.000022    Time 0.017967    
2023-01-06 16:16:44,622 - Epoch: [78][  200/  246]    Overall Loss 0.337618    Objective Loss 0.337618                                        LR 0.000022    Time 0.017904    
2023-01-06 16:16:44,787 - Epoch: [78][  210/  246]    Overall Loss 0.338415    Objective Loss 0.338415                                        LR 0.000022    Time 0.017834    
2023-01-06 16:16:44,956 - Epoch: [78][  220/  246]    Overall Loss 0.338866    Objective Loss 0.338866                                        LR 0.000022    Time 0.017791    
2023-01-06 16:16:45,121 - Epoch: [78][  230/  246]    Overall Loss 0.339199    Objective Loss 0.339199                                        LR 0.000022    Time 0.017733    
2023-01-06 16:16:45,299 - Epoch: [78][  240/  246]    Overall Loss 0.339850    Objective Loss 0.339850                                        LR 0.000022    Time 0.017733    
2023-01-06 16:16:45,373 - Epoch: [78][  246/  246]    Overall Loss 0.340237    Objective Loss 0.340237    Top1 87.320574    LR 0.000022    Time 0.017602    
2023-01-06 16:16:45,504 - --- validate (epoch=78)-----------
2023-01-06 16:16:45,504 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:45,957 - Epoch: [78][   10/   28]    Loss 0.328799    Top1 88.320312    
2023-01-06 16:16:46,080 - Epoch: [78][   20/   28]    Loss 0.332504    Top1 88.125000    
2023-01-06 16:16:46,137 - Epoch: [78][   28/   28]    Loss 0.336324    Top1 87.975952    
2023-01-06 16:16:46,273 - ==> Top1: 87.976    Loss: 0.336

2023-01-06 16:16:46,274 - ==> Confusion:
[[ 159    5  275]
 [   8  137  457]
 [  53   42 5850]]

2023-01-06 16:16:46,275 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:46,275 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:46,279 - 

2023-01-06 16:16:46,279 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:46,970 - Epoch: [79][   10/  246]    Overall Loss 0.325775    Objective Loss 0.325775                                        LR 0.000022    Time 0.068984    
2023-01-06 16:16:47,141 - Epoch: [79][   20/  246]    Overall Loss 0.332745    Objective Loss 0.332745                                        LR 0.000022    Time 0.043022    
2023-01-06 16:16:47,319 - Epoch: [79][   30/  246]    Overall Loss 0.333789    Objective Loss 0.333789                                        LR 0.000022    Time 0.034606    
2023-01-06 16:16:47,491 - Epoch: [79][   40/  246]    Overall Loss 0.330924    Objective Loss 0.330924                                        LR 0.000022    Time 0.030254    
2023-01-06 16:16:47,663 - Epoch: [79][   50/  246]    Overall Loss 0.335354    Objective Loss 0.335354                                        LR 0.000022    Time 0.027628    
2023-01-06 16:16:47,823 - Epoch: [79][   60/  246]    Overall Loss 0.334292    Objective Loss 0.334292                                        LR 0.000022    Time 0.025648    
2023-01-06 16:16:47,987 - Epoch: [79][   70/  246]    Overall Loss 0.336022    Objective Loss 0.336022                                        LR 0.000022    Time 0.024330    
2023-01-06 16:16:48,146 - Epoch: [79][   80/  246]    Overall Loss 0.338433    Objective Loss 0.338433                                        LR 0.000022    Time 0.023273    
2023-01-06 16:16:48,310 - Epoch: [79][   90/  246]    Overall Loss 0.336393    Objective Loss 0.336393                                        LR 0.000022    Time 0.022498    
2023-01-06 16:16:48,468 - Epoch: [79][  100/  246]    Overall Loss 0.336621    Objective Loss 0.336621                                        LR 0.000022    Time 0.021813    
2023-01-06 16:16:48,631 - Epoch: [79][  110/  246]    Overall Loss 0.336430    Objective Loss 0.336430                                        LR 0.000022    Time 0.021308    
2023-01-06 16:16:48,790 - Epoch: [79][  120/  246]    Overall Loss 0.337902    Objective Loss 0.337902                                        LR 0.000022    Time 0.020844    
2023-01-06 16:16:48,942 - Epoch: [79][  130/  246]    Overall Loss 0.337793    Objective Loss 0.337793                                        LR 0.000022    Time 0.020403    
2023-01-06 16:16:49,094 - Epoch: [79][  140/  246]    Overall Loss 0.338241    Objective Loss 0.338241                                        LR 0.000022    Time 0.020027    
2023-01-06 16:16:49,242 - Epoch: [79][  150/  246]    Overall Loss 0.340132    Objective Loss 0.340132                                        LR 0.000022    Time 0.019679    
2023-01-06 16:16:49,387 - Epoch: [79][  160/  246]    Overall Loss 0.339829    Objective Loss 0.339829                                        LR 0.000022    Time 0.019351    
2023-01-06 16:16:49,528 - Epoch: [79][  170/  246]    Overall Loss 0.339261    Objective Loss 0.339261                                        LR 0.000022    Time 0.019041    
2023-01-06 16:16:49,670 - Epoch: [79][  180/  246]    Overall Loss 0.339188    Objective Loss 0.339188                                        LR 0.000022    Time 0.018771    
2023-01-06 16:16:49,815 - Epoch: [79][  190/  246]    Overall Loss 0.339331    Objective Loss 0.339331                                        LR 0.000022    Time 0.018544    
2023-01-06 16:16:49,959 - Epoch: [79][  200/  246]    Overall Loss 0.339641    Objective Loss 0.339641                                        LR 0.000022    Time 0.018334    
2023-01-06 16:16:50,108 - Epoch: [79][  210/  246]    Overall Loss 0.339508    Objective Loss 0.339508                                        LR 0.000022    Time 0.018169    
2023-01-06 16:16:50,271 - Epoch: [79][  220/  246]    Overall Loss 0.339799    Objective Loss 0.339799                                        LR 0.000022    Time 0.018085    
2023-01-06 16:16:50,437 - Epoch: [79][  230/  246]    Overall Loss 0.339452    Objective Loss 0.339452                                        LR 0.000022    Time 0.018017    
2023-01-06 16:16:50,613 - Epoch: [79][  240/  246]    Overall Loss 0.339295    Objective Loss 0.339295                                        LR 0.000022    Time 0.017999    
2023-01-06 16:16:50,694 - Epoch: [79][  246/  246]    Overall Loss 0.339153    Objective Loss 0.339153    Top1 87.559809    LR 0.000022    Time 0.017887    
2023-01-06 16:16:50,836 - --- validate (epoch=79)-----------
2023-01-06 16:16:50,836 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:51,275 - Epoch: [79][   10/   28]    Loss 0.345941    Top1 87.304688    
2023-01-06 16:16:51,387 - Epoch: [79][   20/   28]    Loss 0.346382    Top1 87.402344    
2023-01-06 16:16:51,444 - Epoch: [79][   28/   28]    Loss 0.337032    Top1 87.947323    
2023-01-06 16:16:51,601 - ==> Top1: 87.947    Loss: 0.337

2023-01-06 16:16:51,601 - ==> Confusion:
[[ 154    3  282]
 [   8  129  465]
 [  48   36 5861]]

2023-01-06 16:16:51,602 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:51,602 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:51,607 - 

2023-01-06 16:16:51,607 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:52,145 - Epoch: [80][   10/  246]    Overall Loss 0.322226    Objective Loss 0.322226                                        LR 0.000022    Time 0.053810    
2023-01-06 16:16:52,300 - Epoch: [80][   20/  246]    Overall Loss 0.336690    Objective Loss 0.336690                                        LR 0.000022    Time 0.034632    
2023-01-06 16:16:52,461 - Epoch: [80][   30/  246]    Overall Loss 0.340442    Objective Loss 0.340442                                        LR 0.000022    Time 0.028420    
2023-01-06 16:16:52,621 - Epoch: [80][   40/  246]    Overall Loss 0.341131    Objective Loss 0.341131                                        LR 0.000022    Time 0.025303    
2023-01-06 16:16:52,784 - Epoch: [80][   50/  246]    Overall Loss 0.344714    Objective Loss 0.344714                                        LR 0.000022    Time 0.023495    
2023-01-06 16:16:52,942 - Epoch: [80][   60/  246]    Overall Loss 0.344437    Objective Loss 0.344437                                        LR 0.000022    Time 0.022205    
2023-01-06 16:16:53,099 - Epoch: [80][   70/  246]    Overall Loss 0.343288    Objective Loss 0.343288                                        LR 0.000022    Time 0.021276    
2023-01-06 16:16:53,257 - Epoch: [80][   80/  246]    Overall Loss 0.341037    Objective Loss 0.341037                                        LR 0.000022    Time 0.020583    
2023-01-06 16:16:53,413 - Epoch: [80][   90/  246]    Overall Loss 0.340477    Objective Loss 0.340477                                        LR 0.000022    Time 0.020028    
2023-01-06 16:16:53,560 - Epoch: [80][  100/  246]    Overall Loss 0.339369    Objective Loss 0.339369                                        LR 0.000022    Time 0.019491    
2023-01-06 16:16:53,702 - Epoch: [80][  110/  246]    Overall Loss 0.339769    Objective Loss 0.339769                                        LR 0.000022    Time 0.019010    
2023-01-06 16:16:53,855 - Epoch: [80][  120/  246]    Overall Loss 0.339882    Objective Loss 0.339882                                        LR 0.000022    Time 0.018694    
2023-01-06 16:16:54,008 - Epoch: [80][  130/  246]    Overall Loss 0.339151    Objective Loss 0.339151                                        LR 0.000022    Time 0.018429    
2023-01-06 16:16:54,170 - Epoch: [80][  140/  246]    Overall Loss 0.338391    Objective Loss 0.338391                                        LR 0.000022    Time 0.018271    
2023-01-06 16:16:54,323 - Epoch: [80][  150/  246]    Overall Loss 0.337524    Objective Loss 0.337524                                        LR 0.000022    Time 0.018067    
2023-01-06 16:16:54,471 - Epoch: [80][  160/  246]    Overall Loss 0.336720    Objective Loss 0.336720                                        LR 0.000022    Time 0.017862    
2023-01-06 16:16:54,622 - Epoch: [80][  170/  246]    Overall Loss 0.336645    Objective Loss 0.336645                                        LR 0.000022    Time 0.017695    
2023-01-06 16:16:54,762 - Epoch: [80][  180/  246]    Overall Loss 0.338290    Objective Loss 0.338290                                        LR 0.000022    Time 0.017489    
2023-01-06 16:16:54,901 - Epoch: [80][  190/  246]    Overall Loss 0.338134    Objective Loss 0.338134                                        LR 0.000022    Time 0.017291    
2023-01-06 16:16:55,038 - Epoch: [80][  200/  246]    Overall Loss 0.337722    Objective Loss 0.337722                                        LR 0.000022    Time 0.017109    
2023-01-06 16:16:55,177 - Epoch: [80][  210/  246]    Overall Loss 0.338136    Objective Loss 0.338136                                        LR 0.000022    Time 0.016954    
2023-01-06 16:16:55,314 - Epoch: [80][  220/  246]    Overall Loss 0.337526    Objective Loss 0.337526                                        LR 0.000022    Time 0.016806    
2023-01-06 16:16:55,452 - Epoch: [80][  230/  246]    Overall Loss 0.336942    Objective Loss 0.336942                                        LR 0.000022    Time 0.016675    
2023-01-06 16:16:55,607 - Epoch: [80][  240/  246]    Overall Loss 0.336677    Objective Loss 0.336677                                        LR 0.000022    Time 0.016621    
2023-01-06 16:16:55,681 - Epoch: [80][  246/  246]    Overall Loss 0.337275    Objective Loss 0.337275    Top1 86.602871    LR 0.000022    Time 0.016515    
2023-01-06 16:16:55,817 - --- validate (epoch=80)-----------
2023-01-06 16:16:55,817 - 6986 samples (256 per mini-batch)
2023-01-06 16:16:56,248 - Epoch: [80][   10/   28]    Loss 0.339501    Top1 87.460938    
2023-01-06 16:16:56,351 - Epoch: [80][   20/   28]    Loss 0.340522    Top1 87.773438    
2023-01-06 16:16:56,411 - Epoch: [80][   28/   28]    Loss 0.332820    Top1 87.961638    
2023-01-06 16:16:56,558 - ==> Top1: 87.962    Loss: 0.333

2023-01-06 16:16:56,559 - ==> Confusion:
[[ 157    3  279]
 [   8  125  469]
 [  49   33 5863]]

2023-01-06 16:16:56,560 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 73]
2023-01-06 16:16:56,560 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:16:56,565 - 

2023-01-06 16:16:56,565 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:16:57,266 - Epoch: [81][   10/  246]    Overall Loss 0.335850    Objective Loss 0.335850                                        LR 0.000022    Time 0.069958    
2023-01-06 16:16:57,453 - Epoch: [81][   20/  246]    Overall Loss 0.343159    Objective Loss 0.343159                                        LR 0.000022    Time 0.044321    
2023-01-06 16:16:57,649 - Epoch: [81][   30/  246]    Overall Loss 0.342904    Objective Loss 0.342904                                        LR 0.000022    Time 0.036063    
2023-01-06 16:16:57,841 - Epoch: [81][   40/  246]    Overall Loss 0.339188    Objective Loss 0.339188                                        LR 0.000022    Time 0.031848    
2023-01-06 16:16:58,040 - Epoch: [81][   50/  246]    Overall Loss 0.341370    Objective Loss 0.341370                                        LR 0.000022    Time 0.029434    
2023-01-06 16:16:58,237 - Epoch: [81][   60/  246]    Overall Loss 0.340834    Objective Loss 0.340834                                        LR 0.000022    Time 0.027806    
2023-01-06 16:16:58,437 - Epoch: [81][   70/  246]    Overall Loss 0.342734    Objective Loss 0.342734                                        LR 0.000022    Time 0.026687    
2023-01-06 16:16:58,636 - Epoch: [81][   80/  246]    Overall Loss 0.339816    Objective Loss 0.339816                                        LR 0.000022    Time 0.025843    
2023-01-06 16:16:58,839 - Epoch: [81][   90/  246]    Overall Loss 0.340913    Objective Loss 0.340913                                        LR 0.000022    Time 0.025217    
2023-01-06 16:16:59,039 - Epoch: [81][  100/  246]    Overall Loss 0.339501    Objective Loss 0.339501                                        LR 0.000022    Time 0.024692    
2023-01-06 16:16:59,206 - Epoch: [81][  110/  246]    Overall Loss 0.337355    Objective Loss 0.337355                                        LR 0.000022    Time 0.023964    
2023-01-06 16:16:59,376 - Epoch: [81][  120/  246]    Overall Loss 0.338181    Objective Loss 0.338181                                        LR 0.000022    Time 0.023376    
2023-01-06 16:16:59,538 - Epoch: [81][  130/  246]    Overall Loss 0.338926    Objective Loss 0.338926                                        LR 0.000022    Time 0.022821    
2023-01-06 16:16:59,705 - Epoch: [81][  140/  246]    Overall Loss 0.338693    Objective Loss 0.338693                                        LR 0.000022    Time 0.022380    
2023-01-06 16:16:59,849 - Epoch: [81][  150/  246]    Overall Loss 0.338713    Objective Loss 0.338713                                        LR 0.000022    Time 0.021847    
2023-01-06 16:17:00,023 - Epoch: [81][  160/  246]    Overall Loss 0.339371    Objective Loss 0.339371                                        LR 0.000022    Time 0.021564    
2023-01-06 16:17:00,196 - Epoch: [81][  170/  246]    Overall Loss 0.337501    Objective Loss 0.337501                                        LR 0.000022    Time 0.021314    
2023-01-06 16:17:00,367 - Epoch: [81][  180/  246]    Overall Loss 0.337112    Objective Loss 0.337112                                        LR 0.000022    Time 0.021078    
2023-01-06 16:17:00,545 - Epoch: [81][  190/  246]    Overall Loss 0.338140    Objective Loss 0.338140                                        LR 0.000022    Time 0.020899    
2023-01-06 16:17:00,683 - Epoch: [81][  200/  246]    Overall Loss 0.337880    Objective Loss 0.337880                                        LR 0.000022    Time 0.020545    
2023-01-06 16:17:00,823 - Epoch: [81][  210/  246]    Overall Loss 0.336421    Objective Loss 0.336421                                        LR 0.000022    Time 0.020232    
2023-01-06 16:17:00,969 - Epoch: [81][  220/  246]    Overall Loss 0.337149    Objective Loss 0.337149                                        LR 0.000022    Time 0.019973    
2023-01-06 16:17:01,121 - Epoch: [81][  230/  246]    Overall Loss 0.337258    Objective Loss 0.337258                                        LR 0.000022    Time 0.019763    
2023-01-06 16:17:01,272 - Epoch: [81][  240/  246]    Overall Loss 0.336951    Objective Loss 0.336951                                        LR 0.000022    Time 0.019570    
2023-01-06 16:17:01,340 - Epoch: [81][  246/  246]    Overall Loss 0.336796    Objective Loss 0.336796    Top1 84.928230    LR 0.000022    Time 0.019369    
2023-01-06 16:17:01,473 - --- validate (epoch=81)-----------
2023-01-06 16:17:01,473 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:01,905 - Epoch: [81][   10/   28]    Loss 0.343626    Top1 87.773438    
2023-01-06 16:17:02,008 - Epoch: [81][   20/   28]    Loss 0.343783    Top1 87.734375    
2023-01-06 16:17:02,066 - Epoch: [81][   28/   28]    Loss 0.332444    Top1 88.047524    
2023-01-06 16:17:02,229 - ==> Top1: 88.048    Loss: 0.332

2023-01-06 16:17:02,229 - ==> Confusion:
[[ 178    4  257]
 [  13  132  457]
 [  65   39 5841]]

2023-01-06 16:17:02,230 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:17:02,230 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:02,236 - 

2023-01-06 16:17:02,236 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:02,936 - Epoch: [82][   10/  246]    Overall Loss 0.335985    Objective Loss 0.335985                                        LR 0.000022    Time 0.070002    
2023-01-06 16:17:03,106 - Epoch: [82][   20/  246]    Overall Loss 0.333972    Objective Loss 0.333972                                        LR 0.000022    Time 0.043454    
2023-01-06 16:17:03,284 - Epoch: [82][   30/  246]    Overall Loss 0.338660    Objective Loss 0.338660                                        LR 0.000022    Time 0.034902    
2023-01-06 16:17:03,460 - Epoch: [82][   40/  246]    Overall Loss 0.340304    Objective Loss 0.340304                                        LR 0.000022    Time 0.030565    
2023-01-06 16:17:03,641 - Epoch: [82][   50/  246]    Overall Loss 0.340516    Objective Loss 0.340516                                        LR 0.000022    Time 0.028056    
2023-01-06 16:17:03,833 - Epoch: [82][   60/  246]    Overall Loss 0.340490    Objective Loss 0.340490                                        LR 0.000022    Time 0.026576    
2023-01-06 16:17:04,036 - Epoch: [82][   70/  246]    Overall Loss 0.339485    Objective Loss 0.339485                                        LR 0.000022    Time 0.025674    
2023-01-06 16:17:04,238 - Epoch: [82][   80/  246]    Overall Loss 0.337602    Objective Loss 0.337602                                        LR 0.000022    Time 0.024986    
2023-01-06 16:17:04,441 - Epoch: [82][   90/  246]    Overall Loss 0.336365    Objective Loss 0.336365                                        LR 0.000022    Time 0.024460    
2023-01-06 16:17:04,643 - Epoch: [82][  100/  246]    Overall Loss 0.336141    Objective Loss 0.336141                                        LR 0.000022    Time 0.024029    
2023-01-06 16:17:04,845 - Epoch: [82][  110/  246]    Overall Loss 0.337417    Objective Loss 0.337417                                        LR 0.000022    Time 0.023685    
2023-01-06 16:17:05,044 - Epoch: [82][  120/  246]    Overall Loss 0.338233    Objective Loss 0.338233                                        LR 0.000022    Time 0.023366    
2023-01-06 16:17:05,247 - Epoch: [82][  130/  246]    Overall Loss 0.337662    Objective Loss 0.337662                                        LR 0.000022    Time 0.023120    
2023-01-06 16:17:05,446 - Epoch: [82][  140/  246]    Overall Loss 0.338091    Objective Loss 0.338091                                        LR 0.000022    Time 0.022892    
2023-01-06 16:17:05,647 - Epoch: [82][  150/  246]    Overall Loss 0.337069    Objective Loss 0.337069                                        LR 0.000022    Time 0.022704    
2023-01-06 16:17:05,846 - Epoch: [82][  160/  246]    Overall Loss 0.335832    Objective Loss 0.335832                                        LR 0.000022    Time 0.022526    
2023-01-06 16:17:06,046 - Epoch: [82][  170/  246]    Overall Loss 0.335649    Objective Loss 0.335649                                        LR 0.000022    Time 0.022375    
2023-01-06 16:17:06,246 - Epoch: [82][  180/  246]    Overall Loss 0.334539    Objective Loss 0.334539                                        LR 0.000022    Time 0.022238    
2023-01-06 16:17:06,443 - Epoch: [82][  190/  246]    Overall Loss 0.335668    Objective Loss 0.335668                                        LR 0.000022    Time 0.022106    
2023-01-06 16:17:06,635 - Epoch: [82][  200/  246]    Overall Loss 0.336617    Objective Loss 0.336617                                        LR 0.000022    Time 0.021958    
2023-01-06 16:17:06,831 - Epoch: [82][  210/  246]    Overall Loss 0.336343    Objective Loss 0.336343                                        LR 0.000022    Time 0.021841    
2023-01-06 16:17:07,023 - Epoch: [82][  220/  246]    Overall Loss 0.335434    Objective Loss 0.335434                                        LR 0.000022    Time 0.021718    
2023-01-06 16:17:07,215 - Epoch: [82][  230/  246]    Overall Loss 0.335389    Objective Loss 0.335389                                        LR 0.000022    Time 0.021610    
2023-01-06 16:17:07,403 - Epoch: [82][  240/  246]    Overall Loss 0.335426    Objective Loss 0.335426                                        LR 0.000022    Time 0.021492    
2023-01-06 16:17:07,487 - Epoch: [82][  246/  246]    Overall Loss 0.335103    Objective Loss 0.335103    Top1 88.995215    LR 0.000022    Time 0.021308    
2023-01-06 16:17:07,615 - --- validate (epoch=82)-----------
2023-01-06 16:17:07,615 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:08,047 - Epoch: [82][   10/   28]    Loss 0.325919    Top1 88.007812    
2023-01-06 16:17:08,164 - Epoch: [82][   20/   28]    Loss 0.330746    Top1 87.773438    
2023-01-06 16:17:08,221 - Epoch: [82][   28/   28]    Loss 0.331685    Top1 87.761237    
2023-01-06 16:17:08,387 - ==> Top1: 87.761    Loss: 0.332

2023-01-06 16:17:08,387 - ==> Confusion:
[[ 134    3  302]
 [   7  126  469]
 [  40   34 5871]]

2023-01-06 16:17:08,389 - ==> Best [Top1: 88.048   Sparsity:0.00   Params: 46192 on epoch: 81]
2023-01-06 16:17:08,389 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:08,393 - 

2023-01-06 16:17:08,393 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:08,980 - Epoch: [83][   10/  246]    Overall Loss 0.316306    Objective Loss 0.316306                                        LR 0.000022    Time 0.058613    
2023-01-06 16:17:09,143 - Epoch: [83][   20/  246]    Overall Loss 0.316002    Objective Loss 0.316002                                        LR 0.000022    Time 0.037460    
2023-01-06 16:17:09,281 - Epoch: [83][   30/  246]    Overall Loss 0.323333    Objective Loss 0.323333                                        LR 0.000022    Time 0.029504    
2023-01-06 16:17:09,418 - Epoch: [83][   40/  246]    Overall Loss 0.328250    Objective Loss 0.328250                                        LR 0.000022    Time 0.025522    
2023-01-06 16:17:09,555 - Epoch: [83][   50/  246]    Overall Loss 0.331551    Objective Loss 0.331551                                        LR 0.000022    Time 0.023156    
2023-01-06 16:17:09,693 - Epoch: [83][   60/  246]    Overall Loss 0.333782    Objective Loss 0.333782                                        LR 0.000022    Time 0.021560    
2023-01-06 16:17:09,838 - Epoch: [83][   70/  246]    Overall Loss 0.331353    Objective Loss 0.331353                                        LR 0.000022    Time 0.020528    
2023-01-06 16:17:09,976 - Epoch: [83][   80/  246]    Overall Loss 0.331799    Objective Loss 0.331799                                        LR 0.000022    Time 0.019681    
2023-01-06 16:17:10,120 - Epoch: [83][   90/  246]    Overall Loss 0.332752    Objective Loss 0.332752                                        LR 0.000022    Time 0.019085    
2023-01-06 16:17:10,262 - Epoch: [83][  100/  246]    Overall Loss 0.336069    Objective Loss 0.336069                                        LR 0.000022    Time 0.018591    
2023-01-06 16:17:10,403 - Epoch: [83][  110/  246]    Overall Loss 0.335001    Objective Loss 0.335001                                        LR 0.000022    Time 0.018178    
2023-01-06 16:17:10,543 - Epoch: [83][  120/  246]    Overall Loss 0.334923    Objective Loss 0.334923                                        LR 0.000022    Time 0.017829    
2023-01-06 16:17:10,685 - Epoch: [83][  130/  246]    Overall Loss 0.335009    Objective Loss 0.335009                                        LR 0.000022    Time 0.017551    
2023-01-06 16:17:10,826 - Epoch: [83][  140/  246]    Overall Loss 0.334290    Objective Loss 0.334290                                        LR 0.000022    Time 0.017297    
2023-01-06 16:17:10,966 - Epoch: [83][  150/  246]    Overall Loss 0.334933    Objective Loss 0.334933                                        LR 0.000022    Time 0.017076    
2023-01-06 16:17:11,106 - Epoch: [83][  160/  246]    Overall Loss 0.334948    Objective Loss 0.334948                                        LR 0.000022    Time 0.016882    
2023-01-06 16:17:11,248 - Epoch: [83][  170/  246]    Overall Loss 0.334333    Objective Loss 0.334333                                        LR 0.000022    Time 0.016721    
2023-01-06 16:17:11,389 - Epoch: [83][  180/  246]    Overall Loss 0.333227    Objective Loss 0.333227                                        LR 0.000022    Time 0.016575    
2023-01-06 16:17:11,539 - Epoch: [83][  190/  246]    Overall Loss 0.334425    Objective Loss 0.334425                                        LR 0.000022    Time 0.016490    
2023-01-06 16:17:11,691 - Epoch: [83][  200/  246]    Overall Loss 0.333881    Objective Loss 0.333881                                        LR 0.000022    Time 0.016424    
2023-01-06 16:17:11,855 - Epoch: [83][  210/  246]    Overall Loss 0.333484    Objective Loss 0.333484                                        LR 0.000022    Time 0.016421    
2023-01-06 16:17:12,011 - Epoch: [83][  220/  246]    Overall Loss 0.334396    Objective Loss 0.334396                                        LR 0.000022    Time 0.016382    
2023-01-06 16:17:12,174 - Epoch: [83][  230/  246]    Overall Loss 0.333819    Objective Loss 0.333819                                        LR 0.000022    Time 0.016375    
2023-01-06 16:17:12,352 - Epoch: [83][  240/  246]    Overall Loss 0.333445    Objective Loss 0.333445                                        LR 0.000022    Time 0.016434    
2023-01-06 16:17:12,428 - Epoch: [83][  246/  246]    Overall Loss 0.333328    Objective Loss 0.333328    Top1 87.081340    LR 0.000022    Time 0.016343    
2023-01-06 16:17:12,568 - --- validate (epoch=83)-----------
2023-01-06 16:17:12,569 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:13,014 - Epoch: [83][   10/   28]    Loss 0.324260    Top1 88.320312    
2023-01-06 16:17:13,135 - Epoch: [83][   20/   28]    Loss 0.325486    Top1 88.144531    
2023-01-06 16:17:13,189 - Epoch: [83][   28/   28]    Loss 0.328816    Top1 88.061838    
2023-01-06 16:17:13,337 - ==> Top1: 88.062    Loss: 0.329

2023-01-06 16:17:13,338 - ==> Confusion:
[[ 159    4  276]
 [   8  140  454]
 [  46   46 5853]]

2023-01-06 16:17:13,339 - ==> Best [Top1: 88.062   Sparsity:0.00   Params: 46192 on epoch: 83]
2023-01-06 16:17:13,339 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:13,344 - 

2023-01-06 16:17:13,344 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:14,037 - Epoch: [84][   10/  246]    Overall Loss 0.332046    Objective Loss 0.332046                                        LR 0.000022    Time 0.069222    
2023-01-06 16:17:14,202 - Epoch: [84][   20/  246]    Overall Loss 0.325766    Objective Loss 0.325766                                        LR 0.000022    Time 0.042794    
2023-01-06 16:17:14,369 - Epoch: [84][   30/  246]    Overall Loss 0.325633    Objective Loss 0.325633                                        LR 0.000022    Time 0.034089    
2023-01-06 16:17:14,539 - Epoch: [84][   40/  246]    Overall Loss 0.331750    Objective Loss 0.331750                                        LR 0.000022    Time 0.029813    
2023-01-06 16:17:14,698 - Epoch: [84][   50/  246]    Overall Loss 0.335222    Objective Loss 0.335222                                        LR 0.000022    Time 0.027012    
2023-01-06 16:17:14,867 - Epoch: [84][   60/  246]    Overall Loss 0.333830    Objective Loss 0.333830                                        LR 0.000022    Time 0.025324    
2023-01-06 16:17:15,034 - Epoch: [84][   70/  246]    Overall Loss 0.335094    Objective Loss 0.335094                                        LR 0.000022    Time 0.024089    
2023-01-06 16:17:15,197 - Epoch: [84][   80/  246]    Overall Loss 0.335544    Objective Loss 0.335544                                        LR 0.000022    Time 0.023114    
2023-01-06 16:17:15,359 - Epoch: [84][   90/  246]    Overall Loss 0.334453    Objective Loss 0.334453                                        LR 0.000022    Time 0.022340    
2023-01-06 16:17:15,522 - Epoch: [84][  100/  246]    Overall Loss 0.333179    Objective Loss 0.333179                                        LR 0.000022    Time 0.021730    
2023-01-06 16:17:15,684 - Epoch: [84][  110/  246]    Overall Loss 0.332945    Objective Loss 0.332945                                        LR 0.000022    Time 0.021227    
2023-01-06 16:17:15,845 - Epoch: [84][  120/  246]    Overall Loss 0.333719    Objective Loss 0.333719                                        LR 0.000022    Time 0.020800    
2023-01-06 16:17:16,011 - Epoch: [84][  130/  246]    Overall Loss 0.332417    Objective Loss 0.332417                                        LR 0.000022    Time 0.020474    
2023-01-06 16:17:16,170 - Epoch: [84][  140/  246]    Overall Loss 0.332020    Objective Loss 0.332020                                        LR 0.000022    Time 0.020143    
2023-01-06 16:17:16,335 - Epoch: [84][  150/  246]    Overall Loss 0.331188    Objective Loss 0.331188                                        LR 0.000022    Time 0.019900    
2023-01-06 16:17:16,505 - Epoch: [84][  160/  246]    Overall Loss 0.331003    Objective Loss 0.331003                                        LR 0.000022    Time 0.019714    
2023-01-06 16:17:16,672 - Epoch: [84][  170/  246]    Overall Loss 0.333018    Objective Loss 0.333018                                        LR 0.000022    Time 0.019535    
2023-01-06 16:17:16,837 - Epoch: [84][  180/  246]    Overall Loss 0.333462    Objective Loss 0.333462                                        LR 0.000022    Time 0.019363    
2023-01-06 16:17:16,999 - Epoch: [84][  190/  246]    Overall Loss 0.334067    Objective Loss 0.334067                                        LR 0.000022    Time 0.019198    
2023-01-06 16:17:17,171 - Epoch: [84][  200/  246]    Overall Loss 0.333171    Objective Loss 0.333171                                        LR 0.000022    Time 0.019097    
2023-01-06 16:17:17,337 - Epoch: [84][  210/  246]    Overall Loss 0.332719    Objective Loss 0.332719                                        LR 0.000022    Time 0.018975    
2023-01-06 16:17:17,505 - Epoch: [84][  220/  246]    Overall Loss 0.333535    Objective Loss 0.333535                                        LR 0.000022    Time 0.018873    
2023-01-06 16:17:17,669 - Epoch: [84][  230/  246]    Overall Loss 0.333863    Objective Loss 0.333863                                        LR 0.000022    Time 0.018765    
2023-01-06 16:17:17,846 - Epoch: [84][  240/  246]    Overall Loss 0.333517    Objective Loss 0.333517                                        LR 0.000022    Time 0.018717    
2023-01-06 16:17:17,924 - Epoch: [84][  246/  246]    Overall Loss 0.333561    Objective Loss 0.333561    Top1 89.473684    LR 0.000022    Time 0.018580    
2023-01-06 16:17:18,059 - --- validate (epoch=84)-----------
2023-01-06 16:17:18,060 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:18,484 - Epoch: [84][   10/   28]    Loss 0.350065    Top1 87.734375    
2023-01-06 16:17:18,585 - Epoch: [84][   20/   28]    Loss 0.331504    Top1 88.164062    
2023-01-06 16:17:18,641 - Epoch: [84][   28/   28]    Loss 0.329624    Top1 88.176353    
2023-01-06 16:17:18,801 - ==> Top1: 88.176    Loss: 0.330

2023-01-06 16:17:18,801 - ==> Confusion:
[[ 171    6  262]
 [   7  162  433]
 [  50   68 5827]]

2023-01-06 16:17:18,803 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:18,803 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:18,808 - 

2023-01-06 16:17:18,809 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:19,335 - Epoch: [85][   10/  246]    Overall Loss 0.307621    Objective Loss 0.307621                                        LR 0.000022    Time 0.052594    
2023-01-06 16:17:19,486 - Epoch: [85][   20/  246]    Overall Loss 0.310726    Objective Loss 0.310726                                        LR 0.000022    Time 0.033804    
2023-01-06 16:17:19,656 - Epoch: [85][   30/  246]    Overall Loss 0.326117    Objective Loss 0.326117                                        LR 0.000022    Time 0.028184    
2023-01-06 16:17:19,835 - Epoch: [85][   40/  246]    Overall Loss 0.324228    Objective Loss 0.324228                                        LR 0.000022    Time 0.025609    
2023-01-06 16:17:20,015 - Epoch: [85][   50/  246]    Overall Loss 0.325294    Objective Loss 0.325294                                        LR 0.000022    Time 0.024075    
2023-01-06 16:17:20,183 - Epoch: [85][   60/  246]    Overall Loss 0.325666    Objective Loss 0.325666                                        LR 0.000022    Time 0.022860    
2023-01-06 16:17:20,364 - Epoch: [85][   70/  246]    Overall Loss 0.327969    Objective Loss 0.327969                                        LR 0.000022    Time 0.022178    
2023-01-06 16:17:20,539 - Epoch: [85][   80/  246]    Overall Loss 0.326924    Objective Loss 0.326924                                        LR 0.000022    Time 0.021585    
2023-01-06 16:17:20,715 - Epoch: [85][   90/  246]    Overall Loss 0.328486    Objective Loss 0.328486                                        LR 0.000022    Time 0.021133    
2023-01-06 16:17:20,900 - Epoch: [85][  100/  246]    Overall Loss 0.329552    Objective Loss 0.329552                                        LR 0.000022    Time 0.020869    
2023-01-06 16:17:21,092 - Epoch: [85][  110/  246]    Overall Loss 0.329962    Objective Loss 0.329962                                        LR 0.000022    Time 0.020713    
2023-01-06 16:17:21,271 - Epoch: [85][  120/  246]    Overall Loss 0.330886    Objective Loss 0.330886                                        LR 0.000022    Time 0.020474    
2023-01-06 16:17:21,457 - Epoch: [85][  130/  246]    Overall Loss 0.332234    Objective Loss 0.332234                                        LR 0.000022    Time 0.020326    
2023-01-06 16:17:21,633 - Epoch: [85][  140/  246]    Overall Loss 0.332018    Objective Loss 0.332018                                        LR 0.000022    Time 0.020130    
2023-01-06 16:17:21,819 - Epoch: [85][  150/  246]    Overall Loss 0.331712    Objective Loss 0.331712                                        LR 0.000022    Time 0.020025    
2023-01-06 16:17:21,995 - Epoch: [85][  160/  246]    Overall Loss 0.332846    Objective Loss 0.332846                                        LR 0.000022    Time 0.019876    
2023-01-06 16:17:22,181 - Epoch: [85][  170/  246]    Overall Loss 0.332964    Objective Loss 0.332964                                        LR 0.000022    Time 0.019799    
2023-01-06 16:17:22,358 - Epoch: [85][  180/  246]    Overall Loss 0.332642    Objective Loss 0.332642                                        LR 0.000022    Time 0.019680    
2023-01-06 16:17:22,542 - Epoch: [85][  190/  246]    Overall Loss 0.332249    Objective Loss 0.332249                                        LR 0.000022    Time 0.019611    
2023-01-06 16:17:22,720 - Epoch: [85][  200/  246]    Overall Loss 0.332139    Objective Loss 0.332139                                        LR 0.000022    Time 0.019508    
2023-01-06 16:17:22,904 - Epoch: [85][  210/  246]    Overall Loss 0.331299    Objective Loss 0.331299                                        LR 0.000022    Time 0.019452    
2023-01-06 16:17:23,081 - Epoch: [85][  220/  246]    Overall Loss 0.331815    Objective Loss 0.331815                                        LR 0.000022    Time 0.019364    
2023-01-06 16:17:23,264 - Epoch: [85][  230/  246]    Overall Loss 0.332242    Objective Loss 0.332242                                        LR 0.000022    Time 0.019316    
2023-01-06 16:17:23,451 - Epoch: [85][  240/  246]    Overall Loss 0.331587    Objective Loss 0.331587                                        LR 0.000022    Time 0.019282    
2023-01-06 16:17:23,534 - Epoch: [85][  246/  246]    Overall Loss 0.331596    Objective Loss 0.331596    Top1 86.842105    LR 0.000022    Time 0.019150    
2023-01-06 16:17:23,718 - --- validate (epoch=85)-----------
2023-01-06 16:17:23,718 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:24,150 - Epoch: [85][   10/   28]    Loss 0.329358    Top1 88.203125    
2023-01-06 16:17:24,255 - Epoch: [85][   20/   28]    Loss 0.325134    Top1 88.164062    
2023-01-06 16:17:24,314 - Epoch: [85][   28/   28]    Loss 0.332725    Top1 88.033209    
2023-01-06 16:17:24,456 - ==> Top1: 88.033    Loss: 0.333

2023-01-06 16:17:24,457 - ==> Confusion:
[[ 159    5  275]
 [   8  151  443]
 [  54   51 5840]]

2023-01-06 16:17:24,458 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:24,458 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:24,462 - 

2023-01-06 16:17:24,462 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:25,137 - Epoch: [86][   10/  246]    Overall Loss 0.354592    Objective Loss 0.354592                                        LR 0.000022    Time 0.067440    
2023-01-06 16:17:25,302 - Epoch: [86][   20/  246]    Overall Loss 0.348259    Objective Loss 0.348259                                        LR 0.000022    Time 0.041930    
2023-01-06 16:17:25,470 - Epoch: [86][   30/  246]    Overall Loss 0.343022    Objective Loss 0.343022                                        LR 0.000022    Time 0.033530    
2023-01-06 16:17:25,634 - Epoch: [86][   40/  246]    Overall Loss 0.338665    Objective Loss 0.338665                                        LR 0.000022    Time 0.029250    
2023-01-06 16:17:25,785 - Epoch: [86][   50/  246]    Overall Loss 0.338124    Objective Loss 0.338124                                        LR 0.000022    Time 0.026414    
2023-01-06 16:17:25,926 - Epoch: [86][   60/  246]    Overall Loss 0.333373    Objective Loss 0.333373                                        LR 0.000022    Time 0.024340    
2023-01-06 16:17:26,073 - Epoch: [86][   70/  246]    Overall Loss 0.332113    Objective Loss 0.332113                                        LR 0.000022    Time 0.022921    
2023-01-06 16:17:26,240 - Epoch: [86][   80/  246]    Overall Loss 0.332620    Objective Loss 0.332620                                        LR 0.000022    Time 0.022148    
2023-01-06 16:17:26,405 - Epoch: [86][   90/  246]    Overall Loss 0.330837    Objective Loss 0.330837                                        LR 0.000022    Time 0.021515    
2023-01-06 16:17:26,565 - Epoch: [86][  100/  246]    Overall Loss 0.331941    Objective Loss 0.331941                                        LR 0.000022    Time 0.020956    
2023-01-06 16:17:26,719 - Epoch: [86][  110/  246]    Overall Loss 0.332973    Objective Loss 0.332973                                        LR 0.000022    Time 0.020447    
2023-01-06 16:17:26,876 - Epoch: [86][  120/  246]    Overall Loss 0.333744    Objective Loss 0.333744                                        LR 0.000022    Time 0.020043    
2023-01-06 16:17:27,026 - Epoch: [86][  130/  246]    Overall Loss 0.333839    Objective Loss 0.333839                                        LR 0.000022    Time 0.019655    
2023-01-06 16:17:27,184 - Epoch: [86][  140/  246]    Overall Loss 0.333301    Objective Loss 0.333301                                        LR 0.000022    Time 0.019378    
2023-01-06 16:17:27,337 - Epoch: [86][  150/  246]    Overall Loss 0.333402    Objective Loss 0.333402                                        LR 0.000022    Time 0.019088    
2023-01-06 16:17:27,502 - Epoch: [86][  160/  246]    Overall Loss 0.333915    Objective Loss 0.333915                                        LR 0.000022    Time 0.018928    
2023-01-06 16:17:27,662 - Epoch: [86][  170/  246]    Overall Loss 0.334280    Objective Loss 0.334280                                        LR 0.000022    Time 0.018743    
2023-01-06 16:17:27,826 - Epoch: [86][  180/  246]    Overall Loss 0.332992    Objective Loss 0.332992                                        LR 0.000022    Time 0.018608    
2023-01-06 16:17:27,983 - Epoch: [86][  190/  246]    Overall Loss 0.331937    Objective Loss 0.331937                                        LR 0.000022    Time 0.018455    
2023-01-06 16:17:28,145 - Epoch: [86][  200/  246]    Overall Loss 0.331071    Objective Loss 0.331071                                        LR 0.000022    Time 0.018340    
2023-01-06 16:17:28,303 - Epoch: [86][  210/  246]    Overall Loss 0.330798    Objective Loss 0.330798                                        LR 0.000022    Time 0.018214    
2023-01-06 16:17:28,461 - Epoch: [86][  220/  246]    Overall Loss 0.330279    Objective Loss 0.330279                                        LR 0.000022    Time 0.018107    
2023-01-06 16:17:28,618 - Epoch: [86][  230/  246]    Overall Loss 0.331469    Objective Loss 0.331469                                        LR 0.000022    Time 0.018000    
2023-01-06 16:17:28,785 - Epoch: [86][  240/  246]    Overall Loss 0.330611    Objective Loss 0.330611                                        LR 0.000022    Time 0.017943    
2023-01-06 16:17:28,862 - Epoch: [86][  246/  246]    Overall Loss 0.331208    Objective Loss 0.331208    Top1 86.363636    LR 0.000022    Time 0.017817    
2023-01-06 16:17:28,996 - --- validate (epoch=86)-----------
2023-01-06 16:17:28,997 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:29,424 - Epoch: [86][   10/   28]    Loss 0.328798    Top1 87.617188    
2023-01-06 16:17:29,537 - Epoch: [86][   20/   28]    Loss 0.328151    Top1 88.027344    
2023-01-06 16:17:29,595 - Epoch: [86][   28/   28]    Loss 0.328994    Top1 88.133410    
2023-01-06 16:17:29,729 - ==> Top1: 88.133    Loss: 0.329

2023-01-06 16:17:29,729 - ==> Confusion:
[[ 175    6  258]
 [  10  159  433]
 [  59   63 5823]]

2023-01-06 16:17:29,730 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:29,731 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:29,739 - 

2023-01-06 16:17:29,739 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:30,428 - Epoch: [87][   10/  246]    Overall Loss 0.335220    Objective Loss 0.335220                                        LR 0.000022    Time 0.068759    
2023-01-06 16:17:30,606 - Epoch: [87][   20/  246]    Overall Loss 0.329591    Objective Loss 0.329591                                        LR 0.000022    Time 0.043281    
2023-01-06 16:17:30,791 - Epoch: [87][   30/  246]    Overall Loss 0.327967    Objective Loss 0.327967                                        LR 0.000022    Time 0.035007    
2023-01-06 16:17:30,975 - Epoch: [87][   40/  246]    Overall Loss 0.330294    Objective Loss 0.330294                                        LR 0.000022    Time 0.030838    
2023-01-06 16:17:31,164 - Epoch: [87][   50/  246]    Overall Loss 0.328577    Objective Loss 0.328577                                        LR 0.000022    Time 0.028447    
2023-01-06 16:17:31,353 - Epoch: [87][   60/  246]    Overall Loss 0.330686    Objective Loss 0.330686                                        LR 0.000022    Time 0.026843    
2023-01-06 16:17:31,542 - Epoch: [87][   70/  246]    Overall Loss 0.331243    Objective Loss 0.331243                                        LR 0.000022    Time 0.025699    
2023-01-06 16:17:31,719 - Epoch: [87][   80/  246]    Overall Loss 0.332832    Objective Loss 0.332832                                        LR 0.000022    Time 0.024704    
2023-01-06 16:17:31,888 - Epoch: [87][   90/  246]    Overall Loss 0.333565    Objective Loss 0.333565                                        LR 0.000022    Time 0.023827    
2023-01-06 16:17:32,058 - Epoch: [87][  100/  246]    Overall Loss 0.332606    Objective Loss 0.332606                                        LR 0.000022    Time 0.023139    
2023-01-06 16:17:32,227 - Epoch: [87][  110/  246]    Overall Loss 0.334746    Objective Loss 0.334746                                        LR 0.000022    Time 0.022571    
2023-01-06 16:17:32,394 - Epoch: [87][  120/  246]    Overall Loss 0.334756    Objective Loss 0.334756                                        LR 0.000022    Time 0.022083    
2023-01-06 16:17:32,562 - Epoch: [87][  130/  246]    Overall Loss 0.332661    Objective Loss 0.332661                                        LR 0.000022    Time 0.021675    
2023-01-06 16:17:32,731 - Epoch: [87][  140/  246]    Overall Loss 0.333660    Objective Loss 0.333660                                        LR 0.000022    Time 0.021327    
2023-01-06 16:17:32,899 - Epoch: [87][  150/  246]    Overall Loss 0.333502    Objective Loss 0.333502                                        LR 0.000022    Time 0.021023    
2023-01-06 16:17:33,068 - Epoch: [87][  160/  246]    Overall Loss 0.332269    Objective Loss 0.332269                                        LR 0.000022    Time 0.020764    
2023-01-06 16:17:33,236 - Epoch: [87][  170/  246]    Overall Loss 0.331462    Objective Loss 0.331462                                        LR 0.000022    Time 0.020529    
2023-01-06 16:17:33,407 - Epoch: [87][  180/  246]    Overall Loss 0.331927    Objective Loss 0.331927                                        LR 0.000022    Time 0.020340    
2023-01-06 16:17:33,577 - Epoch: [87][  190/  246]    Overall Loss 0.332755    Objective Loss 0.332755                                        LR 0.000022    Time 0.020161    
2023-01-06 16:17:33,749 - Epoch: [87][  200/  246]    Overall Loss 0.332354    Objective Loss 0.332354                                        LR 0.000022    Time 0.020010    
2023-01-06 16:17:33,917 - Epoch: [87][  210/  246]    Overall Loss 0.331492    Objective Loss 0.331492                                        LR 0.000022    Time 0.019855    
2023-01-06 16:17:34,091 - Epoch: [87][  220/  246]    Overall Loss 0.331550    Objective Loss 0.331550                                        LR 0.000022    Time 0.019742    
2023-01-06 16:17:34,254 - Epoch: [87][  230/  246]    Overall Loss 0.331851    Objective Loss 0.331851                                        LR 0.000022    Time 0.019591    
2023-01-06 16:17:34,422 - Epoch: [87][  240/  246]    Overall Loss 0.332108    Objective Loss 0.332108                                        LR 0.000022    Time 0.019474    
2023-01-06 16:17:34,501 - Epoch: [87][  246/  246]    Overall Loss 0.331523    Objective Loss 0.331523    Top1 89.952153    LR 0.000022    Time 0.019321    
2023-01-06 16:17:34,634 - --- validate (epoch=87)-----------
2023-01-06 16:17:34,634 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:35,058 - Epoch: [87][   10/   28]    Loss 0.330769    Top1 87.812500    
2023-01-06 16:17:35,161 - Epoch: [87][   20/   28]    Loss 0.326487    Top1 88.164062    
2023-01-06 16:17:35,219 - Epoch: [87][   28/   28]    Loss 0.328846    Top1 88.047524    
2023-01-06 16:17:35,359 - ==> Top1: 88.048    Loss: 0.329

2023-01-06 16:17:35,359 - ==> Confusion:
[[ 164    4  271]
 [   8  153  441]
 [  56   55 5834]]

2023-01-06 16:17:35,361 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:35,361 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:35,365 - 

2023-01-06 16:17:35,365 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:35,897 - Epoch: [88][   10/  246]    Overall Loss 0.338979    Objective Loss 0.338979                                        LR 0.000022    Time 0.053117    
2023-01-06 16:17:36,048 - Epoch: [88][   20/  246]    Overall Loss 0.333963    Objective Loss 0.333963                                        LR 0.000022    Time 0.034073    
2023-01-06 16:17:36,206 - Epoch: [88][   30/  246]    Overall Loss 0.340814    Objective Loss 0.340814                                        LR 0.000022    Time 0.027997    
2023-01-06 16:17:36,362 - Epoch: [88][   40/  246]    Overall Loss 0.336546    Objective Loss 0.336546                                        LR 0.000022    Time 0.024868    
2023-01-06 16:17:36,507 - Epoch: [88][   50/  246]    Overall Loss 0.335813    Objective Loss 0.335813                                        LR 0.000022    Time 0.022783    
2023-01-06 16:17:36,657 - Epoch: [88][   60/  246]    Overall Loss 0.335564    Objective Loss 0.335564                                        LR 0.000022    Time 0.021493    
2023-01-06 16:17:36,814 - Epoch: [88][   70/  246]    Overall Loss 0.330891    Objective Loss 0.330891                                        LR 0.000022    Time 0.020648    
2023-01-06 16:17:36,970 - Epoch: [88][   80/  246]    Overall Loss 0.329319    Objective Loss 0.329319                                        LR 0.000022    Time 0.020024    
2023-01-06 16:17:37,131 - Epoch: [88][   90/  246]    Overall Loss 0.329175    Objective Loss 0.329175                                        LR 0.000022    Time 0.019582    
2023-01-06 16:17:37,287 - Epoch: [88][  100/  246]    Overall Loss 0.329222    Objective Loss 0.329222                                        LR 0.000022    Time 0.019165    
2023-01-06 16:17:37,430 - Epoch: [88][  110/  246]    Overall Loss 0.330311    Objective Loss 0.330311                                        LR 0.000022    Time 0.018712    
2023-01-06 16:17:37,590 - Epoch: [88][  120/  246]    Overall Loss 0.330894    Objective Loss 0.330894                                        LR 0.000022    Time 0.018482    
2023-01-06 16:17:37,754 - Epoch: [88][  130/  246]    Overall Loss 0.329120    Objective Loss 0.329120                                        LR 0.000022    Time 0.018322    
2023-01-06 16:17:37,919 - Epoch: [88][  140/  246]    Overall Loss 0.328998    Objective Loss 0.328998                                        LR 0.000022    Time 0.018186    
2023-01-06 16:17:38,083 - Epoch: [88][  150/  246]    Overall Loss 0.328471    Objective Loss 0.328471                                        LR 0.000022    Time 0.018066    
2023-01-06 16:17:38,245 - Epoch: [88][  160/  246]    Overall Loss 0.328698    Objective Loss 0.328698                                        LR 0.000022    Time 0.017946    
2023-01-06 16:17:38,400 - Epoch: [88][  170/  246]    Overall Loss 0.329313    Objective Loss 0.329313                                        LR 0.000022    Time 0.017800    
2023-01-06 16:17:38,557 - Epoch: [88][  180/  246]    Overall Loss 0.328558    Objective Loss 0.328558                                        LR 0.000022    Time 0.017680    
2023-01-06 16:17:38,722 - Epoch: [88][  190/  246]    Overall Loss 0.328809    Objective Loss 0.328809                                        LR 0.000022    Time 0.017618    
2023-01-06 16:17:38,889 - Epoch: [88][  200/  246]    Overall Loss 0.329431    Objective Loss 0.329431                                        LR 0.000022    Time 0.017551    
2023-01-06 16:17:39,051 - Epoch: [88][  210/  246]    Overall Loss 0.329038    Objective Loss 0.329038                                        LR 0.000022    Time 0.017487    
2023-01-06 16:17:39,225 - Epoch: [88][  220/  246]    Overall Loss 0.329525    Objective Loss 0.329525                                        LR 0.000022    Time 0.017483    
2023-01-06 16:17:39,406 - Epoch: [88][  230/  246]    Overall Loss 0.329602    Objective Loss 0.329602                                        LR 0.000022    Time 0.017507    
2023-01-06 16:17:39,597 - Epoch: [88][  240/  246]    Overall Loss 0.329723    Objective Loss 0.329723                                        LR 0.000022    Time 0.017571    
2023-01-06 16:17:39,676 - Epoch: [88][  246/  246]    Overall Loss 0.330057    Objective Loss 0.330057    Top1 85.645933    LR 0.000022    Time 0.017463    
2023-01-06 16:17:39,854 - --- validate (epoch=88)-----------
2023-01-06 16:17:39,854 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:40,297 - Epoch: [88][   10/   28]    Loss 0.326804    Top1 88.320312    
2023-01-06 16:17:40,410 - Epoch: [88][   20/   28]    Loss 0.322875    Top1 88.417969    
2023-01-06 16:17:40,467 - Epoch: [88][   28/   28]    Loss 0.326786    Top1 88.147724    
2023-01-06 16:17:40,618 - ==> Top1: 88.148    Loss: 0.327

2023-01-06 16:17:40,618 - ==> Confusion:
[[ 172    5  262]
 [   7  152  443]
 [  60   51 5834]]

2023-01-06 16:17:40,619 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:40,619 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:40,624 - 

2023-01-06 16:17:40,624 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:41,323 - Epoch: [89][   10/  246]    Overall Loss 0.346974    Objective Loss 0.346974                                        LR 0.000022    Time 0.069895    
2023-01-06 16:17:41,490 - Epoch: [89][   20/  246]    Overall Loss 0.337231    Objective Loss 0.337231                                        LR 0.000022    Time 0.043234    
2023-01-06 16:17:41,656 - Epoch: [89][   30/  246]    Overall Loss 0.343515    Objective Loss 0.343515                                        LR 0.000022    Time 0.034349    
2023-01-06 16:17:41,822 - Epoch: [89][   40/  246]    Overall Loss 0.339930    Objective Loss 0.339930                                        LR 0.000022    Time 0.029912    
2023-01-06 16:17:41,990 - Epoch: [89][   50/  246]    Overall Loss 0.341649    Objective Loss 0.341649                                        LR 0.000022    Time 0.027272    
2023-01-06 16:17:42,156 - Epoch: [89][   60/  246]    Overall Loss 0.337648    Objective Loss 0.337648                                        LR 0.000022    Time 0.025499    
2023-01-06 16:17:42,324 - Epoch: [89][   70/  246]    Overall Loss 0.335365    Objective Loss 0.335365                                        LR 0.000022    Time 0.024250    
2023-01-06 16:17:42,489 - Epoch: [89][   80/  246]    Overall Loss 0.333435    Objective Loss 0.333435                                        LR 0.000022    Time 0.023279    
2023-01-06 16:17:42,657 - Epoch: [89][   90/  246]    Overall Loss 0.336646    Objective Loss 0.336646                                        LR 0.000022    Time 0.022555    
2023-01-06 16:17:42,818 - Epoch: [89][  100/  246]    Overall Loss 0.336066    Objective Loss 0.336066                                        LR 0.000022    Time 0.021908    
2023-01-06 16:17:42,959 - Epoch: [89][  110/  246]    Overall Loss 0.334313    Objective Loss 0.334313                                        LR 0.000022    Time 0.021189    
2023-01-06 16:17:43,109 - Epoch: [89][  120/  246]    Overall Loss 0.333973    Objective Loss 0.333973                                        LR 0.000022    Time 0.020670    
2023-01-06 16:17:43,274 - Epoch: [89][  130/  246]    Overall Loss 0.334064    Objective Loss 0.334064                                        LR 0.000022    Time 0.020348    
2023-01-06 16:17:43,415 - Epoch: [89][  140/  246]    Overall Loss 0.333245    Objective Loss 0.333245                                        LR 0.000022    Time 0.019903    
2023-01-06 16:17:43,553 - Epoch: [89][  150/  246]    Overall Loss 0.332727    Objective Loss 0.332727                                        LR 0.000022    Time 0.019492    
2023-01-06 16:17:43,699 - Epoch: [89][  160/  246]    Overall Loss 0.332366    Objective Loss 0.332366                                        LR 0.000022    Time 0.019186    
2023-01-06 16:17:43,851 - Epoch: [89][  170/  246]    Overall Loss 0.330646    Objective Loss 0.330646                                        LR 0.000022    Time 0.018947    
2023-01-06 16:17:44,005 - Epoch: [89][  180/  246]    Overall Loss 0.330935    Objective Loss 0.330935                                        LR 0.000022    Time 0.018748    
2023-01-06 16:17:44,157 - Epoch: [89][  190/  246]    Overall Loss 0.332355    Objective Loss 0.332355                                        LR 0.000022    Time 0.018561    
2023-01-06 16:17:44,293 - Epoch: [89][  200/  246]    Overall Loss 0.331846    Objective Loss 0.331846                                        LR 0.000022    Time 0.018315    
2023-01-06 16:17:44,435 - Epoch: [89][  210/  246]    Overall Loss 0.331497    Objective Loss 0.331497                                        LR 0.000022    Time 0.018116    
2023-01-06 16:17:44,578 - Epoch: [89][  220/  246]    Overall Loss 0.331863    Objective Loss 0.331863                                        LR 0.000022    Time 0.017938    
2023-01-06 16:17:44,720 - Epoch: [89][  230/  246]    Overall Loss 0.331018    Objective Loss 0.331018                                        LR 0.000022    Time 0.017778    
2023-01-06 16:17:44,874 - Epoch: [89][  240/  246]    Overall Loss 0.330427    Objective Loss 0.330427                                        LR 0.000022    Time 0.017675    
2023-01-06 16:17:44,948 - Epoch: [89][  246/  246]    Overall Loss 0.329837    Objective Loss 0.329837    Top1 90.191388    LR 0.000022    Time 0.017546    
2023-01-06 16:17:45,096 - --- validate (epoch=89)-----------
2023-01-06 16:17:45,096 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:45,538 - Epoch: [89][   10/   28]    Loss 0.324967    Top1 87.734375    
2023-01-06 16:17:45,641 - Epoch: [89][   20/   28]    Loss 0.328355    Top1 87.890625    
2023-01-06 16:17:45,697 - Epoch: [89][   28/   28]    Loss 0.327817    Top1 87.890066    
2023-01-06 16:17:45,836 - ==> Top1: 87.890    Loss: 0.328

2023-01-06 16:17:45,836 - ==> Confusion:
[[ 151    4  284]
 [   8  135  459]
 [  49   42 5854]]

2023-01-06 16:17:45,837 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:45,837 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:45,842 - 

2023-01-06 16:17:45,842 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:46,513 - Epoch: [90][   10/  246]    Overall Loss 0.340579    Objective Loss 0.340579                                        LR 0.000022    Time 0.067061    
2023-01-06 16:17:46,684 - Epoch: [90][   20/  246]    Overall Loss 0.348355    Objective Loss 0.348355                                        LR 0.000022    Time 0.042072    
2023-01-06 16:17:46,843 - Epoch: [90][   30/  246]    Overall Loss 0.349752    Objective Loss 0.349752                                        LR 0.000022    Time 0.033323    
2023-01-06 16:17:47,013 - Epoch: [90][   40/  246]    Overall Loss 0.338661    Objective Loss 0.338661                                        LR 0.000022    Time 0.029227    
2023-01-06 16:17:47,182 - Epoch: [90][   50/  246]    Overall Loss 0.335237    Objective Loss 0.335237                                        LR 0.000022    Time 0.026756    
2023-01-06 16:17:47,351 - Epoch: [90][   60/  246]    Overall Loss 0.327556    Objective Loss 0.327556                                        LR 0.000022    Time 0.025102    
2023-01-06 16:17:47,523 - Epoch: [90][   70/  246]    Overall Loss 0.328886    Objective Loss 0.328886                                        LR 0.000022    Time 0.023968    
2023-01-06 16:17:47,700 - Epoch: [90][   80/  246]    Overall Loss 0.326825    Objective Loss 0.326825                                        LR 0.000022    Time 0.023178    
2023-01-06 16:17:47,870 - Epoch: [90][   90/  246]    Overall Loss 0.325149    Objective Loss 0.325149                                        LR 0.000022    Time 0.022496    
2023-01-06 16:17:48,048 - Epoch: [90][  100/  246]    Overall Loss 0.327237    Objective Loss 0.327237                                        LR 0.000022    Time 0.022013    
2023-01-06 16:17:48,216 - Epoch: [90][  110/  246]    Overall Loss 0.325334    Objective Loss 0.325334                                        LR 0.000022    Time 0.021542    
2023-01-06 16:17:48,389 - Epoch: [90][  120/  246]    Overall Loss 0.324352    Objective Loss 0.324352                                        LR 0.000022    Time 0.021182    
2023-01-06 16:17:48,558 - Epoch: [90][  130/  246]    Overall Loss 0.324231    Objective Loss 0.324231                                        LR 0.000022    Time 0.020851    
2023-01-06 16:17:48,731 - Epoch: [90][  140/  246]    Overall Loss 0.326218    Objective Loss 0.326218                                        LR 0.000022    Time 0.020593    
2023-01-06 16:17:48,902 - Epoch: [90][  150/  246]    Overall Loss 0.326651    Objective Loss 0.326651                                        LR 0.000022    Time 0.020355    
2023-01-06 16:17:49,076 - Epoch: [90][  160/  246]    Overall Loss 0.327929    Objective Loss 0.327929                                        LR 0.000022    Time 0.020166    
2023-01-06 16:17:49,244 - Epoch: [90][  170/  246]    Overall Loss 0.327722    Objective Loss 0.327722                                        LR 0.000022    Time 0.019968    
2023-01-06 16:17:49,414 - Epoch: [90][  180/  246]    Overall Loss 0.327902    Objective Loss 0.327902                                        LR 0.000022    Time 0.019800    
2023-01-06 16:17:49,591 - Epoch: [90][  190/  246]    Overall Loss 0.327613    Objective Loss 0.327613                                        LR 0.000022    Time 0.019687    
2023-01-06 16:17:49,758 - Epoch: [90][  200/  246]    Overall Loss 0.327149    Objective Loss 0.327149                                        LR 0.000022    Time 0.019539    
2023-01-06 16:17:49,935 - Epoch: [90][  210/  246]    Overall Loss 0.327841    Objective Loss 0.327841                                        LR 0.000022    Time 0.019447    
2023-01-06 16:17:50,120 - Epoch: [90][  220/  246]    Overall Loss 0.328129    Objective Loss 0.328129                                        LR 0.000022    Time 0.019400    
2023-01-06 16:17:50,297 - Epoch: [90][  230/  246]    Overall Loss 0.328326    Objective Loss 0.328326                                        LR 0.000022    Time 0.019325    
2023-01-06 16:17:50,487 - Epoch: [90][  240/  246]    Overall Loss 0.328710    Objective Loss 0.328710                                        LR 0.000022    Time 0.019312    
2023-01-06 16:17:50,567 - Epoch: [90][  246/  246]    Overall Loss 0.327977    Objective Loss 0.327977    Top1 87.320574    LR 0.000022    Time 0.019166    
2023-01-06 16:17:50,709 - --- validate (epoch=90)-----------
2023-01-06 16:17:50,709 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:51,141 - Epoch: [90][   10/   28]    Loss 0.341508    Top1 86.796875    
2023-01-06 16:17:51,250 - Epoch: [90][   20/   28]    Loss 0.319269    Top1 88.027344    
2023-01-06 16:17:51,309 - Epoch: [90][   28/   28]    Loss 0.325754    Top1 87.861437    
2023-01-06 16:17:51,467 - ==> Top1: 87.861    Loss: 0.326

2023-01-06 16:17:51,467 - ==> Confusion:
[[ 154    5  280]
 [   8  149  445]
 [  54   56 5835]]

2023-01-06 16:17:51,469 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:51,469 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:51,473 - 

2023-01-06 16:17:51,473 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:52,028 - Epoch: [91][   10/  246]    Overall Loss 0.326913    Objective Loss 0.326913                                        LR 0.000022    Time 0.055393    
2023-01-06 16:17:52,189 - Epoch: [91][   20/  246]    Overall Loss 0.326217    Objective Loss 0.326217                                        LR 0.000022    Time 0.035735    
2023-01-06 16:17:52,368 - Epoch: [91][   30/  246]    Overall Loss 0.324547    Objective Loss 0.324547                                        LR 0.000022    Time 0.029792    
2023-01-06 16:17:52,540 - Epoch: [91][   40/  246]    Overall Loss 0.324882    Objective Loss 0.324882                                        LR 0.000022    Time 0.026583    
2023-01-06 16:17:52,720 - Epoch: [91][   50/  246]    Overall Loss 0.324887    Objective Loss 0.324887                                        LR 0.000022    Time 0.024847    
2023-01-06 16:17:52,892 - Epoch: [91][   60/  246]    Overall Loss 0.325953    Objective Loss 0.325953                                        LR 0.000022    Time 0.023567    
2023-01-06 16:17:53,061 - Epoch: [91][   70/  246]    Overall Loss 0.323216    Objective Loss 0.323216                                        LR 0.000022    Time 0.022617    
2023-01-06 16:17:53,234 - Epoch: [91][   80/  246]    Overall Loss 0.322066    Objective Loss 0.322066                                        LR 0.000022    Time 0.021937    
2023-01-06 16:17:53,406 - Epoch: [91][   90/  246]    Overall Loss 0.321438    Objective Loss 0.321438                                        LR 0.000022    Time 0.021416    
2023-01-06 16:17:53,575 - Epoch: [91][  100/  246]    Overall Loss 0.322406    Objective Loss 0.322406                                        LR 0.000022    Time 0.020957    
2023-01-06 16:17:53,735 - Epoch: [91][  110/  246]    Overall Loss 0.325709    Objective Loss 0.325709                                        LR 0.000022    Time 0.020504    
2023-01-06 16:17:53,913 - Epoch: [91][  120/  246]    Overall Loss 0.325061    Objective Loss 0.325061                                        LR 0.000022    Time 0.020274    
2023-01-06 16:17:54,098 - Epoch: [91][  130/  246]    Overall Loss 0.324871    Objective Loss 0.324871                                        LR 0.000022    Time 0.020133    
2023-01-06 16:17:54,306 - Epoch: [91][  140/  246]    Overall Loss 0.323373    Objective Loss 0.323373                                        LR 0.000022    Time 0.020182    
2023-01-06 16:17:54,519 - Epoch: [91][  150/  246]    Overall Loss 0.323469    Objective Loss 0.323469                                        LR 0.000022    Time 0.020253    
2023-01-06 16:17:54,732 - Epoch: [91][  160/  246]    Overall Loss 0.323520    Objective Loss 0.323520                                        LR 0.000022    Time 0.020316    
2023-01-06 16:17:54,944 - Epoch: [91][  170/  246]    Overall Loss 0.324774    Objective Loss 0.324774                                        LR 0.000022    Time 0.020361    
2023-01-06 16:17:55,153 - Epoch: [91][  180/  246]    Overall Loss 0.326057    Objective Loss 0.326057                                        LR 0.000022    Time 0.020392    
2023-01-06 16:17:55,364 - Epoch: [91][  190/  246]    Overall Loss 0.326827    Objective Loss 0.326827                                        LR 0.000022    Time 0.020427    
2023-01-06 16:17:55,573 - Epoch: [91][  200/  246]    Overall Loss 0.327274    Objective Loss 0.327274                                        LR 0.000022    Time 0.020446    
2023-01-06 16:17:55,783 - Epoch: [91][  210/  246]    Overall Loss 0.327924    Objective Loss 0.327924                                        LR 0.000022    Time 0.020471    
2023-01-06 16:17:55,988 - Epoch: [91][  220/  246]    Overall Loss 0.327977    Objective Loss 0.327977                                        LR 0.000022    Time 0.020470    
2023-01-06 16:17:56,166 - Epoch: [91][  230/  246]    Overall Loss 0.327266    Objective Loss 0.327266                                        LR 0.000022    Time 0.020354    
2023-01-06 16:17:56,339 - Epoch: [91][  240/  246]    Overall Loss 0.327739    Objective Loss 0.327739                                        LR 0.000022    Time 0.020224    
2023-01-06 16:17:56,421 - Epoch: [91][  246/  246]    Overall Loss 0.327707    Objective Loss 0.327707    Top1 88.755981    LR 0.000022    Time 0.020064    
2023-01-06 16:17:56,593 - --- validate (epoch=91)-----------
2023-01-06 16:17:56,594 - 6986 samples (256 per mini-batch)
2023-01-06 16:17:57,026 - Epoch: [91][   10/   28]    Loss 0.322726    Top1 88.164062    
2023-01-06 16:17:57,129 - Epoch: [91][   20/   28]    Loss 0.317903    Top1 88.281250    
2023-01-06 16:17:57,185 - Epoch: [91][   28/   28]    Loss 0.327606    Top1 88.004581    
2023-01-06 16:17:57,320 - ==> Top1: 88.005    Loss: 0.328

2023-01-06 16:17:57,320 - ==> Confusion:
[[ 177    6  256]
 [   9  153  440]
 [  73   54 5818]]

2023-01-06 16:17:57,321 - ==> Best [Top1: 88.176   Sparsity:0.00   Params: 46192 on epoch: 84]
2023-01-06 16:17:57,322 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:17:57,326 - 

2023-01-06 16:17:57,326 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:17:58,006 - Epoch: [92][   10/  246]    Overall Loss 0.323440    Objective Loss 0.323440                                        LR 0.000022    Time 0.067926    
2023-01-06 16:17:58,175 - Epoch: [92][   20/  246]    Overall Loss 0.331802    Objective Loss 0.331802                                        LR 0.000022    Time 0.042391    
2023-01-06 16:17:58,354 - Epoch: [92][   30/  246]    Overall Loss 0.329265    Objective Loss 0.329265                                        LR 0.000022    Time 0.034227    
2023-01-06 16:17:58,536 - Epoch: [92][   40/  246]    Overall Loss 0.329646    Objective Loss 0.329646                                        LR 0.000022    Time 0.030199    
2023-01-06 16:17:58,717 - Epoch: [92][   50/  246]    Overall Loss 0.333885    Objective Loss 0.333885                                        LR 0.000022    Time 0.027755    
2023-01-06 16:17:58,903 - Epoch: [92][   60/  246]    Overall Loss 0.334384    Objective Loss 0.334384                                        LR 0.000022    Time 0.026229    
2023-01-06 16:17:59,079 - Epoch: [92][   70/  246]    Overall Loss 0.333339    Objective Loss 0.333339                                        LR 0.000022    Time 0.024968    
2023-01-06 16:17:59,260 - Epoch: [92][   80/  246]    Overall Loss 0.332315    Objective Loss 0.332315                                        LR 0.000022    Time 0.024111    
2023-01-06 16:17:59,435 - Epoch: [92][   90/  246]    Overall Loss 0.330758    Objective Loss 0.330758                                        LR 0.000022    Time 0.023349    
2023-01-06 16:17:59,616 - Epoch: [92][  100/  246]    Overall Loss 0.330737    Objective Loss 0.330737                                        LR 0.000022    Time 0.022826    
2023-01-06 16:17:59,787 - Epoch: [92][  110/  246]    Overall Loss 0.331138    Objective Loss 0.331138                                        LR 0.000022    Time 0.022282    
2023-01-06 16:17:59,956 - Epoch: [92][  120/  246]    Overall Loss 0.330578    Objective Loss 0.330578                                        LR 0.000022    Time 0.021829    
2023-01-06 16:18:00,125 - Epoch: [92][  130/  246]    Overall Loss 0.330987    Objective Loss 0.330987                                        LR 0.000022    Time 0.021449    
2023-01-06 16:18:00,299 - Epoch: [92][  140/  246]    Overall Loss 0.331759    Objective Loss 0.331759                                        LR 0.000022    Time 0.021148    
2023-01-06 16:18:00,472 - Epoch: [92][  150/  246]    Overall Loss 0.330694    Objective Loss 0.330694                                        LR 0.000022    Time 0.020886    
2023-01-06 16:18:00,644 - Epoch: [92][  160/  246]    Overall Loss 0.330138    Objective Loss 0.330138                                        LR 0.000022    Time 0.020651    
2023-01-06 16:18:00,817 - Epoch: [92][  170/  246]    Overall Loss 0.331267    Objective Loss 0.331267                                        LR 0.000022    Time 0.020451    
2023-01-06 16:18:00,978 - Epoch: [92][  180/  246]    Overall Loss 0.330907    Objective Loss 0.330907                                        LR 0.000022    Time 0.020208    
2023-01-06 16:18:01,135 - Epoch: [92][  190/  246]    Overall Loss 0.330627    Objective Loss 0.330627                                        LR 0.000022    Time 0.019968    
2023-01-06 16:18:01,293 - Epoch: [92][  200/  246]    Overall Loss 0.328624    Objective Loss 0.328624                                        LR 0.000022    Time 0.019758    
2023-01-06 16:18:01,451 - Epoch: [92][  210/  246]    Overall Loss 0.327941    Objective Loss 0.327941                                        LR 0.000022    Time 0.019570    
2023-01-06 16:18:01,610 - Epoch: [92][  220/  246]    Overall Loss 0.328096    Objective Loss 0.328096                                        LR 0.000022    Time 0.019399    
2023-01-06 16:18:01,766 - Epoch: [92][  230/  246]    Overall Loss 0.327732    Objective Loss 0.327732                                        LR 0.000022    Time 0.019234    
2023-01-06 16:18:01,936 - Epoch: [92][  240/  246]    Overall Loss 0.327159    Objective Loss 0.327159                                        LR 0.000022    Time 0.019139    
2023-01-06 16:18:02,018 - Epoch: [92][  246/  246]    Overall Loss 0.327594    Objective Loss 0.327594    Top1 87.799043    LR 0.000022    Time 0.019005    
2023-01-06 16:18:02,169 - --- validate (epoch=92)-----------
2023-01-06 16:18:02,169 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:02,596 - Epoch: [92][   10/   28]    Loss 0.313644    Top1 89.140625    
2023-01-06 16:18:02,702 - Epoch: [92][   20/   28]    Loss 0.325503    Top1 88.222656    
2023-01-06 16:18:02,757 - Epoch: [92][   28/   28]    Loss 0.324702    Top1 88.190667    
2023-01-06 16:18:02,920 - ==> Top1: 88.191    Loss: 0.325

2023-01-06 16:18:02,920 - ==> Confusion:
[[ 169    6  264]
 [  11  168  423]
 [  53   68 5824]]

2023-01-06 16:18:02,921 - ==> Best [Top1: 88.191   Sparsity:0.00   Params: 46192 on epoch: 92]
2023-01-06 16:18:02,921 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:02,926 - 

2023-01-06 16:18:02,926 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:03,445 - Epoch: [93][   10/  246]    Overall Loss 0.337462    Objective Loss 0.337462                                        LR 0.000022    Time 0.051775    
2023-01-06 16:18:03,591 - Epoch: [93][   20/  246]    Overall Loss 0.346389    Objective Loss 0.346389                                        LR 0.000022    Time 0.033183    
2023-01-06 16:18:03,740 - Epoch: [93][   30/  246]    Overall Loss 0.342237    Objective Loss 0.342237                                        LR 0.000022    Time 0.027086    
2023-01-06 16:18:03,887 - Epoch: [93][   40/  246]    Overall Loss 0.343631    Objective Loss 0.343631                                        LR 0.000022    Time 0.023986    
2023-01-06 16:18:04,031 - Epoch: [93][   50/  246]    Overall Loss 0.336512    Objective Loss 0.336512                                        LR 0.000022    Time 0.022044    
2023-01-06 16:18:04,167 - Epoch: [93][   60/  246]    Overall Loss 0.334145    Objective Loss 0.334145                                        LR 0.000022    Time 0.020629    
2023-01-06 16:18:04,301 - Epoch: [93][   70/  246]    Overall Loss 0.333700    Objective Loss 0.333700                                        LR 0.000022    Time 0.019599    
2023-01-06 16:18:04,440 - Epoch: [93][   80/  246]    Overall Loss 0.332068    Objective Loss 0.332068                                        LR 0.000022    Time 0.018873    
2023-01-06 16:18:04,577 - Epoch: [93][   90/  246]    Overall Loss 0.331726    Objective Loss 0.331726                                        LR 0.000022    Time 0.018289    
2023-01-06 16:18:04,713 - Epoch: [93][  100/  246]    Overall Loss 0.331737    Objective Loss 0.331737                                        LR 0.000022    Time 0.017816    
2023-01-06 16:18:04,855 - Epoch: [93][  110/  246]    Overall Loss 0.329614    Objective Loss 0.329614                                        LR 0.000022    Time 0.017481    
2023-01-06 16:18:05,018 - Epoch: [93][  120/  246]    Overall Loss 0.329165    Objective Loss 0.329165                                        LR 0.000022    Time 0.017381    
2023-01-06 16:18:05,176 - Epoch: [93][  130/  246]    Overall Loss 0.329579    Objective Loss 0.329579                                        LR 0.000022    Time 0.017254    
2023-01-06 16:18:05,336 - Epoch: [93][  140/  246]    Overall Loss 0.329020    Objective Loss 0.329020                                        LR 0.000022    Time 0.017162    
2023-01-06 16:18:05,492 - Epoch: [93][  150/  246]    Overall Loss 0.328822    Objective Loss 0.328822                                        LR 0.000022    Time 0.017054    
2023-01-06 16:18:05,649 - Epoch: [93][  160/  246]    Overall Loss 0.328619    Objective Loss 0.328619                                        LR 0.000022    Time 0.016972    
2023-01-06 16:18:05,808 - Epoch: [93][  170/  246]    Overall Loss 0.328807    Objective Loss 0.328807                                        LR 0.000022    Time 0.016907    
2023-01-06 16:18:05,965 - Epoch: [93][  180/  246]    Overall Loss 0.329328    Objective Loss 0.329328                                        LR 0.000022    Time 0.016839    
2023-01-06 16:18:06,124 - Epoch: [93][  190/  246]    Overall Loss 0.329828    Objective Loss 0.329828                                        LR 0.000022    Time 0.016786    
2023-01-06 16:18:06,285 - Epoch: [93][  200/  246]    Overall Loss 0.328647    Objective Loss 0.328647                                        LR 0.000022    Time 0.016748    
2023-01-06 16:18:06,443 - Epoch: [93][  210/  246]    Overall Loss 0.328602    Objective Loss 0.328602                                        LR 0.000022    Time 0.016701    
2023-01-06 16:18:06,600 - Epoch: [93][  220/  246]    Overall Loss 0.327475    Objective Loss 0.327475                                        LR 0.000022    Time 0.016656    
2023-01-06 16:18:06,758 - Epoch: [93][  230/  246]    Overall Loss 0.327085    Objective Loss 0.327085                                        LR 0.000022    Time 0.016618    
2023-01-06 16:18:06,939 - Epoch: [93][  240/  246]    Overall Loss 0.327027    Objective Loss 0.327027                                        LR 0.000022    Time 0.016679    
2023-01-06 16:18:07,021 - Epoch: [93][  246/  246]    Overall Loss 0.326398    Objective Loss 0.326398    Top1 86.602871    LR 0.000022    Time 0.016603    
2023-01-06 16:18:07,161 - --- validate (epoch=93)-----------
2023-01-06 16:18:07,161 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:07,595 - Epoch: [93][   10/   28]    Loss 0.331639    Top1 87.968750    
2023-01-06 16:18:07,714 - Epoch: [93][   20/   28]    Loss 0.322964    Top1 88.027344    
2023-01-06 16:18:07,770 - Epoch: [93][   28/   28]    Loss 0.321068    Top1 88.133410    
2023-01-06 16:18:07,908 - ==> Top1: 88.133    Loss: 0.321

2023-01-06 16:18:07,908 - ==> Confusion:
[[ 161    6  272]
 [   7  164  431]
 [  55   58 5832]]

2023-01-06 16:18:07,910 - ==> Best [Top1: 88.191   Sparsity:0.00   Params: 46192 on epoch: 92]
2023-01-06 16:18:07,910 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:07,914 - 

2023-01-06 16:18:07,914 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:08,575 - Epoch: [94][   10/  246]    Overall Loss 0.320112    Objective Loss 0.320112                                        LR 0.000022    Time 0.066014    
2023-01-06 16:18:08,734 - Epoch: [94][   20/  246]    Overall Loss 0.330242    Objective Loss 0.330242                                        LR 0.000022    Time 0.040918    
2023-01-06 16:18:08,882 - Epoch: [94][   30/  246]    Overall Loss 0.325832    Objective Loss 0.325832                                        LR 0.000022    Time 0.032218    
2023-01-06 16:18:09,044 - Epoch: [94][   40/  246]    Overall Loss 0.327587    Objective Loss 0.327587                                        LR 0.000022    Time 0.028201    
2023-01-06 16:18:09,228 - Epoch: [94][   50/  246]    Overall Loss 0.325064    Objective Loss 0.325064                                        LR 0.000022    Time 0.026233    
2023-01-06 16:18:09,430 - Epoch: [94][   60/  246]    Overall Loss 0.325160    Objective Loss 0.325160                                        LR 0.000022    Time 0.025214    
2023-01-06 16:18:09,634 - Epoch: [94][   70/  246]    Overall Loss 0.324588    Objective Loss 0.324588                                        LR 0.000022    Time 0.024514    
2023-01-06 16:18:09,843 - Epoch: [94][   80/  246]    Overall Loss 0.323573    Objective Loss 0.323573                                        LR 0.000022    Time 0.024065    
2023-01-06 16:18:10,059 - Epoch: [94][   90/  246]    Overall Loss 0.323074    Objective Loss 0.323074                                        LR 0.000022    Time 0.023781    
2023-01-06 16:18:10,280 - Epoch: [94][  100/  246]    Overall Loss 0.322561    Objective Loss 0.322561                                        LR 0.000022    Time 0.023613    
2023-01-06 16:18:10,481 - Epoch: [94][  110/  246]    Overall Loss 0.322569    Objective Loss 0.322569                                        LR 0.000022    Time 0.023288    
2023-01-06 16:18:10,655 - Epoch: [94][  120/  246]    Overall Loss 0.322452    Objective Loss 0.322452                                        LR 0.000022    Time 0.022793    
2023-01-06 16:18:10,825 - Epoch: [94][  130/  246]    Overall Loss 0.324060    Objective Loss 0.324060                                        LR 0.000022    Time 0.022342    
2023-01-06 16:18:10,993 - Epoch: [94][  140/  246]    Overall Loss 0.324753    Objective Loss 0.324753                                        LR 0.000022    Time 0.021941    
2023-01-06 16:18:11,158 - Epoch: [94][  150/  246]    Overall Loss 0.323868    Objective Loss 0.323868                                        LR 0.000022    Time 0.021580    
2023-01-06 16:18:11,327 - Epoch: [94][  160/  246]    Overall Loss 0.324652    Objective Loss 0.324652                                        LR 0.000022    Time 0.021285    
2023-01-06 16:18:11,501 - Epoch: [94][  170/  246]    Overall Loss 0.324681    Objective Loss 0.324681                                        LR 0.000022    Time 0.021052    
2023-01-06 16:18:11,671 - Epoch: [94][  180/  246]    Overall Loss 0.325846    Objective Loss 0.325846                                        LR 0.000022    Time 0.020827    
2023-01-06 16:18:11,844 - Epoch: [94][  190/  246]    Overall Loss 0.325516    Objective Loss 0.325516                                        LR 0.000022    Time 0.020640    
2023-01-06 16:18:12,015 - Epoch: [94][  200/  246]    Overall Loss 0.325799    Objective Loss 0.325799                                        LR 0.000022    Time 0.020460    
2023-01-06 16:18:12,188 - Epoch: [94][  210/  246]    Overall Loss 0.325553    Objective Loss 0.325553                                        LR 0.000022    Time 0.020308    
2023-01-06 16:18:12,359 - Epoch: [94][  220/  246]    Overall Loss 0.325517    Objective Loss 0.325517                                        LR 0.000022    Time 0.020163    
2023-01-06 16:18:12,533 - Epoch: [94][  230/  246]    Overall Loss 0.325295    Objective Loss 0.325295                                        LR 0.000022    Time 0.020040    
2023-01-06 16:18:12,713 - Epoch: [94][  240/  246]    Overall Loss 0.325644    Objective Loss 0.325644                                        LR 0.000022    Time 0.019956    
2023-01-06 16:18:12,794 - Epoch: [94][  246/  246]    Overall Loss 0.325724    Objective Loss 0.325724    Top1 87.559809    LR 0.000022    Time 0.019797    
2023-01-06 16:18:12,923 - --- validate (epoch=94)-----------
2023-01-06 16:18:12,924 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:13,370 - Epoch: [94][   10/   28]    Loss 0.329598    Top1 87.734375    
2023-01-06 16:18:13,479 - Epoch: [94][   20/   28]    Loss 0.325605    Top1 87.949219    
2023-01-06 16:18:13,540 - Epoch: [94][   28/   28]    Loss 0.321140    Top1 88.004581    
2023-01-06 16:18:13,704 - ==> Top1: 88.005    Loss: 0.321

2023-01-06 16:18:13,704 - ==> Confusion:
[[ 155    5  279]
 [   7  151  444]
 [  49   54 5842]]

2023-01-06 16:18:13,705 - ==> Best [Top1: 88.191   Sparsity:0.00   Params: 46192 on epoch: 92]
2023-01-06 16:18:13,705 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:13,710 - 

2023-01-06 16:18:13,710 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:14,380 - Epoch: [95][   10/  246]    Overall Loss 0.316602    Objective Loss 0.316602                                        LR 0.000022    Time 0.066928    
2023-01-06 16:18:14,522 - Epoch: [95][   20/  246]    Overall Loss 0.327231    Objective Loss 0.327231                                        LR 0.000022    Time 0.040562    
2023-01-06 16:18:14,660 - Epoch: [95][   30/  246]    Overall Loss 0.330797    Objective Loss 0.330797                                        LR 0.000022    Time 0.031621    
2023-01-06 16:18:14,802 - Epoch: [95][   40/  246]    Overall Loss 0.332414    Objective Loss 0.332414                                        LR 0.000022    Time 0.027258    
2023-01-06 16:18:14,941 - Epoch: [95][   50/  246]    Overall Loss 0.330283    Objective Loss 0.330283                                        LR 0.000022    Time 0.024576    
2023-01-06 16:18:15,079 - Epoch: [95][   60/  246]    Overall Loss 0.328536    Objective Loss 0.328536                                        LR 0.000022    Time 0.022780    
2023-01-06 16:18:15,231 - Epoch: [95][   70/  246]    Overall Loss 0.327470    Objective Loss 0.327470                                        LR 0.000022    Time 0.021692    
2023-01-06 16:18:15,393 - Epoch: [95][   80/  246]    Overall Loss 0.330366    Objective Loss 0.330366                                        LR 0.000022    Time 0.021006    
2023-01-06 16:18:15,558 - Epoch: [95][   90/  246]    Overall Loss 0.328440    Objective Loss 0.328440                                        LR 0.000022    Time 0.020497    
2023-01-06 16:18:15,724 - Epoch: [95][  100/  246]    Overall Loss 0.328870    Objective Loss 0.328870                                        LR 0.000022    Time 0.020100    
2023-01-06 16:18:15,887 - Epoch: [95][  110/  246]    Overall Loss 0.326599    Objective Loss 0.326599                                        LR 0.000022    Time 0.019761    
2023-01-06 16:18:16,052 - Epoch: [95][  120/  246]    Overall Loss 0.324133    Objective Loss 0.324133                                        LR 0.000022    Time 0.019483    
2023-01-06 16:18:16,217 - Epoch: [95][  130/  246]    Overall Loss 0.323461    Objective Loss 0.323461                                        LR 0.000022    Time 0.019246    
2023-01-06 16:18:16,384 - Epoch: [95][  140/  246]    Overall Loss 0.322983    Objective Loss 0.322983                                        LR 0.000022    Time 0.019062    
2023-01-06 16:18:16,552 - Epoch: [95][  150/  246]    Overall Loss 0.323548    Objective Loss 0.323548                                        LR 0.000022    Time 0.018909    
2023-01-06 16:18:16,719 - Epoch: [95][  160/  246]    Overall Loss 0.322874    Objective Loss 0.322874                                        LR 0.000022    Time 0.018774    
2023-01-06 16:18:16,888 - Epoch: [95][  170/  246]    Overall Loss 0.323096    Objective Loss 0.323096                                        LR 0.000022    Time 0.018660    
2023-01-06 16:18:17,057 - Epoch: [95][  180/  246]    Overall Loss 0.322321    Objective Loss 0.322321                                        LR 0.000022    Time 0.018560    
2023-01-06 16:18:17,198 - Epoch: [95][  190/  246]    Overall Loss 0.322748    Objective Loss 0.322748                                        LR 0.000022    Time 0.018324    
2023-01-06 16:18:17,355 - Epoch: [95][  200/  246]    Overall Loss 0.323039    Objective Loss 0.323039                                        LR 0.000022    Time 0.018189    
2023-01-06 16:18:17,518 - Epoch: [95][  210/  246]    Overall Loss 0.323467    Objective Loss 0.323467                                        LR 0.000022    Time 0.018097    
2023-01-06 16:18:17,681 - Epoch: [95][  220/  246]    Overall Loss 0.323139    Objective Loss 0.323139                                        LR 0.000022    Time 0.018015    
2023-01-06 16:18:17,832 - Epoch: [95][  230/  246]    Overall Loss 0.324107    Objective Loss 0.324107                                        LR 0.000022    Time 0.017888    
2023-01-06 16:18:18,004 - Epoch: [95][  240/  246]    Overall Loss 0.324251    Objective Loss 0.324251                                        LR 0.000022    Time 0.017858    
2023-01-06 16:18:18,086 - Epoch: [95][  246/  246]    Overall Loss 0.324585    Objective Loss 0.324585    Top1 84.688995    LR 0.000022    Time 0.017754    
2023-01-06 16:18:18,208 - --- validate (epoch=95)-----------
2023-01-06 16:18:18,209 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:18,629 - Epoch: [95][   10/   28]    Loss 0.296859    Top1 89.492188    
2023-01-06 16:18:18,728 - Epoch: [95][   20/   28]    Loss 0.315104    Top1 88.476562    
2023-01-06 16:18:18,783 - Epoch: [95][   28/   28]    Loss 0.320122    Top1 88.162038    
2023-01-06 16:18:18,948 - ==> Top1: 88.162    Loss: 0.320

2023-01-06 16:18:18,948 - ==> Confusion:
[[ 168    5  266]
 [   8  169  425]
 [  62   61 5822]]

2023-01-06 16:18:18,949 - ==> Best [Top1: 88.191   Sparsity:0.00   Params: 46192 on epoch: 92]
2023-01-06 16:18:18,949 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:18,954 - 

2023-01-06 16:18:18,954 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:19,507 - Epoch: [96][   10/  246]    Overall Loss 0.298117    Objective Loss 0.298117                                        LR 0.000022    Time 0.055250    
2023-01-06 16:18:19,676 - Epoch: [96][   20/  246]    Overall Loss 0.313818    Objective Loss 0.313818                                        LR 0.000022    Time 0.036013    
2023-01-06 16:18:19,846 - Epoch: [96][   30/  246]    Overall Loss 0.312818    Objective Loss 0.312818                                        LR 0.000022    Time 0.029662    
2023-01-06 16:18:20,011 - Epoch: [96][   40/  246]    Overall Loss 0.313400    Objective Loss 0.313400                                        LR 0.000022    Time 0.026368    
2023-01-06 16:18:20,180 - Epoch: [96][   50/  246]    Overall Loss 0.315750    Objective Loss 0.315750                                        LR 0.000022    Time 0.024470    
2023-01-06 16:18:20,348 - Epoch: [96][   60/  246]    Overall Loss 0.316123    Objective Loss 0.316123                                        LR 0.000022    Time 0.023179    
2023-01-06 16:18:20,525 - Epoch: [96][   70/  246]    Overall Loss 0.318307    Objective Loss 0.318307                                        LR 0.000022    Time 0.022397    
2023-01-06 16:18:20,695 - Epoch: [96][   80/  246]    Overall Loss 0.319538    Objective Loss 0.319538                                        LR 0.000022    Time 0.021721    
2023-01-06 16:18:20,873 - Epoch: [96][   90/  246]    Overall Loss 0.320330    Objective Loss 0.320330                                        LR 0.000022    Time 0.021275    
2023-01-06 16:18:21,045 - Epoch: [96][  100/  246]    Overall Loss 0.324771    Objective Loss 0.324771                                        LR 0.000022    Time 0.020859    
2023-01-06 16:18:21,212 - Epoch: [96][  110/  246]    Overall Loss 0.325054    Objective Loss 0.325054                                        LR 0.000022    Time 0.020481    
2023-01-06 16:18:21,377 - Epoch: [96][  120/  246]    Overall Loss 0.326160    Objective Loss 0.326160                                        LR 0.000022    Time 0.020148    
2023-01-06 16:18:21,546 - Epoch: [96][  130/  246]    Overall Loss 0.325721    Objective Loss 0.325721                                        LR 0.000022    Time 0.019894    
2023-01-06 16:18:21,712 - Epoch: [96][  140/  246]    Overall Loss 0.325956    Objective Loss 0.325956                                        LR 0.000022    Time 0.019654    
2023-01-06 16:18:21,877 - Epoch: [96][  150/  246]    Overall Loss 0.325225    Objective Loss 0.325225                                        LR 0.000022    Time 0.019441    
2023-01-06 16:18:22,041 - Epoch: [96][  160/  246]    Overall Loss 0.324522    Objective Loss 0.324522                                        LR 0.000022    Time 0.019252    
2023-01-06 16:18:22,200 - Epoch: [96][  170/  246]    Overall Loss 0.325414    Objective Loss 0.325414                                        LR 0.000022    Time 0.019052    
2023-01-06 16:18:22,356 - Epoch: [96][  180/  246]    Overall Loss 0.324279    Objective Loss 0.324279                                        LR 0.000022    Time 0.018859    
2023-01-06 16:18:22,516 - Epoch: [96][  190/  246]    Overall Loss 0.324831    Objective Loss 0.324831                                        LR 0.000022    Time 0.018708    
2023-01-06 16:18:22,688 - Epoch: [96][  200/  246]    Overall Loss 0.324779    Objective Loss 0.324779                                        LR 0.000022    Time 0.018629    
2023-01-06 16:18:22,879 - Epoch: [96][  210/  246]    Overall Loss 0.324274    Objective Loss 0.324274                                        LR 0.000022    Time 0.018649    
2023-01-06 16:18:23,086 - Epoch: [96][  220/  246]    Overall Loss 0.324089    Objective Loss 0.324089                                        LR 0.000022    Time 0.018741    
2023-01-06 16:18:23,303 - Epoch: [96][  230/  246]    Overall Loss 0.324368    Objective Loss 0.324368                                        LR 0.000022    Time 0.018866    
2023-01-06 16:18:23,532 - Epoch: [96][  240/  246]    Overall Loss 0.324288    Objective Loss 0.324288                                        LR 0.000022    Time 0.019035    
2023-01-06 16:18:23,630 - Epoch: [96][  246/  246]    Overall Loss 0.324116    Objective Loss 0.324116    Top1 90.191388    LR 0.000022    Time 0.018965    
2023-01-06 16:18:23,766 - --- validate (epoch=96)-----------
2023-01-06 16:18:23,767 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:24,194 - Epoch: [96][   10/   28]    Loss 0.302744    Top1 89.375000    
2023-01-06 16:18:24,296 - Epoch: [96][   20/   28]    Loss 0.316656    Top1 88.613281    
2023-01-06 16:18:24,354 - Epoch: [96][   28/   28]    Loss 0.325087    Top1 88.247924    
2023-01-06 16:18:24,510 - ==> Top1: 88.248    Loss: 0.325

2023-01-06 16:18:24,510 - ==> Confusion:
[[ 181    6  252]
 [  11  176  415]
 [  61   76 5808]]

2023-01-06 16:18:24,511 - ==> Best [Top1: 88.248   Sparsity:0.00   Params: 46192 on epoch: 96]
2023-01-06 16:18:24,511 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:24,516 - 

2023-01-06 16:18:24,516 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:25,186 - Epoch: [97][   10/  246]    Overall Loss 0.319877    Objective Loss 0.319877                                        LR 0.000022    Time 0.066906    
2023-01-06 16:18:25,346 - Epoch: [97][   20/  246]    Overall Loss 0.328049    Objective Loss 0.328049                                        LR 0.000022    Time 0.041457    
2023-01-06 16:18:25,514 - Epoch: [97][   30/  246]    Overall Loss 0.329531    Objective Loss 0.329531                                        LR 0.000022    Time 0.033225    
2023-01-06 16:18:25,691 - Epoch: [97][   40/  246]    Overall Loss 0.329623    Objective Loss 0.329623                                        LR 0.000022    Time 0.029325    
2023-01-06 16:18:25,861 - Epoch: [97][   50/  246]    Overall Loss 0.326147    Objective Loss 0.326147                                        LR 0.000022    Time 0.026851    
2023-01-06 16:18:26,036 - Epoch: [97][   60/  246]    Overall Loss 0.324723    Objective Loss 0.324723                                        LR 0.000022    Time 0.025298    
2023-01-06 16:18:26,204 - Epoch: [97][   70/  246]    Overall Loss 0.327541    Objective Loss 0.327541                                        LR 0.000022    Time 0.024074    
2023-01-06 16:18:26,362 - Epoch: [97][   80/  246]    Overall Loss 0.324453    Objective Loss 0.324453                                        LR 0.000022    Time 0.023039    
2023-01-06 16:18:26,520 - Epoch: [97][   90/  246]    Overall Loss 0.327124    Objective Loss 0.327124                                        LR 0.000022    Time 0.022229    
2023-01-06 16:18:26,677 - Epoch: [97][  100/  246]    Overall Loss 0.325053    Objective Loss 0.325053                                        LR 0.000022    Time 0.021567    
2023-01-06 16:18:26,838 - Epoch: [97][  110/  246]    Overall Loss 0.325065    Objective Loss 0.325065                                        LR 0.000022    Time 0.021069    
2023-01-06 16:18:26,999 - Epoch: [97][  120/  246]    Overall Loss 0.323106    Objective Loss 0.323106                                        LR 0.000022    Time 0.020649    
2023-01-06 16:18:27,160 - Epoch: [97][  130/  246]    Overall Loss 0.323634    Objective Loss 0.323634                                        LR 0.000022    Time 0.020298    
2023-01-06 16:18:27,320 - Epoch: [97][  140/  246]    Overall Loss 0.324866    Objective Loss 0.324866                                        LR 0.000022    Time 0.019988    
2023-01-06 16:18:27,480 - Epoch: [97][  150/  246]    Overall Loss 0.323661    Objective Loss 0.323661                                        LR 0.000022    Time 0.019722    
2023-01-06 16:18:27,640 - Epoch: [97][  160/  246]    Overall Loss 0.324215    Objective Loss 0.324215                                        LR 0.000022    Time 0.019487    
2023-01-06 16:18:27,800 - Epoch: [97][  170/  246]    Overall Loss 0.324297    Objective Loss 0.324297                                        LR 0.000022    Time 0.019278    
2023-01-06 16:18:27,959 - Epoch: [97][  180/  246]    Overall Loss 0.323806    Objective Loss 0.323806                                        LR 0.000022    Time 0.019093    
2023-01-06 16:18:28,121 - Epoch: [97][  190/  246]    Overall Loss 0.323772    Objective Loss 0.323772                                        LR 0.000022    Time 0.018934    
2023-01-06 16:18:28,280 - Epoch: [97][  200/  246]    Overall Loss 0.324141    Objective Loss 0.324141                                        LR 0.000022    Time 0.018784    
2023-01-06 16:18:28,438 - Epoch: [97][  210/  246]    Overall Loss 0.324370    Objective Loss 0.324370                                        LR 0.000022    Time 0.018642    
2023-01-06 16:18:28,593 - Epoch: [97][  220/  246]    Overall Loss 0.323768    Objective Loss 0.323768                                        LR 0.000022    Time 0.018495    
2023-01-06 16:18:28,758 - Epoch: [97][  230/  246]    Overall Loss 0.323414    Objective Loss 0.323414                                        LR 0.000022    Time 0.018408    
2023-01-06 16:18:28,935 - Epoch: [97][  240/  246]    Overall Loss 0.323202    Objective Loss 0.323202                                        LR 0.000022    Time 0.018376    
2023-01-06 16:18:29,014 - Epoch: [97][  246/  246]    Overall Loss 0.323071    Objective Loss 0.323071    Top1 86.124402    LR 0.000022    Time 0.018247    
2023-01-06 16:18:29,167 - --- validate (epoch=97)-----------
2023-01-06 16:18:29,167 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:29,604 - Epoch: [97][   10/   28]    Loss 0.332634    Top1 87.695312    
2023-01-06 16:18:29,720 - Epoch: [97][   20/   28]    Loss 0.337090    Top1 87.011719    
2023-01-06 16:18:29,777 - Epoch: [97][   28/   28]    Loss 0.322003    Top1 87.918695    
2023-01-06 16:18:29,936 - ==> Top1: 87.919    Loss: 0.322

2023-01-06 16:18:29,936 - ==> Confusion:
[[ 171    4  264]
 [   9  137  456]
 [  63   48 5834]]

2023-01-06 16:18:29,938 - ==> Best [Top1: 88.248   Sparsity:0.00   Params: 46192 on epoch: 96]
2023-01-06 16:18:29,938 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:29,943 - 

2023-01-06 16:18:29,943 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:30,471 - Epoch: [98][   10/  246]    Overall Loss 0.309435    Objective Loss 0.309435                                        LR 0.000022    Time 0.052750    
2023-01-06 16:18:30,625 - Epoch: [98][   20/  246]    Overall Loss 0.321403    Objective Loss 0.321403                                        LR 0.000022    Time 0.034036    
2023-01-06 16:18:30,783 - Epoch: [98][   30/  246]    Overall Loss 0.324472    Objective Loss 0.324472                                        LR 0.000022    Time 0.027940    
2023-01-06 16:18:30,946 - Epoch: [98][   40/  246]    Overall Loss 0.322062    Objective Loss 0.322062                                        LR 0.000022    Time 0.025034    
2023-01-06 16:18:31,101 - Epoch: [98][   50/  246]    Overall Loss 0.322081    Objective Loss 0.322081                                        LR 0.000022    Time 0.023110    
2023-01-06 16:18:31,257 - Epoch: [98][   60/  246]    Overall Loss 0.323545    Objective Loss 0.323545                                        LR 0.000022    Time 0.021865    
2023-01-06 16:18:31,414 - Epoch: [98][   70/  246]    Overall Loss 0.323367    Objective Loss 0.323367                                        LR 0.000022    Time 0.020967    
2023-01-06 16:18:31,566 - Epoch: [98][   80/  246]    Overall Loss 0.322870    Objective Loss 0.322870                                        LR 0.000022    Time 0.020247    
2023-01-06 16:18:31,723 - Epoch: [98][   90/  246]    Overall Loss 0.319133    Objective Loss 0.319133                                        LR 0.000022    Time 0.019738    
2023-01-06 16:18:31,861 - Epoch: [98][  100/  246]    Overall Loss 0.320145    Objective Loss 0.320145                                        LR 0.000022    Time 0.019139    
2023-01-06 16:18:32,000 - Epoch: [98][  110/  246]    Overall Loss 0.321125    Objective Loss 0.321125                                        LR 0.000022    Time 0.018659    
2023-01-06 16:18:32,136 - Epoch: [98][  120/  246]    Overall Loss 0.321878    Objective Loss 0.321878                                        LR 0.000022    Time 0.018225    
2023-01-06 16:18:32,276 - Epoch: [98][  130/  246]    Overall Loss 0.322678    Objective Loss 0.322678                                        LR 0.000022    Time 0.017883    
2023-01-06 16:18:32,416 - Epoch: [98][  140/  246]    Overall Loss 0.321787    Objective Loss 0.321787                                        LR 0.000022    Time 0.017602    
2023-01-06 16:18:32,558 - Epoch: [98][  150/  246]    Overall Loss 0.322740    Objective Loss 0.322740                                        LR 0.000022    Time 0.017371    
2023-01-06 16:18:32,696 - Epoch: [98][  160/  246]    Overall Loss 0.322277    Objective Loss 0.322277                                        LR 0.000022    Time 0.017148    
2023-01-06 16:18:32,837 - Epoch: [98][  170/  246]    Overall Loss 0.323700    Objective Loss 0.323700                                        LR 0.000022    Time 0.016968    
2023-01-06 16:18:32,976 - Epoch: [98][  180/  246]    Overall Loss 0.323616    Objective Loss 0.323616                                        LR 0.000022    Time 0.016791    
2023-01-06 16:18:33,117 - Epoch: [98][  190/  246]    Overall Loss 0.324095    Objective Loss 0.324095                                        LR 0.000022    Time 0.016651    
2023-01-06 16:18:33,263 - Epoch: [98][  200/  246]    Overall Loss 0.324448    Objective Loss 0.324448                                        LR 0.000022    Time 0.016546    
2023-01-06 16:18:33,433 - Epoch: [98][  210/  246]    Overall Loss 0.323853    Objective Loss 0.323853                                        LR 0.000022    Time 0.016567    
2023-01-06 16:18:33,596 - Epoch: [98][  220/  246]    Overall Loss 0.323355    Objective Loss 0.323355                                        LR 0.000022    Time 0.016550    
2023-01-06 16:18:33,769 - Epoch: [98][  230/  246]    Overall Loss 0.322419    Objective Loss 0.322419                                        LR 0.000022    Time 0.016582    
2023-01-06 16:18:33,950 - Epoch: [98][  240/  246]    Overall Loss 0.323281    Objective Loss 0.323281                                        LR 0.000022    Time 0.016642    
2023-01-06 16:18:34,027 - Epoch: [98][  246/  246]    Overall Loss 0.322992    Objective Loss 0.322992    Top1 88.516746    LR 0.000022    Time 0.016551    
2023-01-06 16:18:34,163 - --- validate (epoch=98)-----------
2023-01-06 16:18:34,164 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:34,609 - Epoch: [98][   10/   28]    Loss 0.340245    Top1 87.929688    
2023-01-06 16:18:34,711 - Epoch: [98][   20/   28]    Loss 0.320026    Top1 88.535156    
2023-01-06 16:18:34,770 - Epoch: [98][   28/   28]    Loss 0.321019    Top1 88.405382    
2023-01-06 16:18:34,925 - ==> Top1: 88.405    Loss: 0.321

2023-01-06 16:18:34,926 - ==> Confusion:
[[ 185    6  248]
 [   9  176  417]
 [  67   63 5815]]

2023-01-06 16:18:34,927 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:18:34,927 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:34,932 - 

2023-01-06 16:18:34,932 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:35,628 - Epoch: [99][   10/  246]    Overall Loss 0.371924    Objective Loss 0.371924                                        LR 0.000022    Time 0.069477    
2023-01-06 16:18:35,790 - Epoch: [99][   20/  246]    Overall Loss 0.346257    Objective Loss 0.346257                                        LR 0.000022    Time 0.042811    
2023-01-06 16:18:35,955 - Epoch: [99][   30/  246]    Overall Loss 0.344531    Objective Loss 0.344531                                        LR 0.000022    Time 0.034043    
2023-01-06 16:18:36,115 - Epoch: [99][   40/  246]    Overall Loss 0.341860    Objective Loss 0.341860                                        LR 0.000022    Time 0.029528    
2023-01-06 16:18:36,280 - Epoch: [99][   50/  246]    Overall Loss 0.336710    Objective Loss 0.336710                                        LR 0.000022    Time 0.026899    
2023-01-06 16:18:36,438 - Epoch: [99][   60/  246]    Overall Loss 0.336512    Objective Loss 0.336512                                        LR 0.000022    Time 0.025046    
2023-01-06 16:18:36,604 - Epoch: [99][   70/  246]    Overall Loss 0.332683    Objective Loss 0.332683                                        LR 0.000022    Time 0.023841    
2023-01-06 16:18:36,771 - Epoch: [99][   80/  246]    Overall Loss 0.328744    Objective Loss 0.328744                                        LR 0.000022    Time 0.022942    
2023-01-06 16:18:36,941 - Epoch: [99][   90/  246]    Overall Loss 0.329837    Objective Loss 0.329837                                        LR 0.000022    Time 0.022277    
2023-01-06 16:18:37,102 - Epoch: [99][  100/  246]    Overall Loss 0.329423    Objective Loss 0.329423                                        LR 0.000022    Time 0.021660    
2023-01-06 16:18:37,267 - Epoch: [99][  110/  246]    Overall Loss 0.327303    Objective Loss 0.327303                                        LR 0.000022    Time 0.021186    
2023-01-06 16:18:37,432 - Epoch: [99][  120/  246]    Overall Loss 0.326125    Objective Loss 0.326125                                        LR 0.000022    Time 0.020787    
2023-01-06 16:18:37,598 - Epoch: [99][  130/  246]    Overall Loss 0.325512    Objective Loss 0.325512                                        LR 0.000022    Time 0.020464    
2023-01-06 16:18:37,770 - Epoch: [99][  140/  246]    Overall Loss 0.324561    Objective Loss 0.324561                                        LR 0.000022    Time 0.020227    
2023-01-06 16:18:37,940 - Epoch: [99][  150/  246]    Overall Loss 0.324551    Objective Loss 0.324551                                        LR 0.000022    Time 0.020009    
2023-01-06 16:18:38,111 - Epoch: [99][  160/  246]    Overall Loss 0.322991    Objective Loss 0.322991                                        LR 0.000022    Time 0.019826    
2023-01-06 16:18:38,279 - Epoch: [99][  170/  246]    Overall Loss 0.323134    Objective Loss 0.323134                                        LR 0.000022    Time 0.019645    
2023-01-06 16:18:38,424 - Epoch: [99][  180/  246]    Overall Loss 0.323596    Objective Loss 0.323596                                        LR 0.000022    Time 0.019359    
2023-01-06 16:18:38,575 - Epoch: [99][  190/  246]    Overall Loss 0.322757    Objective Loss 0.322757                                        LR 0.000022    Time 0.019132    
2023-01-06 16:18:38,752 - Epoch: [99][  200/  246]    Overall Loss 0.322811    Objective Loss 0.322811                                        LR 0.000022    Time 0.019058    
2023-01-06 16:18:38,933 - Epoch: [99][  210/  246]    Overall Loss 0.321937    Objective Loss 0.321937                                        LR 0.000022    Time 0.019013    
2023-01-06 16:18:39,111 - Epoch: [99][  220/  246]    Overall Loss 0.321465    Objective Loss 0.321465                                        LR 0.000022    Time 0.018955    
2023-01-06 16:18:39,318 - Epoch: [99][  230/  246]    Overall Loss 0.322678    Objective Loss 0.322678                                        LR 0.000022    Time 0.019031    
2023-01-06 16:18:39,518 - Epoch: [99][  240/  246]    Overall Loss 0.323143    Objective Loss 0.323143                                        LR 0.000022    Time 0.019069    
2023-01-06 16:18:39,596 - Epoch: [99][  246/  246]    Overall Loss 0.322279    Objective Loss 0.322279    Top1 90.430622    LR 0.000022    Time 0.018918    
2023-01-06 16:18:39,746 - --- validate (epoch=99)-----------
2023-01-06 16:18:39,746 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:40,185 - Epoch: [99][   10/   28]    Loss 0.299776    Top1 89.296875    
2023-01-06 16:18:40,290 - Epoch: [99][   20/   28]    Loss 0.314908    Top1 88.515625    
2023-01-06 16:18:40,349 - Epoch: [99][   28/   28]    Loss 0.324606    Top1 88.305182    
2023-01-06 16:18:40,494 - ==> Top1: 88.305    Loss: 0.325

2023-01-06 16:18:40,494 - ==> Confusion:
[[ 172    7  260]
 [   8  175  419]
 [  56   67 5822]]

2023-01-06 16:18:40,496 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:18:40,496 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:40,500 - 

2023-01-06 16:18:40,500 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:41,039 - Epoch: [100][   10/  246]    Overall Loss 0.331371    Objective Loss 0.331371                                        LR 0.000013    Time 0.053800    
2023-01-06 16:18:41,204 - Epoch: [100][   20/  246]    Overall Loss 0.330173    Objective Loss 0.330173                                        LR 0.000013    Time 0.035151    
2023-01-06 16:18:41,373 - Epoch: [100][   30/  246]    Overall Loss 0.325306    Objective Loss 0.325306                                        LR 0.000013    Time 0.029050    
2023-01-06 16:18:41,555 - Epoch: [100][   40/  246]    Overall Loss 0.320789    Objective Loss 0.320789                                        LR 0.000013    Time 0.026279    
2023-01-06 16:18:41,728 - Epoch: [100][   50/  246]    Overall Loss 0.319474    Objective Loss 0.319474                                        LR 0.000013    Time 0.024482    
2023-01-06 16:18:41,885 - Epoch: [100][   60/  246]    Overall Loss 0.316909    Objective Loss 0.316909                                        LR 0.000013    Time 0.022997    
2023-01-06 16:18:42,036 - Epoch: [100][   70/  246]    Overall Loss 0.317900    Objective Loss 0.317900                                        LR 0.000013    Time 0.021874    
2023-01-06 16:18:42,182 - Epoch: [100][   80/  246]    Overall Loss 0.319280    Objective Loss 0.319280                                        LR 0.000013    Time 0.020957    
2023-01-06 16:18:42,347 - Epoch: [100][   90/  246]    Overall Loss 0.318781    Objective Loss 0.318781                                        LR 0.000013    Time 0.020458    
2023-01-06 16:18:42,513 - Epoch: [100][  100/  246]    Overall Loss 0.318702    Objective Loss 0.318702                                        LR 0.000013    Time 0.020065    
2023-01-06 16:18:42,654 - Epoch: [100][  110/  246]    Overall Loss 0.321289    Objective Loss 0.321289                                        LR 0.000013    Time 0.019522    
2023-01-06 16:18:42,803 - Epoch: [100][  120/  246]    Overall Loss 0.321220    Objective Loss 0.321220                                        LR 0.000013    Time 0.019136    
2023-01-06 16:18:42,945 - Epoch: [100][  130/  246]    Overall Loss 0.319867    Objective Loss 0.319867                                        LR 0.000013    Time 0.018752    
2023-01-06 16:18:43,089 - Epoch: [100][  140/  246]    Overall Loss 0.320169    Objective Loss 0.320169                                        LR 0.000013    Time 0.018436    
2023-01-06 16:18:43,235 - Epoch: [100][  150/  246]    Overall Loss 0.320031    Objective Loss 0.320031                                        LR 0.000013    Time 0.018178    
2023-01-06 16:18:43,375 - Epoch: [100][  160/  246]    Overall Loss 0.320198    Objective Loss 0.320198                                        LR 0.000013    Time 0.017913    
2023-01-06 16:18:43,537 - Epoch: [100][  170/  246]    Overall Loss 0.319773    Objective Loss 0.319773                                        LR 0.000013    Time 0.017809    
2023-01-06 16:18:43,691 - Epoch: [100][  180/  246]    Overall Loss 0.320118    Objective Loss 0.320118                                        LR 0.000013    Time 0.017674    
2023-01-06 16:18:43,841 - Epoch: [100][  190/  246]    Overall Loss 0.320706    Objective Loss 0.320706                                        LR 0.000013    Time 0.017532    
2023-01-06 16:18:44,003 - Epoch: [100][  200/  246]    Overall Loss 0.320361    Objective Loss 0.320361                                        LR 0.000013    Time 0.017464    
2023-01-06 16:18:44,153 - Epoch: [100][  210/  246]    Overall Loss 0.320176    Objective Loss 0.320176                                        LR 0.000013    Time 0.017347    
2023-01-06 16:18:44,318 - Epoch: [100][  220/  246]    Overall Loss 0.320113    Objective Loss 0.320113                                        LR 0.000013    Time 0.017303    
2023-01-06 16:18:44,470 - Epoch: [100][  230/  246]    Overall Loss 0.320157    Objective Loss 0.320157                                        LR 0.000013    Time 0.017211    
2023-01-06 16:18:44,646 - Epoch: [100][  240/  246]    Overall Loss 0.320783    Objective Loss 0.320783                                        LR 0.000013    Time 0.017229    
2023-01-06 16:18:44,717 - Epoch: [100][  246/  246]    Overall Loss 0.320397    Objective Loss 0.320397    Top1 88.995215    LR 0.000013    Time 0.017095    
2023-01-06 16:18:44,872 - --- validate (epoch=100)-----------
2023-01-06 16:18:44,872 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:45,302 - Epoch: [100][   10/   28]    Loss 0.307286    Top1 89.140625    
2023-01-06 16:18:45,405 - Epoch: [100][   20/   28]    Loss 0.325382    Top1 88.203125    
2023-01-06 16:18:45,465 - Epoch: [100][   28/   28]    Loss 0.323253    Top1 88.319496    
2023-01-06 16:18:45,617 - ==> Top1: 88.319    Loss: 0.323

2023-01-06 16:18:45,617 - ==> Confusion:
[[ 161    8  270]
 [   8  189  405]
 [  51   74 5820]]

2023-01-06 16:18:45,618 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:18:45,618 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:45,623 - 

2023-01-06 16:18:45,623 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:46,285 - Epoch: [101][   10/  246]    Overall Loss 0.320012    Objective Loss 0.320012                                        LR 0.000013    Time 0.066201    
2023-01-06 16:18:46,437 - Epoch: [101][   20/  246]    Overall Loss 0.317450    Objective Loss 0.317450                                        LR 0.000013    Time 0.040682    
2023-01-06 16:18:46,595 - Epoch: [101][   30/  246]    Overall Loss 0.314023    Objective Loss 0.314023                                        LR 0.000013    Time 0.032348    
2023-01-06 16:18:46,756 - Epoch: [101][   40/  246]    Overall Loss 0.317379    Objective Loss 0.317379                                        LR 0.000013    Time 0.028291    
2023-01-06 16:18:46,923 - Epoch: [101][   50/  246]    Overall Loss 0.317949    Objective Loss 0.317949                                        LR 0.000013    Time 0.025955    
2023-01-06 16:18:47,082 - Epoch: [101][   60/  246]    Overall Loss 0.318188    Objective Loss 0.318188                                        LR 0.000013    Time 0.024279    
2023-01-06 16:18:47,248 - Epoch: [101][   70/  246]    Overall Loss 0.320996    Objective Loss 0.320996                                        LR 0.000013    Time 0.023180    
2023-01-06 16:18:47,407 - Epoch: [101][   80/  246]    Overall Loss 0.318624    Objective Loss 0.318624                                        LR 0.000013    Time 0.022268    
2023-01-06 16:18:47,573 - Epoch: [101][   90/  246]    Overall Loss 0.320805    Objective Loss 0.320805                                        LR 0.000013    Time 0.021629    
2023-01-06 16:18:47,740 - Epoch: [101][  100/  246]    Overall Loss 0.322293    Objective Loss 0.322293                                        LR 0.000013    Time 0.021133    
2023-01-06 16:18:47,926 - Epoch: [101][  110/  246]    Overall Loss 0.321138    Objective Loss 0.321138                                        LR 0.000013    Time 0.020903    
2023-01-06 16:18:48,128 - Epoch: [101][  120/  246]    Overall Loss 0.321336    Objective Loss 0.321336                                        LR 0.000013    Time 0.020838    
2023-01-06 16:18:48,332 - Epoch: [101][  130/  246]    Overall Loss 0.320517    Objective Loss 0.320517                                        LR 0.000013    Time 0.020801    
2023-01-06 16:18:48,532 - Epoch: [101][  140/  246]    Overall Loss 0.319428    Objective Loss 0.319428                                        LR 0.000013    Time 0.020742    
2023-01-06 16:18:48,734 - Epoch: [101][  150/  246]    Overall Loss 0.319012    Objective Loss 0.319012                                        LR 0.000013    Time 0.020706    
2023-01-06 16:18:48,939 - Epoch: [101][  160/  246]    Overall Loss 0.319426    Objective Loss 0.319426                                        LR 0.000013    Time 0.020686    
2023-01-06 16:18:49,159 - Epoch: [101][  170/  246]    Overall Loss 0.320535    Objective Loss 0.320535                                        LR 0.000013    Time 0.020766    
2023-01-06 16:18:49,356 - Epoch: [101][  180/  246]    Overall Loss 0.320101    Objective Loss 0.320101                                        LR 0.000013    Time 0.020703    
2023-01-06 16:18:49,556 - Epoch: [101][  190/  246]    Overall Loss 0.321404    Objective Loss 0.321404                                        LR 0.000013    Time 0.020663    
2023-01-06 16:18:49,754 - Epoch: [101][  200/  246]    Overall Loss 0.321239    Objective Loss 0.321239                                        LR 0.000013    Time 0.020617    
2023-01-06 16:18:49,955 - Epoch: [101][  210/  246]    Overall Loss 0.322465    Objective Loss 0.322465                                        LR 0.000013    Time 0.020593    
2023-01-06 16:18:50,152 - Epoch: [101][  220/  246]    Overall Loss 0.322361    Objective Loss 0.322361                                        LR 0.000013    Time 0.020548    
2023-01-06 16:18:50,348 - Epoch: [101][  230/  246]    Overall Loss 0.322247    Objective Loss 0.322247                                        LR 0.000013    Time 0.020506    
2023-01-06 16:18:50,565 - Epoch: [101][  240/  246]    Overall Loss 0.320996    Objective Loss 0.320996                                        LR 0.000013    Time 0.020555    
2023-01-06 16:18:50,658 - Epoch: [101][  246/  246]    Overall Loss 0.320799    Objective Loss 0.320799    Top1 89.952153    LR 0.000013    Time 0.020430    
2023-01-06 16:18:50,793 - --- validate (epoch=101)-----------
2023-01-06 16:18:50,793 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:51,245 - Epoch: [101][   10/   28]    Loss 0.327727    Top1 87.500000    
2023-01-06 16:18:51,350 - Epoch: [101][   20/   28]    Loss 0.327243    Top1 87.929688    
2023-01-06 16:18:51,410 - Epoch: [101][   28/   28]    Loss 0.318881    Top1 88.290867    
2023-01-06 16:18:51,547 - ==> Top1: 88.291    Loss: 0.319

2023-01-06 16:18:51,548 - ==> Confusion:
[[ 176    6  257]
 [  10  175  417]
 [  65   63 5817]]

2023-01-06 16:18:51,549 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:18:51,549 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:51,553 - 

2023-01-06 16:18:51,554 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:52,257 - Epoch: [102][   10/  246]    Overall Loss 0.297743    Objective Loss 0.297743                                        LR 0.000013    Time 0.070274    
2023-01-06 16:18:52,425 - Epoch: [102][   20/  246]    Overall Loss 0.304834    Objective Loss 0.304834                                        LR 0.000013    Time 0.043504    
2023-01-06 16:18:52,614 - Epoch: [102][   30/  246]    Overall Loss 0.303202    Objective Loss 0.303202                                        LR 0.000013    Time 0.035308    
2023-01-06 16:18:52,803 - Epoch: [102][   40/  246]    Overall Loss 0.307689    Objective Loss 0.307689                                        LR 0.000013    Time 0.031177    
2023-01-06 16:18:52,994 - Epoch: [102][   50/  246]    Overall Loss 0.308001    Objective Loss 0.308001                                        LR 0.000013    Time 0.028755    
2023-01-06 16:18:53,183 - Epoch: [102][   60/  246]    Overall Loss 0.311551    Objective Loss 0.311551                                        LR 0.000013    Time 0.027107    
2023-01-06 16:18:53,368 - Epoch: [102][   70/  246]    Overall Loss 0.317387    Objective Loss 0.317387                                        LR 0.000013    Time 0.025876    
2023-01-06 16:18:53,551 - Epoch: [102][   80/  246]    Overall Loss 0.318196    Objective Loss 0.318196                                        LR 0.000013    Time 0.024926    
2023-01-06 16:18:53,736 - Epoch: [102][   90/  246]    Overall Loss 0.318164    Objective Loss 0.318164                                        LR 0.000013    Time 0.024203    
2023-01-06 16:18:53,930 - Epoch: [102][  100/  246]    Overall Loss 0.319904    Objective Loss 0.319904                                        LR 0.000013    Time 0.023719    
2023-01-06 16:18:54,129 - Epoch: [102][  110/  246]    Overall Loss 0.320343    Objective Loss 0.320343                                        LR 0.000013    Time 0.023371    
2023-01-06 16:18:54,333 - Epoch: [102][  120/  246]    Overall Loss 0.320050    Objective Loss 0.320050                                        LR 0.000013    Time 0.023123    
2023-01-06 16:18:54,535 - Epoch: [102][  130/  246]    Overall Loss 0.320459    Objective Loss 0.320459                                        LR 0.000013    Time 0.022894    
2023-01-06 16:18:54,738 - Epoch: [102][  140/  246]    Overall Loss 0.318004    Objective Loss 0.318004                                        LR 0.000013    Time 0.022706    
2023-01-06 16:18:54,940 - Epoch: [102][  150/  246]    Overall Loss 0.317024    Objective Loss 0.317024                                        LR 0.000013    Time 0.022534    
2023-01-06 16:18:55,143 - Epoch: [102][  160/  246]    Overall Loss 0.316511    Objective Loss 0.316511                                        LR 0.000013    Time 0.022390    
2023-01-06 16:18:55,344 - Epoch: [102][  170/  246]    Overall Loss 0.317336    Objective Loss 0.317336                                        LR 0.000013    Time 0.022257    
2023-01-06 16:18:55,547 - Epoch: [102][  180/  246]    Overall Loss 0.318036    Objective Loss 0.318036                                        LR 0.000013    Time 0.022142    
2023-01-06 16:18:55,749 - Epoch: [102][  190/  246]    Overall Loss 0.319002    Objective Loss 0.319002                                        LR 0.000013    Time 0.022037    
2023-01-06 16:18:55,950 - Epoch: [102][  200/  246]    Overall Loss 0.319538    Objective Loss 0.319538                                        LR 0.000013    Time 0.021941    
2023-01-06 16:18:56,146 - Epoch: [102][  210/  246]    Overall Loss 0.320198    Objective Loss 0.320198                                        LR 0.000013    Time 0.021825    
2023-01-06 16:18:56,334 - Epoch: [102][  220/  246]    Overall Loss 0.320184    Objective Loss 0.320184                                        LR 0.000013    Time 0.021687    
2023-01-06 16:18:56,525 - Epoch: [102][  230/  246]    Overall Loss 0.320323    Objective Loss 0.320323                                        LR 0.000013    Time 0.021573    
2023-01-06 16:18:56,699 - Epoch: [102][  240/  246]    Overall Loss 0.320130    Objective Loss 0.320130                                        LR 0.000013    Time 0.021398    
2023-01-06 16:18:56,780 - Epoch: [102][  246/  246]    Overall Loss 0.319966    Objective Loss 0.319966    Top1 86.602871    LR 0.000013    Time 0.021204    
2023-01-06 16:18:56,915 - --- validate (epoch=102)-----------
2023-01-06 16:18:56,915 - 6986 samples (256 per mini-batch)
2023-01-06 16:18:57,363 - Epoch: [102][   10/   28]    Loss 0.301845    Top1 89.492188    
2023-01-06 16:18:57,470 - Epoch: [102][   20/   28]    Loss 0.319778    Top1 88.242188    
2023-01-06 16:18:57,529 - Epoch: [102][   28/   28]    Loss 0.321023    Top1 88.233610    
2023-01-06 16:18:57,691 - ==> Top1: 88.234    Loss: 0.321

2023-01-06 16:18:57,691 - ==> Confusion:
[[ 179    5  255]
 [   9  166  427]
 [  65   61 5819]]

2023-01-06 16:18:57,693 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:18:57,693 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:18:57,697 - 

2023-01-06 16:18:57,697 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:18:58,233 - Epoch: [103][   10/  246]    Overall Loss 0.300970    Objective Loss 0.300970                                        LR 0.000013    Time 0.053528    
2023-01-06 16:18:58,393 - Epoch: [103][   20/  246]    Overall Loss 0.307100    Objective Loss 0.307100                                        LR 0.000013    Time 0.034699    
2023-01-06 16:18:58,553 - Epoch: [103][   30/  246]    Overall Loss 0.312519    Objective Loss 0.312519                                        LR 0.000013    Time 0.028452    
2023-01-06 16:18:58,724 - Epoch: [103][   40/  246]    Overall Loss 0.314777    Objective Loss 0.314777                                        LR 0.000013    Time 0.025606    
2023-01-06 16:18:58,884 - Epoch: [103][   50/  246]    Overall Loss 0.313052    Objective Loss 0.313052                                        LR 0.000013    Time 0.023643    
2023-01-06 16:18:59,043 - Epoch: [103][   60/  246]    Overall Loss 0.312335    Objective Loss 0.312335                                        LR 0.000013    Time 0.022348    
2023-01-06 16:18:59,205 - Epoch: [103][   70/  246]    Overall Loss 0.314969    Objective Loss 0.314969                                        LR 0.000013    Time 0.021472    
2023-01-06 16:18:59,353 - Epoch: [103][   80/  246]    Overall Loss 0.317582    Objective Loss 0.317582                                        LR 0.000013    Time 0.020623    
2023-01-06 16:18:59,491 - Epoch: [103][   90/  246]    Overall Loss 0.316441    Objective Loss 0.316441                                        LR 0.000013    Time 0.019867    
2023-01-06 16:18:59,643 - Epoch: [103][  100/  246]    Overall Loss 0.316015    Objective Loss 0.316015                                        LR 0.000013    Time 0.019399    
2023-01-06 16:18:59,802 - Epoch: [103][  110/  246]    Overall Loss 0.318099    Objective Loss 0.318099                                        LR 0.000013    Time 0.019081    
2023-01-06 16:18:59,962 - Epoch: [103][  120/  246]    Overall Loss 0.318958    Objective Loss 0.318958                                        LR 0.000013    Time 0.018816    
2023-01-06 16:19:00,120 - Epoch: [103][  130/  246]    Overall Loss 0.319403    Objective Loss 0.319403                                        LR 0.000013    Time 0.018584    
2023-01-06 16:19:00,277 - Epoch: [103][  140/  246]    Overall Loss 0.319462    Objective Loss 0.319462                                        LR 0.000013    Time 0.018378    
2023-01-06 16:19:00,434 - Epoch: [103][  150/  246]    Overall Loss 0.318124    Objective Loss 0.318124                                        LR 0.000013    Time 0.018196    
2023-01-06 16:19:00,590 - Epoch: [103][  160/  246]    Overall Loss 0.319145    Objective Loss 0.319145                                        LR 0.000013    Time 0.018028    
2023-01-06 16:19:00,747 - Epoch: [103][  170/  246]    Overall Loss 0.318849    Objective Loss 0.318849                                        LR 0.000013    Time 0.017894    
2023-01-06 16:19:00,902 - Epoch: [103][  180/  246]    Overall Loss 0.319154    Objective Loss 0.319154                                        LR 0.000013    Time 0.017757    
2023-01-06 16:19:01,062 - Epoch: [103][  190/  246]    Overall Loss 0.318204    Objective Loss 0.318204                                        LR 0.000013    Time 0.017665    
2023-01-06 16:19:01,219 - Epoch: [103][  200/  246]    Overall Loss 0.317713    Objective Loss 0.317713                                        LR 0.000013    Time 0.017562    
2023-01-06 16:19:01,375 - Epoch: [103][  210/  246]    Overall Loss 0.317864    Objective Loss 0.317864                                        LR 0.000013    Time 0.017466    
2023-01-06 16:19:01,531 - Epoch: [103][  220/  246]    Overall Loss 0.318382    Objective Loss 0.318382                                        LR 0.000013    Time 0.017380    
2023-01-06 16:19:01,685 - Epoch: [103][  230/  246]    Overall Loss 0.319233    Objective Loss 0.319233                                        LR 0.000013    Time 0.017297    
2023-01-06 16:19:01,855 - Epoch: [103][  240/  246]    Overall Loss 0.319225    Objective Loss 0.319225                                        LR 0.000013    Time 0.017280    
2023-01-06 16:19:01,935 - Epoch: [103][  246/  246]    Overall Loss 0.319627    Objective Loss 0.319627    Top1 84.688995    LR 0.000013    Time 0.017183    
2023-01-06 16:19:02,069 - --- validate (epoch=103)-----------
2023-01-06 16:19:02,069 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:02,509 - Epoch: [103][   10/   28]    Loss 0.330721    Top1 88.007812    
2023-01-06 16:19:02,616 - Epoch: [103][   20/   28]    Loss 0.320887    Top1 88.417969    
2023-01-06 16:19:02,672 - Epoch: [103][   28/   28]    Loss 0.319137    Top1 88.348125    
2023-01-06 16:19:02,824 - ==> Top1: 88.348    Loss: 0.319

2023-01-06 16:19:02,824 - ==> Confusion:
[[ 177    6  256]
 [   9  167  426]
 [  55   62 5828]]

2023-01-06 16:19:02,825 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:02,826 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:02,830 - 

2023-01-06 16:19:02,830 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:03,523 - Epoch: [104][   10/  246]    Overall Loss 0.320534    Objective Loss 0.320534                                        LR 0.000013    Time 0.069239    
2023-01-06 16:19:03,690 - Epoch: [104][   20/  246]    Overall Loss 0.323569    Objective Loss 0.323569                                        LR 0.000013    Time 0.042928    
2023-01-06 16:19:03,852 - Epoch: [104][   30/  246]    Overall Loss 0.325250    Objective Loss 0.325250                                        LR 0.000013    Time 0.033992    
2023-01-06 16:19:04,019 - Epoch: [104][   40/  246]    Overall Loss 0.321050    Objective Loss 0.321050                                        LR 0.000013    Time 0.029658    
2023-01-06 16:19:04,184 - Epoch: [104][   50/  246]    Overall Loss 0.318343    Objective Loss 0.318343                                        LR 0.000013    Time 0.027023    
2023-01-06 16:19:04,343 - Epoch: [104][   60/  246]    Overall Loss 0.316928    Objective Loss 0.316928                                        LR 0.000013    Time 0.025162    
2023-01-06 16:19:04,504 - Epoch: [104][   70/  246]    Overall Loss 0.319879    Objective Loss 0.319879                                        LR 0.000013    Time 0.023864    
2023-01-06 16:19:04,668 - Epoch: [104][   80/  246]    Overall Loss 0.320332    Objective Loss 0.320332                                        LR 0.000013    Time 0.022936    
2023-01-06 16:19:04,834 - Epoch: [104][   90/  246]    Overall Loss 0.321286    Objective Loss 0.321286                                        LR 0.000013    Time 0.022219    
2023-01-06 16:19:04,998 - Epoch: [104][  100/  246]    Overall Loss 0.320958    Objective Loss 0.320958                                        LR 0.000013    Time 0.021638    
2023-01-06 16:19:05,157 - Epoch: [104][  110/  246]    Overall Loss 0.322891    Objective Loss 0.322891                                        LR 0.000013    Time 0.021110    
2023-01-06 16:19:05,322 - Epoch: [104][  120/  246]    Overall Loss 0.322238    Objective Loss 0.322238                                        LR 0.000013    Time 0.020730    
2023-01-06 16:19:05,491 - Epoch: [104][  130/  246]    Overall Loss 0.322401    Objective Loss 0.322401                                        LR 0.000013    Time 0.020428    
2023-01-06 16:19:05,653 - Epoch: [104][  140/  246]    Overall Loss 0.323082    Objective Loss 0.323082                                        LR 0.000013    Time 0.020128    
2023-01-06 16:19:05,817 - Epoch: [104][  150/  246]    Overall Loss 0.322918    Objective Loss 0.322918                                        LR 0.000013    Time 0.019873    
2023-01-06 16:19:05,977 - Epoch: [104][  160/  246]    Overall Loss 0.322025    Objective Loss 0.322025                                        LR 0.000013    Time 0.019628    
2023-01-06 16:19:06,134 - Epoch: [104][  170/  246]    Overall Loss 0.322302    Objective Loss 0.322302                                        LR 0.000013    Time 0.019395    
2023-01-06 16:19:06,292 - Epoch: [104][  180/  246]    Overall Loss 0.322125    Objective Loss 0.322125                                        LR 0.000013    Time 0.019197    
2023-01-06 16:19:06,456 - Epoch: [104][  190/  246]    Overall Loss 0.321391    Objective Loss 0.321391                                        LR 0.000013    Time 0.019048    
2023-01-06 16:19:06,603 - Epoch: [104][  200/  246]    Overall Loss 0.321017    Objective Loss 0.321017                                        LR 0.000013    Time 0.018827    
2023-01-06 16:19:06,765 - Epoch: [104][  210/  246]    Overall Loss 0.319732    Objective Loss 0.319732                                        LR 0.000013    Time 0.018699    
2023-01-06 16:19:06,923 - Epoch: [104][  220/  246]    Overall Loss 0.319429    Objective Loss 0.319429                                        LR 0.000013    Time 0.018570    
2023-01-06 16:19:07,088 - Epoch: [104][  230/  246]    Overall Loss 0.319278    Objective Loss 0.319278                                        LR 0.000013    Time 0.018477    
2023-01-06 16:19:07,258 - Epoch: [104][  240/  246]    Overall Loss 0.318805    Objective Loss 0.318805                                        LR 0.000013    Time 0.018413    
2023-01-06 16:19:07,333 - Epoch: [104][  246/  246]    Overall Loss 0.318516    Objective Loss 0.318516    Top1 89.712919    LR 0.000013    Time 0.018267    
2023-01-06 16:19:07,458 - --- validate (epoch=104)-----------
2023-01-06 16:19:07,458 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:07,889 - Epoch: [104][   10/   28]    Loss 0.340794    Top1 87.929688    
2023-01-06 16:19:07,998 - Epoch: [104][   20/   28]    Loss 0.319534    Top1 88.535156    
2023-01-06 16:19:08,058 - Epoch: [104][   28/   28]    Loss 0.318840    Top1 88.276553    
2023-01-06 16:19:08,198 - ==> Top1: 88.277    Loss: 0.319

2023-01-06 16:19:08,198 - ==> Confusion:
[[ 177    7  255]
 [  10  183  409]
 [  66   72 5807]]

2023-01-06 16:19:08,199 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:08,199 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:08,204 - 

2023-01-06 16:19:08,204 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:08,892 - Epoch: [105][   10/  246]    Overall Loss 0.322733    Objective Loss 0.322733                                        LR 0.000013    Time 0.068772    
2023-01-06 16:19:09,053 - Epoch: [105][   20/  246]    Overall Loss 0.319336    Objective Loss 0.319336                                        LR 0.000013    Time 0.042398    
2023-01-06 16:19:09,204 - Epoch: [105][   30/  246]    Overall Loss 0.320367    Objective Loss 0.320367                                        LR 0.000013    Time 0.033286    
2023-01-06 16:19:09,359 - Epoch: [105][   40/  246]    Overall Loss 0.313444    Objective Loss 0.313444                                        LR 0.000013    Time 0.028835    
2023-01-06 16:19:09,517 - Epoch: [105][   50/  246]    Overall Loss 0.321296    Objective Loss 0.321296                                        LR 0.000013    Time 0.026220    
2023-01-06 16:19:09,682 - Epoch: [105][   60/  246]    Overall Loss 0.325040    Objective Loss 0.325040                                        LR 0.000013    Time 0.024603    
2023-01-06 16:19:09,840 - Epoch: [105][   70/  246]    Overall Loss 0.323910    Objective Loss 0.323910                                        LR 0.000013    Time 0.023340    
2023-01-06 16:19:10,008 - Epoch: [105][   80/  246]    Overall Loss 0.321199    Objective Loss 0.321199                                        LR 0.000013    Time 0.022509    
2023-01-06 16:19:10,174 - Epoch: [105][   90/  246]    Overall Loss 0.318443    Objective Loss 0.318443                                        LR 0.000013    Time 0.021852    
2023-01-06 16:19:10,342 - Epoch: [105][  100/  246]    Overall Loss 0.319598    Objective Loss 0.319598                                        LR 0.000013    Time 0.021340    
2023-01-06 16:19:10,507 - Epoch: [105][  110/  246]    Overall Loss 0.316801    Objective Loss 0.316801                                        LR 0.000013    Time 0.020900    
2023-01-06 16:19:10,679 - Epoch: [105][  120/  246]    Overall Loss 0.316655    Objective Loss 0.316655                                        LR 0.000013    Time 0.020588    
2023-01-06 16:19:10,840 - Epoch: [105][  130/  246]    Overall Loss 0.316852    Objective Loss 0.316852                                        LR 0.000013    Time 0.020243    
2023-01-06 16:19:11,011 - Epoch: [105][  140/  246]    Overall Loss 0.319144    Objective Loss 0.319144                                        LR 0.000013    Time 0.020009    
2023-01-06 16:19:11,187 - Epoch: [105][  150/  246]    Overall Loss 0.318660    Objective Loss 0.318660                                        LR 0.000013    Time 0.019847    
2023-01-06 16:19:11,362 - Epoch: [105][  160/  246]    Overall Loss 0.318699    Objective Loss 0.318699                                        LR 0.000013    Time 0.019697    
2023-01-06 16:19:11,542 - Epoch: [105][  170/  246]    Overall Loss 0.318563    Objective Loss 0.318563                                        LR 0.000013    Time 0.019595    
2023-01-06 16:19:11,709 - Epoch: [105][  180/  246]    Overall Loss 0.319099    Objective Loss 0.319099                                        LR 0.000013    Time 0.019432    
2023-01-06 16:19:11,876 - Epoch: [105][  190/  246]    Overall Loss 0.319348    Objective Loss 0.319348                                        LR 0.000013    Time 0.019287    
2023-01-06 16:19:12,040 - Epoch: [105][  200/  246]    Overall Loss 0.318467    Objective Loss 0.318467                                        LR 0.000013    Time 0.019142    
2023-01-06 16:19:12,206 - Epoch: [105][  210/  246]    Overall Loss 0.318596    Objective Loss 0.318596                                        LR 0.000013    Time 0.019021    
2023-01-06 16:19:12,370 - Epoch: [105][  220/  246]    Overall Loss 0.318532    Objective Loss 0.318532                                        LR 0.000013    Time 0.018897    
2023-01-06 16:19:12,530 - Epoch: [105][  230/  246]    Overall Loss 0.318099    Objective Loss 0.318099                                        LR 0.000013    Time 0.018771    
2023-01-06 16:19:12,692 - Epoch: [105][  240/  246]    Overall Loss 0.318229    Objective Loss 0.318229                                        LR 0.000013    Time 0.018664    
2023-01-06 16:19:12,766 - Epoch: [105][  246/  246]    Overall Loss 0.317968    Objective Loss 0.317968    Top1 88.277512    LR 0.000013    Time 0.018508    
2023-01-06 16:19:12,902 - --- validate (epoch=105)-----------
2023-01-06 16:19:12,902 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:13,344 - Epoch: [105][   10/   28]    Loss 0.319109    Top1 87.773438    
2023-01-06 16:19:13,449 - Epoch: [105][   20/   28]    Loss 0.322469    Top1 87.851562    
2023-01-06 16:19:13,507 - Epoch: [105][   28/   28]    Loss 0.315593    Top1 88.076152    
2023-01-06 16:19:13,649 - ==> Top1: 88.076    Loss: 0.316

2023-01-06 16:19:13,650 - ==> Confusion:
[[ 154    5  280]
 [   7  159  436]
 [  49   56 5840]]

2023-01-06 16:19:13,651 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:13,651 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:13,656 - 

2023-01-06 16:19:13,656 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:14,190 - Epoch: [106][   10/  246]    Overall Loss 0.297975    Objective Loss 0.297975                                        LR 0.000013    Time 0.053338    
2023-01-06 16:19:14,331 - Epoch: [106][   20/  246]    Overall Loss 0.314004    Objective Loss 0.314004                                        LR 0.000013    Time 0.033699    
2023-01-06 16:19:14,478 - Epoch: [106][   30/  246]    Overall Loss 0.317835    Objective Loss 0.317835                                        LR 0.000013    Time 0.027358    
2023-01-06 16:19:14,640 - Epoch: [106][   40/  246]    Overall Loss 0.320617    Objective Loss 0.320617                                        LR 0.000013    Time 0.024563    
2023-01-06 16:19:14,803 - Epoch: [106][   50/  246]    Overall Loss 0.318491    Objective Loss 0.318491                                        LR 0.000013    Time 0.022898    
2023-01-06 16:19:14,969 - Epoch: [106][   60/  246]    Overall Loss 0.316404    Objective Loss 0.316404                                        LR 0.000013    Time 0.021850    
2023-01-06 16:19:15,135 - Epoch: [106][   70/  246]    Overall Loss 0.314342    Objective Loss 0.314342                                        LR 0.000013    Time 0.021093    
2023-01-06 16:19:15,300 - Epoch: [106][   80/  246]    Overall Loss 0.315540    Objective Loss 0.315540                                        LR 0.000013    Time 0.020516    
2023-01-06 16:19:15,464 - Epoch: [106][   90/  246]    Overall Loss 0.315835    Objective Loss 0.315835                                        LR 0.000013    Time 0.020046    
2023-01-06 16:19:15,632 - Epoch: [106][  100/  246]    Overall Loss 0.315049    Objective Loss 0.315049                                        LR 0.000013    Time 0.019723    
2023-01-06 16:19:15,797 - Epoch: [106][  110/  246]    Overall Loss 0.314101    Objective Loss 0.314101                                        LR 0.000013    Time 0.019428    
2023-01-06 16:19:15,957 - Epoch: [106][  120/  246]    Overall Loss 0.313535    Objective Loss 0.313535                                        LR 0.000013    Time 0.019141    
2023-01-06 16:19:16,114 - Epoch: [106][  130/  246]    Overall Loss 0.315210    Objective Loss 0.315210                                        LR 0.000013    Time 0.018870    
2023-01-06 16:19:16,280 - Epoch: [106][  140/  246]    Overall Loss 0.315808    Objective Loss 0.315808                                        LR 0.000013    Time 0.018707    
2023-01-06 16:19:16,445 - Epoch: [106][  150/  246]    Overall Loss 0.316434    Objective Loss 0.316434                                        LR 0.000013    Time 0.018558    
2023-01-06 16:19:16,612 - Epoch: [106][  160/  246]    Overall Loss 0.316478    Objective Loss 0.316478                                        LR 0.000013    Time 0.018436    
2023-01-06 16:19:16,777 - Epoch: [106][  170/  246]    Overall Loss 0.317414    Objective Loss 0.317414                                        LR 0.000013    Time 0.018325    
2023-01-06 16:19:16,940 - Epoch: [106][  180/  246]    Overall Loss 0.318393    Objective Loss 0.318393                                        LR 0.000013    Time 0.018210    
2023-01-06 16:19:17,106 - Epoch: [106][  190/  246]    Overall Loss 0.318630    Objective Loss 0.318630                                        LR 0.000013    Time 0.018121    
2023-01-06 16:19:17,254 - Epoch: [106][  200/  246]    Overall Loss 0.319191    Objective Loss 0.319191                                        LR 0.000013    Time 0.017955    
2023-01-06 16:19:17,406 - Epoch: [106][  210/  246]    Overall Loss 0.319911    Objective Loss 0.319911                                        LR 0.000013    Time 0.017820    
2023-01-06 16:19:17,564 - Epoch: [106][  220/  246]    Overall Loss 0.319184    Objective Loss 0.319184                                        LR 0.000013    Time 0.017727    
2023-01-06 16:19:17,722 - Epoch: [106][  230/  246]    Overall Loss 0.319468    Objective Loss 0.319468                                        LR 0.000013    Time 0.017642    
2023-01-06 16:19:17,891 - Epoch: [106][  240/  246]    Overall Loss 0.318842    Objective Loss 0.318842                                        LR 0.000013    Time 0.017608    
2023-01-06 16:19:17,966 - Epoch: [106][  246/  246]    Overall Loss 0.318329    Objective Loss 0.318329    Top1 90.430622    LR 0.000013    Time 0.017483    
2023-01-06 16:19:18,100 - --- validate (epoch=106)-----------
2023-01-06 16:19:18,100 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:18,527 - Epoch: [106][   10/   28]    Loss 0.315695    Top1 88.007812    
2023-01-06 16:19:18,629 - Epoch: [106][   20/   28]    Loss 0.318526    Top1 88.066406    
2023-01-06 16:19:18,686 - Epoch: [106][   28/   28]    Loss 0.317679    Top1 88.104781    
2023-01-06 16:19:18,844 - ==> Top1: 88.105    Loss: 0.318

2023-01-06 16:19:18,844 - ==> Confusion:
[[ 158    5  276]
 [   7  164  431]
 [  51   61 5833]]

2023-01-06 16:19:18,845 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:18,845 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:18,850 - 

2023-01-06 16:19:18,850 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:19,524 - Epoch: [107][   10/  246]    Overall Loss 0.314793    Objective Loss 0.314793                                        LR 0.000013    Time 0.067379    
2023-01-06 16:19:19,668 - Epoch: [107][   20/  246]    Overall Loss 0.327421    Objective Loss 0.327421                                        LR 0.000013    Time 0.040854    
2023-01-06 16:19:19,807 - Epoch: [107][   30/  246]    Overall Loss 0.323414    Objective Loss 0.323414                                        LR 0.000013    Time 0.031851    
2023-01-06 16:19:19,953 - Epoch: [107][   40/  246]    Overall Loss 0.322766    Objective Loss 0.322766                                        LR 0.000013    Time 0.027537    
2023-01-06 16:19:20,092 - Epoch: [107][   50/  246]    Overall Loss 0.318943    Objective Loss 0.318943                                        LR 0.000013    Time 0.024800    
2023-01-06 16:19:20,230 - Epoch: [107][   60/  246]    Overall Loss 0.322102    Objective Loss 0.322102                                        LR 0.000013    Time 0.022965    
2023-01-06 16:19:20,367 - Epoch: [107][   70/  246]    Overall Loss 0.322271    Objective Loss 0.322271                                        LR 0.000013    Time 0.021637    
2023-01-06 16:19:20,503 - Epoch: [107][   80/  246]    Overall Loss 0.322539    Objective Loss 0.322539                                        LR 0.000013    Time 0.020630    
2023-01-06 16:19:20,639 - Epoch: [107][   90/  246]    Overall Loss 0.321002    Objective Loss 0.321002                                        LR 0.000013    Time 0.019838    
2023-01-06 16:19:20,774 - Epoch: [107][  100/  246]    Overall Loss 0.318864    Objective Loss 0.318864                                        LR 0.000013    Time 0.019203    
2023-01-06 16:19:20,909 - Epoch: [107][  110/  246]    Overall Loss 0.320433    Objective Loss 0.320433                                        LR 0.000013    Time 0.018682    
2023-01-06 16:19:21,049 - Epoch: [107][  120/  246]    Overall Loss 0.321749    Objective Loss 0.321749                                        LR 0.000013    Time 0.018283    
2023-01-06 16:19:21,191 - Epoch: [107][  130/  246]    Overall Loss 0.321423    Objective Loss 0.321423                                        LR 0.000013    Time 0.017969    
2023-01-06 16:19:21,332 - Epoch: [107][  140/  246]    Overall Loss 0.320786    Objective Loss 0.320786                                        LR 0.000013    Time 0.017687    
2023-01-06 16:19:21,474 - Epoch: [107][  150/  246]    Overall Loss 0.320473    Objective Loss 0.320473                                        LR 0.000013    Time 0.017457    
2023-01-06 16:19:21,616 - Epoch: [107][  160/  246]    Overall Loss 0.319185    Objective Loss 0.319185                                        LR 0.000013    Time 0.017250    
2023-01-06 16:19:21,760 - Epoch: [107][  170/  246]    Overall Loss 0.319158    Objective Loss 0.319158                                        LR 0.000013    Time 0.017078    
2023-01-06 16:19:21,901 - Epoch: [107][  180/  246]    Overall Loss 0.318335    Objective Loss 0.318335                                        LR 0.000013    Time 0.016910    
2023-01-06 16:19:22,039 - Epoch: [107][  190/  246]    Overall Loss 0.318815    Objective Loss 0.318815                                        LR 0.000013    Time 0.016744    
2023-01-06 16:19:22,196 - Epoch: [107][  200/  246]    Overall Loss 0.318497    Objective Loss 0.318497                                        LR 0.000013    Time 0.016694    
2023-01-06 16:19:22,353 - Epoch: [107][  210/  246]    Overall Loss 0.318679    Objective Loss 0.318679                                        LR 0.000013    Time 0.016644    
2023-01-06 16:19:22,512 - Epoch: [107][  220/  246]    Overall Loss 0.318816    Objective Loss 0.318816                                        LR 0.000013    Time 0.016609    
2023-01-06 16:19:22,669 - Epoch: [107][  230/  246]    Overall Loss 0.318475    Objective Loss 0.318475                                        LR 0.000013    Time 0.016569    
2023-01-06 16:19:22,843 - Epoch: [107][  240/  246]    Overall Loss 0.318585    Objective Loss 0.318585                                        LR 0.000013    Time 0.016601    
2023-01-06 16:19:22,920 - Epoch: [107][  246/  246]    Overall Loss 0.318354    Objective Loss 0.318354    Top1 88.995215    LR 0.000013    Time 0.016509    
2023-01-06 16:19:23,061 - --- validate (epoch=107)-----------
2023-01-06 16:19:23,061 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:23,491 - Epoch: [107][   10/   28]    Loss 0.311179    Top1 88.007812    
2023-01-06 16:19:23,594 - Epoch: [107][   20/   28]    Loss 0.308930    Top1 88.398438    
2023-01-06 16:19:23,650 - Epoch: [107][   28/   28]    Loss 0.317684    Top1 88.147724    
2023-01-06 16:19:23,797 - ==> Top1: 88.148    Loss: 0.318

2023-01-06 16:19:23,797 - ==> Confusion:
[[ 172    5  262]
 [   8  160  434]
 [  61   58 5826]]

2023-01-06 16:19:23,799 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:23,799 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:23,803 - 

2023-01-06 16:19:23,803 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:24,331 - Epoch: [108][   10/  246]    Overall Loss 0.294382    Objective Loss 0.294382                                        LR 0.000013    Time 0.052712    
2023-01-06 16:19:24,484 - Epoch: [108][   20/  246]    Overall Loss 0.310411    Objective Loss 0.310411                                        LR 0.000013    Time 0.033995    
2023-01-06 16:19:24,634 - Epoch: [108][   30/  246]    Overall Loss 0.308289    Objective Loss 0.308289                                        LR 0.000013    Time 0.027626    
2023-01-06 16:19:24,809 - Epoch: [108][   40/  246]    Overall Loss 0.312490    Objective Loss 0.312490                                        LR 0.000013    Time 0.025088    
2023-01-06 16:19:24,978 - Epoch: [108][   50/  246]    Overall Loss 0.313357    Objective Loss 0.313357                                        LR 0.000013    Time 0.023446    
2023-01-06 16:19:25,155 - Epoch: [108][   60/  246]    Overall Loss 0.310722    Objective Loss 0.310722                                        LR 0.000013    Time 0.022482    
2023-01-06 16:19:25,325 - Epoch: [108][   70/  246]    Overall Loss 0.311415    Objective Loss 0.311415                                        LR 0.000013    Time 0.021695    
2023-01-06 16:19:25,497 - Epoch: [108][   80/  246]    Overall Loss 0.311648    Objective Loss 0.311648                                        LR 0.000013    Time 0.021121    
2023-01-06 16:19:25,664 - Epoch: [108][   90/  246]    Overall Loss 0.311587    Objective Loss 0.311587                                        LR 0.000013    Time 0.020634    
2023-01-06 16:19:25,841 - Epoch: [108][  100/  246]    Overall Loss 0.312490    Objective Loss 0.312490                                        LR 0.000013    Time 0.020332    
2023-01-06 16:19:26,008 - Epoch: [108][  110/  246]    Overall Loss 0.312157    Objective Loss 0.312157                                        LR 0.000013    Time 0.020002    
2023-01-06 16:19:26,174 - Epoch: [108][  120/  246]    Overall Loss 0.310392    Objective Loss 0.310392                                        LR 0.000013    Time 0.019710    
2023-01-06 16:19:26,342 - Epoch: [108][  130/  246]    Overall Loss 0.311513    Objective Loss 0.311513                                        LR 0.000013    Time 0.019487    
2023-01-06 16:19:26,516 - Epoch: [108][  140/  246]    Overall Loss 0.310645    Objective Loss 0.310645                                        LR 0.000013    Time 0.019334    
2023-01-06 16:19:26,684 - Epoch: [108][  150/  246]    Overall Loss 0.311986    Objective Loss 0.311986                                        LR 0.000013    Time 0.019163    
2023-01-06 16:19:26,865 - Epoch: [108][  160/  246]    Overall Loss 0.312452    Objective Loss 0.312452                                        LR 0.000013    Time 0.019094    
2023-01-06 16:19:27,034 - Epoch: [108][  170/  246]    Overall Loss 0.312263    Objective Loss 0.312263                                        LR 0.000013    Time 0.018963    
2023-01-06 16:19:27,208 - Epoch: [108][  180/  246]    Overall Loss 0.312796    Objective Loss 0.312796                                        LR 0.000013    Time 0.018869    
2023-01-06 16:19:27,353 - Epoch: [108][  190/  246]    Overall Loss 0.313322    Objective Loss 0.313322                                        LR 0.000013    Time 0.018641    
2023-01-06 16:19:27,499 - Epoch: [108][  200/  246]    Overall Loss 0.314010    Objective Loss 0.314010                                        LR 0.000013    Time 0.018435    
2023-01-06 16:19:27,640 - Epoch: [108][  210/  246]    Overall Loss 0.314842    Objective Loss 0.314842                                        LR 0.000013    Time 0.018228    
2023-01-06 16:19:27,780 - Epoch: [108][  220/  246]    Overall Loss 0.316354    Objective Loss 0.316354                                        LR 0.000013    Time 0.018033    
2023-01-06 16:19:27,920 - Epoch: [108][  230/  246]    Overall Loss 0.316718    Objective Loss 0.316718                                        LR 0.000013    Time 0.017855    
2023-01-06 16:19:28,075 - Epoch: [108][  240/  246]    Overall Loss 0.317356    Objective Loss 0.317356                                        LR 0.000013    Time 0.017756    
2023-01-06 16:19:28,145 - Epoch: [108][  246/  246]    Overall Loss 0.317298    Objective Loss 0.317298    Top1 88.277512    LR 0.000013    Time 0.017605    
2023-01-06 16:19:28,267 - --- validate (epoch=108)-----------
2023-01-06 16:19:28,270 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:28,713 - Epoch: [108][   10/   28]    Loss 0.330815    Top1 87.773438    
2023-01-06 16:19:28,841 - Epoch: [108][   20/   28]    Loss 0.328366    Top1 87.695312    
2023-01-06 16:19:28,897 - Epoch: [108][   28/   28]    Loss 0.318717    Top1 88.061838    
2023-01-06 16:19:29,042 - ==> Top1: 88.062    Loss: 0.319

2023-01-06 16:19:29,043 - ==> Confusion:
[[ 167    5  267]
 [   8  143  451]
 [  53   50 5842]]

2023-01-06 16:19:29,044 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:29,044 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:29,049 - 

2023-01-06 16:19:29,049 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:29,736 - Epoch: [109][   10/  246]    Overall Loss 0.337138    Objective Loss 0.337138                                        LR 0.000013    Time 0.068674    
2023-01-06 16:19:29,898 - Epoch: [109][   20/  246]    Overall Loss 0.330621    Objective Loss 0.330621                                        LR 0.000013    Time 0.042423    
2023-01-06 16:19:30,053 - Epoch: [109][   30/  246]    Overall Loss 0.324628    Objective Loss 0.324628                                        LR 0.000013    Time 0.033414    
2023-01-06 16:19:30,211 - Epoch: [109][   40/  246]    Overall Loss 0.321945    Objective Loss 0.321945                                        LR 0.000013    Time 0.029019    
2023-01-06 16:19:30,379 - Epoch: [109][   50/  246]    Overall Loss 0.318292    Objective Loss 0.318292                                        LR 0.000013    Time 0.026556    
2023-01-06 16:19:30,546 - Epoch: [109][   60/  246]    Overall Loss 0.320556    Objective Loss 0.320556                                        LR 0.000013    Time 0.024916    
2023-01-06 16:19:30,715 - Epoch: [109][   70/  246]    Overall Loss 0.319290    Objective Loss 0.319290                                        LR 0.000013    Time 0.023754    
2023-01-06 16:19:30,883 - Epoch: [109][   80/  246]    Overall Loss 0.323290    Objective Loss 0.323290                                        LR 0.000013    Time 0.022883    
2023-01-06 16:19:31,051 - Epoch: [109][   90/  246]    Overall Loss 0.322878    Objective Loss 0.322878                                        LR 0.000013    Time 0.022208    
2023-01-06 16:19:31,219 - Epoch: [109][  100/  246]    Overall Loss 0.321018    Objective Loss 0.321018                                        LR 0.000013    Time 0.021664    
2023-01-06 16:19:31,387 - Epoch: [109][  110/  246]    Overall Loss 0.320617    Objective Loss 0.320617                                        LR 0.000013    Time 0.021221    
2023-01-06 16:19:31,555 - Epoch: [109][  120/  246]    Overall Loss 0.319856    Objective Loss 0.319856                                        LR 0.000013    Time 0.020844    
2023-01-06 16:19:31,727 - Epoch: [109][  130/  246]    Overall Loss 0.319247    Objective Loss 0.319247                                        LR 0.000013    Time 0.020564    
2023-01-06 16:19:31,912 - Epoch: [109][  140/  246]    Overall Loss 0.318518    Objective Loss 0.318518                                        LR 0.000013    Time 0.020411    
2023-01-06 16:19:32,083 - Epoch: [109][  150/  246]    Overall Loss 0.319417    Objective Loss 0.319417                                        LR 0.000013    Time 0.020186    
2023-01-06 16:19:32,267 - Epoch: [109][  160/  246]    Overall Loss 0.319882    Objective Loss 0.319882                                        LR 0.000013    Time 0.020071    
2023-01-06 16:19:32,445 - Epoch: [109][  170/  246]    Overall Loss 0.319088    Objective Loss 0.319088                                        LR 0.000013    Time 0.019938    
2023-01-06 16:19:32,613 - Epoch: [109][  180/  246]    Overall Loss 0.319647    Objective Loss 0.319647                                        LR 0.000013    Time 0.019760    
2023-01-06 16:19:32,780 - Epoch: [109][  190/  246]    Overall Loss 0.318968    Objective Loss 0.318968                                        LR 0.000013    Time 0.019595    
2023-01-06 16:19:32,945 - Epoch: [109][  200/  246]    Overall Loss 0.318935    Objective Loss 0.318935                                        LR 0.000013    Time 0.019436    
2023-01-06 16:19:33,113 - Epoch: [109][  210/  246]    Overall Loss 0.318279    Objective Loss 0.318279                                        LR 0.000013    Time 0.019309    
2023-01-06 16:19:33,275 - Epoch: [109][  220/  246]    Overall Loss 0.317892    Objective Loss 0.317892                                        LR 0.000013    Time 0.019163    
2023-01-06 16:19:33,440 - Epoch: [109][  230/  246]    Overall Loss 0.317451    Objective Loss 0.317451                                        LR 0.000013    Time 0.019048    
2023-01-06 16:19:33,597 - Epoch: [109][  240/  246]    Overall Loss 0.317593    Objective Loss 0.317593                                        LR 0.000013    Time 0.018899    
2023-01-06 16:19:33,667 - Epoch: [109][  246/  246]    Overall Loss 0.318069    Objective Loss 0.318069    Top1 84.928230    LR 0.000013    Time 0.018723    
2023-01-06 16:19:33,809 - --- validate (epoch=109)-----------
2023-01-06 16:19:33,809 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:34,242 - Epoch: [109][   10/   28]    Loss 0.315086    Top1 88.632812    
2023-01-06 16:19:34,347 - Epoch: [109][   20/   28]    Loss 0.315204    Top1 88.203125    
2023-01-06 16:19:34,407 - Epoch: [109][   28/   28]    Loss 0.316807    Top1 88.233610    
2023-01-06 16:19:34,554 - ==> Top1: 88.234    Loss: 0.317

2023-01-06 16:19:34,555 - ==> Confusion:
[[ 179    6  254]
 [   8  174  420]
 [  69   65 5811]]

2023-01-06 16:19:34,557 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:34,557 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:34,561 - 

2023-01-06 16:19:34,562 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:35,098 - Epoch: [110][   10/  246]    Overall Loss 0.321033    Objective Loss 0.321033                                        LR 0.000013    Time 0.053594    
2023-01-06 16:19:35,252 - Epoch: [110][   20/  246]    Overall Loss 0.327685    Objective Loss 0.327685                                        LR 0.000013    Time 0.034470    
2023-01-06 16:19:35,407 - Epoch: [110][   30/  246]    Overall Loss 0.326541    Objective Loss 0.326541                                        LR 0.000013    Time 0.028100    
2023-01-06 16:19:35,555 - Epoch: [110][   40/  246]    Overall Loss 0.328213    Objective Loss 0.328213                                        LR 0.000013    Time 0.024762    
2023-01-06 16:19:35,704 - Epoch: [110][   50/  246]    Overall Loss 0.327829    Objective Loss 0.327829                                        LR 0.000013    Time 0.022750    
2023-01-06 16:19:35,855 - Epoch: [110][   60/  246]    Overall Loss 0.329460    Objective Loss 0.329460                                        LR 0.000013    Time 0.021467    
2023-01-06 16:19:36,010 - Epoch: [110][   70/  246]    Overall Loss 0.328272    Objective Loss 0.328272                                        LR 0.000013    Time 0.020622    
2023-01-06 16:19:36,169 - Epoch: [110][   80/  246]    Overall Loss 0.326835    Objective Loss 0.326835                                        LR 0.000013    Time 0.020018    
2023-01-06 16:19:36,324 - Epoch: [110][   90/  246]    Overall Loss 0.324271    Objective Loss 0.324271                                        LR 0.000013    Time 0.019518    
2023-01-06 16:19:36,483 - Epoch: [110][  100/  246]    Overall Loss 0.324006    Objective Loss 0.324006                                        LR 0.000013    Time 0.019139    
2023-01-06 16:19:36,636 - Epoch: [110][  110/  246]    Overall Loss 0.322115    Objective Loss 0.322115                                        LR 0.000013    Time 0.018785    
2023-01-06 16:19:36,781 - Epoch: [110][  120/  246]    Overall Loss 0.321904    Objective Loss 0.321904                                        LR 0.000013    Time 0.018419    
2023-01-06 16:19:36,936 - Epoch: [110][  130/  246]    Overall Loss 0.320256    Objective Loss 0.320256                                        LR 0.000013    Time 0.018189    
2023-01-06 16:19:37,079 - Epoch: [110][  140/  246]    Overall Loss 0.318962    Objective Loss 0.318962                                        LR 0.000013    Time 0.017905    
2023-01-06 16:19:37,229 - Epoch: [110][  150/  246]    Overall Loss 0.319584    Objective Loss 0.319584                                        LR 0.000013    Time 0.017707    
2023-01-06 16:19:37,377 - Epoch: [110][  160/  246]    Overall Loss 0.318284    Objective Loss 0.318284                                        LR 0.000013    Time 0.017520    
2023-01-06 16:19:37,538 - Epoch: [110][  170/  246]    Overall Loss 0.317141    Objective Loss 0.317141                                        LR 0.000013    Time 0.017436    
2023-01-06 16:19:37,685 - Epoch: [110][  180/  246]    Overall Loss 0.316454    Objective Loss 0.316454                                        LR 0.000013    Time 0.017278    
2023-01-06 16:19:37,838 - Epoch: [110][  190/  246]    Overall Loss 0.315982    Objective Loss 0.315982                                        LR 0.000013    Time 0.017174    
2023-01-06 16:19:37,982 - Epoch: [110][  200/  246]    Overall Loss 0.317119    Objective Loss 0.317119                                        LR 0.000013    Time 0.017031    
2023-01-06 16:19:38,137 - Epoch: [110][  210/  246]    Overall Loss 0.316989    Objective Loss 0.316989                                        LR 0.000013    Time 0.016955    
2023-01-06 16:19:38,282 - Epoch: [110][  220/  246]    Overall Loss 0.316926    Objective Loss 0.316926                                        LR 0.000013    Time 0.016844    
2023-01-06 16:19:38,437 - Epoch: [110][  230/  246]    Overall Loss 0.316416    Objective Loss 0.316416                                        LR 0.000013    Time 0.016782    
2023-01-06 16:19:38,596 - Epoch: [110][  240/  246]    Overall Loss 0.316794    Objective Loss 0.316794                                        LR 0.000013    Time 0.016745    
2023-01-06 16:19:38,670 - Epoch: [110][  246/  246]    Overall Loss 0.316858    Objective Loss 0.316858    Top1 89.712919    LR 0.000013    Time 0.016635    
2023-01-06 16:19:38,810 - --- validate (epoch=110)-----------
2023-01-06 16:19:38,811 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:39,247 - Epoch: [110][   10/   28]    Loss 0.305219    Top1 88.710938    
2023-01-06 16:19:39,350 - Epoch: [110][   20/   28]    Loss 0.308025    Top1 88.496094    
2023-01-06 16:19:39,409 - Epoch: [110][   28/   28]    Loss 0.315244    Top1 88.176353    
2023-01-06 16:19:39,551 - ==> Top1: 88.176    Loss: 0.315

2023-01-06 16:19:39,551 - ==> Confusion:
[[ 169    6  264]
 [   7  171  424]
 [  59   66 5820]]

2023-01-06 16:19:39,552 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:39,553 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:39,557 - 

2023-01-06 16:19:39,557 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:40,205 - Epoch: [111][   10/  246]    Overall Loss 0.314820    Objective Loss 0.314820                                        LR 0.000013    Time 0.064732    
2023-01-06 16:19:40,348 - Epoch: [111][   20/  246]    Overall Loss 0.321608    Objective Loss 0.321608                                        LR 0.000013    Time 0.039449    
2023-01-06 16:19:40,485 - Epoch: [111][   30/  246]    Overall Loss 0.318991    Objective Loss 0.318991                                        LR 0.000013    Time 0.030860    
2023-01-06 16:19:40,620 - Epoch: [111][   40/  246]    Overall Loss 0.318825    Objective Loss 0.318825                                        LR 0.000013    Time 0.026523    
2023-01-06 16:19:40,768 - Epoch: [111][   50/  246]    Overall Loss 0.318624    Objective Loss 0.318624                                        LR 0.000013    Time 0.024160    
2023-01-06 16:19:40,919 - Epoch: [111][   60/  246]    Overall Loss 0.318225    Objective Loss 0.318225                                        LR 0.000013    Time 0.022640    
2023-01-06 16:19:41,067 - Epoch: [111][   70/  246]    Overall Loss 0.316850    Objective Loss 0.316850                                        LR 0.000013    Time 0.021525    
2023-01-06 16:19:41,209 - Epoch: [111][   80/  246]    Overall Loss 0.317761    Objective Loss 0.317761                                        LR 0.000013    Time 0.020603    
2023-01-06 16:19:41,355 - Epoch: [111][   90/  246]    Overall Loss 0.317251    Objective Loss 0.317251                                        LR 0.000013    Time 0.019927    
2023-01-06 16:19:41,501 - Epoch: [111][  100/  246]    Overall Loss 0.317698    Objective Loss 0.317698                                        LR 0.000013    Time 0.019391    
2023-01-06 16:19:41,656 - Epoch: [111][  110/  246]    Overall Loss 0.316356    Objective Loss 0.316356                                        LR 0.000013    Time 0.019037    
2023-01-06 16:19:41,819 - Epoch: [111][  120/  246]    Overall Loss 0.314929    Objective Loss 0.314929                                        LR 0.000013    Time 0.018804    
2023-01-06 16:19:41,973 - Epoch: [111][  130/  246]    Overall Loss 0.315292    Objective Loss 0.315292                                        LR 0.000013    Time 0.018537    
2023-01-06 16:19:42,117 - Epoch: [111][  140/  246]    Overall Loss 0.314834    Objective Loss 0.314834                                        LR 0.000013    Time 0.018240    
2023-01-06 16:19:42,260 - Epoch: [111][  150/  246]    Overall Loss 0.314847    Objective Loss 0.314847                                        LR 0.000013    Time 0.017972    
2023-01-06 16:19:42,401 - Epoch: [111][  160/  246]    Overall Loss 0.314418    Objective Loss 0.314418                                        LR 0.000013    Time 0.017732    
2023-01-06 16:19:42,546 - Epoch: [111][  170/  246]    Overall Loss 0.314046    Objective Loss 0.314046                                        LR 0.000013    Time 0.017536    
2023-01-06 16:19:42,695 - Epoch: [111][  180/  246]    Overall Loss 0.315209    Objective Loss 0.315209                                        LR 0.000013    Time 0.017387    
2023-01-06 16:19:42,860 - Epoch: [111][  190/  246]    Overall Loss 0.316657    Objective Loss 0.316657                                        LR 0.000013    Time 0.017339    
2023-01-06 16:19:43,023 - Epoch: [111][  200/  246]    Overall Loss 0.317179    Objective Loss 0.317179                                        LR 0.000013    Time 0.017285    
2023-01-06 16:19:43,189 - Epoch: [111][  210/  246]    Overall Loss 0.316685    Objective Loss 0.316685                                        LR 0.000013    Time 0.017248    
2023-01-06 16:19:43,353 - Epoch: [111][  220/  246]    Overall Loss 0.315643    Objective Loss 0.315643                                        LR 0.000013    Time 0.017209    
2023-01-06 16:19:43,519 - Epoch: [111][  230/  246]    Overall Loss 0.315721    Objective Loss 0.315721                                        LR 0.000013    Time 0.017182    
2023-01-06 16:19:43,696 - Epoch: [111][  240/  246]    Overall Loss 0.316705    Objective Loss 0.316705                                        LR 0.000013    Time 0.017202    
2023-01-06 16:19:43,777 - Epoch: [111][  246/  246]    Overall Loss 0.316952    Objective Loss 0.316952    Top1 88.516746    LR 0.000013    Time 0.017110    
2023-01-06 16:19:43,921 - --- validate (epoch=111)-----------
2023-01-06 16:19:43,921 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:44,353 - Epoch: [111][   10/   28]    Loss 0.319261    Top1 87.890625    
2023-01-06 16:19:44,461 - Epoch: [111][   20/   28]    Loss 0.309650    Top1 88.359375    
2023-01-06 16:19:44,517 - Epoch: [111][   28/   28]    Loss 0.316841    Top1 88.290867    
2023-01-06 16:19:44,693 - ==> Top1: 88.291    Loss: 0.317

2023-01-06 16:19:44,694 - ==> Confusion:
[[ 174    6  259]
 [   8  172  422]
 [  62   61 5822]]

2023-01-06 16:19:44,695 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:44,695 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:44,699 - 

2023-01-06 16:19:44,700 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:45,385 - Epoch: [112][   10/  246]    Overall Loss 0.316315    Objective Loss 0.316315                                        LR 0.000013    Time 0.068490    
2023-01-06 16:19:45,560 - Epoch: [112][   20/  246]    Overall Loss 0.306327    Objective Loss 0.306327                                        LR 0.000013    Time 0.042989    
2023-01-06 16:19:45,744 - Epoch: [112][   30/  246]    Overall Loss 0.313700    Objective Loss 0.313700                                        LR 0.000013    Time 0.034751    
2023-01-06 16:19:45,919 - Epoch: [112][   40/  246]    Overall Loss 0.316350    Objective Loss 0.316350                                        LR 0.000013    Time 0.030394    
2023-01-06 16:19:46,101 - Epoch: [112][   50/  246]    Overall Loss 0.315879    Objective Loss 0.315879                                        LR 0.000013    Time 0.027944    
2023-01-06 16:19:46,278 - Epoch: [112][   60/  246]    Overall Loss 0.318333    Objective Loss 0.318333                                        LR 0.000013    Time 0.026197    
2023-01-06 16:19:46,460 - Epoch: [112][   70/  246]    Overall Loss 0.319220    Objective Loss 0.319220                                        LR 0.000013    Time 0.025054    
2023-01-06 16:19:46,636 - Epoch: [112][   80/  246]    Overall Loss 0.319031    Objective Loss 0.319031                                        LR 0.000013    Time 0.024093    
2023-01-06 16:19:46,818 - Epoch: [112][   90/  246]    Overall Loss 0.319000    Objective Loss 0.319000                                        LR 0.000013    Time 0.023438    
2023-01-06 16:19:46,992 - Epoch: [112][  100/  246]    Overall Loss 0.320415    Objective Loss 0.320415                                        LR 0.000013    Time 0.022834    
2023-01-06 16:19:47,176 - Epoch: [112][  110/  246]    Overall Loss 0.320836    Objective Loss 0.320836                                        LR 0.000013    Time 0.022422    
2023-01-06 16:19:47,349 - Epoch: [112][  120/  246]    Overall Loss 0.320521    Objective Loss 0.320521                                        LR 0.000013    Time 0.021997    
2023-01-06 16:19:47,533 - Epoch: [112][  130/  246]    Overall Loss 0.321835    Objective Loss 0.321835                                        LR 0.000013    Time 0.021712    
2023-01-06 16:19:47,706 - Epoch: [112][  140/  246]    Overall Loss 0.322105    Objective Loss 0.322105                                        LR 0.000013    Time 0.021395    
2023-01-06 16:19:47,886 - Epoch: [112][  150/  246]    Overall Loss 0.321795    Objective Loss 0.321795                                        LR 0.000013    Time 0.021168    
2023-01-06 16:19:48,060 - Epoch: [112][  160/  246]    Overall Loss 0.320514    Objective Loss 0.320514                                        LR 0.000013    Time 0.020928    
2023-01-06 16:19:48,244 - Epoch: [112][  170/  246]    Overall Loss 0.319486    Objective Loss 0.319486                                        LR 0.000013    Time 0.020779    
2023-01-06 16:19:48,421 - Epoch: [112][  180/  246]    Overall Loss 0.319509    Objective Loss 0.319509                                        LR 0.000013    Time 0.020603    
2023-01-06 16:19:48,605 - Epoch: [112][  190/  246]    Overall Loss 0.318682    Objective Loss 0.318682                                        LR 0.000013    Time 0.020486    
2023-01-06 16:19:48,782 - Epoch: [112][  200/  246]    Overall Loss 0.319813    Objective Loss 0.319813                                        LR 0.000013    Time 0.020348    
2023-01-06 16:19:48,966 - Epoch: [112][  210/  246]    Overall Loss 0.318681    Objective Loss 0.318681                                        LR 0.000013    Time 0.020253    
2023-01-06 16:19:49,139 - Epoch: [112][  220/  246]    Overall Loss 0.318610    Objective Loss 0.318610                                        LR 0.000013    Time 0.020116    
2023-01-06 16:19:49,302 - Epoch: [112][  230/  246]    Overall Loss 0.317889    Objective Loss 0.317889                                        LR 0.000013    Time 0.019948    
2023-01-06 16:19:49,479 - Epoch: [112][  240/  246]    Overall Loss 0.318093    Objective Loss 0.318093                                        LR 0.000013    Time 0.019853    
2023-01-06 16:19:49,553 - Epoch: [112][  246/  246]    Overall Loss 0.317944    Objective Loss 0.317944    Top1 87.559809    LR 0.000013    Time 0.019671    
2023-01-06 16:19:49,690 - --- validate (epoch=112)-----------
2023-01-06 16:19:49,690 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:50,131 - Epoch: [112][   10/   28]    Loss 0.314427    Top1 88.085938    
2023-01-06 16:19:50,237 - Epoch: [112][   20/   28]    Loss 0.313872    Top1 88.183594    
2023-01-06 16:19:50,296 - Epoch: [112][   28/   28]    Loss 0.316806    Top1 88.104781    
2023-01-06 16:19:50,437 - ==> Top1: 88.105    Loss: 0.317

2023-01-06 16:19:50,437 - ==> Confusion:
[[ 168    5  266]
 [   7  156  439]
 [  58   56 5831]]

2023-01-06 16:19:50,438 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:50,438 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:50,443 - 

2023-01-06 16:19:50,443 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:50,958 - Epoch: [113][   10/  246]    Overall Loss 0.285675    Objective Loss 0.285675                                        LR 0.000013    Time 0.051498    
2023-01-06 16:19:51,106 - Epoch: [113][   20/  246]    Overall Loss 0.301420    Objective Loss 0.301420                                        LR 0.000013    Time 0.033097    
2023-01-06 16:19:51,266 - Epoch: [113][   30/  246]    Overall Loss 0.312223    Objective Loss 0.312223                                        LR 0.000013    Time 0.027405    
2023-01-06 16:19:51,424 - Epoch: [113][   40/  246]    Overall Loss 0.313436    Objective Loss 0.313436                                        LR 0.000013    Time 0.024475    
2023-01-06 16:19:51,584 - Epoch: [113][   50/  246]    Overall Loss 0.316667    Objective Loss 0.316667                                        LR 0.000013    Time 0.022786    
2023-01-06 16:19:51,745 - Epoch: [113][   60/  246]    Overall Loss 0.315314    Objective Loss 0.315314                                        LR 0.000013    Time 0.021664    
2023-01-06 16:19:51,905 - Epoch: [113][   70/  246]    Overall Loss 0.312310    Objective Loss 0.312310                                        LR 0.000013    Time 0.020851    
2023-01-06 16:19:52,064 - Epoch: [113][   80/  246]    Overall Loss 0.315159    Objective Loss 0.315159                                        LR 0.000013    Time 0.020226    
2023-01-06 16:19:52,235 - Epoch: [113][   90/  246]    Overall Loss 0.316555    Objective Loss 0.316555                                        LR 0.000013    Time 0.019876    
2023-01-06 16:19:52,408 - Epoch: [113][  100/  246]    Overall Loss 0.316102    Objective Loss 0.316102                                        LR 0.000013    Time 0.019617    
2023-01-06 16:19:52,579 - Epoch: [113][  110/  246]    Overall Loss 0.318708    Objective Loss 0.318708                                        LR 0.000013    Time 0.019383    
2023-01-06 16:19:52,747 - Epoch: [113][  120/  246]    Overall Loss 0.317811    Objective Loss 0.317811                                        LR 0.000013    Time 0.019160    
2023-01-06 16:19:52,915 - Epoch: [113][  130/  246]    Overall Loss 0.316973    Objective Loss 0.316973                                        LR 0.000013    Time 0.018980    
2023-01-06 16:19:53,082 - Epoch: [113][  140/  246]    Overall Loss 0.317355    Objective Loss 0.317355                                        LR 0.000013    Time 0.018809    
2023-01-06 16:19:53,247 - Epoch: [113][  150/  246]    Overall Loss 0.317871    Objective Loss 0.317871                                        LR 0.000013    Time 0.018654    
2023-01-06 16:19:53,415 - Epoch: [113][  160/  246]    Overall Loss 0.317232    Objective Loss 0.317232                                        LR 0.000013    Time 0.018538    
2023-01-06 16:19:53,582 - Epoch: [113][  170/  246]    Overall Loss 0.317640    Objective Loss 0.317640                                        LR 0.000013    Time 0.018426    
2023-01-06 16:19:53,759 - Epoch: [113][  180/  246]    Overall Loss 0.317631    Objective Loss 0.317631                                        LR 0.000013    Time 0.018385    
2023-01-06 16:19:53,939 - Epoch: [113][  190/  246]    Overall Loss 0.316996    Objective Loss 0.316996                                        LR 0.000013    Time 0.018361    
2023-01-06 16:19:54,115 - Epoch: [113][  200/  246]    Overall Loss 0.317395    Objective Loss 0.317395                                        LR 0.000013    Time 0.018313    
2023-01-06 16:19:54,296 - Epoch: [113][  210/  246]    Overall Loss 0.317220    Objective Loss 0.317220                                        LR 0.000013    Time 0.018299    
2023-01-06 16:19:54,470 - Epoch: [113][  220/  246]    Overall Loss 0.317329    Objective Loss 0.317329                                        LR 0.000013    Time 0.018250    
2023-01-06 16:19:54,654 - Epoch: [113][  230/  246]    Overall Loss 0.317729    Objective Loss 0.317729                                        LR 0.000013    Time 0.018254    
2023-01-06 16:19:54,847 - Epoch: [113][  240/  246]    Overall Loss 0.317649    Objective Loss 0.317649                                        LR 0.000013    Time 0.018287    
2023-01-06 16:19:54,929 - Epoch: [113][  246/  246]    Overall Loss 0.316769    Objective Loss 0.316769    Top1 89.712919    LR 0.000013    Time 0.018175    
2023-01-06 16:19:55,082 - --- validate (epoch=113)-----------
2023-01-06 16:19:55,083 - 6986 samples (256 per mini-batch)
2023-01-06 16:19:55,510 - Epoch: [113][   10/   28]    Loss 0.348721    Top1 86.679688    
2023-01-06 16:19:55,612 - Epoch: [113][   20/   28]    Loss 0.319236    Top1 87.949219    
2023-01-06 16:19:55,672 - Epoch: [113][   28/   28]    Loss 0.317205    Top1 88.133410    
2023-01-06 16:19:55,816 - ==> Top1: 88.133    Loss: 0.317

2023-01-06 16:19:55,817 - ==> Confusion:
[[ 165    6  268]
 [   7  155  440]
 [  55   53 5837]]

2023-01-06 16:19:55,818 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:19:55,818 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:19:55,822 - 

2023-01-06 16:19:55,822 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:19:56,509 - Epoch: [114][   10/  246]    Overall Loss 0.337408    Objective Loss 0.337408                                        LR 0.000013    Time 0.068606    
2023-01-06 16:19:56,668 - Epoch: [114][   20/  246]    Overall Loss 0.329769    Objective Loss 0.329769                                        LR 0.000013    Time 0.042215    
2023-01-06 16:19:56,838 - Epoch: [114][   30/  246]    Overall Loss 0.325775    Objective Loss 0.325775                                        LR 0.000013    Time 0.033806    
2023-01-06 16:19:57,012 - Epoch: [114][   40/  246]    Overall Loss 0.323871    Objective Loss 0.323871                                        LR 0.000013    Time 0.029703    
2023-01-06 16:19:57,192 - Epoch: [114][   50/  246]    Overall Loss 0.320992    Objective Loss 0.320992                                        LR 0.000013    Time 0.027339    
2023-01-06 16:19:57,356 - Epoch: [114][   60/  246]    Overall Loss 0.319669    Objective Loss 0.319669                                        LR 0.000013    Time 0.025520    
2023-01-06 16:19:57,538 - Epoch: [114][   70/  246]    Overall Loss 0.316352    Objective Loss 0.316352                                        LR 0.000013    Time 0.024464    
2023-01-06 16:19:57,717 - Epoch: [114][   80/  246]    Overall Loss 0.315789    Objective Loss 0.315789                                        LR 0.000013    Time 0.023638    
2023-01-06 16:19:57,912 - Epoch: [114][   90/  246]    Overall Loss 0.317582    Objective Loss 0.317582                                        LR 0.000013    Time 0.023180    
2023-01-06 16:19:58,114 - Epoch: [114][  100/  246]    Overall Loss 0.317929    Objective Loss 0.317929                                        LR 0.000013    Time 0.022871    
2023-01-06 16:19:58,318 - Epoch: [114][  110/  246]    Overall Loss 0.317404    Objective Loss 0.317404                                        LR 0.000013    Time 0.022647    
2023-01-06 16:19:58,521 - Epoch: [114][  120/  246]    Overall Loss 0.317386    Objective Loss 0.317386                                        LR 0.000013    Time 0.022447    
2023-01-06 16:19:58,726 - Epoch: [114][  130/  246]    Overall Loss 0.318242    Objective Loss 0.318242                                        LR 0.000013    Time 0.022293    
2023-01-06 16:19:58,928 - Epoch: [114][  140/  246]    Overall Loss 0.319054    Objective Loss 0.319054                                        LR 0.000013    Time 0.022144    
2023-01-06 16:19:59,121 - Epoch: [114][  150/  246]    Overall Loss 0.318664    Objective Loss 0.318664                                        LR 0.000013    Time 0.021954    
2023-01-06 16:19:59,315 - Epoch: [114][  160/  246]    Overall Loss 0.317826    Objective Loss 0.317826                                        LR 0.000013    Time 0.021789    
2023-01-06 16:19:59,510 - Epoch: [114][  170/  246]    Overall Loss 0.317702    Objective Loss 0.317702                                        LR 0.000013    Time 0.021651    
2023-01-06 16:19:59,703 - Epoch: [114][  180/  246]    Overall Loss 0.316330    Objective Loss 0.316330                                        LR 0.000013    Time 0.021519    
2023-01-06 16:19:59,897 - Epoch: [114][  190/  246]    Overall Loss 0.316035    Objective Loss 0.316035                                        LR 0.000013    Time 0.021404    
2023-01-06 16:20:00,091 - Epoch: [114][  200/  246]    Overall Loss 0.316030    Objective Loss 0.316030                                        LR 0.000013    Time 0.021305    
2023-01-06 16:20:00,285 - Epoch: [114][  210/  246]    Overall Loss 0.315953    Objective Loss 0.315953                                        LR 0.000013    Time 0.021209    
2023-01-06 16:20:00,477 - Epoch: [114][  220/  246]    Overall Loss 0.316204    Objective Loss 0.316204                                        LR 0.000013    Time 0.021117    
2023-01-06 16:20:00,678 - Epoch: [114][  230/  246]    Overall Loss 0.316741    Objective Loss 0.316741                                        LR 0.000013    Time 0.021070    
2023-01-06 16:20:00,886 - Epoch: [114][  240/  246]    Overall Loss 0.316256    Objective Loss 0.316256                                        LR 0.000013    Time 0.021058    
2023-01-06 16:20:00,971 - Epoch: [114][  246/  246]    Overall Loss 0.316739    Objective Loss 0.316739    Top1 86.842105    LR 0.000013    Time 0.020888    
2023-01-06 16:20:01,132 - --- validate (epoch=114)-----------
2023-01-06 16:20:01,132 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:01,588 - Epoch: [114][   10/   28]    Loss 0.316897    Top1 88.750000    
2023-01-06 16:20:01,698 - Epoch: [114][   20/   28]    Loss 0.318611    Top1 88.320312    
2023-01-06 16:20:01,756 - Epoch: [114][   28/   28]    Loss 0.316094    Top1 88.290867    
2023-01-06 16:20:01,910 - ==> Top1: 88.291    Loss: 0.316

2023-01-06 16:20:01,910 - ==> Confusion:
[[ 168    6  265]
 [   9  179  414]
 [  56   68 5821]]

2023-01-06 16:20:01,911 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:01,911 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:01,916 - 

2023-01-06 16:20:01,916 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:02,605 - Epoch: [115][   10/  246]    Overall Loss 0.297586    Objective Loss 0.297586                                        LR 0.000013    Time 0.068811    
2023-01-06 16:20:02,773 - Epoch: [115][   20/  246]    Overall Loss 0.305847    Objective Loss 0.305847                                        LR 0.000013    Time 0.042730    
2023-01-06 16:20:02,947 - Epoch: [115][   30/  246]    Overall Loss 0.306286    Objective Loss 0.306286                                        LR 0.000013    Time 0.034281    
2023-01-06 16:20:03,127 - Epoch: [115][   40/  246]    Overall Loss 0.311745    Objective Loss 0.311745                                        LR 0.000013    Time 0.030199    
2023-01-06 16:20:03,309 - Epoch: [115][   50/  246]    Overall Loss 0.310830    Objective Loss 0.310830                                        LR 0.000013    Time 0.027792    
2023-01-06 16:20:03,495 - Epoch: [115][   60/  246]    Overall Loss 0.309485    Objective Loss 0.309485                                        LR 0.000013    Time 0.026246    
2023-01-06 16:20:03,676 - Epoch: [115][   70/  246]    Overall Loss 0.310665    Objective Loss 0.310665                                        LR 0.000013    Time 0.025055    
2023-01-06 16:20:03,859 - Epoch: [115][   80/  246]    Overall Loss 0.311371    Objective Loss 0.311371                                        LR 0.000013    Time 0.024215    
2023-01-06 16:20:04,040 - Epoch: [115][   90/  246]    Overall Loss 0.311497    Objective Loss 0.311497                                        LR 0.000013    Time 0.023531    
2023-01-06 16:20:04,210 - Epoch: [115][  100/  246]    Overall Loss 0.310421    Objective Loss 0.310421                                        LR 0.000013    Time 0.022867    
2023-01-06 16:20:04,380 - Epoch: [115][  110/  246]    Overall Loss 0.310449    Objective Loss 0.310449                                        LR 0.000013    Time 0.022330    
2023-01-06 16:20:04,542 - Epoch: [115][  120/  246]    Overall Loss 0.311899    Objective Loss 0.311899                                        LR 0.000013    Time 0.021821    
2023-01-06 16:20:04,726 - Epoch: [115][  130/  246]    Overall Loss 0.311906    Objective Loss 0.311906                                        LR 0.000013    Time 0.021552    
2023-01-06 16:20:04,911 - Epoch: [115][  140/  246]    Overall Loss 0.312024    Objective Loss 0.312024                                        LR 0.000013    Time 0.021336    
2023-01-06 16:20:05,102 - Epoch: [115][  150/  246]    Overall Loss 0.311653    Objective Loss 0.311653                                        LR 0.000013    Time 0.021182    
2023-01-06 16:20:05,294 - Epoch: [115][  160/  246]    Overall Loss 0.312845    Objective Loss 0.312845                                        LR 0.000013    Time 0.021054    
2023-01-06 16:20:05,488 - Epoch: [115][  170/  246]    Overall Loss 0.313749    Objective Loss 0.313749                                        LR 0.000013    Time 0.020956    
2023-01-06 16:20:05,680 - Epoch: [115][  180/  246]    Overall Loss 0.313968    Objective Loss 0.313968                                        LR 0.000013    Time 0.020856    
2023-01-06 16:20:05,874 - Epoch: [115][  190/  246]    Overall Loss 0.313803    Objective Loss 0.313803                                        LR 0.000013    Time 0.020778    
2023-01-06 16:20:06,060 - Epoch: [115][  200/  246]    Overall Loss 0.314250    Objective Loss 0.314250                                        LR 0.000013    Time 0.020666    
2023-01-06 16:20:06,252 - Epoch: [115][  210/  246]    Overall Loss 0.314934    Objective Loss 0.314934                                        LR 0.000013    Time 0.020592    
2023-01-06 16:20:06,455 - Epoch: [115][  220/  246]    Overall Loss 0.315437    Objective Loss 0.315437                                        LR 0.000013    Time 0.020579    
2023-01-06 16:20:06,658 - Epoch: [115][  230/  246]    Overall Loss 0.316164    Objective Loss 0.316164                                        LR 0.000013    Time 0.020563    
2023-01-06 16:20:06,847 - Epoch: [115][  240/  246]    Overall Loss 0.316117    Objective Loss 0.316117                                        LR 0.000013    Time 0.020488    
2023-01-06 16:20:06,929 - Epoch: [115][  246/  246]    Overall Loss 0.315936    Objective Loss 0.315936    Top1 89.234450    LR 0.000013    Time 0.020322    
2023-01-06 16:20:07,068 - --- validate (epoch=115)-----------
2023-01-06 16:20:07,068 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:07,513 - Epoch: [115][   10/   28]    Loss 0.315956    Top1 88.164062    
2023-01-06 16:20:07,619 - Epoch: [115][   20/   28]    Loss 0.310756    Top1 88.417969    
2023-01-06 16:20:07,678 - Epoch: [115][   28/   28]    Loss 0.319265    Top1 88.018895    
2023-01-06 16:20:07,822 - ==> Top1: 88.019    Loss: 0.319

2023-01-06 16:20:07,823 - ==> Confusion:
[[ 172    6  261]
 [   9  148  445]
 [  63   53 5829]]

2023-01-06 16:20:07,824 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:07,824 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:07,828 - 

2023-01-06 16:20:07,828 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:08,402 - Epoch: [116][   10/  246]    Overall Loss 0.319992    Objective Loss 0.319992                                        LR 0.000013    Time 0.057247    
2023-01-06 16:20:08,600 - Epoch: [116][   20/  246]    Overall Loss 0.321260    Objective Loss 0.321260                                        LR 0.000013    Time 0.038493    
2023-01-06 16:20:08,798 - Epoch: [116][   30/  246]    Overall Loss 0.321418    Objective Loss 0.321418                                        LR 0.000013    Time 0.032249    
2023-01-06 16:20:08,994 - Epoch: [116][   40/  246]    Overall Loss 0.319111    Objective Loss 0.319111                                        LR 0.000013    Time 0.029075    
2023-01-06 16:20:09,190 - Epoch: [116][   50/  246]    Overall Loss 0.315850    Objective Loss 0.315850                                        LR 0.000013    Time 0.027172    
2023-01-06 16:20:09,385 - Epoch: [116][   60/  246]    Overall Loss 0.311375    Objective Loss 0.311375                                        LR 0.000013    Time 0.025899    
2023-01-06 16:20:09,580 - Epoch: [116][   70/  246]    Overall Loss 0.315902    Objective Loss 0.315902                                        LR 0.000013    Time 0.024974    
2023-01-06 16:20:09,776 - Epoch: [116][   80/  246]    Overall Loss 0.314830    Objective Loss 0.314830                                        LR 0.000013    Time 0.024297    
2023-01-06 16:20:09,972 - Epoch: [116][   90/  246]    Overall Loss 0.314647    Objective Loss 0.314647                                        LR 0.000013    Time 0.023764    
2023-01-06 16:20:10,179 - Epoch: [116][  100/  246]    Overall Loss 0.314840    Objective Loss 0.314840                                        LR 0.000013    Time 0.023461    
2023-01-06 16:20:10,382 - Epoch: [116][  110/  246]    Overall Loss 0.312248    Objective Loss 0.312248                                        LR 0.000013    Time 0.023170    
2023-01-06 16:20:10,582 - Epoch: [116][  120/  246]    Overall Loss 0.312421    Objective Loss 0.312421                                        LR 0.000013    Time 0.022902    
2023-01-06 16:20:10,778 - Epoch: [116][  130/  246]    Overall Loss 0.313487    Objective Loss 0.313487                                        LR 0.000013    Time 0.022646    
2023-01-06 16:20:10,975 - Epoch: [116][  140/  246]    Overall Loss 0.313560    Objective Loss 0.313560                                        LR 0.000013    Time 0.022430    
2023-01-06 16:20:11,163 - Epoch: [116][  150/  246]    Overall Loss 0.313500    Objective Loss 0.313500                                        LR 0.000013    Time 0.022183    
2023-01-06 16:20:11,349 - Epoch: [116][  160/  246]    Overall Loss 0.314117    Objective Loss 0.314117                                        LR 0.000013    Time 0.021959    
2023-01-06 16:20:11,535 - Epoch: [116][  170/  246]    Overall Loss 0.314790    Objective Loss 0.314790                                        LR 0.000013    Time 0.021757    
2023-01-06 16:20:11,721 - Epoch: [116][  180/  246]    Overall Loss 0.314589    Objective Loss 0.314589                                        LR 0.000013    Time 0.021581    
2023-01-06 16:20:11,907 - Epoch: [116][  190/  246]    Overall Loss 0.313966    Objective Loss 0.313966                                        LR 0.000013    Time 0.021423    
2023-01-06 16:20:12,095 - Epoch: [116][  200/  246]    Overall Loss 0.314547    Objective Loss 0.314547                                        LR 0.000013    Time 0.021287    
2023-01-06 16:20:12,254 - Epoch: [116][  210/  246]    Overall Loss 0.314709    Objective Loss 0.314709                                        LR 0.000013    Time 0.021031    
2023-01-06 16:20:12,412 - Epoch: [116][  220/  246]    Overall Loss 0.316335    Objective Loss 0.316335                                        LR 0.000013    Time 0.020789    
2023-01-06 16:20:12,572 - Epoch: [116][  230/  246]    Overall Loss 0.316451    Objective Loss 0.316451                                        LR 0.000013    Time 0.020581    
2023-01-06 16:20:12,747 - Epoch: [116][  240/  246]    Overall Loss 0.315895    Objective Loss 0.315895                                        LR 0.000013    Time 0.020448    
2023-01-06 16:20:12,825 - Epoch: [116][  246/  246]    Overall Loss 0.315583    Objective Loss 0.315583    Top1 89.473684    LR 0.000013    Time 0.020266    
2023-01-06 16:20:12,968 - --- validate (epoch=116)-----------
2023-01-06 16:20:12,968 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:13,417 - Epoch: [116][   10/   28]    Loss 0.318838    Top1 88.593750    
2023-01-06 16:20:13,517 - Epoch: [116][   20/   28]    Loss 0.322205    Top1 87.968750    
2023-01-06 16:20:13,576 - Epoch: [116][   28/   28]    Loss 0.313760    Top1 88.348125    
2023-01-06 16:20:13,711 - ==> Top1: 88.348    Loss: 0.314

2023-01-06 16:20:13,712 - ==> Confusion:
[[ 183    6  250]
 [   8  184  410]
 [  74   66 5805]]

2023-01-06 16:20:13,713 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:13,713 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:13,717 - 

2023-01-06 16:20:13,717 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:14,426 - Epoch: [117][   10/  246]    Overall Loss 0.300709    Objective Loss 0.300709                                        LR 0.000013    Time 0.070785    
2023-01-06 16:20:14,623 - Epoch: [117][   20/  246]    Overall Loss 0.306729    Objective Loss 0.306729                                        LR 0.000013    Time 0.045256    
2023-01-06 16:20:14,823 - Epoch: [117][   30/  246]    Overall Loss 0.312657    Objective Loss 0.312657                                        LR 0.000013    Time 0.036819    
2023-01-06 16:20:15,025 - Epoch: [117][   40/  246]    Overall Loss 0.315081    Objective Loss 0.315081                                        LR 0.000013    Time 0.032657    
2023-01-06 16:20:15,223 - Epoch: [117][   50/  246]    Overall Loss 0.319092    Objective Loss 0.319092                                        LR 0.000013    Time 0.030076    
2023-01-06 16:20:15,420 - Epoch: [117][   60/  246]    Overall Loss 0.318425    Objective Loss 0.318425                                        LR 0.000013    Time 0.028340    
2023-01-06 16:20:15,617 - Epoch: [117][   70/  246]    Overall Loss 0.318505    Objective Loss 0.318505                                        LR 0.000013    Time 0.027098    
2023-01-06 16:20:15,803 - Epoch: [117][   80/  246]    Overall Loss 0.317752    Objective Loss 0.317752                                        LR 0.000013    Time 0.026030    
2023-01-06 16:20:15,987 - Epoch: [117][   90/  246]    Overall Loss 0.317305    Objective Loss 0.317305                                        LR 0.000013    Time 0.025178    
2023-01-06 16:20:16,179 - Epoch: [117][  100/  246]    Overall Loss 0.318222    Objective Loss 0.318222                                        LR 0.000013    Time 0.024575    
2023-01-06 16:20:16,373 - Epoch: [117][  110/  246]    Overall Loss 0.316629    Objective Loss 0.316629                                        LR 0.000013    Time 0.024099    
2023-01-06 16:20:16,567 - Epoch: [117][  120/  246]    Overall Loss 0.316748    Objective Loss 0.316748                                        LR 0.000013    Time 0.023710    
2023-01-06 16:20:16,762 - Epoch: [117][  130/  246]    Overall Loss 0.316562    Objective Loss 0.316562                                        LR 0.000013    Time 0.023382    
2023-01-06 16:20:16,955 - Epoch: [117][  140/  246]    Overall Loss 0.316107    Objective Loss 0.316107                                        LR 0.000013    Time 0.023082    
2023-01-06 16:20:17,141 - Epoch: [117][  150/  246]    Overall Loss 0.316607    Objective Loss 0.316607                                        LR 0.000013    Time 0.022782    
2023-01-06 16:20:17,297 - Epoch: [117][  160/  246]    Overall Loss 0.316005    Objective Loss 0.316005                                        LR 0.000013    Time 0.022333    
2023-01-06 16:20:17,449 - Epoch: [117][  170/  246]    Overall Loss 0.316382    Objective Loss 0.316382                                        LR 0.000013    Time 0.021913    
2023-01-06 16:20:17,612 - Epoch: [117][  180/  246]    Overall Loss 0.316284    Objective Loss 0.316284                                        LR 0.000013    Time 0.021599    
2023-01-06 16:20:17,785 - Epoch: [117][  190/  246]    Overall Loss 0.315664    Objective Loss 0.315664                                        LR 0.000013    Time 0.021369    
2023-01-06 16:20:17,952 - Epoch: [117][  200/  246]    Overall Loss 0.316246    Objective Loss 0.316246                                        LR 0.000013    Time 0.021132    
2023-01-06 16:20:18,124 - Epoch: [117][  210/  246]    Overall Loss 0.316231    Objective Loss 0.316231                                        LR 0.000013    Time 0.020946    
2023-01-06 16:20:18,292 - Epoch: [117][  220/  246]    Overall Loss 0.316561    Objective Loss 0.316561                                        LR 0.000013    Time 0.020757    
2023-01-06 16:20:18,465 - Epoch: [117][  230/  246]    Overall Loss 0.316489    Objective Loss 0.316489                                        LR 0.000013    Time 0.020606    
2023-01-06 16:20:18,663 - Epoch: [117][  240/  246]    Overall Loss 0.316485    Objective Loss 0.316485                                        LR 0.000013    Time 0.020569    
2023-01-06 16:20:18,744 - Epoch: [117][  246/  246]    Overall Loss 0.315463    Objective Loss 0.315463    Top1 88.755981    LR 0.000013    Time 0.020394    
2023-01-06 16:20:18,884 - --- validate (epoch=117)-----------
2023-01-06 16:20:18,884 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:19,307 - Epoch: [117][   10/   28]    Loss 0.320960    Top1 87.929688    
2023-01-06 16:20:19,409 - Epoch: [117][   20/   28]    Loss 0.319997    Top1 88.085938    
2023-01-06 16:20:19,468 - Epoch: [117][   28/   28]    Loss 0.315842    Top1 88.119095    
2023-01-06 16:20:19,602 - ==> Top1: 88.119    Loss: 0.316

2023-01-06 16:20:19,602 - ==> Confusion:
[[ 162    5  272]
 [   7  154  441]
 [  58   47 5840]]

2023-01-06 16:20:19,603 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:19,603 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:19,608 - 

2023-01-06 16:20:19,608 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:20,135 - Epoch: [118][   10/  246]    Overall Loss 0.325553    Objective Loss 0.325553                                        LR 0.000013    Time 0.052657    
2023-01-06 16:20:20,286 - Epoch: [118][   20/  246]    Overall Loss 0.317566    Objective Loss 0.317566                                        LR 0.000013    Time 0.033822    
2023-01-06 16:20:20,446 - Epoch: [118][   30/  246]    Overall Loss 0.320516    Objective Loss 0.320516                                        LR 0.000013    Time 0.027883    
2023-01-06 16:20:20,607 - Epoch: [118][   40/  246]    Overall Loss 0.318667    Objective Loss 0.318667                                        LR 0.000013    Time 0.024924    
2023-01-06 16:20:20,773 - Epoch: [118][   50/  246]    Overall Loss 0.318638    Objective Loss 0.318638                                        LR 0.000013    Time 0.023246    
2023-01-06 16:20:20,938 - Epoch: [118][   60/  246]    Overall Loss 0.319851    Objective Loss 0.319851                                        LR 0.000013    Time 0.022124    
2023-01-06 16:20:21,098 - Epoch: [118][   70/  246]    Overall Loss 0.316402    Objective Loss 0.316402                                        LR 0.000013    Time 0.021248    
2023-01-06 16:20:21,252 - Epoch: [118][   80/  246]    Overall Loss 0.318249    Objective Loss 0.318249                                        LR 0.000013    Time 0.020505    
2023-01-06 16:20:21,406 - Epoch: [118][   90/  246]    Overall Loss 0.317635    Objective Loss 0.317635                                        LR 0.000013    Time 0.019938    
2023-01-06 16:20:21,567 - Epoch: [118][  100/  246]    Overall Loss 0.317442    Objective Loss 0.317442                                        LR 0.000013    Time 0.019547    
2023-01-06 16:20:21,721 - Epoch: [118][  110/  246]    Overall Loss 0.316744    Objective Loss 0.316744                                        LR 0.000013    Time 0.019173    
2023-01-06 16:20:21,897 - Epoch: [118][  120/  246]    Overall Loss 0.316782    Objective Loss 0.316782                                        LR 0.000013    Time 0.019031    
2023-01-06 16:20:22,067 - Epoch: [118][  130/  246]    Overall Loss 0.315181    Objective Loss 0.315181                                        LR 0.000013    Time 0.018879    
2023-01-06 16:20:22,242 - Epoch: [118][  140/  246]    Overall Loss 0.314803    Objective Loss 0.314803                                        LR 0.000013    Time 0.018773    
2023-01-06 16:20:22,413 - Epoch: [118][  150/  246]    Overall Loss 0.314400    Objective Loss 0.314400                                        LR 0.000013    Time 0.018657    
2023-01-06 16:20:22,587 - Epoch: [118][  160/  246]    Overall Loss 0.315414    Objective Loss 0.315414                                        LR 0.000013    Time 0.018578    
2023-01-06 16:20:22,765 - Epoch: [118][  170/  246]    Overall Loss 0.315828    Objective Loss 0.315828                                        LR 0.000013    Time 0.018530    
2023-01-06 16:20:22,935 - Epoch: [118][  180/  246]    Overall Loss 0.315980    Objective Loss 0.315980                                        LR 0.000013    Time 0.018445    
2023-01-06 16:20:23,110 - Epoch: [118][  190/  246]    Overall Loss 0.315837    Objective Loss 0.315837                                        LR 0.000013    Time 0.018392    
2023-01-06 16:20:23,292 - Epoch: [118][  200/  246]    Overall Loss 0.316214    Objective Loss 0.316214                                        LR 0.000013    Time 0.018382    
2023-01-06 16:20:23,464 - Epoch: [118][  210/  246]    Overall Loss 0.316065    Objective Loss 0.316065                                        LR 0.000013    Time 0.018323    
2023-01-06 16:20:23,639 - Epoch: [118][  220/  246]    Overall Loss 0.316343    Objective Loss 0.316343                                        LR 0.000013    Time 0.018286    
2023-01-06 16:20:23,811 - Epoch: [118][  230/  246]    Overall Loss 0.315986    Objective Loss 0.315986                                        LR 0.000013    Time 0.018234    
2023-01-06 16:20:23,995 - Epoch: [118][  240/  246]    Overall Loss 0.315724    Objective Loss 0.315724                                        LR 0.000013    Time 0.018240    
2023-01-06 16:20:24,075 - Epoch: [118][  246/  246]    Overall Loss 0.315669    Objective Loss 0.315669    Top1 87.320574    LR 0.000013    Time 0.018122    
2023-01-06 16:20:24,214 - --- validate (epoch=118)-----------
2023-01-06 16:20:24,215 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:24,646 - Epoch: [118][   10/   28]    Loss 0.317233    Top1 88.125000    
2023-01-06 16:20:24,771 - Epoch: [118][   20/   28]    Loss 0.320499    Top1 88.007812    
2023-01-06 16:20:24,829 - Epoch: [118][   28/   28]    Loss 0.312323    Top1 88.319496    
2023-01-06 16:20:24,988 - ==> Top1: 88.319    Loss: 0.312

2023-01-06 16:20:24,988 - ==> Confusion:
[[ 171    6  262]
 [   7  174  421]
 [  59   61 5825]]

2023-01-06 16:20:24,990 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:24,990 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:24,996 - 

2023-01-06 16:20:24,996 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:25,689 - Epoch: [119][   10/  246]    Overall Loss 0.304643    Objective Loss 0.304643                                        LR 0.000013    Time 0.069147    
2023-01-06 16:20:25,837 - Epoch: [119][   20/  246]    Overall Loss 0.303496    Objective Loss 0.303496                                        LR 0.000013    Time 0.041949    
2023-01-06 16:20:25,988 - Epoch: [119][   30/  246]    Overall Loss 0.308234    Objective Loss 0.308234                                        LR 0.000013    Time 0.033000    
2023-01-06 16:20:26,137 - Epoch: [119][   40/  246]    Overall Loss 0.310094    Objective Loss 0.310094                                        LR 0.000013    Time 0.028421    
2023-01-06 16:20:26,289 - Epoch: [119][   50/  246]    Overall Loss 0.313163    Objective Loss 0.313163                                        LR 0.000013    Time 0.025737    
2023-01-06 16:20:26,439 - Epoch: [119][   60/  246]    Overall Loss 0.313532    Objective Loss 0.313532                                        LR 0.000013    Time 0.023906    
2023-01-06 16:20:26,590 - Epoch: [119][   70/  246]    Overall Loss 0.311489    Objective Loss 0.311489                                        LR 0.000013    Time 0.022636    
2023-01-06 16:20:26,746 - Epoch: [119][   80/  246]    Overall Loss 0.315316    Objective Loss 0.315316                                        LR 0.000013    Time 0.021750    
2023-01-06 16:20:26,915 - Epoch: [119][   90/  246]    Overall Loss 0.315158    Objective Loss 0.315158                                        LR 0.000013    Time 0.021201    
2023-01-06 16:20:27,074 - Epoch: [119][  100/  246]    Overall Loss 0.314813    Objective Loss 0.314813                                        LR 0.000013    Time 0.020667    
2023-01-06 16:20:27,219 - Epoch: [119][  110/  246]    Overall Loss 0.314408    Objective Loss 0.314408                                        LR 0.000013    Time 0.020096    
2023-01-06 16:20:27,362 - Epoch: [119][  120/  246]    Overall Loss 0.312848    Objective Loss 0.312848                                        LR 0.000013    Time 0.019610    
2023-01-06 16:20:27,499 - Epoch: [119][  130/  246]    Overall Loss 0.313185    Objective Loss 0.313185                                        LR 0.000013    Time 0.019149    
2023-01-06 16:20:27,644 - Epoch: [119][  140/  246]    Overall Loss 0.312688    Objective Loss 0.312688                                        LR 0.000013    Time 0.018812    
2023-01-06 16:20:27,788 - Epoch: [119][  150/  246]    Overall Loss 0.313712    Objective Loss 0.313712                                        LR 0.000013    Time 0.018516    
2023-01-06 16:20:27,931 - Epoch: [119][  160/  246]    Overall Loss 0.313513    Objective Loss 0.313513                                        LR 0.000013    Time 0.018251    
2023-01-06 16:20:28,085 - Epoch: [119][  170/  246]    Overall Loss 0.313827    Objective Loss 0.313827                                        LR 0.000013    Time 0.018080    
2023-01-06 16:20:28,257 - Epoch: [119][  180/  246]    Overall Loss 0.312686    Objective Loss 0.312686                                        LR 0.000013    Time 0.018032    
2023-01-06 16:20:28,424 - Epoch: [119][  190/  246]    Overall Loss 0.313535    Objective Loss 0.313535                                        LR 0.000013    Time 0.017957    
2023-01-06 16:20:28,572 - Epoch: [119][  200/  246]    Overall Loss 0.313347    Objective Loss 0.313347                                        LR 0.000013    Time 0.017796    
2023-01-06 16:20:28,716 - Epoch: [119][  210/  246]    Overall Loss 0.314498    Objective Loss 0.314498                                        LR 0.000013    Time 0.017635    
2023-01-06 16:20:28,883 - Epoch: [119][  220/  246]    Overall Loss 0.315150    Objective Loss 0.315150                                        LR 0.000013    Time 0.017589    
2023-01-06 16:20:29,057 - Epoch: [119][  230/  246]    Overall Loss 0.315175    Objective Loss 0.315175                                        LR 0.000013    Time 0.017580    
2023-01-06 16:20:29,242 - Epoch: [119][  240/  246]    Overall Loss 0.315592    Objective Loss 0.315592                                        LR 0.000013    Time 0.017616    
2023-01-06 16:20:29,323 - Epoch: [119][  246/  246]    Overall Loss 0.315329    Objective Loss 0.315329    Top1 89.234450    LR 0.000013    Time 0.017514    
2023-01-06 16:20:29,461 - --- validate (epoch=119)-----------
2023-01-06 16:20:29,461 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:29,892 - Epoch: [119][   10/   28]    Loss 0.309829    Top1 88.671875    
2023-01-06 16:20:29,997 - Epoch: [119][   20/   28]    Loss 0.305002    Top1 88.437500    
2023-01-06 16:20:30,058 - Epoch: [119][   28/   28]    Loss 0.317220    Top1 88.233610    
2023-01-06 16:20:30,234 - ==> Top1: 88.234    Loss: 0.317

2023-01-06 16:20:30,234 - ==> Confusion:
[[ 164    6  269]
 [   7  182  413]
 [  57   70 5818]]

2023-01-06 16:20:30,236 - ==> Best [Top1: 88.405   Sparsity:0.00   Params: 46192 on epoch: 98]
2023-01-06 16:20:30,236 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:30,240 - 

2023-01-06 16:20:30,240 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:30,929 - Epoch: [120][   10/  246]    Overall Loss 0.309599    Objective Loss 0.309599                                        LR 0.000013    Time 0.068813    
2023-01-06 16:20:31,092 - Epoch: [120][   20/  246]    Overall Loss 0.319795    Objective Loss 0.319795                                        LR 0.000013    Time 0.042510    
2023-01-06 16:20:31,264 - Epoch: [120][   30/  246]    Overall Loss 0.318921    Objective Loss 0.318921                                        LR 0.000013    Time 0.034053    
2023-01-06 16:20:31,436 - Epoch: [120][   40/  246]    Overall Loss 0.314256    Objective Loss 0.314256                                        LR 0.000013    Time 0.029853    
2023-01-06 16:20:31,599 - Epoch: [120][   50/  246]    Overall Loss 0.318249    Objective Loss 0.318249                                        LR 0.000013    Time 0.027130    
2023-01-06 16:20:31,766 - Epoch: [120][   60/  246]    Overall Loss 0.318897    Objective Loss 0.318897                                        LR 0.000013    Time 0.025381    
2023-01-06 16:20:31,943 - Epoch: [120][   70/  246]    Overall Loss 0.316421    Objective Loss 0.316421                                        LR 0.000013    Time 0.024268    
2023-01-06 16:20:32,112 - Epoch: [120][   80/  246]    Overall Loss 0.317499    Objective Loss 0.317499                                        LR 0.000013    Time 0.023350    
2023-01-06 16:20:32,289 - Epoch: [120][   90/  246]    Overall Loss 0.317463    Objective Loss 0.317463                                        LR 0.000013    Time 0.022714    
2023-01-06 16:20:32,461 - Epoch: [120][  100/  246]    Overall Loss 0.316765    Objective Loss 0.316765                                        LR 0.000013    Time 0.022146    
2023-01-06 16:20:32,635 - Epoch: [120][  110/  246]    Overall Loss 0.318072    Objective Loss 0.318072                                        LR 0.000013    Time 0.021708    
2023-01-06 16:20:32,806 - Epoch: [120][  120/  246]    Overall Loss 0.316892    Objective Loss 0.316892                                        LR 0.000013    Time 0.021308    
2023-01-06 16:20:32,983 - Epoch: [120][  130/  246]    Overall Loss 0.315733    Objective Loss 0.315733                                        LR 0.000013    Time 0.021024    
2023-01-06 16:20:33,154 - Epoch: [120][  140/  246]    Overall Loss 0.314876    Objective Loss 0.314876                                        LR 0.000013    Time 0.020744    
2023-01-06 16:20:33,336 - Epoch: [120][  150/  246]    Overall Loss 0.315253    Objective Loss 0.315253                                        LR 0.000013    Time 0.020568    
2023-01-06 16:20:33,505 - Epoch: [120][  160/  246]    Overall Loss 0.314862    Objective Loss 0.314862                                        LR 0.000013    Time 0.020328    
2023-01-06 16:20:33,662 - Epoch: [120][  170/  246]    Overall Loss 0.315718    Objective Loss 0.315718                                        LR 0.000013    Time 0.020052    
2023-01-06 16:20:33,808 - Epoch: [120][  180/  246]    Overall Loss 0.316317    Objective Loss 0.316317                                        LR 0.000013    Time 0.019747    
2023-01-06 16:20:33,953 - Epoch: [120][  190/  246]    Overall Loss 0.315603    Objective Loss 0.315603                                        LR 0.000013    Time 0.019468    
2023-01-06 16:20:34,096 - Epoch: [120][  200/  246]    Overall Loss 0.315769    Objective Loss 0.315769                                        LR 0.000013    Time 0.019207    
2023-01-06 16:20:34,231 - Epoch: [120][  210/  246]    Overall Loss 0.315964    Objective Loss 0.315964                                        LR 0.000013    Time 0.018933    
2023-01-06 16:20:34,364 - Epoch: [120][  220/  246]    Overall Loss 0.316482    Objective Loss 0.316482                                        LR 0.000013    Time 0.018679    
2023-01-06 16:20:34,498 - Epoch: [120][  230/  246]    Overall Loss 0.316016    Objective Loss 0.316016                                        LR 0.000013    Time 0.018449    
2023-01-06 16:20:34,649 - Epoch: [120][  240/  246]    Overall Loss 0.315272    Objective Loss 0.315272                                        LR 0.000013    Time 0.018307    
2023-01-06 16:20:34,721 - Epoch: [120][  246/  246]    Overall Loss 0.315069    Objective Loss 0.315069    Top1 90.191388    LR 0.000013    Time 0.018152    
2023-01-06 16:20:34,848 - --- validate (epoch=120)-----------
2023-01-06 16:20:34,848 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:35,276 - Epoch: [120][   10/   28]    Loss 0.311013    Top1 88.867188    
2023-01-06 16:20:35,380 - Epoch: [120][   20/   28]    Loss 0.317592    Top1 88.554688    
2023-01-06 16:20:35,438 - Epoch: [120][   28/   28]    Loss 0.314025    Top1 88.491268    
2023-01-06 16:20:35,583 - ==> Top1: 88.491    Loss: 0.314

2023-01-06 16:20:35,583 - ==> Confusion:
[[ 187    6  246]
 [  12  188  402]
 [  70   68 5807]]

2023-01-06 16:20:35,584 - ==> Best [Top1: 88.491   Sparsity:0.00   Params: 46192 on epoch: 120]
2023-01-06 16:20:35,584 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:35,589 - 

2023-01-06 16:20:35,589 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:36,124 - Epoch: [121][   10/  246]    Overall Loss 0.298099    Objective Loss 0.298099                                        LR 0.000013    Time 0.053374    
2023-01-06 16:20:36,269 - Epoch: [121][   20/  246]    Overall Loss 0.312032    Objective Loss 0.312032                                        LR 0.000013    Time 0.033935    
2023-01-06 16:20:36,418 - Epoch: [121][   30/  246]    Overall Loss 0.319811    Objective Loss 0.319811                                        LR 0.000013    Time 0.027591    
2023-01-06 16:20:36,563 - Epoch: [121][   40/  246]    Overall Loss 0.311867    Objective Loss 0.311867                                        LR 0.000013    Time 0.024298    
2023-01-06 16:20:36,712 - Epoch: [121][   50/  246]    Overall Loss 0.314034    Objective Loss 0.314034                                        LR 0.000013    Time 0.022413    
2023-01-06 16:20:36,863 - Epoch: [121][   60/  246]    Overall Loss 0.314841    Objective Loss 0.314841                                        LR 0.000013    Time 0.021189    
2023-01-06 16:20:37,036 - Epoch: [121][   70/  246]    Overall Loss 0.313257    Objective Loss 0.313257                                        LR 0.000013    Time 0.020621    
2023-01-06 16:20:37,180 - Epoch: [121][   80/  246]    Overall Loss 0.312570    Objective Loss 0.312570                                        LR 0.000013    Time 0.019840    
2023-01-06 16:20:37,363 - Epoch: [121][   90/  246]    Overall Loss 0.313129    Objective Loss 0.313129                                        LR 0.000013    Time 0.019671    
2023-01-06 16:20:37,547 - Epoch: [121][  100/  246]    Overall Loss 0.313816    Objective Loss 0.313816                                        LR 0.000013    Time 0.019534    
2023-01-06 16:20:37,728 - Epoch: [121][  110/  246]    Overall Loss 0.314247    Objective Loss 0.314247                                        LR 0.000013    Time 0.019398    
2023-01-06 16:20:37,912 - Epoch: [121][  120/  246]    Overall Loss 0.314091    Objective Loss 0.314091                                        LR 0.000013    Time 0.019316    
2023-01-06 16:20:38,094 - Epoch: [121][  130/  246]    Overall Loss 0.315723    Objective Loss 0.315723                                        LR 0.000013    Time 0.019228    
2023-01-06 16:20:38,268 - Epoch: [121][  140/  246]    Overall Loss 0.315633    Objective Loss 0.315633                                        LR 0.000013    Time 0.019088    
2023-01-06 16:20:38,456 - Epoch: [121][  150/  246]    Overall Loss 0.316923    Objective Loss 0.316923                                        LR 0.000013    Time 0.019049    
2023-01-06 16:20:38,644 - Epoch: [121][  160/  246]    Overall Loss 0.316409    Objective Loss 0.316409                                        LR 0.000013    Time 0.019020    
2023-01-06 16:20:38,829 - Epoch: [121][  170/  246]    Overall Loss 0.315646    Objective Loss 0.315646                                        LR 0.000013    Time 0.018977    
2023-01-06 16:20:39,003 - Epoch: [121][  180/  246]    Overall Loss 0.315588    Objective Loss 0.315588                                        LR 0.000013    Time 0.018885    
2023-01-06 16:20:39,188 - Epoch: [121][  190/  246]    Overall Loss 0.314964    Objective Loss 0.314964                                        LR 0.000013    Time 0.018861    
2023-01-06 16:20:39,372 - Epoch: [121][  200/  246]    Overall Loss 0.315577    Objective Loss 0.315577                                        LR 0.000013    Time 0.018825    
2023-01-06 16:20:39,548 - Epoch: [121][  210/  246]    Overall Loss 0.314905    Objective Loss 0.314905                                        LR 0.000013    Time 0.018764    
2023-01-06 16:20:39,708 - Epoch: [121][  220/  246]    Overall Loss 0.315752    Objective Loss 0.315752                                        LR 0.000013    Time 0.018640    
2023-01-06 16:20:39,883 - Epoch: [121][  230/  246]    Overall Loss 0.314883    Objective Loss 0.314883                                        LR 0.000013    Time 0.018587    
2023-01-06 16:20:40,056 - Epoch: [121][  240/  246]    Overall Loss 0.314900    Objective Loss 0.314900                                        LR 0.000013    Time 0.018533    
2023-01-06 16:20:40,135 - Epoch: [121][  246/  246]    Overall Loss 0.314981    Objective Loss 0.314981    Top1 88.038278    LR 0.000013    Time 0.018399    
2023-01-06 16:20:40,274 - --- validate (epoch=121)-----------
2023-01-06 16:20:40,274 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:40,730 - Epoch: [121][   10/   28]    Loss 0.297967    Top1 88.476562    
2023-01-06 16:20:40,846 - Epoch: [121][   20/   28]    Loss 0.315395    Top1 88.066406    
2023-01-06 16:20:40,903 - Epoch: [121][   28/   28]    Loss 0.316235    Top1 88.147724    
2023-01-06 16:20:41,052 - ==> Top1: 88.148    Loss: 0.316

2023-01-06 16:20:41,052 - ==> Confusion:
[[ 174    6  259]
 [   9  157  436]
 [  63   55 5827]]

2023-01-06 16:20:41,053 - ==> Best [Top1: 88.491   Sparsity:0.00   Params: 46192 on epoch: 120]
2023-01-06 16:20:41,054 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:41,058 - 

2023-01-06 16:20:41,058 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:41,728 - Epoch: [122][   10/  246]    Overall Loss 0.334811    Objective Loss 0.334811                                        LR 0.000013    Time 0.066930    
2023-01-06 16:20:41,888 - Epoch: [122][   20/  246]    Overall Loss 0.333309    Objective Loss 0.333309                                        LR 0.000013    Time 0.041425    
2023-01-06 16:20:42,051 - Epoch: [122][   30/  246]    Overall Loss 0.326961    Objective Loss 0.326961                                        LR 0.000013    Time 0.033055    
2023-01-06 16:20:42,231 - Epoch: [122][   40/  246]    Overall Loss 0.321256    Objective Loss 0.321256                                        LR 0.000013    Time 0.029279    
2023-01-06 16:20:42,403 - Epoch: [122][   50/  246]    Overall Loss 0.313704    Objective Loss 0.313704                                        LR 0.000013    Time 0.026859    
2023-01-06 16:20:42,581 - Epoch: [122][   60/  246]    Overall Loss 0.308404    Objective Loss 0.308404                                        LR 0.000013    Time 0.025341    
2023-01-06 16:20:42,757 - Epoch: [122][   70/  246]    Overall Loss 0.306135    Objective Loss 0.306135                                        LR 0.000013    Time 0.024224    
2023-01-06 16:20:42,936 - Epoch: [122][   80/  246]    Overall Loss 0.307873    Objective Loss 0.307873                                        LR 0.000013    Time 0.023428    
2023-01-06 16:20:43,103 - Epoch: [122][   90/  246]    Overall Loss 0.307913    Objective Loss 0.307913                                        LR 0.000013    Time 0.022683    
2023-01-06 16:20:43,270 - Epoch: [122][  100/  246]    Overall Loss 0.309584    Objective Loss 0.309584                                        LR 0.000013    Time 0.022082    
2023-01-06 16:20:43,440 - Epoch: [122][  110/  246]    Overall Loss 0.311152    Objective Loss 0.311152                                        LR 0.000013    Time 0.021613    
2023-01-06 16:20:43,608 - Epoch: [122][  120/  246]    Overall Loss 0.311408    Objective Loss 0.311408                                        LR 0.000013    Time 0.021214    
2023-01-06 16:20:43,778 - Epoch: [122][  130/  246]    Overall Loss 0.310885    Objective Loss 0.310885                                        LR 0.000013    Time 0.020884    
2023-01-06 16:20:43,950 - Epoch: [122][  140/  246]    Overall Loss 0.309363    Objective Loss 0.309363                                        LR 0.000013    Time 0.020617    
2023-01-06 16:20:44,125 - Epoch: [122][  150/  246]    Overall Loss 0.308985    Objective Loss 0.308985                                        LR 0.000013    Time 0.020407    
2023-01-06 16:20:44,292 - Epoch: [122][  160/  246]    Overall Loss 0.308505    Objective Loss 0.308505                                        LR 0.000013    Time 0.020178    
2023-01-06 16:20:44,461 - Epoch: [122][  170/  246]    Overall Loss 0.309238    Objective Loss 0.309238                                        LR 0.000013    Time 0.019980    
2023-01-06 16:20:44,629 - Epoch: [122][  180/  246]    Overall Loss 0.310325    Objective Loss 0.310325                                        LR 0.000013    Time 0.019804    
2023-01-06 16:20:44,802 - Epoch: [122][  190/  246]    Overall Loss 0.313206    Objective Loss 0.313206                                        LR 0.000013    Time 0.019672    
2023-01-06 16:20:44,977 - Epoch: [122][  200/  246]    Overall Loss 0.313096    Objective Loss 0.313096                                        LR 0.000013    Time 0.019559    
2023-01-06 16:20:45,151 - Epoch: [122][  210/  246]    Overall Loss 0.312886    Objective Loss 0.312886                                        LR 0.000013    Time 0.019457    
2023-01-06 16:20:45,326 - Epoch: [122][  220/  246]    Overall Loss 0.313454    Objective Loss 0.313454                                        LR 0.000013    Time 0.019364    
2023-01-06 16:20:45,501 - Epoch: [122][  230/  246]    Overall Loss 0.313974    Objective Loss 0.313974                                        LR 0.000013    Time 0.019281    
2023-01-06 16:20:45,684 - Epoch: [122][  240/  246]    Overall Loss 0.313987    Objective Loss 0.313987                                        LR 0.000013    Time 0.019242    
2023-01-06 16:20:45,767 - Epoch: [122][  246/  246]    Overall Loss 0.314832    Objective Loss 0.314832    Top1 88.038278    LR 0.000013    Time 0.019109    
2023-01-06 16:20:45,901 - --- validate (epoch=122)-----------
2023-01-06 16:20:45,901 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:46,333 - Epoch: [122][   10/   28]    Loss 0.306963    Top1 89.492188    
2023-01-06 16:20:46,438 - Epoch: [122][   20/   28]    Loss 0.307988    Top1 88.886719    
2023-01-06 16:20:46,498 - Epoch: [122][   28/   28]    Loss 0.314355    Top1 88.677355    
2023-01-06 16:20:46,631 - ==> Top1: 88.677    Loss: 0.314

2023-01-06 16:20:46,632 - ==> Confusion:
[[ 189    7  243]
 [  11  192  399]
 [  62   69 5814]]

2023-01-06 16:20:46,633 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:20:46,633 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:46,638 - 

2023-01-06 16:20:46,638 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:47,172 - Epoch: [123][   10/  246]    Overall Loss 0.321123    Objective Loss 0.321123                                        LR 0.000013    Time 0.053287    
2023-01-06 16:20:47,332 - Epoch: [123][   20/  246]    Overall Loss 0.315192    Objective Loss 0.315192                                        LR 0.000013    Time 0.034576    
2023-01-06 16:20:47,492 - Epoch: [123][   30/  246]    Overall Loss 0.309287    Objective Loss 0.309287                                        LR 0.000013    Time 0.028374    
2023-01-06 16:20:47,651 - Epoch: [123][   40/  246]    Overall Loss 0.311342    Objective Loss 0.311342                                        LR 0.000013    Time 0.025224    
2023-01-06 16:20:47,814 - Epoch: [123][   50/  246]    Overall Loss 0.310696    Objective Loss 0.310696                                        LR 0.000013    Time 0.023443    
2023-01-06 16:20:47,974 - Epoch: [123][   60/  246]    Overall Loss 0.308790    Objective Loss 0.308790                                        LR 0.000013    Time 0.022177    
2023-01-06 16:20:48,140 - Epoch: [123][   70/  246]    Overall Loss 0.311611    Objective Loss 0.311611                                        LR 0.000013    Time 0.021369    
2023-01-06 16:20:48,304 - Epoch: [123][   80/  246]    Overall Loss 0.313105    Objective Loss 0.313105                                        LR 0.000013    Time 0.020719    
2023-01-06 16:20:48,469 - Epoch: [123][   90/  246]    Overall Loss 0.314170    Objective Loss 0.314170                                        LR 0.000013    Time 0.020255    
2023-01-06 16:20:48,634 - Epoch: [123][  100/  246]    Overall Loss 0.312536    Objective Loss 0.312536                                        LR 0.000013    Time 0.019866    
2023-01-06 16:20:48,807 - Epoch: [123][  110/  246]    Overall Loss 0.311603    Objective Loss 0.311603                                        LR 0.000013    Time 0.019635    
2023-01-06 16:20:48,972 - Epoch: [123][  120/  246]    Overall Loss 0.312053    Objective Loss 0.312053                                        LR 0.000013    Time 0.019372    
2023-01-06 16:20:49,143 - Epoch: [123][  130/  246]    Overall Loss 0.310849    Objective Loss 0.310849                                        LR 0.000013    Time 0.019192    
2023-01-06 16:20:49,312 - Epoch: [123][  140/  246]    Overall Loss 0.311747    Objective Loss 0.311747                                        LR 0.000013    Time 0.019026    
2023-01-06 16:20:49,481 - Epoch: [123][  150/  246]    Overall Loss 0.311958    Objective Loss 0.311958                                        LR 0.000013    Time 0.018884    
2023-01-06 16:20:49,650 - Epoch: [123][  160/  246]    Overall Loss 0.312515    Objective Loss 0.312515                                        LR 0.000013    Time 0.018755    
2023-01-06 16:20:49,820 - Epoch: [123][  170/  246]    Overall Loss 0.313041    Objective Loss 0.313041                                        LR 0.000013    Time 0.018647    
2023-01-06 16:20:49,988 - Epoch: [123][  180/  246]    Overall Loss 0.312167    Objective Loss 0.312167                                        LR 0.000013    Time 0.018548    
2023-01-06 16:20:50,158 - Epoch: [123][  190/  246]    Overall Loss 0.312398    Objective Loss 0.312398                                        LR 0.000013    Time 0.018460    
2023-01-06 16:20:50,325 - Epoch: [123][  200/  246]    Overall Loss 0.312305    Objective Loss 0.312305                                        LR 0.000013    Time 0.018375    
2023-01-06 16:20:50,494 - Epoch: [123][  210/  246]    Overall Loss 0.313059    Objective Loss 0.313059                                        LR 0.000013    Time 0.018301    
2023-01-06 16:20:50,664 - Epoch: [123][  220/  246]    Overall Loss 0.313171    Objective Loss 0.313171                                        LR 0.000013    Time 0.018240    
2023-01-06 16:20:50,832 - Epoch: [123][  230/  246]    Overall Loss 0.313093    Objective Loss 0.313093                                        LR 0.000013    Time 0.018178    
2023-01-06 16:20:51,012 - Epoch: [123][  240/  246]    Overall Loss 0.313706    Objective Loss 0.313706                                        LR 0.000013    Time 0.018168    
2023-01-06 16:20:51,095 - Epoch: [123][  246/  246]    Overall Loss 0.313660    Objective Loss 0.313660    Top1 89.712919    LR 0.000013    Time 0.018059    
2023-01-06 16:20:51,214 - --- validate (epoch=123)-----------
2023-01-06 16:20:51,215 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:51,651 - Epoch: [123][   10/   28]    Loss 0.318765    Top1 87.617188    
2023-01-06 16:20:51,754 - Epoch: [123][   20/   28]    Loss 0.322825    Top1 87.968750    
2023-01-06 16:20:51,814 - Epoch: [123][   28/   28]    Loss 0.319939    Top1 88.119095    
2023-01-06 16:20:51,945 - ==> Top1: 88.119    Loss: 0.320

2023-01-06 16:20:51,945 - ==> Confusion:
[[ 142    6  291]
 [   7  165  430]
 [  38   58 5849]]

2023-01-06 16:20:51,946 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:20:51,946 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:51,951 - 

2023-01-06 16:20:51,951 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:52,636 - Epoch: [124][   10/  246]    Overall Loss 0.315620    Objective Loss 0.315620                                        LR 0.000013    Time 0.068470    
2023-01-06 16:20:52,798 - Epoch: [124][   20/  246]    Overall Loss 0.311940    Objective Loss 0.311940                                        LR 0.000013    Time 0.042315    
2023-01-06 16:20:52,971 - Epoch: [124][   30/  246]    Overall Loss 0.311741    Objective Loss 0.311741                                        LR 0.000013    Time 0.033953    
2023-01-06 16:20:53,147 - Epoch: [124][   40/  246]    Overall Loss 0.313709    Objective Loss 0.313709                                        LR 0.000013    Time 0.029854    
2023-01-06 16:20:53,322 - Epoch: [124][   50/  246]    Overall Loss 0.313239    Objective Loss 0.313239                                        LR 0.000013    Time 0.027353    
2023-01-06 16:20:53,500 - Epoch: [124][   60/  246]    Overall Loss 0.315952    Objective Loss 0.315952                                        LR 0.000013    Time 0.025750    
2023-01-06 16:20:53,678 - Epoch: [124][   70/  246]    Overall Loss 0.317333    Objective Loss 0.317333                                        LR 0.000013    Time 0.024610    
2023-01-06 16:20:53,861 - Epoch: [124][   80/  246]    Overall Loss 0.314749    Objective Loss 0.314749                                        LR 0.000013    Time 0.023815    
2023-01-06 16:20:54,040 - Epoch: [124][   90/  246]    Overall Loss 0.313220    Objective Loss 0.313220                                        LR 0.000013    Time 0.023147    
2023-01-06 16:20:54,216 - Epoch: [124][  100/  246]    Overall Loss 0.313038    Objective Loss 0.313038                                        LR 0.000013    Time 0.022594    
2023-01-06 16:20:54,397 - Epoch: [124][  110/  246]    Overall Loss 0.315802    Objective Loss 0.315802                                        LR 0.000013    Time 0.022182    
2023-01-06 16:20:54,579 - Epoch: [124][  120/  246]    Overall Loss 0.314068    Objective Loss 0.314068                                        LR 0.000013    Time 0.021848    
2023-01-06 16:20:54,761 - Epoch: [124][  130/  246]    Overall Loss 0.314018    Objective Loss 0.314018                                        LR 0.000013    Time 0.021558    
2023-01-06 16:20:54,940 - Epoch: [124][  140/  246]    Overall Loss 0.314109    Objective Loss 0.314109                                        LR 0.000013    Time 0.021299    
2023-01-06 16:20:55,121 - Epoch: [124][  150/  246]    Overall Loss 0.314181    Objective Loss 0.314181                                        LR 0.000013    Time 0.021084    
2023-01-06 16:20:55,311 - Epoch: [124][  160/  246]    Overall Loss 0.313832    Objective Loss 0.313832                                        LR 0.000013    Time 0.020948    
2023-01-06 16:20:55,495 - Epoch: [124][  170/  246]    Overall Loss 0.315012    Objective Loss 0.315012                                        LR 0.000013    Time 0.020795    
2023-01-06 16:20:55,684 - Epoch: [124][  180/  246]    Overall Loss 0.314849    Objective Loss 0.314849                                        LR 0.000013    Time 0.020687    
2023-01-06 16:20:55,861 - Epoch: [124][  190/  246]    Overall Loss 0.315066    Objective Loss 0.315066                                        LR 0.000013    Time 0.020530    
2023-01-06 16:20:56,040 - Epoch: [124][  200/  246]    Overall Loss 0.315572    Objective Loss 0.315572                                        LR 0.000013    Time 0.020394    
2023-01-06 16:20:56,218 - Epoch: [124][  210/  246]    Overall Loss 0.314897    Objective Loss 0.314897                                        LR 0.000013    Time 0.020271    
2023-01-06 16:20:56,399 - Epoch: [124][  220/  246]    Overall Loss 0.314825    Objective Loss 0.314825                                        LR 0.000013    Time 0.020173    
2023-01-06 16:20:56,578 - Epoch: [124][  230/  246]    Overall Loss 0.314960    Objective Loss 0.314960                                        LR 0.000013    Time 0.020073    
2023-01-06 16:20:56,767 - Epoch: [124][  240/  246]    Overall Loss 0.314402    Objective Loss 0.314402                                        LR 0.000013    Time 0.020020    
2023-01-06 16:20:56,847 - Epoch: [124][  246/  246]    Overall Loss 0.313903    Objective Loss 0.313903    Top1 88.516746    LR 0.000013    Time 0.019856    
2023-01-06 16:20:56,992 - --- validate (epoch=124)-----------
2023-01-06 16:20:56,992 - 6986 samples (256 per mini-batch)
2023-01-06 16:20:57,420 - Epoch: [124][   10/   28]    Loss 0.310979    Top1 88.125000    
2023-01-06 16:20:57,523 - Epoch: [124][   20/   28]    Loss 0.310743    Top1 88.378906    
2023-01-06 16:20:57,583 - Epoch: [124][   28/   28]    Loss 0.312871    Top1 88.376754    
2023-01-06 16:20:57,733 - ==> Top1: 88.377    Loss: 0.313

2023-01-06 16:20:57,733 - ==> Confusion:
[[ 161    7  271]
 [   7  173  422]
 [  46   59 5840]]

2023-01-06 16:20:57,735 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:20:57,735 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:20:57,739 - 

2023-01-06 16:20:57,739 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:20:58,284 - Epoch: [125][   10/  246]    Overall Loss 0.317504    Objective Loss 0.317504                                        LR 0.000013    Time 0.054432    
2023-01-06 16:20:58,451 - Epoch: [125][   20/  246]    Overall Loss 0.322773    Objective Loss 0.322773                                        LR 0.000013    Time 0.035546    
2023-01-06 16:20:58,619 - Epoch: [125][   30/  246]    Overall Loss 0.322119    Objective Loss 0.322119                                        LR 0.000013    Time 0.029271    
2023-01-06 16:20:58,785 - Epoch: [125][   40/  246]    Overall Loss 0.322999    Objective Loss 0.322999                                        LR 0.000013    Time 0.026093    
2023-01-06 16:20:58,951 - Epoch: [125][   50/  246]    Overall Loss 0.323890    Objective Loss 0.323890                                        LR 0.000013    Time 0.024185    
2023-01-06 16:20:59,119 - Epoch: [125][   60/  246]    Overall Loss 0.323613    Objective Loss 0.323613                                        LR 0.000013    Time 0.022961    
2023-01-06 16:20:59,295 - Epoch: [125][   70/  246]    Overall Loss 0.320565    Objective Loss 0.320565                                        LR 0.000013    Time 0.022183    
2023-01-06 16:20:59,466 - Epoch: [125][   80/  246]    Overall Loss 0.323481    Objective Loss 0.323481                                        LR 0.000013    Time 0.021542    
2023-01-06 16:20:59,642 - Epoch: [125][   90/  246]    Overall Loss 0.321601    Objective Loss 0.321601                                        LR 0.000013    Time 0.021105    
2023-01-06 16:20:59,813 - Epoch: [125][  100/  246]    Overall Loss 0.321032    Objective Loss 0.321032                                        LR 0.000013    Time 0.020702    
2023-01-06 16:20:59,988 - Epoch: [125][  110/  246]    Overall Loss 0.321112    Objective Loss 0.321112                                        LR 0.000013    Time 0.020411    
2023-01-06 16:21:00,158 - Epoch: [125][  120/  246]    Overall Loss 0.320247    Objective Loss 0.320247                                        LR 0.000013    Time 0.020123    
2023-01-06 16:21:00,325 - Epoch: [125][  130/  246]    Overall Loss 0.317246    Objective Loss 0.317246                                        LR 0.000013    Time 0.019854    
2023-01-06 16:21:00,491 - Epoch: [125][  140/  246]    Overall Loss 0.318231    Objective Loss 0.318231                                        LR 0.000013    Time 0.019622    
2023-01-06 16:21:00,658 - Epoch: [125][  150/  246]    Overall Loss 0.316354    Objective Loss 0.316354                                        LR 0.000013    Time 0.019426    
2023-01-06 16:21:00,822 - Epoch: [125][  160/  246]    Overall Loss 0.315694    Objective Loss 0.315694                                        LR 0.000013    Time 0.019234    
2023-01-06 16:21:00,985 - Epoch: [125][  170/  246]    Overall Loss 0.315946    Objective Loss 0.315946                                        LR 0.000013    Time 0.019060    
2023-01-06 16:21:01,155 - Epoch: [125][  180/  246]    Overall Loss 0.315729    Objective Loss 0.315729                                        LR 0.000013    Time 0.018939    
2023-01-06 16:21:01,322 - Epoch: [125][  190/  246]    Overall Loss 0.312698    Objective Loss 0.312698                                        LR 0.000013    Time 0.018821    
2023-01-06 16:21:01,491 - Epoch: [125][  200/  246]    Overall Loss 0.312543    Objective Loss 0.312543                                        LR 0.000013    Time 0.018722    
2023-01-06 16:21:01,649 - Epoch: [125][  210/  246]    Overall Loss 0.312344    Objective Loss 0.312344                                        LR 0.000013    Time 0.018582    
2023-01-06 16:21:01,818 - Epoch: [125][  220/  246]    Overall Loss 0.312982    Objective Loss 0.312982                                        LR 0.000013    Time 0.018505    
2023-01-06 16:21:01,986 - Epoch: [125][  230/  246]    Overall Loss 0.312964    Objective Loss 0.312964                                        LR 0.000013    Time 0.018431    
2023-01-06 16:21:02,167 - Epoch: [125][  240/  246]    Overall Loss 0.312889    Objective Loss 0.312889                                        LR 0.000013    Time 0.018417    
2023-01-06 16:21:02,248 - Epoch: [125][  246/  246]    Overall Loss 0.312452    Objective Loss 0.312452    Top1 90.191388    LR 0.000013    Time 0.018294    
2023-01-06 16:21:02,393 - --- validate (epoch=125)-----------
2023-01-06 16:21:02,393 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:02,835 - Epoch: [125][   10/   28]    Loss 0.319964    Top1 88.320312    
2023-01-06 16:21:02,941 - Epoch: [125][   20/   28]    Loss 0.316175    Top1 88.457031    
2023-01-06 16:21:02,998 - Epoch: [125][   28/   28]    Loss 0.313672    Top1 88.505583    
2023-01-06 16:21:03,160 - ==> Top1: 88.506    Loss: 0.314

2023-01-06 16:21:03,161 - ==> Confusion:
[[ 167    7  265]
 [   9  193  400]
 [  49   73 5823]]

2023-01-06 16:21:03,162 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:21:03,162 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:03,167 - 

2023-01-06 16:21:03,167 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:03,833 - Epoch: [126][   10/  246]    Overall Loss 0.323621    Objective Loss 0.323621                                        LR 0.000013    Time 0.066601    
2023-01-06 16:21:03,996 - Epoch: [126][   20/  246]    Overall Loss 0.324177    Objective Loss 0.324177                                        LR 0.000013    Time 0.041396    
2023-01-06 16:21:04,156 - Epoch: [126][   30/  246]    Overall Loss 0.318653    Objective Loss 0.318653                                        LR 0.000013    Time 0.032934    
2023-01-06 16:21:04,327 - Epoch: [126][   40/  246]    Overall Loss 0.318632    Objective Loss 0.318632                                        LR 0.000013    Time 0.028956    
2023-01-06 16:21:04,486 - Epoch: [126][   50/  246]    Overall Loss 0.314626    Objective Loss 0.314626                                        LR 0.000013    Time 0.026349    
2023-01-06 16:21:04,644 - Epoch: [126][   60/  246]    Overall Loss 0.314553    Objective Loss 0.314553                                        LR 0.000013    Time 0.024576    
2023-01-06 16:21:04,801 - Epoch: [126][   70/  246]    Overall Loss 0.314769    Objective Loss 0.314769                                        LR 0.000013    Time 0.023312    
2023-01-06 16:21:04,970 - Epoch: [126][   80/  246]    Overall Loss 0.313936    Objective Loss 0.313936                                        LR 0.000013    Time 0.022498    
2023-01-06 16:21:05,127 - Epoch: [126][   90/  246]    Overall Loss 0.313353    Objective Loss 0.313353                                        LR 0.000013    Time 0.021743    
2023-01-06 16:21:05,291 - Epoch: [126][  100/  246]    Overall Loss 0.313039    Objective Loss 0.313039                                        LR 0.000013    Time 0.021205    
2023-01-06 16:21:05,457 - Epoch: [126][  110/  246]    Overall Loss 0.313781    Objective Loss 0.313781                                        LR 0.000013    Time 0.020785    
2023-01-06 16:21:05,629 - Epoch: [126][  120/  246]    Overall Loss 0.313370    Objective Loss 0.313370                                        LR 0.000013    Time 0.020475    
2023-01-06 16:21:05,795 - Epoch: [126][  130/  246]    Overall Loss 0.313353    Objective Loss 0.313353                                        LR 0.000013    Time 0.020177    
2023-01-06 16:21:05,967 - Epoch: [126][  140/  246]    Overall Loss 0.312967    Objective Loss 0.312967                                        LR 0.000013    Time 0.019958    
2023-01-06 16:21:06,121 - Epoch: [126][  150/  246]    Overall Loss 0.313514    Objective Loss 0.313514                                        LR 0.000013    Time 0.019657    
2023-01-06 16:21:06,293 - Epoch: [126][  160/  246]    Overall Loss 0.313930    Objective Loss 0.313930                                        LR 0.000013    Time 0.019499    
2023-01-06 16:21:06,457 - Epoch: [126][  170/  246]    Overall Loss 0.313289    Objective Loss 0.313289                                        LR 0.000013    Time 0.019316    
2023-01-06 16:21:06,626 - Epoch: [126][  180/  246]    Overall Loss 0.313661    Objective Loss 0.313661                                        LR 0.000013    Time 0.019178    
2023-01-06 16:21:06,765 - Epoch: [126][  190/  246]    Overall Loss 0.313336    Objective Loss 0.313336                                        LR 0.000013    Time 0.018900    
2023-01-06 16:21:06,902 - Epoch: [126][  200/  246]    Overall Loss 0.313354    Objective Loss 0.313354                                        LR 0.000013    Time 0.018637    
2023-01-06 16:21:07,039 - Epoch: [126][  210/  246]    Overall Loss 0.312219    Objective Loss 0.312219                                        LR 0.000013    Time 0.018400    
2023-01-06 16:21:07,176 - Epoch: [126][  220/  246]    Overall Loss 0.312910    Objective Loss 0.312910                                        LR 0.000013    Time 0.018182    
2023-01-06 16:21:07,313 - Epoch: [126][  230/  246]    Overall Loss 0.311968    Objective Loss 0.311968                                        LR 0.000013    Time 0.017988    
2023-01-06 16:21:07,464 - Epoch: [126][  240/  246]    Overall Loss 0.312032    Objective Loss 0.312032                                        LR 0.000013    Time 0.017866    
2023-01-06 16:21:07,531 - Epoch: [126][  246/  246]    Overall Loss 0.312920    Objective Loss 0.312920    Top1 86.124402    LR 0.000013    Time 0.017702    
2023-01-06 16:21:07,673 - --- validate (epoch=126)-----------
2023-01-06 16:21:07,673 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:08,144 - Epoch: [126][   10/   28]    Loss 0.306516    Top1 88.632812    
2023-01-06 16:21:08,259 - Epoch: [126][   20/   28]    Loss 0.310232    Top1 88.320312    
2023-01-06 16:21:08,317 - Epoch: [126][   28/   28]    Loss 0.310510    Top1 88.176353    
2023-01-06 16:21:08,451 - ==> Top1: 88.176    Loss: 0.311

2023-01-06 16:21:08,451 - ==> Confusion:
[[ 165    6  268]
 [   7  171  424]
 [  63   58 5824]]

2023-01-06 16:21:08,452 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:21:08,453 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:08,457 - 

2023-01-06 16:21:08,457 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:09,121 - Epoch: [127][   10/  246]    Overall Loss 0.338623    Objective Loss 0.338623                                        LR 0.000013    Time 0.066289    
2023-01-06 16:21:09,255 - Epoch: [127][   20/  246]    Overall Loss 0.330631    Objective Loss 0.330631                                        LR 0.000013    Time 0.039831    
2023-01-06 16:21:09,391 - Epoch: [127][   30/  246]    Overall Loss 0.321714    Objective Loss 0.321714                                        LR 0.000013    Time 0.031087    
2023-01-06 16:21:09,532 - Epoch: [127][   40/  246]    Overall Loss 0.322265    Objective Loss 0.322265                                        LR 0.000013    Time 0.026811    
2023-01-06 16:21:09,702 - Epoch: [127][   50/  246]    Overall Loss 0.324445    Objective Loss 0.324445                                        LR 0.000013    Time 0.024838    
2023-01-06 16:21:09,875 - Epoch: [127][   60/  246]    Overall Loss 0.323121    Objective Loss 0.323121                                        LR 0.000013    Time 0.023587    
2023-01-06 16:21:10,052 - Epoch: [127][   70/  246]    Overall Loss 0.320554    Objective Loss 0.320554                                        LR 0.000013    Time 0.022732    
2023-01-06 16:21:10,222 - Epoch: [127][   80/  246]    Overall Loss 0.320506    Objective Loss 0.320506                                        LR 0.000013    Time 0.022018    
2023-01-06 16:21:10,400 - Epoch: [127][   90/  246]    Overall Loss 0.319325    Objective Loss 0.319325                                        LR 0.000013    Time 0.021541    
2023-01-06 16:21:10,572 - Epoch: [127][  100/  246]    Overall Loss 0.317489    Objective Loss 0.317489                                        LR 0.000013    Time 0.021085    
2023-01-06 16:21:10,747 - Epoch: [127][  110/  246]    Overall Loss 0.316518    Objective Loss 0.316518                                        LR 0.000013    Time 0.020762    
2023-01-06 16:21:10,917 - Epoch: [127][  120/  246]    Overall Loss 0.314068    Objective Loss 0.314068                                        LR 0.000013    Time 0.020438    
2023-01-06 16:21:11,089 - Epoch: [127][  130/  246]    Overall Loss 0.313368    Objective Loss 0.313368                                        LR 0.000013    Time 0.020186    
2023-01-06 16:21:11,259 - Epoch: [127][  140/  246]    Overall Loss 0.313827    Objective Loss 0.313827                                        LR 0.000013    Time 0.019956    
2023-01-06 16:21:11,432 - Epoch: [127][  150/  246]    Overall Loss 0.313538    Objective Loss 0.313538                                        LR 0.000013    Time 0.019778    
2023-01-06 16:21:11,604 - Epoch: [127][  160/  246]    Overall Loss 0.312397    Objective Loss 0.312397                                        LR 0.000013    Time 0.019606    
2023-01-06 16:21:11,778 - Epoch: [127][  170/  246]    Overall Loss 0.312639    Objective Loss 0.312639                                        LR 0.000013    Time 0.019479    
2023-01-06 16:21:11,951 - Epoch: [127][  180/  246]    Overall Loss 0.312578    Objective Loss 0.312578                                        LR 0.000013    Time 0.019353    
2023-01-06 16:21:12,095 - Epoch: [127][  190/  246]    Overall Loss 0.312236    Objective Loss 0.312236                                        LR 0.000013    Time 0.019090    
2023-01-06 16:21:12,231 - Epoch: [127][  200/  246]    Overall Loss 0.312842    Objective Loss 0.312842                                        LR 0.000013    Time 0.018817    
2023-01-06 16:21:12,367 - Epoch: [127][  210/  246]    Overall Loss 0.312111    Objective Loss 0.312111                                        LR 0.000013    Time 0.018564    
2023-01-06 16:21:12,501 - Epoch: [127][  220/  246]    Overall Loss 0.311446    Objective Loss 0.311446                                        LR 0.000013    Time 0.018329    
2023-01-06 16:21:12,634 - Epoch: [127][  230/  246]    Overall Loss 0.311599    Objective Loss 0.311599                                        LR 0.000013    Time 0.018110    
2023-01-06 16:21:12,782 - Epoch: [127][  240/  246]    Overall Loss 0.312187    Objective Loss 0.312187                                        LR 0.000013    Time 0.017971    
2023-01-06 16:21:12,850 - Epoch: [127][  246/  246]    Overall Loss 0.312170    Objective Loss 0.312170    Top1 88.755981    LR 0.000013    Time 0.017806    
2023-01-06 16:21:13,033 - --- validate (epoch=127)-----------
2023-01-06 16:21:13,034 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:13,470 - Epoch: [127][   10/   28]    Loss 0.304207    Top1 88.476562    
2023-01-06 16:21:13,570 - Epoch: [127][   20/   28]    Loss 0.308876    Top1 88.574219    
2023-01-06 16:21:13,628 - Epoch: [127][   28/   28]    Loss 0.311713    Top1 88.247924    
2023-01-06 16:21:13,761 - ==> Top1: 88.248    Loss: 0.312

2023-01-06 16:21:13,761 - ==> Confusion:
[[ 167    7  265]
 [   8  178  416]
 [  60   65 5820]]

2023-01-06 16:21:13,763 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:21:13,763 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:13,767 - 

2023-01-06 16:21:13,767 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:14,285 - Epoch: [128][   10/  246]    Overall Loss 0.299200    Objective Loss 0.299200                                        LR 0.000013    Time 0.051736    
2023-01-06 16:21:14,442 - Epoch: [128][   20/  246]    Overall Loss 0.295430    Objective Loss 0.295430                                        LR 0.000013    Time 0.033688    
2023-01-06 16:21:14,599 - Epoch: [128][   30/  246]    Overall Loss 0.304013    Objective Loss 0.304013                                        LR 0.000013    Time 0.027696    
2023-01-06 16:21:14,759 - Epoch: [128][   40/  246]    Overall Loss 0.305124    Objective Loss 0.305124                                        LR 0.000013    Time 0.024761    
2023-01-06 16:21:14,924 - Epoch: [128][   50/  246]    Overall Loss 0.309103    Objective Loss 0.309103                                        LR 0.000013    Time 0.023088    
2023-01-06 16:21:15,094 - Epoch: [128][   60/  246]    Overall Loss 0.307084    Objective Loss 0.307084                                        LR 0.000013    Time 0.022079    
2023-01-06 16:21:15,251 - Epoch: [128][   70/  246]    Overall Loss 0.308324    Objective Loss 0.308324                                        LR 0.000013    Time 0.021155    
2023-01-06 16:21:15,410 - Epoch: [128][   80/  246]    Overall Loss 0.306325    Objective Loss 0.306325                                        LR 0.000013    Time 0.020494    
2023-01-06 16:21:15,577 - Epoch: [128][   90/  246]    Overall Loss 0.308520    Objective Loss 0.308520                                        LR 0.000013    Time 0.020065    
2023-01-06 16:21:15,748 - Epoch: [128][  100/  246]    Overall Loss 0.308108    Objective Loss 0.308108                                        LR 0.000013    Time 0.019766    
2023-01-06 16:21:15,914 - Epoch: [128][  110/  246]    Overall Loss 0.307968    Objective Loss 0.307968                                        LR 0.000013    Time 0.019478    
2023-01-06 16:21:16,085 - Epoch: [128][  120/  246]    Overall Loss 0.308995    Objective Loss 0.308995                                        LR 0.000013    Time 0.019276    
2023-01-06 16:21:16,251 - Epoch: [128][  130/  246]    Overall Loss 0.309564    Objective Loss 0.309564                                        LR 0.000013    Time 0.019066    
2023-01-06 16:21:16,422 - Epoch: [128][  140/  246]    Overall Loss 0.308737    Objective Loss 0.308737                                        LR 0.000013    Time 0.018922    
2023-01-06 16:21:16,590 - Epoch: [128][  150/  246]    Overall Loss 0.309721    Objective Loss 0.309721                                        LR 0.000013    Time 0.018775    
2023-01-06 16:21:16,761 - Epoch: [128][  160/  246]    Overall Loss 0.308770    Objective Loss 0.308770                                        LR 0.000013    Time 0.018669    
2023-01-06 16:21:16,927 - Epoch: [128][  170/  246]    Overall Loss 0.310060    Objective Loss 0.310060                                        LR 0.000013    Time 0.018547    
2023-01-06 16:21:17,099 - Epoch: [128][  180/  246]    Overall Loss 0.310048    Objective Loss 0.310048                                        LR 0.000013    Time 0.018468    
2023-01-06 16:21:17,264 - Epoch: [128][  190/  246]    Overall Loss 0.310397    Objective Loss 0.310397                                        LR 0.000013    Time 0.018366    
2023-01-06 16:21:17,435 - Epoch: [128][  200/  246]    Overall Loss 0.311014    Objective Loss 0.311014                                        LR 0.000013    Time 0.018298    
2023-01-06 16:21:17,601 - Epoch: [128][  210/  246]    Overall Loss 0.310887    Objective Loss 0.310887                                        LR 0.000013    Time 0.018214    
2023-01-06 16:21:17,771 - Epoch: [128][  220/  246]    Overall Loss 0.311625    Objective Loss 0.311625                                        LR 0.000013    Time 0.018159    
2023-01-06 16:21:17,937 - Epoch: [128][  230/  246]    Overall Loss 0.312332    Objective Loss 0.312332                                        LR 0.000013    Time 0.018090    
2023-01-06 16:21:18,116 - Epoch: [128][  240/  246]    Overall Loss 0.312385    Objective Loss 0.312385                                        LR 0.000013    Time 0.018081    
2023-01-06 16:21:18,197 - Epoch: [128][  246/  246]    Overall Loss 0.312547    Objective Loss 0.312547    Top1 89.473684    LR 0.000013    Time 0.017969    
2023-01-06 16:21:18,329 - --- validate (epoch=128)-----------
2023-01-06 16:21:18,329 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:18,765 - Epoch: [128][   10/   28]    Loss 0.322042    Top1 87.734375    
2023-01-06 16:21:18,870 - Epoch: [128][   20/   28]    Loss 0.308855    Top1 88.535156    
2023-01-06 16:21:18,927 - Epoch: [128][   28/   28]    Loss 0.315980    Top1 88.476954    
2023-01-06 16:21:19,071 - ==> Top1: 88.477    Loss: 0.316

2023-01-06 16:21:19,071 - ==> Confusion:
[[ 190    7  242]
 [  10  192  400]
 [  72   74 5799]]

2023-01-06 16:21:19,073 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 122]
2023-01-06 16:21:19,073 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:19,077 - 

2023-01-06 16:21:19,077 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:19,773 - Epoch: [129][   10/  246]    Overall Loss 0.306317    Objective Loss 0.306317                                        LR 0.000013    Time 0.069536    
2023-01-06 16:21:19,944 - Epoch: [129][   20/  246]    Overall Loss 0.315974    Objective Loss 0.315974                                        LR 0.000013    Time 0.043304    
2023-01-06 16:21:20,117 - Epoch: [129][   30/  246]    Overall Loss 0.307059    Objective Loss 0.307059                                        LR 0.000013    Time 0.034625    
2023-01-06 16:21:20,286 - Epoch: [129][   40/  246]    Overall Loss 0.305686    Objective Loss 0.305686                                        LR 0.000013    Time 0.030169    
2023-01-06 16:21:20,452 - Epoch: [129][   50/  246]    Overall Loss 0.304314    Objective Loss 0.304314                                        LR 0.000013    Time 0.027421    
2023-01-06 16:21:20,622 - Epoch: [129][   60/  246]    Overall Loss 0.305546    Objective Loss 0.305546                                        LR 0.000013    Time 0.025674    
2023-01-06 16:21:20,792 - Epoch: [129][   70/  246]    Overall Loss 0.307770    Objective Loss 0.307770                                        LR 0.000013    Time 0.024400    
2023-01-06 16:21:20,966 - Epoch: [129][   80/  246]    Overall Loss 0.306310    Objective Loss 0.306310                                        LR 0.000013    Time 0.023520    
2023-01-06 16:21:21,136 - Epoch: [129][   90/  246]    Overall Loss 0.306891    Objective Loss 0.306891                                        LR 0.000013    Time 0.022779    
2023-01-06 16:21:21,311 - Epoch: [129][  100/  246]    Overall Loss 0.306707    Objective Loss 0.306707                                        LR 0.000013    Time 0.022243    
2023-01-06 16:21:21,479 - Epoch: [129][  110/  246]    Overall Loss 0.307005    Objective Loss 0.307005                                        LR 0.000013    Time 0.021742    
2023-01-06 16:21:21,650 - Epoch: [129][  120/  246]    Overall Loss 0.308515    Objective Loss 0.308515                                        LR 0.000013    Time 0.021353    
2023-01-06 16:21:21,816 - Epoch: [129][  130/  246]    Overall Loss 0.309101    Objective Loss 0.309101                                        LR 0.000013    Time 0.020983    
2023-01-06 16:21:21,989 - Epoch: [129][  140/  246]    Overall Loss 0.312463    Objective Loss 0.312463                                        LR 0.000013    Time 0.020719    
2023-01-06 16:21:22,161 - Epoch: [129][  150/  246]    Overall Loss 0.311326    Objective Loss 0.311326                                        LR 0.000013    Time 0.020472    
2023-01-06 16:21:22,335 - Epoch: [129][  160/  246]    Overall Loss 0.310547    Objective Loss 0.310547                                        LR 0.000013    Time 0.020277    
2023-01-06 16:21:22,510 - Epoch: [129][  170/  246]    Overall Loss 0.311230    Objective Loss 0.311230                                        LR 0.000013    Time 0.020114    
2023-01-06 16:21:22,696 - Epoch: [129][  180/  246]    Overall Loss 0.310520    Objective Loss 0.310520                                        LR 0.000013    Time 0.020023    
2023-01-06 16:21:22,866 - Epoch: [129][  190/  246]    Overall Loss 0.311112    Objective Loss 0.311112                                        LR 0.000013    Time 0.019856    
2023-01-06 16:21:23,042 - Epoch: [129][  200/  246]    Overall Loss 0.310662    Objective Loss 0.310662                                        LR 0.000013    Time 0.019738    
2023-01-06 16:21:23,237 - Epoch: [129][  210/  246]    Overall Loss 0.311082    Objective Loss 0.311082                                        LR 0.000013    Time 0.019724    
2023-01-06 16:21:23,445 - Epoch: [129][  220/  246]    Overall Loss 0.311262    Objective Loss 0.311262                                        LR 0.000013    Time 0.019775    
2023-01-06 16:21:23,657 - Epoch: [129][  230/  246]    Overall Loss 0.312538    Objective Loss 0.312538                                        LR 0.000013    Time 0.019832    
2023-01-06 16:21:23,884 - Epoch: [129][  240/  246]    Overall Loss 0.312444    Objective Loss 0.312444                                        LR 0.000013    Time 0.019953    
2023-01-06 16:21:23,982 - Epoch: [129][  246/  246]    Overall Loss 0.312139    Objective Loss 0.312139    Top1 90.909091    LR 0.000013    Time 0.019861    
2023-01-06 16:21:24,131 - --- validate (epoch=129)-----------
2023-01-06 16:21:24,131 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:24,567 - Epoch: [129][   10/   28]    Loss 0.315727    Top1 88.515625    
2023-01-06 16:21:24,668 - Epoch: [129][   20/   28]    Loss 0.313194    Top1 88.671875    
2023-01-06 16:21:24,724 - Epoch: [129][   28/   28]    Loss 0.314091    Top1 88.677355    
2023-01-06 16:21:24,846 - ==> Top1: 88.677    Loss: 0.314

2023-01-06 16:21:24,846 - ==> Confusion:
[[ 168    8  263]
 [  11  216  375]
 [  46   88 5811]]

2023-01-06 16:21:24,848 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:24,848 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:24,853 - 

2023-01-06 16:21:24,853 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:25,545 - Epoch: [130][   10/  246]    Overall Loss 0.298514    Objective Loss 0.298514                                        LR 0.000013    Time 0.069159    
2023-01-06 16:21:25,712 - Epoch: [130][   20/  246]    Overall Loss 0.303043    Objective Loss 0.303043                                        LR 0.000013    Time 0.042902    
2023-01-06 16:21:25,879 - Epoch: [130][   30/  246]    Overall Loss 0.307217    Objective Loss 0.307217                                        LR 0.000013    Time 0.034150    
2023-01-06 16:21:26,045 - Epoch: [130][   40/  246]    Overall Loss 0.310823    Objective Loss 0.310823                                        LR 0.000013    Time 0.029756    
2023-01-06 16:21:26,210 - Epoch: [130][   50/  246]    Overall Loss 0.311695    Objective Loss 0.311695                                        LR 0.000013    Time 0.027108    
2023-01-06 16:21:26,376 - Epoch: [130][   60/  246]    Overall Loss 0.317001    Objective Loss 0.317001                                        LR 0.000013    Time 0.025345    
2023-01-06 16:21:26,541 - Epoch: [130][   70/  246]    Overall Loss 0.317316    Objective Loss 0.317316                                        LR 0.000013    Time 0.024086    
2023-01-06 16:21:26,708 - Epoch: [130][   80/  246]    Overall Loss 0.316653    Objective Loss 0.316653                                        LR 0.000013    Time 0.023151    
2023-01-06 16:21:26,874 - Epoch: [130][   90/  246]    Overall Loss 0.318727    Objective Loss 0.318727                                        LR 0.000013    Time 0.022420    
2023-01-06 16:21:27,041 - Epoch: [130][  100/  246]    Overall Loss 0.316403    Objective Loss 0.316403                                        LR 0.000013    Time 0.021846    
2023-01-06 16:21:27,207 - Epoch: [130][  110/  246]    Overall Loss 0.315029    Objective Loss 0.315029                                        LR 0.000013    Time 0.021364    
2023-01-06 16:21:27,374 - Epoch: [130][  120/  246]    Overall Loss 0.315282    Objective Loss 0.315282                                        LR 0.000013    Time 0.020972    
2023-01-06 16:21:27,550 - Epoch: [130][  130/  246]    Overall Loss 0.312864    Objective Loss 0.312864                                        LR 0.000013    Time 0.020711    
2023-01-06 16:21:27,722 - Epoch: [130][  140/  246]    Overall Loss 0.312713    Objective Loss 0.312713                                        LR 0.000013    Time 0.020460    
2023-01-06 16:21:27,896 - Epoch: [130][  150/  246]    Overall Loss 0.312688    Objective Loss 0.312688                                        LR 0.000013    Time 0.020250    
2023-01-06 16:21:28,063 - Epoch: [130][  160/  246]    Overall Loss 0.312944    Objective Loss 0.312944                                        LR 0.000013    Time 0.020026    
2023-01-06 16:21:28,230 - Epoch: [130][  170/  246]    Overall Loss 0.312031    Objective Loss 0.312031                                        LR 0.000013    Time 0.019828    
2023-01-06 16:21:28,398 - Epoch: [130][  180/  246]    Overall Loss 0.312063    Objective Loss 0.312063                                        LR 0.000013    Time 0.019661    
2023-01-06 16:21:28,569 - Epoch: [130][  190/  246]    Overall Loss 0.312218    Objective Loss 0.312218                                        LR 0.000013    Time 0.019521    
2023-01-06 16:21:28,737 - Epoch: [130][  200/  246]    Overall Loss 0.312225    Objective Loss 0.312225                                        LR 0.000013    Time 0.019383    
2023-01-06 16:21:28,905 - Epoch: [130][  210/  246]    Overall Loss 0.312211    Objective Loss 0.312211                                        LR 0.000013    Time 0.019258    
2023-01-06 16:21:29,075 - Epoch: [130][  220/  246]    Overall Loss 0.312135    Objective Loss 0.312135                                        LR 0.000013    Time 0.019152    
2023-01-06 16:21:29,253 - Epoch: [130][  230/  246]    Overall Loss 0.311725    Objective Loss 0.311725                                        LR 0.000013    Time 0.019092    
2023-01-06 16:21:29,436 - Epoch: [130][  240/  246]    Overall Loss 0.311826    Objective Loss 0.311826                                        LR 0.000013    Time 0.019059    
2023-01-06 16:21:29,516 - Epoch: [130][  246/  246]    Overall Loss 0.311663    Objective Loss 0.311663    Top1 89.712919    LR 0.000013    Time 0.018920    
2023-01-06 16:21:29,660 - --- validate (epoch=130)-----------
2023-01-06 16:21:29,660 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:30,083 - Epoch: [130][   10/   28]    Loss 0.322504    Top1 88.593750    
2023-01-06 16:21:30,185 - Epoch: [130][   20/   28]    Loss 0.314542    Top1 88.652344    
2023-01-06 16:21:30,245 - Epoch: [130][   28/   28]    Loss 0.311634    Top1 88.591469    
2023-01-06 16:21:30,407 - ==> Top1: 88.591    Loss: 0.312

2023-01-06 16:21:30,407 - ==> Confusion:
[[ 193    8  238]
 [  11  200  391]
 [  69   80 5796]]

2023-01-06 16:21:30,409 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:30,409 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:30,413 - 

2023-01-06 16:21:30,413 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:30,957 - Epoch: [131][   10/  246]    Overall Loss 0.311732    Objective Loss 0.311732                                        LR 0.000013    Time 0.054305    
2023-01-06 16:21:31,119 - Epoch: [131][   20/  246]    Overall Loss 0.310630    Objective Loss 0.310630                                        LR 0.000013    Time 0.035179    
2023-01-06 16:21:31,265 - Epoch: [131][   30/  246]    Overall Loss 0.304505    Objective Loss 0.304505                                        LR 0.000013    Time 0.028277    
2023-01-06 16:21:31,405 - Epoch: [131][   40/  246]    Overall Loss 0.306827    Objective Loss 0.306827                                        LR 0.000013    Time 0.024690    
2023-01-06 16:21:31,543 - Epoch: [131][   50/  246]    Overall Loss 0.312620    Objective Loss 0.312620                                        LR 0.000013    Time 0.022508    
2023-01-06 16:21:31,682 - Epoch: [131][   60/  246]    Overall Loss 0.309459    Objective Loss 0.309459                                        LR 0.000013    Time 0.021062    
2023-01-06 16:21:31,818 - Epoch: [131][   70/  246]    Overall Loss 0.306784    Objective Loss 0.306784                                        LR 0.000013    Time 0.019985    
2023-01-06 16:21:31,961 - Epoch: [131][   80/  246]    Overall Loss 0.305453    Objective Loss 0.305453                                        LR 0.000013    Time 0.019275    
2023-01-06 16:21:32,107 - Epoch: [131][   90/  246]    Overall Loss 0.307527    Objective Loss 0.307527                                        LR 0.000013    Time 0.018752    
2023-01-06 16:21:32,270 - Epoch: [131][  100/  246]    Overall Loss 0.308255    Objective Loss 0.308255                                        LR 0.000013    Time 0.018499    
2023-01-06 16:21:32,440 - Epoch: [131][  110/  246]    Overall Loss 0.307953    Objective Loss 0.307953                                        LR 0.000013    Time 0.018359    
2023-01-06 16:21:32,609 - Epoch: [131][  120/  246]    Overall Loss 0.307170    Objective Loss 0.307170                                        LR 0.000013    Time 0.018235    
2023-01-06 16:21:32,778 - Epoch: [131][  130/  246]    Overall Loss 0.308475    Objective Loss 0.308475                                        LR 0.000013    Time 0.018134    
2023-01-06 16:21:32,947 - Epoch: [131][  140/  246]    Overall Loss 0.309022    Objective Loss 0.309022                                        LR 0.000013    Time 0.018039    
2023-01-06 16:21:33,109 - Epoch: [131][  150/  246]    Overall Loss 0.309937    Objective Loss 0.309937                                        LR 0.000013    Time 0.017910    
2023-01-06 16:21:33,275 - Epoch: [131][  160/  246]    Overall Loss 0.310202    Objective Loss 0.310202                                        LR 0.000013    Time 0.017826    
2023-01-06 16:21:33,435 - Epoch: [131][  170/  246]    Overall Loss 0.310680    Objective Loss 0.310680                                        LR 0.000013    Time 0.017718    
2023-01-06 16:21:33,600 - Epoch: [131][  180/  246]    Overall Loss 0.310826    Objective Loss 0.310826                                        LR 0.000013    Time 0.017645    
2023-01-06 16:21:33,751 - Epoch: [131][  190/  246]    Overall Loss 0.310622    Objective Loss 0.310622                                        LR 0.000013    Time 0.017510    
2023-01-06 16:21:33,901 - Epoch: [131][  200/  246]    Overall Loss 0.311123    Objective Loss 0.311123                                        LR 0.000013    Time 0.017385    
2023-01-06 16:21:34,051 - Epoch: [131][  210/  246]    Overall Loss 0.311213    Objective Loss 0.311213                                        LR 0.000013    Time 0.017270    
2023-01-06 16:21:34,199 - Epoch: [131][  220/  246]    Overall Loss 0.311762    Objective Loss 0.311762                                        LR 0.000013    Time 0.017155    
2023-01-06 16:21:34,346 - Epoch: [131][  230/  246]    Overall Loss 0.311966    Objective Loss 0.311966                                        LR 0.000013    Time 0.017046    
2023-01-06 16:21:34,507 - Epoch: [131][  240/  246]    Overall Loss 0.311230    Objective Loss 0.311230                                        LR 0.000013    Time 0.017002    
2023-01-06 16:21:34,580 - Epoch: [131][  246/  246]    Overall Loss 0.310743    Objective Loss 0.310743    Top1 88.995215    LR 0.000013    Time 0.016883    
2023-01-06 16:21:34,702 - --- validate (epoch=131)-----------
2023-01-06 16:21:34,702 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:35,133 - Epoch: [131][   10/   28]    Loss 0.332647    Top1 87.148438    
2023-01-06 16:21:35,237 - Epoch: [131][   20/   28]    Loss 0.309859    Top1 88.164062    
2023-01-06 16:21:35,296 - Epoch: [131][   28/   28]    Loss 0.308600    Top1 88.204981    
2023-01-06 16:21:35,458 - ==> Top1: 88.205    Loss: 0.309

2023-01-06 16:21:35,458 - ==> Confusion:
[[ 158    5  276]
 [   7  168  427]
 [  49   60 5836]]

2023-01-06 16:21:35,459 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:35,459 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:35,463 - 

2023-01-06 16:21:35,464 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:36,122 - Epoch: [132][   10/  246]    Overall Loss 0.331734    Objective Loss 0.331734                                        LR 0.000013    Time 0.065759    
2023-01-06 16:21:36,260 - Epoch: [132][   20/  246]    Overall Loss 0.314561    Objective Loss 0.314561                                        LR 0.000013    Time 0.039792    
2023-01-06 16:21:36,400 - Epoch: [132][   30/  246]    Overall Loss 0.314260    Objective Loss 0.314260                                        LR 0.000013    Time 0.031154    
2023-01-06 16:21:36,534 - Epoch: [132][   40/  246]    Overall Loss 0.312968    Objective Loss 0.312968                                        LR 0.000013    Time 0.026711    
2023-01-06 16:21:36,672 - Epoch: [132][   50/  246]    Overall Loss 0.311931    Objective Loss 0.311931                                        LR 0.000013    Time 0.024120    
2023-01-06 16:21:36,812 - Epoch: [132][   60/  246]    Overall Loss 0.308979    Objective Loss 0.308979                                        LR 0.000013    Time 0.022425    
2023-01-06 16:21:36,963 - Epoch: [132][   70/  246]    Overall Loss 0.310685    Objective Loss 0.310685                                        LR 0.000013    Time 0.021384    
2023-01-06 16:21:37,131 - Epoch: [132][   80/  246]    Overall Loss 0.310491    Objective Loss 0.310491                                        LR 0.000013    Time 0.020799    
2023-01-06 16:21:37,309 - Epoch: [132][   90/  246]    Overall Loss 0.309770    Objective Loss 0.309770                                        LR 0.000013    Time 0.020459    
2023-01-06 16:21:37,479 - Epoch: [132][  100/  246]    Overall Loss 0.309115    Objective Loss 0.309115                                        LR 0.000013    Time 0.020096    
2023-01-06 16:21:37,652 - Epoch: [132][  110/  246]    Overall Loss 0.308488    Objective Loss 0.308488                                        LR 0.000013    Time 0.019836    
2023-01-06 16:21:37,824 - Epoch: [132][  120/  246]    Overall Loss 0.310655    Objective Loss 0.310655                                        LR 0.000013    Time 0.019617    
2023-01-06 16:21:37,995 - Epoch: [132][  130/  246]    Overall Loss 0.310334    Objective Loss 0.310334                                        LR 0.000013    Time 0.019416    
2023-01-06 16:21:38,168 - Epoch: [132][  140/  246]    Overall Loss 0.311264    Objective Loss 0.311264                                        LR 0.000013    Time 0.019262    
2023-01-06 16:21:38,338 - Epoch: [132][  150/  246]    Overall Loss 0.309940    Objective Loss 0.309940                                        LR 0.000013    Time 0.019112    
2023-01-06 16:21:38,510 - Epoch: [132][  160/  246]    Overall Loss 0.308959    Objective Loss 0.308959                                        LR 0.000013    Time 0.018987    
2023-01-06 16:21:38,685 - Epoch: [132][  170/  246]    Overall Loss 0.309030    Objective Loss 0.309030                                        LR 0.000013    Time 0.018897    
2023-01-06 16:21:38,867 - Epoch: [132][  180/  246]    Overall Loss 0.310183    Objective Loss 0.310183                                        LR 0.000013    Time 0.018857    
2023-01-06 16:21:39,041 - Epoch: [132][  190/  246]    Overall Loss 0.311371    Objective Loss 0.311371                                        LR 0.000013    Time 0.018778    
2023-01-06 16:21:39,214 - Epoch: [132][  200/  246]    Overall Loss 0.312120    Objective Loss 0.312120                                        LR 0.000013    Time 0.018700    
2023-01-06 16:21:39,389 - Epoch: [132][  210/  246]    Overall Loss 0.311642    Objective Loss 0.311642                                        LR 0.000013    Time 0.018645    
2023-01-06 16:21:39,571 - Epoch: [132][  220/  246]    Overall Loss 0.311696    Objective Loss 0.311696                                        LR 0.000013    Time 0.018621    
2023-01-06 16:21:39,743 - Epoch: [132][  230/  246]    Overall Loss 0.311341    Objective Loss 0.311341                                        LR 0.000013    Time 0.018558    
2023-01-06 16:21:39,933 - Epoch: [132][  240/  246]    Overall Loss 0.310969    Objective Loss 0.310969                                        LR 0.000013    Time 0.018573    
2023-01-06 16:21:40,013 - Epoch: [132][  246/  246]    Overall Loss 0.310570    Objective Loss 0.310570    Top1 89.473684    LR 0.000013    Time 0.018445    
2023-01-06 16:21:40,156 - --- validate (epoch=132)-----------
2023-01-06 16:21:40,156 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:40,611 - Epoch: [132][   10/   28]    Loss 0.302144    Top1 88.320312    
2023-01-06 16:21:40,722 - Epoch: [132][   20/   28]    Loss 0.310816    Top1 88.437500    
2023-01-06 16:21:40,779 - Epoch: [132][   28/   28]    Loss 0.315016    Top1 88.076152    
2023-01-06 16:21:40,938 - ==> Top1: 88.076    Loss: 0.315

2023-01-06 16:21:40,938 - ==> Confusion:
[[ 147    7  285]
 [   6  157  439]
 [  40   56 5849]]

2023-01-06 16:21:40,939 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:40,940 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:40,944 - 

2023-01-06 16:21:40,944 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:41,459 - Epoch: [133][   10/  246]    Overall Loss 0.323412    Objective Loss 0.323412                                        LR 0.000013    Time 0.051352    
2023-01-06 16:21:41,620 - Epoch: [133][   20/  246]    Overall Loss 0.311345    Objective Loss 0.311345                                        LR 0.000013    Time 0.033704    
2023-01-06 16:21:41,782 - Epoch: [133][   30/  246]    Overall Loss 0.311988    Objective Loss 0.311988                                        LR 0.000013    Time 0.027875    
2023-01-06 16:21:41,940 - Epoch: [133][   40/  246]    Overall Loss 0.311102    Objective Loss 0.311102                                        LR 0.000013    Time 0.024834    
2023-01-06 16:21:42,105 - Epoch: [133][   50/  246]    Overall Loss 0.306766    Objective Loss 0.306766                                        LR 0.000013    Time 0.023166    
2023-01-06 16:21:42,267 - Epoch: [133][   60/  246]    Overall Loss 0.308823    Objective Loss 0.308823                                        LR 0.000013    Time 0.022000    
2023-01-06 16:21:42,433 - Epoch: [133][   70/  246]    Overall Loss 0.310234    Objective Loss 0.310234                                        LR 0.000013    Time 0.021228    
2023-01-06 16:21:42,610 - Epoch: [133][   80/  246]    Overall Loss 0.310784    Objective Loss 0.310784                                        LR 0.000013    Time 0.020776    
2023-01-06 16:21:42,785 - Epoch: [133][   90/  246]    Overall Loss 0.312253    Objective Loss 0.312253                                        LR 0.000013    Time 0.020387    
2023-01-06 16:21:42,959 - Epoch: [133][  100/  246]    Overall Loss 0.313054    Objective Loss 0.313054                                        LR 0.000013    Time 0.020082    
2023-01-06 16:21:43,126 - Epoch: [133][  110/  246]    Overall Loss 0.314912    Objective Loss 0.314912                                        LR 0.000013    Time 0.019765    
2023-01-06 16:21:43,297 - Epoch: [133][  120/  246]    Overall Loss 0.313398    Objective Loss 0.313398                                        LR 0.000013    Time 0.019532    
2023-01-06 16:21:43,465 - Epoch: [133][  130/  246]    Overall Loss 0.314401    Objective Loss 0.314401                                        LR 0.000013    Time 0.019310    
2023-01-06 16:21:43,627 - Epoch: [133][  140/  246]    Overall Loss 0.314684    Objective Loss 0.314684                                        LR 0.000013    Time 0.019088    
2023-01-06 16:21:43,791 - Epoch: [133][  150/  246]    Overall Loss 0.312776    Objective Loss 0.312776                                        LR 0.000013    Time 0.018904    
2023-01-06 16:21:43,952 - Epoch: [133][  160/  246]    Overall Loss 0.313040    Objective Loss 0.313040                                        LR 0.000013    Time 0.018724    
2023-01-06 16:21:44,117 - Epoch: [133][  170/  246]    Overall Loss 0.313633    Objective Loss 0.313633                                        LR 0.000013    Time 0.018589    
2023-01-06 16:21:44,280 - Epoch: [133][  180/  246]    Overall Loss 0.313252    Objective Loss 0.313252                                        LR 0.000013    Time 0.018462    
2023-01-06 16:21:44,443 - Epoch: [133][  190/  246]    Overall Loss 0.312033    Objective Loss 0.312033                                        LR 0.000013    Time 0.018348    
2023-01-06 16:21:44,603 - Epoch: [133][  200/  246]    Overall Loss 0.311790    Objective Loss 0.311790                                        LR 0.000013    Time 0.018222    
2023-01-06 16:21:44,769 - Epoch: [133][  210/  246]    Overall Loss 0.311998    Objective Loss 0.311998                                        LR 0.000013    Time 0.018143    
2023-01-06 16:21:44,929 - Epoch: [133][  220/  246]    Overall Loss 0.311601    Objective Loss 0.311601                                        LR 0.000013    Time 0.018044    
2023-01-06 16:21:45,092 - Epoch: [133][  230/  246]    Overall Loss 0.311386    Objective Loss 0.311386                                        LR 0.000013    Time 0.017967    
2023-01-06 16:21:45,267 - Epoch: [133][  240/  246]    Overall Loss 0.311745    Objective Loss 0.311745                                        LR 0.000013    Time 0.017940    
2023-01-06 16:21:45,346 - Epoch: [133][  246/  246]    Overall Loss 0.311252    Objective Loss 0.311252    Top1 91.626794    LR 0.000013    Time 0.017824    
2023-01-06 16:21:45,479 - --- validate (epoch=133)-----------
2023-01-06 16:21:45,479 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:45,911 - Epoch: [133][   10/   28]    Loss 0.307565    Top1 89.023438    
2023-01-06 16:21:46,032 - Epoch: [133][   20/   28]    Loss 0.310307    Top1 88.554688    
2023-01-06 16:21:46,089 - Epoch: [133][   28/   28]    Loss 0.311899    Top1 88.405382    
2023-01-06 16:21:46,233 - ==> Top1: 88.405    Loss: 0.312

2023-01-06 16:21:46,233 - ==> Confusion:
[[ 176    7  256]
 [   9  189  404]
 [  61   73 5811]]

2023-01-06 16:21:46,234 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:46,234 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:46,238 - 

2023-01-06 16:21:46,239 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:46,927 - Epoch: [134][   10/  246]    Overall Loss 0.323880    Objective Loss 0.323880                                        LR 0.000013    Time 0.068786    
2023-01-06 16:21:47,090 - Epoch: [134][   20/  246]    Overall Loss 0.320083    Objective Loss 0.320083                                        LR 0.000013    Time 0.042528    
2023-01-06 16:21:47,261 - Epoch: [134][   30/  246]    Overall Loss 0.320292    Objective Loss 0.320292                                        LR 0.000013    Time 0.034032    
2023-01-06 16:21:47,430 - Epoch: [134][   40/  246]    Overall Loss 0.324660    Objective Loss 0.324660                                        LR 0.000013    Time 0.029735    
2023-01-06 16:21:47,595 - Epoch: [134][   50/  246]    Overall Loss 0.321272    Objective Loss 0.321272                                        LR 0.000013    Time 0.027086    
2023-01-06 16:21:47,770 - Epoch: [134][   60/  246]    Overall Loss 0.320875    Objective Loss 0.320875                                        LR 0.000013    Time 0.025474    
2023-01-06 16:21:47,950 - Epoch: [134][   70/  246]    Overall Loss 0.321037    Objective Loss 0.321037                                        LR 0.000013    Time 0.024411    
2023-01-06 16:21:48,127 - Epoch: [134][   80/  246]    Overall Loss 0.320853    Objective Loss 0.320853                                        LR 0.000013    Time 0.023561    
2023-01-06 16:21:48,307 - Epoch: [134][   90/  246]    Overall Loss 0.318645    Objective Loss 0.318645                                        LR 0.000013    Time 0.022937    
2023-01-06 16:21:48,483 - Epoch: [134][  100/  246]    Overall Loss 0.315968    Objective Loss 0.315968                                        LR 0.000013    Time 0.022405    
2023-01-06 16:21:48,662 - Epoch: [134][  110/  246]    Overall Loss 0.314499    Objective Loss 0.314499                                        LR 0.000013    Time 0.021995    
2023-01-06 16:21:48,839 - Epoch: [134][  120/  246]    Overall Loss 0.313733    Objective Loss 0.313733                                        LR 0.000013    Time 0.021632    
2023-01-06 16:21:49,016 - Epoch: [134][  130/  246]    Overall Loss 0.312870    Objective Loss 0.312870                                        LR 0.000013    Time 0.021326    
2023-01-06 16:21:49,195 - Epoch: [134][  140/  246]    Overall Loss 0.312447    Objective Loss 0.312447                                        LR 0.000013    Time 0.021083    
2023-01-06 16:21:49,369 - Epoch: [134][  150/  246]    Overall Loss 0.312106    Objective Loss 0.312106                                        LR 0.000013    Time 0.020830    
2023-01-06 16:21:49,541 - Epoch: [134][  160/  246]    Overall Loss 0.311449    Objective Loss 0.311449                                        LR 0.000013    Time 0.020605    
2023-01-06 16:21:49,710 - Epoch: [134][  170/  246]    Overall Loss 0.311210    Objective Loss 0.311210                                        LR 0.000013    Time 0.020384    
2023-01-06 16:21:49,884 - Epoch: [134][  180/  246]    Overall Loss 0.310168    Objective Loss 0.310168                                        LR 0.000013    Time 0.020216    
2023-01-06 16:21:50,053 - Epoch: [134][  190/  246]    Overall Loss 0.310150    Objective Loss 0.310150                                        LR 0.000013    Time 0.020039    
2023-01-06 16:21:50,223 - Epoch: [134][  200/  246]    Overall Loss 0.311728    Objective Loss 0.311728                                        LR 0.000013    Time 0.019888    
2023-01-06 16:21:50,392 - Epoch: [134][  210/  246]    Overall Loss 0.311511    Objective Loss 0.311511                                        LR 0.000013    Time 0.019742    
2023-01-06 16:21:50,575 - Epoch: [134][  220/  246]    Overall Loss 0.309846    Objective Loss 0.309846                                        LR 0.000013    Time 0.019674    
2023-01-06 16:21:50,756 - Epoch: [134][  230/  246]    Overall Loss 0.309420    Objective Loss 0.309420                                        LR 0.000013    Time 0.019606    
2023-01-06 16:21:50,947 - Epoch: [134][  240/  246]    Overall Loss 0.309999    Objective Loss 0.309999                                        LR 0.000013    Time 0.019581    
2023-01-06 16:21:51,030 - Epoch: [134][  246/  246]    Overall Loss 0.310029    Objective Loss 0.310029    Top1 87.081340    LR 0.000013    Time 0.019442    
2023-01-06 16:21:51,166 - --- validate (epoch=134)-----------
2023-01-06 16:21:51,166 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:51,612 - Epoch: [134][   10/   28]    Loss 0.292943    Top1 89.179688    
2023-01-06 16:21:51,718 - Epoch: [134][   20/   28]    Loss 0.302517    Top1 88.710938    
2023-01-06 16:21:51,775 - Epoch: [134][   28/   28]    Loss 0.308785    Top1 88.591469    
2023-01-06 16:21:51,925 - ==> Top1: 88.591    Loss: 0.309

2023-01-06 16:21:51,926 - ==> Confusion:
[[ 186    8  245]
 [  11  198  393]
 [  62   78 5805]]

2023-01-06 16:21:51,927 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:51,927 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:51,932 - 

2023-01-06 16:21:51,932 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:52,619 - Epoch: [135][   10/  246]    Overall Loss 0.307884    Objective Loss 0.307884                                        LR 0.000013    Time 0.068675    
2023-01-06 16:21:52,788 - Epoch: [135][   20/  246]    Overall Loss 0.306351    Objective Loss 0.306351                                        LR 0.000013    Time 0.042764    
2023-01-06 16:21:52,967 - Epoch: [135][   30/  246]    Overall Loss 0.305106    Objective Loss 0.305106                                        LR 0.000013    Time 0.034444    
2023-01-06 16:21:53,168 - Epoch: [135][   40/  246]    Overall Loss 0.305868    Objective Loss 0.305868                                        LR 0.000013    Time 0.030853    
2023-01-06 16:21:53,368 - Epoch: [135][   50/  246]    Overall Loss 0.306320    Objective Loss 0.306320                                        LR 0.000013    Time 0.028682    
2023-01-06 16:21:53,581 - Epoch: [135][   60/  246]    Overall Loss 0.304425    Objective Loss 0.304425                                        LR 0.000013    Time 0.027441    
2023-01-06 16:21:53,801 - Epoch: [135][   70/  246]    Overall Loss 0.303601    Objective Loss 0.303601                                        LR 0.000013    Time 0.026657    
2023-01-06 16:21:54,023 - Epoch: [135][   80/  246]    Overall Loss 0.307511    Objective Loss 0.307511                                        LR 0.000013    Time 0.026096    
2023-01-06 16:21:54,245 - Epoch: [135][   90/  246]    Overall Loss 0.304903    Objective Loss 0.304903                                        LR 0.000013    Time 0.025661    
2023-01-06 16:21:54,468 - Epoch: [135][  100/  246]    Overall Loss 0.306599    Objective Loss 0.306599                                        LR 0.000013    Time 0.025316    
2023-01-06 16:21:54,690 - Epoch: [135][  110/  246]    Overall Loss 0.307352    Objective Loss 0.307352                                        LR 0.000013    Time 0.025027    
2023-01-06 16:21:54,913 - Epoch: [135][  120/  246]    Overall Loss 0.308034    Objective Loss 0.308034                                        LR 0.000013    Time 0.024797    
2023-01-06 16:21:55,136 - Epoch: [135][  130/  246]    Overall Loss 0.307449    Objective Loss 0.307449                                        LR 0.000013    Time 0.024597    
2023-01-06 16:21:55,359 - Epoch: [135][  140/  246]    Overall Loss 0.306596    Objective Loss 0.306596                                        LR 0.000013    Time 0.024433    
2023-01-06 16:21:55,580 - Epoch: [135][  150/  246]    Overall Loss 0.307734    Objective Loss 0.307734                                        LR 0.000013    Time 0.024273    
2023-01-06 16:21:55,781 - Epoch: [135][  160/  246]    Overall Loss 0.309287    Objective Loss 0.309287                                        LR 0.000013    Time 0.024007    
2023-01-06 16:21:55,980 - Epoch: [135][  170/  246]    Overall Loss 0.308653    Objective Loss 0.308653                                        LR 0.000013    Time 0.023764    
2023-01-06 16:21:56,175 - Epoch: [135][  180/  246]    Overall Loss 0.309428    Objective Loss 0.309428                                        LR 0.000013    Time 0.023528    
2023-01-06 16:21:56,375 - Epoch: [135][  190/  246]    Overall Loss 0.307841    Objective Loss 0.307841                                        LR 0.000013    Time 0.023338    
2023-01-06 16:21:56,549 - Epoch: [135][  200/  246]    Overall Loss 0.307556    Objective Loss 0.307556                                        LR 0.000013    Time 0.023037    
2023-01-06 16:21:56,720 - Epoch: [135][  210/  246]    Overall Loss 0.307967    Objective Loss 0.307967                                        LR 0.000013    Time 0.022757    
2023-01-06 16:21:56,888 - Epoch: [135][  220/  246]    Overall Loss 0.308098    Objective Loss 0.308098                                        LR 0.000013    Time 0.022483    
2023-01-06 16:21:57,048 - Epoch: [135][  230/  246]    Overall Loss 0.308949    Objective Loss 0.308949                                        LR 0.000013    Time 0.022200    
2023-01-06 16:21:57,208 - Epoch: [135][  240/  246]    Overall Loss 0.309824    Objective Loss 0.309824                                        LR 0.000013    Time 0.021940    
2023-01-06 16:21:57,280 - Epoch: [135][  246/  246]    Overall Loss 0.309671    Objective Loss 0.309671    Top1 89.234450    LR 0.000013    Time 0.021695    
2023-01-06 16:21:57,412 - --- validate (epoch=135)-----------
2023-01-06 16:21:57,412 - 6986 samples (256 per mini-batch)
2023-01-06 16:21:57,849 - Epoch: [135][   10/   28]    Loss 0.314369    Top1 87.929688    
2023-01-06 16:21:57,966 - Epoch: [135][   20/   28]    Loss 0.315355    Top1 88.203125    
2023-01-06 16:21:58,022 - Epoch: [135][   28/   28]    Loss 0.307019    Top1 88.591469    
2023-01-06 16:21:58,161 - ==> Top1: 88.591    Loss: 0.307

2023-01-06 16:21:58,161 - ==> Confusion:
[[ 180    6  253]
 [  10  198  394]
 [  60   74 5811]]

2023-01-06 16:21:58,162 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:21:58,163 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:21:58,167 - 

2023-01-06 16:21:58,167 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:21:58,703 - Epoch: [136][   10/  246]    Overall Loss 0.328010    Objective Loss 0.328010                                        LR 0.000013    Time 0.053455    
2023-01-06 16:21:58,854 - Epoch: [136][   20/  246]    Overall Loss 0.314791    Objective Loss 0.314791                                        LR 0.000013    Time 0.034256    
2023-01-06 16:21:59,005 - Epoch: [136][   30/  246]    Overall Loss 0.314399    Objective Loss 0.314399                                        LR 0.000013    Time 0.027865    
2023-01-06 16:21:59,155 - Epoch: [136][   40/  246]    Overall Loss 0.311731    Objective Loss 0.311731                                        LR 0.000013    Time 0.024648    
2023-01-06 16:21:59,300 - Epoch: [136][   50/  246]    Overall Loss 0.307404    Objective Loss 0.307404                                        LR 0.000013    Time 0.022606    
2023-01-06 16:21:59,445 - Epoch: [136][   60/  246]    Overall Loss 0.305104    Objective Loss 0.305104                                        LR 0.000013    Time 0.021247    
2023-01-06 16:21:59,591 - Epoch: [136][   70/  246]    Overall Loss 0.308360    Objective Loss 0.308360                                        LR 0.000013    Time 0.020291    
2023-01-06 16:21:59,738 - Epoch: [136][   80/  246]    Overall Loss 0.307062    Objective Loss 0.307062                                        LR 0.000013    Time 0.019596    
2023-01-06 16:21:59,883 - Epoch: [136][   90/  246]    Overall Loss 0.310478    Objective Loss 0.310478                                        LR 0.000013    Time 0.019025    
2023-01-06 16:22:00,031 - Epoch: [136][  100/  246]    Overall Loss 0.309903    Objective Loss 0.309903                                        LR 0.000013    Time 0.018595    
2023-01-06 16:22:00,178 - Epoch: [136][  110/  246]    Overall Loss 0.309319    Objective Loss 0.309319                                        LR 0.000013    Time 0.018244    
2023-01-06 16:22:00,329 - Epoch: [136][  120/  246]    Overall Loss 0.309549    Objective Loss 0.309549                                        LR 0.000013    Time 0.017978    
2023-01-06 16:22:00,472 - Epoch: [136][  130/  246]    Overall Loss 0.310153    Objective Loss 0.310153                                        LR 0.000013    Time 0.017690    
2023-01-06 16:22:00,632 - Epoch: [136][  140/  246]    Overall Loss 0.310437    Objective Loss 0.310437                                        LR 0.000013    Time 0.017567    
2023-01-06 16:22:00,795 - Epoch: [136][  150/  246]    Overall Loss 0.309109    Objective Loss 0.309109                                        LR 0.000013    Time 0.017479    
2023-01-06 16:22:00,958 - Epoch: [136][  160/  246]    Overall Loss 0.309891    Objective Loss 0.309891                                        LR 0.000013    Time 0.017407    
2023-01-06 16:22:01,121 - Epoch: [136][  170/  246]    Overall Loss 0.310603    Objective Loss 0.310603                                        LR 0.000013    Time 0.017339    
2023-01-06 16:22:01,282 - Epoch: [136][  180/  246]    Overall Loss 0.311295    Objective Loss 0.311295                                        LR 0.000013    Time 0.017265    
2023-01-06 16:22:01,445 - Epoch: [136][  190/  246]    Overall Loss 0.312143    Objective Loss 0.312143                                        LR 0.000013    Time 0.017214    
2023-01-06 16:22:01,609 - Epoch: [136][  200/  246]    Overall Loss 0.311441    Objective Loss 0.311441                                        LR 0.000013    Time 0.017170    
2023-01-06 16:22:01,771 - Epoch: [136][  210/  246]    Overall Loss 0.310984    Objective Loss 0.310984                                        LR 0.000013    Time 0.017123    
2023-01-06 16:22:01,913 - Epoch: [136][  220/  246]    Overall Loss 0.310442    Objective Loss 0.310442                                        LR 0.000013    Time 0.016991    
2023-01-06 16:22:02,060 - Epoch: [136][  230/  246]    Overall Loss 0.310093    Objective Loss 0.310093                                        LR 0.000013    Time 0.016890    
2023-01-06 16:22:02,218 - Epoch: [136][  240/  246]    Overall Loss 0.309833    Objective Loss 0.309833                                        LR 0.000013    Time 0.016841    
2023-01-06 16:22:02,295 - Epoch: [136][  246/  246]    Overall Loss 0.309860    Objective Loss 0.309860    Top1 88.516746    LR 0.000013    Time 0.016742    
2023-01-06 16:22:02,436 - --- validate (epoch=136)-----------
2023-01-06 16:22:02,436 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:02,875 - Epoch: [136][   10/   28]    Loss 0.316333    Top1 87.773438    
2023-01-06 16:22:02,992 - Epoch: [136][   20/   28]    Loss 0.321679    Top1 87.695312    
2023-01-06 16:22:03,047 - Epoch: [136][   28/   28]    Loss 0.316314    Top1 88.319496    
2023-01-06 16:22:03,191 - ==> Top1: 88.319    Loss: 0.316

2023-01-06 16:22:03,191 - ==> Confusion:
[[ 151    7  281]
 [   7  178  417]
 [  40   64 5841]]

2023-01-06 16:22:03,193 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:03,193 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:03,197 - 

2023-01-06 16:22:03,197 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:03,872 - Epoch: [137][   10/  246]    Overall Loss 0.309711    Objective Loss 0.309711                                        LR 0.000013    Time 0.067397    
2023-01-06 16:22:04,018 - Epoch: [137][   20/  246]    Overall Loss 0.307412    Objective Loss 0.307412                                        LR 0.000013    Time 0.041001    
2023-01-06 16:22:04,157 - Epoch: [137][   30/  246]    Overall Loss 0.307878    Objective Loss 0.307878                                        LR 0.000013    Time 0.031945    
2023-01-06 16:22:04,299 - Epoch: [137][   40/  246]    Overall Loss 0.304553    Objective Loss 0.304553                                        LR 0.000013    Time 0.027480    
2023-01-06 16:22:04,439 - Epoch: [137][   50/  246]    Overall Loss 0.306842    Objective Loss 0.306842                                        LR 0.000013    Time 0.024767    
2023-01-06 16:22:04,580 - Epoch: [137][   60/  246]    Overall Loss 0.308502    Objective Loss 0.308502                                        LR 0.000013    Time 0.022979    
2023-01-06 16:22:04,722 - Epoch: [137][   70/  246]    Overall Loss 0.308018    Objective Loss 0.308018                                        LR 0.000013    Time 0.021721    
2023-01-06 16:22:04,872 - Epoch: [137][   80/  246]    Overall Loss 0.311610    Objective Loss 0.311610                                        LR 0.000013    Time 0.020874    
2023-01-06 16:22:05,022 - Epoch: [137][   90/  246]    Overall Loss 0.309305    Objective Loss 0.309305                                        LR 0.000013    Time 0.020208    
2023-01-06 16:22:05,173 - Epoch: [137][  100/  246]    Overall Loss 0.307758    Objective Loss 0.307758                                        LR 0.000013    Time 0.019693    
2023-01-06 16:22:05,317 - Epoch: [137][  110/  246]    Overall Loss 0.308065    Objective Loss 0.308065                                        LR 0.000013    Time 0.019216    
2023-01-06 16:22:05,468 - Epoch: [137][  120/  246]    Overall Loss 0.310066    Objective Loss 0.310066                                        LR 0.000013    Time 0.018866    
2023-01-06 16:22:05,614 - Epoch: [137][  130/  246]    Overall Loss 0.310249    Objective Loss 0.310249                                        LR 0.000013    Time 0.018535    
2023-01-06 16:22:05,763 - Epoch: [137][  140/  246]    Overall Loss 0.310803    Objective Loss 0.310803                                        LR 0.000013    Time 0.018273    
2023-01-06 16:22:05,909 - Epoch: [137][  150/  246]    Overall Loss 0.310880    Objective Loss 0.310880                                        LR 0.000013    Time 0.018026    
2023-01-06 16:22:06,047 - Epoch: [137][  160/  246]    Overall Loss 0.310265    Objective Loss 0.310265                                        LR 0.000013    Time 0.017760    
2023-01-06 16:22:06,200 - Epoch: [137][  170/  246]    Overall Loss 0.309741    Objective Loss 0.309741                                        LR 0.000013    Time 0.017613    
2023-01-06 16:22:06,357 - Epoch: [137][  180/  246]    Overall Loss 0.309416    Objective Loss 0.309416                                        LR 0.000013    Time 0.017500    
2023-01-06 16:22:06,513 - Epoch: [137][  190/  246]    Overall Loss 0.309967    Objective Loss 0.309967                                        LR 0.000013    Time 0.017400    
2023-01-06 16:22:06,652 - Epoch: [137][  200/  246]    Overall Loss 0.309913    Objective Loss 0.309913                                        LR 0.000013    Time 0.017225    
2023-01-06 16:22:06,788 - Epoch: [137][  210/  246]    Overall Loss 0.310692    Objective Loss 0.310692                                        LR 0.000013    Time 0.017046    
2023-01-06 16:22:06,960 - Epoch: [137][  220/  246]    Overall Loss 0.309748    Objective Loss 0.309748                                        LR 0.000013    Time 0.017056    
2023-01-06 16:22:07,139 - Epoch: [137][  230/  246]    Overall Loss 0.311024    Objective Loss 0.311024                                        LR 0.000013    Time 0.017085    
2023-01-06 16:22:07,328 - Epoch: [137][  240/  246]    Overall Loss 0.311242    Objective Loss 0.311242                                        LR 0.000013    Time 0.017149    
2023-01-06 16:22:07,404 - Epoch: [137][  246/  246]    Overall Loss 0.310174    Objective Loss 0.310174    Top1 89.952153    LR 0.000013    Time 0.017040    
2023-01-06 16:22:07,549 - --- validate (epoch=137)-----------
2023-01-06 16:22:07,549 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:07,989 - Epoch: [137][   10/   28]    Loss 0.305121    Top1 88.828125    
2023-01-06 16:22:08,089 - Epoch: [137][   20/   28]    Loss 0.315832    Top1 88.144531    
2023-01-06 16:22:08,146 - Epoch: [137][   28/   28]    Loss 0.311560    Top1 88.434011    
2023-01-06 16:22:08,294 - ==> Top1: 88.434    Loss: 0.312

2023-01-06 16:22:08,295 - ==> Confusion:
[[ 178    7  254]
 [   9  187  406]
 [  63   69 5813]]

2023-01-06 16:22:08,296 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:08,296 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:08,300 - 

2023-01-06 16:22:08,301 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:08,841 - Epoch: [138][   10/  246]    Overall Loss 0.314269    Objective Loss 0.314269                                        LR 0.000013    Time 0.053939    
2023-01-06 16:22:09,004 - Epoch: [138][   20/  246]    Overall Loss 0.316254    Objective Loss 0.316254                                        LR 0.000013    Time 0.035108    
2023-01-06 16:22:09,165 - Epoch: [138][   30/  246]    Overall Loss 0.319540    Objective Loss 0.319540                                        LR 0.000013    Time 0.028762    
2023-01-06 16:22:09,331 - Epoch: [138][   40/  246]    Overall Loss 0.313191    Objective Loss 0.313191                                        LR 0.000013    Time 0.025697    
2023-01-06 16:22:09,496 - Epoch: [138][   50/  246]    Overall Loss 0.317989    Objective Loss 0.317989                                        LR 0.000013    Time 0.023859    
2023-01-06 16:22:09,661 - Epoch: [138][   60/  246]    Overall Loss 0.315577    Objective Loss 0.315577                                        LR 0.000013    Time 0.022632    
2023-01-06 16:22:09,827 - Epoch: [138][   70/  246]    Overall Loss 0.314930    Objective Loss 0.314930                                        LR 0.000013    Time 0.021753    
2023-01-06 16:22:09,993 - Epoch: [138][   80/  246]    Overall Loss 0.315749    Objective Loss 0.315749                                        LR 0.000013    Time 0.021107    
2023-01-06 16:22:10,157 - Epoch: [138][   90/  246]    Overall Loss 0.316225    Objective Loss 0.316225                                        LR 0.000013    Time 0.020584    
2023-01-06 16:22:10,321 - Epoch: [138][  100/  246]    Overall Loss 0.317703    Objective Loss 0.317703                                        LR 0.000013    Time 0.020160    
2023-01-06 16:22:10,485 - Epoch: [138][  110/  246]    Overall Loss 0.315359    Objective Loss 0.315359                                        LR 0.000013    Time 0.019815    
2023-01-06 16:22:10,649 - Epoch: [138][  120/  246]    Overall Loss 0.314501    Objective Loss 0.314501                                        LR 0.000013    Time 0.019532    
2023-01-06 16:22:10,799 - Epoch: [138][  130/  246]    Overall Loss 0.312100    Objective Loss 0.312100                                        LR 0.000013    Time 0.019175    
2023-01-06 16:22:10,957 - Epoch: [138][  140/  246]    Overall Loss 0.311730    Objective Loss 0.311730                                        LR 0.000013    Time 0.018935    
2023-01-06 16:22:11,109 - Epoch: [138][  150/  246]    Overall Loss 0.311599    Objective Loss 0.311599                                        LR 0.000013    Time 0.018683    
2023-01-06 16:22:11,269 - Epoch: [138][  160/  246]    Overall Loss 0.312318    Objective Loss 0.312318                                        LR 0.000013    Time 0.018515    
2023-01-06 16:22:11,425 - Epoch: [138][  170/  246]    Overall Loss 0.311698    Objective Loss 0.311698                                        LR 0.000013    Time 0.018338    
2023-01-06 16:22:11,585 - Epoch: [138][  180/  246]    Overall Loss 0.311175    Objective Loss 0.311175                                        LR 0.000013    Time 0.018210    
2023-01-06 16:22:11,741 - Epoch: [138][  190/  246]    Overall Loss 0.310864    Objective Loss 0.310864                                        LR 0.000013    Time 0.018070    
2023-01-06 16:22:11,897 - Epoch: [138][  200/  246]    Overall Loss 0.310878    Objective Loss 0.310878                                        LR 0.000013    Time 0.017945    
2023-01-06 16:22:12,046 - Epoch: [138][  210/  246]    Overall Loss 0.310925    Objective Loss 0.310925                                        LR 0.000013    Time 0.017799    
2023-01-06 16:22:12,201 - Epoch: [138][  220/  246]    Overall Loss 0.310990    Objective Loss 0.310990                                        LR 0.000013    Time 0.017692    
2023-01-06 16:22:12,356 - Epoch: [138][  230/  246]    Overall Loss 0.310931    Objective Loss 0.310931                                        LR 0.000013    Time 0.017594    
2023-01-06 16:22:12,529 - Epoch: [138][  240/  246]    Overall Loss 0.309783    Objective Loss 0.309783                                        LR 0.000013    Time 0.017582    
2023-01-06 16:22:12,607 - Epoch: [138][  246/  246]    Overall Loss 0.309710    Objective Loss 0.309710    Top1 88.995215    LR 0.000013    Time 0.017467    
2023-01-06 16:22:12,732 - --- validate (epoch=138)-----------
2023-01-06 16:22:12,732 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:13,192 - Epoch: [138][   10/   28]    Loss 0.312415    Top1 88.437500    
2023-01-06 16:22:13,312 - Epoch: [138][   20/   28]    Loss 0.320036    Top1 88.046875    
2023-01-06 16:22:13,368 - Epoch: [138][   28/   28]    Loss 0.309821    Top1 88.491268    
2023-01-06 16:22:13,527 - ==> Top1: 88.491    Loss: 0.310

2023-01-06 16:22:13,528 - ==> Confusion:
[[ 172    8  259]
 [   8  189  405]
 [  52   72 5821]]

2023-01-06 16:22:13,529 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:13,529 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:13,533 - 

2023-01-06 16:22:13,533 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:14,201 - Epoch: [139][   10/  246]    Overall Loss 0.310835    Objective Loss 0.310835                                        LR 0.000013    Time 0.066733    
2023-01-06 16:22:14,343 - Epoch: [139][   20/  246]    Overall Loss 0.311917    Objective Loss 0.311917                                        LR 0.000013    Time 0.040438    
2023-01-06 16:22:14,486 - Epoch: [139][   30/  246]    Overall Loss 0.313218    Objective Loss 0.313218                                        LR 0.000013    Time 0.031705    
2023-01-06 16:22:14,627 - Epoch: [139][   40/  246]    Overall Loss 0.306757    Objective Loss 0.306757                                        LR 0.000013    Time 0.027292    
2023-01-06 16:22:14,767 - Epoch: [139][   50/  246]    Overall Loss 0.308992    Objective Loss 0.308992                                        LR 0.000013    Time 0.024634    
2023-01-06 16:22:14,911 - Epoch: [139][   60/  246]    Overall Loss 0.307401    Objective Loss 0.307401                                        LR 0.000013    Time 0.022922    
2023-01-06 16:22:15,058 - Epoch: [139][   70/  246]    Overall Loss 0.307442    Objective Loss 0.307442                                        LR 0.000013    Time 0.021732    
2023-01-06 16:22:15,201 - Epoch: [139][   80/  246]    Overall Loss 0.309791    Objective Loss 0.309791                                        LR 0.000013    Time 0.020806    
2023-01-06 16:22:15,359 - Epoch: [139][   90/  246]    Overall Loss 0.308387    Objective Loss 0.308387                                        LR 0.000013    Time 0.020248    
2023-01-06 16:22:15,515 - Epoch: [139][  100/  246]    Overall Loss 0.308352    Objective Loss 0.308352                                        LR 0.000013    Time 0.019779    
2023-01-06 16:22:15,676 - Epoch: [139][  110/  246]    Overall Loss 0.307245    Objective Loss 0.307245                                        LR 0.000013    Time 0.019434    
2023-01-06 16:22:15,820 - Epoch: [139][  120/  246]    Overall Loss 0.306987    Objective Loss 0.306987                                        LR 0.000013    Time 0.019012    
2023-01-06 16:22:15,971 - Epoch: [139][  130/  246]    Overall Loss 0.307809    Objective Loss 0.307809                                        LR 0.000013    Time 0.018713    
2023-01-06 16:22:16,121 - Epoch: [139][  140/  246]    Overall Loss 0.307242    Objective Loss 0.307242                                        LR 0.000013    Time 0.018444    
2023-01-06 16:22:16,276 - Epoch: [139][  150/  246]    Overall Loss 0.307441    Objective Loss 0.307441                                        LR 0.000013    Time 0.018241    
2023-01-06 16:22:16,428 - Epoch: [139][  160/  246]    Overall Loss 0.307552    Objective Loss 0.307552                                        LR 0.000013    Time 0.018050    
2023-01-06 16:22:16,589 - Epoch: [139][  170/  246]    Overall Loss 0.307100    Objective Loss 0.307100                                        LR 0.000013    Time 0.017934    
2023-01-06 16:22:16,742 - Epoch: [139][  180/  246]    Overall Loss 0.308394    Objective Loss 0.308394                                        LR 0.000013    Time 0.017786    
2023-01-06 16:22:16,906 - Epoch: [139][  190/  246]    Overall Loss 0.308475    Objective Loss 0.308475                                        LR 0.000013    Time 0.017712    
2023-01-06 16:22:17,066 - Epoch: [139][  200/  246]    Overall Loss 0.308560    Objective Loss 0.308560                                        LR 0.000013    Time 0.017613    
2023-01-06 16:22:17,224 - Epoch: [139][  210/  246]    Overall Loss 0.309424    Objective Loss 0.309424                                        LR 0.000013    Time 0.017528    
2023-01-06 16:22:17,390 - Epoch: [139][  220/  246]    Overall Loss 0.309600    Objective Loss 0.309600                                        LR 0.000013    Time 0.017481    
2023-01-06 16:22:17,564 - Epoch: [139][  230/  246]    Overall Loss 0.309652    Objective Loss 0.309652                                        LR 0.000013    Time 0.017476    
2023-01-06 16:22:17,744 - Epoch: [139][  240/  246]    Overall Loss 0.309808    Objective Loss 0.309808                                        LR 0.000013    Time 0.017496    
2023-01-06 16:22:17,825 - Epoch: [139][  246/  246]    Overall Loss 0.309403    Objective Loss 0.309403    Top1 87.799043    LR 0.000013    Time 0.017399    
2023-01-06 16:22:17,972 - --- validate (epoch=139)-----------
2023-01-06 16:22:17,972 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:18,403 - Epoch: [139][   10/   28]    Loss 0.314650    Top1 87.929688    
2023-01-06 16:22:18,506 - Epoch: [139][   20/   28]    Loss 0.324608    Top1 87.910156    
2023-01-06 16:22:18,567 - Epoch: [139][   28/   28]    Loss 0.310614    Top1 88.519897    
2023-01-06 16:22:18,712 - ==> Top1: 88.520    Loss: 0.311

2023-01-06 16:22:18,712 - ==> Confusion:
[[ 184    7  248]
 [   9  183  410]
 [  65   63 5817]]

2023-01-06 16:22:18,714 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:18,714 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:18,718 - 

2023-01-06 16:22:18,718 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:19,249 - Epoch: [140][   10/  246]    Overall Loss 0.302407    Objective Loss 0.302407                                        LR 0.000008    Time 0.053030    
2023-01-06 16:22:19,403 - Epoch: [140][   20/  246]    Overall Loss 0.307207    Objective Loss 0.307207                                        LR 0.000008    Time 0.034190    
2023-01-06 16:22:19,556 - Epoch: [140][   30/  246]    Overall Loss 0.300334    Objective Loss 0.300334                                        LR 0.000008    Time 0.027875    
2023-01-06 16:22:19,713 - Epoch: [140][   40/  246]    Overall Loss 0.306991    Objective Loss 0.306991                                        LR 0.000008    Time 0.024829    
2023-01-06 16:22:19,874 - Epoch: [140][   50/  246]    Overall Loss 0.303422    Objective Loss 0.303422                                        LR 0.000008    Time 0.023064    
2023-01-06 16:22:20,030 - Epoch: [140][   60/  246]    Overall Loss 0.306418    Objective Loss 0.306418                                        LR 0.000008    Time 0.021823    
2023-01-06 16:22:20,189 - Epoch: [140][   70/  246]    Overall Loss 0.306302    Objective Loss 0.306302                                        LR 0.000008    Time 0.020968    
2023-01-06 16:22:20,359 - Epoch: [140][   80/  246]    Overall Loss 0.306762    Objective Loss 0.306762                                        LR 0.000008    Time 0.020467    
2023-01-06 16:22:20,526 - Epoch: [140][   90/  246]    Overall Loss 0.306474    Objective Loss 0.306474                                        LR 0.000008    Time 0.020046    
2023-01-06 16:22:20,694 - Epoch: [140][  100/  246]    Overall Loss 0.308231    Objective Loss 0.308231                                        LR 0.000008    Time 0.019715    
2023-01-06 16:22:20,859 - Epoch: [140][  110/  246]    Overall Loss 0.309887    Objective Loss 0.309887                                        LR 0.000008    Time 0.019421    
2023-01-06 16:22:21,010 - Epoch: [140][  120/  246]    Overall Loss 0.310039    Objective Loss 0.310039                                        LR 0.000008    Time 0.019056    
2023-01-06 16:22:21,162 - Epoch: [140][  130/  246]    Overall Loss 0.311200    Objective Loss 0.311200                                        LR 0.000008    Time 0.018751    
2023-01-06 16:22:21,321 - Epoch: [140][  140/  246]    Overall Loss 0.311109    Objective Loss 0.311109                                        LR 0.000008    Time 0.018546    
2023-01-06 16:22:21,470 - Epoch: [140][  150/  246]    Overall Loss 0.311409    Objective Loss 0.311409                                        LR 0.000008    Time 0.018301    
2023-01-06 16:22:21,625 - Epoch: [140][  160/  246]    Overall Loss 0.311013    Objective Loss 0.311013                                        LR 0.000008    Time 0.018125    
2023-01-06 16:22:21,786 - Epoch: [140][  170/  246]    Overall Loss 0.310391    Objective Loss 0.310391                                        LR 0.000008    Time 0.018004    
2023-01-06 16:22:21,950 - Epoch: [140][  180/  246]    Overall Loss 0.310568    Objective Loss 0.310568                                        LR 0.000008    Time 0.017911    
2023-01-06 16:22:22,103 - Epoch: [140][  190/  246]    Overall Loss 0.309616    Objective Loss 0.309616                                        LR 0.000008    Time 0.017771    
2023-01-06 16:22:22,245 - Epoch: [140][  200/  246]    Overall Loss 0.309080    Objective Loss 0.309080                                        LR 0.000008    Time 0.017590    
2023-01-06 16:22:22,384 - Epoch: [140][  210/  246]    Overall Loss 0.308569    Objective Loss 0.308569                                        LR 0.000008    Time 0.017413    
2023-01-06 16:22:22,524 - Epoch: [140][  220/  246]    Overall Loss 0.308100    Objective Loss 0.308100                                        LR 0.000008    Time 0.017254    
2023-01-06 16:22:22,660 - Epoch: [140][  230/  246]    Overall Loss 0.308948    Objective Loss 0.308948                                        LR 0.000008    Time 0.017097    
2023-01-06 16:22:22,811 - Epoch: [140][  240/  246]    Overall Loss 0.309340    Objective Loss 0.309340                                        LR 0.000008    Time 0.017011    
2023-01-06 16:22:22,879 - Epoch: [140][  246/  246]    Overall Loss 0.309302    Objective Loss 0.309302    Top1 88.995215    LR 0.000008    Time 0.016870    
2023-01-06 16:22:23,013 - --- validate (epoch=140)-----------
2023-01-06 16:22:23,014 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:23,443 - Epoch: [140][   10/   28]    Loss 0.313995    Top1 88.046875    
2023-01-06 16:22:23,542 - Epoch: [140][   20/   28]    Loss 0.313597    Top1 88.359375    
2023-01-06 16:22:23,597 - Epoch: [140][   28/   28]    Loss 0.312572    Top1 88.462640    
2023-01-06 16:22:23,736 - ==> Top1: 88.463    Loss: 0.313

2023-01-06 16:22:23,737 - ==> Confusion:
[[ 187    6  246]
 [  12  184  406]
 [  67   69 5809]]

2023-01-06 16:22:23,738 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:23,738 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:23,742 - 

2023-01-06 16:22:23,742 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:24,419 - Epoch: [141][   10/  246]    Overall Loss 0.315465    Objective Loss 0.315465                                        LR 0.000008    Time 0.067640    
2023-01-06 16:22:24,581 - Epoch: [141][   20/  246]    Overall Loss 0.314136    Objective Loss 0.314136                                        LR 0.000008    Time 0.041868    
2023-01-06 16:22:24,740 - Epoch: [141][   30/  246]    Overall Loss 0.324611    Objective Loss 0.324611                                        LR 0.000008    Time 0.033186    
2023-01-06 16:22:24,901 - Epoch: [141][   40/  246]    Overall Loss 0.322779    Objective Loss 0.322779                                        LR 0.000008    Time 0.028907    
2023-01-06 16:22:25,063 - Epoch: [141][   50/  246]    Overall Loss 0.317747    Objective Loss 0.317747                                        LR 0.000008    Time 0.026360    
2023-01-06 16:22:25,216 - Epoch: [141][   60/  246]    Overall Loss 0.318107    Objective Loss 0.318107                                        LR 0.000008    Time 0.024518    
2023-01-06 16:22:25,372 - Epoch: [141][   70/  246]    Overall Loss 0.317192    Objective Loss 0.317192                                        LR 0.000008    Time 0.023231    
2023-01-06 16:22:25,538 - Epoch: [141][   80/  246]    Overall Loss 0.316268    Objective Loss 0.316268                                        LR 0.000008    Time 0.022399    
2023-01-06 16:22:25,695 - Epoch: [141][   90/  246]    Overall Loss 0.316300    Objective Loss 0.316300                                        LR 0.000008    Time 0.021645    
2023-01-06 16:22:25,861 - Epoch: [141][  100/  246]    Overall Loss 0.315523    Objective Loss 0.315523                                        LR 0.000008    Time 0.021139    
2023-01-06 16:22:26,008 - Epoch: [141][  110/  246]    Overall Loss 0.315897    Objective Loss 0.315897                                        LR 0.000008    Time 0.020555    
2023-01-06 16:22:26,168 - Epoch: [141][  120/  246]    Overall Loss 0.314613    Objective Loss 0.314613                                        LR 0.000008    Time 0.020170    
2023-01-06 16:22:26,339 - Epoch: [141][  130/  246]    Overall Loss 0.311145    Objective Loss 0.311145                                        LR 0.000008    Time 0.019934    
2023-01-06 16:22:26,501 - Epoch: [141][  140/  246]    Overall Loss 0.310197    Objective Loss 0.310197                                        LR 0.000008    Time 0.019651    
2023-01-06 16:22:26,671 - Epoch: [141][  150/  246]    Overall Loss 0.310408    Objective Loss 0.310408                                        LR 0.000008    Time 0.019469    
2023-01-06 16:22:26,836 - Epoch: [141][  160/  246]    Overall Loss 0.309156    Objective Loss 0.309156                                        LR 0.000008    Time 0.019279    
2023-01-06 16:22:27,002 - Epoch: [141][  170/  246]    Overall Loss 0.309332    Objective Loss 0.309332                                        LR 0.000008    Time 0.019124    
2023-01-06 16:22:27,166 - Epoch: [141][  180/  246]    Overall Loss 0.309526    Objective Loss 0.309526                                        LR 0.000008    Time 0.018966    
2023-01-06 16:22:27,333 - Epoch: [141][  190/  246]    Overall Loss 0.309242    Objective Loss 0.309242                                        LR 0.000008    Time 0.018845    
2023-01-06 16:22:27,499 - Epoch: [141][  200/  246]    Overall Loss 0.309527    Objective Loss 0.309527                                        LR 0.000008    Time 0.018730    
2023-01-06 16:22:27,666 - Epoch: [141][  210/  246]    Overall Loss 0.308844    Objective Loss 0.308844                                        LR 0.000008    Time 0.018632    
2023-01-06 16:22:27,831 - Epoch: [141][  220/  246]    Overall Loss 0.308759    Objective Loss 0.308759                                        LR 0.000008    Time 0.018535    
2023-01-06 16:22:27,997 - Epoch: [141][  230/  246]    Overall Loss 0.309364    Objective Loss 0.309364                                        LR 0.000008    Time 0.018451    
2023-01-06 16:22:28,178 - Epoch: [141][  240/  246]    Overall Loss 0.309118    Objective Loss 0.309118                                        LR 0.000008    Time 0.018434    
2023-01-06 16:22:28,261 - Epoch: [141][  246/  246]    Overall Loss 0.308704    Objective Loss 0.308704    Top1 91.866029    LR 0.000008    Time 0.018319    
2023-01-06 16:22:28,439 - --- validate (epoch=141)-----------
2023-01-06 16:22:28,439 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:28,878 - Epoch: [141][   10/   28]    Loss 0.316630    Top1 88.867188    
2023-01-06 16:22:28,982 - Epoch: [141][   20/   28]    Loss 0.311461    Top1 88.554688    
2023-01-06 16:22:29,044 - Epoch: [141][   28/   28]    Loss 0.307521    Top1 88.634412    
2023-01-06 16:22:29,184 - ==> Top1: 88.634    Loss: 0.308

2023-01-06 16:22:29,184 - ==> Confusion:
[[ 171    7  261]
 [   9  201  392]
 [  50   75 5820]]

2023-01-06 16:22:29,185 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:29,185 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:29,190 - 

2023-01-06 16:22:29,190 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:29,882 - Epoch: [142][   10/  246]    Overall Loss 0.317463    Objective Loss 0.317463                                        LR 0.000008    Time 0.069096    
2023-01-06 16:22:30,044 - Epoch: [142][   20/  246]    Overall Loss 0.315289    Objective Loss 0.315289                                        LR 0.000008    Time 0.042650    
2023-01-06 16:22:30,194 - Epoch: [142][   30/  246]    Overall Loss 0.310269    Objective Loss 0.310269                                        LR 0.000008    Time 0.033402    
2023-01-06 16:22:30,342 - Epoch: [142][   40/  246]    Overall Loss 0.311317    Objective Loss 0.311317                                        LR 0.000008    Time 0.028749    
2023-01-06 16:22:30,490 - Epoch: [142][   50/  246]    Overall Loss 0.310701    Objective Loss 0.310701                                        LR 0.000008    Time 0.025919    
2023-01-06 16:22:30,638 - Epoch: [142][   60/  246]    Overall Loss 0.309076    Objective Loss 0.309076                                        LR 0.000008    Time 0.024075    
2023-01-06 16:22:30,789 - Epoch: [142][   70/  246]    Overall Loss 0.308115    Objective Loss 0.308115                                        LR 0.000008    Time 0.022787    
2023-01-06 16:22:30,935 - Epoch: [142][   80/  246]    Overall Loss 0.307131    Objective Loss 0.307131                                        LR 0.000008    Time 0.021750    
2023-01-06 16:22:31,090 - Epoch: [142][   90/  246]    Overall Loss 0.308671    Objective Loss 0.308671                                        LR 0.000008    Time 0.021050    
2023-01-06 16:22:31,244 - Epoch: [142][  100/  246]    Overall Loss 0.307877    Objective Loss 0.307877                                        LR 0.000008    Time 0.020487    
2023-01-06 16:22:31,406 - Epoch: [142][  110/  246]    Overall Loss 0.308452    Objective Loss 0.308452                                        LR 0.000008    Time 0.020089    
2023-01-06 16:22:31,570 - Epoch: [142][  120/  246]    Overall Loss 0.309166    Objective Loss 0.309166                                        LR 0.000008    Time 0.019778    
2023-01-06 16:22:31,726 - Epoch: [142][  130/  246]    Overall Loss 0.309173    Objective Loss 0.309173                                        LR 0.000008    Time 0.019458    
2023-01-06 16:22:31,877 - Epoch: [142][  140/  246]    Overall Loss 0.307848    Objective Loss 0.307848                                        LR 0.000008    Time 0.019145    
2023-01-06 16:22:32,042 - Epoch: [142][  150/  246]    Overall Loss 0.308173    Objective Loss 0.308173                                        LR 0.000008    Time 0.018963    
2023-01-06 16:22:32,199 - Epoch: [142][  160/  246]    Overall Loss 0.309026    Objective Loss 0.309026                                        LR 0.000008    Time 0.018756    
2023-01-06 16:22:32,359 - Epoch: [142][  170/  246]    Overall Loss 0.309008    Objective Loss 0.309008                                        LR 0.000008    Time 0.018592    
2023-01-06 16:22:32,519 - Epoch: [142][  180/  246]    Overall Loss 0.308985    Objective Loss 0.308985                                        LR 0.000008    Time 0.018450    
2023-01-06 16:22:32,682 - Epoch: [142][  190/  246]    Overall Loss 0.307967    Objective Loss 0.307967                                        LR 0.000008    Time 0.018333    
2023-01-06 16:22:32,840 - Epoch: [142][  200/  246]    Overall Loss 0.308178    Objective Loss 0.308178                                        LR 0.000008    Time 0.018204    
2023-01-06 16:22:33,001 - Epoch: [142][  210/  246]    Overall Loss 0.308656    Objective Loss 0.308656                                        LR 0.000008    Time 0.018105    
2023-01-06 16:22:33,160 - Epoch: [142][  220/  246]    Overall Loss 0.308244    Objective Loss 0.308244                                        LR 0.000008    Time 0.018001    
2023-01-06 16:22:33,325 - Epoch: [142][  230/  246]    Overall Loss 0.308055    Objective Loss 0.308055                                        LR 0.000008    Time 0.017935    
2023-01-06 16:22:33,505 - Epoch: [142][  240/  246]    Overall Loss 0.308078    Objective Loss 0.308078                                        LR 0.000008    Time 0.017936    
2023-01-06 16:22:33,578 - Epoch: [142][  246/  246]    Overall Loss 0.308171    Objective Loss 0.308171    Top1 87.559809    LR 0.000008    Time 0.017796    
2023-01-06 16:22:33,738 - --- validate (epoch=142)-----------
2023-01-06 16:22:33,738 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:34,170 - Epoch: [142][   10/   28]    Loss 0.302492    Top1 88.906250    
2023-01-06 16:22:34,277 - Epoch: [142][   20/   28]    Loss 0.313246    Top1 88.476562    
2023-01-06 16:22:34,335 - Epoch: [142][   28/   28]    Loss 0.307025    Top1 88.605783    
2023-01-06 16:22:34,473 - ==> Top1: 88.606    Loss: 0.307

2023-01-06 16:22:34,473 - ==> Confusion:
[[ 190    6  243]
 [  10  194  398]
 [  64   75 5806]]

2023-01-06 16:22:34,474 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:34,474 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:34,479 - 

2023-01-06 16:22:34,479 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:35,039 - Epoch: [143][   10/  246]    Overall Loss 0.310250    Objective Loss 0.310250                                        LR 0.000008    Time 0.055968    
2023-01-06 16:22:35,208 - Epoch: [143][   20/  246]    Overall Loss 0.309692    Objective Loss 0.309692                                        LR 0.000008    Time 0.036390    
2023-01-06 16:22:35,375 - Epoch: [143][   30/  246]    Overall Loss 0.309745    Objective Loss 0.309745                                        LR 0.000008    Time 0.029835    
2023-01-06 16:22:35,541 - Epoch: [143][   40/  246]    Overall Loss 0.303801    Objective Loss 0.303801                                        LR 0.000008    Time 0.026516    
2023-01-06 16:22:35,710 - Epoch: [143][   50/  246]    Overall Loss 0.302600    Objective Loss 0.302600                                        LR 0.000008    Time 0.024574    
2023-01-06 16:22:35,878 - Epoch: [143][   60/  246]    Overall Loss 0.302648    Objective Loss 0.302648                                        LR 0.000008    Time 0.023275    
2023-01-06 16:22:36,046 - Epoch: [143][   70/  246]    Overall Loss 0.306996    Objective Loss 0.306996                                        LR 0.000008    Time 0.022352    
2023-01-06 16:22:36,220 - Epoch: [143][   80/  246]    Overall Loss 0.308220    Objective Loss 0.308220                                        LR 0.000008    Time 0.021723    
2023-01-06 16:22:36,393 - Epoch: [143][   90/  246]    Overall Loss 0.305387    Objective Loss 0.305387                                        LR 0.000008    Time 0.021232    
2023-01-06 16:22:36,547 - Epoch: [143][  100/  246]    Overall Loss 0.303418    Objective Loss 0.303418                                        LR 0.000008    Time 0.020642    
2023-01-06 16:22:36,695 - Epoch: [143][  110/  246]    Overall Loss 0.305218    Objective Loss 0.305218                                        LR 0.000008    Time 0.020105    
2023-01-06 16:22:36,845 - Epoch: [143][  120/  246]    Overall Loss 0.307389    Objective Loss 0.307389                                        LR 0.000008    Time 0.019674    
2023-01-06 16:22:37,009 - Epoch: [143][  130/  246]    Overall Loss 0.307677    Objective Loss 0.307677                                        LR 0.000008    Time 0.019415    
2023-01-06 16:22:37,180 - Epoch: [143][  140/  246]    Overall Loss 0.307756    Objective Loss 0.307756                                        LR 0.000008    Time 0.019250    
2023-01-06 16:22:37,351 - Epoch: [143][  150/  246]    Overall Loss 0.307547    Objective Loss 0.307547                                        LR 0.000008    Time 0.019100    
2023-01-06 16:22:37,518 - Epoch: [143][  160/  246]    Overall Loss 0.308344    Objective Loss 0.308344                                        LR 0.000008    Time 0.018952    
2023-01-06 16:22:37,675 - Epoch: [143][  170/  246]    Overall Loss 0.307872    Objective Loss 0.307872                                        LR 0.000008    Time 0.018757    
2023-01-06 16:22:37,831 - Epoch: [143][  180/  246]    Overall Loss 0.307526    Objective Loss 0.307526                                        LR 0.000008    Time 0.018575    
2023-01-06 16:22:38,003 - Epoch: [143][  190/  246]    Overall Loss 0.306785    Objective Loss 0.306785                                        LR 0.000008    Time 0.018499    
2023-01-06 16:22:38,177 - Epoch: [143][  200/  246]    Overall Loss 0.307523    Objective Loss 0.307523                                        LR 0.000008    Time 0.018442    
2023-01-06 16:22:38,337 - Epoch: [143][  210/  246]    Overall Loss 0.307942    Objective Loss 0.307942                                        LR 0.000008    Time 0.018318    
2023-01-06 16:22:38,490 - Epoch: [143][  220/  246]    Overall Loss 0.307558    Objective Loss 0.307558                                        LR 0.000008    Time 0.018180    
2023-01-06 16:22:38,665 - Epoch: [143][  230/  246]    Overall Loss 0.308586    Objective Loss 0.308586                                        LR 0.000008    Time 0.018145    
2023-01-06 16:22:38,855 - Epoch: [143][  240/  246]    Overall Loss 0.308382    Objective Loss 0.308382                                        LR 0.000008    Time 0.018179    
2023-01-06 16:22:38,934 - Epoch: [143][  246/  246]    Overall Loss 0.308340    Objective Loss 0.308340    Top1 88.038278    LR 0.000008    Time 0.018060    
2023-01-06 16:22:39,072 - --- validate (epoch=143)-----------
2023-01-06 16:22:39,072 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:39,508 - Epoch: [143][   10/   28]    Loss 0.304346    Top1 88.984375    
2023-01-06 16:22:39,618 - Epoch: [143][   20/   28]    Loss 0.314578    Top1 88.320312    
2023-01-06 16:22:39,675 - Epoch: [143][   28/   28]    Loss 0.308896    Top1 88.534211    
2023-01-06 16:22:39,812 - ==> Top1: 88.534    Loss: 0.309

2023-01-06 16:22:39,812 - ==> Confusion:
[[ 180    7  252]
 [   8  194  400]
 [  61   73 5811]]

2023-01-06 16:22:39,813 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:39,813 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:39,818 - 

2023-01-06 16:22:39,818 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:40,500 - Epoch: [144][   10/  246]    Overall Loss 0.313479    Objective Loss 0.313479                                        LR 0.000008    Time 0.068209    
2023-01-06 16:22:40,664 - Epoch: [144][   20/  246]    Overall Loss 0.317020    Objective Loss 0.317020                                        LR 0.000008    Time 0.042266    
2023-01-06 16:22:40,837 - Epoch: [144][   30/  246]    Overall Loss 0.312869    Objective Loss 0.312869                                        LR 0.000008    Time 0.033937    
2023-01-06 16:22:41,011 - Epoch: [144][   40/  246]    Overall Loss 0.307432    Objective Loss 0.307432                                        LR 0.000008    Time 0.029781    
2023-01-06 16:22:41,186 - Epoch: [144][   50/  246]    Overall Loss 0.308185    Objective Loss 0.308185                                        LR 0.000008    Time 0.027328    
2023-01-06 16:22:41,362 - Epoch: [144][   60/  246]    Overall Loss 0.313034    Objective Loss 0.313034                                        LR 0.000008    Time 0.025687    
2023-01-06 16:22:41,544 - Epoch: [144][   70/  246]    Overall Loss 0.311045    Objective Loss 0.311045                                        LR 0.000008    Time 0.024615    
2023-01-06 16:22:41,717 - Epoch: [144][   80/  246]    Overall Loss 0.309008    Objective Loss 0.309008                                        LR 0.000008    Time 0.023700    
2023-01-06 16:22:41,896 - Epoch: [144][   90/  246]    Overall Loss 0.308917    Objective Loss 0.308917                                        LR 0.000008    Time 0.023051    
2023-01-06 16:22:42,059 - Epoch: [144][  100/  246]    Overall Loss 0.307034    Objective Loss 0.307034                                        LR 0.000008    Time 0.022365    
2023-01-06 16:22:42,222 - Epoch: [144][  110/  246]    Overall Loss 0.309075    Objective Loss 0.309075                                        LR 0.000008    Time 0.021800    
2023-01-06 16:22:42,389 - Epoch: [144][  120/  246]    Overall Loss 0.308904    Objective Loss 0.308904                                        LR 0.000008    Time 0.021377    
2023-01-06 16:22:42,547 - Epoch: [144][  130/  246]    Overall Loss 0.308069    Objective Loss 0.308069                                        LR 0.000008    Time 0.020932    
2023-01-06 16:22:42,699 - Epoch: [144][  140/  246]    Overall Loss 0.308353    Objective Loss 0.308353                                        LR 0.000008    Time 0.020523    
2023-01-06 16:22:42,856 - Epoch: [144][  150/  246]    Overall Loss 0.309071    Objective Loss 0.309071                                        LR 0.000008    Time 0.020195    
2023-01-06 16:22:43,028 - Epoch: [144][  160/  246]    Overall Loss 0.309105    Objective Loss 0.309105                                        LR 0.000008    Time 0.020005    
2023-01-06 16:22:43,203 - Epoch: [144][  170/  246]    Overall Loss 0.308762    Objective Loss 0.308762                                        LR 0.000008    Time 0.019858    
2023-01-06 16:22:43,375 - Epoch: [144][  180/  246]    Overall Loss 0.309332    Objective Loss 0.309332                                        LR 0.000008    Time 0.019707    
2023-01-06 16:22:43,550 - Epoch: [144][  190/  246]    Overall Loss 0.309150    Objective Loss 0.309150                                        LR 0.000008    Time 0.019588    
2023-01-06 16:22:43,721 - Epoch: [144][  200/  246]    Overall Loss 0.308589    Objective Loss 0.308589                                        LR 0.000008    Time 0.019463    
2023-01-06 16:22:43,893 - Epoch: [144][  210/  246]    Overall Loss 0.309220    Objective Loss 0.309220                                        LR 0.000008    Time 0.019356    
2023-01-06 16:22:44,062 - Epoch: [144][  220/  246]    Overall Loss 0.309500    Objective Loss 0.309500                                        LR 0.000008    Time 0.019235    
2023-01-06 16:22:44,233 - Epoch: [144][  230/  246]    Overall Loss 0.309476    Objective Loss 0.309476                                        LR 0.000008    Time 0.019137    
2023-01-06 16:22:44,415 - Epoch: [144][  240/  246]    Overall Loss 0.309068    Objective Loss 0.309068                                        LR 0.000008    Time 0.019098    
2023-01-06 16:22:44,497 - Epoch: [144][  246/  246]    Overall Loss 0.308448    Objective Loss 0.308448    Top1 88.038278    LR 0.000008    Time 0.018964    
2023-01-06 16:22:44,672 - --- validate (epoch=144)-----------
2023-01-06 16:22:44,673 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:45,104 - Epoch: [144][   10/   28]    Loss 0.307227    Top1 88.125000    
2023-01-06 16:22:45,207 - Epoch: [144][   20/   28]    Loss 0.306088    Top1 88.339844    
2023-01-06 16:22:45,264 - Epoch: [144][   28/   28]    Loss 0.308603    Top1 88.391068    
2023-01-06 16:22:45,407 - ==> Top1: 88.391    Loss: 0.309

2023-01-06 16:22:45,407 - ==> Confusion:
[[ 182    7  250]
 [   9  177  416]
 [  65   64 5816]]

2023-01-06 16:22:45,408 - ==> Best [Top1: 88.677   Sparsity:0.00   Params: 46192 on epoch: 129]
2023-01-06 16:22:45,409 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:45,413 - 

2023-01-06 16:22:45,413 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:45,953 - Epoch: [145][   10/  246]    Overall Loss 0.325635    Objective Loss 0.325635                                        LR 0.000008    Time 0.053920    
2023-01-06 16:22:46,116 - Epoch: [145][   20/  246]    Overall Loss 0.312430    Objective Loss 0.312430                                        LR 0.000008    Time 0.035100    
2023-01-06 16:22:46,281 - Epoch: [145][   30/  246]    Overall Loss 0.308965    Objective Loss 0.308965                                        LR 0.000008    Time 0.028868    
2023-01-06 16:22:46,449 - Epoch: [145][   40/  246]    Overall Loss 0.310707    Objective Loss 0.310707                                        LR 0.000008    Time 0.025850    
2023-01-06 16:22:46,612 - Epoch: [145][   50/  246]    Overall Loss 0.310673    Objective Loss 0.310673                                        LR 0.000008    Time 0.023906    
2023-01-06 16:22:46,775 - Epoch: [145][   60/  246]    Overall Loss 0.310014    Objective Loss 0.310014                                        LR 0.000008    Time 0.022643    
2023-01-06 16:22:46,943 - Epoch: [145][   70/  246]    Overall Loss 0.308522    Objective Loss 0.308522                                        LR 0.000008    Time 0.021802    
2023-01-06 16:22:47,117 - Epoch: [145][   80/  246]    Overall Loss 0.306625    Objective Loss 0.306625                                        LR 0.000008    Time 0.021243    
2023-01-06 16:22:47,285 - Epoch: [145][   90/  246]    Overall Loss 0.308101    Objective Loss 0.308101                                        LR 0.000008    Time 0.020741    
2023-01-06 16:22:47,456 - Epoch: [145][  100/  246]    Overall Loss 0.310235    Objective Loss 0.310235                                        LR 0.000008    Time 0.020373    
2023-01-06 16:22:47,620 - Epoch: [145][  110/  246]    Overall Loss 0.309492    Objective Loss 0.309492                                        LR 0.000008    Time 0.020016    
2023-01-06 16:22:47,780 - Epoch: [145][  120/  246]    Overall Loss 0.308170    Objective Loss 0.308170                                        LR 0.000008    Time 0.019676    
2023-01-06 16:22:47,946 - Epoch: [145][  130/  246]    Overall Loss 0.307531    Objective Loss 0.307531                                        LR 0.000008    Time 0.019432    
2023-01-06 16:22:48,119 - Epoch: [145][  140/  246]    Overall Loss 0.308959    Objective Loss 0.308959                                        LR 0.000008    Time 0.019278    
2023-01-06 16:22:48,288 - Epoch: [145][  150/  246]    Overall Loss 0.310022    Objective Loss 0.310022                                        LR 0.000008    Time 0.019118    
2023-01-06 16:22:48,450 - Epoch: [145][  160/  246]    Overall Loss 0.309717    Objective Loss 0.309717                                        LR 0.000008    Time 0.018933    
2023-01-06 16:22:48,614 - Epoch: [145][  170/  246]    Overall Loss 0.309068    Objective Loss 0.309068                                        LR 0.000008    Time 0.018785    
2023-01-06 16:22:48,777 - Epoch: [145][  180/  246]    Overall Loss 0.308773    Objective Loss 0.308773                                        LR 0.000008    Time 0.018647    
2023-01-06 16:22:48,941 - Epoch: [145][  190/  246]    Overall Loss 0.310030    Objective Loss 0.310030                                        LR 0.000008    Time 0.018526    
2023-01-06 16:22:49,105 - Epoch: [145][  200/  246]    Overall Loss 0.309356    Objective Loss 0.309356                                        LR 0.000008    Time 0.018417    
2023-01-06 16:22:49,265 - Epoch: [145][  210/  246]    Overall Loss 0.309209    Objective Loss 0.309209                                        LR 0.000008    Time 0.018301    
2023-01-06 16:22:49,428 - Epoch: [145][  220/  246]    Overall Loss 0.308742    Objective Loss 0.308742                                        LR 0.000008    Time 0.018207    
2023-01-06 16:22:49,591 - Epoch: [145][  230/  246]    Overall Loss 0.308484    Objective Loss 0.308484                                        LR 0.000008    Time 0.018125    
2023-01-06 16:22:49,764 - Epoch: [145][  240/  246]    Overall Loss 0.307969    Objective Loss 0.307969                                        LR 0.000008    Time 0.018090    
2023-01-06 16:22:49,847 - Epoch: [145][  246/  246]    Overall Loss 0.308248    Objective Loss 0.308248    Top1 89.234450    LR 0.000008    Time 0.017985    
2023-01-06 16:22:49,984 - --- validate (epoch=145)-----------
2023-01-06 16:22:49,984 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:50,424 - Epoch: [145][   10/   28]    Loss 0.332963    Top1 87.968750    
2023-01-06 16:22:50,539 - Epoch: [145][   20/   28]    Loss 0.310290    Top1 88.750000    
2023-01-06 16:22:50,595 - Epoch: [145][   28/   28]    Loss 0.314976    Top1 88.777555    
2023-01-06 16:22:50,737 - ==> Top1: 88.778    Loss: 0.315

2023-01-06 16:22:50,737 - ==> Confusion:
[[ 178    7  254]
 [  11  204  387]
 [  51   74 5820]]

2023-01-06 16:22:50,739 - ==> Best [Top1: 88.778   Sparsity:0.00   Params: 46192 on epoch: 145]
2023-01-06 16:22:50,739 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:50,744 - 

2023-01-06 16:22:50,744 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:51,431 - Epoch: [146][   10/  246]    Overall Loss 0.310831    Objective Loss 0.310831                                        LR 0.000008    Time 0.068611    
2023-01-06 16:22:51,603 - Epoch: [146][   20/  246]    Overall Loss 0.326552    Objective Loss 0.326552                                        LR 0.000008    Time 0.042863    
2023-01-06 16:22:51,750 - Epoch: [146][   30/  246]    Overall Loss 0.318291    Objective Loss 0.318291                                        LR 0.000008    Time 0.033475    
2023-01-06 16:22:51,888 - Epoch: [146][   40/  246]    Overall Loss 0.315227    Objective Loss 0.315227                                        LR 0.000008    Time 0.028562    
2023-01-06 16:22:52,042 - Epoch: [146][   50/  246]    Overall Loss 0.311414    Objective Loss 0.311414                                        LR 0.000008    Time 0.025919    
2023-01-06 16:22:52,197 - Epoch: [146][   60/  246]    Overall Loss 0.307063    Objective Loss 0.307063                                        LR 0.000008    Time 0.024165    
2023-01-06 16:22:52,350 - Epoch: [146][   70/  246]    Overall Loss 0.308277    Objective Loss 0.308277                                        LR 0.000008    Time 0.022895    
2023-01-06 16:22:52,500 - Epoch: [146][   80/  246]    Overall Loss 0.307967    Objective Loss 0.307967                                        LR 0.000008    Time 0.021910    
2023-01-06 16:22:52,652 - Epoch: [146][   90/  246]    Overall Loss 0.307776    Objective Loss 0.307776                                        LR 0.000008    Time 0.021152    
2023-01-06 16:22:52,803 - Epoch: [146][  100/  246]    Overall Loss 0.308478    Objective Loss 0.308478                                        LR 0.000008    Time 0.020547    
2023-01-06 16:22:52,953 - Epoch: [146][  110/  246]    Overall Loss 0.308913    Objective Loss 0.308913                                        LR 0.000008    Time 0.020039    
2023-01-06 16:22:53,106 - Epoch: [146][  120/  246]    Overall Loss 0.311237    Objective Loss 0.311237                                        LR 0.000008    Time 0.019643    
2023-01-06 16:22:53,265 - Epoch: [146][  130/  246]    Overall Loss 0.312036    Objective Loss 0.312036                                        LR 0.000008    Time 0.019342    
2023-01-06 16:22:53,426 - Epoch: [146][  140/  246]    Overall Loss 0.310599    Objective Loss 0.310599                                        LR 0.000008    Time 0.019109    
2023-01-06 16:22:53,582 - Epoch: [146][  150/  246]    Overall Loss 0.310411    Objective Loss 0.310411                                        LR 0.000008    Time 0.018873    
2023-01-06 16:22:53,739 - Epoch: [146][  160/  246]    Overall Loss 0.310433    Objective Loss 0.310433                                        LR 0.000008    Time 0.018670    
2023-01-06 16:22:53,890 - Epoch: [146][  170/  246]    Overall Loss 0.309949    Objective Loss 0.309949                                        LR 0.000008    Time 0.018457    
2023-01-06 16:22:54,043 - Epoch: [146][  180/  246]    Overall Loss 0.308536    Objective Loss 0.308536                                        LR 0.000008    Time 0.018280    
2023-01-06 16:22:54,200 - Epoch: [146][  190/  246]    Overall Loss 0.308600    Objective Loss 0.308600                                        LR 0.000008    Time 0.018145    
2023-01-06 16:22:54,364 - Epoch: [146][  200/  246]    Overall Loss 0.309910    Objective Loss 0.309910                                        LR 0.000008    Time 0.018057    
2023-01-06 16:22:54,552 - Epoch: [146][  210/  246]    Overall Loss 0.309706    Objective Loss 0.309706                                        LR 0.000008    Time 0.018088    
2023-01-06 16:22:54,736 - Epoch: [146][  220/  246]    Overall Loss 0.308736    Objective Loss 0.308736                                        LR 0.000008    Time 0.018100    
2023-01-06 16:22:54,928 - Epoch: [146][  230/  246]    Overall Loss 0.308559    Objective Loss 0.308559                                        LR 0.000008    Time 0.018150    
2023-01-06 16:22:55,151 - Epoch: [146][  240/  246]    Overall Loss 0.307887    Objective Loss 0.307887                                        LR 0.000008    Time 0.018318    
2023-01-06 16:22:55,246 - Epoch: [146][  246/  246]    Overall Loss 0.307385    Objective Loss 0.307385    Top1 90.191388    LR 0.000008    Time 0.018255    
2023-01-06 16:22:55,393 - --- validate (epoch=146)-----------
2023-01-06 16:22:55,393 - 6986 samples (256 per mini-batch)
2023-01-06 16:22:55,835 - Epoch: [146][   10/   28]    Loss 0.322628    Top1 88.750000    
2023-01-06 16:22:55,940 - Epoch: [146][   20/   28]    Loss 0.317999    Top1 88.554688    
2023-01-06 16:22:55,997 - Epoch: [146][   28/   28]    Loss 0.313068    Top1 88.620097    
2023-01-06 16:22:56,142 - ==> Top1: 88.620    Loss: 0.313

2023-01-06 16:22:56,143 - ==> Confusion:
[[ 166    7  266]
 [   8  192  402]
 [  41   71 5833]]

2023-01-06 16:22:56,144 - ==> Best [Top1: 88.778   Sparsity:0.00   Params: 46192 on epoch: 145]
2023-01-06 16:22:56,144 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:22:56,148 - 

2023-01-06 16:22:56,149 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:22:56,864 - Epoch: [147][   10/  246]    Overall Loss 0.306975    Objective Loss 0.306975                                        LR 0.000008    Time 0.071503    
2023-01-06 16:22:57,056 - Epoch: [147][   20/  246]    Overall Loss 0.307366    Objective Loss 0.307366                                        LR 0.000008    Time 0.045316    
2023-01-06 16:22:57,252 - Epoch: [147][   30/  246]    Overall Loss 0.310834    Objective Loss 0.310834                                        LR 0.000008    Time 0.036712    
2023-01-06 16:22:57,431 - Epoch: [147][   40/  246]    Overall Loss 0.311035    Objective Loss 0.311035                                        LR 0.000008    Time 0.032004    
2023-01-06 16:22:57,626 - Epoch: [147][   50/  246]    Overall Loss 0.306398    Objective Loss 0.306398                                        LR 0.000008    Time 0.029497    
2023-01-06 16:22:57,823 - Epoch: [147][   60/  246]    Overall Loss 0.307059    Objective Loss 0.307059                                        LR 0.000008    Time 0.027854    
2023-01-06 16:22:58,016 - Epoch: [147][   70/  246]    Overall Loss 0.308328    Objective Loss 0.308328                                        LR 0.000008    Time 0.026635    
2023-01-06 16:22:58,171 - Epoch: [147][   80/  246]    Overall Loss 0.306654    Objective Loss 0.306654                                        LR 0.000008    Time 0.025236    
2023-01-06 16:22:58,310 - Epoch: [147][   90/  246]    Overall Loss 0.308656    Objective Loss 0.308656                                        LR 0.000008    Time 0.023967    
2023-01-06 16:22:58,448 - Epoch: [147][  100/  246]    Overall Loss 0.308953    Objective Loss 0.308953                                        LR 0.000008    Time 0.022952    
2023-01-06 16:22:58,584 - Epoch: [147][  110/  246]    Overall Loss 0.308209    Objective Loss 0.308209                                        LR 0.000008    Time 0.022095    
2023-01-06 16:22:58,720 - Epoch: [147][  120/  246]    Overall Loss 0.308844    Objective Loss 0.308844                                        LR 0.000008    Time 0.021383    
2023-01-06 16:22:58,855 - Epoch: [147][  130/  246]    Overall Loss 0.308893    Objective Loss 0.308893                                        LR 0.000008    Time 0.020773    
2023-01-06 16:22:58,992 - Epoch: [147][  140/  246]    Overall Loss 0.307067    Objective Loss 0.307067                                        LR 0.000008    Time 0.020267    
2023-01-06 16:22:59,128 - Epoch: [147][  150/  246]    Overall Loss 0.307273    Objective Loss 0.307273                                        LR 0.000008    Time 0.019819    
2023-01-06 16:22:59,264 - Epoch: [147][  160/  246]    Overall Loss 0.307411    Objective Loss 0.307411                                        LR 0.000008    Time 0.019425    
2023-01-06 16:22:59,401 - Epoch: [147][  170/  246]    Overall Loss 0.307208    Objective Loss 0.307208                                        LR 0.000008    Time 0.019084    
2023-01-06 16:22:59,536 - Epoch: [147][  180/  246]    Overall Loss 0.307095    Objective Loss 0.307095                                        LR 0.000008    Time 0.018772    
2023-01-06 16:22:59,672 - Epoch: [147][  190/  246]    Overall Loss 0.308221    Objective Loss 0.308221                                        LR 0.000008    Time 0.018497    
2023-01-06 16:22:59,809 - Epoch: [147][  200/  246]    Overall Loss 0.308364    Objective Loss 0.308364                                        LR 0.000008    Time 0.018255    
2023-01-06 16:22:59,944 - Epoch: [147][  210/  246]    Overall Loss 0.308095    Objective Loss 0.308095                                        LR 0.000008    Time 0.018028    
2023-01-06 16:23:00,082 - Epoch: [147][  220/  246]    Overall Loss 0.308321    Objective Loss 0.308321                                        LR 0.000008    Time 0.017834    
2023-01-06 16:23:00,225 - Epoch: [147][  230/  246]    Overall Loss 0.308523    Objective Loss 0.308523                                        LR 0.000008    Time 0.017679    
2023-01-06 16:23:00,397 - Epoch: [147][  240/  246]    Overall Loss 0.308824    Objective Loss 0.308824                                        LR 0.000008    Time 0.017657    
2023-01-06 16:23:00,475 - Epoch: [147][  246/  246]    Overall Loss 0.308239    Objective Loss 0.308239    Top1 89.952153    LR 0.000008    Time 0.017541    
2023-01-06 16:23:00,616 - --- validate (epoch=147)-----------
2023-01-06 16:23:00,616 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:01,061 - Epoch: [147][   10/   28]    Loss 0.316195    Top1 88.359375    
2023-01-06 16:23:01,171 - Epoch: [147][   20/   28]    Loss 0.303510    Top1 88.769531    
2023-01-06 16:23:01,226 - Epoch: [147][   28/   28]    Loss 0.311587    Top1 88.348125    
2023-01-06 16:23:01,388 - ==> Top1: 88.348    Loss: 0.312

2023-01-06 16:23:01,388 - ==> Confusion:
[[ 149    6  284]
 [   8  175  419]
 [  37   60 5848]]

2023-01-06 16:23:01,389 - ==> Best [Top1: 88.778   Sparsity:0.00   Params: 46192 on epoch: 145]
2023-01-06 16:23:01,389 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:01,394 - 

2023-01-06 16:23:01,394 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:01,937 - Epoch: [148][   10/  246]    Overall Loss 0.290868    Objective Loss 0.290868                                        LR 0.000008    Time 0.054249    
2023-01-06 16:23:02,091 - Epoch: [148][   20/  246]    Overall Loss 0.311476    Objective Loss 0.311476                                        LR 0.000008    Time 0.034797    
2023-01-06 16:23:02,241 - Epoch: [148][   30/  246]    Overall Loss 0.306373    Objective Loss 0.306373                                        LR 0.000008    Time 0.028196    
2023-01-06 16:23:02,388 - Epoch: [148][   40/  246]    Overall Loss 0.306831    Objective Loss 0.306831                                        LR 0.000008    Time 0.024755    
2023-01-06 16:23:02,531 - Epoch: [148][   50/  246]    Overall Loss 0.309121    Objective Loss 0.309121                                        LR 0.000008    Time 0.022667    
2023-01-06 16:23:02,673 - Epoch: [148][   60/  246]    Overall Loss 0.309633    Objective Loss 0.309633                                        LR 0.000008    Time 0.021241    
2023-01-06 16:23:02,818 - Epoch: [148][   70/  246]    Overall Loss 0.308951    Objective Loss 0.308951                                        LR 0.000008    Time 0.020280    
2023-01-06 16:23:02,962 - Epoch: [148][   80/  246]    Overall Loss 0.311399    Objective Loss 0.311399                                        LR 0.000008    Time 0.019538    
2023-01-06 16:23:03,127 - Epoch: [148][   90/  246]    Overall Loss 0.309175    Objective Loss 0.309175                                        LR 0.000008    Time 0.019197    
2023-01-06 16:23:03,292 - Epoch: [148][  100/  246]    Overall Loss 0.308880    Objective Loss 0.308880                                        LR 0.000008    Time 0.018923    
2023-01-06 16:23:03,457 - Epoch: [148][  110/  246]    Overall Loss 0.311334    Objective Loss 0.311334                                        LR 0.000008    Time 0.018695    
2023-01-06 16:23:03,622 - Epoch: [148][  120/  246]    Overall Loss 0.311392    Objective Loss 0.311392                                        LR 0.000008    Time 0.018511    
2023-01-06 16:23:03,787 - Epoch: [148][  130/  246]    Overall Loss 0.311464    Objective Loss 0.311464                                        LR 0.000008    Time 0.018352    
2023-01-06 16:23:03,952 - Epoch: [148][  140/  246]    Overall Loss 0.311356    Objective Loss 0.311356                                        LR 0.000008    Time 0.018218    
2023-01-06 16:23:04,118 - Epoch: [148][  150/  246]    Overall Loss 0.309697    Objective Loss 0.309697                                        LR 0.000008    Time 0.018109    
2023-01-06 16:23:04,290 - Epoch: [148][  160/  246]    Overall Loss 0.309842    Objective Loss 0.309842                                        LR 0.000008    Time 0.018048    
2023-01-06 16:23:04,462 - Epoch: [148][  170/  246]    Overall Loss 0.308892    Objective Loss 0.308892                                        LR 0.000008    Time 0.017996    
2023-01-06 16:23:04,640 - Epoch: [148][  180/  246]    Overall Loss 0.308543    Objective Loss 0.308543                                        LR 0.000008    Time 0.017982    
2023-01-06 16:23:04,823 - Epoch: [148][  190/  246]    Overall Loss 0.309026    Objective Loss 0.309026                                        LR 0.000008    Time 0.017999    
2023-01-06 16:23:04,994 - Epoch: [148][  200/  246]    Overall Loss 0.309761    Objective Loss 0.309761                                        LR 0.000008    Time 0.017952    
2023-01-06 16:23:05,167 - Epoch: [148][  210/  246]    Overall Loss 0.309992    Objective Loss 0.309992                                        LR 0.000008    Time 0.017920    
2023-01-06 16:23:05,339 - Epoch: [148][  220/  246]    Overall Loss 0.309070    Objective Loss 0.309070                                        LR 0.000008    Time 0.017885    
2023-01-06 16:23:05,512 - Epoch: [148][  230/  246]    Overall Loss 0.309445    Objective Loss 0.309445                                        LR 0.000008    Time 0.017857    
2023-01-06 16:23:05,694 - Epoch: [148][  240/  246]    Overall Loss 0.307808    Objective Loss 0.307808                                        LR 0.000008    Time 0.017872    
2023-01-06 16:23:05,776 - Epoch: [148][  246/  246]    Overall Loss 0.307598    Objective Loss 0.307598    Top1 88.755981    LR 0.000008    Time 0.017767    
2023-01-06 16:23:05,899 - --- validate (epoch=148)-----------
2023-01-06 16:23:05,899 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:06,334 - Epoch: [148][   10/   28]    Loss 0.295848    Top1 89.023438    
2023-01-06 16:23:06,449 - Epoch: [148][   20/   28]    Loss 0.312605    Top1 88.730469    
2023-01-06 16:23:06,506 - Epoch: [148][   28/   28]    Loss 0.307643    Top1 88.877756    
2023-01-06 16:23:06,664 - ==> Top1: 88.878    Loss: 0.308

2023-01-06 16:23:06,664 - ==> Confusion:
[[ 178    8  253]
 [  11  213  378]
 [  49   78 5818]]

2023-01-06 16:23:06,666 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:06,666 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:06,671 - 

2023-01-06 16:23:06,671 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:07,352 - Epoch: [149][   10/  246]    Overall Loss 0.294502    Objective Loss 0.294502                                        LR 0.000008    Time 0.068074    
2023-01-06 16:23:07,532 - Epoch: [149][   20/  246]    Overall Loss 0.300356    Objective Loss 0.300356                                        LR 0.000008    Time 0.043007    
2023-01-06 16:23:07,703 - Epoch: [149][   30/  246]    Overall Loss 0.303349    Objective Loss 0.303349                                        LR 0.000008    Time 0.034309    
2023-01-06 16:23:07,882 - Epoch: [149][   40/  246]    Overall Loss 0.307103    Objective Loss 0.307103                                        LR 0.000008    Time 0.030188    
2023-01-06 16:23:08,048 - Epoch: [149][   50/  246]    Overall Loss 0.309152    Objective Loss 0.309152                                        LR 0.000008    Time 0.027467    
2023-01-06 16:23:08,213 - Epoch: [149][   60/  246]    Overall Loss 0.307052    Objective Loss 0.307052                                        LR 0.000008    Time 0.025636    
2023-01-06 16:23:08,379 - Epoch: [149][   70/  246]    Overall Loss 0.306974    Objective Loss 0.306974                                        LR 0.000008    Time 0.024334    
2023-01-06 16:23:08,543 - Epoch: [149][   80/  246]    Overall Loss 0.305226    Objective Loss 0.305226                                        LR 0.000008    Time 0.023341    
2023-01-06 16:23:08,708 - Epoch: [149][   90/  246]    Overall Loss 0.306457    Objective Loss 0.306457                                        LR 0.000008    Time 0.022576    
2023-01-06 16:23:08,882 - Epoch: [149][  100/  246]    Overall Loss 0.306867    Objective Loss 0.306867                                        LR 0.000008    Time 0.022052    
2023-01-06 16:23:09,057 - Epoch: [149][  110/  246]    Overall Loss 0.307441    Objective Loss 0.307441                                        LR 0.000008    Time 0.021631    
2023-01-06 16:23:09,226 - Epoch: [149][  120/  246]    Overall Loss 0.307841    Objective Loss 0.307841                                        LR 0.000008    Time 0.021220    
2023-01-06 16:23:09,401 - Epoch: [149][  130/  246]    Overall Loss 0.308616    Objective Loss 0.308616                                        LR 0.000008    Time 0.020934    
2023-01-06 16:23:09,573 - Epoch: [149][  140/  246]    Overall Loss 0.308426    Objective Loss 0.308426                                        LR 0.000008    Time 0.020666    
2023-01-06 16:23:09,748 - Epoch: [149][  150/  246]    Overall Loss 0.309363    Objective Loss 0.309363                                        LR 0.000008    Time 0.020437    
2023-01-06 16:23:09,920 - Epoch: [149][  160/  246]    Overall Loss 0.308756    Objective Loss 0.308756                                        LR 0.000008    Time 0.020225    
2023-01-06 16:23:10,093 - Epoch: [149][  170/  246]    Overall Loss 0.309131    Objective Loss 0.309131                                        LR 0.000008    Time 0.020053    
2023-01-06 16:23:10,265 - Epoch: [149][  180/  246]    Overall Loss 0.309591    Objective Loss 0.309591                                        LR 0.000008    Time 0.019878    
2023-01-06 16:23:10,441 - Epoch: [149][  190/  246]    Overall Loss 0.309444    Objective Loss 0.309444                                        LR 0.000008    Time 0.019756    
2023-01-06 16:23:10,611 - Epoch: [149][  200/  246]    Overall Loss 0.308670    Objective Loss 0.308670                                        LR 0.000008    Time 0.019610    
2023-01-06 16:23:10,789 - Epoch: [149][  210/  246]    Overall Loss 0.308935    Objective Loss 0.308935                                        LR 0.000008    Time 0.019524    
2023-01-06 16:23:10,958 - Epoch: [149][  220/  246]    Overall Loss 0.308680    Objective Loss 0.308680                                        LR 0.000008    Time 0.019394    
2023-01-06 16:23:11,135 - Epoch: [149][  230/  246]    Overall Loss 0.308369    Objective Loss 0.308369                                        LR 0.000008    Time 0.019317    
2023-01-06 16:23:11,315 - Epoch: [149][  240/  246]    Overall Loss 0.307886    Objective Loss 0.307886                                        LR 0.000008    Time 0.019254    
2023-01-06 16:23:11,398 - Epoch: [149][  246/  246]    Overall Loss 0.307819    Objective Loss 0.307819    Top1 88.038278    LR 0.000008    Time 0.019124    
2023-01-06 16:23:11,541 - --- validate (epoch=149)-----------
2023-01-06 16:23:11,541 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:11,977 - Epoch: [149][   10/   28]    Loss 0.302046    Top1 89.023438    
2023-01-06 16:23:12,079 - Epoch: [149][   20/   28]    Loss 0.305154    Top1 88.574219    
2023-01-06 16:23:12,135 - Epoch: [149][   28/   28]    Loss 0.310622    Top1 88.562840    
2023-01-06 16:23:12,289 - ==> Top1: 88.563    Loss: 0.311

2023-01-06 16:23:12,289 - ==> Confusion:
[[ 172    7  260]
 [   8  191  403]
 [  54   67 5824]]

2023-01-06 16:23:12,290 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:12,291 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:12,295 - 

2023-01-06 16:23:12,295 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:12,970 - Epoch: [150][   10/  246]    Overall Loss 0.294435    Objective Loss 0.294435                                        LR 0.000008    Time 0.067418    
2023-01-06 16:23:13,118 - Epoch: [150][   20/  246]    Overall Loss 0.289579    Objective Loss 0.289579                                        LR 0.000008    Time 0.041074    
2023-01-06 16:23:13,265 - Epoch: [150][   30/  246]    Overall Loss 0.296356    Objective Loss 0.296356                                        LR 0.000008    Time 0.032269    
2023-01-06 16:23:13,404 - Epoch: [150][   40/  246]    Overall Loss 0.303285    Objective Loss 0.303285                                        LR 0.000008    Time 0.027683    
2023-01-06 16:23:13,546 - Epoch: [150][   50/  246]    Overall Loss 0.303841    Objective Loss 0.303841                                        LR 0.000008    Time 0.024981    
2023-01-06 16:23:13,692 - Epoch: [150][   60/  246]    Overall Loss 0.303601    Objective Loss 0.303601                                        LR 0.000008    Time 0.023244    
2023-01-06 16:23:13,836 - Epoch: [150][   70/  246]    Overall Loss 0.309309    Objective Loss 0.309309                                        LR 0.000008    Time 0.021962    
2023-01-06 16:23:13,980 - Epoch: [150][   80/  246]    Overall Loss 0.309516    Objective Loss 0.309516                                        LR 0.000008    Time 0.021012    
2023-01-06 16:23:14,123 - Epoch: [150][   90/  246]    Overall Loss 0.309575    Objective Loss 0.309575                                        LR 0.000008    Time 0.020268    
2023-01-06 16:23:14,265 - Epoch: [150][  100/  246]    Overall Loss 0.307637    Objective Loss 0.307637                                        LR 0.000008    Time 0.019654    
2023-01-06 16:23:14,407 - Epoch: [150][  110/  246]    Overall Loss 0.306370    Objective Loss 0.306370                                        LR 0.000008    Time 0.019153    
2023-01-06 16:23:14,549 - Epoch: [150][  120/  246]    Overall Loss 0.306228    Objective Loss 0.306228                                        LR 0.000008    Time 0.018740    
2023-01-06 16:23:14,694 - Epoch: [150][  130/  246]    Overall Loss 0.307036    Objective Loss 0.307036                                        LR 0.000008    Time 0.018408    
2023-01-06 16:23:14,835 - Epoch: [150][  140/  246]    Overall Loss 0.305747    Objective Loss 0.305747                                        LR 0.000008    Time 0.018101    
2023-01-06 16:23:14,982 - Epoch: [150][  150/  246]    Overall Loss 0.305003    Objective Loss 0.305003                                        LR 0.000008    Time 0.017867    
2023-01-06 16:23:15,126 - Epoch: [150][  160/  246]    Overall Loss 0.304545    Objective Loss 0.304545                                        LR 0.000008    Time 0.017651    
2023-01-06 16:23:15,274 - Epoch: [150][  170/  246]    Overall Loss 0.304202    Objective Loss 0.304202                                        LR 0.000008    Time 0.017483    
2023-01-06 16:23:15,419 - Epoch: [150][  180/  246]    Overall Loss 0.305398    Objective Loss 0.305398                                        LR 0.000008    Time 0.017312    
2023-01-06 16:23:15,566 - Epoch: [150][  190/  246]    Overall Loss 0.305593    Objective Loss 0.305593                                        LR 0.000008    Time 0.017170    
2023-01-06 16:23:15,711 - Epoch: [150][  200/  246]    Overall Loss 0.305734    Objective Loss 0.305734                                        LR 0.000008    Time 0.017037    
2023-01-06 16:23:15,854 - Epoch: [150][  210/  246]    Overall Loss 0.306031    Objective Loss 0.306031                                        LR 0.000008    Time 0.016903    
2023-01-06 16:23:15,999 - Epoch: [150][  220/  246]    Overall Loss 0.305660    Objective Loss 0.305660                                        LR 0.000008    Time 0.016794    
2023-01-06 16:23:16,145 - Epoch: [150][  230/  246]    Overall Loss 0.305963    Objective Loss 0.305963                                        LR 0.000008    Time 0.016694    
2023-01-06 16:23:16,301 - Epoch: [150][  240/  246]    Overall Loss 0.306729    Objective Loss 0.306729                                        LR 0.000008    Time 0.016647    
2023-01-06 16:23:16,370 - Epoch: [150][  246/  246]    Overall Loss 0.306891    Objective Loss 0.306891    Top1 89.712919    LR 0.000008    Time 0.016521    
2023-01-06 16:23:16,503 - --- validate (epoch=150)-----------
2023-01-06 16:23:16,503 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:16,938 - Epoch: [150][   10/   28]    Loss 0.311447    Top1 88.476562    
2023-01-06 16:23:17,039 - Epoch: [150][   20/   28]    Loss 0.314201    Top1 88.574219    
2023-01-06 16:23:17,096 - Epoch: [150][   28/   28]    Loss 0.306745    Top1 88.705983    
2023-01-06 16:23:17,237 - ==> Top1: 88.706    Loss: 0.307

2023-01-06 16:23:17,237 - ==> Confusion:
[[ 184    7  248]
 [   9  206  387]
 [  62   76 5807]]

2023-01-06 16:23:17,238 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:17,238 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:17,243 - 

2023-01-06 16:23:17,243 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:17,769 - Epoch: [151][   10/  246]    Overall Loss 0.296207    Objective Loss 0.296207                                        LR 0.000008    Time 0.052588    
2023-01-06 16:23:17,915 - Epoch: [151][   20/  246]    Overall Loss 0.297548    Objective Loss 0.297548                                        LR 0.000008    Time 0.033549    
2023-01-06 16:23:18,063 - Epoch: [151][   30/  246]    Overall Loss 0.300806    Objective Loss 0.300806                                        LR 0.000008    Time 0.027276    
2023-01-06 16:23:18,206 - Epoch: [151][   40/  246]    Overall Loss 0.303371    Objective Loss 0.303371                                        LR 0.000008    Time 0.024040    
2023-01-06 16:23:18,346 - Epoch: [151][   50/  246]    Overall Loss 0.308761    Objective Loss 0.308761                                        LR 0.000008    Time 0.022011    
2023-01-06 16:23:18,484 - Epoch: [151][   60/  246]    Overall Loss 0.310839    Objective Loss 0.310839                                        LR 0.000008    Time 0.020648    
2023-01-06 16:23:18,623 - Epoch: [151][   70/  246]    Overall Loss 0.308305    Objective Loss 0.308305                                        LR 0.000008    Time 0.019672    
2023-01-06 16:23:18,772 - Epoch: [151][   80/  246]    Overall Loss 0.310022    Objective Loss 0.310022                                        LR 0.000008    Time 0.019072    
2023-01-06 16:23:18,935 - Epoch: [151][   90/  246]    Overall Loss 0.307825    Objective Loss 0.307825                                        LR 0.000008    Time 0.018763    
2023-01-06 16:23:19,098 - Epoch: [151][  100/  246]    Overall Loss 0.306575    Objective Loss 0.306575                                        LR 0.000008    Time 0.018511    
2023-01-06 16:23:19,260 - Epoch: [151][  110/  246]    Overall Loss 0.305244    Objective Loss 0.305244                                        LR 0.000008    Time 0.018284    
2023-01-06 16:23:19,423 - Epoch: [151][  120/  246]    Overall Loss 0.304396    Objective Loss 0.304396                                        LR 0.000008    Time 0.018114    
2023-01-06 16:23:19,582 - Epoch: [151][  130/  246]    Overall Loss 0.304827    Objective Loss 0.304827                                        LR 0.000008    Time 0.017929    
2023-01-06 16:23:19,742 - Epoch: [151][  140/  246]    Overall Loss 0.304999    Objective Loss 0.304999                                        LR 0.000008    Time 0.017790    
2023-01-06 16:23:19,903 - Epoch: [151][  150/  246]    Overall Loss 0.303650    Objective Loss 0.303650                                        LR 0.000008    Time 0.017670    
2023-01-06 16:23:20,063 - Epoch: [151][  160/  246]    Overall Loss 0.304971    Objective Loss 0.304971                                        LR 0.000008    Time 0.017566    
2023-01-06 16:23:20,222 - Epoch: [151][  170/  246]    Overall Loss 0.305628    Objective Loss 0.305628                                        LR 0.000008    Time 0.017453    
2023-01-06 16:23:20,381 - Epoch: [151][  180/  246]    Overall Loss 0.306288    Objective Loss 0.306288                                        LR 0.000008    Time 0.017367    
2023-01-06 16:23:20,544 - Epoch: [151][  190/  246]    Overall Loss 0.305474    Objective Loss 0.305474                                        LR 0.000008    Time 0.017296    
2023-01-06 16:23:20,703 - Epoch: [151][  200/  246]    Overall Loss 0.305021    Objective Loss 0.305021                                        LR 0.000008    Time 0.017215    
2023-01-06 16:23:20,863 - Epoch: [151][  210/  246]    Overall Loss 0.304906    Objective Loss 0.304906                                        LR 0.000008    Time 0.017150    
2023-01-06 16:23:21,027 - Epoch: [151][  220/  246]    Overall Loss 0.305766    Objective Loss 0.305766                                        LR 0.000008    Time 0.017112    
2023-01-06 16:23:21,189 - Epoch: [151][  230/  246]    Overall Loss 0.306709    Objective Loss 0.306709                                        LR 0.000008    Time 0.017070    
2023-01-06 16:23:21,365 - Epoch: [151][  240/  246]    Overall Loss 0.307095    Objective Loss 0.307095                                        LR 0.000008    Time 0.017090    
2023-01-06 16:23:21,447 - Epoch: [151][  246/  246]    Overall Loss 0.307324    Objective Loss 0.307324    Top1 90.191388    LR 0.000008    Time 0.017008    
2023-01-06 16:23:21,597 - --- validate (epoch=151)-----------
2023-01-06 16:23:21,597 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:22,038 - Epoch: [151][   10/   28]    Loss 0.318338    Top1 88.671875    
2023-01-06 16:23:22,141 - Epoch: [151][   20/   28]    Loss 0.310819    Top1 88.515625    
2023-01-06 16:23:22,202 - Epoch: [151][   28/   28]    Loss 0.308196    Top1 88.548526    
2023-01-06 16:23:22,347 - ==> Top1: 88.549    Loss: 0.308

2023-01-06 16:23:22,347 - ==> Confusion:
[[ 172    7  260]
 [   8  199  395]
 [  54   76 5815]]

2023-01-06 16:23:22,348 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:22,349 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:22,353 - 

2023-01-06 16:23:22,353 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:23,036 - Epoch: [152][   10/  246]    Overall Loss 0.310411    Objective Loss 0.310411                                        LR 0.000008    Time 0.068229    
2023-01-06 16:23:23,216 - Epoch: [152][   20/  246]    Overall Loss 0.307750    Objective Loss 0.307750                                        LR 0.000008    Time 0.043103    
2023-01-06 16:23:23,396 - Epoch: [152][   30/  246]    Overall Loss 0.311133    Objective Loss 0.311133                                        LR 0.000008    Time 0.034710    
2023-01-06 16:23:23,581 - Epoch: [152][   40/  246]    Overall Loss 0.309614    Objective Loss 0.309614                                        LR 0.000008    Time 0.030654    
2023-01-06 16:23:23,758 - Epoch: [152][   50/  246]    Overall Loss 0.306988    Objective Loss 0.306988                                        LR 0.000008    Time 0.028028    
2023-01-06 16:23:23,942 - Epoch: [152][   60/  246]    Overall Loss 0.304770    Objective Loss 0.304770                                        LR 0.000008    Time 0.026407    
2023-01-06 16:23:24,124 - Epoch: [152][   70/  246]    Overall Loss 0.305285    Objective Loss 0.305285                                        LR 0.000008    Time 0.025204    
2023-01-06 16:23:24,308 - Epoch: [152][   80/  246]    Overall Loss 0.304779    Objective Loss 0.304779                                        LR 0.000008    Time 0.024348    
2023-01-06 16:23:24,503 - Epoch: [152][   90/  246]    Overall Loss 0.305282    Objective Loss 0.305282                                        LR 0.000008    Time 0.023809    
2023-01-06 16:23:24,709 - Epoch: [152][  100/  246]    Overall Loss 0.306021    Objective Loss 0.306021                                        LR 0.000008    Time 0.023478    
2023-01-06 16:23:24,914 - Epoch: [152][  110/  246]    Overall Loss 0.305006    Objective Loss 0.305006                                        LR 0.000008    Time 0.023209    
2023-01-06 16:23:25,124 - Epoch: [152][  120/  246]    Overall Loss 0.304676    Objective Loss 0.304676                                        LR 0.000008    Time 0.023018    
2023-01-06 16:23:25,333 - Epoch: [152][  130/  246]    Overall Loss 0.304023    Objective Loss 0.304023                                        LR 0.000008    Time 0.022849    
2023-01-06 16:23:25,540 - Epoch: [152][  140/  246]    Overall Loss 0.305263    Objective Loss 0.305263                                        LR 0.000008    Time 0.022696    
2023-01-06 16:23:25,747 - Epoch: [152][  150/  246]    Overall Loss 0.304837    Objective Loss 0.304837                                        LR 0.000008    Time 0.022563    
2023-01-06 16:23:25,955 - Epoch: [152][  160/  246]    Overall Loss 0.305289    Objective Loss 0.305289                                        LR 0.000008    Time 0.022451    
2023-01-06 16:23:26,155 - Epoch: [152][  170/  246]    Overall Loss 0.305523    Objective Loss 0.305523                                        LR 0.000008    Time 0.022303    
2023-01-06 16:23:26,344 - Epoch: [152][  180/  246]    Overall Loss 0.304725    Objective Loss 0.304725                                        LR 0.000008    Time 0.022112    
2023-01-06 16:23:26,506 - Epoch: [152][  190/  246]    Overall Loss 0.303780    Objective Loss 0.303780                                        LR 0.000008    Time 0.021800    
2023-01-06 16:23:26,665 - Epoch: [152][  200/  246]    Overall Loss 0.303238    Objective Loss 0.303238                                        LR 0.000008    Time 0.021499    
2023-01-06 16:23:26,822 - Epoch: [152][  210/  246]    Overall Loss 0.304442    Objective Loss 0.304442                                        LR 0.000008    Time 0.021222    
2023-01-06 16:23:26,978 - Epoch: [152][  220/  246]    Overall Loss 0.305482    Objective Loss 0.305482                                        LR 0.000008    Time 0.020965    
2023-01-06 16:23:27,134 - Epoch: [152][  230/  246]    Overall Loss 0.306144    Objective Loss 0.306144                                        LR 0.000008    Time 0.020733    
2023-01-06 16:23:27,291 - Epoch: [152][  240/  246]    Overall Loss 0.306710    Objective Loss 0.306710                                        LR 0.000008    Time 0.020520    
2023-01-06 16:23:27,368 - Epoch: [152][  246/  246]    Overall Loss 0.306631    Objective Loss 0.306631    Top1 87.799043    LR 0.000008    Time 0.020331    
2023-01-06 16:23:27,559 - --- validate (epoch=152)-----------
2023-01-06 16:23:27,559 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:28,001 - Epoch: [152][   10/   28]    Loss 0.301687    Top1 89.335938    
2023-01-06 16:23:28,110 - Epoch: [152][   20/   28]    Loss 0.307659    Top1 88.945312    
2023-01-06 16:23:28,166 - Epoch: [152][   28/   28]    Loss 0.308369    Top1 88.777555    
2023-01-06 16:23:28,318 - ==> Top1: 88.778    Loss: 0.308

2023-01-06 16:23:28,318 - ==> Confusion:
[[ 190    7  242]
 [  13  213  376]
 [  64   82 5799]]

2023-01-06 16:23:28,320 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:28,320 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:28,324 - 

2023-01-06 16:23:28,324 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:28,857 - Epoch: [153][   10/  246]    Overall Loss 0.283585    Objective Loss 0.283585                                        LR 0.000008    Time 0.053251    
2023-01-06 16:23:29,010 - Epoch: [153][   20/  246]    Overall Loss 0.305952    Objective Loss 0.305952                                        LR 0.000008    Time 0.034254    
2023-01-06 16:23:29,166 - Epoch: [153][   30/  246]    Overall Loss 0.306152    Objective Loss 0.306152                                        LR 0.000008    Time 0.028009    
2023-01-06 16:23:29,332 - Epoch: [153][   40/  246]    Overall Loss 0.306818    Objective Loss 0.306818                                        LR 0.000008    Time 0.025152    
2023-01-06 16:23:29,487 - Epoch: [153][   50/  246]    Overall Loss 0.300323    Objective Loss 0.300323                                        LR 0.000008    Time 0.023179    
2023-01-06 16:23:29,657 - Epoch: [153][   60/  246]    Overall Loss 0.301515    Objective Loss 0.301515                                        LR 0.000008    Time 0.022143    
2023-01-06 16:23:29,824 - Epoch: [153][   70/  246]    Overall Loss 0.300200    Objective Loss 0.300200                                        LR 0.000008    Time 0.021351    
2023-01-06 16:23:29,984 - Epoch: [153][   80/  246]    Overall Loss 0.301072    Objective Loss 0.301072                                        LR 0.000008    Time 0.020680    
2023-01-06 16:23:30,126 - Epoch: [153][   90/  246]    Overall Loss 0.303842    Objective Loss 0.303842                                        LR 0.000008    Time 0.019954    
2023-01-06 16:23:30,263 - Epoch: [153][  100/  246]    Overall Loss 0.305363    Objective Loss 0.305363                                        LR 0.000008    Time 0.019323    
2023-01-06 16:23:30,405 - Epoch: [153][  110/  246]    Overall Loss 0.304457    Objective Loss 0.304457                                        LR 0.000008    Time 0.018857    
2023-01-06 16:23:30,544 - Epoch: [153][  120/  246]    Overall Loss 0.303119    Objective Loss 0.303119                                        LR 0.000008    Time 0.018442    
2023-01-06 16:23:30,685 - Epoch: [153][  130/  246]    Overall Loss 0.302246    Objective Loss 0.302246                                        LR 0.000008    Time 0.018102    
2023-01-06 16:23:30,824 - Epoch: [153][  140/  246]    Overall Loss 0.302164    Objective Loss 0.302164                                        LR 0.000008    Time 0.017800    
2023-01-06 16:23:30,976 - Epoch: [153][  150/  246]    Overall Loss 0.303234    Objective Loss 0.303234                                        LR 0.000008    Time 0.017623    
2023-01-06 16:23:31,152 - Epoch: [153][  160/  246]    Overall Loss 0.305031    Objective Loss 0.305031                                        LR 0.000008    Time 0.017619    
2023-01-06 16:23:31,324 - Epoch: [153][  170/  246]    Overall Loss 0.305690    Objective Loss 0.305690                                        LR 0.000008    Time 0.017595    
2023-01-06 16:23:31,492 - Epoch: [153][  180/  246]    Overall Loss 0.306087    Objective Loss 0.306087                                        LR 0.000008    Time 0.017538    
2023-01-06 16:23:31,664 - Epoch: [153][  190/  246]    Overall Loss 0.306678    Objective Loss 0.306678                                        LR 0.000008    Time 0.017520    
2023-01-06 16:23:31,829 - Epoch: [153][  200/  246]    Overall Loss 0.306737    Objective Loss 0.306737                                        LR 0.000008    Time 0.017458    
2023-01-06 16:23:32,002 - Epoch: [153][  210/  246]    Overall Loss 0.307101    Objective Loss 0.307101                                        LR 0.000008    Time 0.017447    
2023-01-06 16:23:32,145 - Epoch: [153][  220/  246]    Overall Loss 0.306810    Objective Loss 0.306810                                        LR 0.000008    Time 0.017293    
2023-01-06 16:23:32,280 - Epoch: [153][  230/  246]    Overall Loss 0.306748    Objective Loss 0.306748                                        LR 0.000008    Time 0.017130    
2023-01-06 16:23:32,441 - Epoch: [153][  240/  246]    Overall Loss 0.307108    Objective Loss 0.307108                                        LR 0.000008    Time 0.017083    
2023-01-06 16:23:32,521 - Epoch: [153][  246/  246]    Overall Loss 0.307180    Objective Loss 0.307180    Top1 89.473684    LR 0.000008    Time 0.016992    
2023-01-06 16:23:32,655 - --- validate (epoch=153)-----------
2023-01-06 16:23:32,655 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:33,089 - Epoch: [153][   10/   28]    Loss 0.292967    Top1 89.765625    
2023-01-06 16:23:33,208 - Epoch: [153][   20/   28]    Loss 0.308227    Top1 88.730469    
2023-01-06 16:23:33,264 - Epoch: [153][   28/   28]    Loss 0.310297    Top1 88.620097    
2023-01-06 16:23:33,413 - ==> Top1: 88.620    Loss: 0.310

2023-01-06 16:23:33,414 - ==> Confusion:
[[ 200    8  231]
 [  11  187  404]
 [  77   64 5804]]

2023-01-06 16:23:33,415 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:33,415 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:33,420 - 

2023-01-06 16:23:33,420 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:34,082 - Epoch: [154][   10/  246]    Overall Loss 0.307362    Objective Loss 0.307362                                        LR 0.000008    Time 0.066160    
2023-01-06 16:23:34,237 - Epoch: [154][   20/  246]    Overall Loss 0.308930    Objective Loss 0.308930                                        LR 0.000008    Time 0.040803    
2023-01-06 16:23:34,392 - Epoch: [154][   30/  246]    Overall Loss 0.298562    Objective Loss 0.298562                                        LR 0.000008    Time 0.032344    
2023-01-06 16:23:34,561 - Epoch: [154][   40/  246]    Overall Loss 0.296018    Objective Loss 0.296018                                        LR 0.000008    Time 0.028479    
2023-01-06 16:23:34,737 - Epoch: [154][   50/  246]    Overall Loss 0.300517    Objective Loss 0.300517                                        LR 0.000008    Time 0.026282    
2023-01-06 16:23:34,897 - Epoch: [154][   60/  246]    Overall Loss 0.304830    Objective Loss 0.304830                                        LR 0.000008    Time 0.024555    
2023-01-06 16:23:35,072 - Epoch: [154][   70/  246]    Overall Loss 0.305012    Objective Loss 0.305012                                        LR 0.000008    Time 0.023542    
2023-01-06 16:23:35,246 - Epoch: [154][   80/  246]    Overall Loss 0.305678    Objective Loss 0.305678                                        LR 0.000008    Time 0.022746    
2023-01-06 16:23:35,422 - Epoch: [154][   90/  246]    Overall Loss 0.305775    Objective Loss 0.305775                                        LR 0.000008    Time 0.022169    
2023-01-06 16:23:35,574 - Epoch: [154][  100/  246]    Overall Loss 0.306424    Objective Loss 0.306424                                        LR 0.000008    Time 0.021472    
2023-01-06 16:23:35,732 - Epoch: [154][  110/  246]    Overall Loss 0.308624    Objective Loss 0.308624                                        LR 0.000008    Time 0.020951    
2023-01-06 16:23:35,895 - Epoch: [154][  120/  246]    Overall Loss 0.308693    Objective Loss 0.308693                                        LR 0.000008    Time 0.020562    
2023-01-06 16:23:36,070 - Epoch: [154][  130/  246]    Overall Loss 0.308935    Objective Loss 0.308935                                        LR 0.000008    Time 0.020327    
2023-01-06 16:23:36,255 - Epoch: [154][  140/  246]    Overall Loss 0.309181    Objective Loss 0.309181                                        LR 0.000008    Time 0.020192    
2023-01-06 16:23:36,444 - Epoch: [154][  150/  246]    Overall Loss 0.308842    Objective Loss 0.308842                                        LR 0.000008    Time 0.020104    
2023-01-06 16:23:36,641 - Epoch: [154][  160/  246]    Overall Loss 0.308074    Objective Loss 0.308074                                        LR 0.000008    Time 0.020076    
2023-01-06 16:23:36,845 - Epoch: [154][  170/  246]    Overall Loss 0.307078    Objective Loss 0.307078                                        LR 0.000008    Time 0.020092    
2023-01-06 16:23:37,062 - Epoch: [154][  180/  246]    Overall Loss 0.306506    Objective Loss 0.306506                                        LR 0.000008    Time 0.020178    
2023-01-06 16:23:37,265 - Epoch: [154][  190/  246]    Overall Loss 0.306778    Objective Loss 0.306778                                        LR 0.000008    Time 0.020187    
2023-01-06 16:23:37,461 - Epoch: [154][  200/  246]    Overall Loss 0.307092    Objective Loss 0.307092                                        LR 0.000008    Time 0.020154    
2023-01-06 16:23:37,658 - Epoch: [154][  210/  246]    Overall Loss 0.307563    Objective Loss 0.307563                                        LR 0.000008    Time 0.020128    
2023-01-06 16:23:37,819 - Epoch: [154][  220/  246]    Overall Loss 0.307433    Objective Loss 0.307433                                        LR 0.000008    Time 0.019946    
2023-01-06 16:23:37,976 - Epoch: [154][  230/  246]    Overall Loss 0.307624    Objective Loss 0.307624                                        LR 0.000008    Time 0.019760    
2023-01-06 16:23:38,167 - Epoch: [154][  240/  246]    Overall Loss 0.306675    Objective Loss 0.306675                                        LR 0.000008    Time 0.019733    
2023-01-06 16:23:38,248 - Epoch: [154][  246/  246]    Overall Loss 0.306720    Objective Loss 0.306720    Top1 85.167464    LR 0.000008    Time 0.019580    
2023-01-06 16:23:38,399 - --- validate (epoch=154)-----------
2023-01-06 16:23:38,400 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:38,841 - Epoch: [154][   10/   28]    Loss 0.313731    Top1 88.789062    
2023-01-06 16:23:38,948 - Epoch: [154][   20/   28]    Loss 0.306430    Top1 88.769531    
2023-01-06 16:23:39,005 - Epoch: [154][   28/   28]    Loss 0.306626    Top1 88.663040    
2023-01-06 16:23:39,149 - ==> Top1: 88.663    Loss: 0.307

2023-01-06 16:23:39,149 - ==> Confusion:
[[ 188    7  244]
 [  10  189  403]
 [  63   65 5817]]

2023-01-06 16:23:39,150 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:39,150 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:39,155 - 

2023-01-06 16:23:39,155 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:39,834 - Epoch: [155][   10/  246]    Overall Loss 0.314175    Objective Loss 0.314175                                        LR 0.000008    Time 0.067894    
2023-01-06 16:23:39,995 - Epoch: [155][   20/  246]    Overall Loss 0.307037    Objective Loss 0.307037                                        LR 0.000008    Time 0.041966    
2023-01-06 16:23:40,165 - Epoch: [155][   30/  246]    Overall Loss 0.303478    Objective Loss 0.303478                                        LR 0.000008    Time 0.033616    
2023-01-06 16:23:40,319 - Epoch: [155][   40/  246]    Overall Loss 0.305538    Objective Loss 0.305538                                        LR 0.000008    Time 0.029062    
2023-01-06 16:23:40,478 - Epoch: [155][   50/  246]    Overall Loss 0.305963    Objective Loss 0.305963                                        LR 0.000008    Time 0.026412    
2023-01-06 16:23:40,635 - Epoch: [155][   60/  246]    Overall Loss 0.306718    Objective Loss 0.306718                                        LR 0.000008    Time 0.024620    
2023-01-06 16:23:40,795 - Epoch: [155][   70/  246]    Overall Loss 0.308797    Objective Loss 0.308797                                        LR 0.000008    Time 0.023384    
2023-01-06 16:23:40,955 - Epoch: [155][   80/  246]    Overall Loss 0.306427    Objective Loss 0.306427                                        LR 0.000008    Time 0.022458    
2023-01-06 16:23:41,118 - Epoch: [155][   90/  246]    Overall Loss 0.305890    Objective Loss 0.305890                                        LR 0.000008    Time 0.021771    
2023-01-06 16:23:41,279 - Epoch: [155][  100/  246]    Overall Loss 0.304774    Objective Loss 0.304774                                        LR 0.000008    Time 0.021204    
2023-01-06 16:23:41,441 - Epoch: [155][  110/  246]    Overall Loss 0.304747    Objective Loss 0.304747                                        LR 0.000008    Time 0.020744    
2023-01-06 16:23:41,602 - Epoch: [155][  120/  246]    Overall Loss 0.303698    Objective Loss 0.303698                                        LR 0.000008    Time 0.020349    
2023-01-06 16:23:41,778 - Epoch: [155][  130/  246]    Overall Loss 0.304504    Objective Loss 0.304504                                        LR 0.000008    Time 0.020138    
2023-01-06 16:23:41,950 - Epoch: [155][  140/  246]    Overall Loss 0.303834    Objective Loss 0.303834                                        LR 0.000008    Time 0.019925    
2023-01-06 16:23:42,128 - Epoch: [155][  150/  246]    Overall Loss 0.304698    Objective Loss 0.304698                                        LR 0.000008    Time 0.019778    
2023-01-06 16:23:42,300 - Epoch: [155][  160/  246]    Overall Loss 0.304140    Objective Loss 0.304140                                        LR 0.000008    Time 0.019617    
2023-01-06 16:23:42,478 - Epoch: [155][  170/  246]    Overall Loss 0.304904    Objective Loss 0.304904                                        LR 0.000008    Time 0.019505    
2023-01-06 16:23:42,649 - Epoch: [155][  180/  246]    Overall Loss 0.305382    Objective Loss 0.305382                                        LR 0.000008    Time 0.019373    
2023-01-06 16:23:42,828 - Epoch: [155][  190/  246]    Overall Loss 0.305332    Objective Loss 0.305332                                        LR 0.000008    Time 0.019289    
2023-01-06 16:23:42,999 - Epoch: [155][  200/  246]    Overall Loss 0.306083    Objective Loss 0.306083                                        LR 0.000008    Time 0.019181    
2023-01-06 16:23:43,178 - Epoch: [155][  210/  246]    Overall Loss 0.306825    Objective Loss 0.306825                                        LR 0.000008    Time 0.019116    
2023-01-06 16:23:43,362 - Epoch: [155][  220/  246]    Overall Loss 0.306514    Objective Loss 0.306514                                        LR 0.000008    Time 0.019081    
2023-01-06 16:23:43,548 - Epoch: [155][  230/  246]    Overall Loss 0.306720    Objective Loss 0.306720                                        LR 0.000008    Time 0.019057    
2023-01-06 16:23:43,738 - Epoch: [155][  240/  246]    Overall Loss 0.306656    Objective Loss 0.306656                                        LR 0.000008    Time 0.019057    
2023-01-06 16:23:43,821 - Epoch: [155][  246/  246]    Overall Loss 0.306650    Objective Loss 0.306650    Top1 88.038278    LR 0.000008    Time 0.018928    
2023-01-06 16:23:43,987 - --- validate (epoch=155)-----------
2023-01-06 16:23:43,987 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:44,417 - Epoch: [155][   10/   28]    Loss 0.312306    Top1 88.476562    
2023-01-06 16:23:44,520 - Epoch: [155][   20/   28]    Loss 0.314505    Top1 88.339844    
2023-01-06 16:23:44,577 - Epoch: [155][   28/   28]    Loss 0.310568    Top1 88.491268    
2023-01-06 16:23:44,725 - ==> Top1: 88.491    Loss: 0.311

2023-01-06 16:23:44,726 - ==> Confusion:
[[ 180    7  252]
 [   8  187  407]
 [  62   68 5815]]

2023-01-06 16:23:44,727 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:44,727 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:44,731 - 

2023-01-06 16:23:44,732 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:45,276 - Epoch: [156][   10/  246]    Overall Loss 0.310782    Objective Loss 0.310782                                        LR 0.000008    Time 0.054354    
2023-01-06 16:23:45,430 - Epoch: [156][   20/  246]    Overall Loss 0.311047    Objective Loss 0.311047                                        LR 0.000008    Time 0.034856    
2023-01-06 16:23:45,589 - Epoch: [156][   30/  246]    Overall Loss 0.308124    Objective Loss 0.308124                                        LR 0.000008    Time 0.028534    
2023-01-06 16:23:45,766 - Epoch: [156][   40/  246]    Overall Loss 0.306941    Objective Loss 0.306941                                        LR 0.000008    Time 0.025823    
2023-01-06 16:23:45,942 - Epoch: [156][   50/  246]    Overall Loss 0.303363    Objective Loss 0.303363                                        LR 0.000008    Time 0.024159    
2023-01-06 16:23:46,121 - Epoch: [156][   60/  246]    Overall Loss 0.304000    Objective Loss 0.304000                                        LR 0.000008    Time 0.023114    
2023-01-06 16:23:46,302 - Epoch: [156][   70/  246]    Overall Loss 0.303804    Objective Loss 0.303804                                        LR 0.000008    Time 0.022390    
2023-01-06 16:23:46,492 - Epoch: [156][   80/  246]    Overall Loss 0.304618    Objective Loss 0.304618                                        LR 0.000008    Time 0.021961    
2023-01-06 16:23:46,669 - Epoch: [156][   90/  246]    Overall Loss 0.304219    Objective Loss 0.304219                                        LR 0.000008    Time 0.021490    
2023-01-06 16:23:46,860 - Epoch: [156][  100/  246]    Overall Loss 0.305466    Objective Loss 0.305466                                        LR 0.000008    Time 0.021244    
2023-01-06 16:23:47,034 - Epoch: [156][  110/  246]    Overall Loss 0.304257    Objective Loss 0.304257                                        LR 0.000008    Time 0.020889    
2023-01-06 16:23:47,208 - Epoch: [156][  120/  246]    Overall Loss 0.304214    Objective Loss 0.304214                                        LR 0.000008    Time 0.020595    
2023-01-06 16:23:47,383 - Epoch: [156][  130/  246]    Overall Loss 0.304346    Objective Loss 0.304346                                        LR 0.000008    Time 0.020357    
2023-01-06 16:23:47,558 - Epoch: [156][  140/  246]    Overall Loss 0.306047    Objective Loss 0.306047                                        LR 0.000008    Time 0.020149    
2023-01-06 16:23:47,734 - Epoch: [156][  150/  246]    Overall Loss 0.304924    Objective Loss 0.304924                                        LR 0.000008    Time 0.019978    
2023-01-06 16:23:47,908 - Epoch: [156][  160/  246]    Overall Loss 0.305062    Objective Loss 0.305062                                        LR 0.000008    Time 0.019813    
2023-01-06 16:23:48,083 - Epoch: [156][  170/  246]    Overall Loss 0.303983    Objective Loss 0.303983                                        LR 0.000008    Time 0.019675    
2023-01-06 16:23:48,255 - Epoch: [156][  180/  246]    Overall Loss 0.304049    Objective Loss 0.304049                                        LR 0.000008    Time 0.019537    
2023-01-06 16:23:48,428 - Epoch: [156][  190/  246]    Overall Loss 0.304017    Objective Loss 0.304017                                        LR 0.000008    Time 0.019419    
2023-01-06 16:23:48,600 - Epoch: [156][  200/  246]    Overall Loss 0.304593    Objective Loss 0.304593                                        LR 0.000008    Time 0.019304    
2023-01-06 16:23:48,774 - Epoch: [156][  210/  246]    Overall Loss 0.304715    Objective Loss 0.304715                                        LR 0.000008    Time 0.019210    
2023-01-06 16:23:48,947 - Epoch: [156][  220/  246]    Overall Loss 0.304574    Objective Loss 0.304574                                        LR 0.000008    Time 0.019125    
2023-01-06 16:23:49,131 - Epoch: [156][  230/  246]    Overall Loss 0.305529    Objective Loss 0.305529                                        LR 0.000008    Time 0.019091    
2023-01-06 16:23:49,345 - Epoch: [156][  240/  246]    Overall Loss 0.306672    Objective Loss 0.306672                                        LR 0.000008    Time 0.019186    
2023-01-06 16:23:49,427 - Epoch: [156][  246/  246]    Overall Loss 0.306859    Objective Loss 0.306859    Top1 86.842105    LR 0.000008    Time 0.019048    
2023-01-06 16:23:49,599 - --- validate (epoch=156)-----------
2023-01-06 16:23:49,599 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:50,054 - Epoch: [156][   10/   28]    Loss 0.297536    Top1 89.140625    
2023-01-06 16:23:50,164 - Epoch: [156][   20/   28]    Loss 0.300153    Top1 88.984375    
2023-01-06 16:23:50,223 - Epoch: [156][   28/   28]    Loss 0.308019    Top1 88.591469    
2023-01-06 16:23:50,373 - ==> Top1: 88.591    Loss: 0.308

2023-01-06 16:23:50,373 - ==> Confusion:
[[ 190    7  242]
 [  11  185  406]
 [  68   63 5814]]

2023-01-06 16:23:50,374 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:50,375 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:50,379 - 

2023-01-06 16:23:50,379 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:51,068 - Epoch: [157][   10/  246]    Overall Loss 0.291852    Objective Loss 0.291852                                        LR 0.000008    Time 0.068854    
2023-01-06 16:23:51,239 - Epoch: [157][   20/  246]    Overall Loss 0.299076    Objective Loss 0.299076                                        LR 0.000008    Time 0.042925    
2023-01-06 16:23:51,417 - Epoch: [157][   30/  246]    Overall Loss 0.296438    Objective Loss 0.296438                                        LR 0.000008    Time 0.034537    
2023-01-06 16:23:51,586 - Epoch: [157][   40/  246]    Overall Loss 0.298784    Objective Loss 0.298784                                        LR 0.000008    Time 0.030124    
2023-01-06 16:23:51,763 - Epoch: [157][   50/  246]    Overall Loss 0.300301    Objective Loss 0.300301                                        LR 0.000008    Time 0.027631    
2023-01-06 16:23:51,932 - Epoch: [157][   60/  246]    Overall Loss 0.303428    Objective Loss 0.303428                                        LR 0.000008    Time 0.025844    
2023-01-06 16:23:52,092 - Epoch: [157][   70/  246]    Overall Loss 0.303368    Objective Loss 0.303368                                        LR 0.000008    Time 0.024428    
2023-01-06 16:23:52,256 - Epoch: [157][   80/  246]    Overall Loss 0.302902    Objective Loss 0.302902                                        LR 0.000008    Time 0.023421    
2023-01-06 16:23:52,424 - Epoch: [157][   90/  246]    Overall Loss 0.302039    Objective Loss 0.302039                                        LR 0.000008    Time 0.022682    
2023-01-06 16:23:52,590 - Epoch: [157][  100/  246]    Overall Loss 0.304120    Objective Loss 0.304120                                        LR 0.000008    Time 0.022069    
2023-01-06 16:23:52,758 - Epoch: [157][  110/  246]    Overall Loss 0.305369    Objective Loss 0.305369                                        LR 0.000008    Time 0.021585    
2023-01-06 16:23:52,925 - Epoch: [157][  120/  246]    Overall Loss 0.305550    Objective Loss 0.305550                                        LR 0.000008    Time 0.021172    
2023-01-06 16:23:53,090 - Epoch: [157][  130/  246]    Overall Loss 0.305666    Objective Loss 0.305666                                        LR 0.000008    Time 0.020814    
2023-01-06 16:23:53,253 - Epoch: [157][  140/  246]    Overall Loss 0.305352    Objective Loss 0.305352                                        LR 0.000008    Time 0.020486    
2023-01-06 16:23:53,417 - Epoch: [157][  150/  246]    Overall Loss 0.304935    Objective Loss 0.304935                                        LR 0.000008    Time 0.020216    
2023-01-06 16:23:53,585 - Epoch: [157][  160/  246]    Overall Loss 0.305226    Objective Loss 0.305226                                        LR 0.000008    Time 0.020000    
2023-01-06 16:23:53,750 - Epoch: [157][  170/  246]    Overall Loss 0.304753    Objective Loss 0.304753                                        LR 0.000008    Time 0.019791    
2023-01-06 16:23:53,916 - Epoch: [157][  180/  246]    Overall Loss 0.304800    Objective Loss 0.304800                                        LR 0.000008    Time 0.019608    
2023-01-06 16:23:54,078 - Epoch: [157][  190/  246]    Overall Loss 0.305219    Objective Loss 0.305219                                        LR 0.000008    Time 0.019431    
2023-01-06 16:23:54,248 - Epoch: [157][  200/  246]    Overall Loss 0.304691    Objective Loss 0.304691                                        LR 0.000008    Time 0.019306    
2023-01-06 16:23:54,405 - Epoch: [157][  210/  246]    Overall Loss 0.305315    Objective Loss 0.305315                                        LR 0.000008    Time 0.019135    
2023-01-06 16:23:54,564 - Epoch: [157][  220/  246]    Overall Loss 0.305111    Objective Loss 0.305111                                        LR 0.000008    Time 0.018983    
2023-01-06 16:23:54,721 - Epoch: [157][  230/  246]    Overall Loss 0.306232    Objective Loss 0.306232                                        LR 0.000008    Time 0.018838    
2023-01-06 16:23:54,892 - Epoch: [157][  240/  246]    Overall Loss 0.306074    Objective Loss 0.306074                                        LR 0.000008    Time 0.018768    
2023-01-06 16:23:54,972 - Epoch: [157][  246/  246]    Overall Loss 0.306264    Objective Loss 0.306264    Top1 88.995215    LR 0.000008    Time 0.018632    
2023-01-06 16:23:55,151 - --- validate (epoch=157)-----------
2023-01-06 16:23:55,151 - 6986 samples (256 per mini-batch)
2023-01-06 16:23:55,592 - Epoch: [157][   10/   28]    Loss 0.294764    Top1 89.179688    
2023-01-06 16:23:55,705 - Epoch: [157][   20/   28]    Loss 0.309479    Top1 88.613281    
2023-01-06 16:23:55,761 - Epoch: [157][   28/   28]    Loss 0.308652    Top1 88.720298    
2023-01-06 16:23:55,896 - ==> Top1: 88.720    Loss: 0.309

2023-01-06 16:23:55,896 - ==> Confusion:
[[ 189    7  243]
 [  11  192  399]
 [  60   68 5817]]

2023-01-06 16:23:55,897 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:23:55,897 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:23:55,902 - 

2023-01-06 16:23:55,902 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:23:56,441 - Epoch: [158][   10/  246]    Overall Loss 0.310697    Objective Loss 0.310697                                        LR 0.000008    Time 0.053818    
2023-01-06 16:23:56,608 - Epoch: [158][   20/  246]    Overall Loss 0.315486    Objective Loss 0.315486                                        LR 0.000008    Time 0.035179    
2023-01-06 16:23:56,787 - Epoch: [158][   30/  246]    Overall Loss 0.312327    Objective Loss 0.312327                                        LR 0.000008    Time 0.029400    
2023-01-06 16:23:56,963 - Epoch: [158][   40/  246]    Overall Loss 0.314990    Objective Loss 0.314990                                        LR 0.000008    Time 0.026437    
2023-01-06 16:23:57,136 - Epoch: [158][   50/  246]    Overall Loss 0.313237    Objective Loss 0.313237                                        LR 0.000008    Time 0.024600    
2023-01-06 16:23:57,313 - Epoch: [158][   60/  246]    Overall Loss 0.312207    Objective Loss 0.312207                                        LR 0.000008    Time 0.023445    
2023-01-06 16:23:57,505 - Epoch: [158][   70/  246]    Overall Loss 0.309212    Objective Loss 0.309212                                        LR 0.000008    Time 0.022826    
2023-01-06 16:23:57,709 - Epoch: [158][   80/  246]    Overall Loss 0.308164    Objective Loss 0.308164                                        LR 0.000008    Time 0.022524    
2023-01-06 16:23:57,912 - Epoch: [158][   90/  246]    Overall Loss 0.307696    Objective Loss 0.307696                                        LR 0.000008    Time 0.022274    
2023-01-06 16:23:58,113 - Epoch: [158][  100/  246]    Overall Loss 0.308588    Objective Loss 0.308588                                        LR 0.000008    Time 0.022049    
2023-01-06 16:23:58,309 - Epoch: [158][  110/  246]    Overall Loss 0.307652    Objective Loss 0.307652                                        LR 0.000008    Time 0.021824    
2023-01-06 16:23:58,497 - Epoch: [158][  120/  246]    Overall Loss 0.306165    Objective Loss 0.306165                                        LR 0.000008    Time 0.021565    
2023-01-06 16:23:58,685 - Epoch: [158][  130/  246]    Overall Loss 0.307072    Objective Loss 0.307072                                        LR 0.000008    Time 0.021348    
2023-01-06 16:23:58,872 - Epoch: [158][  140/  246]    Overall Loss 0.307715    Objective Loss 0.307715                                        LR 0.000008    Time 0.021160    
2023-01-06 16:23:59,065 - Epoch: [158][  150/  246]    Overall Loss 0.309244    Objective Loss 0.309244                                        LR 0.000008    Time 0.021034    
2023-01-06 16:23:59,252 - Epoch: [158][  160/  246]    Overall Loss 0.309467    Objective Loss 0.309467                                        LR 0.000008    Time 0.020883    
2023-01-06 16:23:59,438 - Epoch: [158][  170/  246]    Overall Loss 0.308122    Objective Loss 0.308122                                        LR 0.000008    Time 0.020749    
2023-01-06 16:23:59,626 - Epoch: [158][  180/  246]    Overall Loss 0.307016    Objective Loss 0.307016                                        LR 0.000008    Time 0.020634    
2023-01-06 16:23:59,813 - Epoch: [158][  190/  246]    Overall Loss 0.307229    Objective Loss 0.307229                                        LR 0.000008    Time 0.020531    
2023-01-06 16:24:00,000 - Epoch: [158][  200/  246]    Overall Loss 0.307726    Objective Loss 0.307726                                        LR 0.000008    Time 0.020438    
2023-01-06 16:24:00,186 - Epoch: [158][  210/  246]    Overall Loss 0.307575    Objective Loss 0.307575                                        LR 0.000008    Time 0.020351    
2023-01-06 16:24:00,373 - Epoch: [158][  220/  246]    Overall Loss 0.306355    Objective Loss 0.306355                                        LR 0.000008    Time 0.020274    
2023-01-06 16:24:00,559 - Epoch: [158][  230/  246]    Overall Loss 0.306122    Objective Loss 0.306122                                        LR 0.000008    Time 0.020199    
2023-01-06 16:24:00,754 - Epoch: [158][  240/  246]    Overall Loss 0.306463    Objective Loss 0.306463                                        LR 0.000008    Time 0.020169    
2023-01-06 16:24:00,835 - Epoch: [158][  246/  246]    Overall Loss 0.306586    Objective Loss 0.306586    Top1 87.320574    LR 0.000008    Time 0.020005    
2023-01-06 16:24:00,976 - --- validate (epoch=158)-----------
2023-01-06 16:24:00,977 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:01,554 - Epoch: [158][   10/   28]    Loss 0.330209    Top1 88.281250    
2023-01-06 16:24:01,660 - Epoch: [158][   20/   28]    Loss 0.314418    Top1 88.281250    
2023-01-06 16:24:01,716 - Epoch: [158][   28/   28]    Loss 0.308370    Top1 88.419697    
2023-01-06 16:24:01,889 - ==> Top1: 88.420    Loss: 0.308

2023-01-06 16:24:01,889 - ==> Confusion:
[[ 161    8  270]
 [   8  185  409]
 [  48   66 5831]]

2023-01-06 16:24:01,891 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:24:01,891 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:01,895 - 

2023-01-06 16:24:01,895 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:02,453 - Epoch: [159][   10/  246]    Overall Loss 0.311163    Objective Loss 0.311163                                        LR 0.000008    Time 0.055682    
2023-01-06 16:24:02,620 - Epoch: [159][   20/  246]    Overall Loss 0.309109    Objective Loss 0.309109                                        LR 0.000008    Time 0.036115    
2023-01-06 16:24:02,794 - Epoch: [159][   30/  246]    Overall Loss 0.311864    Objective Loss 0.311864                                        LR 0.000008    Time 0.029834    
2023-01-06 16:24:02,971 - Epoch: [159][   40/  246]    Overall Loss 0.310732    Objective Loss 0.310732                                        LR 0.000008    Time 0.026813    
2023-01-06 16:24:03,142 - Epoch: [159][   50/  246]    Overall Loss 0.311988    Objective Loss 0.311988                                        LR 0.000008    Time 0.024860    
2023-01-06 16:24:03,288 - Epoch: [159][   60/  246]    Overall Loss 0.311276    Objective Loss 0.311276                                        LR 0.000008    Time 0.023136    
2023-01-06 16:24:03,423 - Epoch: [159][   70/  246]    Overall Loss 0.310332    Objective Loss 0.310332                                        LR 0.000008    Time 0.021753    
2023-01-06 16:24:03,569 - Epoch: [159][   80/  246]    Overall Loss 0.309645    Objective Loss 0.309645                                        LR 0.000008    Time 0.020832    
2023-01-06 16:24:03,709 - Epoch: [159][   90/  246]    Overall Loss 0.308715    Objective Loss 0.308715                                        LR 0.000008    Time 0.020063    
2023-01-06 16:24:03,851 - Epoch: [159][  100/  246]    Overall Loss 0.306243    Objective Loss 0.306243                                        LR 0.000008    Time 0.019477    
2023-01-06 16:24:03,999 - Epoch: [159][  110/  246]    Overall Loss 0.305595    Objective Loss 0.305595                                        LR 0.000008    Time 0.019043    
2023-01-06 16:24:04,144 - Epoch: [159][  120/  246]    Overall Loss 0.304900    Objective Loss 0.304900                                        LR 0.000008    Time 0.018664    
2023-01-06 16:24:04,286 - Epoch: [159][  130/  246]    Overall Loss 0.306781    Objective Loss 0.306781                                        LR 0.000008    Time 0.018318    
2023-01-06 16:24:04,431 - Epoch: [159][  140/  246]    Overall Loss 0.306580    Objective Loss 0.306580                                        LR 0.000008    Time 0.018043    
2023-01-06 16:24:04,577 - Epoch: [159][  150/  246]    Overall Loss 0.306063    Objective Loss 0.306063                                        LR 0.000008    Time 0.017807    
2023-01-06 16:24:04,730 - Epoch: [159][  160/  246]    Overall Loss 0.306667    Objective Loss 0.306667                                        LR 0.000008    Time 0.017649    
2023-01-06 16:24:04,875 - Epoch: [159][  170/  246]    Overall Loss 0.306999    Objective Loss 0.306999                                        LR 0.000008    Time 0.017460    
2023-01-06 16:24:05,026 - Epoch: [159][  180/  246]    Overall Loss 0.307142    Objective Loss 0.307142                                        LR 0.000008    Time 0.017330    
2023-01-06 16:24:05,179 - Epoch: [159][  190/  246]    Overall Loss 0.306754    Objective Loss 0.306754                                        LR 0.000008    Time 0.017219    
2023-01-06 16:24:05,329 - Epoch: [159][  200/  246]    Overall Loss 0.307214    Objective Loss 0.307214                                        LR 0.000008    Time 0.017105    
2023-01-06 16:24:05,480 - Epoch: [159][  210/  246]    Overall Loss 0.307287    Objective Loss 0.307287                                        LR 0.000008    Time 0.017009    
2023-01-06 16:24:05,624 - Epoch: [159][  220/  246]    Overall Loss 0.306467    Objective Loss 0.306467                                        LR 0.000008    Time 0.016888    
2023-01-06 16:24:05,769 - Epoch: [159][  230/  246]    Overall Loss 0.306503    Objective Loss 0.306503                                        LR 0.000008    Time 0.016781    
2023-01-06 16:24:05,941 - Epoch: [159][  240/  246]    Overall Loss 0.306450    Objective Loss 0.306450                                        LR 0.000008    Time 0.016797    
2023-01-06 16:24:06,021 - Epoch: [159][  246/  246]    Overall Loss 0.306019    Objective Loss 0.306019    Top1 90.430622    LR 0.000008    Time 0.016712    
2023-01-06 16:24:06,162 - --- validate (epoch=159)-----------
2023-01-06 16:24:06,162 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:06,601 - Epoch: [159][   10/   28]    Loss 0.323522    Top1 87.695312    
2023-01-06 16:24:06,703 - Epoch: [159][   20/   28]    Loss 0.314560    Top1 88.300781    
2023-01-06 16:24:06,761 - Epoch: [159][   28/   28]    Loss 0.306498    Top1 88.505583    
2023-01-06 16:24:06,894 - ==> Top1: 88.506    Loss: 0.306

2023-01-06 16:24:06,894 - ==> Confusion:
[[ 168    9  262]
 [   8  187  407]
 [  51   66 5828]]

2023-01-06 16:24:06,896 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:24:06,896 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:06,900 - 

2023-01-06 16:24:06,900 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:07,591 - Epoch: [160][   10/  246]    Overall Loss 0.309942    Objective Loss 0.309942                                        LR 0.000008    Time 0.069046    
2023-01-06 16:24:07,768 - Epoch: [160][   20/  246]    Overall Loss 0.309499    Objective Loss 0.309499                                        LR 0.000008    Time 0.043332    
2023-01-06 16:24:07,944 - Epoch: [160][   30/  246]    Overall Loss 0.307143    Objective Loss 0.307143                                        LR 0.000008    Time 0.034739    
2023-01-06 16:24:08,123 - Epoch: [160][   40/  246]    Overall Loss 0.302307    Objective Loss 0.302307                                        LR 0.000008    Time 0.030515    
2023-01-06 16:24:08,299 - Epoch: [160][   50/  246]    Overall Loss 0.301180    Objective Loss 0.301180                                        LR 0.000008    Time 0.027927    
2023-01-06 16:24:08,481 - Epoch: [160][   60/  246]    Overall Loss 0.302613    Objective Loss 0.302613                                        LR 0.000008    Time 0.026302    
2023-01-06 16:24:08,658 - Epoch: [160][   70/  246]    Overall Loss 0.305661    Objective Loss 0.305661                                        LR 0.000008    Time 0.025061    
2023-01-06 16:24:08,837 - Epoch: [160][   80/  246]    Overall Loss 0.307439    Objective Loss 0.307439                                        LR 0.000008    Time 0.024166    
2023-01-06 16:24:09,020 - Epoch: [160][   90/  246]    Overall Loss 0.307611    Objective Loss 0.307611                                        LR 0.000008    Time 0.023511    
2023-01-06 16:24:09,200 - Epoch: [160][  100/  246]    Overall Loss 0.306836    Objective Loss 0.306836                                        LR 0.000008    Time 0.022951    
2023-01-06 16:24:09,412 - Epoch: [160][  110/  246]    Overall Loss 0.307174    Objective Loss 0.307174                                        LR 0.000008    Time 0.022791    
2023-01-06 16:24:09,633 - Epoch: [160][  120/  246]    Overall Loss 0.306956    Objective Loss 0.306956                                        LR 0.000008    Time 0.022727    
2023-01-06 16:24:09,830 - Epoch: [160][  130/  246]    Overall Loss 0.307731    Objective Loss 0.307731                                        LR 0.000008    Time 0.022495    
2023-01-06 16:24:09,989 - Epoch: [160][  140/  246]    Overall Loss 0.307540    Objective Loss 0.307540                                        LR 0.000008    Time 0.022022    
2023-01-06 16:24:10,172 - Epoch: [160][  150/  246]    Overall Loss 0.307586    Objective Loss 0.307586                                        LR 0.000008    Time 0.021770    
2023-01-06 16:24:10,355 - Epoch: [160][  160/  246]    Overall Loss 0.308232    Objective Loss 0.308232                                        LR 0.000008    Time 0.021545    
2023-01-06 16:24:10,519 - Epoch: [160][  170/  246]    Overall Loss 0.306914    Objective Loss 0.306914                                        LR 0.000008    Time 0.021243    
2023-01-06 16:24:10,683 - Epoch: [160][  180/  246]    Overall Loss 0.307280    Objective Loss 0.307280                                        LR 0.000008    Time 0.020969    
2023-01-06 16:24:10,844 - Epoch: [160][  190/  246]    Overall Loss 0.307702    Objective Loss 0.307702                                        LR 0.000008    Time 0.020715    
2023-01-06 16:24:11,009 - Epoch: [160][  200/  246]    Overall Loss 0.306963    Objective Loss 0.306963                                        LR 0.000008    Time 0.020500    
2023-01-06 16:24:11,170 - Epoch: [160][  210/  246]    Overall Loss 0.306808    Objective Loss 0.306808                                        LR 0.000008    Time 0.020289    
2023-01-06 16:24:11,335 - Epoch: [160][  220/  246]    Overall Loss 0.306729    Objective Loss 0.306729                                        LR 0.000008    Time 0.020117    
2023-01-06 16:24:11,497 - Epoch: [160][  230/  246]    Overall Loss 0.305911    Objective Loss 0.305911                                        LR 0.000008    Time 0.019944    
2023-01-06 16:24:11,674 - Epoch: [160][  240/  246]    Overall Loss 0.305855    Objective Loss 0.305855                                        LR 0.000008    Time 0.019848    
2023-01-06 16:24:11,756 - Epoch: [160][  246/  246]    Overall Loss 0.305951    Objective Loss 0.305951    Top1 89.952153    LR 0.000008    Time 0.019696    
2023-01-06 16:24:11,940 - --- validate (epoch=160)-----------
2023-01-06 16:24:11,940 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:12,370 - Epoch: [160][   10/   28]    Loss 0.313929    Top1 88.476562    
2023-01-06 16:24:12,470 - Epoch: [160][   20/   28]    Loss 0.308265    Top1 88.750000    
2023-01-06 16:24:12,529 - Epoch: [160][   28/   28]    Loss 0.308395    Top1 88.591469    
2023-01-06 16:24:12,691 - ==> Top1: 88.591    Loss: 0.308

2023-01-06 16:24:12,692 - ==> Confusion:
[[ 179    8  252]
 [   8  191  403]
 [  59   67 5819]]

2023-01-06 16:24:12,693 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:24:12,693 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:12,698 - 

2023-01-06 16:24:12,698 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:13,264 - Epoch: [161][   10/  246]    Overall Loss 0.314231    Objective Loss 0.314231                                        LR 0.000008    Time 0.056524    
2023-01-06 16:24:13,444 - Epoch: [161][   20/  246]    Overall Loss 0.298861    Objective Loss 0.298861                                        LR 0.000008    Time 0.037252    
2023-01-06 16:24:13,623 - Epoch: [161][   30/  246]    Overall Loss 0.302743    Objective Loss 0.302743                                        LR 0.000008    Time 0.030776    
2023-01-06 16:24:13,808 - Epoch: [161][   40/  246]    Overall Loss 0.300766    Objective Loss 0.300766                                        LR 0.000008    Time 0.027714    
2023-01-06 16:24:13,982 - Epoch: [161][   50/  246]    Overall Loss 0.303136    Objective Loss 0.303136                                        LR 0.000008    Time 0.025641    
2023-01-06 16:24:14,164 - Epoch: [161][   60/  246]    Overall Loss 0.303778    Objective Loss 0.303778                                        LR 0.000008    Time 0.024398    
2023-01-06 16:24:14,344 - Epoch: [161][   70/  246]    Overall Loss 0.306282    Objective Loss 0.306282                                        LR 0.000008    Time 0.023455    
2023-01-06 16:24:14,526 - Epoch: [161][   80/  246]    Overall Loss 0.305501    Objective Loss 0.305501                                        LR 0.000008    Time 0.022784    
2023-01-06 16:24:14,703 - Epoch: [161][   90/  246]    Overall Loss 0.306344    Objective Loss 0.306344                                        LR 0.000008    Time 0.022213    
2023-01-06 16:24:14,886 - Epoch: [161][  100/  246]    Overall Loss 0.303632    Objective Loss 0.303632                                        LR 0.000008    Time 0.021823    
2023-01-06 16:24:15,061 - Epoch: [161][  110/  246]    Overall Loss 0.306020    Objective Loss 0.306020                                        LR 0.000008    Time 0.021426    
2023-01-06 16:24:15,243 - Epoch: [161][  120/  246]    Overall Loss 0.305761    Objective Loss 0.305761                                        LR 0.000008    Time 0.021151    
2023-01-06 16:24:15,419 - Epoch: [161][  130/  246]    Overall Loss 0.306284    Objective Loss 0.306284                                        LR 0.000008    Time 0.020876    
2023-01-06 16:24:15,603 - Epoch: [161][  140/  246]    Overall Loss 0.306392    Objective Loss 0.306392                                        LR 0.000008    Time 0.020700    
2023-01-06 16:24:15,779 - Epoch: [161][  150/  246]    Overall Loss 0.306709    Objective Loss 0.306709                                        LR 0.000008    Time 0.020487    
2023-01-06 16:24:15,961 - Epoch: [161][  160/  246]    Overall Loss 0.307568    Objective Loss 0.307568                                        LR 0.000008    Time 0.020345    
2023-01-06 16:24:16,137 - Epoch: [161][  170/  246]    Overall Loss 0.307527    Objective Loss 0.307527                                        LR 0.000008    Time 0.020183    
2023-01-06 16:24:16,320 - Epoch: [161][  180/  246]    Overall Loss 0.306583    Objective Loss 0.306583                                        LR 0.000008    Time 0.020071    
2023-01-06 16:24:16,502 - Epoch: [161][  190/  246]    Overall Loss 0.307342    Objective Loss 0.307342                                        LR 0.000008    Time 0.019973    
2023-01-06 16:24:16,698 - Epoch: [161][  200/  246]    Overall Loss 0.307390    Objective Loss 0.307390                                        LR 0.000008    Time 0.019954    
2023-01-06 16:24:16,894 - Epoch: [161][  210/  246]    Overall Loss 0.306538    Objective Loss 0.306538                                        LR 0.000008    Time 0.019932    
2023-01-06 16:24:17,089 - Epoch: [161][  220/  246]    Overall Loss 0.306595    Objective Loss 0.306595                                        LR 0.000008    Time 0.019912    
2023-01-06 16:24:17,281 - Epoch: [161][  230/  246]    Overall Loss 0.306570    Objective Loss 0.306570                                        LR 0.000008    Time 0.019879    
2023-01-06 16:24:17,486 - Epoch: [161][  240/  246]    Overall Loss 0.306418    Objective Loss 0.306418                                        LR 0.000008    Time 0.019904    
2023-01-06 16:24:17,567 - Epoch: [161][  246/  246]    Overall Loss 0.306145    Objective Loss 0.306145    Top1 89.952153    LR 0.000008    Time 0.019746    
2023-01-06 16:24:17,706 - --- validate (epoch=161)-----------
2023-01-06 16:24:17,707 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:18,154 - Epoch: [161][   10/   28]    Loss 0.323917    Top1 88.320312    
2023-01-06 16:24:18,253 - Epoch: [161][   20/   28]    Loss 0.314168    Top1 88.554688    
2023-01-06 16:24:18,312 - Epoch: [161][   28/   28]    Loss 0.310049    Top1 88.863441    
2023-01-06 16:24:18,472 - ==> Top1: 88.863    Loss: 0.310

2023-01-06 16:24:18,472 - ==> Confusion:
[[ 188    7  244]
 [  12  214  376]
 [  58   81 5806]]

2023-01-06 16:24:18,473 - ==> Best [Top1: 88.878   Sparsity:0.00   Params: 46192 on epoch: 148]
2023-01-06 16:24:18,473 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:18,478 - 

2023-01-06 16:24:18,478 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:19,173 - Epoch: [162][   10/  246]    Overall Loss 0.306623    Objective Loss 0.306623                                        LR 0.000008    Time 0.069428    
2023-01-06 16:24:19,350 - Epoch: [162][   20/  246]    Overall Loss 0.304014    Objective Loss 0.304014                                        LR 0.000008    Time 0.043584    
2023-01-06 16:24:19,527 - Epoch: [162][   30/  246]    Overall Loss 0.300539    Objective Loss 0.300539                                        LR 0.000008    Time 0.034939    
2023-01-06 16:24:19,707 - Epoch: [162][   40/  246]    Overall Loss 0.298261    Objective Loss 0.298261                                        LR 0.000008    Time 0.030694    
2023-01-06 16:24:19,885 - Epoch: [162][   50/  246]    Overall Loss 0.299982    Objective Loss 0.299982                                        LR 0.000008    Time 0.028116    
2023-01-06 16:24:20,064 - Epoch: [162][   60/  246]    Overall Loss 0.300370    Objective Loss 0.300370                                        LR 0.000008    Time 0.026403    
2023-01-06 16:24:20,234 - Epoch: [162][   70/  246]    Overall Loss 0.300222    Objective Loss 0.300222                                        LR 0.000008    Time 0.025054    
2023-01-06 16:24:20,409 - Epoch: [162][   80/  246]    Overall Loss 0.302832    Objective Loss 0.302832                                        LR 0.000008    Time 0.024111    
2023-01-06 16:24:20,580 - Epoch: [162][   90/  246]    Overall Loss 0.303452    Objective Loss 0.303452                                        LR 0.000008    Time 0.023319    
2023-01-06 16:24:20,756 - Epoch: [162][  100/  246]    Overall Loss 0.301732    Objective Loss 0.301732                                        LR 0.000008    Time 0.022746    
2023-01-06 16:24:20,927 - Epoch: [162][  110/  246]    Overall Loss 0.302464    Objective Loss 0.302464                                        LR 0.000008    Time 0.022233    
2023-01-06 16:24:21,103 - Epoch: [162][  120/  246]    Overall Loss 0.303291    Objective Loss 0.303291                                        LR 0.000008    Time 0.021843    
2023-01-06 16:24:21,273 - Epoch: [162][  130/  246]    Overall Loss 0.304557    Objective Loss 0.304557                                        LR 0.000008    Time 0.021466    
2023-01-06 16:24:21,449 - Epoch: [162][  140/  246]    Overall Loss 0.304982    Objective Loss 0.304982                                        LR 0.000008    Time 0.021191    
2023-01-06 16:24:21,622 - Epoch: [162][  150/  246]    Overall Loss 0.305450    Objective Loss 0.305450                                        LR 0.000008    Time 0.020928    
2023-01-06 16:24:21,795 - Epoch: [162][  160/  246]    Overall Loss 0.306028    Objective Loss 0.306028                                        LR 0.000008    Time 0.020696    
2023-01-06 16:24:21,962 - Epoch: [162][  170/  246]    Overall Loss 0.305872    Objective Loss 0.305872                                        LR 0.000008    Time 0.020462    
2023-01-06 16:24:22,129 - Epoch: [162][  180/  246]    Overall Loss 0.305885    Objective Loss 0.305885                                        LR 0.000008    Time 0.020250    
2023-01-06 16:24:22,310 - Epoch: [162][  190/  246]    Overall Loss 0.305824    Objective Loss 0.305824                                        LR 0.000008    Time 0.020135    
2023-01-06 16:24:22,502 - Epoch: [162][  200/  246]    Overall Loss 0.305630    Objective Loss 0.305630                                        LR 0.000008    Time 0.020084    
2023-01-06 16:24:22,682 - Epoch: [162][  210/  246]    Overall Loss 0.305258    Objective Loss 0.305258                                        LR 0.000008    Time 0.019986    
2023-01-06 16:24:22,872 - Epoch: [162][  220/  246]    Overall Loss 0.305046    Objective Loss 0.305046                                        LR 0.000008    Time 0.019940    
2023-01-06 16:24:23,054 - Epoch: [162][  230/  246]    Overall Loss 0.305858    Objective Loss 0.305858                                        LR 0.000008    Time 0.019854    
2023-01-06 16:24:23,248 - Epoch: [162][  240/  246]    Overall Loss 0.306156    Objective Loss 0.306156                                        LR 0.000008    Time 0.019833    
2023-01-06 16:24:23,329 - Epoch: [162][  246/  246]    Overall Loss 0.306482    Objective Loss 0.306482    Top1 86.363636    LR 0.000008    Time 0.019676    
2023-01-06 16:24:23,464 - --- validate (epoch=162)-----------
2023-01-06 16:24:23,464 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:23,893 - Epoch: [162][   10/   28]    Loss 0.303929    Top1 89.140625    
2023-01-06 16:24:23,997 - Epoch: [162][   20/   28]    Loss 0.307641    Top1 88.710938    
2023-01-06 16:24:24,054 - Epoch: [162][   28/   28]    Loss 0.307829    Top1 88.906384    
2023-01-06 16:24:24,174 - ==> Top1: 88.906    Loss: 0.308

2023-01-06 16:24:24,174 - ==> Confusion:
[[ 194    7  238]
 [  12  211  379]
 [  66   73 5806]]

2023-01-06 16:24:24,176 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:24,176 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:24,181 - 

2023-01-06 16:24:24,181 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:24,859 - Epoch: [163][   10/  246]    Overall Loss 0.288591    Objective Loss 0.288591                                        LR 0.000008    Time 0.067706    
2023-01-06 16:24:25,015 - Epoch: [163][   20/  246]    Overall Loss 0.302715    Objective Loss 0.302715                                        LR 0.000008    Time 0.041658    
2023-01-06 16:24:25,171 - Epoch: [163][   30/  246]    Overall Loss 0.302137    Objective Loss 0.302137                                        LR 0.000008    Time 0.032951    
2023-01-06 16:24:25,332 - Epoch: [163][   40/  246]    Overall Loss 0.306239    Objective Loss 0.306239                                        LR 0.000008    Time 0.028719    
2023-01-06 16:24:25,493 - Epoch: [163][   50/  246]    Overall Loss 0.302949    Objective Loss 0.302949                                        LR 0.000008    Time 0.026185    
2023-01-06 16:24:25,637 - Epoch: [163][   60/  246]    Overall Loss 0.301099    Objective Loss 0.301099                                        LR 0.000008    Time 0.024227    
2023-01-06 16:24:25,789 - Epoch: [163][   70/  246]    Overall Loss 0.300205    Objective Loss 0.300205                                        LR 0.000008    Time 0.022931    
2023-01-06 16:24:25,953 - Epoch: [163][   80/  246]    Overall Loss 0.300647    Objective Loss 0.300647                                        LR 0.000008    Time 0.022105    
2023-01-06 16:24:26,115 - Epoch: [163][   90/  246]    Overall Loss 0.301634    Objective Loss 0.301634                                        LR 0.000008    Time 0.021445    
2023-01-06 16:24:26,285 - Epoch: [163][  100/  246]    Overall Loss 0.302676    Objective Loss 0.302676                                        LR 0.000008    Time 0.021003    
2023-01-06 16:24:26,435 - Epoch: [163][  110/  246]    Overall Loss 0.302761    Objective Loss 0.302761                                        LR 0.000008    Time 0.020449    
2023-01-06 16:24:26,584 - Epoch: [163][  120/  246]    Overall Loss 0.303323    Objective Loss 0.303323                                        LR 0.000008    Time 0.019987    
2023-01-06 16:24:26,730 - Epoch: [163][  130/  246]    Overall Loss 0.302525    Objective Loss 0.302525                                        LR 0.000008    Time 0.019571    
2023-01-06 16:24:26,881 - Epoch: [163][  140/  246]    Overall Loss 0.303656    Objective Loss 0.303656                                        LR 0.000008    Time 0.019233    
2023-01-06 16:24:27,037 - Epoch: [163][  150/  246]    Overall Loss 0.305114    Objective Loss 0.305114                                        LR 0.000008    Time 0.018988    
2023-01-06 16:24:27,194 - Epoch: [163][  160/  246]    Overall Loss 0.305510    Objective Loss 0.305510                                        LR 0.000008    Time 0.018787    
2023-01-06 16:24:27,344 - Epoch: [163][  170/  246]    Overall Loss 0.305326    Objective Loss 0.305326                                        LR 0.000008    Time 0.018559    
2023-01-06 16:24:27,506 - Epoch: [163][  180/  246]    Overall Loss 0.304988    Objective Loss 0.304988                                        LR 0.000008    Time 0.018429    
2023-01-06 16:24:27,668 - Epoch: [163][  190/  246]    Overall Loss 0.306044    Objective Loss 0.306044                                        LR 0.000008    Time 0.018306    
2023-01-06 16:24:27,832 - Epoch: [163][  200/  246]    Overall Loss 0.305680    Objective Loss 0.305680                                        LR 0.000008    Time 0.018212    
2023-01-06 16:24:27,992 - Epoch: [163][  210/  246]    Overall Loss 0.305832    Objective Loss 0.305832                                        LR 0.000008    Time 0.018104    
2023-01-06 16:24:28,151 - Epoch: [163][  220/  246]    Overall Loss 0.305524    Objective Loss 0.305524                                        LR 0.000008    Time 0.018001    
2023-01-06 16:24:28,307 - Epoch: [163][  230/  246]    Overall Loss 0.305413    Objective Loss 0.305413                                        LR 0.000008    Time 0.017898    
2023-01-06 16:24:28,480 - Epoch: [163][  240/  246]    Overall Loss 0.305945    Objective Loss 0.305945                                        LR 0.000008    Time 0.017872    
2023-01-06 16:24:28,558 - Epoch: [163][  246/  246]    Overall Loss 0.305192    Objective Loss 0.305192    Top1 90.430622    LR 0.000008    Time 0.017751    
2023-01-06 16:24:28,743 - --- validate (epoch=163)-----------
2023-01-06 16:24:28,744 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:29,189 - Epoch: [163][   10/   28]    Loss 0.323783    Top1 87.773438    
2023-01-06 16:24:29,293 - Epoch: [163][   20/   28]    Loss 0.313814    Top1 88.222656    
2023-01-06 16:24:29,351 - Epoch: [163][   28/   28]    Loss 0.309665    Top1 88.362439    
2023-01-06 16:24:29,493 - ==> Top1: 88.362    Loss: 0.310

2023-01-06 16:24:29,493 - ==> Confusion:
[[ 165    7  267]
 [   8  181  413]
 [  50   68 5827]]

2023-01-06 16:24:29,495 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:29,495 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:29,499 - 

2023-01-06 16:24:29,499 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:30,041 - Epoch: [164][   10/  246]    Overall Loss 0.301098    Objective Loss 0.301098                                        LR 0.000008    Time 0.054087    
2023-01-06 16:24:30,217 - Epoch: [164][   20/  246]    Overall Loss 0.291859    Objective Loss 0.291859                                        LR 0.000008    Time 0.035810    
2023-01-06 16:24:30,391 - Epoch: [164][   30/  246]    Overall Loss 0.294495    Objective Loss 0.294495                                        LR 0.000008    Time 0.029681    
2023-01-06 16:24:30,556 - Epoch: [164][   40/  246]    Overall Loss 0.305801    Objective Loss 0.305801                                        LR 0.000008    Time 0.026330    
2023-01-06 16:24:30,722 - Epoch: [164][   50/  246]    Overall Loss 0.306267    Objective Loss 0.306267                                        LR 0.000008    Time 0.024369    
2023-01-06 16:24:30,889 - Epoch: [164][   60/  246]    Overall Loss 0.306182    Objective Loss 0.306182                                        LR 0.000008    Time 0.023050    
2023-01-06 16:24:31,064 - Epoch: [164][   70/  246]    Overall Loss 0.306975    Objective Loss 0.306975                                        LR 0.000008    Time 0.022262    
2023-01-06 16:24:31,237 - Epoch: [164][   80/  246]    Overall Loss 0.305255    Objective Loss 0.305255                                        LR 0.000008    Time 0.021629    
2023-01-06 16:24:31,410 - Epoch: [164][   90/  246]    Overall Loss 0.304793    Objective Loss 0.304793                                        LR 0.000008    Time 0.021143    
2023-01-06 16:24:31,577 - Epoch: [164][  100/  246]    Overall Loss 0.304639    Objective Loss 0.304639                                        LR 0.000008    Time 0.020701    
2023-01-06 16:24:31,748 - Epoch: [164][  110/  246]    Overall Loss 0.304975    Objective Loss 0.304975                                        LR 0.000008    Time 0.020368    
2023-01-06 16:24:31,914 - Epoch: [164][  120/  246]    Overall Loss 0.303186    Objective Loss 0.303186                                        LR 0.000008    Time 0.020051    
2023-01-06 16:24:32,088 - Epoch: [164][  130/  246]    Overall Loss 0.303083    Objective Loss 0.303083                                        LR 0.000008    Time 0.019841    
2023-01-06 16:24:32,267 - Epoch: [164][  140/  246]    Overall Loss 0.304229    Objective Loss 0.304229                                        LR 0.000008    Time 0.019691    
2023-01-06 16:24:32,432 - Epoch: [164][  150/  246]    Overall Loss 0.303602    Objective Loss 0.303602                                        LR 0.000008    Time 0.019479    
2023-01-06 16:24:32,604 - Epoch: [164][  160/  246]    Overall Loss 0.304141    Objective Loss 0.304141                                        LR 0.000008    Time 0.019332    
2023-01-06 16:24:32,770 - Epoch: [164][  170/  246]    Overall Loss 0.304184    Objective Loss 0.304184                                        LR 0.000008    Time 0.019169    
2023-01-06 16:24:32,941 - Epoch: [164][  180/  246]    Overall Loss 0.304734    Objective Loss 0.304734                                        LR 0.000008    Time 0.019050    
2023-01-06 16:24:33,108 - Epoch: [164][  190/  246]    Overall Loss 0.304967    Objective Loss 0.304967                                        LR 0.000008    Time 0.018920    
2023-01-06 16:24:33,273 - Epoch: [164][  200/  246]    Overall Loss 0.306013    Objective Loss 0.306013                                        LR 0.000008    Time 0.018797    
2023-01-06 16:24:33,440 - Epoch: [164][  210/  246]    Overall Loss 0.306346    Objective Loss 0.306346                                        LR 0.000008    Time 0.018693    
2023-01-06 16:24:33,599 - Epoch: [164][  220/  246]    Overall Loss 0.306515    Objective Loss 0.306515                                        LR 0.000008    Time 0.018567    
2023-01-06 16:24:33,756 - Epoch: [164][  230/  246]    Overall Loss 0.306800    Objective Loss 0.306800                                        LR 0.000008    Time 0.018439    
2023-01-06 16:24:33,925 - Epoch: [164][  240/  246]    Overall Loss 0.306561    Objective Loss 0.306561                                        LR 0.000008    Time 0.018373    
2023-01-06 16:24:34,004 - Epoch: [164][  246/  246]    Overall Loss 0.306201    Objective Loss 0.306201    Top1 88.038278    LR 0.000008    Time 0.018246    
2023-01-06 16:24:34,151 - --- validate (epoch=164)-----------
2023-01-06 16:24:34,151 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:34,575 - Epoch: [164][   10/   28]    Loss 0.304781    Top1 88.593750    
2023-01-06 16:24:34,682 - Epoch: [164][   20/   28]    Loss 0.304059    Top1 88.554688    
2023-01-06 16:24:34,741 - Epoch: [164][   28/   28]    Loss 0.305839    Top1 88.591469    
2023-01-06 16:24:34,880 - ==> Top1: 88.591    Loss: 0.306

2023-01-06 16:24:34,880 - ==> Confusion:
[[ 181    7  251]
 [  11  193  398]
 [  56   74 5815]]

2023-01-06 16:24:34,881 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:34,882 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:34,886 - 

2023-01-06 16:24:34,886 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:35,556 - Epoch: [165][   10/  246]    Overall Loss 0.300366    Objective Loss 0.300366                                        LR 0.000008    Time 0.066882    
2023-01-06 16:24:35,700 - Epoch: [165][   20/  246]    Overall Loss 0.306198    Objective Loss 0.306198                                        LR 0.000008    Time 0.040668    
2023-01-06 16:24:35,859 - Epoch: [165][   30/  246]    Overall Loss 0.303304    Objective Loss 0.303304                                        LR 0.000008    Time 0.032389    
2023-01-06 16:24:36,020 - Epoch: [165][   40/  246]    Overall Loss 0.303086    Objective Loss 0.303086                                        LR 0.000008    Time 0.028304    
2023-01-06 16:24:36,181 - Epoch: [165][   50/  246]    Overall Loss 0.302768    Objective Loss 0.302768                                        LR 0.000008    Time 0.025844    
2023-01-06 16:24:36,342 - Epoch: [165][   60/  246]    Overall Loss 0.299856    Objective Loss 0.299856                                        LR 0.000008    Time 0.024214    
2023-01-06 16:24:36,510 - Epoch: [165][   70/  246]    Overall Loss 0.297846    Objective Loss 0.297846                                        LR 0.000008    Time 0.023159    
2023-01-06 16:24:36,692 - Epoch: [165][   80/  246]    Overall Loss 0.297926    Objective Loss 0.297926                                        LR 0.000008    Time 0.022530    
2023-01-06 16:24:36,895 - Epoch: [165][   90/  246]    Overall Loss 0.299687    Objective Loss 0.299687                                        LR 0.000008    Time 0.022282    
2023-01-06 16:24:37,097 - Epoch: [165][  100/  246]    Overall Loss 0.300406    Objective Loss 0.300406                                        LR 0.000008    Time 0.022062    
2023-01-06 16:24:37,311 - Epoch: [165][  110/  246]    Overall Loss 0.301182    Objective Loss 0.301182                                        LR 0.000008    Time 0.021997    
2023-01-06 16:24:37,524 - Epoch: [165][  120/  246]    Overall Loss 0.301122    Objective Loss 0.301122                                        LR 0.000008    Time 0.021938    
2023-01-06 16:24:37,735 - Epoch: [165][  130/  246]    Overall Loss 0.301532    Objective Loss 0.301532                                        LR 0.000008    Time 0.021867    
2023-01-06 16:24:37,943 - Epoch: [165][  140/  246]    Overall Loss 0.302202    Objective Loss 0.302202                                        LR 0.000008    Time 0.021789    
2023-01-06 16:24:38,153 - Epoch: [165][  150/  246]    Overall Loss 0.303238    Objective Loss 0.303238                                        LR 0.000008    Time 0.021736    
2023-01-06 16:24:38,363 - Epoch: [165][  160/  246]    Overall Loss 0.302385    Objective Loss 0.302385                                        LR 0.000008    Time 0.021685    
2023-01-06 16:24:38,571 - Epoch: [165][  170/  246]    Overall Loss 0.303377    Objective Loss 0.303377                                        LR 0.000008    Time 0.021632    
2023-01-06 16:24:38,770 - Epoch: [165][  180/  246]    Overall Loss 0.302993    Objective Loss 0.302993                                        LR 0.000008    Time 0.021532    
2023-01-06 16:24:38,947 - Epoch: [165][  190/  246]    Overall Loss 0.302420    Objective Loss 0.302420                                        LR 0.000008    Time 0.021327    
2023-01-06 16:24:39,121 - Epoch: [165][  200/  246]    Overall Loss 0.303148    Objective Loss 0.303148                                        LR 0.000008    Time 0.021129    
2023-01-06 16:24:39,292 - Epoch: [165][  210/  246]    Overall Loss 0.304567    Objective Loss 0.304567                                        LR 0.000008    Time 0.020936    
2023-01-06 16:24:39,465 - Epoch: [165][  220/  246]    Overall Loss 0.304553    Objective Loss 0.304553                                        LR 0.000008    Time 0.020768    
2023-01-06 16:24:39,640 - Epoch: [165][  230/  246]    Overall Loss 0.305254    Objective Loss 0.305254                                        LR 0.000008    Time 0.020626    
2023-01-06 16:24:39,826 - Epoch: [165][  240/  246]    Overall Loss 0.305305    Objective Loss 0.305305                                        LR 0.000008    Time 0.020539    
2023-01-06 16:24:39,908 - Epoch: [165][  246/  246]    Overall Loss 0.305937    Objective Loss 0.305937    Top1 87.320574    LR 0.000008    Time 0.020370    
2023-01-06 16:24:40,041 - --- validate (epoch=165)-----------
2023-01-06 16:24:40,041 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:40,472 - Epoch: [165][   10/   28]    Loss 0.325485    Top1 87.773438    
2023-01-06 16:24:40,574 - Epoch: [165][   20/   28]    Loss 0.309129    Top1 88.652344    
2023-01-06 16:24:40,633 - Epoch: [165][   28/   28]    Loss 0.307892    Top1 88.634412    
2023-01-06 16:24:40,794 - ==> Top1: 88.634    Loss: 0.308

2023-01-06 16:24:40,795 - ==> Confusion:
[[ 195    8  236]
 [   9  211  382]
 [  73   86 5786]]

2023-01-06 16:24:40,796 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:40,796 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:40,800 - 

2023-01-06 16:24:40,800 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:41,344 - Epoch: [166][   10/  246]    Overall Loss 0.321583    Objective Loss 0.321583                                        LR 0.000008    Time 0.054252    
2023-01-06 16:24:41,488 - Epoch: [166][   20/  246]    Overall Loss 0.316179    Objective Loss 0.316179                                        LR 0.000008    Time 0.034332    
2023-01-06 16:24:41,647 - Epoch: [166][   30/  246]    Overall Loss 0.313819    Objective Loss 0.313819                                        LR 0.000008    Time 0.028181    
2023-01-06 16:24:41,808 - Epoch: [166][   40/  246]    Overall Loss 0.312843    Objective Loss 0.312843                                        LR 0.000008    Time 0.025144    
2023-01-06 16:24:41,973 - Epoch: [166][   50/  246]    Overall Loss 0.313329    Objective Loss 0.313329                                        LR 0.000008    Time 0.023386    
2023-01-06 16:24:42,139 - Epoch: [166][   60/  246]    Overall Loss 0.309007    Objective Loss 0.309007                                        LR 0.000008    Time 0.022253    
2023-01-06 16:24:42,307 - Epoch: [166][   70/  246]    Overall Loss 0.304070    Objective Loss 0.304070                                        LR 0.000008    Time 0.021466    
2023-01-06 16:24:42,474 - Epoch: [166][   80/  246]    Overall Loss 0.303543    Objective Loss 0.303543                                        LR 0.000008    Time 0.020872    
2023-01-06 16:24:42,641 - Epoch: [166][   90/  246]    Overall Loss 0.303995    Objective Loss 0.303995                                        LR 0.000008    Time 0.020399    
2023-01-06 16:24:42,807 - Epoch: [166][  100/  246]    Overall Loss 0.304396    Objective Loss 0.304396                                        LR 0.000008    Time 0.020021    
2023-01-06 16:24:42,975 - Epoch: [166][  110/  246]    Overall Loss 0.304954    Objective Loss 0.304954                                        LR 0.000008    Time 0.019719    
2023-01-06 16:24:43,142 - Epoch: [166][  120/  246]    Overall Loss 0.304298    Objective Loss 0.304298                                        LR 0.000008    Time 0.019465    
2023-01-06 16:24:43,308 - Epoch: [166][  130/  246]    Overall Loss 0.304600    Objective Loss 0.304600                                        LR 0.000008    Time 0.019246    
2023-01-06 16:24:43,475 - Epoch: [166][  140/  246]    Overall Loss 0.304361    Objective Loss 0.304361                                        LR 0.000008    Time 0.019059    
2023-01-06 16:24:43,641 - Epoch: [166][  150/  246]    Overall Loss 0.303829    Objective Loss 0.303829                                        LR 0.000008    Time 0.018896    
2023-01-06 16:24:43,808 - Epoch: [166][  160/  246]    Overall Loss 0.303810    Objective Loss 0.303810                                        LR 0.000008    Time 0.018752    
2023-01-06 16:24:43,975 - Epoch: [166][  170/  246]    Overall Loss 0.304436    Objective Loss 0.304436                                        LR 0.000008    Time 0.018633    
2023-01-06 16:24:44,143 - Epoch: [166][  180/  246]    Overall Loss 0.305048    Objective Loss 0.305048                                        LR 0.000008    Time 0.018527    
2023-01-06 16:24:44,316 - Epoch: [166][  190/  246]    Overall Loss 0.304003    Objective Loss 0.304003                                        LR 0.000008    Time 0.018460    
2023-01-06 16:24:44,488 - Epoch: [166][  200/  246]    Overall Loss 0.304133    Objective Loss 0.304133                                        LR 0.000008    Time 0.018395    
2023-01-06 16:24:44,664 - Epoch: [166][  210/  246]    Overall Loss 0.305861    Objective Loss 0.305861                                        LR 0.000008    Time 0.018355    
2023-01-06 16:24:44,836 - Epoch: [166][  220/  246]    Overall Loss 0.305714    Objective Loss 0.305714                                        LR 0.000008    Time 0.018301    
2023-01-06 16:24:45,010 - Epoch: [166][  230/  246]    Overall Loss 0.305948    Objective Loss 0.305948                                        LR 0.000008    Time 0.018263    
2023-01-06 16:24:45,192 - Epoch: [166][  240/  246]    Overall Loss 0.306058    Objective Loss 0.306058                                        LR 0.000008    Time 0.018258    
2023-01-06 16:24:45,275 - Epoch: [166][  246/  246]    Overall Loss 0.305840    Objective Loss 0.305840    Top1 87.799043    LR 0.000008    Time 0.018148    
2023-01-06 16:24:45,433 - --- validate (epoch=166)-----------
2023-01-06 16:24:45,433 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:45,871 - Epoch: [166][   10/   28]    Loss 0.297183    Top1 88.984375    
2023-01-06 16:24:45,975 - Epoch: [166][   20/   28]    Loss 0.307366    Top1 88.769531    
2023-01-06 16:24:46,035 - Epoch: [166][   28/   28]    Loss 0.306385    Top1 88.720298    
2023-01-06 16:24:46,172 - ==> Top1: 88.720    Loss: 0.306

2023-01-06 16:24:46,172 - ==> Confusion:
[[ 192    7  240]
 [  10  199  393]
 [  67   71 5807]]

2023-01-06 16:24:46,174 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:46,174 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:46,179 - 

2023-01-06 16:24:46,179 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:46,862 - Epoch: [167][   10/  246]    Overall Loss 0.302729    Objective Loss 0.302729                                        LR 0.000008    Time 0.068286    
2023-01-06 16:24:47,024 - Epoch: [167][   20/  246]    Overall Loss 0.291576    Objective Loss 0.291576                                        LR 0.000008    Time 0.042210    
2023-01-06 16:24:47,183 - Epoch: [167][   30/  246]    Overall Loss 0.302680    Objective Loss 0.302680                                        LR 0.000008    Time 0.033443    
2023-01-06 16:24:47,346 - Epoch: [167][   40/  246]    Overall Loss 0.303329    Objective Loss 0.303329                                        LR 0.000008    Time 0.029143    
2023-01-06 16:24:47,510 - Epoch: [167][   50/  246]    Overall Loss 0.305635    Objective Loss 0.305635                                        LR 0.000008    Time 0.026579    
2023-01-06 16:24:47,672 - Epoch: [167][   60/  246]    Overall Loss 0.307047    Objective Loss 0.307047                                        LR 0.000008    Time 0.024849    
2023-01-06 16:24:47,836 - Epoch: [167][   70/  246]    Overall Loss 0.307021    Objective Loss 0.307021                                        LR 0.000008    Time 0.023639    
2023-01-06 16:24:48,003 - Epoch: [167][   80/  246]    Overall Loss 0.308996    Objective Loss 0.308996                                        LR 0.000008    Time 0.022758    
2023-01-06 16:24:48,171 - Epoch: [167][   90/  246]    Overall Loss 0.307287    Objective Loss 0.307287                                        LR 0.000008    Time 0.022101    
2023-01-06 16:24:48,346 - Epoch: [167][  100/  246]    Overall Loss 0.304798    Objective Loss 0.304798                                        LR 0.000008    Time 0.021630    
2023-01-06 16:24:48,523 - Epoch: [167][  110/  246]    Overall Loss 0.304356    Objective Loss 0.304356                                        LR 0.000008    Time 0.021271    
2023-01-06 16:24:48,700 - Epoch: [167][  120/  246]    Overall Loss 0.305134    Objective Loss 0.305134                                        LR 0.000008    Time 0.020968    
2023-01-06 16:24:48,875 - Epoch: [167][  130/  246]    Overall Loss 0.305327    Objective Loss 0.305327                                        LR 0.000008    Time 0.020705    
2023-01-06 16:24:49,048 - Epoch: [167][  140/  246]    Overall Loss 0.305816    Objective Loss 0.305816                                        LR 0.000008    Time 0.020460    
2023-01-06 16:24:49,233 - Epoch: [167][  150/  246]    Overall Loss 0.305709    Objective Loss 0.305709                                        LR 0.000008    Time 0.020327    
2023-01-06 16:24:49,418 - Epoch: [167][  160/  246]    Overall Loss 0.304758    Objective Loss 0.304758                                        LR 0.000008    Time 0.020208    
2023-01-06 16:24:49,601 - Epoch: [167][  170/  246]    Overall Loss 0.304697    Objective Loss 0.304697                                        LR 0.000008    Time 0.020095    
2023-01-06 16:24:49,778 - Epoch: [167][  180/  246]    Overall Loss 0.304796    Objective Loss 0.304796                                        LR 0.000008    Time 0.019959    
2023-01-06 16:24:49,956 - Epoch: [167][  190/  246]    Overall Loss 0.305696    Objective Loss 0.305696                                        LR 0.000008    Time 0.019842    
2023-01-06 16:24:50,128 - Epoch: [167][  200/  246]    Overall Loss 0.305632    Objective Loss 0.305632                                        LR 0.000008    Time 0.019708    
2023-01-06 16:24:50,302 - Epoch: [167][  210/  246]    Overall Loss 0.305703    Objective Loss 0.305703                                        LR 0.000008    Time 0.019595    
2023-01-06 16:24:50,475 - Epoch: [167][  220/  246]    Overall Loss 0.306129    Objective Loss 0.306129                                        LR 0.000008    Time 0.019493    
2023-01-06 16:24:50,651 - Epoch: [167][  230/  246]    Overall Loss 0.306323    Objective Loss 0.306323                                        LR 0.000008    Time 0.019408    
2023-01-06 16:24:50,840 - Epoch: [167][  240/  246]    Overall Loss 0.306849    Objective Loss 0.306849                                        LR 0.000008    Time 0.019384    
2023-01-06 16:24:50,923 - Epoch: [167][  246/  246]    Overall Loss 0.306957    Objective Loss 0.306957    Top1 89.234450    LR 0.000008    Time 0.019249    
2023-01-06 16:24:51,084 - --- validate (epoch=167)-----------
2023-01-06 16:24:51,085 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:51,530 - Epoch: [167][   10/   28]    Loss 0.304124    Top1 88.984375    
2023-01-06 16:24:51,644 - Epoch: [167][   20/   28]    Loss 0.310712    Top1 88.632812    
2023-01-06 16:24:51,702 - Epoch: [167][   28/   28]    Loss 0.307729    Top1 88.834812    
2023-01-06 16:24:51,851 - ==> Top1: 88.835    Loss: 0.308

2023-01-06 16:24:51,852 - ==> Confusion:
[[ 188    8  243]
 [  15  221  366]
 [  62   86 5797]]

2023-01-06 16:24:51,853 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:51,853 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:51,858 - 

2023-01-06 16:24:51,858 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:52,579 - Epoch: [168][   10/  246]    Overall Loss 0.317454    Objective Loss 0.317454                                        LR 0.000008    Time 0.072045    
2023-01-06 16:24:52,772 - Epoch: [168][   20/  246]    Overall Loss 0.307007    Objective Loss 0.307007                                        LR 0.000008    Time 0.045625    
2023-01-06 16:24:52,947 - Epoch: [168][   30/  246]    Overall Loss 0.307243    Objective Loss 0.307243                                        LR 0.000008    Time 0.036264    
2023-01-06 16:24:53,115 - Epoch: [168][   40/  246]    Overall Loss 0.305375    Objective Loss 0.305375                                        LR 0.000008    Time 0.031323    
2023-01-06 16:24:53,287 - Epoch: [168][   50/  246]    Overall Loss 0.304168    Objective Loss 0.304168                                        LR 0.000008    Time 0.028498    
2023-01-06 16:24:53,455 - Epoch: [168][   60/  246]    Overall Loss 0.301474    Objective Loss 0.301474                                        LR 0.000008    Time 0.026548    
2023-01-06 16:24:53,624 - Epoch: [168][   70/  246]    Overall Loss 0.298485    Objective Loss 0.298485                                        LR 0.000008    Time 0.025155    
2023-01-06 16:24:53,791 - Epoch: [168][   80/  246]    Overall Loss 0.300759    Objective Loss 0.300759                                        LR 0.000008    Time 0.024089    
2023-01-06 16:24:53,957 - Epoch: [168][   90/  246]    Overall Loss 0.301216    Objective Loss 0.301216                                        LR 0.000008    Time 0.023250    
2023-01-06 16:24:54,116 - Epoch: [168][  100/  246]    Overall Loss 0.301797    Objective Loss 0.301797                                        LR 0.000008    Time 0.022510    
2023-01-06 16:24:54,276 - Epoch: [168][  110/  246]    Overall Loss 0.301916    Objective Loss 0.301916                                        LR 0.000008    Time 0.021917    
2023-01-06 16:24:54,441 - Epoch: [168][  120/  246]    Overall Loss 0.300571    Objective Loss 0.300571                                        LR 0.000008    Time 0.021462    
2023-01-06 16:24:54,609 - Epoch: [168][  130/  246]    Overall Loss 0.298653    Objective Loss 0.298653                                        LR 0.000008    Time 0.021101    
2023-01-06 16:24:54,777 - Epoch: [168][  140/  246]    Overall Loss 0.299001    Objective Loss 0.299001                                        LR 0.000008    Time 0.020783    
2023-01-06 16:24:54,950 - Epoch: [168][  150/  246]    Overall Loss 0.298658    Objective Loss 0.298658                                        LR 0.000008    Time 0.020548    
2023-01-06 16:24:55,116 - Epoch: [168][  160/  246]    Overall Loss 0.300522    Objective Loss 0.300522                                        LR 0.000008    Time 0.020296    
2023-01-06 16:24:55,282 - Epoch: [168][  170/  246]    Overall Loss 0.301385    Objective Loss 0.301385                                        LR 0.000008    Time 0.020076    
2023-01-06 16:24:55,447 - Epoch: [168][  180/  246]    Overall Loss 0.302158    Objective Loss 0.302158                                        LR 0.000008    Time 0.019879    
2023-01-06 16:24:55,615 - Epoch: [168][  190/  246]    Overall Loss 0.302204    Objective Loss 0.302204                                        LR 0.000008    Time 0.019716    
2023-01-06 16:24:55,789 - Epoch: [168][  200/  246]    Overall Loss 0.302485    Objective Loss 0.302485                                        LR 0.000008    Time 0.019598    
2023-01-06 16:24:55,961 - Epoch: [168][  210/  246]    Overall Loss 0.303667    Objective Loss 0.303667                                        LR 0.000008    Time 0.019480    
2023-01-06 16:24:56,134 - Epoch: [168][  220/  246]    Overall Loss 0.303710    Objective Loss 0.303710                                        LR 0.000008    Time 0.019380    
2023-01-06 16:24:56,313 - Epoch: [168][  230/  246]    Overall Loss 0.304638    Objective Loss 0.304638                                        LR 0.000008    Time 0.019316    
2023-01-06 16:24:56,504 - Epoch: [168][  240/  246]    Overall Loss 0.305037    Objective Loss 0.305037                                        LR 0.000008    Time 0.019304    
2023-01-06 16:24:56,585 - Epoch: [168][  246/  246]    Overall Loss 0.305592    Objective Loss 0.305592    Top1 86.363636    LR 0.000008    Time 0.019163    
2023-01-06 16:24:56,725 - --- validate (epoch=168)-----------
2023-01-06 16:24:56,725 - 6986 samples (256 per mini-batch)
2023-01-06 16:24:57,155 - Epoch: [168][   10/   28]    Loss 0.285330    Top1 89.375000    
2023-01-06 16:24:57,265 - Epoch: [168][   20/   28]    Loss 0.299974    Top1 88.652344    
2023-01-06 16:24:57,322 - Epoch: [168][   28/   28]    Loss 0.302608    Top1 88.605783    
2023-01-06 16:24:57,463 - ==> Top1: 88.606    Loss: 0.303

2023-01-06 16:24:57,463 - ==> Confusion:
[[ 181    9  249]
 [  10  197  395]
 [  60   73 5812]]

2023-01-06 16:24:57,464 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:24:57,464 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:24:57,469 - 

2023-01-06 16:24:57,469 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:24:57,998 - Epoch: [169][   10/  246]    Overall Loss 0.289286    Objective Loss 0.289286                                        LR 0.000008    Time 0.052871    
2023-01-06 16:24:58,143 - Epoch: [169][   20/  246]    Overall Loss 0.299467    Objective Loss 0.299467                                        LR 0.000008    Time 0.033621    
2023-01-06 16:24:58,290 - Epoch: [169][   30/  246]    Overall Loss 0.302902    Objective Loss 0.302902                                        LR 0.000008    Time 0.027302    
2023-01-06 16:24:58,435 - Epoch: [169][   40/  246]    Overall Loss 0.304182    Objective Loss 0.304182                                        LR 0.000008    Time 0.024106    
2023-01-06 16:24:58,577 - Epoch: [169][   50/  246]    Overall Loss 0.303352    Objective Loss 0.303352                                        LR 0.000008    Time 0.022111    
2023-01-06 16:24:58,726 - Epoch: [169][   60/  246]    Overall Loss 0.306258    Objective Loss 0.306258                                        LR 0.000008    Time 0.020907    
2023-01-06 16:24:58,893 - Epoch: [169][   70/  246]    Overall Loss 0.306664    Objective Loss 0.306664                                        LR 0.000008    Time 0.020273    
2023-01-06 16:24:59,055 - Epoch: [169][   80/  246]    Overall Loss 0.307130    Objective Loss 0.307130                                        LR 0.000008    Time 0.019756    
2023-01-06 16:24:59,221 - Epoch: [169][   90/  246]    Overall Loss 0.306805    Objective Loss 0.306805                                        LR 0.000008    Time 0.019379    
2023-01-06 16:24:59,390 - Epoch: [169][  100/  246]    Overall Loss 0.305789    Objective Loss 0.305789                                        LR 0.000008    Time 0.019127    
2023-01-06 16:24:59,553 - Epoch: [169][  110/  246]    Overall Loss 0.304363    Objective Loss 0.304363                                        LR 0.000008    Time 0.018872    
2023-01-06 16:24:59,718 - Epoch: [169][  120/  246]    Overall Loss 0.304905    Objective Loss 0.304905                                        LR 0.000008    Time 0.018668    
2023-01-06 16:24:59,882 - Epoch: [169][  130/  246]    Overall Loss 0.305660    Objective Loss 0.305660                                        LR 0.000008    Time 0.018489    
2023-01-06 16:25:00,046 - Epoch: [169][  140/  246]    Overall Loss 0.304473    Objective Loss 0.304473                                        LR 0.000008    Time 0.018340    
2023-01-06 16:25:00,210 - Epoch: [169][  150/  246]    Overall Loss 0.305334    Objective Loss 0.305334                                        LR 0.000008    Time 0.018207    
2023-01-06 16:25:00,375 - Epoch: [169][  160/  246]    Overall Loss 0.306188    Objective Loss 0.306188                                        LR 0.000008    Time 0.018098    
2023-01-06 16:25:00,540 - Epoch: [169][  170/  246]    Overall Loss 0.305979    Objective Loss 0.305979                                        LR 0.000008    Time 0.018006    
2023-01-06 16:25:00,706 - Epoch: [169][  180/  246]    Overall Loss 0.306274    Objective Loss 0.306274                                        LR 0.000008    Time 0.017923    
2023-01-06 16:25:00,857 - Epoch: [169][  190/  246]    Overall Loss 0.305851    Objective Loss 0.305851                                        LR 0.000008    Time 0.017771    
2023-01-06 16:25:01,016 - Epoch: [169][  200/  246]    Overall Loss 0.306160    Objective Loss 0.306160                                        LR 0.000008    Time 0.017678    
2023-01-06 16:25:01,176 - Epoch: [169][  210/  246]    Overall Loss 0.304778    Objective Loss 0.304778                                        LR 0.000008    Time 0.017596    
2023-01-06 16:25:01,336 - Epoch: [169][  220/  246]    Overall Loss 0.304310    Objective Loss 0.304310                                        LR 0.000008    Time 0.017523    
2023-01-06 16:25:01,496 - Epoch: [169][  230/  246]    Overall Loss 0.304392    Objective Loss 0.304392                                        LR 0.000008    Time 0.017456    
2023-01-06 16:25:01,670 - Epoch: [169][  240/  246]    Overall Loss 0.304486    Objective Loss 0.304486                                        LR 0.000008    Time 0.017450    
2023-01-06 16:25:01,751 - Epoch: [169][  246/  246]    Overall Loss 0.304755    Objective Loss 0.304755    Top1 89.712919    LR 0.000008    Time 0.017354    
2023-01-06 16:25:01,938 - --- validate (epoch=169)-----------
2023-01-06 16:25:01,938 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:02,390 - Epoch: [169][   10/   28]    Loss 0.299482    Top1 89.140625    
2023-01-06 16:25:02,515 - Epoch: [169][   20/   28]    Loss 0.308279    Top1 88.750000    
2023-01-06 16:25:02,573 - Epoch: [169][   28/   28]    Loss 0.308152    Top1 88.620097    
2023-01-06 16:25:02,743 - ==> Top1: 88.620    Loss: 0.308

2023-01-06 16:25:02,744 - ==> Confusion:
[[ 174    9  256]
 [   9  197  396]
 [  55   70 5820]]

2023-01-06 16:25:02,745 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:25:02,745 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:02,750 - 

2023-01-06 16:25:02,750 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:03,443 - Epoch: [170][   10/  246]    Overall Loss 0.295118    Objective Loss 0.295118                                        LR 0.000008    Time 0.069206    
2023-01-06 16:25:03,624 - Epoch: [170][   20/  246]    Overall Loss 0.301758    Objective Loss 0.301758                                        LR 0.000008    Time 0.043674    
2023-01-06 16:25:03,809 - Epoch: [170][   30/  246]    Overall Loss 0.304081    Objective Loss 0.304081                                        LR 0.000008    Time 0.035218    
2023-01-06 16:25:04,000 - Epoch: [170][   40/  246]    Overall Loss 0.310092    Objective Loss 0.310092                                        LR 0.000008    Time 0.031172    
2023-01-06 16:25:04,176 - Epoch: [170][   50/  246]    Overall Loss 0.312223    Objective Loss 0.312223                                        LR 0.000008    Time 0.028442    
2023-01-06 16:25:04,353 - Epoch: [170][   60/  246]    Overall Loss 0.310035    Objective Loss 0.310035                                        LR 0.000008    Time 0.026657    
2023-01-06 16:25:04,527 - Epoch: [170][   70/  246]    Overall Loss 0.307214    Objective Loss 0.307214                                        LR 0.000008    Time 0.025317    
2023-01-06 16:25:04,702 - Epoch: [170][   80/  246]    Overall Loss 0.306722    Objective Loss 0.306722                                        LR 0.000008    Time 0.024346    
2023-01-06 16:25:04,875 - Epoch: [170][   90/  246]    Overall Loss 0.306783    Objective Loss 0.306783                                        LR 0.000008    Time 0.023552    
2023-01-06 16:25:05,051 - Epoch: [170][  100/  246]    Overall Loss 0.305671    Objective Loss 0.305671                                        LR 0.000008    Time 0.022958    
2023-01-06 16:25:05,225 - Epoch: [170][  110/  246]    Overall Loss 0.305592    Objective Loss 0.305592                                        LR 0.000008    Time 0.022450    
2023-01-06 16:25:05,402 - Epoch: [170][  120/  246]    Overall Loss 0.305562    Objective Loss 0.305562                                        LR 0.000008    Time 0.022047    
2023-01-06 16:25:05,574 - Epoch: [170][  130/  246]    Overall Loss 0.306290    Objective Loss 0.306290                                        LR 0.000008    Time 0.021674    
2023-01-06 16:25:05,751 - Epoch: [170][  140/  246]    Overall Loss 0.304851    Objective Loss 0.304851                                        LR 0.000008    Time 0.021389    
2023-01-06 16:25:05,923 - Epoch: [170][  150/  246]    Overall Loss 0.306010    Objective Loss 0.306010                                        LR 0.000008    Time 0.021110    
2023-01-06 16:25:06,101 - Epoch: [170][  160/  246]    Overall Loss 0.305997    Objective Loss 0.305997                                        LR 0.000008    Time 0.020895    
2023-01-06 16:25:06,274 - Epoch: [170][  170/  246]    Overall Loss 0.304478    Objective Loss 0.304478                                        LR 0.000008    Time 0.020684    
2023-01-06 16:25:06,451 - Epoch: [170][  180/  246]    Overall Loss 0.304908    Objective Loss 0.304908                                        LR 0.000008    Time 0.020516    
2023-01-06 16:25:06,624 - Epoch: [170][  190/  246]    Overall Loss 0.304738    Objective Loss 0.304738                                        LR 0.000008    Time 0.020345    
2023-01-06 16:25:06,801 - Epoch: [170][  200/  246]    Overall Loss 0.304560    Objective Loss 0.304560                                        LR 0.000008    Time 0.020212    
2023-01-06 16:25:06,974 - Epoch: [170][  210/  246]    Overall Loss 0.304371    Objective Loss 0.304371                                        LR 0.000008    Time 0.020071    
2023-01-06 16:25:07,150 - Epoch: [170][  220/  246]    Overall Loss 0.304900    Objective Loss 0.304900                                        LR 0.000008    Time 0.019957    
2023-01-06 16:25:07,312 - Epoch: [170][  230/  246]    Overall Loss 0.304571    Objective Loss 0.304571                                        LR 0.000008    Time 0.019793    
2023-01-06 16:25:07,494 - Epoch: [170][  240/  246]    Overall Loss 0.305026    Objective Loss 0.305026                                        LR 0.000008    Time 0.019724    
2023-01-06 16:25:07,579 - Epoch: [170][  246/  246]    Overall Loss 0.305508    Objective Loss 0.305508    Top1 89.712919    LR 0.000008    Time 0.019588    
2023-01-06 16:25:07,715 - --- validate (epoch=170)-----------
2023-01-06 16:25:07,715 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:08,152 - Epoch: [170][   10/   28]    Loss 0.317220    Top1 88.164062    
2023-01-06 16:25:08,261 - Epoch: [170][   20/   28]    Loss 0.310314    Top1 88.378906    
2023-01-06 16:25:08,321 - Epoch: [170][   28/   28]    Loss 0.305896    Top1 88.605783    
2023-01-06 16:25:08,478 - ==> Top1: 88.606    Loss: 0.306

2023-01-06 16:25:08,479 - ==> Confusion:
[[ 195    7  237]
 [  11  198  393]
 [  72   76 5797]]

2023-01-06 16:25:08,480 - ==> Best [Top1: 88.906   Sparsity:0.00   Params: 46192 on epoch: 162]
2023-01-06 16:25:08,480 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:08,484 - 

2023-01-06 16:25:08,485 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:09,040 - Epoch: [171][   10/  246]    Overall Loss 0.303388    Objective Loss 0.303388                                        LR 0.000008    Time 0.055511    
2023-01-06 16:25:09,220 - Epoch: [171][   20/  246]    Overall Loss 0.308559    Objective Loss 0.308559                                        LR 0.000008    Time 0.036697    
2023-01-06 16:25:09,391 - Epoch: [171][   30/  246]    Overall Loss 0.306472    Objective Loss 0.306472                                        LR 0.000008    Time 0.030180    
2023-01-06 16:25:09,557 - Epoch: [171][   40/  246]    Overall Loss 0.309636    Objective Loss 0.309636                                        LR 0.000008    Time 0.026769    
2023-01-06 16:25:09,723 - Epoch: [171][   50/  246]    Overall Loss 0.306442    Objective Loss 0.306442                                        LR 0.000008    Time 0.024729    
2023-01-06 16:25:09,893 - Epoch: [171][   60/  246]    Overall Loss 0.306572    Objective Loss 0.306572                                        LR 0.000008    Time 0.023438    
2023-01-06 16:25:10,062 - Epoch: [171][   70/  246]    Overall Loss 0.308959    Objective Loss 0.308959                                        LR 0.000008    Time 0.022493    
2023-01-06 16:25:10,236 - Epoch: [171][   80/  246]    Overall Loss 0.306562    Objective Loss 0.306562                                        LR 0.000008    Time 0.021854    
2023-01-06 16:25:10,409 - Epoch: [171][   90/  246]    Overall Loss 0.306048    Objective Loss 0.306048                                        LR 0.000008    Time 0.021339    
2023-01-06 16:25:10,580 - Epoch: [171][  100/  246]    Overall Loss 0.305963    Objective Loss 0.305963                                        LR 0.000008    Time 0.020911    
2023-01-06 16:25:10,755 - Epoch: [171][  110/  246]    Overall Loss 0.306455    Objective Loss 0.306455                                        LR 0.000008    Time 0.020605    
2023-01-06 16:25:10,927 - Epoch: [171][  120/  246]    Overall Loss 0.305659    Objective Loss 0.305659                                        LR 0.000008    Time 0.020311    
2023-01-06 16:25:11,107 - Epoch: [171][  130/  246]    Overall Loss 0.305987    Objective Loss 0.305987                                        LR 0.000008    Time 0.020136    
2023-01-06 16:25:11,280 - Epoch: [171][  140/  246]    Overall Loss 0.303551    Objective Loss 0.303551                                        LR 0.000008    Time 0.019931    
2023-01-06 16:25:11,457 - Epoch: [171][  150/  246]    Overall Loss 0.302825    Objective Loss 0.302825                                        LR 0.000008    Time 0.019778    
2023-01-06 16:25:11,629 - Epoch: [171][  160/  246]    Overall Loss 0.304559    Objective Loss 0.304559                                        LR 0.000008    Time 0.019604    
2023-01-06 16:25:11,806 - Epoch: [171][  170/  246]    Overall Loss 0.305000    Objective Loss 0.305000                                        LR 0.000008    Time 0.019489    
2023-01-06 16:25:11,976 - Epoch: [171][  180/  246]    Overall Loss 0.305650    Objective Loss 0.305650                                        LR 0.000008    Time 0.019338    
2023-01-06 16:25:12,143 - Epoch: [171][  190/  246]    Overall Loss 0.305517    Objective Loss 0.305517                                        LR 0.000008    Time 0.019201    
2023-01-06 16:25:12,311 - Epoch: [171][  200/  246]    Overall Loss 0.306584    Objective Loss 0.306584                                        LR 0.000008    Time 0.019075    
2023-01-06 16:25:12,477 - Epoch: [171][  210/  246]    Overall Loss 0.305098    Objective Loss 0.305098                                        LR 0.000008    Time 0.018956    
2023-01-06 16:25:12,644 - Epoch: [171][  220/  246]    Overall Loss 0.304840    Objective Loss 0.304840                                        LR 0.000008    Time 0.018852    
2023-01-06 16:25:12,810 - Epoch: [171][  230/  246]    Overall Loss 0.304473    Objective Loss 0.304473                                        LR 0.000008    Time 0.018756    
2023-01-06 16:25:12,990 - Epoch: [171][  240/  246]    Overall Loss 0.304380    Objective Loss 0.304380                                        LR 0.000008    Time 0.018722    
2023-01-06 16:25:13,072 - Epoch: [171][  246/  246]    Overall Loss 0.303946    Objective Loss 0.303946    Top1 88.277512    LR 0.000008    Time 0.018597    
2023-01-06 16:25:13,203 - --- validate (epoch=171)-----------
2023-01-06 16:25:13,204 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:13,645 - Epoch: [171][   10/   28]    Loss 0.308992    Top1 88.750000    
2023-01-06 16:25:13,749 - Epoch: [171][   20/   28]    Loss 0.310688    Top1 88.867188    
2023-01-06 16:25:13,807 - Epoch: [171][   28/   28]    Loss 0.308757    Top1 88.977956    
2023-01-06 16:25:13,960 - ==> Top1: 88.978    Loss: 0.309

2023-01-06 16:25:13,960 - ==> Confusion:
[[ 201    7  231]
 [  15  215  372]
 [  66   79 5800]]

2023-01-06 16:25:13,961 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 171]
2023-01-06 16:25:13,962 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:13,966 - 

2023-01-06 16:25:13,966 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:14,636 - Epoch: [172][   10/  246]    Overall Loss 0.318197    Objective Loss 0.318197                                        LR 0.000008    Time 0.066850    
2023-01-06 16:25:14,805 - Epoch: [172][   20/  246]    Overall Loss 0.305906    Objective Loss 0.305906                                        LR 0.000008    Time 0.041884    
2023-01-06 16:25:14,962 - Epoch: [172][   30/  246]    Overall Loss 0.301311    Objective Loss 0.301311                                        LR 0.000008    Time 0.033153    
2023-01-06 16:25:15,124 - Epoch: [172][   40/  246]    Overall Loss 0.300800    Objective Loss 0.300800                                        LR 0.000008    Time 0.028856    
2023-01-06 16:25:15,267 - Epoch: [172][   50/  246]    Overall Loss 0.302334    Objective Loss 0.302334                                        LR 0.000008    Time 0.025932    
2023-01-06 16:25:15,405 - Epoch: [172][   60/  246]    Overall Loss 0.300265    Objective Loss 0.300265                                        LR 0.000008    Time 0.023910    
2023-01-06 16:25:15,542 - Epoch: [172][   70/  246]    Overall Loss 0.301399    Objective Loss 0.301399                                        LR 0.000008    Time 0.022423    
2023-01-06 16:25:15,678 - Epoch: [172][   80/  246]    Overall Loss 0.300293    Objective Loss 0.300293                                        LR 0.000008    Time 0.021307    
2023-01-06 16:25:15,821 - Epoch: [172][   90/  246]    Overall Loss 0.301046    Objective Loss 0.301046                                        LR 0.000008    Time 0.020520    
2023-01-06 16:25:15,963 - Epoch: [172][  100/  246]    Overall Loss 0.302342    Objective Loss 0.302342                                        LR 0.000008    Time 0.019886    
2023-01-06 16:25:16,132 - Epoch: [172][  110/  246]    Overall Loss 0.303498    Objective Loss 0.303498                                        LR 0.000008    Time 0.019591    
2023-01-06 16:25:16,314 - Epoch: [172][  120/  246]    Overall Loss 0.303790    Objective Loss 0.303790                                        LR 0.000008    Time 0.019477    
2023-01-06 16:25:16,510 - Epoch: [172][  130/  246]    Overall Loss 0.305675    Objective Loss 0.305675                                        LR 0.000008    Time 0.019480    
2023-01-06 16:25:16,721 - Epoch: [172][  140/  246]    Overall Loss 0.306118    Objective Loss 0.306118                                        LR 0.000008    Time 0.019595    
2023-01-06 16:25:16,931 - Epoch: [172][  150/  246]    Overall Loss 0.305354    Objective Loss 0.305354                                        LR 0.000008    Time 0.019683    
2023-01-06 16:25:17,143 - Epoch: [172][  160/  246]    Overall Loss 0.306245    Objective Loss 0.306245                                        LR 0.000008    Time 0.019774    
2023-01-06 16:25:17,352 - Epoch: [172][  170/  246]    Overall Loss 0.305965    Objective Loss 0.305965                                        LR 0.000008    Time 0.019843    
2023-01-06 16:25:17,566 - Epoch: [172][  180/  246]    Overall Loss 0.306253    Objective Loss 0.306253                                        LR 0.000008    Time 0.019926    
2023-01-06 16:25:17,778 - Epoch: [172][  190/  246]    Overall Loss 0.305635    Objective Loss 0.305635                                        LR 0.000008    Time 0.019988    
2023-01-06 16:25:17,987 - Epoch: [172][  200/  246]    Overall Loss 0.305361    Objective Loss 0.305361                                        LR 0.000008    Time 0.020034    
2023-01-06 16:25:18,196 - Epoch: [172][  210/  246]    Overall Loss 0.305229    Objective Loss 0.305229                                        LR 0.000008    Time 0.020072    
2023-01-06 16:25:18,405 - Epoch: [172][  220/  246]    Overall Loss 0.305089    Objective Loss 0.305089                                        LR 0.000008    Time 0.020106    
2023-01-06 16:25:18,589 - Epoch: [172][  230/  246]    Overall Loss 0.305111    Objective Loss 0.305111                                        LR 0.000008    Time 0.020030    
2023-01-06 16:25:18,774 - Epoch: [172][  240/  246]    Overall Loss 0.305122    Objective Loss 0.305122                                        LR 0.000008    Time 0.019960    
2023-01-06 16:25:18,846 - Epoch: [172][  246/  246]    Overall Loss 0.305089    Objective Loss 0.305089    Top1 89.952153    LR 0.000008    Time 0.019765    
2023-01-06 16:25:18,986 - --- validate (epoch=172)-----------
2023-01-06 16:25:18,986 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:19,418 - Epoch: [172][   10/   28]    Loss 0.298118    Top1 88.789062    
2023-01-06 16:25:19,520 - Epoch: [172][   20/   28]    Loss 0.296208    Top1 88.945312    
2023-01-06 16:25:19,578 - Epoch: [172][   28/   28]    Loss 0.310828    Top1 88.476954    
2023-01-06 16:25:19,736 - ==> Top1: 88.477    Loss: 0.311

2023-01-06 16:25:19,737 - ==> Confusion:
[[ 179    9  251]
 [   9  197  396]
 [  64   76 5805]]

2023-01-06 16:25:19,738 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 171]
2023-01-06 16:25:19,738 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:19,742 - 

2023-01-06 16:25:19,742 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:20,418 - Epoch: [173][   10/  246]    Overall Loss 0.312579    Objective Loss 0.312579                                        LR 0.000008    Time 0.067500    
2023-01-06 16:25:20,568 - Epoch: [173][   20/  246]    Overall Loss 0.311380    Objective Loss 0.311380                                        LR 0.000008    Time 0.041210    
2023-01-06 16:25:20,725 - Epoch: [173][   30/  246]    Overall Loss 0.311593    Objective Loss 0.311593                                        LR 0.000008    Time 0.032715    
2023-01-06 16:25:20,894 - Epoch: [173][   40/  246]    Overall Loss 0.313270    Objective Loss 0.313270                                        LR 0.000008    Time 0.028738    
2023-01-06 16:25:21,065 - Epoch: [173][   50/  246]    Overall Loss 0.309916    Objective Loss 0.309916                                        LR 0.000008    Time 0.026406    
2023-01-06 16:25:21,236 - Epoch: [173][   60/  246]    Overall Loss 0.308201    Objective Loss 0.308201                                        LR 0.000008    Time 0.024851    
2023-01-06 16:25:21,413 - Epoch: [173][   70/  246]    Overall Loss 0.308680    Objective Loss 0.308680                                        LR 0.000008    Time 0.023825    
2023-01-06 16:25:21,589 - Epoch: [173][   80/  246]    Overall Loss 0.309105    Objective Loss 0.309105                                        LR 0.000008    Time 0.023039    
2023-01-06 16:25:21,767 - Epoch: [173][   90/  246]    Overall Loss 0.308776    Objective Loss 0.308776                                        LR 0.000008    Time 0.022457    
2023-01-06 16:25:21,936 - Epoch: [173][  100/  246]    Overall Loss 0.307925    Objective Loss 0.307925                                        LR 0.000008    Time 0.021898    
2023-01-06 16:25:22,116 - Epoch: [173][  110/  246]    Overall Loss 0.308541    Objective Loss 0.308541                                        LR 0.000008    Time 0.021535    
2023-01-06 16:25:22,288 - Epoch: [173][  120/  246]    Overall Loss 0.305870    Objective Loss 0.305870                                        LR 0.000008    Time 0.021176    
2023-01-06 16:25:22,474 - Epoch: [173][  130/  246]    Overall Loss 0.305910    Objective Loss 0.305910                                        LR 0.000008    Time 0.020970    
2023-01-06 16:25:22,653 - Epoch: [173][  140/  246]    Overall Loss 0.305038    Objective Loss 0.305038                                        LR 0.000008    Time 0.020751    
2023-01-06 16:25:22,839 - Epoch: [173][  150/  246]    Overall Loss 0.305988    Objective Loss 0.305988                                        LR 0.000008    Time 0.020606    
2023-01-06 16:25:23,018 - Epoch: [173][  160/  246]    Overall Loss 0.305874    Objective Loss 0.305874                                        LR 0.000008    Time 0.020433    
2023-01-06 16:25:23,203 - Epoch: [173][  170/  246]    Overall Loss 0.305759    Objective Loss 0.305759                                        LR 0.000008    Time 0.020319    
2023-01-06 16:25:23,377 - Epoch: [173][  180/  246]    Overall Loss 0.306004    Objective Loss 0.306004                                        LR 0.000008    Time 0.020152    
2023-01-06 16:25:23,560 - Epoch: [173][  190/  246]    Overall Loss 0.305421    Objective Loss 0.305421                                        LR 0.000008    Time 0.020051    
2023-01-06 16:25:23,739 - Epoch: [173][  200/  246]    Overall Loss 0.305627    Objective Loss 0.305627                                        LR 0.000008    Time 0.019945    
2023-01-06 16:25:23,926 - Epoch: [173][  210/  246]    Overall Loss 0.305937    Objective Loss 0.305937                                        LR 0.000008    Time 0.019883    
2023-01-06 16:25:24,106 - Epoch: [173][  220/  246]    Overall Loss 0.305444    Objective Loss 0.305444                                        LR 0.000008    Time 0.019794    
2023-01-06 16:25:24,293 - Epoch: [173][  230/  246]    Overall Loss 0.305258    Objective Loss 0.305258                                        LR 0.000008    Time 0.019745    
2023-01-06 16:25:24,480 - Epoch: [173][  240/  246]    Overall Loss 0.304688    Objective Loss 0.304688                                        LR 0.000008    Time 0.019700    
2023-01-06 16:25:24,558 - Epoch: [173][  246/  246]    Overall Loss 0.304699    Objective Loss 0.304699    Top1 88.516746    LR 0.000008    Time 0.019538    
2023-01-06 16:25:24,705 - --- validate (epoch=173)-----------
2023-01-06 16:25:24,705 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:25,141 - Epoch: [173][   10/   28]    Loss 0.295379    Top1 90.000000    
2023-01-06 16:25:25,242 - Epoch: [173][   20/   28]    Loss 0.306435    Top1 88.847656    
2023-01-06 16:25:25,299 - Epoch: [173][   28/   28]    Loss 0.304430    Top1 88.791869    
2023-01-06 16:25:25,445 - ==> Top1: 88.792    Loss: 0.304

2023-01-06 16:25:25,445 - ==> Confusion:
[[ 198    8  233]
 [  12  199  391]
 [  69   70 5806]]

2023-01-06 16:25:25,447 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 171]
2023-01-06 16:25:25,447 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:25,451 - 

2023-01-06 16:25:25,451 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:25,995 - Epoch: [174][   10/  246]    Overall Loss 0.295874    Objective Loss 0.295874                                        LR 0.000008    Time 0.054341    
2023-01-06 16:25:26,160 - Epoch: [174][   20/  246]    Overall Loss 0.298912    Objective Loss 0.298912                                        LR 0.000008    Time 0.035399    
2023-01-06 16:25:26,336 - Epoch: [174][   30/  246]    Overall Loss 0.295530    Objective Loss 0.295530                                        LR 0.000008    Time 0.029437    
2023-01-06 16:25:26,512 - Epoch: [174][   40/  246]    Overall Loss 0.298390    Objective Loss 0.298390                                        LR 0.000008    Time 0.026484    
2023-01-06 16:25:26,687 - Epoch: [174][   50/  246]    Overall Loss 0.297449    Objective Loss 0.297449                                        LR 0.000008    Time 0.024669    
2023-01-06 16:25:26,853 - Epoch: [174][   60/  246]    Overall Loss 0.298863    Objective Loss 0.298863                                        LR 0.000008    Time 0.023317    
2023-01-06 16:25:27,020 - Epoch: [174][   70/  246]    Overall Loss 0.302962    Objective Loss 0.302962                                        LR 0.000008    Time 0.022377    
2023-01-06 16:25:27,187 - Epoch: [174][   80/  246]    Overall Loss 0.302811    Objective Loss 0.302811                                        LR 0.000008    Time 0.021664    
2023-01-06 16:25:27,354 - Epoch: [174][   90/  246]    Overall Loss 0.301660    Objective Loss 0.301660                                        LR 0.000008    Time 0.021108    
2023-01-06 16:25:27,521 - Epoch: [174][  100/  246]    Overall Loss 0.303485    Objective Loss 0.303485                                        LR 0.000008    Time 0.020658    
2023-01-06 16:25:27,686 - Epoch: [174][  110/  246]    Overall Loss 0.303409    Objective Loss 0.303409                                        LR 0.000008    Time 0.020278    
2023-01-06 16:25:27,851 - Epoch: [174][  120/  246]    Overall Loss 0.304202    Objective Loss 0.304202                                        LR 0.000008    Time 0.019964    
2023-01-06 16:25:28,019 - Epoch: [174][  130/  246]    Overall Loss 0.303340    Objective Loss 0.303340                                        LR 0.000008    Time 0.019715    
2023-01-06 16:25:28,184 - Epoch: [174][  140/  246]    Overall Loss 0.304370    Objective Loss 0.304370                                        LR 0.000008    Time 0.019483    
2023-01-06 16:25:28,350 - Epoch: [174][  150/  246]    Overall Loss 0.305109    Objective Loss 0.305109                                        LR 0.000008    Time 0.019291    
2023-01-06 16:25:28,509 - Epoch: [174][  160/  246]    Overall Loss 0.305507    Objective Loss 0.305507                                        LR 0.000008    Time 0.019075    
2023-01-06 16:25:28,666 - Epoch: [174][  170/  246]    Overall Loss 0.306851    Objective Loss 0.306851                                        LR 0.000008    Time 0.018875    
2023-01-06 16:25:28,823 - Epoch: [174][  180/  246]    Overall Loss 0.306126    Objective Loss 0.306126                                        LR 0.000008    Time 0.018697    
2023-01-06 16:25:28,987 - Epoch: [174][  190/  246]    Overall Loss 0.305826    Objective Loss 0.305826                                        LR 0.000008    Time 0.018575    
2023-01-06 16:25:29,149 - Epoch: [174][  200/  246]    Overall Loss 0.305898    Objective Loss 0.305898                                        LR 0.000008    Time 0.018452    
2023-01-06 16:25:29,319 - Epoch: [174][  210/  246]    Overall Loss 0.304794    Objective Loss 0.304794                                        LR 0.000008    Time 0.018383    
2023-01-06 16:25:29,489 - Epoch: [174][  220/  246]    Overall Loss 0.304746    Objective Loss 0.304746                                        LR 0.000008    Time 0.018319    
2023-01-06 16:25:29,658 - Epoch: [174][  230/  246]    Overall Loss 0.304869    Objective Loss 0.304869                                        LR 0.000008    Time 0.018253    
2023-01-06 16:25:29,838 - Epoch: [174][  240/  246]    Overall Loss 0.304575    Objective Loss 0.304575                                        LR 0.000008    Time 0.018243    
2023-01-06 16:25:29,920 - Epoch: [174][  246/  246]    Overall Loss 0.304395    Objective Loss 0.304395    Top1 91.626794    LR 0.000008    Time 0.018129    
2023-01-06 16:25:30,048 - --- validate (epoch=174)-----------
2023-01-06 16:25:30,048 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:30,480 - Epoch: [174][   10/   28]    Loss 0.297566    Top1 88.789062    
2023-01-06 16:25:30,583 - Epoch: [174][   20/   28]    Loss 0.302241    Top1 88.789062    
2023-01-06 16:25:30,642 - Epoch: [174][   28/   28]    Loss 0.307384    Top1 88.748926    
2023-01-06 16:25:30,776 - ==> Top1: 88.749    Loss: 0.307

2023-01-06 16:25:30,776 - ==> Confusion:
[[ 186    8  245]
 [  10  199  393]
 [  60   70 5815]]

2023-01-06 16:25:30,778 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 171]
2023-01-06 16:25:30,778 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:30,782 - 

2023-01-06 16:25:30,782 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:31,476 - Epoch: [175][   10/  246]    Overall Loss 0.291039    Objective Loss 0.291039                                        LR 0.000008    Time 0.069287    
2023-01-06 16:25:31,644 - Epoch: [175][   20/  246]    Overall Loss 0.288842    Objective Loss 0.288842                                        LR 0.000008    Time 0.043020    
2023-01-06 16:25:31,811 - Epoch: [175][   30/  246]    Overall Loss 0.298488    Objective Loss 0.298488                                        LR 0.000008    Time 0.034240    
2023-01-06 16:25:31,978 - Epoch: [175][   40/  246]    Overall Loss 0.300438    Objective Loss 0.300438                                        LR 0.000008    Time 0.029851    
2023-01-06 16:25:32,146 - Epoch: [175][   50/  246]    Overall Loss 0.303282    Objective Loss 0.303282                                        LR 0.000008    Time 0.027228    
2023-01-06 16:25:32,315 - Epoch: [175][   60/  246]    Overall Loss 0.304380    Objective Loss 0.304380                                        LR 0.000008    Time 0.025505    
2023-01-06 16:25:32,484 - Epoch: [175][   70/  246]    Overall Loss 0.303124    Objective Loss 0.303124                                        LR 0.000008    Time 0.024276    
2023-01-06 16:25:32,656 - Epoch: [175][   80/  246]    Overall Loss 0.302234    Objective Loss 0.302234                                        LR 0.000008    Time 0.023384    
2023-01-06 16:25:32,827 - Epoch: [175][   90/  246]    Overall Loss 0.302647    Objective Loss 0.302647                                        LR 0.000008    Time 0.022684    
2023-01-06 16:25:33,000 - Epoch: [175][  100/  246]    Overall Loss 0.303043    Objective Loss 0.303043                                        LR 0.000008    Time 0.022137    
2023-01-06 16:25:33,175 - Epoch: [175][  110/  246]    Overall Loss 0.302716    Objective Loss 0.302716                                        LR 0.000008    Time 0.021712    
2023-01-06 16:25:33,344 - Epoch: [175][  120/  246]    Overall Loss 0.304049    Objective Loss 0.304049                                        LR 0.000008    Time 0.021307    
2023-01-06 16:25:33,506 - Epoch: [175][  130/  246]    Overall Loss 0.302954    Objective Loss 0.302954                                        LR 0.000008    Time 0.020912    
2023-01-06 16:25:33,670 - Epoch: [175][  140/  246]    Overall Loss 0.303537    Objective Loss 0.303537                                        LR 0.000008    Time 0.020585    
2023-01-06 16:25:33,836 - Epoch: [175][  150/  246]    Overall Loss 0.304968    Objective Loss 0.304968                                        LR 0.000008    Time 0.020320    
2023-01-06 16:25:34,002 - Epoch: [175][  160/  246]    Overall Loss 0.304564    Objective Loss 0.304564                                        LR 0.000008    Time 0.020086    
2023-01-06 16:25:34,168 - Epoch: [175][  170/  246]    Overall Loss 0.302874    Objective Loss 0.302874                                        LR 0.000008    Time 0.019875    
2023-01-06 16:25:34,333 - Epoch: [175][  180/  246]    Overall Loss 0.302916    Objective Loss 0.302916                                        LR 0.000008    Time 0.019690    
2023-01-06 16:25:34,501 - Epoch: [175][  190/  246]    Overall Loss 0.302368    Objective Loss 0.302368                                        LR 0.000008    Time 0.019535    
2023-01-06 16:25:34,671 - Epoch: [175][  200/  246]    Overall Loss 0.302931    Objective Loss 0.302931                                        LR 0.000008    Time 0.019406    
2023-01-06 16:25:34,839 - Epoch: [175][  210/  246]    Overall Loss 0.303767    Objective Loss 0.303767                                        LR 0.000008    Time 0.019278    
2023-01-06 16:25:35,001 - Epoch: [175][  220/  246]    Overall Loss 0.304384    Objective Loss 0.304384                                        LR 0.000008    Time 0.019137    
2023-01-06 16:25:35,168 - Epoch: [175][  230/  246]    Overall Loss 0.304239    Objective Loss 0.304239                                        LR 0.000008    Time 0.019031    
2023-01-06 16:25:35,358 - Epoch: [175][  240/  246]    Overall Loss 0.304208    Objective Loss 0.304208                                        LR 0.000008    Time 0.019027    
2023-01-06 16:25:35,441 - Epoch: [175][  246/  246]    Overall Loss 0.304088    Objective Loss 0.304088    Top1 87.799043    LR 0.000008    Time 0.018901    
2023-01-06 16:25:35,609 - --- validate (epoch=175)-----------
2023-01-06 16:25:35,610 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:36,038 - Epoch: [175][   10/   28]    Loss 0.319303    Top1 87.851562    
2023-01-06 16:25:36,143 - Epoch: [175][   20/   28]    Loss 0.302901    Top1 88.906250    
2023-01-06 16:25:36,201 - Epoch: [175][   28/   28]    Loss 0.306210    Top1 88.877756    
2023-01-06 16:25:36,331 - ==> Top1: 88.878    Loss: 0.306

2023-01-06 16:25:36,331 - ==> Confusion:
[[ 180    9  250]
 [  10  204  388]
 [  46   74 5825]]

2023-01-06 16:25:36,332 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 171]
2023-01-06 16:25:36,333 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:36,337 - 

2023-01-06 16:25:36,337 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:36,876 - Epoch: [176][   10/  246]    Overall Loss 0.276709    Objective Loss 0.276709                                        LR 0.000008    Time 0.053843    
2023-01-06 16:25:37,036 - Epoch: [176][   20/  246]    Overall Loss 0.297681    Objective Loss 0.297681                                        LR 0.000008    Time 0.034882    
2023-01-06 16:25:37,190 - Epoch: [176][   30/  246]    Overall Loss 0.301785    Objective Loss 0.301785                                        LR 0.000008    Time 0.028383    
2023-01-06 16:25:37,336 - Epoch: [176][   40/  246]    Overall Loss 0.298601    Objective Loss 0.298601                                        LR 0.000008    Time 0.024927    
2023-01-06 16:25:37,476 - Epoch: [176][   50/  246]    Overall Loss 0.299933    Objective Loss 0.299933                                        LR 0.000008    Time 0.022734    
2023-01-06 16:25:37,621 - Epoch: [176][   60/  246]    Overall Loss 0.300375    Objective Loss 0.300375                                        LR 0.000008    Time 0.021357    
2023-01-06 16:25:37,794 - Epoch: [176][   70/  246]    Overall Loss 0.299763    Objective Loss 0.299763                                        LR 0.000008    Time 0.020771    
2023-01-06 16:25:37,969 - Epoch: [176][   80/  246]    Overall Loss 0.302580    Objective Loss 0.302580                                        LR 0.000008    Time 0.020358    
2023-01-06 16:25:38,146 - Epoch: [176][   90/  246]    Overall Loss 0.304689    Objective Loss 0.304689                                        LR 0.000008    Time 0.020060    
2023-01-06 16:25:38,324 - Epoch: [176][  100/  246]    Overall Loss 0.304774    Objective Loss 0.304774                                        LR 0.000008    Time 0.019830    
2023-01-06 16:25:38,511 - Epoch: [176][  110/  246]    Overall Loss 0.305071    Objective Loss 0.305071                                        LR 0.000008    Time 0.019719    
2023-01-06 16:25:38,697 - Epoch: [176][  120/  246]    Overall Loss 0.306470    Objective Loss 0.306470                                        LR 0.000008    Time 0.019619    
2023-01-06 16:25:38,870 - Epoch: [176][  130/  246]    Overall Loss 0.305222    Objective Loss 0.305222                                        LR 0.000008    Time 0.019444    
2023-01-06 16:25:39,020 - Epoch: [176][  140/  246]    Overall Loss 0.305896    Objective Loss 0.305896                                        LR 0.000008    Time 0.019122    
2023-01-06 16:25:39,184 - Epoch: [176][  150/  246]    Overall Loss 0.304078    Objective Loss 0.304078                                        LR 0.000008    Time 0.018935    
2023-01-06 16:25:39,334 - Epoch: [176][  160/  246]    Overall Loss 0.302987    Objective Loss 0.302987                                        LR 0.000008    Time 0.018687    
2023-01-06 16:25:39,486 - Epoch: [176][  170/  246]    Overall Loss 0.302313    Objective Loss 0.302313                                        LR 0.000008    Time 0.018481    
2023-01-06 16:25:39,639 - Epoch: [176][  180/  246]    Overall Loss 0.302099    Objective Loss 0.302099                                        LR 0.000008    Time 0.018298    
2023-01-06 16:25:39,810 - Epoch: [176][  190/  246]    Overall Loss 0.302704    Objective Loss 0.302704                                        LR 0.000008    Time 0.018232    
2023-01-06 16:25:39,981 - Epoch: [176][  200/  246]    Overall Loss 0.302715    Objective Loss 0.302715                                        LR 0.000008    Time 0.018173    
2023-01-06 16:25:40,150 - Epoch: [176][  210/  246]    Overall Loss 0.303489    Objective Loss 0.303489                                        LR 0.000008    Time 0.018106    
2023-01-06 16:25:40,322 - Epoch: [176][  220/  246]    Overall Loss 0.304736    Objective Loss 0.304736                                        LR 0.000008    Time 0.018061    
2023-01-06 16:25:40,494 - Epoch: [176][  230/  246]    Overall Loss 0.304543    Objective Loss 0.304543                                        LR 0.000008    Time 0.018021    
2023-01-06 16:25:40,681 - Epoch: [176][  240/  246]    Overall Loss 0.304352    Objective Loss 0.304352                                        LR 0.000008    Time 0.018047    
2023-01-06 16:25:40,760 - Epoch: [176][  246/  246]    Overall Loss 0.304302    Objective Loss 0.304302    Top1 89.712919    LR 0.000008    Time 0.017925    
2023-01-06 16:25:40,886 - --- validate (epoch=176)-----------
2023-01-06 16:25:40,887 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:41,345 - Epoch: [176][   10/   28]    Loss 0.309272    Top1 89.257812    
2023-01-06 16:25:41,449 - Epoch: [176][   20/   28]    Loss 0.304916    Top1 89.140625    
2023-01-06 16:25:41,506 - Epoch: [176][   28/   28]    Loss 0.309595    Top1 88.977956    
2023-01-06 16:25:41,667 - ==> Top1: 88.978    Loss: 0.310

2023-01-06 16:25:41,667 - ==> Confusion:
[[ 185    8  246]
 [  11  215  376]
 [  52   77 5816]]

2023-01-06 16:25:41,668 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:25:41,669 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:41,674 - 

2023-01-06 16:25:41,674 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:42,347 - Epoch: [177][   10/  246]    Overall Loss 0.295159    Objective Loss 0.295159                                        LR 0.000008    Time 0.067236    
2023-01-06 16:25:42,490 - Epoch: [177][   20/  246]    Overall Loss 0.298288    Objective Loss 0.298288                                        LR 0.000008    Time 0.040764    
2023-01-06 16:25:42,632 - Epoch: [177][   30/  246]    Overall Loss 0.302874    Objective Loss 0.302874                                        LR 0.000008    Time 0.031901    
2023-01-06 16:25:42,791 - Epoch: [177][   40/  246]    Overall Loss 0.301587    Objective Loss 0.301587                                        LR 0.000008    Time 0.027849    
2023-01-06 16:25:42,945 - Epoch: [177][   50/  246]    Overall Loss 0.308429    Objective Loss 0.308429                                        LR 0.000008    Time 0.025336    
2023-01-06 16:25:43,107 - Epoch: [177][   60/  246]    Overall Loss 0.308853    Objective Loss 0.308853                                        LR 0.000008    Time 0.023811    
2023-01-06 16:25:43,250 - Epoch: [177][   70/  246]    Overall Loss 0.309417    Objective Loss 0.309417                                        LR 0.000008    Time 0.022449    
2023-01-06 16:25:43,398 - Epoch: [177][   80/  246]    Overall Loss 0.306712    Objective Loss 0.306712                                        LR 0.000008    Time 0.021486    
2023-01-06 16:25:43,558 - Epoch: [177][   90/  246]    Overall Loss 0.307072    Objective Loss 0.307072                                        LR 0.000008    Time 0.020876    
2023-01-06 16:25:43,729 - Epoch: [177][  100/  246]    Overall Loss 0.306211    Objective Loss 0.306211                                        LR 0.000008    Time 0.020493    
2023-01-06 16:25:43,896 - Epoch: [177][  110/  246]    Overall Loss 0.306812    Objective Loss 0.306812                                        LR 0.000008    Time 0.020149    
2023-01-06 16:25:44,058 - Epoch: [177][  120/  246]    Overall Loss 0.307773    Objective Loss 0.307773                                        LR 0.000008    Time 0.019811    
2023-01-06 16:25:44,233 - Epoch: [177][  130/  246]    Overall Loss 0.306727    Objective Loss 0.306727                                        LR 0.000008    Time 0.019631    
2023-01-06 16:25:44,395 - Epoch: [177][  140/  246]    Overall Loss 0.306221    Objective Loss 0.306221                                        LR 0.000008    Time 0.019385    
2023-01-06 16:25:44,570 - Epoch: [177][  150/  246]    Overall Loss 0.305330    Objective Loss 0.305330                                        LR 0.000008    Time 0.019256    
2023-01-06 16:25:44,763 - Epoch: [177][  160/  246]    Overall Loss 0.303677    Objective Loss 0.303677                                        LR 0.000008    Time 0.019252    
2023-01-06 16:25:44,933 - Epoch: [177][  170/  246]    Overall Loss 0.303354    Objective Loss 0.303354                                        LR 0.000008    Time 0.019118    
2023-01-06 16:25:45,099 - Epoch: [177][  180/  246]    Overall Loss 0.302394    Objective Loss 0.302394                                        LR 0.000008    Time 0.018975    
2023-01-06 16:25:45,263 - Epoch: [177][  190/  246]    Overall Loss 0.302617    Objective Loss 0.302617                                        LR 0.000008    Time 0.018839    
2023-01-06 16:25:45,428 - Epoch: [177][  200/  246]    Overall Loss 0.302242    Objective Loss 0.302242                                        LR 0.000008    Time 0.018721    
2023-01-06 16:25:45,581 - Epoch: [177][  210/  246]    Overall Loss 0.302875    Objective Loss 0.302875                                        LR 0.000008    Time 0.018554    
2023-01-06 16:25:45,732 - Epoch: [177][  220/  246]    Overall Loss 0.302712    Objective Loss 0.302712                                        LR 0.000008    Time 0.018398    
2023-01-06 16:25:45,885 - Epoch: [177][  230/  246]    Overall Loss 0.302838    Objective Loss 0.302838                                        LR 0.000008    Time 0.018261    
2023-01-06 16:25:46,051 - Epoch: [177][  240/  246]    Overall Loss 0.303879    Objective Loss 0.303879                                        LR 0.000008    Time 0.018189    
2023-01-06 16:25:46,129 - Epoch: [177][  246/  246]    Overall Loss 0.303868    Objective Loss 0.303868    Top1 90.909091    LR 0.000008    Time 0.018060    
2023-01-06 16:25:46,261 - --- validate (epoch=177)-----------
2023-01-06 16:25:46,262 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:46,708 - Epoch: [177][   10/   28]    Loss 0.310587    Top1 88.554688    
2023-01-06 16:25:46,809 - Epoch: [177][   20/   28]    Loss 0.308675    Top1 88.710938    
2023-01-06 16:25:46,865 - Epoch: [177][   28/   28]    Loss 0.309184    Top1 88.648726    
2023-01-06 16:25:47,017 - ==> Top1: 88.649    Loss: 0.309

2023-01-06 16:25:47,017 - ==> Confusion:
[[ 193    9  237]
 [  11  193  398]
 [  72   66 5807]]

2023-01-06 16:25:47,018 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:25:47,018 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:47,023 - 

2023-01-06 16:25:47,023 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:47,712 - Epoch: [178][   10/  246]    Overall Loss 0.294424    Objective Loss 0.294424                                        LR 0.000008    Time 0.068846    
2023-01-06 16:25:47,889 - Epoch: [178][   20/  246]    Overall Loss 0.310642    Objective Loss 0.310642                                        LR 0.000008    Time 0.043261    
2023-01-06 16:25:48,071 - Epoch: [178][   30/  246]    Overall Loss 0.307086    Objective Loss 0.307086                                        LR 0.000008    Time 0.034890    
2023-01-06 16:25:48,243 - Epoch: [178][   40/  246]    Overall Loss 0.312087    Objective Loss 0.312087                                        LR 0.000008    Time 0.030445    
2023-01-06 16:25:48,418 - Epoch: [178][   50/  246]    Overall Loss 0.308951    Objective Loss 0.308951                                        LR 0.000008    Time 0.027850    
2023-01-06 16:25:48,593 - Epoch: [178][   60/  246]    Overall Loss 0.306190    Objective Loss 0.306190                                        LR 0.000008    Time 0.026114    
2023-01-06 16:25:48,768 - Epoch: [178][   70/  246]    Overall Loss 0.307185    Objective Loss 0.307185                                        LR 0.000008    Time 0.024877    
2023-01-06 16:25:48,950 - Epoch: [178][   80/  246]    Overall Loss 0.307722    Objective Loss 0.307722                                        LR 0.000008    Time 0.024049    
2023-01-06 16:25:49,130 - Epoch: [178][   90/  246]    Overall Loss 0.305606    Objective Loss 0.305606                                        LR 0.000008    Time 0.023348    
2023-01-06 16:25:49,296 - Epoch: [178][  100/  246]    Overall Loss 0.303686    Objective Loss 0.303686                                        LR 0.000008    Time 0.022663    
2023-01-06 16:25:49,458 - Epoch: [178][  110/  246]    Overall Loss 0.303415    Objective Loss 0.303415                                        LR 0.000008    Time 0.022072    
2023-01-06 16:25:49,623 - Epoch: [178][  120/  246]    Overall Loss 0.302636    Objective Loss 0.302636                                        LR 0.000008    Time 0.021611    
2023-01-06 16:25:49,796 - Epoch: [178][  130/  246]    Overall Loss 0.302204    Objective Loss 0.302204                                        LR 0.000008    Time 0.021270    
2023-01-06 16:25:49,966 - Epoch: [178][  140/  246]    Overall Loss 0.303041    Objective Loss 0.303041                                        LR 0.000008    Time 0.020965    
2023-01-06 16:25:50,125 - Epoch: [178][  150/  246]    Overall Loss 0.302198    Objective Loss 0.302198                                        LR 0.000008    Time 0.020626    
2023-01-06 16:25:50,293 - Epoch: [178][  160/  246]    Overall Loss 0.303919    Objective Loss 0.303919                                        LR 0.000008    Time 0.020385    
2023-01-06 16:25:50,455 - Epoch: [178][  170/  246]    Overall Loss 0.303112    Objective Loss 0.303112                                        LR 0.000008    Time 0.020137    
2023-01-06 16:25:50,619 - Epoch: [178][  180/  246]    Overall Loss 0.303696    Objective Loss 0.303696                                        LR 0.000008    Time 0.019927    
2023-01-06 16:25:50,785 - Epoch: [178][  190/  246]    Overall Loss 0.303060    Objective Loss 0.303060                                        LR 0.000008    Time 0.019750    
2023-01-06 16:25:50,946 - Epoch: [178][  200/  246]    Overall Loss 0.303567    Objective Loss 0.303567                                        LR 0.000008    Time 0.019566    
2023-01-06 16:25:51,115 - Epoch: [178][  210/  246]    Overall Loss 0.303563    Objective Loss 0.303563                                        LR 0.000008    Time 0.019437    
2023-01-06 16:25:51,293 - Epoch: [178][  220/  246]    Overall Loss 0.303479    Objective Loss 0.303479                                        LR 0.000008    Time 0.019361    
2023-01-06 16:25:51,475 - Epoch: [178][  230/  246]    Overall Loss 0.302902    Objective Loss 0.302902                                        LR 0.000008    Time 0.019306    
2023-01-06 16:25:51,659 - Epoch: [178][  240/  246]    Overall Loss 0.303453    Objective Loss 0.303453                                        LR 0.000008    Time 0.019267    
2023-01-06 16:25:51,734 - Epoch: [178][  246/  246]    Overall Loss 0.304125    Objective Loss 0.304125    Top1 86.124402    LR 0.000008    Time 0.019104    
2023-01-06 16:25:51,907 - --- validate (epoch=178)-----------
2023-01-06 16:25:51,907 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:52,343 - Epoch: [178][   10/   28]    Loss 0.308773    Top1 88.554688    
2023-01-06 16:25:52,447 - Epoch: [178][   20/   28]    Loss 0.308671    Top1 88.808594    
2023-01-06 16:25:52,505 - Epoch: [178][   28/   28]    Loss 0.309291    Top1 88.834812    
2023-01-06 16:25:52,642 - ==> Top1: 88.835    Loss: 0.309

2023-01-06 16:25:52,643 - ==> Confusion:
[[ 204    9  226]
 [  15  223  364]
 [  79   87 5779]]

2023-01-06 16:25:52,644 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:25:52,644 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:52,648 - 

2023-01-06 16:25:52,648 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:53,203 - Epoch: [179][   10/  246]    Overall Loss 0.310093    Objective Loss 0.310093                                        LR 0.000008    Time 0.055409    
2023-01-06 16:25:53,364 - Epoch: [179][   20/  246]    Overall Loss 0.312174    Objective Loss 0.312174                                        LR 0.000008    Time 0.035716    
2023-01-06 16:25:53,528 - Epoch: [179][   30/  246]    Overall Loss 0.308050    Objective Loss 0.308050                                        LR 0.000008    Time 0.029281    
2023-01-06 16:25:53,692 - Epoch: [179][   40/  246]    Overall Loss 0.304045    Objective Loss 0.304045                                        LR 0.000008    Time 0.026040    
2023-01-06 16:25:53,850 - Epoch: [179][   50/  246]    Overall Loss 0.300515    Objective Loss 0.300515                                        LR 0.000008    Time 0.023992    
2023-01-06 16:25:54,009 - Epoch: [179][   60/  246]    Overall Loss 0.302924    Objective Loss 0.302924                                        LR 0.000008    Time 0.022637    
2023-01-06 16:25:54,178 - Epoch: [179][   70/  246]    Overall Loss 0.303600    Objective Loss 0.303600                                        LR 0.000008    Time 0.021804    
2023-01-06 16:25:54,355 - Epoch: [179][   80/  246]    Overall Loss 0.301477    Objective Loss 0.301477                                        LR 0.000008    Time 0.021287    
2023-01-06 16:25:54,530 - Epoch: [179][   90/  246]    Overall Loss 0.302441    Objective Loss 0.302441                                        LR 0.000008    Time 0.020859    
2023-01-06 16:25:54,704 - Epoch: [179][  100/  246]    Overall Loss 0.306018    Objective Loss 0.306018                                        LR 0.000008    Time 0.020508    
2023-01-06 16:25:54,911 - Epoch: [179][  110/  246]    Overall Loss 0.306929    Objective Loss 0.306929                                        LR 0.000008    Time 0.020528    
2023-01-06 16:25:55,109 - Epoch: [179][  120/  246]    Overall Loss 0.307787    Objective Loss 0.307787                                        LR 0.000008    Time 0.020464    
2023-01-06 16:25:55,292 - Epoch: [179][  130/  246]    Overall Loss 0.307630    Objective Loss 0.307630                                        LR 0.000008    Time 0.020276    
2023-01-06 16:25:55,473 - Epoch: [179][  140/  246]    Overall Loss 0.307095    Objective Loss 0.307095                                        LR 0.000008    Time 0.020117    
2023-01-06 16:25:55,655 - Epoch: [179][  150/  246]    Overall Loss 0.305477    Objective Loss 0.305477                                        LR 0.000008    Time 0.019985    
2023-01-06 16:25:55,836 - Epoch: [179][  160/  246]    Overall Loss 0.304285    Objective Loss 0.304285                                        LR 0.000008    Time 0.019858    
2023-01-06 16:25:56,024 - Epoch: [179][  170/  246]    Overall Loss 0.304306    Objective Loss 0.304306                                        LR 0.000008    Time 0.019793    
2023-01-06 16:25:56,207 - Epoch: [179][  180/  246]    Overall Loss 0.304127    Objective Loss 0.304127                                        LR 0.000008    Time 0.019704    
2023-01-06 16:25:56,392 - Epoch: [179][  190/  246]    Overall Loss 0.304019    Objective Loss 0.304019                                        LR 0.000008    Time 0.019637    
2023-01-06 16:25:56,567 - Epoch: [179][  200/  246]    Overall Loss 0.303867    Objective Loss 0.303867                                        LR 0.000008    Time 0.019522    
2023-01-06 16:25:56,739 - Epoch: [179][  210/  246]    Overall Loss 0.303371    Objective Loss 0.303371                                        LR 0.000008    Time 0.019411    
2023-01-06 16:25:56,901 - Epoch: [179][  220/  246]    Overall Loss 0.303402    Objective Loss 0.303402                                        LR 0.000008    Time 0.019262    
2023-01-06 16:25:57,074 - Epoch: [179][  230/  246]    Overall Loss 0.303798    Objective Loss 0.303798                                        LR 0.000008    Time 0.019177    
2023-01-06 16:25:57,257 - Epoch: [179][  240/  246]    Overall Loss 0.303208    Objective Loss 0.303208                                        LR 0.000008    Time 0.019139    
2023-01-06 16:25:57,339 - Epoch: [179][  246/  246]    Overall Loss 0.303378    Objective Loss 0.303378    Top1 89.952153    LR 0.000008    Time 0.019002    
2023-01-06 16:25:57,476 - --- validate (epoch=179)-----------
2023-01-06 16:25:57,477 - 6986 samples (256 per mini-batch)
2023-01-06 16:25:57,905 - Epoch: [179][   10/   28]    Loss 0.327394    Top1 87.734375    
2023-01-06 16:25:58,010 - Epoch: [179][   20/   28]    Loss 0.307394    Top1 88.417969    
2023-01-06 16:25:58,070 - Epoch: [179][   28/   28]    Loss 0.309050    Top1 88.491268    
2023-01-06 16:25:58,203 - ==> Top1: 88.491    Loss: 0.309

2023-01-06 16:25:58,203 - ==> Confusion:
[[ 160    8  271]
 [   8  182  412]
 [  41   64 5840]]

2023-01-06 16:25:58,205 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:25:58,205 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:25:58,209 - 

2023-01-06 16:25:58,209 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:25:58,914 - Epoch: [180][   10/  246]    Overall Loss 0.306459    Objective Loss 0.306459                                        LR 0.000005    Time 0.070412    
2023-01-06 16:25:59,099 - Epoch: [180][   20/  246]    Overall Loss 0.305101    Objective Loss 0.305101                                        LR 0.000005    Time 0.044461    
2023-01-06 16:25:59,264 - Epoch: [180][   30/  246]    Overall Loss 0.311525    Objective Loss 0.311525                                        LR 0.000005    Time 0.035127    
2023-01-06 16:25:59,422 - Epoch: [180][   40/  246]    Overall Loss 0.305277    Objective Loss 0.305277                                        LR 0.000005    Time 0.030285    
2023-01-06 16:25:59,576 - Epoch: [180][   50/  246]    Overall Loss 0.300939    Objective Loss 0.300939                                        LR 0.000005    Time 0.027290    
2023-01-06 16:25:59,746 - Epoch: [180][   60/  246]    Overall Loss 0.297960    Objective Loss 0.297960                                        LR 0.000005    Time 0.025563    
2023-01-06 16:25:59,905 - Epoch: [180][   70/  246]    Overall Loss 0.300795    Objective Loss 0.300795                                        LR 0.000005    Time 0.024189    
2023-01-06 16:26:00,081 - Epoch: [180][   80/  246]    Overall Loss 0.300027    Objective Loss 0.300027                                        LR 0.000005    Time 0.023349    
2023-01-06 16:26:00,253 - Epoch: [180][   90/  246]    Overall Loss 0.298659    Objective Loss 0.298659                                        LR 0.000005    Time 0.022671    
2023-01-06 16:26:00,415 - Epoch: [180][  100/  246]    Overall Loss 0.302098    Objective Loss 0.302098                                        LR 0.000005    Time 0.022015    
2023-01-06 16:26:00,578 - Epoch: [180][  110/  246]    Overall Loss 0.304920    Objective Loss 0.304920                                        LR 0.000005    Time 0.021487    
2023-01-06 16:26:00,738 - Epoch: [180][  120/  246]    Overall Loss 0.303651    Objective Loss 0.303651                                        LR 0.000005    Time 0.021027    
2023-01-06 16:26:00,886 - Epoch: [180][  130/  246]    Overall Loss 0.302971    Objective Loss 0.302971                                        LR 0.000005    Time 0.020547    
2023-01-06 16:26:01,045 - Epoch: [180][  140/  246]    Overall Loss 0.304204    Objective Loss 0.304204                                        LR 0.000005    Time 0.020211    
2023-01-06 16:26:01,188 - Epoch: [180][  150/  246]    Overall Loss 0.305548    Objective Loss 0.305548                                        LR 0.000005    Time 0.019813    
2023-01-06 16:26:01,332 - Epoch: [180][  160/  246]    Overall Loss 0.305283    Objective Loss 0.305283                                        LR 0.000005    Time 0.019473    
2023-01-06 16:26:01,490 - Epoch: [180][  170/  246]    Overall Loss 0.305303    Objective Loss 0.305303                                        LR 0.000005    Time 0.019254    
2023-01-06 16:26:01,657 - Epoch: [180][  180/  246]    Overall Loss 0.305589    Objective Loss 0.305589                                        LR 0.000005    Time 0.019109    
2023-01-06 16:26:01,804 - Epoch: [180][  190/  246]    Overall Loss 0.305486    Objective Loss 0.305486                                        LR 0.000005    Time 0.018878    
2023-01-06 16:26:01,945 - Epoch: [180][  200/  246]    Overall Loss 0.304923    Objective Loss 0.304923                                        LR 0.000005    Time 0.018636    
2023-01-06 16:26:02,086 - Epoch: [180][  210/  246]    Overall Loss 0.304230    Objective Loss 0.304230                                        LR 0.000005    Time 0.018415    
2023-01-06 16:26:02,229 - Epoch: [180][  220/  246]    Overall Loss 0.304908    Objective Loss 0.304908                                        LR 0.000005    Time 0.018230    
2023-01-06 16:26:02,373 - Epoch: [180][  230/  246]    Overall Loss 0.304027    Objective Loss 0.304027                                        LR 0.000005    Time 0.018058    
2023-01-06 16:26:02,528 - Epoch: [180][  240/  246]    Overall Loss 0.303482    Objective Loss 0.303482                                        LR 0.000005    Time 0.017952    
2023-01-06 16:26:02,596 - Epoch: [180][  246/  246]    Overall Loss 0.303396    Objective Loss 0.303396    Top1 89.473684    LR 0.000005    Time 0.017788    
2023-01-06 16:26:02,737 - --- validate (epoch=180)-----------
2023-01-06 16:26:02,737 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:03,174 - Epoch: [180][   10/   28]    Loss 0.323858    Top1 88.085938    
2023-01-06 16:26:03,282 - Epoch: [180][   20/   28]    Loss 0.314153    Top1 88.535156    
2023-01-06 16:26:03,343 - Epoch: [180][   28/   28]    Loss 0.305999    Top1 88.920699    
2023-01-06 16:26:03,482 - ==> Top1: 88.921    Loss: 0.306

2023-01-06 16:26:03,483 - ==> Confusion:
[[ 183    8  248]
 [  10  207  385]
 [  53   70 5822]]

2023-01-06 16:26:03,484 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:03,484 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:03,489 - 

2023-01-06 16:26:03,489 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:04,021 - Epoch: [181][   10/  246]    Overall Loss 0.305373    Objective Loss 0.305373                                        LR 0.000005    Time 0.053164    
2023-01-06 16:26:04,184 - Epoch: [181][   20/  246]    Overall Loss 0.300928    Objective Loss 0.300928                                        LR 0.000005    Time 0.034725    
2023-01-06 16:26:04,354 - Epoch: [181][   30/  246]    Overall Loss 0.302912    Objective Loss 0.302912                                        LR 0.000005    Time 0.028796    
2023-01-06 16:26:04,537 - Epoch: [181][   40/  246]    Overall Loss 0.299699    Objective Loss 0.299699                                        LR 0.000005    Time 0.026146    
2023-01-06 16:26:04,700 - Epoch: [181][   50/  246]    Overall Loss 0.298219    Objective Loss 0.298219                                        LR 0.000005    Time 0.024184    
2023-01-06 16:26:04,855 - Epoch: [181][   60/  246]    Overall Loss 0.299421    Objective Loss 0.299421                                        LR 0.000005    Time 0.022725    
2023-01-06 16:26:05,012 - Epoch: [181][   70/  246]    Overall Loss 0.296716    Objective Loss 0.296716                                        LR 0.000005    Time 0.021717    
2023-01-06 16:26:05,170 - Epoch: [181][   80/  246]    Overall Loss 0.296555    Objective Loss 0.296555                                        LR 0.000005    Time 0.020971    
2023-01-06 16:26:05,343 - Epoch: [181][   90/  246]    Overall Loss 0.296014    Objective Loss 0.296014                                        LR 0.000005    Time 0.020557    
2023-01-06 16:26:05,523 - Epoch: [181][  100/  246]    Overall Loss 0.297509    Objective Loss 0.297509                                        LR 0.000005    Time 0.020280    
2023-01-06 16:26:05,706 - Epoch: [181][  110/  246]    Overall Loss 0.297754    Objective Loss 0.297754                                        LR 0.000005    Time 0.020079    
2023-01-06 16:26:05,884 - Epoch: [181][  120/  246]    Overall Loss 0.299100    Objective Loss 0.299100                                        LR 0.000005    Time 0.019887    
2023-01-06 16:26:06,060 - Epoch: [181][  130/  246]    Overall Loss 0.300413    Objective Loss 0.300413                                        LR 0.000005    Time 0.019693    
2023-01-06 16:26:06,239 - Epoch: [181][  140/  246]    Overall Loss 0.301518    Objective Loss 0.301518                                        LR 0.000005    Time 0.019566    
2023-01-06 16:26:06,405 - Epoch: [181][  150/  246]    Overall Loss 0.302621    Objective Loss 0.302621                                        LR 0.000005    Time 0.019364    
2023-01-06 16:26:06,570 - Epoch: [181][  160/  246]    Overall Loss 0.302561    Objective Loss 0.302561                                        LR 0.000005    Time 0.019180    
2023-01-06 16:26:06,740 - Epoch: [181][  170/  246]    Overall Loss 0.302627    Objective Loss 0.302627                                        LR 0.000005    Time 0.019049    
2023-01-06 16:26:06,912 - Epoch: [181][  180/  246]    Overall Loss 0.303453    Objective Loss 0.303453                                        LR 0.000005    Time 0.018948    
2023-01-06 16:26:07,087 - Epoch: [181][  190/  246]    Overall Loss 0.303588    Objective Loss 0.303588                                        LR 0.000005    Time 0.018870    
2023-01-06 16:26:07,264 - Epoch: [181][  200/  246]    Overall Loss 0.303383    Objective Loss 0.303383                                        LR 0.000005    Time 0.018810    
2023-01-06 16:26:07,439 - Epoch: [181][  210/  246]    Overall Loss 0.303596    Objective Loss 0.303596                                        LR 0.000005    Time 0.018744    
2023-01-06 16:26:07,609 - Epoch: [181][  220/  246]    Overall Loss 0.303207    Objective Loss 0.303207                                        LR 0.000005    Time 0.018655    
2023-01-06 16:26:07,775 - Epoch: [181][  230/  246]    Overall Loss 0.303254    Objective Loss 0.303254                                        LR 0.000005    Time 0.018562    
2023-01-06 16:26:07,956 - Epoch: [181][  240/  246]    Overall Loss 0.302795    Objective Loss 0.302795                                        LR 0.000005    Time 0.018540    
2023-01-06 16:26:08,038 - Epoch: [181][  246/  246]    Overall Loss 0.302990    Objective Loss 0.302990    Top1 86.842105    LR 0.000005    Time 0.018423    
2023-01-06 16:26:08,172 - --- validate (epoch=181)-----------
2023-01-06 16:26:08,173 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:08,605 - Epoch: [181][   10/   28]    Loss 0.287183    Top1 89.179688    
2023-01-06 16:26:08,709 - Epoch: [181][   20/   28]    Loss 0.296188    Top1 88.964844    
2023-01-06 16:26:08,770 - Epoch: [181][   28/   28]    Loss 0.308363    Top1 88.519897    
2023-01-06 16:26:08,924 - ==> Top1: 88.520    Loss: 0.308

2023-01-06 16:26:08,925 - ==> Confusion:
[[ 170    8  261]
 [   8  188  406]
 [  52   67 5826]]

2023-01-06 16:26:08,926 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:08,926 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:08,931 - 

2023-01-06 16:26:08,931 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:09,612 - Epoch: [182][   10/  246]    Overall Loss 0.303070    Objective Loss 0.303070                                        LR 0.000005    Time 0.068030    
2023-01-06 16:26:09,764 - Epoch: [182][   20/  246]    Overall Loss 0.303708    Objective Loss 0.303708                                        LR 0.000005    Time 0.041577    
2023-01-06 16:26:09,916 - Epoch: [182][   30/  246]    Overall Loss 0.300514    Objective Loss 0.300514                                        LR 0.000005    Time 0.032778    
2023-01-06 16:26:10,073 - Epoch: [182][   40/  246]    Overall Loss 0.297786    Objective Loss 0.297786                                        LR 0.000005    Time 0.028481    
2023-01-06 16:26:10,227 - Epoch: [182][   50/  246]    Overall Loss 0.305219    Objective Loss 0.305219                                        LR 0.000005    Time 0.025862    
2023-01-06 16:26:10,383 - Epoch: [182][   60/  246]    Overall Loss 0.303825    Objective Loss 0.303825                                        LR 0.000005    Time 0.024135    
2023-01-06 16:26:10,532 - Epoch: [182][   70/  246]    Overall Loss 0.304441    Objective Loss 0.304441                                        LR 0.000005    Time 0.022820    
2023-01-06 16:26:10,690 - Epoch: [182][   80/  246]    Overall Loss 0.302909    Objective Loss 0.302909                                        LR 0.000005    Time 0.021936    
2023-01-06 16:26:10,842 - Epoch: [182][   90/  246]    Overall Loss 0.302013    Objective Loss 0.302013                                        LR 0.000005    Time 0.021185    
2023-01-06 16:26:10,986 - Epoch: [182][  100/  246]    Overall Loss 0.301973    Objective Loss 0.301973                                        LR 0.000005    Time 0.020497    
2023-01-06 16:26:11,123 - Epoch: [182][  110/  246]    Overall Loss 0.302066    Objective Loss 0.302066                                        LR 0.000005    Time 0.019879    
2023-01-06 16:26:11,258 - Epoch: [182][  120/  246]    Overall Loss 0.302691    Objective Loss 0.302691                                        LR 0.000005    Time 0.019344    
2023-01-06 16:26:11,390 - Epoch: [182][  130/  246]    Overall Loss 0.301009    Objective Loss 0.301009                                        LR 0.000005    Time 0.018866    
2023-01-06 16:26:11,522 - Epoch: [182][  140/  246]    Overall Loss 0.300100    Objective Loss 0.300100                                        LR 0.000005    Time 0.018456    
2023-01-06 16:26:11,652 - Epoch: [182][  150/  246]    Overall Loss 0.299059    Objective Loss 0.299059                                        LR 0.000005    Time 0.018091    
2023-01-06 16:26:11,784 - Epoch: [182][  160/  246]    Overall Loss 0.300473    Objective Loss 0.300473                                        LR 0.000005    Time 0.017782    
2023-01-06 16:26:11,914 - Epoch: [182][  170/  246]    Overall Loss 0.300235    Objective Loss 0.300235                                        LR 0.000005    Time 0.017499    
2023-01-06 16:26:12,047 - Epoch: [182][  180/  246]    Overall Loss 0.300346    Objective Loss 0.300346                                        LR 0.000005    Time 0.017257    
2023-01-06 16:26:12,184 - Epoch: [182][  190/  246]    Overall Loss 0.301246    Objective Loss 0.301246                                        LR 0.000005    Time 0.017068    
2023-01-06 16:26:12,321 - Epoch: [182][  200/  246]    Overall Loss 0.301139    Objective Loss 0.301139                                        LR 0.000005    Time 0.016897    
2023-01-06 16:26:12,459 - Epoch: [182][  210/  246]    Overall Loss 0.302080    Objective Loss 0.302080                                        LR 0.000005    Time 0.016745    
2023-01-06 16:26:12,595 - Epoch: [182][  220/  246]    Overall Loss 0.301571    Objective Loss 0.301571                                        LR 0.000005    Time 0.016601    
2023-01-06 16:26:12,728 - Epoch: [182][  230/  246]    Overall Loss 0.302325    Objective Loss 0.302325                                        LR 0.000005    Time 0.016459    
2023-01-06 16:26:12,877 - Epoch: [182][  240/  246]    Overall Loss 0.302430    Objective Loss 0.302430                                        LR 0.000005    Time 0.016392    
2023-01-06 16:26:12,947 - Epoch: [182][  246/  246]    Overall Loss 0.303112    Objective Loss 0.303112    Top1 89.234450    LR 0.000005    Time 0.016277    
2023-01-06 16:26:13,091 - --- validate (epoch=182)-----------
2023-01-06 16:26:13,091 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:13,528 - Epoch: [182][   10/   28]    Loss 0.301230    Top1 88.828125    
2023-01-06 16:26:13,632 - Epoch: [182][   20/   28]    Loss 0.300905    Top1 88.945312    
2023-01-06 16:26:13,690 - Epoch: [182][   28/   28]    Loss 0.304399    Top1 88.648726    
2023-01-06 16:26:13,840 - ==> Top1: 88.649    Loss: 0.304

2023-01-06 16:26:13,840 - ==> Confusion:
[[ 188    8  243]
 [   9  199  394]
 [  66   73 5806]]

2023-01-06 16:26:13,842 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:13,842 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:13,847 - 

2023-01-06 16:26:13,847 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:14,519 - Epoch: [183][   10/  246]    Overall Loss 0.300879    Objective Loss 0.300879                                        LR 0.000005    Time 0.067123    
2023-01-06 16:26:14,655 - Epoch: [183][   20/  246]    Overall Loss 0.303609    Objective Loss 0.303609                                        LR 0.000005    Time 0.040337    
2023-01-06 16:26:14,803 - Epoch: [183][   30/  246]    Overall Loss 0.304500    Objective Loss 0.304500                                        LR 0.000005    Time 0.031812    
2023-01-06 16:26:14,945 - Epoch: [183][   40/  246]    Overall Loss 0.305345    Objective Loss 0.305345                                        LR 0.000005    Time 0.027420    
2023-01-06 16:26:15,093 - Epoch: [183][   50/  246]    Overall Loss 0.308197    Objective Loss 0.308197                                        LR 0.000005    Time 0.024871    
2023-01-06 16:26:15,238 - Epoch: [183][   60/  246]    Overall Loss 0.307269    Objective Loss 0.307269                                        LR 0.000005    Time 0.023140    
2023-01-06 16:26:15,388 - Epoch: [183][   70/  246]    Overall Loss 0.305143    Objective Loss 0.305143                                        LR 0.000005    Time 0.021981    
2023-01-06 16:26:15,552 - Epoch: [183][   80/  246]    Overall Loss 0.303939    Objective Loss 0.303939                                        LR 0.000005    Time 0.021273    
2023-01-06 16:26:15,714 - Epoch: [183][   90/  246]    Overall Loss 0.302569    Objective Loss 0.302569                                        LR 0.000005    Time 0.020707    
2023-01-06 16:26:15,888 - Epoch: [183][  100/  246]    Overall Loss 0.304354    Objective Loss 0.304354                                        LR 0.000005    Time 0.020375    
2023-01-06 16:26:16,070 - Epoch: [183][  110/  246]    Overall Loss 0.305345    Objective Loss 0.305345                                        LR 0.000005    Time 0.020172    
2023-01-06 16:26:16,237 - Epoch: [183][  120/  246]    Overall Loss 0.304334    Objective Loss 0.304334                                        LR 0.000005    Time 0.019877    
2023-01-06 16:26:16,383 - Epoch: [183][  130/  246]    Overall Loss 0.303888    Objective Loss 0.303888                                        LR 0.000005    Time 0.019470    
2023-01-06 16:26:16,520 - Epoch: [183][  140/  246]    Overall Loss 0.304818    Objective Loss 0.304818                                        LR 0.000005    Time 0.019055    
2023-01-06 16:26:16,662 - Epoch: [183][  150/  246]    Overall Loss 0.304669    Objective Loss 0.304669                                        LR 0.000005    Time 0.018721    
2023-01-06 16:26:16,809 - Epoch: [183][  160/  246]    Overall Loss 0.304526    Objective Loss 0.304526                                        LR 0.000005    Time 0.018464    
2023-01-06 16:26:16,979 - Epoch: [183][  170/  246]    Overall Loss 0.305143    Objective Loss 0.305143                                        LR 0.000005    Time 0.018379    
2023-01-06 16:26:17,161 - Epoch: [183][  180/  246]    Overall Loss 0.305708    Objective Loss 0.305708                                        LR 0.000005    Time 0.018364    
2023-01-06 16:26:17,364 - Epoch: [183][  190/  246]    Overall Loss 0.305061    Objective Loss 0.305061                                        LR 0.000005    Time 0.018467    
2023-01-06 16:26:17,564 - Epoch: [183][  200/  246]    Overall Loss 0.304130    Objective Loss 0.304130                                        LR 0.000005    Time 0.018540    
2023-01-06 16:26:17,763 - Epoch: [183][  210/  246]    Overall Loss 0.304219    Objective Loss 0.304219                                        LR 0.000005    Time 0.018601    
2023-01-06 16:26:17,967 - Epoch: [183][  220/  246]    Overall Loss 0.304542    Objective Loss 0.304542                                        LR 0.000005    Time 0.018680    
2023-01-06 16:26:18,160 - Epoch: [183][  230/  246]    Overall Loss 0.302865    Objective Loss 0.302865                                        LR 0.000005    Time 0.018706    
2023-01-06 16:26:18,338 - Epoch: [183][  240/  246]    Overall Loss 0.302164    Objective Loss 0.302164                                        LR 0.000005    Time 0.018669    
2023-01-06 16:26:18,411 - Epoch: [183][  246/  246]    Overall Loss 0.302442    Objective Loss 0.302442    Top1 88.277512    LR 0.000005    Time 0.018509    
2023-01-06 16:26:18,554 - --- validate (epoch=183)-----------
2023-01-06 16:26:18,554 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:19,019 - Epoch: [183][   10/   28]    Loss 0.294619    Top1 89.453125    
2023-01-06 16:26:19,141 - Epoch: [183][   20/   28]    Loss 0.303220    Top1 89.140625    
2023-01-06 16:26:19,199 - Epoch: [183][   28/   28]    Loss 0.306991    Top1 88.863441    
2023-01-06 16:26:19,351 - ==> Top1: 88.863    Loss: 0.307

2023-01-06 16:26:19,351 - ==> Confusion:
[[ 181    8  250]
 [  10  218  374]
 [  55   81 5809]]

2023-01-06 16:26:19,353 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:19,353 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:19,358 - 

2023-01-06 16:26:19,358 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:19,910 - Epoch: [184][   10/  246]    Overall Loss 0.296984    Objective Loss 0.296984                                        LR 0.000005    Time 0.055094    
2023-01-06 16:26:20,067 - Epoch: [184][   20/  246]    Overall Loss 0.301275    Objective Loss 0.301275                                        LR 0.000005    Time 0.035393    
2023-01-06 16:26:20,229 - Epoch: [184][   30/  246]    Overall Loss 0.305101    Objective Loss 0.305101                                        LR 0.000005    Time 0.028990    
2023-01-06 16:26:20,396 - Epoch: [184][   40/  246]    Overall Loss 0.300210    Objective Loss 0.300210                                        LR 0.000005    Time 0.025899    
2023-01-06 16:26:20,560 - Epoch: [184][   50/  246]    Overall Loss 0.300955    Objective Loss 0.300955                                        LR 0.000005    Time 0.023995    
2023-01-06 16:26:20,712 - Epoch: [184][   60/  246]    Overall Loss 0.299803    Objective Loss 0.299803                                        LR 0.000005    Time 0.022517    
2023-01-06 16:26:20,850 - Epoch: [184][   70/  246]    Overall Loss 0.299781    Objective Loss 0.299781                                        LR 0.000005    Time 0.021274    
2023-01-06 16:26:21,000 - Epoch: [184][   80/  246]    Overall Loss 0.302935    Objective Loss 0.302935                                        LR 0.000005    Time 0.020482    
2023-01-06 16:26:21,160 - Epoch: [184][   90/  246]    Overall Loss 0.303087    Objective Loss 0.303087                                        LR 0.000005    Time 0.019981    
2023-01-06 16:26:21,301 - Epoch: [184][  100/  246]    Overall Loss 0.302889    Objective Loss 0.302889                                        LR 0.000005    Time 0.019387    
2023-01-06 16:26:21,443 - Epoch: [184][  110/  246]    Overall Loss 0.301169    Objective Loss 0.301169                                        LR 0.000005    Time 0.018914    
2023-01-06 16:26:21,594 - Epoch: [184][  120/  246]    Overall Loss 0.300842    Objective Loss 0.300842                                        LR 0.000005    Time 0.018589    
2023-01-06 16:26:21,746 - Epoch: [184][  130/  246]    Overall Loss 0.301445    Objective Loss 0.301445                                        LR 0.000005    Time 0.018325    
2023-01-06 16:26:21,910 - Epoch: [184][  140/  246]    Overall Loss 0.301236    Objective Loss 0.301236                                        LR 0.000005    Time 0.018184    
2023-01-06 16:26:22,068 - Epoch: [184][  150/  246]    Overall Loss 0.299964    Objective Loss 0.299964                                        LR 0.000005    Time 0.018019    
2023-01-06 16:26:22,213 - Epoch: [184][  160/  246]    Overall Loss 0.301731    Objective Loss 0.301731                                        LR 0.000005    Time 0.017801    
2023-01-06 16:26:22,353 - Epoch: [184][  170/  246]    Overall Loss 0.302351    Objective Loss 0.302351                                        LR 0.000005    Time 0.017573    
2023-01-06 16:26:22,490 - Epoch: [184][  180/  246]    Overall Loss 0.302494    Objective Loss 0.302494                                        LR 0.000005    Time 0.017357    
2023-01-06 16:26:22,630 - Epoch: [184][  190/  246]    Overall Loss 0.302973    Objective Loss 0.302973                                        LR 0.000005    Time 0.017174    
2023-01-06 16:26:22,772 - Epoch: [184][  200/  246]    Overall Loss 0.303066    Objective Loss 0.303066                                        LR 0.000005    Time 0.017026    
2023-01-06 16:26:22,912 - Epoch: [184][  210/  246]    Overall Loss 0.302629    Objective Loss 0.302629                                        LR 0.000005    Time 0.016880    
2023-01-06 16:26:23,079 - Epoch: [184][  220/  246]    Overall Loss 0.302294    Objective Loss 0.302294                                        LR 0.000005    Time 0.016869    
2023-01-06 16:26:23,229 - Epoch: [184][  230/  246]    Overall Loss 0.302287    Objective Loss 0.302287                                        LR 0.000005    Time 0.016786    
2023-01-06 16:26:23,385 - Epoch: [184][  240/  246]    Overall Loss 0.302391    Objective Loss 0.302391                                        LR 0.000005    Time 0.016736    
2023-01-06 16:26:23,461 - Epoch: [184][  246/  246]    Overall Loss 0.301952    Objective Loss 0.301952    Top1 90.191388    LR 0.000005    Time 0.016635    
2023-01-06 16:26:23,600 - --- validate (epoch=184)-----------
2023-01-06 16:26:23,600 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:24,032 - Epoch: [184][   10/   28]    Loss 0.293969    Top1 89.843750    
2023-01-06 16:26:24,136 - Epoch: [184][   20/   28]    Loss 0.306761    Top1 88.847656    
2023-01-06 16:26:24,192 - Epoch: [184][   28/   28]    Loss 0.308340    Top1 88.920699    
2023-01-06 16:26:24,323 - ==> Top1: 88.921    Loss: 0.308

2023-01-06 16:26:24,324 - ==> Confusion:
[[ 184    9  246]
 [  11  210  381]
 [  54   73 5818]]

2023-01-06 16:26:24,325 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:24,325 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:24,329 - 

2023-01-06 16:26:24,329 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:25,001 - Epoch: [185][   10/  246]    Overall Loss 0.307466    Objective Loss 0.307466                                        LR 0.000005    Time 0.067117    
2023-01-06 16:26:25,159 - Epoch: [185][   20/  246]    Overall Loss 0.306462    Objective Loss 0.306462                                        LR 0.000005    Time 0.041383    
2023-01-06 16:26:25,335 - Epoch: [185][   30/  246]    Overall Loss 0.307374    Objective Loss 0.307374                                        LR 0.000005    Time 0.033419    
2023-01-06 16:26:25,503 - Epoch: [185][   40/  246]    Overall Loss 0.307285    Objective Loss 0.307285                                        LR 0.000005    Time 0.029272    
2023-01-06 16:26:25,671 - Epoch: [185][   50/  246]    Overall Loss 0.309404    Objective Loss 0.309404                                        LR 0.000005    Time 0.026765    
2023-01-06 16:26:25,839 - Epoch: [185][   60/  246]    Overall Loss 0.311551    Objective Loss 0.311551                                        LR 0.000005    Time 0.025090    
2023-01-06 16:26:26,001 - Epoch: [185][   70/  246]    Overall Loss 0.308805    Objective Loss 0.308805                                        LR 0.000005    Time 0.023823    
2023-01-06 16:26:26,168 - Epoch: [185][   80/  246]    Overall Loss 0.308907    Objective Loss 0.308907                                        LR 0.000005    Time 0.022921    
2023-01-06 16:26:26,340 - Epoch: [185][   90/  246]    Overall Loss 0.309661    Objective Loss 0.309661                                        LR 0.000005    Time 0.022289    
2023-01-06 16:26:26,510 - Epoch: [185][  100/  246]    Overall Loss 0.310107    Objective Loss 0.310107                                        LR 0.000005    Time 0.021754    
2023-01-06 16:26:26,684 - Epoch: [185][  110/  246]    Overall Loss 0.309352    Objective Loss 0.309352                                        LR 0.000005    Time 0.021358    
2023-01-06 16:26:26,857 - Epoch: [185][  120/  246]    Overall Loss 0.308557    Objective Loss 0.308557                                        LR 0.000005    Time 0.021011    
2023-01-06 16:26:27,031 - Epoch: [185][  130/  246]    Overall Loss 0.309403    Objective Loss 0.309403                                        LR 0.000005    Time 0.020734    
2023-01-06 16:26:27,206 - Epoch: [185][  140/  246]    Overall Loss 0.308057    Objective Loss 0.308057                                        LR 0.000005    Time 0.020499    
2023-01-06 16:26:27,376 - Epoch: [185][  150/  246]    Overall Loss 0.307151    Objective Loss 0.307151                                        LR 0.000005    Time 0.020260    
2023-01-06 16:26:27,547 - Epoch: [185][  160/  246]    Overall Loss 0.306333    Objective Loss 0.306333                                        LR 0.000005    Time 0.020063    
2023-01-06 16:26:27,723 - Epoch: [185][  170/  246]    Overall Loss 0.304720    Objective Loss 0.304720                                        LR 0.000005    Time 0.019916    
2023-01-06 16:26:27,894 - Epoch: [185][  180/  246]    Overall Loss 0.304219    Objective Loss 0.304219                                        LR 0.000005    Time 0.019757    
2023-01-06 16:26:28,069 - Epoch: [185][  190/  246]    Overall Loss 0.304548    Objective Loss 0.304548                                        LR 0.000005    Time 0.019640    
2023-01-06 16:26:28,242 - Epoch: [185][  200/  246]    Overall Loss 0.304149    Objective Loss 0.304149                                        LR 0.000005    Time 0.019519    
2023-01-06 16:26:28,417 - Epoch: [185][  210/  246]    Overall Loss 0.302769    Objective Loss 0.302769                                        LR 0.000005    Time 0.019421    
2023-01-06 16:26:28,591 - Epoch: [185][  220/  246]    Overall Loss 0.303214    Objective Loss 0.303214                                        LR 0.000005    Time 0.019325    
2023-01-06 16:26:28,764 - Epoch: [185][  230/  246]    Overall Loss 0.302556    Objective Loss 0.302556                                        LR 0.000005    Time 0.019239    
2023-01-06 16:26:28,950 - Epoch: [185][  240/  246]    Overall Loss 0.302635    Objective Loss 0.302635                                        LR 0.000005    Time 0.019210    
2023-01-06 16:26:29,034 - Epoch: [185][  246/  246]    Overall Loss 0.302365    Objective Loss 0.302365    Top1 89.473684    LR 0.000005    Time 0.019081    
2023-01-06 16:26:29,173 - --- validate (epoch=185)-----------
2023-01-06 16:26:29,173 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:29,606 - Epoch: [185][   10/   28]    Loss 0.292189    Top1 89.140625    
2023-01-06 16:26:29,712 - Epoch: [185][   20/   28]    Loss 0.302608    Top1 88.789062    
2023-01-06 16:26:29,771 - Epoch: [185][   28/   28]    Loss 0.303529    Top1 88.720298    
2023-01-06 16:26:29,932 - ==> Top1: 88.720    Loss: 0.304

2023-01-06 16:26:29,932 - ==> Confusion:
[[ 175    8  256]
 [  10  202  390]
 [  55   69 5821]]

2023-01-06 16:26:29,933 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:29,934 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:29,938 - 

2023-01-06 16:26:29,938 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:30,472 - Epoch: [186][   10/  246]    Overall Loss 0.285459    Objective Loss 0.285459                                        LR 0.000005    Time 0.053303    
2023-01-06 16:26:30,635 - Epoch: [186][   20/  246]    Overall Loss 0.283235    Objective Loss 0.283235                                        LR 0.000005    Time 0.034813    
2023-01-06 16:26:30,808 - Epoch: [186][   30/  246]    Overall Loss 0.288549    Objective Loss 0.288549                                        LR 0.000005    Time 0.028958    
2023-01-06 16:26:30,973 - Epoch: [186][   40/  246]    Overall Loss 0.294531    Objective Loss 0.294531                                        LR 0.000005    Time 0.025833    
2023-01-06 16:26:31,140 - Epoch: [186][   50/  246]    Overall Loss 0.289050    Objective Loss 0.289050                                        LR 0.000005    Time 0.023991    
2023-01-06 16:26:31,315 - Epoch: [186][   60/  246]    Overall Loss 0.291376    Objective Loss 0.291376                                        LR 0.000005    Time 0.022907    
2023-01-06 16:26:31,486 - Epoch: [186][   70/  246]    Overall Loss 0.294697    Objective Loss 0.294697                                        LR 0.000005    Time 0.022070    
2023-01-06 16:26:31,650 - Epoch: [186][   80/  246]    Overall Loss 0.295215    Objective Loss 0.295215                                        LR 0.000005    Time 0.021349    
2023-01-06 16:26:31,815 - Epoch: [186][   90/  246]    Overall Loss 0.296567    Objective Loss 0.296567                                        LR 0.000005    Time 0.020815    
2023-01-06 16:26:31,981 - Epoch: [186][  100/  246]    Overall Loss 0.296325    Objective Loss 0.296325                                        LR 0.000005    Time 0.020385    
2023-01-06 16:26:32,146 - Epoch: [186][  110/  246]    Overall Loss 0.296289    Objective Loss 0.296289                                        LR 0.000005    Time 0.020030    
2023-01-06 16:26:32,316 - Epoch: [186][  120/  246]    Overall Loss 0.297634    Objective Loss 0.297634                                        LR 0.000005    Time 0.019778    
2023-01-06 16:26:32,491 - Epoch: [186][  130/  246]    Overall Loss 0.297652    Objective Loss 0.297652                                        LR 0.000005    Time 0.019595    
2023-01-06 16:26:32,663 - Epoch: [186][  140/  246]    Overall Loss 0.299876    Objective Loss 0.299876                                        LR 0.000005    Time 0.019421    
2023-01-06 16:26:32,832 - Epoch: [186][  150/  246]    Overall Loss 0.300061    Objective Loss 0.300061                                        LR 0.000005    Time 0.019255    
2023-01-06 16:26:33,004 - Epoch: [186][  160/  246]    Overall Loss 0.300155    Objective Loss 0.300155                                        LR 0.000005    Time 0.019122    
2023-01-06 16:26:33,190 - Epoch: [186][  170/  246]    Overall Loss 0.299506    Objective Loss 0.299506                                        LR 0.000005    Time 0.019088    
2023-01-06 16:26:33,351 - Epoch: [186][  180/  246]    Overall Loss 0.299698    Objective Loss 0.299698                                        LR 0.000005    Time 0.018920    
2023-01-06 16:26:33,515 - Epoch: [186][  190/  246]    Overall Loss 0.300320    Objective Loss 0.300320                                        LR 0.000005    Time 0.018785    
2023-01-06 16:26:33,684 - Epoch: [186][  200/  246]    Overall Loss 0.301094    Objective Loss 0.301094                                        LR 0.000005    Time 0.018691    
2023-01-06 16:26:33,859 - Epoch: [186][  210/  246]    Overall Loss 0.301878    Objective Loss 0.301878                                        LR 0.000005    Time 0.018630    
2023-01-06 16:26:34,026 - Epoch: [186][  220/  246]    Overall Loss 0.301792    Objective Loss 0.301792                                        LR 0.000005    Time 0.018542    
2023-01-06 16:26:34,199 - Epoch: [186][  230/  246]    Overall Loss 0.301646    Objective Loss 0.301646                                        LR 0.000005    Time 0.018485    
2023-01-06 16:26:34,384 - Epoch: [186][  240/  246]    Overall Loss 0.301651    Objective Loss 0.301651                                        LR 0.000005    Time 0.018487    
2023-01-06 16:26:34,463 - Epoch: [186][  246/  246]    Overall Loss 0.302089    Objective Loss 0.302089    Top1 88.755981    LR 0.000005    Time 0.018357    
2023-01-06 16:26:34,603 - --- validate (epoch=186)-----------
2023-01-06 16:26:34,603 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:35,035 - Epoch: [186][   10/   28]    Loss 0.296761    Top1 88.554688    
2023-01-06 16:26:35,137 - Epoch: [186][   20/   28]    Loss 0.306821    Top1 88.457031    
2023-01-06 16:26:35,193 - Epoch: [186][   28/   28]    Loss 0.305627    Top1 88.491268    
2023-01-06 16:26:35,322 - ==> Top1: 88.491    Loss: 0.306

2023-01-06 16:26:35,323 - ==> Confusion:
[[ 169    9  261]
 [  10  194  398]
 [  53   73 5819]]

2023-01-06 16:26:35,324 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:35,324 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:35,329 - 

2023-01-06 16:26:35,329 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:36,020 - Epoch: [187][   10/  246]    Overall Loss 0.290417    Objective Loss 0.290417                                        LR 0.000005    Time 0.069035    
2023-01-06 16:26:36,192 - Epoch: [187][   20/  246]    Overall Loss 0.299296    Objective Loss 0.299296                                        LR 0.000005    Time 0.043088    
2023-01-06 16:26:36,373 - Epoch: [187][   30/  246]    Overall Loss 0.302326    Objective Loss 0.302326                                        LR 0.000005    Time 0.034766    
2023-01-06 16:26:36,543 - Epoch: [187][   40/  246]    Overall Loss 0.295147    Objective Loss 0.295147                                        LR 0.000005    Time 0.030310    
2023-01-06 16:26:36,720 - Epoch: [187][   50/  246]    Overall Loss 0.297189    Objective Loss 0.297189                                        LR 0.000005    Time 0.027756    
2023-01-06 16:26:36,891 - Epoch: [187][   60/  246]    Overall Loss 0.298434    Objective Loss 0.298434                                        LR 0.000005    Time 0.025984    
2023-01-06 16:26:37,067 - Epoch: [187][   70/  246]    Overall Loss 0.300317    Objective Loss 0.300317                                        LR 0.000005    Time 0.024771    
2023-01-06 16:26:37,221 - Epoch: [187][   80/  246]    Overall Loss 0.301376    Objective Loss 0.301376                                        LR 0.000005    Time 0.023593    
2023-01-06 16:26:37,367 - Epoch: [187][   90/  246]    Overall Loss 0.302631    Objective Loss 0.302631                                        LR 0.000005    Time 0.022591    
2023-01-06 16:26:37,529 - Epoch: [187][  100/  246]    Overall Loss 0.301024    Objective Loss 0.301024                                        LR 0.000005    Time 0.021949    
2023-01-06 16:26:37,696 - Epoch: [187][  110/  246]    Overall Loss 0.300789    Objective Loss 0.300789                                        LR 0.000005    Time 0.021468    
2023-01-06 16:26:37,861 - Epoch: [187][  120/  246]    Overall Loss 0.301503    Objective Loss 0.301503                                        LR 0.000005    Time 0.021052    
2023-01-06 16:26:38,028 - Epoch: [187][  130/  246]    Overall Loss 0.302338    Objective Loss 0.302338                                        LR 0.000005    Time 0.020711    
2023-01-06 16:26:38,176 - Epoch: [187][  140/  246]    Overall Loss 0.304285    Objective Loss 0.304285                                        LR 0.000005    Time 0.020288    
2023-01-06 16:26:38,321 - Epoch: [187][  150/  246]    Overall Loss 0.305146    Objective Loss 0.305146                                        LR 0.000005    Time 0.019896    
2023-01-06 16:26:38,462 - Epoch: [187][  160/  246]    Overall Loss 0.305262    Objective Loss 0.305262                                        LR 0.000005    Time 0.019537    
2023-01-06 16:26:38,622 - Epoch: [187][  170/  246]    Overall Loss 0.304678    Objective Loss 0.304678                                        LR 0.000005    Time 0.019322    
2023-01-06 16:26:38,777 - Epoch: [187][  180/  246]    Overall Loss 0.304219    Objective Loss 0.304219                                        LR 0.000005    Time 0.019111    
2023-01-06 16:26:38,932 - Epoch: [187][  190/  246]    Overall Loss 0.303918    Objective Loss 0.303918                                        LR 0.000005    Time 0.018915    
2023-01-06 16:26:39,075 - Epoch: [187][  200/  246]    Overall Loss 0.303718    Objective Loss 0.303718                                        LR 0.000005    Time 0.018683    
2023-01-06 16:26:39,251 - Epoch: [187][  210/  246]    Overall Loss 0.303281    Objective Loss 0.303281                                        LR 0.000005    Time 0.018633    
2023-01-06 16:26:39,426 - Epoch: [187][  220/  246]    Overall Loss 0.302966    Objective Loss 0.302966                                        LR 0.000005    Time 0.018576    
2023-01-06 16:26:39,614 - Epoch: [187][  230/  246]    Overall Loss 0.301629    Objective Loss 0.301629                                        LR 0.000005    Time 0.018587    
2023-01-06 16:26:39,830 - Epoch: [187][  240/  246]    Overall Loss 0.301118    Objective Loss 0.301118                                        LR 0.000005    Time 0.018711    
2023-01-06 16:26:39,929 - Epoch: [187][  246/  246]    Overall Loss 0.301427    Objective Loss 0.301427    Top1 88.277512    LR 0.000005    Time 0.018653    
2023-01-06 16:26:40,071 - --- validate (epoch=187)-----------
2023-01-06 16:26:40,072 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:40,512 - Epoch: [187][   10/   28]    Loss 0.332024    Top1 87.343750    
2023-01-06 16:26:40,628 - Epoch: [187][   20/   28]    Loss 0.309688    Top1 88.535156    
2023-01-06 16:26:40,685 - Epoch: [187][   28/   28]    Loss 0.302793    Top1 88.806184    
2023-01-06 16:26:40,813 - ==> Top1: 88.806    Loss: 0.303

2023-01-06 16:26:40,813 - ==> Confusion:
[[ 180    8  251]
 [  11  210  381]
 [  55   76 5814]]

2023-01-06 16:26:40,815 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:40,815 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:40,819 - 

2023-01-06 16:26:40,819 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:41,513 - Epoch: [188][   10/  246]    Overall Loss 0.301917    Objective Loss 0.301917                                        LR 0.000005    Time 0.069338    
2023-01-06 16:26:41,686 - Epoch: [188][   20/  246]    Overall Loss 0.291076    Objective Loss 0.291076                                        LR 0.000005    Time 0.043292    
2023-01-06 16:26:41,830 - Epoch: [188][   30/  246]    Overall Loss 0.309672    Objective Loss 0.309672                                        LR 0.000005    Time 0.033644    
2023-01-06 16:26:41,990 - Epoch: [188][   40/  246]    Overall Loss 0.313834    Objective Loss 0.313834                                        LR 0.000005    Time 0.029221    
2023-01-06 16:26:42,154 - Epoch: [188][   50/  246]    Overall Loss 0.312241    Objective Loss 0.312241                                        LR 0.000005    Time 0.026654    
2023-01-06 16:26:42,327 - Epoch: [188][   60/  246]    Overall Loss 0.309899    Objective Loss 0.309899                                        LR 0.000005    Time 0.025090    
2023-01-06 16:26:42,465 - Epoch: [188][   70/  246]    Overall Loss 0.307600    Objective Loss 0.307600                                        LR 0.000005    Time 0.023464    
2023-01-06 16:26:42,600 - Epoch: [188][   80/  246]    Overall Loss 0.306855    Objective Loss 0.306855                                        LR 0.000005    Time 0.022218    
2023-01-06 16:26:42,735 - Epoch: [188][   90/  246]    Overall Loss 0.306455    Objective Loss 0.306455                                        LR 0.000005    Time 0.021250    
2023-01-06 16:26:42,870 - Epoch: [188][  100/  246]    Overall Loss 0.305694    Objective Loss 0.305694                                        LR 0.000005    Time 0.020466    
2023-01-06 16:26:43,017 - Epoch: [188][  110/  246]    Overall Loss 0.306476    Objective Loss 0.306476                                        LR 0.000005    Time 0.019937    
2023-01-06 16:26:43,178 - Epoch: [188][  120/  246]    Overall Loss 0.304367    Objective Loss 0.304367                                        LR 0.000005    Time 0.019619    
2023-01-06 16:26:43,341 - Epoch: [188][  130/  246]    Overall Loss 0.304425    Objective Loss 0.304425                                        LR 0.000005    Time 0.019364    
2023-01-06 16:26:43,505 - Epoch: [188][  140/  246]    Overall Loss 0.306491    Objective Loss 0.306491                                        LR 0.000005    Time 0.019143    
2023-01-06 16:26:43,665 - Epoch: [188][  150/  246]    Overall Loss 0.306106    Objective Loss 0.306106                                        LR 0.000005    Time 0.018937    
2023-01-06 16:26:43,820 - Epoch: [188][  160/  246]    Overall Loss 0.306000    Objective Loss 0.306000                                        LR 0.000005    Time 0.018717    
2023-01-06 16:26:43,971 - Epoch: [188][  170/  246]    Overall Loss 0.304279    Objective Loss 0.304279                                        LR 0.000005    Time 0.018497    
2023-01-06 16:26:44,124 - Epoch: [188][  180/  246]    Overall Loss 0.302659    Objective Loss 0.302659                                        LR 0.000005    Time 0.018316    
2023-01-06 16:26:44,276 - Epoch: [188][  190/  246]    Overall Loss 0.302879    Objective Loss 0.302879                                        LR 0.000005    Time 0.018151    
2023-01-06 16:26:44,424 - Epoch: [188][  200/  246]    Overall Loss 0.301813    Objective Loss 0.301813                                        LR 0.000005    Time 0.017980    
2023-01-06 16:26:44,577 - Epoch: [188][  210/  246]    Overall Loss 0.301306    Objective Loss 0.301306                                        LR 0.000005    Time 0.017853    
2023-01-06 16:26:44,751 - Epoch: [188][  220/  246]    Overall Loss 0.301037    Objective Loss 0.301037                                        LR 0.000005    Time 0.017831    
2023-01-06 16:26:44,928 - Epoch: [188][  230/  246]    Overall Loss 0.301836    Objective Loss 0.301836                                        LR 0.000005    Time 0.017822    
2023-01-06 16:26:45,132 - Epoch: [188][  240/  246]    Overall Loss 0.301078    Objective Loss 0.301078                                        LR 0.000005    Time 0.017929    
2023-01-06 16:26:45,230 - Epoch: [188][  246/  246]    Overall Loss 0.301135    Objective Loss 0.301135    Top1 88.755981    LR 0.000005    Time 0.017890    
2023-01-06 16:26:45,375 - --- validate (epoch=188)-----------
2023-01-06 16:26:45,376 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:45,817 - Epoch: [188][   10/   28]    Loss 0.285287    Top1 89.843750    
2023-01-06 16:26:45,930 - Epoch: [188][   20/   28]    Loss 0.293015    Top1 89.355469    
2023-01-06 16:26:45,987 - Epoch: [188][   28/   28]    Loss 0.301764    Top1 88.935013    
2023-01-06 16:26:46,146 - ==> Top1: 88.935    Loss: 0.302

2023-01-06 16:26:46,146 - ==> Confusion:
[[ 187    7  245]
 [  12  216  374]
 [  60   75 5810]]

2023-01-06 16:26:46,147 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:46,147 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:46,152 - 

2023-01-06 16:26:46,152 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:46,694 - Epoch: [189][   10/  246]    Overall Loss 0.294886    Objective Loss 0.294886                                        LR 0.000005    Time 0.054193    
2023-01-06 16:26:46,844 - Epoch: [189][   20/  246]    Overall Loss 0.297680    Objective Loss 0.297680                                        LR 0.000005    Time 0.034573    
2023-01-06 16:26:46,996 - Epoch: [189][   30/  246]    Overall Loss 0.292176    Objective Loss 0.292176                                        LR 0.000005    Time 0.028082    
2023-01-06 16:26:47,142 - Epoch: [189][   40/  246]    Overall Loss 0.301820    Objective Loss 0.301820                                        LR 0.000005    Time 0.024718    
2023-01-06 16:26:47,297 - Epoch: [189][   50/  246]    Overall Loss 0.299639    Objective Loss 0.299639                                        LR 0.000005    Time 0.022869    
2023-01-06 16:26:47,456 - Epoch: [189][   60/  246]    Overall Loss 0.298271    Objective Loss 0.298271                                        LR 0.000005    Time 0.021688    
2023-01-06 16:26:47,627 - Epoch: [189][   70/  246]    Overall Loss 0.300867    Objective Loss 0.300867                                        LR 0.000005    Time 0.021024    
2023-01-06 16:26:47,800 - Epoch: [189][   80/  246]    Overall Loss 0.301610    Objective Loss 0.301610                                        LR 0.000005    Time 0.020542    
2023-01-06 16:26:47,985 - Epoch: [189][   90/  246]    Overall Loss 0.299315    Objective Loss 0.299315                                        LR 0.000005    Time 0.020315    
2023-01-06 16:26:48,165 - Epoch: [189][  100/  246]    Overall Loss 0.299901    Objective Loss 0.299901                                        LR 0.000005    Time 0.020077    
2023-01-06 16:26:48,337 - Epoch: [189][  110/  246]    Overall Loss 0.300559    Objective Loss 0.300559                                        LR 0.000005    Time 0.019816    
2023-01-06 16:26:48,507 - Epoch: [189][  120/  246]    Overall Loss 0.299060    Objective Loss 0.299060                                        LR 0.000005    Time 0.019576    
2023-01-06 16:26:48,677 - Epoch: [189][  130/  246]    Overall Loss 0.298148    Objective Loss 0.298148                                        LR 0.000005    Time 0.019374    
2023-01-06 16:26:48,845 - Epoch: [189][  140/  246]    Overall Loss 0.299375    Objective Loss 0.299375                                        LR 0.000005    Time 0.019188    
2023-01-06 16:26:49,017 - Epoch: [189][  150/  246]    Overall Loss 0.298116    Objective Loss 0.298116                                        LR 0.000005    Time 0.019053    
2023-01-06 16:26:49,163 - Epoch: [189][  160/  246]    Overall Loss 0.299925    Objective Loss 0.299925                                        LR 0.000005    Time 0.018774    
2023-01-06 16:26:49,314 - Epoch: [189][  170/  246]    Overall Loss 0.299209    Objective Loss 0.299209                                        LR 0.000005    Time 0.018557    
2023-01-06 16:26:49,454 - Epoch: [189][  180/  246]    Overall Loss 0.299552    Objective Loss 0.299552                                        LR 0.000005    Time 0.018297    
2023-01-06 16:26:49,609 - Epoch: [189][  190/  246]    Overall Loss 0.300040    Objective Loss 0.300040                                        LR 0.000005    Time 0.018149    
2023-01-06 16:26:49,770 - Epoch: [189][  200/  246]    Overall Loss 0.300411    Objective Loss 0.300411                                        LR 0.000005    Time 0.018044    
2023-01-06 16:26:49,956 - Epoch: [189][  210/  246]    Overall Loss 0.300837    Objective Loss 0.300837                                        LR 0.000005    Time 0.018068    
2023-01-06 16:26:50,126 - Epoch: [189][  220/  246]    Overall Loss 0.301127    Objective Loss 0.301127                                        LR 0.000005    Time 0.018018    
2023-01-06 16:26:50,302 - Epoch: [189][  230/  246]    Overall Loss 0.301564    Objective Loss 0.301564                                        LR 0.000005    Time 0.017998    
2023-01-06 16:26:50,471 - Epoch: [189][  240/  246]    Overall Loss 0.301549    Objective Loss 0.301549                                        LR 0.000005    Time 0.017953    
2023-01-06 16:26:50,549 - Epoch: [189][  246/  246]    Overall Loss 0.301795    Objective Loss 0.301795    Top1 89.234450    LR 0.000005    Time 0.017831    
2023-01-06 16:26:50,700 - --- validate (epoch=189)-----------
2023-01-06 16:26:50,700 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:51,143 - Epoch: [189][   10/   28]    Loss 0.291682    Top1 89.687500    
2023-01-06 16:26:51,248 - Epoch: [189][   20/   28]    Loss 0.302701    Top1 88.984375    
2023-01-06 16:26:51,304 - Epoch: [189][   28/   28]    Loss 0.306622    Top1 88.791869    
2023-01-06 16:26:51,443 - ==> Top1: 88.792    Loss: 0.307

2023-01-06 16:26:51,443 - ==> Confusion:
[[ 199    8  232]
 [  16  211  375]
 [  73   79 5793]]

2023-01-06 16:26:51,445 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:51,445 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:51,450 - 

2023-01-06 16:26:51,450 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:52,106 - Epoch: [190][   10/  246]    Overall Loss 0.300122    Objective Loss 0.300122                                        LR 0.000003    Time 0.065612    
2023-01-06 16:26:52,249 - Epoch: [190][   20/  246]    Overall Loss 0.306691    Objective Loss 0.306691                                        LR 0.000003    Time 0.039889    
2023-01-06 16:26:52,396 - Epoch: [190][   30/  246]    Overall Loss 0.306546    Objective Loss 0.306546                                        LR 0.000003    Time 0.031426    
2023-01-06 16:26:52,538 - Epoch: [190][   40/  246]    Overall Loss 0.314388    Objective Loss 0.314388                                        LR 0.000003    Time 0.027100    
2023-01-06 16:26:52,676 - Epoch: [190][   50/  246]    Overall Loss 0.308064    Objective Loss 0.308064                                        LR 0.000003    Time 0.024433    
2023-01-06 16:26:52,814 - Epoch: [190][   60/  246]    Overall Loss 0.307734    Objective Loss 0.307734                                        LR 0.000003    Time 0.022652    
2023-01-06 16:26:52,946 - Epoch: [190][   70/  246]    Overall Loss 0.308982    Objective Loss 0.308982                                        LR 0.000003    Time 0.021304    
2023-01-06 16:26:53,076 - Epoch: [190][   80/  246]    Overall Loss 0.308770    Objective Loss 0.308770                                        LR 0.000003    Time 0.020267    
2023-01-06 16:26:53,210 - Epoch: [190][   90/  246]    Overall Loss 0.308303    Objective Loss 0.308303                                        LR 0.000003    Time 0.019498    
2023-01-06 16:26:53,344 - Epoch: [190][  100/  246]    Overall Loss 0.306226    Objective Loss 0.306226                                        LR 0.000003    Time 0.018878    
2023-01-06 16:26:53,475 - Epoch: [190][  110/  246]    Overall Loss 0.305719    Objective Loss 0.305719                                        LR 0.000003    Time 0.018354    
2023-01-06 16:26:53,607 - Epoch: [190][  120/  246]    Overall Loss 0.305382    Objective Loss 0.305382                                        LR 0.000003    Time 0.017924    
2023-01-06 16:26:53,751 - Epoch: [190][  130/  246]    Overall Loss 0.304165    Objective Loss 0.304165                                        LR 0.000003    Time 0.017644    
2023-01-06 16:26:53,910 - Epoch: [190][  140/  246]    Overall Loss 0.302954    Objective Loss 0.302954                                        LR 0.000003    Time 0.017518    
2023-01-06 16:26:54,073 - Epoch: [190][  150/  246]    Overall Loss 0.302692    Objective Loss 0.302692                                        LR 0.000003    Time 0.017436    
2023-01-06 16:26:54,228 - Epoch: [190][  160/  246]    Overall Loss 0.300574    Objective Loss 0.300574                                        LR 0.000003    Time 0.017314    
2023-01-06 16:26:54,360 - Epoch: [190][  170/  246]    Overall Loss 0.300370    Objective Loss 0.300370                                        LR 0.000003    Time 0.017068    
2023-01-06 16:26:54,502 - Epoch: [190][  180/  246]    Overall Loss 0.301927    Objective Loss 0.301927                                        LR 0.000003    Time 0.016908    
2023-01-06 16:26:54,643 - Epoch: [190][  190/  246]    Overall Loss 0.302657    Objective Loss 0.302657                                        LR 0.000003    Time 0.016756    
2023-01-06 16:26:54,778 - Epoch: [190][  200/  246]    Overall Loss 0.302447    Objective Loss 0.302447                                        LR 0.000003    Time 0.016590    
2023-01-06 16:26:54,913 - Epoch: [190][  210/  246]    Overall Loss 0.301630    Objective Loss 0.301630                                        LR 0.000003    Time 0.016441    
2023-01-06 16:26:55,050 - Epoch: [190][  220/  246]    Overall Loss 0.300524    Objective Loss 0.300524                                        LR 0.000003    Time 0.016318    
2023-01-06 16:26:55,188 - Epoch: [190][  230/  246]    Overall Loss 0.301433    Objective Loss 0.301433                                        LR 0.000003    Time 0.016207    
2023-01-06 16:26:55,343 - Epoch: [190][  240/  246]    Overall Loss 0.301027    Objective Loss 0.301027                                        LR 0.000003    Time 0.016173    
2023-01-06 16:26:55,411 - Epoch: [190][  246/  246]    Overall Loss 0.301174    Objective Loss 0.301174    Top1 88.995215    LR 0.000003    Time 0.016055    
2023-01-06 16:26:55,546 - --- validate (epoch=190)-----------
2023-01-06 16:26:55,547 - 6986 samples (256 per mini-batch)
2023-01-06 16:26:55,989 - Epoch: [190][   10/   28]    Loss 0.312962    Top1 88.203125    
2023-01-06 16:26:56,099 - Epoch: [190][   20/   28]    Loss 0.297434    Top1 88.750000    
2023-01-06 16:26:56,157 - Epoch: [190][   28/   28]    Loss 0.302394    Top1 88.491268    
2023-01-06 16:26:56,304 - ==> Top1: 88.491    Loss: 0.302

2023-01-06 16:26:56,304 - ==> Confusion:
[[ 170    8  261]
 [  10  194  398]
 [  55   72 5818]]

2023-01-06 16:26:56,305 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:26:56,305 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:26:56,309 - 

2023-01-06 16:26:56,310 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:26:57,009 - Epoch: [191][   10/  246]    Overall Loss 0.288382    Objective Loss 0.288382                                        LR 0.000003    Time 0.069905    
2023-01-06 16:26:57,171 - Epoch: [191][   20/  246]    Overall Loss 0.289574    Objective Loss 0.289574                                        LR 0.000003    Time 0.043027    
2023-01-06 16:26:57,317 - Epoch: [191][   30/  246]    Overall Loss 0.294721    Objective Loss 0.294721                                        LR 0.000003    Time 0.033529    
2023-01-06 16:26:57,484 - Epoch: [191][   40/  246]    Overall Loss 0.300112    Objective Loss 0.300112                                        LR 0.000003    Time 0.029306    
2023-01-06 16:26:57,666 - Epoch: [191][   50/  246]    Overall Loss 0.301056    Objective Loss 0.301056                                        LR 0.000003    Time 0.027038    
2023-01-06 16:26:57,849 - Epoch: [191][   60/  246]    Overall Loss 0.299793    Objective Loss 0.299793                                        LR 0.000003    Time 0.025583    
2023-01-06 16:26:58,032 - Epoch: [191][   70/  246]    Overall Loss 0.299666    Objective Loss 0.299666                                        LR 0.000003    Time 0.024510    
2023-01-06 16:26:58,215 - Epoch: [191][   80/  246]    Overall Loss 0.301596    Objective Loss 0.301596                                        LR 0.000003    Time 0.023703    
2023-01-06 16:26:58,396 - Epoch: [191][   90/  246]    Overall Loss 0.303626    Objective Loss 0.303626                                        LR 0.000003    Time 0.023074    
2023-01-06 16:26:58,570 - Epoch: [191][  100/  246]    Overall Loss 0.302986    Objective Loss 0.302986                                        LR 0.000003    Time 0.022508    
2023-01-06 16:26:58,737 - Epoch: [191][  110/  246]    Overall Loss 0.303275    Objective Loss 0.303275                                        LR 0.000003    Time 0.021973    
2023-01-06 16:26:58,909 - Epoch: [191][  120/  246]    Overall Loss 0.301996    Objective Loss 0.301996                                        LR 0.000003    Time 0.021577    
2023-01-06 16:26:59,081 - Epoch: [191][  130/  246]    Overall Loss 0.300849    Objective Loss 0.300849                                        LR 0.000003    Time 0.021235    
2023-01-06 16:26:59,252 - Epoch: [191][  140/  246]    Overall Loss 0.301470    Objective Loss 0.301470                                        LR 0.000003    Time 0.020940    
2023-01-06 16:26:59,418 - Epoch: [191][  150/  246]    Overall Loss 0.301782    Objective Loss 0.301782                                        LR 0.000003    Time 0.020648    
2023-01-06 16:26:59,593 - Epoch: [191][  160/  246]    Overall Loss 0.301538    Objective Loss 0.301538                                        LR 0.000003    Time 0.020445    
2023-01-06 16:26:59,768 - Epoch: [191][  170/  246]    Overall Loss 0.301281    Objective Loss 0.301281                                        LR 0.000003    Time 0.020274    
2023-01-06 16:26:59,941 - Epoch: [191][  180/  246]    Overall Loss 0.301526    Objective Loss 0.301526                                        LR 0.000003    Time 0.020106    
2023-01-06 16:27:00,113 - Epoch: [191][  190/  246]    Overall Loss 0.301762    Objective Loss 0.301762                                        LR 0.000003    Time 0.019948    
2023-01-06 16:27:00,285 - Epoch: [191][  200/  246]    Overall Loss 0.301929    Objective Loss 0.301929                                        LR 0.000003    Time 0.019812    
2023-01-06 16:27:00,454 - Epoch: [191][  210/  246]    Overall Loss 0.300477    Objective Loss 0.300477                                        LR 0.000003    Time 0.019671    
2023-01-06 16:27:00,624 - Epoch: [191][  220/  246]    Overall Loss 0.299936    Objective Loss 0.299936                                        LR 0.000003    Time 0.019548    
2023-01-06 16:27:00,797 - Epoch: [191][  230/  246]    Overall Loss 0.300063    Objective Loss 0.300063                                        LR 0.000003    Time 0.019446    
2023-01-06 16:27:00,980 - Epoch: [191][  240/  246]    Overall Loss 0.300040    Objective Loss 0.300040                                        LR 0.000003    Time 0.019396    
2023-01-06 16:27:01,060 - Epoch: [191][  246/  246]    Overall Loss 0.300886    Objective Loss 0.300886    Top1 85.406699    LR 0.000003    Time 0.019248    
2023-01-06 16:27:01,206 - --- validate (epoch=191)-----------
2023-01-06 16:27:01,206 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:01,650 - Epoch: [191][   10/   28]    Loss 0.313614    Top1 88.710938    
2023-01-06 16:27:01,769 - Epoch: [191][   20/   28]    Loss 0.311226    Top1 88.554688    
2023-01-06 16:27:01,826 - Epoch: [191][   28/   28]    Loss 0.305067    Top1 88.634412    
2023-01-06 16:27:02,007 - ==> Top1: 88.634    Loss: 0.305

2023-01-06 16:27:02,007 - ==> Confusion:
[[ 195    8  236]
 [  11  197  394]
 [  77   68 5800]]

2023-01-06 16:27:02,009 - ==> Best [Top1: 88.978   Sparsity:0.00   Params: 46192 on epoch: 176]
2023-01-06 16:27:02,009 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:02,013 - 

2023-01-06 16:27:02,013 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:02,547 - Epoch: [192][   10/  246]    Overall Loss 0.298479    Objective Loss 0.298479                                        LR 0.000003    Time 0.053312    
2023-01-06 16:27:02,698 - Epoch: [192][   20/  246]    Overall Loss 0.287615    Objective Loss 0.287615                                        LR 0.000003    Time 0.034174    
2023-01-06 16:27:02,854 - Epoch: [192][   30/  246]    Overall Loss 0.294593    Objective Loss 0.294593                                        LR 0.000003    Time 0.027972    
2023-01-06 16:27:03,016 - Epoch: [192][   40/  246]    Overall Loss 0.297068    Objective Loss 0.297068                                        LR 0.000003    Time 0.025031    
2023-01-06 16:27:03,177 - Epoch: [192][   50/  246]    Overall Loss 0.299254    Objective Loss 0.299254                                        LR 0.000003    Time 0.023226    
2023-01-06 16:27:03,336 - Epoch: [192][   60/  246]    Overall Loss 0.305082    Objective Loss 0.305082                                        LR 0.000003    Time 0.022000    
2023-01-06 16:27:03,482 - Epoch: [192][   70/  246]    Overall Loss 0.302189    Objective Loss 0.302189                                        LR 0.000003    Time 0.020934    
2023-01-06 16:27:03,636 - Epoch: [192][   80/  246]    Overall Loss 0.301340    Objective Loss 0.301340                                        LR 0.000003    Time 0.020244    
2023-01-06 16:27:03,798 - Epoch: [192][   90/  246]    Overall Loss 0.298707    Objective Loss 0.298707                                        LR 0.000003    Time 0.019782    
2023-01-06 16:27:03,960 - Epoch: [192][  100/  246]    Overall Loss 0.300025    Objective Loss 0.300025                                        LR 0.000003    Time 0.019419    
2023-01-06 16:27:04,120 - Epoch: [192][  110/  246]    Overall Loss 0.299891    Objective Loss 0.299891                                        LR 0.000003    Time 0.019107    
2023-01-06 16:27:04,287 - Epoch: [192][  120/  246]    Overall Loss 0.300046    Objective Loss 0.300046                                        LR 0.000003    Time 0.018904    
2023-01-06 16:27:04,458 - Epoch: [192][  130/  246]    Overall Loss 0.301456    Objective Loss 0.301456                                        LR 0.000003    Time 0.018759    
2023-01-06 16:27:04,626 - Epoch: [192][  140/  246]    Overall Loss 0.300722    Objective Loss 0.300722                                        LR 0.000003    Time 0.018622    
2023-01-06 16:27:04,797 - Epoch: [192][  150/  246]    Overall Loss 0.300389    Objective Loss 0.300389                                        LR 0.000003    Time 0.018518    
2023-01-06 16:27:04,965 - Epoch: [192][  160/  246]    Overall Loss 0.301466    Objective Loss 0.301466                                        LR 0.000003    Time 0.018406    
2023-01-06 16:27:05,130 - Epoch: [192][  170/  246]    Overall Loss 0.300623    Objective Loss 0.300623                                        LR 0.000003    Time 0.018289    
2023-01-06 16:27:05,305 - Epoch: [192][  180/  246]    Overall Loss 0.300244    Objective Loss 0.300244                                        LR 0.000003    Time 0.018246    
2023-01-06 16:27:05,475 - Epoch: [192][  190/  246]    Overall Loss 0.300367    Objective Loss 0.300367                                        LR 0.000003    Time 0.018178    
2023-01-06 16:27:05,649 - Epoch: [192][  200/  246]    Overall Loss 0.300760    Objective Loss 0.300760                                        LR 0.000003    Time 0.018135    
2023-01-06 16:27:05,840 - Epoch: [192][  210/  246]    Overall Loss 0.301322    Objective Loss 0.301322                                        LR 0.000003    Time 0.018183    
2023-01-06 16:27:06,016 - Epoch: [192][  220/  246]    Overall Loss 0.301528    Objective Loss 0.301528                                        LR 0.000003    Time 0.018151    
2023-01-06 16:27:06,164 - Epoch: [192][  230/  246]    Overall Loss 0.301328    Objective Loss 0.301328                                        LR 0.000003    Time 0.018004    
2023-01-06 16:27:06,321 - Epoch: [192][  240/  246]    Overall Loss 0.301197    Objective Loss 0.301197                                        LR 0.000003    Time 0.017908    
2023-01-06 16:27:06,396 - Epoch: [192][  246/  246]    Overall Loss 0.300775    Objective Loss 0.300775    Top1 92.583732    LR 0.000003    Time 0.017776    
2023-01-06 16:27:06,532 - --- validate (epoch=192)-----------
2023-01-06 16:27:06,532 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:06,961 - Epoch: [192][   10/   28]    Loss 0.297357    Top1 89.375000    
2023-01-06 16:27:07,062 - Epoch: [192][   20/   28]    Loss 0.308806    Top1 88.691406    
2023-01-06 16:27:07,118 - Epoch: [192][   28/   28]    Loss 0.303441    Top1 89.020899    
2023-01-06 16:27:07,277 - ==> Top1: 89.021    Loss: 0.303

2023-01-06 16:27:07,277 - ==> Confusion:
[[ 188    9  242]
 [  11  222  369]
 [  54   82 5809]]

2023-01-06 16:27:07,279 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:07,279 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:07,284 - 

2023-01-06 16:27:07,284 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:07,987 - Epoch: [193][   10/  246]    Overall Loss 0.282827    Objective Loss 0.282827                                        LR 0.000003    Time 0.070233    
2023-01-06 16:27:08,161 - Epoch: [193][   20/  246]    Overall Loss 0.287048    Objective Loss 0.287048                                        LR 0.000003    Time 0.043798    
2023-01-06 16:27:08,330 - Epoch: [193][   30/  246]    Overall Loss 0.285982    Objective Loss 0.285982                                        LR 0.000003    Time 0.034811    
2023-01-06 16:27:08,504 - Epoch: [193][   40/  246]    Overall Loss 0.291308    Objective Loss 0.291308                                        LR 0.000003    Time 0.030440    
2023-01-06 16:27:08,668 - Epoch: [193][   50/  246]    Overall Loss 0.290614    Objective Loss 0.290614                                        LR 0.000003    Time 0.027637    
2023-01-06 16:27:08,833 - Epoch: [193][   60/  246]    Overall Loss 0.293433    Objective Loss 0.293433                                        LR 0.000003    Time 0.025771    
2023-01-06 16:27:09,000 - Epoch: [193][   70/  246]    Overall Loss 0.293171    Objective Loss 0.293171                                        LR 0.000003    Time 0.024474    
2023-01-06 16:27:09,171 - Epoch: [193][   80/  246]    Overall Loss 0.293345    Objective Loss 0.293345                                        LR 0.000003    Time 0.023541    
2023-01-06 16:27:09,340 - Epoch: [193][   90/  246]    Overall Loss 0.293642    Objective Loss 0.293642                                        LR 0.000003    Time 0.022807    
2023-01-06 16:27:09,523 - Epoch: [193][  100/  246]    Overall Loss 0.293062    Objective Loss 0.293062                                        LR 0.000003    Time 0.022349    
2023-01-06 16:27:09,700 - Epoch: [193][  110/  246]    Overall Loss 0.293247    Objective Loss 0.293247                                        LR 0.000003    Time 0.021920    
2023-01-06 16:27:09,883 - Epoch: [193][  120/  246]    Overall Loss 0.294811    Objective Loss 0.294811                                        LR 0.000003    Time 0.021616    
2023-01-06 16:27:10,061 - Epoch: [193][  130/  246]    Overall Loss 0.296271    Objective Loss 0.296271                                        LR 0.000003    Time 0.021317    
2023-01-06 16:27:10,236 - Epoch: [193][  140/  246]    Overall Loss 0.297814    Objective Loss 0.297814                                        LR 0.000003    Time 0.021040    
2023-01-06 16:27:10,405 - Epoch: [193][  150/  246]    Overall Loss 0.298036    Objective Loss 0.298036                                        LR 0.000003    Time 0.020764    
2023-01-06 16:27:10,575 - Epoch: [193][  160/  246]    Overall Loss 0.298010    Objective Loss 0.298010                                        LR 0.000003    Time 0.020525    
2023-01-06 16:27:10,745 - Epoch: [193][  170/  246]    Overall Loss 0.298710    Objective Loss 0.298710                                        LR 0.000003    Time 0.020317    
2023-01-06 16:27:10,915 - Epoch: [193][  180/  246]    Overall Loss 0.299352    Objective Loss 0.299352                                        LR 0.000003    Time 0.020132    
2023-01-06 16:27:11,085 - Epoch: [193][  190/  246]    Overall Loss 0.298939    Objective Loss 0.298939                                        LR 0.000003    Time 0.019964    
2023-01-06 16:27:11,252 - Epoch: [193][  200/  246]    Overall Loss 0.298600    Objective Loss 0.298600                                        LR 0.000003    Time 0.019801    
2023-01-06 16:27:11,422 - Epoch: [193][  210/  246]    Overall Loss 0.299813    Objective Loss 0.299813                                        LR 0.000003    Time 0.019666    
2023-01-06 16:27:11,592 - Epoch: [193][  220/  246]    Overall Loss 0.300887    Objective Loss 0.300887                                        LR 0.000003    Time 0.019543    
2023-01-06 16:27:11,761 - Epoch: [193][  230/  246]    Overall Loss 0.301693    Objective Loss 0.301693                                        LR 0.000003    Time 0.019425    
2023-01-06 16:27:11,941 - Epoch: [193][  240/  246]    Overall Loss 0.301186    Objective Loss 0.301186                                        LR 0.000003    Time 0.019367    
2023-01-06 16:27:12,021 - Epoch: [193][  246/  246]    Overall Loss 0.301252    Objective Loss 0.301252    Top1 87.799043    LR 0.000003    Time 0.019219    
2023-01-06 16:27:12,154 - --- validate (epoch=193)-----------
2023-01-06 16:27:12,154 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:12,596 - Epoch: [193][   10/   28]    Loss 0.304492    Top1 88.320312    
2023-01-06 16:27:12,700 - Epoch: [193][   20/   28]    Loss 0.302597    Top1 88.476562    
2023-01-06 16:27:12,758 - Epoch: [193][   28/   28]    Loss 0.304387    Top1 88.620097    
2023-01-06 16:27:12,876 - ==> Top1: 88.620    Loss: 0.304

2023-01-06 16:27:12,877 - ==> Confusion:
[[ 172    9  258]
 [  10  202  390]
 [  55   73 5817]]

2023-01-06 16:27:12,878 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:12,878 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:12,883 - 

2023-01-06 16:27:12,883 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:13,420 - Epoch: [194][   10/  246]    Overall Loss 0.310311    Objective Loss 0.310311                                        LR 0.000003    Time 0.053629    
2023-01-06 16:27:13,584 - Epoch: [194][   20/  246]    Overall Loss 0.309560    Objective Loss 0.309560                                        LR 0.000003    Time 0.034988    
2023-01-06 16:27:13,749 - Epoch: [194][   30/  246]    Overall Loss 0.309175    Objective Loss 0.309175                                        LR 0.000003    Time 0.028825    
2023-01-06 16:27:13,918 - Epoch: [194][   40/  246]    Overall Loss 0.310782    Objective Loss 0.310782                                        LR 0.000003    Time 0.025844    
2023-01-06 16:27:14,084 - Epoch: [194][   50/  246]    Overall Loss 0.310128    Objective Loss 0.310128                                        LR 0.000003    Time 0.023975    
2023-01-06 16:27:14,233 - Epoch: [194][   60/  246]    Overall Loss 0.306224    Objective Loss 0.306224                                        LR 0.000003    Time 0.022456    
2023-01-06 16:27:14,397 - Epoch: [194][   70/  246]    Overall Loss 0.304985    Objective Loss 0.304985                                        LR 0.000003    Time 0.021588    
2023-01-06 16:27:14,555 - Epoch: [194][   80/  246]    Overall Loss 0.304188    Objective Loss 0.304188                                        LR 0.000003    Time 0.020867    
2023-01-06 16:27:14,731 - Epoch: [194][   90/  246]    Overall Loss 0.302906    Objective Loss 0.302906                                        LR 0.000003    Time 0.020497    
2023-01-06 16:27:14,905 - Epoch: [194][  100/  246]    Overall Loss 0.301599    Objective Loss 0.301599                                        LR 0.000003    Time 0.020182    
2023-01-06 16:27:15,075 - Epoch: [194][  110/  246]    Overall Loss 0.299861    Objective Loss 0.299861                                        LR 0.000003    Time 0.019891    
2023-01-06 16:27:15,244 - Epoch: [194][  120/  246]    Overall Loss 0.298337    Objective Loss 0.298337                                        LR 0.000003    Time 0.019638    
2023-01-06 16:27:15,412 - Epoch: [194][  130/  246]    Overall Loss 0.297956    Objective Loss 0.297956                                        LR 0.000003    Time 0.019413    
2023-01-06 16:27:15,577 - Epoch: [194][  140/  246]    Overall Loss 0.299814    Objective Loss 0.299814                                        LR 0.000003    Time 0.019209    
2023-01-06 16:27:15,744 - Epoch: [194][  150/  246]    Overall Loss 0.302695    Objective Loss 0.302695                                        LR 0.000003    Time 0.019036    
2023-01-06 16:27:15,910 - Epoch: [194][  160/  246]    Overall Loss 0.302340    Objective Loss 0.302340                                        LR 0.000003    Time 0.018885    
2023-01-06 16:27:16,077 - Epoch: [194][  170/  246]    Overall Loss 0.301769    Objective Loss 0.301769                                        LR 0.000003    Time 0.018750    
2023-01-06 16:27:16,243 - Epoch: [194][  180/  246]    Overall Loss 0.301251    Objective Loss 0.301251                                        LR 0.000003    Time 0.018629    
2023-01-06 16:27:16,410 - Epoch: [194][  190/  246]    Overall Loss 0.302092    Objective Loss 0.302092                                        LR 0.000003    Time 0.018525    
2023-01-06 16:27:16,577 - Epoch: [194][  200/  246]    Overall Loss 0.301951    Objective Loss 0.301951                                        LR 0.000003    Time 0.018434    
2023-01-06 16:27:16,744 - Epoch: [194][  210/  246]    Overall Loss 0.301659    Objective Loss 0.301659                                        LR 0.000003    Time 0.018351    
2023-01-06 16:27:16,911 - Epoch: [194][  220/  246]    Overall Loss 0.301643    Objective Loss 0.301643                                        LR 0.000003    Time 0.018274    
2023-01-06 16:27:17,078 - Epoch: [194][  230/  246]    Overall Loss 0.300936    Objective Loss 0.300936                                        LR 0.000003    Time 0.018206    
2023-01-06 16:27:17,258 - Epoch: [194][  240/  246]    Overall Loss 0.300351    Objective Loss 0.300351                                        LR 0.000003    Time 0.018195    
2023-01-06 16:27:17,341 - Epoch: [194][  246/  246]    Overall Loss 0.300512    Objective Loss 0.300512    Top1 89.234450    LR 0.000003    Time 0.018086    
2023-01-06 16:27:17,478 - --- validate (epoch=194)-----------
2023-01-06 16:27:17,478 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:17,904 - Epoch: [194][   10/   28]    Loss 0.300492    Top1 88.671875    
2023-01-06 16:27:18,007 - Epoch: [194][   20/   28]    Loss 0.309524    Top1 88.593750    
2023-01-06 16:27:18,065 - Epoch: [194][   28/   28]    Loss 0.304701    Top1 88.763241    
2023-01-06 16:27:18,209 - ==> Top1: 88.763    Loss: 0.305

2023-01-06 16:27:18,209 - ==> Confusion:
[[ 197    8  234]
 [  11  200  391]
 [  69   72 5804]]

2023-01-06 16:27:18,211 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:18,211 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:18,215 - 

2023-01-06 16:27:18,215 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:18,901 - Epoch: [195][   10/  246]    Overall Loss 0.313597    Objective Loss 0.313597                                        LR 0.000003    Time 0.068506    
2023-01-06 16:27:19,063 - Epoch: [195][   20/  246]    Overall Loss 0.299006    Objective Loss 0.299006                                        LR 0.000003    Time 0.042360    
2023-01-06 16:27:19,231 - Epoch: [195][   30/  246]    Overall Loss 0.304130    Objective Loss 0.304130                                        LR 0.000003    Time 0.033824    
2023-01-06 16:27:19,406 - Epoch: [195][   40/  246]    Overall Loss 0.302939    Objective Loss 0.302939                                        LR 0.000003    Time 0.029715    
2023-01-06 16:27:19,577 - Epoch: [195][   50/  246]    Overall Loss 0.302199    Objective Loss 0.302199                                        LR 0.000003    Time 0.027191    
2023-01-06 16:27:19,748 - Epoch: [195][   60/  246]    Overall Loss 0.300072    Objective Loss 0.300072                                        LR 0.000003    Time 0.025501    
2023-01-06 16:27:19,916 - Epoch: [195][   70/  246]    Overall Loss 0.302717    Objective Loss 0.302717                                        LR 0.000003    Time 0.024254    
2023-01-06 16:27:20,080 - Epoch: [195][   80/  246]    Overall Loss 0.305431    Objective Loss 0.305431                                        LR 0.000003    Time 0.023271    
2023-01-06 16:27:20,249 - Epoch: [195][   90/  246]    Overall Loss 0.304436    Objective Loss 0.304436                                        LR 0.000003    Time 0.022561    
2023-01-06 16:27:20,415 - Epoch: [195][  100/  246]    Overall Loss 0.304594    Objective Loss 0.304594                                        LR 0.000003    Time 0.021958    
2023-01-06 16:27:20,585 - Epoch: [195][  110/  246]    Overall Loss 0.303843    Objective Loss 0.303843                                        LR 0.000003    Time 0.021507    
2023-01-06 16:27:20,753 - Epoch: [195][  120/  246]    Overall Loss 0.304030    Objective Loss 0.304030                                        LR 0.000003    Time 0.021104    
2023-01-06 16:27:20,925 - Epoch: [195][  130/  246]    Overall Loss 0.301493    Objective Loss 0.301493                                        LR 0.000003    Time 0.020802    
2023-01-06 16:27:21,091 - Epoch: [195][  140/  246]    Overall Loss 0.301728    Objective Loss 0.301728                                        LR 0.000003    Time 0.020503    
2023-01-06 16:27:21,266 - Epoch: [195][  150/  246]    Overall Loss 0.301012    Objective Loss 0.301012                                        LR 0.000003    Time 0.020302    
2023-01-06 16:27:21,433 - Epoch: [195][  160/  246]    Overall Loss 0.301456    Objective Loss 0.301456                                        LR 0.000003    Time 0.020073    
2023-01-06 16:27:21,607 - Epoch: [195][  170/  246]    Overall Loss 0.300069    Objective Loss 0.300069                                        LR 0.000003    Time 0.019910    
2023-01-06 16:27:21,778 - Epoch: [195][  180/  246]    Overall Loss 0.300683    Objective Loss 0.300683                                        LR 0.000003    Time 0.019755    
2023-01-06 16:27:21,953 - Epoch: [195][  190/  246]    Overall Loss 0.300436    Objective Loss 0.300436                                        LR 0.000003    Time 0.019634    
2023-01-06 16:27:22,123 - Epoch: [195][  200/  246]    Overall Loss 0.300434    Objective Loss 0.300434                                        LR 0.000003    Time 0.019498    
2023-01-06 16:27:22,288 - Epoch: [195][  210/  246]    Overall Loss 0.300090    Objective Loss 0.300090                                        LR 0.000003    Time 0.019358    
2023-01-06 16:27:22,458 - Epoch: [195][  220/  246]    Overall Loss 0.300261    Objective Loss 0.300261                                        LR 0.000003    Time 0.019248    
2023-01-06 16:27:22,624 - Epoch: [195][  230/  246]    Overall Loss 0.300341    Objective Loss 0.300341                                        LR 0.000003    Time 0.019131    
2023-01-06 16:27:22,790 - Epoch: [195][  240/  246]    Overall Loss 0.299953    Objective Loss 0.299953                                        LR 0.000003    Time 0.019025    
2023-01-06 16:27:22,870 - Epoch: [195][  246/  246]    Overall Loss 0.300245    Objective Loss 0.300245    Top1 87.320574    LR 0.000003    Time 0.018884    
2023-01-06 16:27:23,005 - --- validate (epoch=195)-----------
2023-01-06 16:27:23,005 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:23,451 - Epoch: [195][   10/   28]    Loss 0.290262    Top1 88.984375    
2023-01-06 16:27:23,554 - Epoch: [195][   20/   28]    Loss 0.297554    Top1 88.964844    
2023-01-06 16:27:23,614 - Epoch: [195][   28/   28]    Loss 0.302986    Top1 88.720298    
2023-01-06 16:27:23,745 - ==> Top1: 88.720    Loss: 0.303

2023-01-06 16:27:23,745 - ==> Confusion:
[[ 195    8  236]
 [  11  203  388]
 [  73   72 5800]]

2023-01-06 16:27:23,747 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:23,747 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:23,751 - 

2023-01-06 16:27:23,751 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:24,431 - Epoch: [196][   10/  246]    Overall Loss 0.309829    Objective Loss 0.309829                                        LR 0.000003    Time 0.067933    
2023-01-06 16:27:24,575 - Epoch: [196][   20/  246]    Overall Loss 0.303314    Objective Loss 0.303314                                        LR 0.000003    Time 0.041146    
2023-01-06 16:27:24,717 - Epoch: [196][   30/  246]    Overall Loss 0.301830    Objective Loss 0.301830                                        LR 0.000003    Time 0.032157    
2023-01-06 16:27:24,861 - Epoch: [196][   40/  246]    Overall Loss 0.298957    Objective Loss 0.298957                                        LR 0.000003    Time 0.027699    
2023-01-06 16:27:25,011 - Epoch: [196][   50/  246]    Overall Loss 0.299332    Objective Loss 0.299332                                        LR 0.000003    Time 0.025148    
2023-01-06 16:27:25,151 - Epoch: [196][   60/  246]    Overall Loss 0.298674    Objective Loss 0.298674                                        LR 0.000003    Time 0.023297    
2023-01-06 16:27:25,297 - Epoch: [196][   70/  246]    Overall Loss 0.298803    Objective Loss 0.298803                                        LR 0.000003    Time 0.022042    
2023-01-06 16:27:25,444 - Epoch: [196][   80/  246]    Overall Loss 0.299529    Objective Loss 0.299529                                        LR 0.000003    Time 0.021121    
2023-01-06 16:27:25,585 - Epoch: [196][   90/  246]    Overall Loss 0.301514    Objective Loss 0.301514                                        LR 0.000003    Time 0.020335    
2023-01-06 16:27:25,733 - Epoch: [196][  100/  246]    Overall Loss 0.302434    Objective Loss 0.302434                                        LR 0.000003    Time 0.019764    
2023-01-06 16:27:25,879 - Epoch: [196][  110/  246]    Overall Loss 0.303963    Objective Loss 0.303963                                        LR 0.000003    Time 0.019294    
2023-01-06 16:27:26,031 - Epoch: [196][  120/  246]    Overall Loss 0.303392    Objective Loss 0.303392                                        LR 0.000003    Time 0.018948    
2023-01-06 16:27:26,166 - Epoch: [196][  130/  246]    Overall Loss 0.301987    Objective Loss 0.301987                                        LR 0.000003    Time 0.018527    
2023-01-06 16:27:26,306 - Epoch: [196][  140/  246]    Overall Loss 0.301249    Objective Loss 0.301249                                        LR 0.000003    Time 0.018188    
2023-01-06 16:27:26,435 - Epoch: [196][  150/  246]    Overall Loss 0.301288    Objective Loss 0.301288                                        LR 0.000003    Time 0.017837    
2023-01-06 16:27:26,567 - Epoch: [196][  160/  246]    Overall Loss 0.302547    Objective Loss 0.302547                                        LR 0.000003    Time 0.017541    
2023-01-06 16:27:26,696 - Epoch: [196][  170/  246]    Overall Loss 0.303014    Objective Loss 0.303014                                        LR 0.000003    Time 0.017266    
2023-01-06 16:27:26,828 - Epoch: [196][  180/  246]    Overall Loss 0.302962    Objective Loss 0.302962                                        LR 0.000003    Time 0.017040    
2023-01-06 16:27:26,957 - Epoch: [196][  190/  246]    Overall Loss 0.302594    Objective Loss 0.302594                                        LR 0.000003    Time 0.016821    
2023-01-06 16:27:27,089 - Epoch: [196][  200/  246]    Overall Loss 0.302419    Objective Loss 0.302419                                        LR 0.000003    Time 0.016636    
2023-01-06 16:27:27,219 - Epoch: [196][  210/  246]    Overall Loss 0.303028    Objective Loss 0.303028                                        LR 0.000003    Time 0.016461    
2023-01-06 16:27:27,348 - Epoch: [196][  220/  246]    Overall Loss 0.302645    Objective Loss 0.302645                                        LR 0.000003    Time 0.016299    
2023-01-06 16:27:27,476 - Epoch: [196][  230/  246]    Overall Loss 0.302201    Objective Loss 0.302201                                        LR 0.000003    Time 0.016140    
2023-01-06 16:27:27,618 - Epoch: [196][  240/  246]    Overall Loss 0.301029    Objective Loss 0.301029                                        LR 0.000003    Time 0.016060    
2023-01-06 16:27:27,687 - Epoch: [196][  246/  246]    Overall Loss 0.300822    Objective Loss 0.300822    Top1 89.473684    LR 0.000003    Time 0.015946    
2023-01-06 16:27:27,844 - --- validate (epoch=196)-----------
2023-01-06 16:27:27,845 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:28,277 - Epoch: [196][   10/   28]    Loss 0.305774    Top1 88.984375    
2023-01-06 16:27:28,381 - Epoch: [196][   20/   28]    Loss 0.298793    Top1 89.101562    
2023-01-06 16:27:28,439 - Epoch: [196][   28/   28]    Loss 0.305475    Top1 88.849127    
2023-01-06 16:27:28,598 - ==> Top1: 88.849    Loss: 0.305

2023-01-06 16:27:28,599 - ==> Confusion:
[[ 204    7  228]
 [  14  211  377]
 [  74   79 5792]]

2023-01-06 16:27:28,600 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:28,600 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:28,604 - 

2023-01-06 16:27:28,605 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:29,135 - Epoch: [197][   10/  246]    Overall Loss 0.303376    Objective Loss 0.303376                                        LR 0.000003    Time 0.052962    
2023-01-06 16:27:29,293 - Epoch: [197][   20/  246]    Overall Loss 0.301330    Objective Loss 0.301330                                        LR 0.000003    Time 0.034338    
2023-01-06 16:27:29,451 - Epoch: [197][   30/  246]    Overall Loss 0.297519    Objective Loss 0.297519                                        LR 0.000003    Time 0.028126    
2023-01-06 16:27:29,591 - Epoch: [197][   40/  246]    Overall Loss 0.299586    Objective Loss 0.299586                                        LR 0.000003    Time 0.024575    
2023-01-06 16:27:29,730 - Epoch: [197][   50/  246]    Overall Loss 0.298560    Objective Loss 0.298560                                        LR 0.000003    Time 0.022409    
2023-01-06 16:27:29,870 - Epoch: [197][   60/  246]    Overall Loss 0.295032    Objective Loss 0.295032                                        LR 0.000003    Time 0.021000    
2023-01-06 16:27:30,005 - Epoch: [197][   70/  246]    Overall Loss 0.292118    Objective Loss 0.292118                                        LR 0.000003    Time 0.019925    
2023-01-06 16:27:30,140 - Epoch: [197][   80/  246]    Overall Loss 0.295883    Objective Loss 0.295883                                        LR 0.000003    Time 0.019115    
2023-01-06 16:27:30,278 - Epoch: [197][   90/  246]    Overall Loss 0.294391    Objective Loss 0.294391                                        LR 0.000003    Time 0.018528    
2023-01-06 16:27:30,416 - Epoch: [197][  100/  246]    Overall Loss 0.294246    Objective Loss 0.294246                                        LR 0.000003    Time 0.018043    
2023-01-06 16:27:30,552 - Epoch: [197][  110/  246]    Overall Loss 0.296718    Objective Loss 0.296718                                        LR 0.000003    Time 0.017640    
2023-01-06 16:27:30,690 - Epoch: [197][  120/  246]    Overall Loss 0.299683    Objective Loss 0.299683                                        LR 0.000003    Time 0.017313    
2023-01-06 16:27:30,827 - Epoch: [197][  130/  246]    Overall Loss 0.298634    Objective Loss 0.298634                                        LR 0.000003    Time 0.017032    
2023-01-06 16:27:30,969 - Epoch: [197][  140/  246]    Overall Loss 0.298930    Objective Loss 0.298930                                        LR 0.000003    Time 0.016830    
2023-01-06 16:27:31,122 - Epoch: [197][  150/  246]    Overall Loss 0.299242    Objective Loss 0.299242                                        LR 0.000003    Time 0.016721    
2023-01-06 16:27:31,268 - Epoch: [197][  160/  246]    Overall Loss 0.299539    Objective Loss 0.299539                                        LR 0.000003    Time 0.016585    
2023-01-06 16:27:31,415 - Epoch: [197][  170/  246]    Overall Loss 0.300792    Objective Loss 0.300792                                        LR 0.000003    Time 0.016472    
2023-01-06 16:27:31,562 - Epoch: [197][  180/  246]    Overall Loss 0.301077    Objective Loss 0.301077                                        LR 0.000003    Time 0.016376    
2023-01-06 16:27:31,711 - Epoch: [197][  190/  246]    Overall Loss 0.301016    Objective Loss 0.301016                                        LR 0.000003    Time 0.016295    
2023-01-06 16:27:31,854 - Epoch: [197][  200/  246]    Overall Loss 0.301688    Objective Loss 0.301688                                        LR 0.000003    Time 0.016195    
2023-01-06 16:27:31,999 - Epoch: [197][  210/  246]    Overall Loss 0.301388    Objective Loss 0.301388                                        LR 0.000003    Time 0.016112    
2023-01-06 16:27:32,141 - Epoch: [197][  220/  246]    Overall Loss 0.301266    Objective Loss 0.301266                                        LR 0.000003    Time 0.016025    
2023-01-06 16:27:32,289 - Epoch: [197][  230/  246]    Overall Loss 0.300899    Objective Loss 0.300899                                        LR 0.000003    Time 0.015967    
2023-01-06 16:27:32,445 - Epoch: [197][  240/  246]    Overall Loss 0.300252    Objective Loss 0.300252                                        LR 0.000003    Time 0.015952    
2023-01-06 16:27:32,515 - Epoch: [197][  246/  246]    Overall Loss 0.300237    Objective Loss 0.300237    Top1 89.473684    LR 0.000003    Time 0.015847    
2023-01-06 16:27:32,650 - --- validate (epoch=197)-----------
2023-01-06 16:27:32,650 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:33,081 - Epoch: [197][   10/   28]    Loss 0.296138    Top1 88.789062    
2023-01-06 16:27:33,181 - Epoch: [197][   20/   28]    Loss 0.306174    Top1 88.222656    
2023-01-06 16:27:33,237 - Epoch: [197][   28/   28]    Loss 0.304784    Top1 88.448325    
2023-01-06 16:27:33,374 - ==> Top1: 88.448    Loss: 0.305

2023-01-06 16:27:33,374 - ==> Confusion:
[[ 160    9  270]
 [   8  185  409]
 [  47   64 5834]]

2023-01-06 16:27:33,375 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:33,375 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:33,380 - 

2023-01-06 16:27:33,380 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:34,039 - Epoch: [198][   10/  246]    Overall Loss 0.311808    Objective Loss 0.311808                                        LR 0.000003    Time 0.065887    
2023-01-06 16:27:34,193 - Epoch: [198][   20/  246]    Overall Loss 0.312651    Objective Loss 0.312651                                        LR 0.000003    Time 0.040587    
2023-01-06 16:27:34,341 - Epoch: [198][   30/  246]    Overall Loss 0.304904    Objective Loss 0.304904                                        LR 0.000003    Time 0.031970    
2023-01-06 16:27:34,497 - Epoch: [198][   40/  246]    Overall Loss 0.299390    Objective Loss 0.299390                                        LR 0.000003    Time 0.027881    
2023-01-06 16:27:34,653 - Epoch: [198][   50/  246]    Overall Loss 0.296047    Objective Loss 0.296047                                        LR 0.000003    Time 0.025415    
2023-01-06 16:27:34,813 - Epoch: [198][   60/  246]    Overall Loss 0.296715    Objective Loss 0.296715                                        LR 0.000003    Time 0.023844    
2023-01-06 16:27:34,975 - Epoch: [198][   70/  246]    Overall Loss 0.296761    Objective Loss 0.296761                                        LR 0.000003    Time 0.022743    
2023-01-06 16:27:35,144 - Epoch: [198][   80/  246]    Overall Loss 0.297237    Objective Loss 0.297237                                        LR 0.000003    Time 0.022009    
2023-01-06 16:27:35,318 - Epoch: [198][   90/  246]    Overall Loss 0.299623    Objective Loss 0.299623                                        LR 0.000003    Time 0.021490    
2023-01-06 16:27:35,495 - Epoch: [198][  100/  246]    Overall Loss 0.299829    Objective Loss 0.299829                                        LR 0.000003    Time 0.021102    
2023-01-06 16:27:35,670 - Epoch: [198][  110/  246]    Overall Loss 0.300921    Objective Loss 0.300921                                        LR 0.000003    Time 0.020774    
2023-01-06 16:27:35,846 - Epoch: [198][  120/  246]    Overall Loss 0.300067    Objective Loss 0.300067                                        LR 0.000003    Time 0.020510    
2023-01-06 16:27:36,021 - Epoch: [198][  130/  246]    Overall Loss 0.299720    Objective Loss 0.299720                                        LR 0.000003    Time 0.020271    
2023-01-06 16:27:36,194 - Epoch: [198][  140/  246]    Overall Loss 0.300076    Objective Loss 0.300076                                        LR 0.000003    Time 0.020060    
2023-01-06 16:27:36,365 - Epoch: [198][  150/  246]    Overall Loss 0.300632    Objective Loss 0.300632                                        LR 0.000003    Time 0.019861    
2023-01-06 16:27:36,531 - Epoch: [198][  160/  246]    Overall Loss 0.301432    Objective Loss 0.301432                                        LR 0.000003    Time 0.019654    
2023-01-06 16:27:36,706 - Epoch: [198][  170/  246]    Overall Loss 0.302248    Objective Loss 0.302248                                        LR 0.000003    Time 0.019522    
2023-01-06 16:27:36,883 - Epoch: [198][  180/  246]    Overall Loss 0.301723    Objective Loss 0.301723                                        LR 0.000003    Time 0.019419    
2023-01-06 16:27:37,057 - Epoch: [198][  190/  246]    Overall Loss 0.301644    Objective Loss 0.301644                                        LR 0.000003    Time 0.019314    
2023-01-06 16:27:37,232 - Epoch: [198][  200/  246]    Overall Loss 0.301202    Objective Loss 0.301202                                        LR 0.000003    Time 0.019219    
2023-01-06 16:27:37,405 - Epoch: [198][  210/  246]    Overall Loss 0.300966    Objective Loss 0.300966                                        LR 0.000003    Time 0.019128    
2023-01-06 16:27:37,581 - Epoch: [198][  220/  246]    Overall Loss 0.300113    Objective Loss 0.300113                                        LR 0.000003    Time 0.019058    
2023-01-06 16:27:37,751 - Epoch: [198][  230/  246]    Overall Loss 0.300076    Objective Loss 0.300076                                        LR 0.000003    Time 0.018964    
2023-01-06 16:27:37,935 - Epoch: [198][  240/  246]    Overall Loss 0.299765    Objective Loss 0.299765                                        LR 0.000003    Time 0.018939    
2023-01-06 16:27:38,017 - Epoch: [198][  246/  246]    Overall Loss 0.300025    Objective Loss 0.300025    Top1 87.081340    LR 0.000003    Time 0.018810    
2023-01-06 16:27:38,175 - --- validate (epoch=198)-----------
2023-01-06 16:27:38,175 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:38,610 - Epoch: [198][   10/   28]    Loss 0.305153    Top1 88.671875    
2023-01-06 16:27:38,713 - Epoch: [198][   20/   28]    Loss 0.298286    Top1 88.945312    
2023-01-06 16:27:38,773 - Epoch: [198][   28/   28]    Loss 0.304473    Top1 88.777555    
2023-01-06 16:27:38,937 - ==> Top1: 88.778    Loss: 0.304

2023-01-06 16:27:38,937 - ==> Confusion:
[[ 187    8  244]
 [  11  204  387]
 [  59   75 5811]]

2023-01-06 16:27:38,938 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:38,939 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:38,943 - 

2023-01-06 16:27:38,943 - Training epoch: 62882 samples (256 per mini-batch)
2023-01-06 16:27:39,491 - Epoch: [199][   10/  246]    Overall Loss 0.305834    Objective Loss 0.305834                                        LR 0.000003    Time 0.054768    
2023-01-06 16:27:39,669 - Epoch: [199][   20/  246]    Overall Loss 0.303171    Objective Loss 0.303171                                        LR 0.000003    Time 0.036242    
2023-01-06 16:27:39,842 - Epoch: [199][   30/  246]    Overall Loss 0.301081    Objective Loss 0.301081                                        LR 0.000003    Time 0.029928    
2023-01-06 16:27:40,017 - Epoch: [199][   40/  246]    Overall Loss 0.297286    Objective Loss 0.297286                                        LR 0.000003    Time 0.026792    
2023-01-06 16:27:40,187 - Epoch: [199][   50/  246]    Overall Loss 0.298077    Objective Loss 0.298077                                        LR 0.000003    Time 0.024837    
2023-01-06 16:27:40,355 - Epoch: [199][   60/  246]    Overall Loss 0.301925    Objective Loss 0.301925                                        LR 0.000003    Time 0.023489    
2023-01-06 16:27:40,538 - Epoch: [199][   70/  246]    Overall Loss 0.300914    Objective Loss 0.300914                                        LR 0.000003    Time 0.022738    
2023-01-06 16:27:40,714 - Epoch: [199][   80/  246]    Overall Loss 0.303997    Objective Loss 0.303997                                        LR 0.000003    Time 0.022089    
2023-01-06 16:27:40,894 - Epoch: [199][   90/  246]    Overall Loss 0.304792    Objective Loss 0.304792                                        LR 0.000003    Time 0.021626    
2023-01-06 16:27:41,073 - Epoch: [199][  100/  246]    Overall Loss 0.304783    Objective Loss 0.304783                                        LR 0.000003    Time 0.021255    
2023-01-06 16:27:41,256 - Epoch: [199][  110/  246]    Overall Loss 0.304699    Objective Loss 0.304699                                        LR 0.000003    Time 0.020978    
2023-01-06 16:27:41,431 - Epoch: [199][  120/  246]    Overall Loss 0.304195    Objective Loss 0.304195                                        LR 0.000003    Time 0.020689    
2023-01-06 16:27:41,601 - Epoch: [199][  130/  246]    Overall Loss 0.301207    Objective Loss 0.301207                                        LR 0.000003    Time 0.020401    
2023-01-06 16:27:41,776 - Epoch: [199][  140/  246]    Overall Loss 0.300531    Objective Loss 0.300531                                        LR 0.000003    Time 0.020190    
2023-01-06 16:27:41,949 - Epoch: [199][  150/  246]    Overall Loss 0.299795    Objective Loss 0.299795                                        LR 0.000003    Time 0.019998    
2023-01-06 16:27:42,127 - Epoch: [199][  160/  246]    Overall Loss 0.301036    Objective Loss 0.301036                                        LR 0.000003    Time 0.019854    
2023-01-06 16:27:42,298 - Epoch: [199][  170/  246]    Overall Loss 0.300773    Objective Loss 0.300773                                        LR 0.000003    Time 0.019692    
2023-01-06 16:27:42,468 - Epoch: [199][  180/  246]    Overall Loss 0.299720    Objective Loss 0.299720                                        LR 0.000003    Time 0.019541    
2023-01-06 16:27:42,637 - Epoch: [199][  190/  246]    Overall Loss 0.300683    Objective Loss 0.300683                                        LR 0.000003    Time 0.019397    
2023-01-06 16:27:42,809 - Epoch: [199][  200/  246]    Overall Loss 0.300320    Objective Loss 0.300320                                        LR 0.000003    Time 0.019285    
2023-01-06 16:27:42,967 - Epoch: [199][  210/  246]    Overall Loss 0.299492    Objective Loss 0.299492                                        LR 0.000003    Time 0.019119    
2023-01-06 16:27:43,111 - Epoch: [199][  220/  246]    Overall Loss 0.300235    Objective Loss 0.300235                                        LR 0.000003    Time 0.018904    
2023-01-06 16:27:43,252 - Epoch: [199][  230/  246]    Overall Loss 0.300337    Objective Loss 0.300337                                        LR 0.000003    Time 0.018690    
2023-01-06 16:27:43,406 - Epoch: [199][  240/  246]    Overall Loss 0.300096    Objective Loss 0.300096                                        LR 0.000003    Time 0.018553    
2023-01-06 16:27:43,477 - Epoch: [199][  246/  246]    Overall Loss 0.300065    Objective Loss 0.300065    Top1 88.516746    LR 0.000003    Time 0.018391    
2023-01-06 16:27:43,626 - --- validate (epoch=199)-----------
2023-01-06 16:27:43,627 - 6986 samples (256 per mini-batch)
2023-01-06 16:27:44,058 - Epoch: [199][   10/   28]    Loss 0.306955    Top1 88.085938    
2023-01-06 16:27:44,161 - Epoch: [199][   20/   28]    Loss 0.301597    Top1 88.964844    
2023-01-06 16:27:44,217 - Epoch: [199][   28/   28]    Loss 0.300918    Top1 88.849127    
2023-01-06 16:27:44,357 - ==> Top1: 88.849    Loss: 0.301

2023-01-06 16:27:44,358 - ==> Confusion:
[[ 194    8  237]
 [  10  202  390]
 [  62   72 5811]]

2023-01-06 16:27:44,359 - ==> Best [Top1: 89.021   Sparsity:0.00   Params: 46192 on epoch: 192]
2023-01-06 16:27:44,359 - Saving checkpoint to: logs/2023.01.06-160846/qat_checkpoint.pth.tar
2023-01-06 16:27:44,363 - --- test ---------------------
2023-01-06 16:27:44,364 - 13117 samples (256 per mini-batch)
2023-01-06 16:27:44,946 - Test: [   10/   52]    Loss 0.246185    Top1 91.132812    
2023-01-06 16:27:45,059 - Test: [   20/   52]    Loss 0.251040    Top1 90.937500    
2023-01-06 16:27:45,168 - Test: [   30/   52]    Loss 0.248009    Top1 91.184896    
2023-01-06 16:27:45,266 - Test: [   40/   52]    Loss 0.253746    Top1 91.123047    
2023-01-06 16:27:45,353 - Test: [   50/   52]    Loss 0.255082    Top1 91.015625    
2023-01-06 16:27:45,366 - Test: [   52/   52]    Loss 0.255195    Top1 91.004041    
2023-01-06 16:27:45,533 - ==> Top1: 91.004    Loss: 0.255

2023-01-06 16:27:45,533 - ==> Confusion:
[[  185    18   358]
 [   14   215   527]
 [  136   127 11537]]

2023-01-06 16:27:45,607 - 
2023-01-06 16:27:45,607 - Log file for this run: /home/ubuntumaschin/KeyWordSpotting_on_MAX78000/ai8x-training/logs/2023.01.06-160846/2023.01.06-160846.log
