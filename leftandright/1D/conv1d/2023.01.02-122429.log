2023-01-02 12:24:29,376 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.02-122429/2023.01.02-122429.log
2023-01-02 12:24:29,385 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-01-02 12:24:29,385 - Optimizer Args: {'lr': 6e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-01-02 12:24:38,562 - Dataset sizes:
	training=9438
	validation=1048
	test=1317
2023-01-02 12:24:38,562 - Reading compression schedule from: policies/schedule_kws20_v2.yaml
2023-01-02 12:24:38,565 - 

2023-01-02 12:24:38,565 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:24:39,731 - Epoch: [0][   10/   37]    Overall Loss 1.098140    Objective Loss 1.098140                                        LR 0.000060    Time 0.116507    
2023-01-02 12:24:40,716 - Epoch: [0][   20/   37]    Overall Loss 1.096960    Objective Loss 1.096960                                        LR 0.000060    Time 0.107482    
2023-01-02 12:24:41,718 - Epoch: [0][   30/   37]    Overall Loss 1.093258    Objective Loss 1.093258                                        LR 0.000060    Time 0.105013    
2023-01-02 12:24:42,356 - Epoch: [0][   37/   37]    Overall Loss 1.086493    Objective Loss 1.086493    Top1 56.276151    LR 0.000060    Time 0.102399    
2023-01-02 12:24:42,388 - --- validate (epoch=0)-----------
2023-01-02 12:24:42,389 - 1048 samples (256 per mini-batch)
2023-01-02 12:24:42,666 - Epoch: [0][    5/    5]    Loss 1.028270    Top1 59.064885    
2023-01-02 12:24:42,693 - ==> Top1: 59.065    Loss: 1.028

2023-01-02 12:24:42,693 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:24:42,696 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 0]
2023-01-02 12:24:42,696 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:24:42,709 - 

2023-01-02 12:24:42,710 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:24:43,923 - Epoch: [1][   10/   37]    Overall Loss 0.968588    Objective Loss 0.968588                                        LR 0.000060    Time 0.121163    
2023-01-02 12:24:44,975 - Epoch: [1][   20/   37]    Overall Loss 0.883137    Objective Loss 0.883137                                        LR 0.000060    Time 0.113150    
2023-01-02 12:24:46,155 - Epoch: [1][   30/   37]    Overall Loss 0.838541    Objective Loss 0.838541                                        LR 0.000060    Time 0.114759    
2023-01-02 12:24:46,824 - Epoch: [1][   37/   37]    Overall Loss 0.818628    Objective Loss 0.818628    Top1 56.903766    LR 0.000060    Time 0.111126    
2023-01-02 12:24:46,857 - --- validate (epoch=1)-----------
2023-01-02 12:24:46,858 - 1048 samples (256 per mini-batch)
2023-01-02 12:24:47,148 - Epoch: [1][    5/    5]    Loss 0.719077    Top1 59.064885    
2023-01-02 12:24:47,183 - ==> Top1: 59.065    Loss: 0.719

2023-01-02 12:24:47,184 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:24:47,185 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 1]
2023-01-02 12:24:47,186 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:24:47,208 - 

2023-01-02 12:24:47,208 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:24:48,474 - Epoch: [2][   10/   37]    Overall Loss 0.727495    Objective Loss 0.727495                                        LR 0.000060    Time 0.126479    
2023-01-02 12:24:49,534 - Epoch: [2][   20/   37]    Overall Loss 0.724079    Objective Loss 0.724079                                        LR 0.000060    Time 0.116182    
2023-01-02 12:24:50,585 - Epoch: [2][   30/   37]    Overall Loss 0.720393    Objective Loss 0.720393                                        LR 0.000060    Time 0.112475    
2023-01-02 12:24:51,251 - Epoch: [2][   37/   37]    Overall Loss 0.718892    Objective Loss 0.718892    Top1 58.368201    LR 0.000060    Time 0.109200    
2023-01-02 12:24:51,290 - --- validate (epoch=2)-----------
2023-01-02 12:24:51,291 - 1048 samples (256 per mini-batch)
2023-01-02 12:24:51,620 - Epoch: [2][    5/    5]    Loss 0.714124    Top1 59.064885    
2023-01-02 12:24:51,663 - ==> Top1: 59.065    Loss: 0.714

2023-01-02 12:24:51,663 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:24:51,665 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 2]
2023-01-02 12:24:51,666 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:24:51,685 - 

2023-01-02 12:24:51,686 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:24:53,008 - Epoch: [3][   10/   37]    Overall Loss 0.714149    Objective Loss 0.714149                                        LR 0.000060    Time 0.132091    
2023-01-02 12:24:54,086 - Epoch: [3][   20/   37]    Overall Loss 0.711609    Objective Loss 0.711609                                        LR 0.000060    Time 0.119932    
2023-01-02 12:24:55,162 - Epoch: [3][   30/   37]    Overall Loss 0.711795    Objective Loss 0.711795                                        LR 0.000060    Time 0.115792    
2023-01-02 12:24:55,847 - Epoch: [3][   37/   37]    Overall Loss 0.709611    Objective Loss 0.709611    Top1 58.786611    LR 0.000060    Time 0.112406    
2023-01-02 12:24:55,890 - --- validate (epoch=3)-----------
2023-01-02 12:24:55,890 - 1048 samples (256 per mini-batch)
2023-01-02 12:24:56,193 - Epoch: [3][    5/    5]    Loss 0.695658    Top1 59.064885    
2023-01-02 12:24:56,230 - ==> Top1: 59.065    Loss: 0.696

2023-01-02 12:24:56,230 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:24:56,232 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 3]
2023-01-02 12:24:56,232 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:24:56,257 - 

2023-01-02 12:24:56,258 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:24:57,588 - Epoch: [4][   10/   37]    Overall Loss 0.708342    Objective Loss 0.708342                                        LR 0.000060    Time 0.132907    
2023-01-02 12:24:58,683 - Epoch: [4][   20/   37]    Overall Loss 0.706878    Objective Loss 0.706878                                        LR 0.000060    Time 0.121175    
2023-01-02 12:24:59,765 - Epoch: [4][   30/   37]    Overall Loss 0.705171    Objective Loss 0.705171                                        LR 0.000060    Time 0.116831    
2023-01-02 12:25:00,448 - Epoch: [4][   37/   37]    Overall Loss 0.704612    Objective Loss 0.704612    Top1 55.439331    LR 0.000060    Time 0.113172    
2023-01-02 12:25:00,489 - --- validate (epoch=4)-----------
2023-01-02 12:25:00,489 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:00,834 - Epoch: [4][    5/    5]    Loss 0.687107    Top1 59.064885    
2023-01-02 12:25:00,867 - ==> Top1: 59.065    Loss: 0.687

2023-01-02 12:25:00,868 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:00,878 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 4]
2023-01-02 12:25:00,878 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:00,892 - 

2023-01-02 12:25:00,893 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:02,217 - Epoch: [5][   10/   37]    Overall Loss 0.707790    Objective Loss 0.707790                                        LR 0.000060    Time 0.132356    
2023-01-02 12:25:03,303 - Epoch: [5][   20/   37]    Overall Loss 0.705373    Objective Loss 0.705373                                        LR 0.000060    Time 0.120446    
2023-01-02 12:25:04,400 - Epoch: [5][   30/   37]    Overall Loss 0.702936    Objective Loss 0.702936                                        LR 0.000060    Time 0.116844    
2023-01-02 12:25:05,084 - Epoch: [5][   37/   37]    Overall Loss 0.701234    Objective Loss 0.701234    Top1 58.158996    LR 0.000060    Time 0.113199    
2023-01-02 12:25:05,123 - --- validate (epoch=5)-----------
2023-01-02 12:25:05,124 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:05,432 - Epoch: [5][    5/    5]    Loss 0.683130    Top1 59.064885    
2023-01-02 12:25:05,462 - ==> Top1: 59.065    Loss: 0.683

2023-01-02 12:25:05,462 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:05,465 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 5]
2023-01-02 12:25:05,465 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:05,486 - 

2023-01-02 12:25:05,486 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:06,797 - Epoch: [6][   10/   37]    Overall Loss 0.700313    Objective Loss 0.700313                                        LR 0.000060    Time 0.130963    
2023-01-02 12:25:07,948 - Epoch: [6][   20/   37]    Overall Loss 0.697443    Objective Loss 0.697443                                        LR 0.000060    Time 0.123005    
2023-01-02 12:25:09,047 - Epoch: [6][   30/   37]    Overall Loss 0.698259    Objective Loss 0.698259                                        LR 0.000060    Time 0.118605    
2023-01-02 12:25:09,739 - Epoch: [6][   37/   37]    Overall Loss 0.698860    Objective Loss 0.698860    Top1 52.719665    LR 0.000060    Time 0.114852    
2023-01-02 12:25:09,780 - --- validate (epoch=6)-----------
2023-01-02 12:25:09,781 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:10,109 - Epoch: [6][    5/    5]    Loss 0.692095    Top1 59.064885    
2023-01-02 12:25:10,139 - ==> Top1: 59.065    Loss: 0.692

2023-01-02 12:25:10,139 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:10,141 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 6]
2023-01-02 12:25:10,141 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:10,166 - 

2023-01-02 12:25:10,166 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:11,495 - Epoch: [7][   10/   37]    Overall Loss 0.700078    Objective Loss 0.700078                                        LR 0.000060    Time 0.132732    
2023-01-02 12:25:12,628 - Epoch: [7][   20/   37]    Overall Loss 0.698644    Objective Loss 0.698644                                        LR 0.000060    Time 0.122973    
2023-01-02 12:25:13,735 - Epoch: [7][   30/   37]    Overall Loss 0.697837    Objective Loss 0.697837                                        LR 0.000060    Time 0.118888    
2023-01-02 12:25:14,429 - Epoch: [7][   37/   37]    Overall Loss 0.696697    Objective Loss 0.696697    Top1 59.623431    LR 0.000060    Time 0.115126    
2023-01-02 12:25:14,466 - --- validate (epoch=7)-----------
2023-01-02 12:25:14,466 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:14,816 - Epoch: [7][    5/    5]    Loss 0.687703    Top1 59.064885    
2023-01-02 12:25:14,859 - ==> Top1: 59.065    Loss: 0.688

2023-01-02 12:25:14,859 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:14,861 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 7]
2023-01-02 12:25:14,862 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:14,882 - 

2023-01-02 12:25:14,882 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:16,308 - Epoch: [8][   10/   37]    Overall Loss 0.697317    Objective Loss 0.697317                                        LR 0.000060    Time 0.142526    
2023-01-02 12:25:17,417 - Epoch: [8][   20/   37]    Overall Loss 0.695616    Objective Loss 0.695616                                        LR 0.000060    Time 0.126682    
2023-01-02 12:25:18,539 - Epoch: [8][   30/   37]    Overall Loss 0.695381    Objective Loss 0.695381                                        LR 0.000060    Time 0.121848    
2023-01-02 12:25:19,223 - Epoch: [8][   37/   37]    Overall Loss 0.695026    Objective Loss 0.695026    Top1 56.066946    LR 0.000060    Time 0.117255    
2023-01-02 12:25:19,272 - --- validate (epoch=8)-----------
2023-01-02 12:25:19,272 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:19,620 - Epoch: [8][    5/    5]    Loss 0.699706    Top1 59.064885    
2023-01-02 12:25:19,658 - ==> Top1: 59.065    Loss: 0.700

2023-01-02 12:25:19,658 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:19,661 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 8]
2023-01-02 12:25:19,661 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:19,687 - 

2023-01-02 12:25:19,687 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:21,016 - Epoch: [9][   10/   37]    Overall Loss 0.692725    Objective Loss 0.692725                                        LR 0.000060    Time 0.132731    
2023-01-02 12:25:22,127 - Epoch: [9][   20/   37]    Overall Loss 0.695249    Objective Loss 0.695249                                        LR 0.000060    Time 0.121883    
2023-01-02 12:25:23,242 - Epoch: [9][   30/   37]    Overall Loss 0.694771    Objective Loss 0.694771                                        LR 0.000060    Time 0.118421    
2023-01-02 12:25:23,936 - Epoch: [9][   37/   37]    Overall Loss 0.693918    Objective Loss 0.693918    Top1 55.230126    LR 0.000060    Time 0.114771    
2023-01-02 12:25:23,980 - --- validate (epoch=9)-----------
2023-01-02 12:25:23,981 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:24,305 - Epoch: [9][    5/    5]    Loss 0.694419    Top1 59.064885    
2023-01-02 12:25:24,340 - ==> Top1: 59.065    Loss: 0.694

2023-01-02 12:25:24,340 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:24,343 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 9]
2023-01-02 12:25:24,343 - Saving checkpoint to: logs/2023.01.02-122429/checkpoint.pth.tar
2023-01-02 12:25:24,387 - 

2023-01-02 12:25:24,388 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:26,096 - Epoch: [10][   10/   37]    Overall Loss 0.896426    Objective Loss 0.896426                                        LR 0.000060    Time 0.170681    
2023-01-02 12:25:27,590 - Epoch: [10][   20/   37]    Overall Loss 0.806821    Objective Loss 0.806821                                        LR 0.000060    Time 0.160026    
2023-01-02 12:25:29,126 - Epoch: [10][   30/   37]    Overall Loss 0.766941    Objective Loss 0.766941                                        LR 0.000060    Time 0.157857    
2023-01-02 12:25:30,120 - Epoch: [10][   37/   37]    Overall Loss 0.751018    Objective Loss 0.751018    Top1 60.251046    LR 0.000060    Time 0.154864    
2023-01-02 12:25:30,155 - --- validate (epoch=10)-----------
2023-01-02 12:25:30,156 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:30,681 - Epoch: [10][    5/    5]    Loss 0.680484    Top1 59.064885    
2023-01-02 12:25:30,717 - ==> Top1: 59.065    Loss: 0.680

2023-01-02 12:25:30,717 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:30,719 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 10]
2023-01-02 12:25:30,719 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:25:30,731 - 

2023-01-02 12:25:30,731 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:32,452 - Epoch: [11][   10/   37]    Overall Loss 0.686957    Objective Loss 0.686957                                        LR 0.000060    Time 0.171923    
2023-01-02 12:25:33,976 - Epoch: [11][   20/   37]    Overall Loss 0.683408    Objective Loss 0.683408                                        LR 0.000060    Time 0.162161    
2023-01-02 12:25:35,473 - Epoch: [11][   30/   37]    Overall Loss 0.684296    Objective Loss 0.684296                                        LR 0.000060    Time 0.157989    
2023-01-02 12:25:36,455 - Epoch: [11][   37/   37]    Overall Loss 0.683645    Objective Loss 0.683645    Top1 58.158996    LR 0.000060    Time 0.154623    
2023-01-02 12:25:36,498 - --- validate (epoch=11)-----------
2023-01-02 12:25:36,498 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:37,018 - Epoch: [11][    5/    5]    Loss 0.677848    Top1 59.064885    
2023-01-02 12:25:37,051 - ==> Top1: 59.065    Loss: 0.678

2023-01-02 12:25:37,052 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:37,055 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 11]
2023-01-02 12:25:37,055 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:25:37,076 - 

2023-01-02 12:25:37,076 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:38,797 - Epoch: [12][   10/   37]    Overall Loss 0.684729    Objective Loss 0.684729                                        LR 0.000060    Time 0.171930    
2023-01-02 12:25:40,303 - Epoch: [12][   20/   37]    Overall Loss 0.683602    Objective Loss 0.683602                                        LR 0.000060    Time 0.161254    
2023-01-02 12:25:41,792 - Epoch: [12][   30/   37]    Overall Loss 0.684197    Objective Loss 0.684197                                        LR 0.000060    Time 0.157127    
2023-01-02 12:25:42,781 - Epoch: [12][   37/   37]    Overall Loss 0.683887    Objective Loss 0.683887    Top1 56.903766    LR 0.000060    Time 0.154102    
2023-01-02 12:25:42,824 - --- validate (epoch=12)-----------
2023-01-02 12:25:42,825 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:43,351 - Epoch: [12][    5/    5]    Loss 0.682150    Top1 59.064885    
2023-01-02 12:25:43,382 - ==> Top1: 59.065    Loss: 0.682

2023-01-02 12:25:43,382 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:43,385 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 12]
2023-01-02 12:25:43,385 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:25:43,404 - 

2023-01-02 12:25:43,404 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:45,120 - Epoch: [13][   10/   37]    Overall Loss 0.684142    Objective Loss 0.684142                                        LR 0.000060    Time 0.171489    
2023-01-02 12:25:46,610 - Epoch: [13][   20/   37]    Overall Loss 0.682723    Objective Loss 0.682723                                        LR 0.000060    Time 0.160202    
2023-01-02 12:25:48,103 - Epoch: [13][   30/   37]    Overall Loss 0.682309    Objective Loss 0.682309                                        LR 0.000060    Time 0.156529    
2023-01-02 12:25:49,094 - Epoch: [13][   37/   37]    Overall Loss 0.683293    Objective Loss 0.683293    Top1 57.531381    LR 0.000060    Time 0.153697    
2023-01-02 12:25:49,138 - --- validate (epoch=13)-----------
2023-01-02 12:25:49,141 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:49,681 - Epoch: [13][    5/    5]    Loss 0.672113    Top1 59.064885    
2023-01-02 12:25:49,713 - ==> Top1: 59.065    Loss: 0.672

2023-01-02 12:25:49,713 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:49,715 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 13]
2023-01-02 12:25:49,715 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:25:49,729 - 

2023-01-02 12:25:49,729 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:51,444 - Epoch: [14][   10/   37]    Overall Loss 0.683168    Objective Loss 0.683168                                        LR 0.000060    Time 0.171333    
2023-01-02 12:25:52,934 - Epoch: [14][   20/   37]    Overall Loss 0.685416    Objective Loss 0.685416                                        LR 0.000060    Time 0.160159    
2023-01-02 12:25:54,429 - Epoch: [14][   30/   37]    Overall Loss 0.684323    Objective Loss 0.684323                                        LR 0.000060    Time 0.156585    
2023-01-02 12:25:55,426 - Epoch: [14][   37/   37]    Overall Loss 0.683392    Objective Loss 0.683392    Top1 58.786611    LR 0.000060    Time 0.153880    
2023-01-02 12:25:55,465 - --- validate (epoch=14)-----------
2023-01-02 12:25:55,466 - 1048 samples (256 per mini-batch)
2023-01-02 12:25:55,993 - Epoch: [14][    5/    5]    Loss 0.671730    Top1 59.064885    
2023-01-02 12:25:56,025 - ==> Top1: 59.065    Loss: 0.672

2023-01-02 12:25:56,025 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:25:56,027 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 14]
2023-01-02 12:25:56,027 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:25:56,048 - 

2023-01-02 12:25:56,048 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:25:57,767 - Epoch: [15][   10/   37]    Overall Loss 0.677530    Objective Loss 0.677530                                        LR 0.000060    Time 0.171755    
2023-01-02 12:25:59,285 - Epoch: [15][   20/   37]    Overall Loss 0.683647    Objective Loss 0.683647                                        LR 0.000060    Time 0.161762    
2023-01-02 12:26:00,824 - Epoch: [15][   30/   37]    Overall Loss 0.684632    Objective Loss 0.684632                                        LR 0.000060    Time 0.159121    
2023-01-02 12:26:01,821 - Epoch: [15][   37/   37]    Overall Loss 0.683985    Objective Loss 0.683985    Top1 56.485356    LR 0.000060    Time 0.155945    
2023-01-02 12:26:01,861 - --- validate (epoch=15)-----------
2023-01-02 12:26:01,861 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:02,430 - Epoch: [15][    5/    5]    Loss 0.680120    Top1 59.064885    
2023-01-02 12:26:02,463 - ==> Top1: 59.065    Loss: 0.680

2023-01-02 12:26:02,463 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:02,465 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 15]
2023-01-02 12:26:02,465 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:02,484 - 

2023-01-02 12:26:02,485 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:04,193 - Epoch: [16][   10/   37]    Overall Loss 0.684386    Objective Loss 0.684386                                        LR 0.000060    Time 0.170678    
2023-01-02 12:26:05,740 - Epoch: [16][   20/   37]    Overall Loss 0.682680    Objective Loss 0.682680                                        LR 0.000060    Time 0.162676    
2023-01-02 12:26:07,260 - Epoch: [16][   30/   37]    Overall Loss 0.682634    Objective Loss 0.682634                                        LR 0.000060    Time 0.159087    
2023-01-02 12:26:08,263 - Epoch: [16][   37/   37]    Overall Loss 0.683341    Objective Loss 0.683341    Top1 54.393305    LR 0.000060    Time 0.156099    
2023-01-02 12:26:08,299 - --- validate (epoch=16)-----------
2023-01-02 12:26:08,299 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:08,833 - Epoch: [16][    5/    5]    Loss 0.673762    Top1 59.064885    
2023-01-02 12:26:08,871 - ==> Top1: 59.065    Loss: 0.674

2023-01-02 12:26:08,872 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:08,874 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 16]
2023-01-02 12:26:08,874 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:08,894 - 

2023-01-02 12:26:08,895 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:10,694 - Epoch: [17][   10/   37]    Overall Loss 0.684339    Objective Loss 0.684339                                        LR 0.000060    Time 0.179802    
2023-01-02 12:26:12,190 - Epoch: [17][   20/   37]    Overall Loss 0.682805    Objective Loss 0.682805                                        LR 0.000060    Time 0.164669    
2023-01-02 12:26:13,683 - Epoch: [17][   30/   37]    Overall Loss 0.683931    Objective Loss 0.683931                                        LR 0.000060    Time 0.159545    
2023-01-02 12:26:14,675 - Epoch: [17][   37/   37]    Overall Loss 0.683395    Objective Loss 0.683395    Top1 57.740586    LR 0.000060    Time 0.156160    
2023-01-02 12:26:14,714 - --- validate (epoch=17)-----------
2023-01-02 12:26:14,714 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:15,254 - Epoch: [17][    5/    5]    Loss 0.675833    Top1 59.064885    
2023-01-02 12:26:15,290 - ==> Top1: 59.065    Loss: 0.676

2023-01-02 12:26:15,290 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:15,292 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 17]
2023-01-02 12:26:15,292 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:15,306 - 

2023-01-02 12:26:15,307 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:17,070 - Epoch: [18][   10/   37]    Overall Loss 0.680175    Objective Loss 0.680175                                        LR 0.000060    Time 0.176197    
2023-01-02 12:26:18,582 - Epoch: [18][   20/   37]    Overall Loss 0.683646    Objective Loss 0.683646                                        LR 0.000060    Time 0.163659    
2023-01-02 12:26:20,089 - Epoch: [18][   30/   37]    Overall Loss 0.683355    Objective Loss 0.683355                                        LR 0.000060    Time 0.159327    
2023-01-02 12:26:21,092 - Epoch: [18][   37/   37]    Overall Loss 0.683924    Objective Loss 0.683924    Top1 57.322176    LR 0.000060    Time 0.156276    
2023-01-02 12:26:21,139 - --- validate (epoch=18)-----------
2023-01-02 12:26:21,139 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:21,664 - Epoch: [18][    5/    5]    Loss 0.666368    Top1 59.064885    
2023-01-02 12:26:21,692 - ==> Top1: 59.065    Loss: 0.666

2023-01-02 12:26:21,692 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:21,695 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 18]
2023-01-02 12:26:21,695 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:21,709 - 

2023-01-02 12:26:21,710 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:23,419 - Epoch: [19][   10/   37]    Overall Loss 0.684292    Objective Loss 0.684292                                        LR 0.000060    Time 0.170797    
2023-01-02 12:26:24,940 - Epoch: [19][   20/   37]    Overall Loss 0.682113    Objective Loss 0.682113                                        LR 0.000060    Time 0.161438    
2023-01-02 12:26:26,460 - Epoch: [19][   30/   37]    Overall Loss 0.683219    Objective Loss 0.683219                                        LR 0.000060    Time 0.158265    
2023-01-02 12:26:27,458 - Epoch: [19][   37/   37]    Overall Loss 0.683409    Objective Loss 0.683409    Top1 53.765690    LR 0.000060    Time 0.155281    
2023-01-02 12:26:27,503 - --- validate (epoch=19)-----------
2023-01-02 12:26:27,504 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:28,031 - Epoch: [19][    5/    5]    Loss 0.684637    Top1 59.064885    
2023-01-02 12:26:28,065 - ==> Top1: 59.065    Loss: 0.685

2023-01-02 12:26:28,066 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:28,068 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 19]
2023-01-02 12:26:28,068 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:28,093 - 

2023-01-02 12:26:28,093 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:29,811 - Epoch: [20][   10/   37]    Overall Loss 0.684800    Objective Loss 0.684800                                        LR 0.000060    Time 0.171632    
2023-01-02 12:26:31,383 - Epoch: [20][   20/   37]    Overall Loss 0.683262    Objective Loss 0.683262                                        LR 0.000060    Time 0.164377    
2023-01-02 12:26:32,900 - Epoch: [20][   30/   37]    Overall Loss 0.683087    Objective Loss 0.683087                                        LR 0.000060    Time 0.160143    
2023-01-02 12:26:33,891 - Epoch: [20][   37/   37]    Overall Loss 0.683138    Objective Loss 0.683138    Top1 58.368201    LR 0.000060    Time 0.156608    
2023-01-02 12:26:33,928 - --- validate (epoch=20)-----------
2023-01-02 12:26:33,929 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:34,455 - Epoch: [20][    5/    5]    Loss 0.676252    Top1 59.064885    
2023-01-02 12:26:34,483 - ==> Top1: 59.065    Loss: 0.676

2023-01-02 12:26:34,484 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:34,486 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 20]
2023-01-02 12:26:34,486 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:34,511 - 

2023-01-02 12:26:34,511 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:36,221 - Epoch: [21][   10/   37]    Overall Loss 0.681096    Objective Loss 0.681096                                        LR 0.000060    Time 0.170888    
2023-01-02 12:26:37,754 - Epoch: [21][   20/   37]    Overall Loss 0.684499    Objective Loss 0.684499                                        LR 0.000060    Time 0.162037    
2023-01-02 12:26:39,261 - Epoch: [21][   30/   37]    Overall Loss 0.682661    Objective Loss 0.682661                                        LR 0.000060    Time 0.158246    
2023-01-02 12:26:40,252 - Epoch: [21][   37/   37]    Overall Loss 0.682949    Objective Loss 0.682949    Top1 54.602510    LR 0.000060    Time 0.155081    
2023-01-02 12:26:40,296 - --- validate (epoch=21)-----------
2023-01-02 12:26:40,297 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:40,828 - Epoch: [21][    5/    5]    Loss 0.673501    Top1 59.064885    
2023-01-02 12:26:40,862 - ==> Top1: 59.065    Loss: 0.674

2023-01-02 12:26:40,863 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:40,865 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 21]
2023-01-02 12:26:40,865 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:40,891 - 

2023-01-02 12:26:40,892 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:42,671 - Epoch: [22][   10/   37]    Overall Loss 0.683740    Objective Loss 0.683740                                        LR 0.000060    Time 0.177834    
2023-01-02 12:26:44,219 - Epoch: [22][   20/   37]    Overall Loss 0.682511    Objective Loss 0.682511                                        LR 0.000060    Time 0.166286    
2023-01-02 12:26:45,734 - Epoch: [22][   30/   37]    Overall Loss 0.681775    Objective Loss 0.681775                                        LR 0.000060    Time 0.161314    
2023-01-02 12:26:46,728 - Epoch: [22][   37/   37]    Overall Loss 0.681465    Objective Loss 0.681465    Top1 54.602510    LR 0.000060    Time 0.157665    
2023-01-02 12:26:46,778 - --- validate (epoch=22)-----------
2023-01-02 12:26:46,779 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:47,347 - Epoch: [22][    5/    5]    Loss 0.677049    Top1 59.064885    
2023-01-02 12:26:47,375 - ==> Top1: 59.065    Loss: 0.677

2023-01-02 12:26:47,375 - ==> Confusion:
[[  0 429   0]
 [  0 619   0]
 [  0   0   0]]

2023-01-02 12:26:47,377 - ==> Best [Top1: 59.065   Sparsity:0.00   Params: 360896 on epoch: 22]
2023-01-02 12:26:47,377 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:47,402 - 

2023-01-02 12:26:47,402 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:49,214 - Epoch: [23][   10/   37]    Overall Loss 0.676880    Objective Loss 0.676880                                        LR 0.000060    Time 0.181078    
2023-01-02 12:26:50,729 - Epoch: [23][   20/   37]    Overall Loss 0.674435    Objective Loss 0.674435                                        LR 0.000060    Time 0.166250    
2023-01-02 12:26:52,246 - Epoch: [23][   30/   37]    Overall Loss 0.672172    Objective Loss 0.672172                                        LR 0.000060    Time 0.161410    
2023-01-02 12:26:53,239 - Epoch: [23][   37/   37]    Overall Loss 0.668215    Objective Loss 0.668215    Top1 61.506276    LR 0.000060    Time 0.157699    
2023-01-02 12:26:53,279 - --- validate (epoch=23)-----------
2023-01-02 12:26:53,279 - 1048 samples (256 per mini-batch)
2023-01-02 12:26:53,818 - Epoch: [23][    5/    5]    Loss 0.640316    Top1 60.496183    
2023-01-02 12:26:53,857 - ==> Top1: 60.496    Loss: 0.640

2023-01-02 12:26:53,858 - ==> Confusion:
[[ 25 404   0]
 [ 10 609   0]
 [  0   0   0]]

2023-01-02 12:26:53,860 - ==> Best [Top1: 60.496   Sparsity:0.00   Params: 360896 on epoch: 23]
2023-01-02 12:26:53,860 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:26:53,886 - 

2023-01-02 12:26:53,886 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:26:55,625 - Epoch: [24][   10/   37]    Overall Loss 0.639892    Objective Loss 0.639892                                        LR 0.000060    Time 0.173811    
2023-01-02 12:26:57,130 - Epoch: [24][   20/   37]    Overall Loss 0.636577    Objective Loss 0.636577                                        LR 0.000060    Time 0.162086    
2023-01-02 12:26:58,633 - Epoch: [24][   30/   37]    Overall Loss 0.630564    Objective Loss 0.630564                                        LR 0.000060    Time 0.158160    
2023-01-02 12:26:59,629 - Epoch: [24][   37/   37]    Overall Loss 0.626131    Objective Loss 0.626131    Top1 65.690377    LR 0.000060    Time 0.155132    
2023-01-02 12:26:59,672 - --- validate (epoch=24)-----------
2023-01-02 12:26:59,672 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:00,205 - Epoch: [24][    5/    5]    Loss 0.582986    Top1 67.843511    
2023-01-02 12:27:00,241 - ==> Top1: 67.844    Loss: 0.583

2023-01-02 12:27:00,242 - ==> Confusion:
[[213 216   0]
 [121 498   0]
 [  0   0   0]]

2023-01-02 12:27:00,244 - ==> Best [Top1: 67.844   Sparsity:0.00   Params: 360896 on epoch: 24]
2023-01-02 12:27:00,244 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:00,270 - 

2023-01-02 12:27:00,270 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:02,010 - Epoch: [25][   10/   37]    Overall Loss 0.608486    Objective Loss 0.608486                                        LR 0.000060    Time 0.173849    
2023-01-02 12:27:03,550 - Epoch: [25][   20/   37]    Overall Loss 0.599400    Objective Loss 0.599400                                        LR 0.000060    Time 0.163904    
2023-01-02 12:27:05,078 - Epoch: [25][   30/   37]    Overall Loss 0.590619    Objective Loss 0.590619                                        LR 0.000060    Time 0.160194    
2023-01-02 12:27:06,065 - Epoch: [25][   37/   37]    Overall Loss 0.590330    Objective Loss 0.590330    Top1 67.991632    LR 0.000060    Time 0.156543    
2023-01-02 12:27:06,103 - --- validate (epoch=25)-----------
2023-01-02 12:27:06,104 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:06,652 - Epoch: [25][    5/    5]    Loss 0.570840    Top1 70.610687    
2023-01-02 12:27:06,689 - ==> Top1: 70.611    Loss: 0.571

2023-01-02 12:27:06,690 - ==> Confusion:
[[287 142   0]
 [166 453   0]
 [  0   0   0]]

2023-01-02 12:27:06,692 - ==> Best [Top1: 70.611   Sparsity:0.00   Params: 360896 on epoch: 25]
2023-01-02 12:27:06,692 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:06,716 - 

2023-01-02 12:27:06,717 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:08,458 - Epoch: [26][   10/   37]    Overall Loss 0.582032    Objective Loss 0.582032                                        LR 0.000060    Time 0.173974    
2023-01-02 12:27:09,969 - Epoch: [26][   20/   37]    Overall Loss 0.577423    Objective Loss 0.577423                                        LR 0.000060    Time 0.162539    
2023-01-02 12:27:11,484 - Epoch: [26][   30/   37]    Overall Loss 0.575456    Objective Loss 0.575456                                        LR 0.000060    Time 0.158825    
2023-01-02 12:27:12,468 - Epoch: [26][   37/   37]    Overall Loss 0.578123    Objective Loss 0.578123    Top1 69.665272    LR 0.000060    Time 0.155380    
2023-01-02 12:27:12,512 - --- validate (epoch=26)-----------
2023-01-02 12:27:12,513 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:13,052 - Epoch: [26][    5/    5]    Loss 0.579856    Top1 69.847328    
2023-01-02 12:27:13,087 - ==> Top1: 69.847    Loss: 0.580

2023-01-02 12:27:13,088 - ==> Confusion:
[[215 214   0]
 [102 517   0]
 [  0   0   0]]

2023-01-02 12:27:13,089 - ==> Best [Top1: 70.611   Sparsity:0.00   Params: 360896 on epoch: 25]
2023-01-02 12:27:13,090 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:13,108 - 

2023-01-02 12:27:13,108 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:14,878 - Epoch: [27][   10/   37]    Overall Loss 0.560400    Objective Loss 0.560400                                        LR 0.000060    Time 0.176905    
2023-01-02 12:27:16,397 - Epoch: [27][   20/   37]    Overall Loss 0.565002    Objective Loss 0.565002                                        LR 0.000060    Time 0.164367    
2023-01-02 12:27:17,907 - Epoch: [27][   30/   37]    Overall Loss 0.566269    Objective Loss 0.566269                                        LR 0.000060    Time 0.159869    
2023-01-02 12:27:18,918 - Epoch: [27][   37/   37]    Overall Loss 0.564529    Objective Loss 0.564529    Top1 70.711297    LR 0.000060    Time 0.156954    
2023-01-02 12:27:18,964 - --- validate (epoch=27)-----------
2023-01-02 12:27:18,964 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:19,531 - Epoch: [27][    5/    5]    Loss 0.547214    Top1 72.328244    
2023-01-02 12:27:19,567 - ==> Top1: 72.328    Loss: 0.547

2023-01-02 12:27:19,567 - ==> Confusion:
[[296 133   0]
 [157 462   0]
 [  0   0   0]]

2023-01-02 12:27:19,569 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-02 12:27:19,570 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:19,584 - 

2023-01-02 12:27:19,585 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:21,279 - Epoch: [28][   10/   37]    Overall Loss 0.552173    Objective Loss 0.552173                                        LR 0.000060    Time 0.169372    
2023-01-02 12:27:22,780 - Epoch: [28][   20/   37]    Overall Loss 0.557438    Objective Loss 0.557438                                        LR 0.000060    Time 0.159669    
2023-01-02 12:27:24,291 - Epoch: [28][   30/   37]    Overall Loss 0.554597    Objective Loss 0.554597                                        LR 0.000060    Time 0.156810    
2023-01-02 12:27:25,277 - Epoch: [28][   37/   37]    Overall Loss 0.556183    Objective Loss 0.556183    Top1 67.991632    LR 0.000060    Time 0.153775    
2023-01-02 12:27:25,322 - --- validate (epoch=28)-----------
2023-01-02 12:27:25,322 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:25,850 - Epoch: [28][    5/    5]    Loss 0.533008    Top1 70.038168    
2023-01-02 12:27:25,886 - ==> Top1: 70.038    Loss: 0.533

2023-01-02 12:27:25,886 - ==> Confusion:
[[197 232   0]
 [ 82 537   0]
 [  0   0   0]]

2023-01-02 12:27:25,889 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-02 12:27:25,889 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:25,908 - 

2023-01-02 12:27:25,909 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:27,620 - Epoch: [29][   10/   37]    Overall Loss 0.555803    Objective Loss 0.555803                                        LR 0.000060    Time 0.171035    
2023-01-02 12:27:29,154 - Epoch: [29][   20/   37]    Overall Loss 0.555176    Objective Loss 0.555176                                        LR 0.000060    Time 0.162164    
2023-01-02 12:27:30,674 - Epoch: [29][   30/   37]    Overall Loss 0.554577    Objective Loss 0.554577                                        LR 0.000060    Time 0.158760    
2023-01-02 12:27:31,663 - Epoch: [29][   37/   37]    Overall Loss 0.551949    Objective Loss 0.551949    Top1 72.803347    LR 0.000060    Time 0.155439    
2023-01-02 12:27:31,703 - --- validate (epoch=29)-----------
2023-01-02 12:27:31,703 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:32,246 - Epoch: [29][    5/    5]    Loss 0.533952    Top1 71.087786    
2023-01-02 12:27:32,283 - ==> Top1: 71.088    Loss: 0.534

2023-01-02 12:27:32,283 - ==> Confusion:
[[350  79   0]
 [224 395   0]
 [  0   0   0]]

2023-01-02 12:27:32,285 - ==> Best [Top1: 72.328   Sparsity:0.00   Params: 360896 on epoch: 27]
2023-01-02 12:27:32,285 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:32,304 - 

2023-01-02 12:27:32,305 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:34,066 - Epoch: [30][   10/   37]    Overall Loss 0.545362    Objective Loss 0.545362                                        LR 0.000060    Time 0.176031    
2023-01-02 12:27:35,596 - Epoch: [30][   20/   37]    Overall Loss 0.546090    Objective Loss 0.546090                                        LR 0.000060    Time 0.164482    
2023-01-02 12:27:37,088 - Epoch: [30][   30/   37]    Overall Loss 0.544420    Objective Loss 0.544420                                        LR 0.000060    Time 0.159358    
2023-01-02 12:27:38,081 - Epoch: [30][   37/   37]    Overall Loss 0.548753    Objective Loss 0.548753    Top1 68.828452    LR 0.000060    Time 0.156052    
2023-01-02 12:27:38,116 - --- validate (epoch=30)-----------
2023-01-02 12:27:38,116 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:38,651 - Epoch: [30][    5/    5]    Loss 0.521214    Top1 73.377863    
2023-01-02 12:27:38,688 - ==> Top1: 73.378    Loss: 0.521

2023-01-02 12:27:38,688 - ==> Confusion:
[[314 115   0]
 [164 455   0]
 [  0   0   0]]

2023-01-02 12:27:38,690 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 30]
2023-01-02 12:27:38,690 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:38,716 - 

2023-01-02 12:27:38,716 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:40,465 - Epoch: [31][   10/   37]    Overall Loss 0.541181    Objective Loss 0.541181                                        LR 0.000060    Time 0.174737    
2023-01-02 12:27:41,969 - Epoch: [31][   20/   37]    Overall Loss 0.539717    Objective Loss 0.539717                                        LR 0.000060    Time 0.162558    
2023-01-02 12:27:43,466 - Epoch: [31][   30/   37]    Overall Loss 0.548967    Objective Loss 0.548967                                        LR 0.000060    Time 0.158229    
2023-01-02 12:27:44,452 - Epoch: [31][   37/   37]    Overall Loss 0.545155    Objective Loss 0.545155    Top1 73.012552    LR 0.000060    Time 0.154953    
2023-01-02 12:27:44,499 - --- validate (epoch=31)-----------
2023-01-02 12:27:44,499 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:45,091 - Epoch: [31][    5/    5]    Loss 0.517242    Top1 72.614504    
2023-01-02 12:27:45,129 - ==> Top1: 72.615    Loss: 0.517

2023-01-02 12:27:45,130 - ==> Confusion:
[[234 195   0]
 [ 92 527   0]
 [  0   0   0]]

2023-01-02 12:27:45,132 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 30]
2023-01-02 12:27:45,132 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:45,141 - 

2023-01-02 12:27:45,142 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:46,857 - Epoch: [32][   10/   37]    Overall Loss 0.534603    Objective Loss 0.534603                                        LR 0.000060    Time 0.171338    
2023-01-02 12:27:48,363 - Epoch: [32][   20/   37]    Overall Loss 0.532923    Objective Loss 0.532923                                        LR 0.000060    Time 0.160962    
2023-01-02 12:27:49,873 - Epoch: [32][   30/   37]    Overall Loss 0.531661    Objective Loss 0.531661                                        LR 0.000060    Time 0.157629    
2023-01-02 12:27:50,880 - Epoch: [32][   37/   37]    Overall Loss 0.534479    Objective Loss 0.534479    Top1 72.594142    LR 0.000060    Time 0.155005    
2023-01-02 12:27:50,917 - --- validate (epoch=32)-----------
2023-01-02 12:27:50,917 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:51,474 - Epoch: [32][    5/    5]    Loss 0.509270    Top1 73.377863    
2023-01-02 12:27:51,507 - ==> Top1: 73.378    Loss: 0.509

2023-01-02 12:27:51,507 - ==> Confusion:
[[243 186   0]
 [ 93 526   0]
 [  0   0   0]]

2023-01-02 12:27:51,510 - ==> Best [Top1: 73.378   Sparsity:0.00   Params: 360896 on epoch: 32]
2023-01-02 12:27:51,510 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:51,530 - 

2023-01-02 12:27:51,531 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:53,241 - Epoch: [33][   10/   37]    Overall Loss 0.528447    Objective Loss 0.528447                                        LR 0.000060    Time 0.170880    
2023-01-02 12:27:54,751 - Epoch: [33][   20/   37]    Overall Loss 0.527353    Objective Loss 0.527353                                        LR 0.000060    Time 0.160951    
2023-01-02 12:27:56,271 - Epoch: [33][   30/   37]    Overall Loss 0.525250    Objective Loss 0.525250                                        LR 0.000060    Time 0.157941    
2023-01-02 12:27:57,262 - Epoch: [33][   37/   37]    Overall Loss 0.526336    Objective Loss 0.526336    Top1 71.757322    LR 0.000060    Time 0.154819    
2023-01-02 12:27:57,303 - --- validate (epoch=33)-----------
2023-01-02 12:27:57,303 - 1048 samples (256 per mini-batch)
2023-01-02 12:27:57,827 - Epoch: [33][    5/    5]    Loss 0.491994    Top1 73.473282    
2023-01-02 12:27:57,862 - ==> Top1: 73.473    Loss: 0.492

2023-01-02 12:27:57,863 - ==> Confusion:
[[242 187   0]
 [ 91 528   0]
 [  0   0   0]]

2023-01-02 12:27:57,865 - ==> Best [Top1: 73.473   Sparsity:0.00   Params: 360896 on epoch: 33]
2023-01-02 12:27:57,865 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:27:57,885 - 

2023-01-02 12:27:57,885 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:27:59,604 - Epoch: [34][   10/   37]    Overall Loss 0.521053    Objective Loss 0.521053                                        LR 0.000060    Time 0.171687    
2023-01-02 12:28:01,138 - Epoch: [34][   20/   37]    Overall Loss 0.530690    Objective Loss 0.530690                                        LR 0.000060    Time 0.162505    
2023-01-02 12:28:02,645 - Epoch: [34][   30/   37]    Overall Loss 0.528179    Objective Loss 0.528179                                        LR 0.000060    Time 0.158551    
2023-01-02 12:28:03,643 - Epoch: [34][   37/   37]    Overall Loss 0.521929    Objective Loss 0.521929    Top1 77.824268    LR 0.000060    Time 0.155514    
2023-01-02 12:28:03,677 - --- validate (epoch=34)-----------
2023-01-02 12:28:03,677 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:04,222 - Epoch: [34][    5/    5]    Loss 0.490624    Top1 74.236641    
2023-01-02 12:28:04,263 - ==> Top1: 74.237    Loss: 0.491

2023-01-02 12:28:04,263 - ==> Confusion:
[[274 155   0]
 [115 504   0]
 [  0   0   0]]

2023-01-02 12:28:04,265 - ==> Best [Top1: 74.237   Sparsity:0.00   Params: 360896 on epoch: 34]
2023-01-02 12:28:04,265 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:04,289 - 

2023-01-02 12:28:04,289 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:06,011 - Epoch: [35][   10/   37]    Overall Loss 0.507534    Objective Loss 0.507534                                        LR 0.000060    Time 0.172034    
2023-01-02 12:28:07,526 - Epoch: [35][   20/   37]    Overall Loss 0.504536    Objective Loss 0.504536                                        LR 0.000060    Time 0.161735    
2023-01-02 12:28:09,035 - Epoch: [35][   30/   37]    Overall Loss 0.508094    Objective Loss 0.508094                                        LR 0.000060    Time 0.158104    
2023-01-02 12:28:10,031 - Epoch: [35][   37/   37]    Overall Loss 0.510527    Objective Loss 0.510527    Top1 70.920502    LR 0.000060    Time 0.155109    
2023-01-02 12:28:10,074 - --- validate (epoch=35)-----------
2023-01-02 12:28:10,074 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:10,595 - Epoch: [35][    5/    5]    Loss 0.494550    Top1 75.381679    
2023-01-02 12:28:10,631 - ==> Top1: 75.382    Loss: 0.495

2023-01-02 12:28:10,631 - ==> Confusion:
[[266 163   0]
 [ 95 524   0]
 [  0   0   0]]

2023-01-02 12:28:10,633 - ==> Best [Top1: 75.382   Sparsity:0.00   Params: 360896 on epoch: 35]
2023-01-02 12:28:10,633 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:10,653 - 

2023-01-02 12:28:10,654 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:12,404 - Epoch: [36][   10/   37]    Overall Loss 0.504608    Objective Loss 0.504608                                        LR 0.000060    Time 0.174875    
2023-01-02 12:28:13,930 - Epoch: [36][   20/   37]    Overall Loss 0.499761    Objective Loss 0.499761                                        LR 0.000060    Time 0.163724    
2023-01-02 12:28:15,442 - Epoch: [36][   30/   37]    Overall Loss 0.502986    Objective Loss 0.502986                                        LR 0.000060    Time 0.159534    
2023-01-02 12:28:16,445 - Epoch: [36][   37/   37]    Overall Loss 0.502245    Objective Loss 0.502245    Top1 74.895397    LR 0.000060    Time 0.156447    
2023-01-02 12:28:16,487 - --- validate (epoch=36)-----------
2023-01-02 12:28:16,487 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:17,044 - Epoch: [36][    5/    5]    Loss 0.477329    Top1 75.763359    
2023-01-02 12:28:17,070 - ==> Top1: 75.763    Loss: 0.477

2023-01-02 12:28:17,071 - ==> Confusion:
[[249 180   0]
 [ 74 545   0]
 [  0   0   0]]

2023-01-02 12:28:17,073 - ==> Best [Top1: 75.763   Sparsity:0.00   Params: 360896 on epoch: 36]
2023-01-02 12:28:17,073 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:17,093 - 

2023-01-02 12:28:17,093 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:18,820 - Epoch: [37][   10/   37]    Overall Loss 0.483305    Objective Loss 0.483305                                        LR 0.000060    Time 0.172614    
2023-01-02 12:28:20,345 - Epoch: [37][   20/   37]    Overall Loss 0.484079    Objective Loss 0.484079                                        LR 0.000060    Time 0.162486    
2023-01-02 12:28:21,854 - Epoch: [37][   30/   37]    Overall Loss 0.493611    Objective Loss 0.493611                                        LR 0.000060    Time 0.158631    
2023-01-02 12:28:22,852 - Epoch: [37][   37/   37]    Overall Loss 0.494442    Objective Loss 0.494442    Top1 72.594142    LR 0.000060    Time 0.155565    
2023-01-02 12:28:22,893 - --- validate (epoch=37)-----------
2023-01-02 12:28:22,893 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:23,415 - Epoch: [37][    5/    5]    Loss 0.470207    Top1 77.003817    
2023-01-02 12:28:23,453 - ==> Top1: 77.004    Loss: 0.470

2023-01-02 12:28:23,453 - ==> Confusion:
[[354  75   0]
 [166 453   0]
 [  0   0   0]]

2023-01-02 12:28:23,455 - ==> Best [Top1: 77.004   Sparsity:0.00   Params: 360896 on epoch: 37]
2023-01-02 12:28:23,455 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:23,474 - 

2023-01-02 12:28:23,474 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:25,208 - Epoch: [38][   10/   37]    Overall Loss 0.492337    Objective Loss 0.492337                                        LR 0.000060    Time 0.173312    
2023-01-02 12:28:26,736 - Epoch: [38][   20/   37]    Overall Loss 0.490552    Objective Loss 0.490552                                        LR 0.000060    Time 0.163019    
2023-01-02 12:28:28,239 - Epoch: [38][   30/   37]    Overall Loss 0.486630    Objective Loss 0.486630                                        LR 0.000060    Time 0.158751    
2023-01-02 12:28:29,236 - Epoch: [38][   37/   37]    Overall Loss 0.487051    Objective Loss 0.487051    Top1 74.476987    LR 0.000060    Time 0.155653    
2023-01-02 12:28:29,278 - --- validate (epoch=38)-----------
2023-01-02 12:28:29,278 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:29,816 - Epoch: [38][    5/    5]    Loss 0.505163    Top1 77.767176    
2023-01-02 12:28:29,852 - ==> Top1: 77.767    Loss: 0.505

2023-01-02 12:28:29,852 - ==> Confusion:
[[342  87   0]
 [146 473   0]
 [  0   0   0]]

2023-01-02 12:28:29,854 - ==> Best [Top1: 77.767   Sparsity:0.00   Params: 360896 on epoch: 38]
2023-01-02 12:28:29,854 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:29,874 - 

2023-01-02 12:28:29,875 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:31,882 - Epoch: [39][   10/   37]    Overall Loss 0.475242    Objective Loss 0.475242                                        LR 0.000060    Time 0.200612    
2023-01-02 12:28:33,395 - Epoch: [39][   20/   37]    Overall Loss 0.486757    Objective Loss 0.486757                                        LR 0.000060    Time 0.175931    
2023-01-02 12:28:34,881 - Epoch: [39][   30/   37]    Overall Loss 0.486222    Objective Loss 0.486222                                        LR 0.000060    Time 0.166802    
2023-01-02 12:28:35,865 - Epoch: [39][   37/   37]    Overall Loss 0.478628    Objective Loss 0.478628    Top1 81.171548    LR 0.000060    Time 0.161818    
2023-01-02 12:28:35,909 - --- validate (epoch=39)-----------
2023-01-02 12:28:35,909 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:36,442 - Epoch: [39][    5/    5]    Loss 0.485111    Top1 77.576336    
2023-01-02 12:28:36,480 - ==> Top1: 77.576    Loss: 0.485

2023-01-02 12:28:36,481 - ==> Confusion:
[[350  79   0]
 [156 463   0]
 [  0   0   0]]

2023-01-02 12:28:36,483 - ==> Best [Top1: 77.767   Sparsity:0.00   Params: 360896 on epoch: 38]
2023-01-02 12:28:36,483 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:36,493 - 

2023-01-02 12:28:36,494 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:38,234 - Epoch: [40][   10/   37]    Overall Loss 0.458324    Objective Loss 0.458324                                        LR 0.000036    Time 0.173878    
2023-01-02 12:28:39,768 - Epoch: [40][   20/   37]    Overall Loss 0.470816    Objective Loss 0.470816                                        LR 0.000036    Time 0.163640    
2023-01-02 12:28:41,287 - Epoch: [40][   30/   37]    Overall Loss 0.468741    Objective Loss 0.468741                                        LR 0.000036    Time 0.159698    
2023-01-02 12:28:42,293 - Epoch: [40][   37/   37]    Overall Loss 0.464795    Objective Loss 0.464795    Top1 78.870293    LR 0.000036    Time 0.156659    
2023-01-02 12:28:42,329 - --- validate (epoch=40)-----------
2023-01-02 12:28:42,329 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:42,895 - Epoch: [40][    5/    5]    Loss 0.465672    Top1 78.053435    
2023-01-02 12:28:42,930 - ==> Top1: 78.053    Loss: 0.466

2023-01-02 12:28:42,931 - ==> Confusion:
[[319 110   0]
 [120 499   0]
 [  0   0   0]]

2023-01-02 12:28:42,933 - ==> Best [Top1: 78.053   Sparsity:0.00   Params: 360896 on epoch: 40]
2023-01-02 12:28:42,933 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:42,954 - 

2023-01-02 12:28:42,954 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:44,693 - Epoch: [41][   10/   37]    Overall Loss 0.458183    Objective Loss 0.458183                                        LR 0.000036    Time 0.173741    
2023-01-02 12:28:46,209 - Epoch: [41][   20/   37]    Overall Loss 0.463736    Objective Loss 0.463736                                        LR 0.000036    Time 0.162670    
2023-01-02 12:28:47,735 - Epoch: [41][   30/   37]    Overall Loss 0.455568    Objective Loss 0.455568                                        LR 0.000036    Time 0.159279    
2023-01-02 12:28:48,742 - Epoch: [41][   37/   37]    Overall Loss 0.458671    Objective Loss 0.458671    Top1 76.987448    LR 0.000036    Time 0.156362    
2023-01-02 12:28:48,788 - --- validate (epoch=41)-----------
2023-01-02 12:28:48,788 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:49,303 - Epoch: [41][    5/    5]    Loss 0.446147    Top1 77.576336    
2023-01-02 12:28:49,337 - ==> Top1: 77.576    Loss: 0.446

2023-01-02 12:28:49,337 - ==> Confusion:
[[359  70   0]
 [165 454   0]
 [  0   0   0]]

2023-01-02 12:28:49,339 - ==> Best [Top1: 78.053   Sparsity:0.00   Params: 360896 on epoch: 40]
2023-01-02 12:28:49,340 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:49,350 - 

2023-01-02 12:28:49,350 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:51,086 - Epoch: [42][   10/   37]    Overall Loss 0.462631    Objective Loss 0.462631                                        LR 0.000036    Time 0.173548    
2023-01-02 12:28:52,610 - Epoch: [42][   20/   37]    Overall Loss 0.465576    Objective Loss 0.465576                                        LR 0.000036    Time 0.162899    
2023-01-02 12:28:54,136 - Epoch: [42][   30/   37]    Overall Loss 0.456454    Objective Loss 0.456454                                        LR 0.000036    Time 0.159454    
2023-01-02 12:28:55,126 - Epoch: [42][   37/   37]    Overall Loss 0.455820    Objective Loss 0.455820    Top1 79.707113    LR 0.000036    Time 0.156037    
2023-01-02 12:28:55,171 - --- validate (epoch=42)-----------
2023-01-02 12:28:55,171 - 1048 samples (256 per mini-batch)
2023-01-02 12:28:55,701 - Epoch: [42][    5/    5]    Loss 0.451607    Top1 78.244275    
2023-01-02 12:28:55,739 - ==> Top1: 78.244    Loss: 0.452

2023-01-02 12:28:55,739 - ==> Confusion:
[[358  71   0]
 [157 462   0]
 [  0   0   0]]

2023-01-02 12:28:55,741 - ==> Best [Top1: 78.244   Sparsity:0.00   Params: 360896 on epoch: 42]
2023-01-02 12:28:55,742 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:28:55,762 - 

2023-01-02 12:28:55,763 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:28:57,472 - Epoch: [43][   10/   37]    Overall Loss 0.450843    Objective Loss 0.450843                                        LR 0.000036    Time 0.170749    
2023-01-02 12:28:59,000 - Epoch: [43][   20/   37]    Overall Loss 0.448284    Objective Loss 0.448284                                        LR 0.000036    Time 0.161757    
2023-01-02 12:29:00,515 - Epoch: [43][   30/   37]    Overall Loss 0.453720    Objective Loss 0.453720                                        LR 0.000036    Time 0.158321    
2023-01-02 12:29:01,514 - Epoch: [43][   37/   37]    Overall Loss 0.453131    Objective Loss 0.453131    Top1 79.916318    LR 0.000036    Time 0.155352    
2023-01-02 12:29:01,557 - --- validate (epoch=43)-----------
2023-01-02 12:29:01,558 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:02,096 - Epoch: [43][    5/    5]    Loss 0.451307    Top1 78.435115    
2023-01-02 12:29:02,125 - ==> Top1: 78.435    Loss: 0.451

2023-01-02 12:29:02,125 - ==> Confusion:
[[293 136   0]
 [ 90 529   0]
 [  0   0   0]]

2023-01-02 12:29:02,127 - ==> Best [Top1: 78.435   Sparsity:0.00   Params: 360896 on epoch: 43]
2023-01-02 12:29:02,127 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:02,147 - 

2023-01-02 12:29:02,147 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:03,882 - Epoch: [44][   10/   37]    Overall Loss 0.452857    Objective Loss 0.452857                                        LR 0.000036    Time 0.173314    
2023-01-02 12:29:05,402 - Epoch: [44][   20/   37]    Overall Loss 0.455990    Objective Loss 0.455990                                        LR 0.000036    Time 0.162628    
2023-01-02 12:29:06,912 - Epoch: [44][   30/   37]    Overall Loss 0.446562    Objective Loss 0.446562                                        LR 0.000036    Time 0.158750    
2023-01-02 12:29:07,906 - Epoch: [44][   37/   37]    Overall Loss 0.447849    Objective Loss 0.447849    Top1 78.661088    LR 0.000036    Time 0.155551    
2023-01-02 12:29:07,942 - --- validate (epoch=44)-----------
2023-01-02 12:29:07,942 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:08,489 - Epoch: [44][    5/    5]    Loss 0.411968    Top1 80.629771    
2023-01-02 12:29:08,522 - ==> Top1: 80.630    Loss: 0.412

2023-01-02 12:29:08,522 - ==> Confusion:
[[311 118   0]
 [ 85 534   0]
 [  0   0   0]]

2023-01-02 12:29:08,524 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-02 12:29:08,524 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:08,539 - 

2023-01-02 12:29:08,540 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:10,291 - Epoch: [45][   10/   37]    Overall Loss 0.443146    Objective Loss 0.443146                                        LR 0.000036    Time 0.174943    
2023-01-02 12:29:11,794 - Epoch: [45][   20/   37]    Overall Loss 0.439572    Objective Loss 0.439572                                        LR 0.000036    Time 0.162595    
2023-01-02 12:29:13,291 - Epoch: [45][   30/   37]    Overall Loss 0.437534    Objective Loss 0.437534                                        LR 0.000036    Time 0.158296    
2023-01-02 12:29:14,304 - Epoch: [45][   37/   37]    Overall Loss 0.436786    Objective Loss 0.436786    Top1 78.870293    LR 0.000036    Time 0.155695    
2023-01-02 12:29:14,341 - --- validate (epoch=45)-----------
2023-01-02 12:29:14,342 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:14,887 - Epoch: [45][    5/    5]    Loss 0.433202    Top1 78.339695    
2023-01-02 12:29:14,922 - ==> Top1: 78.340    Loss: 0.433

2023-01-02 12:29:14,923 - ==> Confusion:
[[256 173   0]
 [ 54 565   0]
 [  0   0   0]]

2023-01-02 12:29:14,925 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-02 12:29:14,925 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:14,936 - 

2023-01-02 12:29:14,936 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:16,655 - Epoch: [46][   10/   37]    Overall Loss 0.424307    Objective Loss 0.424307                                        LR 0.000036    Time 0.171720    
2023-01-02 12:29:18,155 - Epoch: [46][   20/   37]    Overall Loss 0.433686    Objective Loss 0.433686                                        LR 0.000036    Time 0.160813    
2023-01-02 12:29:19,673 - Epoch: [46][   30/   37]    Overall Loss 0.432379    Objective Loss 0.432379                                        LR 0.000036    Time 0.157805    
2023-01-02 12:29:20,672 - Epoch: [46][   37/   37]    Overall Loss 0.433963    Objective Loss 0.433963    Top1 79.288703    LR 0.000036    Time 0.154936    
2023-01-02 12:29:20,710 - --- validate (epoch=46)-----------
2023-01-02 12:29:20,711 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:21,234 - Epoch: [46][    5/    5]    Loss 0.441081    Top1 79.198473    
2023-01-02 12:29:21,276 - ==> Top1: 79.198    Loss: 0.441

2023-01-02 12:29:21,277 - ==> Confusion:
[[359  70   0]
 [148 471   0]
 [  0   0   0]]

2023-01-02 12:29:21,279 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-02 12:29:21,279 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:21,290 - 

2023-01-02 12:29:21,290 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:23,066 - Epoch: [47][   10/   37]    Overall Loss 0.445148    Objective Loss 0.445148                                        LR 0.000036    Time 0.177420    
2023-01-02 12:29:24,617 - Epoch: [47][   20/   37]    Overall Loss 0.442492    Objective Loss 0.442492                                        LR 0.000036    Time 0.166235    
2023-01-02 12:29:26,155 - Epoch: [47][   30/   37]    Overall Loss 0.434832    Objective Loss 0.434832                                        LR 0.000036    Time 0.162079    
2023-01-02 12:29:27,169 - Epoch: [47][   37/   37]    Overall Loss 0.432296    Objective Loss 0.432296    Top1 81.380753    LR 0.000036    Time 0.158811    
2023-01-02 12:29:27,212 - --- validate (epoch=47)-----------
2023-01-02 12:29:27,212 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:27,739 - Epoch: [47][    5/    5]    Loss 0.407892    Top1 79.675573    
2023-01-02 12:29:27,774 - ==> Top1: 79.676    Loss: 0.408

2023-01-02 12:29:27,775 - ==> Confusion:
[[351  78   0]
 [135 484   0]
 [  0   0   0]]

2023-01-02 12:29:27,777 - ==> Best [Top1: 80.630   Sparsity:0.00   Params: 360896 on epoch: 44]
2023-01-02 12:29:27,777 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:27,788 - 

2023-01-02 12:29:27,788 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:29,548 - Epoch: [48][   10/   37]    Overall Loss 0.421775    Objective Loss 0.421775                                        LR 0.000036    Time 0.175886    
2023-01-02 12:29:31,120 - Epoch: [48][   20/   37]    Overall Loss 0.418497    Objective Loss 0.418497                                        LR 0.000036    Time 0.166505    
2023-01-02 12:29:32,664 - Epoch: [48][   30/   37]    Overall Loss 0.428619    Objective Loss 0.428619                                        LR 0.000036    Time 0.162449    
2023-01-02 12:29:33,687 - Epoch: [48][   37/   37]    Overall Loss 0.428286    Objective Loss 0.428286    Top1 78.033473    LR 0.000036    Time 0.159344    
2023-01-02 12:29:33,729 - --- validate (epoch=48)-----------
2023-01-02 12:29:33,729 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:34,258 - Epoch: [48][    5/    5]    Loss 0.399850    Top1 80.725191    
2023-01-02 12:29:34,296 - ==> Top1: 80.725    Loss: 0.400

2023-01-02 12:29:34,296 - ==> Confusion:
[[305 124   0]
 [ 78 541   0]
 [  0   0   0]]

2023-01-02 12:29:34,299 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 360896 on epoch: 48]
2023-01-02 12:29:34,299 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:34,325 - 

2023-01-02 12:29:34,325 - Training epoch: 9438 samples (256 per mini-batch)
2023-01-02 12:29:36,074 - Epoch: [49][   10/   37]    Overall Loss 0.445050    Objective Loss 0.445050                                        LR 0.000036    Time 0.174717    
2023-01-02 12:29:37,613 - Epoch: [49][   20/   37]    Overall Loss 0.433374    Objective Loss 0.433374                                        LR 0.000036    Time 0.164277    
2023-01-02 12:29:39,143 - Epoch: [49][   30/   37]    Overall Loss 0.424039    Objective Loss 0.424039                                        LR 0.000036    Time 0.160530    
2023-01-02 12:29:40,164 - Epoch: [49][   37/   37]    Overall Loss 0.424788    Objective Loss 0.424788    Top1 80.962343    LR 0.000036    Time 0.157739    
2023-01-02 12:29:40,224 - --- validate (epoch=49)-----------
2023-01-02 12:29:40,224 - 1048 samples (256 per mini-batch)
2023-01-02 12:29:40,757 - Epoch: [49][    5/    5]    Loss 0.430492    Top1 80.629771    
2023-01-02 12:29:40,792 - ==> Top1: 80.630    Loss: 0.430

2023-01-02 12:29:40,793 - ==> Confusion:
[[298 131   0]
 [ 72 547   0]
 [  0   0   0]]

2023-01-02 12:29:40,795 - ==> Best [Top1: 80.725   Sparsity:0.00   Params: 360896 on epoch: 48]
2023-01-02 12:29:40,795 - Saving checkpoint to: logs/2023.01.02-122429/qat_checkpoint.pth.tar
2023-01-02 12:29:40,814 - --- test ---------------------
2023-01-02 12:29:40,815 - 1317 samples (256 per mini-batch)
2023-01-02 12:29:41,442 - Test: [    6/    6]    Loss 0.464377    Top1 77.524677    
2023-01-02 12:29:41,489 - ==> Top1: 77.525    Loss: 0.464

2023-01-02 12:29:41,490 - ==> Confusion:
[[385 176   0]
 [120 636   0]
 [  0   0   0]]

2023-01-02 12:29:41,512 - 
2023-01-02 12:29:41,513 - Log file for this run: /home/philipp/keyWordSpotting/ai8x-training/logs/2023.01.02-122429/2023.01.02-122429.log
